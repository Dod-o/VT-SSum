{
    "id": "lrw2fnbnt4wbj2lzp5a2chcchw3zitam",
    "title": "Separating Sources and Analysing Connectivity in EEG/MEG Using Probabilistic Models",
    "info": {
        "author": [
            "Aapo Hyv\u00e4rinen, University of Helsinki"
        ],
        "published": "Dec. 3, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Technology->Neurotechnology",
            "Top->Computer Science->Machine Learning",
            "Top->Medicine->Neuroscience"
        ]
    },
    "url": "http://videolectures.net/bbci2012_hyvarinen_probabilistic_models/",
    "segmentation": [
        [
            "So what I'm going to talk about is basically two things that kind of general theory, machine learning, theory of separating sources and then analyzing connectivity, particularly concentrate."
        ],
        [
            "EG and MG.",
            "So basically the first part.",
            "Well, the first half of my talk is about kind of a general introduction to ICA.",
            "Basically assuming that you don't know anything about the theory of ICA.",
            "So I will start with the introduction of the basic problem, light source separation.",
            "Then I will consider the importance of non gaussianity in this kind of models, an especially when I want to emphasize is the difference to principal component analysis.",
            "And then the second half of the talk will be about applying this kind of ideas to analysis of EG and MG, and in particular I will talk about spontaneous activity an which and the kind of kind of archetypal case of spontaneous activity is the resting state activity.",
            "So I will first talk about some improvements to the basic theory of ICA for the purposes of analyzing this kind of resting state or spontaneous EG.",
            "OMG by applying ice, it's time, frequency decompositions and also by using a spatial version of ICA.",
            "Then I will go to another.",
            "Rather different topic, well kind of different but linked topic which is about testing this independent components.",
            "So basically when we do do ICA, how do we know that the components actually correspond to something real?",
            "I mean usually always when you do some kind of estimation you should always also do some testing, but in I see it is kind of being pretty much neglected so far.",
            "So I have recently been developing a framework for that using the idea of intersubject consistency and then in the end I will go to this vast topic of effective connectivity or causal analysis.",
            "By basically introducing a framework for that kind of analysis which is closely related to ICA, it is it uses non gaussianity in structural equation modeling."
        ],
        [
            "OK.",
            "So I will start so from the very beginning of ICA, and this is also kind of a historical background, so I see a.",
            "So the point is that let's assume that we have a system where we have a number of source signals.",
            "Well, in the context of brain imaging, especially GM, it's kind of easy to imagine this as being four different sources in the brain, but what often happens is that we can only observe linear mixtures of this kind of source signals.",
            "That is obviously true for the when you have EG or image recordings.",
            "Where you are, of course you measure some kind of complicated linear mixture, so many sources, but so in this case we for illustration purposes we assume we have just a small number of source signals for an.",
            "What we observe is this.",
            "For mixtures of the signals.",
            "So how can we actually recover the original signals from these mixtures without actually knowing anything about the mixing system?",
            "So that's why it is called blind.",
            "Well."
        ],
        [
            "Some people used to think.",
            "I hope nobody thinks that anymore is that you could do something like this with principal component analysis, but it just doesn't work.",
            "There is actually no theoretical reason why principal component analysis would actually find the original signals, and this is actually what you get with."
        ],
        [
            "Sea.",
            "But if we now use information on the statistical independence of these components.",
            "Then we can actually recover the original signals.",
            "So statistical independence basically means simply that we think consider the amplitudes of these signals.",
            "So the values in each time point as a random variable, and then we say that these random variables before random variable should be rather independent.",
            "So.",
            "Which may be a useful approximation, although of course we know.",
            "Actually I realized that I'm this schedule of the summer School is a bit.",
            "Bit reversed in the sense that yesterday Ricardo Vigario was basically talking about various kinds of problems with this kind of framework, while only now I'm actually introducing the basic framework.",
            "But let me just say that, well, I think that in many cases this kind of basic assumption of independence is quite quite useful.",
            "An approximately full field.",
            "But OK so.",
            "That's of course in an application dependent question.",
            "In the general theoretical framework, we just assume that's the sources are independent.",
            "So this then led to the."
        ],
        [
            "So the model of independent component analysis in the late 80s.",
            "So where the idea is that?",
            "So we basically say that the opposite random variables denoted by XI, some linear combinations of some hidden variables, SJ called the independent components and they basically the observations are then a linear mixing of these original independent components by some mixing made by some mixing coefficients which we don't know.",
            "So it's actually rather similar models factor analysis if you happen to know the classical theory of multivarious statistics.",
            "Yes, and so then the problem really is to estimate both the Ace AI JS and the SJS by observation of XI only.",
            "So it is a form of unsupervised learning.",
            "In the terminology of machine learning, if we for example knew both X&S, then it would be a case a classical case of supervised learning, and if we knew both X and the AI JS, that would be kind of a classical inverse problem.",
            "A linear inverse problem.",
            "But here in this case we basically don't know neither 8th nor the SS Anne.",
            "It's unsupervised learning then."
        ],
        [
            "So in order for us to be able to estimate this model, it's actually enough to make just a couple of assumptions.",
            "First of all, we assume that these.",
            "It leaves hidden variables SJSIR mutual statistically independent and 2nd, which is the most important thing is that we assume that they have non Gaussian or non normal distributions and this is really the departure from the classical framework of factor analysis or principal component analysis.",
            "The idea of non Gaussian source signals or independent components.",
            "It is usually always.",
            "It usually also assumed in the theory that's the number of independent components is equal to the number of observed variables.",
            "But that is not strictly necessary.",
            "It's basically to simplify the theory.",
            "It simplifies the theory because the mixing matrix and has a unique inverse.",
            "The system can be can be uniquely inverted, but it's not.",
            "It's not strictly necessary actually.",
            "OK, so then, well, basically it's usually the proof that in this case the mixing matrix and components can be identified is usually attributed to common, although of course there were some earlier results in the same in the same field.",
            "So what it means is that we can, so we can basically recover both the a sun dresses up to certain throughout the trivial indeterminacies like scaling and signs of the components.",
            "So this is actually, well, this is a very surprising results in the framework of classical factor analysis where people have been working for had been working for almost 100 years on this kind of a problem, and always they found that, well, the problem is basically just undetermined.",
            "There is what they call a factor rotation that always remains undetermined.",
            "But well, the idea non gaussianity really helps us quite a lot."
        ],
        [
            "So it is important to understand then the connection.",
            "So classical ideas like classical methods like principal component analysis and factor analysis.",
            "So I will go through them.",
            "Well, very very briefly.",
            "So in principal component analysis, well, the basic idea is that we look at linear combinations of the opposite variables that have maximum variance.",
            "In order for this I have this problem to be well defined, we have to fix the norm of W2, something.",
            "Otherwise W would simply become infinite.",
            "And then we can find many principal components by iterating this so that we find always new components which are orthogonal to those components previously found 1 by 1.",
            "An essentially classical factor analysis has, well, it's.",
            "It's in certain ways equivalent to this, although the basic, although it is usually formulated in terms of a generative model abit like the ICA model, but in any case in both cases the really the basic idea is to.",
            "The basic goal is really to explain Max the maximum amount of variance with a limited number of components, so this is actually if you look at kind of really at the classical theory this is you can see that this is what.",
            "Factor analysis and PCI can actually do.",
            "So some people then kind of think that in addition to this factor analysis or PCA would be actually able to do this kind of blind source separation, But actually that that is not the case.",
            "It can easily be shown in simulations, and there is actually no theorie in factor analysis that would even claim that you can actually separate the original signals, but it is still a classical misunderstanding."
        ],
        [
            "So basically what we can if well, this is for people who basically know that there's something about like the theory of cloud factor analysis.",
            "So if you happen to know factor in the theory of factor analysis, you can simply see ICA as a non Gaussian version of factor analysis which is also simplified in the sense that you don't have any kind of noise or specific factors in your model.",
            "Well, if you don't know this year of factor analysis, you can just perhaps ignore much of this slide.",
            "Also.",
            "So in the theory of factor analysis, they talk about the problem of factor rotation, which means basically that the model is very strongly in determined.",
            "There is a lot of things that you cannot estimate in the model.",
            "But well, basically in ICA we I see it can be seen as basically a method of factor rotation.",
            "The identifiability result by common that I just cited actually shows that there is no factor rotation left undetermined like in ordinary factor analysis.",
            "So that's why then in ICA the components really give those origianal so signals or underlying latent variables.",
            "Of course, in the theoretical case, assuming that that's the model exactly holds.",
            "So then important catch here is that well, I mean, the reason why why this is ICA?",
            "Well, one of the main reasons why ICA has is a very recent method.",
            "Very recent invention is really this idea of non gaussianity.",
            "It is certainly true that in many cases, in classical applications of statistics.",
            "The variables also Gaussian, that you cannot actually do something like ICA like classically.",
            "Perhaps for this audience, the most closest closest field would be Psycho metrics where you have psychological variables like intelligence for example, which is actually historically the very first factor that was estimated, and those may actually be Gaussian because they are as the classical argument goes, they are sums of of a large number of hidden hidden.",
            "Well.",
            "Independent variables which might be called factors, but that would be confusing in this case and by the central limit theorem when you have a sum of many independent variables that will be close to Gaussian.",
            "So in the case of intelligence it is presumably a sum of many genetic factors.",
            "Many environmental factors that perhaps come together more or less independently, so you might assume that some kind of general factor of intelligence is rather gauss.",
            "But the point is that now if we go to more modern applications of statistics or machine learning like for an L analyse basically another way we analyze outputs of set of physical sensors like e.g image then this kind of logic simply doesn't hold at all and the signals can be actually very very very non Gaussian and they typically are.",
            "So here's some examples of case."
        ],
        [
            "Is where we might have known goes easy to get non gaussianity.",
            "First of all, if you have a strong oscillatory signal like this, well then it is non Gaussian.",
            "So what I'm plotting here on the first top row is 3 signals and here I'm putting the history.",
            "I'm plotting the histograms.",
            "So always when we talk about non gaussianity in the case of basic ICA, we're really talking about the non gaussianity of simply a histogram of the signal.",
            "So the values the distribution of the variable when you just think about.",
            "The amplitudes as the as the value of the random variable.",
            "So now if you plot the histogram of this kind of a signal, it will be strongly non Gaussian.",
            "The closest Gaussian approximation that is the Gaussian distribution with the same mean and variance is actually.",
            "Put it here as the red curve and clearly it's a rather bad approximation of this distribution.",
            "Another case would be something where you basically have.",
            "Periods of hardly any activity.",
            "Activity in some kind of EG sense and then you have short periods of activity like here.",
            "So you have basically just noise here and then you have something a bit like an evoked response, and this is also non Gaussian.",
            "It is actually as you can see here, it has basically a kind of a peak at zero.",
            "This is what they call sparsity.",
            "I will have a bit more on that a bit later and now here is another example where we have basically two periods of short activity and peak at zero is even higher.",
            "So this kind of signals are kind of not very likely to be very Gaussian.",
            "The noise maybe Gaussian, which kind of then confuses things and makes things a little bit more Gaussian, but the underlying signals typically are quite non Gaussian I would say."
        ],
        [
            "So, So what is wrong with things like PCA and factor analysis?",
            "Well, basically there are many ways of looking at this.",
            "One simple way of looking at it is that well in PCA and factor analysis.",
            "The idea is that we basically find components which are uncorrelated and maximize the explained variance well.",
            "Correlations or covariances.",
            "An variances are basically all what they call 2nd order statistics.",
            "They can all be derived from the matrix of covariances, so all the information used by these two by factor analysis or PCA.",
            "It's basically contained in the covariance matrix of the data.",
            "In fact, these methods fall into the larger category of what some people call the covariance analysis methods.",
            "Now, however, it's rather easy to see why this kind of methods cannot actually determine the whole mixing matrix.",
            "That is because this matrix of the covariance is symmetric and that's why it has approximately 1/2 of N squared well numbers.",
            "But the number of elements in the mixing matrix or what I call here the factor loading matrix is basically N squared because it has no restriction of symmetric and it is an N * N matrix.",
            "So basically then there is simply not enough information in this covariance matrix to solve all these N squared elements in the mixing matrix, because there's simply you would get something where you have a system where you have much more variables, twice as many variables as you have equations.",
            "So we basically need more information than what is contained in the covariances.",
            "And that's actually."
        ],
        [
            "That is the whole point in non gaussianity in non Gaussian variables we actually can then use the non gaussianity to get more information well and non gaussianity together with independence.",
            "Well, one theory or in one theorem in probability says that basically if we have two random variables which are independent, Y1 and Y2, then we can basically take any nonlinear transformations or any only functions H1 and H2, and those transformations will always be uncorrelated.",
            "So well, this is 1 intuitive way of seeing how we could use the non gaussianity so we could look at some kind of nonlinear nonlinear covariances of the two variables.",
            "Well, this is not.",
            "Well, some it is not how most ICA methods actually work, but this is one way of looking at how you get more information by using non Gaussian.",
            "But this is simply now you might think that why don't?",
            "Why can't you then take this nonlinear transformations for Gaussian variables as well?",
            "Well, it's because those nonlinear transformations would actually not give you any more information because the Gaussian distribution is completely determined by the covariances and actually also the means.",
            "So I'm actually actually.",
            "I'm always here assuming.",
            "That the variables are centered and the means are zero.",
            "So then the only information that we have is in the covariance.",
            "Another way of looking at this is that now if we find Gaussian variables which are uncorrelated, then they are already independent.",
            "Now this is a deep theorem in probability theory.",
            "If you have uncorrelated jointly Gaussian variables, then they are also independent.",
            "So this independence doesn't bring you any any more information than what uncorrelatedness does.",
            "So if you calculate it, all kinds of nonlinear transformations for cows have variables.",
            "It would simply be the calculations would simply be pointless.",
            "You could actually the nonlinear correlations are simply some complicated functions of the linear correlations.",
            "So the Gaussian distribution is in that sense you might save apps that generate.",
            "So yeah, so this is basically because why the ICM model cannot be estimated for Gaussian data.",
            "There is actually a third, a Third Point, third viewpoint.",
            "So there are here many, many viewpoints on why girls share variables are kind of a no good in this kind of a factor analysis.",
            "I say context.",
            "A third one here is that when you have a distribution, when you look at the distribution of uncorrelated Gaussian variables which have the same variance, then it is.",
            "Actually, it's a symmetric distribution.",
            "We will see that in and graphically abit late."
        ],
        [
            "So here's a simple illustration of what happens.",
            "So here we have first a distribution of the of two independent components, which have uniform distributions.",
            "So if the independent components the two axis have independent sort of uniform distributions and they are independent, then the joint distribution is simply a uniform distribution on this kind of a square.",
            "So now if we take a linear transformation so we have a linear mixing like in the ICM model, we will get the distribution which looks something a bit like this.",
            "Now if we do PCA.",
            "Anne, and also standardize the variance each of the components.",
            "For illustration, we will actually get something like this, so we see that PCA does not actually recover the original components because the square is.",
            "The square is essentially rotated.",
            "This is actually the factor rotation indeterminacy.",
            "If you know that terminology.",
            "But at the same time you will see that.",
            "Well, there's intuitively speaking, there must be some way of figuring out the original independent components, because in this trivial illustration you could simply kind of locate the edges of the square and those will.",
            "More or less obviously give you the right directions of the independent components.",
            "And, well, that's actually if you run an IC algorithm, you will actually then get this distribution.",
            "That is the original 1 and you recover the independent components."
        ],
        [
            "So why doesn't this work with with the Gaussian distribution?",
            "Here I have a kind of a graphical graphical viewpoint on that on this problem.",
            "So we have two independent Gaussian variables of unit variance and we get this kind of rather.",
            "Non informative distribution an the observed mixtures after a linear mixing.",
            "Then we'll have a distribution which is something like this.",
            "So the opposite variables exercise.",
            "Now if we do PCA and standardize the components to unit variance, we get uncorrelated Gaussian variables which have unit variance.",
            "And by the theory I explained before, so because they aren't correlated, they are actually also independent.",
            "So we actually have here exactly the same distribution.",
            "As here 2 uncorrelated random, two uncorrelated Gaussian variables with unit variance?",
            "So well from that viewpoint also, you would think that, well, you can't really recover the you don't actually know.",
            "Well, OK, from that viewpoint you might think that maybe you actually found the original components because you have here exactly the same distribution as here, but the point is that this distribution here is completely spherically symmetric, although it's a bit difficult to see in this kind of a scatter plot, so that's why I have plotted with the red curves the ISO contours of the probability distribution function.",
            "So the probability distribution function of the of this data here has is spherical in the sense that it's it's constant on all spheres.",
            "So what that means is that you can basically then rotate this distribution anyway you like, and the distribution will not change at all.",
            "Because of this spherical symmetry.",
            "And so that's.",
            "That's again, so that is actually the essence of the factor rotation problem.",
            "So we can basically rotate this data anyway we like, and the distribution will not change at all, so there's basically no way we could figure out what is the right rotation.",
            "But there is certainly a single right single rotation that will give you the original components.",
            "That's what we would like to find, but there's no way we can find it from this data, because any rotation will give the same distribution for the data.",
            "OK."
        ],
        [
            "So how do we then proceed so we know that we have to use non gaussianity together with independence, but how?",
            "So we had so simple intuitive ideas before, like locating these edges, which is really kind of trivial and doesn't certainly work in real data.",
            "And then the idea of using non non non linear correlations.",
            "But actually most it kind of the perhaps the best approach that actually will lead to simple algorithms for solving this problem is not based on those ideas, it is actually based on the idea of maximizing.",
            "Of maximizing the non gaussianity.",
            "So the idea is the following.",
            "Well, the central limit theorem basically says tells you that if you have many independent random variables then and you take some kind of an average of them.",
            "So maybe a weighted average, then the average will have a distribution that is closer to Gaussian than you original variance.",
            "So well, the central limit theorem actually says this in the limit of an infinite number of random variables with weights that are constraint in certain ways, but kind of an intuitive, intuitive interpretation of the central limit theorem would be that maybe if we just have a finite number limited number of of independent components, then the sum will be closer to ghost.",
            "So now let us consider a linear combination of the opposite variables.",
            "So the linear combination being denoted by weights WI.",
            "Now, because the because XI is a linear transformation of the original independent components, this linear combination here must also be a linear combination of the original independent components, although we don't know the way it's Q here because the weights Q depend not only on the W's here, but also on the mixing matrix A.",
            "But now, because of this kind of the intuitive reasoning based on the central limit theorem.",
            "This linear combination Qi of some of the qis I should actually be closer to Gaussian than any of the original independent variables.",
            "So what that means is that if we maximize the non gaussianity of this linear combination, it should be maximized at the points where this linear combination equals some one of the independent components.",
            "That is the basic logic.",
            "So even though we can't calculate these qis, that doesn't matter.",
            "We just look at all possible linear combinations of the opposite variables and the maximum of non gaussianity should then correspond to the original independent components.",
            "Because if you don't have if you have anything that's not equal, so then so the so in the independent components that should be more non Gaussian.",
            "Sorry that sorry that would be more Gaussian.",
            "Any mixture of the independent components would be more Gaussian.",
            "Well, what I'm saying here is kind of just kind of intuitive logic.",
            "It's not exactly true, but it can be shown that this method works by.",
            "It can be rigorously shown under certain assumptions and certain measures of non gaussianity.",
            "Of course, what we haven't defined here is the measure of non gaussianity, which is what we need for the practical for any practical algorithm.",
            "So actually this kind of an idea of finding a linear combination that maximizes non gaussianity was published already in the 70s under the heading of projection pursuit.",
            "So in projection pursuit the idea is was not to solve this kind of a blind source separation problem to recover any or any underlying variables.",
            "The idea was simply to find.",
            "Projections of the data that enable enable some interesting visualizations.",
            "So it's kind of a very interesting thing that, well, this the purpose of visualization in projection pursuit eventually ended up using exactly the same objective function that we can use here for ICA.",
            "So now actually here we see clearly very very clearly.",
            "I would say the difference between PCA and ICA.",
            "So in PCA, what we do we look at linear combinations of the of the origin of the object variables and we maximize the variance.",
            "So in ICA now we maximized the non gaussianity and these two of course completely different things.",
            "They have nothing to do with each other.",
            "You can have.",
            "I mean basically changing the variance of a variable does not affect its non gaussianity at all.",
            "And for any given variance you can have, you can have any variables of any any kind of non Gaussian.",
            "Do you ever lie?",
            "So we see in this sense that ICN PCA actually completely different things, although they are sometimes confused because the acronyms are so similar."
        ],
        [
            "So here's a very simple illustration of this idea of changing non gaussianity.",
            "So we have independent components of with the uniform distribution as earlier.",
            "Here's a 2 dimensional scatterplot.",
            "Now if we look at the solution given by PCA in the previous slide or in, well, this could also be just seen as kind of a some kind of a random mixing, mixing with some arbitrary to arbitrarily chosen coefficients of the independent components.",
            "Then what if you look at now the distribution of the obtained component components while they are something like this, so much closer to Gaussian than the original independent uniform independent components.",
            "So you can of course figure to do this kind of simulations very very easily yourself."
        ],
        [
            "So.",
            "So this thing basically opens up.",
            "One way of developing a large number of different.",
            "I see algorithms, So what you basically need is now a measure of non gaussianity.",
            "It can certainly be measured in many different ways and then some method for optimizing it.",
            "So regarding the measure, some non gaussianity I will not go here into details, but there are some rather classical methods of non girls, some rather classical measures or non gaussianity.",
            "The most logical one is kurtosis, which is basically a form of the fourth moment expectation of the 4th power of X.",
            "It is very nice algebraically, that's why it is a very classical measure of non gaussianity.",
            "You can prove things like you can basically prove using those.",
            "Is that this method of maximizing non gaussianity works, it will it will give you consistent estimates.",
            "That means estimators that converge in the limit of an infinite sample size.",
            "But it is actually.",
            "It is actually rather bad statistical properties in the sense.",
            "Well, one of them being that it is rather non robust against outliers.",
            "Because if you calculate 4th moments, well, you're basically taking the 4th power of your data points and so that can be.",
            "Can be very very strongly influenced by some outliers.",
            "Well, from statistical criteria, people tend to find that the optimal way of measuring non gaussianity for this context and actually for many other purposes also is to use differential entropy, which is basically a continuous valued version of Shannon's entropy.",
            "But the problem is that this differential entropy is very, very difficult to compute and to estimate.",
            "Well, it turns out that actually using differential entropy is essentially the same thing as much as formulating a kind of a likelihood of the ICA model.",
            "And it is also related to this well known neural network idea of maximizing information flow in a neural network.",
            "Anne.",
            "But then if you want using differential entropy, then well, basically the question is that how do you actually calculate that?",
            "Or how do you compute an estimate?",
            "How do you estimate that?",
            "How you how do you compute the estimators?",
            "In practice?",
            "What people would use this various kinds of rough approximations of entropy, which typically then go come relatively close to the idea inherent in keratosis, so they're kind of a compromise between the noise, noise, computational properties of kurtosis analyze.",
            "Statistical properties of different length.",
            "So once you have figured out your measuring non Gaussian then you need to optimize that.",
            "Well there are basically two methods which are widely used.",
            "One of them is a gradient method.",
            "Well basically.",
            "Is that is especially used with this info Max likelihood framework.",
            "But basically there you can make kind of a rather funny funny trick and use what is called the natural gradient, which is a very simple modification of the gradient that simplifies the computation and also seems to have some some good some better statistical properties.",
            "Or then you can use a fixed point algorithm which which I published some 10 years ago."
        ],
        [
            "OK.",
            "But actually I need to make here as a short remark on what kind of non gaussianity is.",
            "Do you actually have you, you might actually expect to having your date.",
            "That is also important because.",
            "Well, often these if you want to have a simple measure of non gaussianity you can you can you can use prior information on the kinds of non gaussianity's that you have in your data.",
            "If you know that the kind of non gaussianity that you have in your data is is for example this positive that I'm going to talk about then we can drastically simplify your measures from non gaussianity and your algorithms.",
            "So actually this is kind of basically an empirical finding that, well, the typical form of non gaussianity that we find in most kinds of data analyzed by ICA is what is called sparsity.",
            "So the idea is positive is basically that the probability densities have heavy tails and a peak at 0.",
            "Which is classically illustrated by this law policy and distribution, which is plotted here in the solid curve and the best best Gaussian approximation of that is plotted as the dotted curve here.",
            "So as you can see, now the Laplacian distribution has a very sharp peak here, and what you can't really see very well in this plot is that it also has heavier tails than the Gaussian distribution.",
            "So much more data very far away from zero.",
            "Perhaps another illustration of this is that well, here we have basically just drawn samples.",
            "Observations from a Gaussian distribution and hear from his past distribution from exact of exactly the same mean and exactly the same variance, and So what you can see is that the sparse distribution is basically much of that.",
            "I'm very close to 0, almost indistinguishable from zero, but then every now and then it obtains very large values, which is of course just another way of looking at this idea of peak at zero and heavy tails.",
            "So many I see algorithms.",
            "Actually kind of assume that the non gaussianity in your data is it takes the form of sparsity and then then that's how you get much simpler and faster algorithms.",
            "Actually both the most of the classical.",
            "Well, actually."
        ],
        [
            "Actually, both the most widely used natural gradient methods and fixed point algorithms."
        ],
        [
            "Make a bit an assumption which is a bit in this direction.",
            "It is a bit more general, but it is essentially something it is essentially looking at.",
            "In practice it is really looking at something like this as possible.",
            "But of course part it is not the whole story.",
            "Well, another important form of non gaussianity that you find in your data is typically skewness or asymmetry.",
            "So simply the data is not symmetric around 0, but it is.",
            "It is some kind of an asymmetric shape.",
            "So that's perhaps the.",
            "Well, let's say that a couple of years ago I used to think that, well, you just that's positive is really what you find in almost any kind of data.",
            "But recently I have encountered also data where skewness may be important, where you would typically have both skewness and sparsity, so it may still be possible to just use sparsity, but it may your algorithms may improve if you if you also use the idea of skewness.",
            "OK."
        ],
        [
            "So in this talk I have been pretty much like bashing PCA and factor analysis saying that they are no good but OK, but that's not actually my purpose.",
            "My purpose is to say that, well, PCM factor analysis do.",
            "Very.",
            "They are very well in doing what they are supposed to do.",
            "But you have to understand that they only do what they're supposed to do, and they don't do I see a.",
            "On the other hand, I see a does not do what PC and factor analysis are supposed to do.",
            "So what PC and factor analysis are supposed to do is to find a subspace, so reduce the dimension of the data, find the subspace which explains the maximum amount of variance in your day.",
            "And I see a supposed to find then the underlying variables in your data.",
            "So of course then the obvious way of combining these two is that you first do PCA or factor analysis and then do ICA in that's in it stop subspace.",
            "That should combine the both kind of the strong points of both methods.",
            "And so in this case then again, if you happen to know the classical terminology of factor analysis in this case I see is really strictly a form on factor rotation.",
            "Based on non gaussianity, so it's very different from classical methods for factor rotation, like very Max quality, Max and so on, because those classical methods do not use the statistical structure of your data.",
            "In particular this non gaussianity.",
            "They are basically concerned about about finding about the structure of your mixing matrix or the matrix of your factor loadings.",
            "And so the good point about, I mean, the classical justification for this kind of PCA factor analysis.",
            "Kind of doing PCA factor analysis in kind of signal processing context would be that they reduced noise in your signals.",
            "The idea being that, well, the main signal is contained in the PCA subspace.",
            "And also then they of course reduce the computation, because then you need to only do ICA in a smaller subspace."
        ],
        [
            "OK, so this was actually the end of the first half of my talk.",
            "That is the introduction to ICA.",
            "OK.",
            "So I will then go to the start, the second half, which is about applying ICA on EG and MG, especially in the case of spontaneous activity.",
            "So perhaps a short introduction on this idea of resting state analysis, which is very fashionable in brain imaging circles at the moment, so the idea being simply that, well, you measure a subject's brain while the subject has no task and no stimulation.",
            "And presumably then, this object is at rest, although of course it's not.",
            "There are some problems here in the definition, because it might be that that that that's actually the subject is thinking about something with great intensity, even though there is no experimental task.",
            "But let's hope we don't have that kind of problems.",
            "So then we can of course measure this kind of activity by any well known imaging method.",
            "So why is this data so interesting?",
            "Well, one reason why it's very interesting is that it's somehow kind of objective in the sense that it's not.",
            "It's not dependent on the subjective choices in the experimental design, or maybe objective is not the right word.",
            "Let's say perhaps it's kind of universal, so it can somehow reflects the function of your brain in kind of a very general way, and not just in the case of a particular task or stimulation.",
            "Also, it's interesting because not much analysis for this kind of data has been done so far.",
            "Well, in the fMRI literature, this is very fashionable and more and more is being done, but especially in the case of edion image, there is very little analysis on this kind of data.",
            "An from a more philosophical viewpoint, it kind of gives you kind of an IUD viewpoint on or kind of it, as it is kind of a different philosophical assumptions on what the brain is doing.",
            "So in a more classical neuroscientific context, the brain is more like a stimulus response machine where you give some stimuli and you get responses.",
            "Or maybe you have a task that slightly different, but in this case what we're talking about is some kind of rich internal dynamics which are there all the time, even if there is no.",
            "Simulation are no task given from the outside.",
            "So."
        ],
        [
            "So of course, the first thing that you might ask that is anything actually happening in the brain during rest.",
            "Well, of course you might think that there is not much happening, but actually it turns out that some brain areas are more active at rest than during most most tasks, so this is what Reichel and others called.",
            "Basically the default mode network based on their measurements on in pets, an later fMRI.",
            "So this is kind of an intrinsic brain activity.",
            "Instead of being responses to any some kind of stimulation, so this is a figure of one of their papers and shows kind of the very classical form of this kind of a default mode network, which is really basically.",
            "It is both at the at the medial frontal cortex, so this is a medial view of the brain.",
            "Medial frontal cortex and also here at the place called.",
            "Typically the posterior singlet cortex, or perhaps the precuneus.",
            "And the point is that even though the kind of surprising thing is that even though these two areas are quite far from each other, their activities tend to be very correlated at rest.",
            "So here those the activities are shown for the for this part in Redan for this part in.",
            "So for this sorry for this area in yellow and for this area in red and they tend to be actually.",
            "Extremely correlated in this.",
            "In this fMRI result.",
            "But OK, so how do we actually analyze?",
            "How would we analyze this kind of resting state activity in more detail?",
            "Well, one thing that."
        ],
        [
            "We have done in ICA sorry in fMRI teacher is to basically just take this data and apply ICA on it.",
            "One well known result was why Beckman and others where they basically had rather limited number of independent components which they considered to be somehow consistent over many over different subjects.",
            "So what they had was a was basically components which corresponds to visual areas.",
            "Typically you get a couple of them, then an auditory system and sensory motor system, and then of course this default mode network.",
            "Here, pretty much like the posterior signet politics and the media frontal cortex, and then a couple of other other things which are perhaps like executive control and dorsal visual stream.",
            "Well, the actual number.",
            "Of course this is only one result using ICA, so the number of components and basically the resolution of the components depends on how much, how many, how many dimensions you basically leave after PCA.",
            "So it might be for example that in some results the default mode network is split into.",
            "Different components the visual area might be split into many more components than two, and so on.",
            "But so the interesting thing you said you're basically even incomplete.",
            "Resting state data.",
            "You get this kind of this kind of a division into areas which corresponds to well known well known brain systems.",
            "An is an even sensory systems, although there is actually no sensory stimulation coming in.",
            "Another another interesting point is that."
        ],
        [
            "Actually, you get very similar results if you if the subjects are stimulated.",
            "If, for example, if their subjects are watching a movie and you apply ICA, you get actually a very similar decomposition.",
            "So this seems to be something which is not only about the resting state, but some kind of a something about kind of very general properties of some kind of spontaneous activity in the brain.",
            "Well, of course, using the red spontaneous when people are watching a movie maybe a little bit controversial, but I'm just using here the word spontaneous as opposed to some kind of a two responses to some kind of repetetive experimental protocol where you show the same thing many times face locked.",
            "OK, so."
        ],
        [
            "Because of this.",
            "Very rapidly expanding fMRI leeches, so there's a great interest these days to use the new similar kind of analysis using e.g an image 'cause he is a kind of a classical introductory slide.",
            "So even me I will not go into details.",
            "We I think we all know that.",
            "Well Legion images simply basically have a very high temporal accuracy where slider slightly with temporal accuracy, because this kind of activity in spontaneous resting state activities basic characterized by various kinds of oscillations in different parts of the brain with different frequencies.",
            "The most prominent ones being around 10 Hertz."
        ],
        [
            "OK.",
            "So now I will then go.",
            "To this.",
            "I will explain a couple of ways of using ICA on e.g an image data.",
            "This kind of spontaneous image engine image data.",
            "Basically some of our most recent advances in this in this.",
            "On this topic.",
            "So the first question is that, well, actually.",
            "We saw earlier that ICA basically finds components by maximizing non gaussianity, which typically called means maximizing sparsity, sparsity being the dominant form of non gaussianity.",
            "But we have to ask that the sparsity of what is the sparsity.",
            "I mean what?",
            "What are we actually maximizing the sparsity?",
            "Well, that depends basically on the pre processing and representation.",
            "Now each specially this is a good question in EG and Amy G where we have also the oscillatory structure.",
            "Which basically means that we can talk also about spectral.",
            "The spectral properties of the data.",
            "So basically we can talk about."
        ],
        [
            "Three different kinds of sparsity's.",
            "First of all, we have the kind of very classical one.",
            "I mean, which is basically implicitly in the introductory part, so I see I was implicitly always talking about this kind of this kind of sparsity on gaussianity, which means that we basically look at the temporal.",
            "We basically look at these time series of the independent components and look at the histograms.",
            "So for example, if you have this kind of a temporally modulated signal which is has has a certain time segment where it is simply.",
            "Only much more strongly active than elsewhere.",
            "Then you would actually get a very strongly sparse and non Gaussian signal and you can separate these by ICA."
        ],
        [
            "But another thing is that we could actually also look at the sparsity in space on the cortical space, basically so this is what this is actually how people usually apply ICA on fMRI.",
            "What they basically?",
            "Kind of put the data into the ICAO ICAO algorithm in such a way that basically it is the cortical points.",
            "Which take the place of observations.",
            "So in some sense they are taking the transpose of the matrix of the data matrix before putting it in, putting into an icy algorithm.",
            "So then basically the independent components will be some kind of some kind of cortical distributions, and you're maximizing the sparsity of those.",
            "I will actually have a slide on that a bit late, but then."
        ],
        [
            "In the case of Ejen Emaji, we can also talk about sparsity in frequency.",
            "Now, it might be that we have, for example, if we have this kind of a signal, it is actually not sparse.",
            "Its histogram is actually not sparse, but if you go to the Fourier domain it will be sparse.",
            "Well, I will have a slide on that later, so actually the idea of sparsity has then many different ways of applying it for, in which we're going to apply it on easy Remedy data."
        ],
        [
            "So first let me talk about this idea of spectral sparsity.",
            "So the idea here is that, well, if we have this kind of rhythmic sources also like the resources in the brain.",
            "They may actually.",
            "The distribution may actually not be very sparse.",
            "Well or actually non Gaussian.",
            "So for example if we now take a kind of a.",
            "And also at the resource where which originally would have oscillations like this.",
            "And then we basically modulate its strength, like usually the strength of these oscillations is modulated as in the brain as a function of stimulation or task or kind of.",
            "It is just some for some other reason it is stimulated at rest.",
            "So now if we take for example this kind of an envelope and multiply this what we might call the carrier oscillation a carrier signal.",
            "By this kind of this kind of an envelope, we get this kind of signal.",
            "And actually, well, the distribution of this thing.",
            "Well it is non Gaussian certainly, but it's kind of very funny it has.",
            "It is actually.",
            "It is not very sparse and it is not skewed.",
            "It's non gaussian.",
            "There may be very difficult so difficult to measure and especially if you input this.",
            "If you have this kind of signals and you use a kind of a classical icy algorithm which is mainly looking at sparsity, you may run into big trouble.",
            "Now if you have something in another envelope which is modulating the signal, much more spa much more strongly, so their modulation is going near closer to 0 and you get the signal like this.",
            "Then the signal is perhaps a bit more, looks a bit more like sparse.",
            "But actually if you measure the non gaussianity of this kind of a signal by keratosis, so conventional measures, it is still not not very Gaussian, is not still not very non Gaussian, not very sparse."
        ],
        [
            "So a solution for this kind of a problem might be now actually to go to the Fourier domain.",
            "An look at the sparsity in this in the Fourier domain, so the sparsity of Spectra basically.",
            "So this is what we proposed in your image 2010.",
            "So the idea being that we basically take short time Fourier transforms of the data, we divide each channel so immediately channel into short time windows of maybe 1 second and then take a Fourier transform in each of those windows, and then we apply or I see and this kind of data.",
            "The idea being that if you have like and also that there is signal like this and you do the short time Fourier transform, you transform it into something like this.",
            "Where basically so like the wind in each window you have one peak like this, adds the frequency corresponding to the so the frequency of this oscillation and this signal here is sparse.",
            "So ICA might work much better when we do this kind of a short time Fourier transform and then we which means then essentially that we're basically measuring sparsity in the spectral domain.",
            "Well in the spectral domain.",
            "But actually because we're taking a short time Fourier transform, we will also be measuring all the sparsity in the time domain as well.",
            "Actually combining the two.",
            "So another."
        ],
        [
            "It would be to use then.",
            "Look at a spatial sparsity, so this is as I told you, this is how basically almost always people apply.",
            "I see an fMRI, so the idea being that now you look at the kind of images of the cortex which are here shown in this as this kind of caricatures of kind of a cortical sheet with some active areas.",
            "You consider each of these observed Coop saved brain activity Maps as one object variable, and then you input this into an icy algorithm, which then essentially means that you have you are finding some kind of that independent components are some kind of a origonal activity patterns, presumably very simple activity patterns, which then are combined together to give the activity patterns in the object data typical.",
            "This would be activity patterns in different time points.",
            "So now so this basically then reverses the roles of the observations of the Varian variables to the compared to the classical case.",
            "So now then, because we are maximizing the sparsity and independence of these independent components, it means that this origonal activity patterns should be somehow simple simple because of sparsity, they should be 0 most of the time and only occasionally only in some parts take non zero values.",
            "Now, can we actually do so?",
            "This is what how they do fMRI, can we actually?"
        ],
        [
            "Do it on me G. Well, actually we can do it on energy, but what we need to do is then the first project data from the sensor space to the source space.",
            "So on assault you some some inverse solver to get out to put our data into the into the cortical space.",
            "And we also then combine this idea with this idea of short time Fourier transforms.",
            "Because that seems to work better.",
            "So basically then we are in this in our paper in human brain mapping, what we're doing is that we're basically maximizing at the same time spatial sparsity and spectral sparsity.",
            "And well, actually it seems to find.",
            "It seems to find pretty much the same kind of sources in this kind of resting state data as the temporal as temporal of temporal spectral ICA methods.",
            "So in that sense, it doesn't seem in this paper the results where in that sense not particularly interesting, but but the point here is that it actually relax is one of the main assumptions of ICA.",
            "So basically now there is no assumption of temporal independence of the signals anymore because all the other statistical properties are now concerning concerning the spatial and spectral distributions.",
            "So now now going back, referring back to some, some critics of ICA like I think maybe one lecture yesterday.",
            "It might be claimed that perhaps in the brain the sources are actually not.",
            "So this kind of brain sources.",
            "Components they are actually not independent because there's all kinds of complicated interactions between between the brain sources.",
            "Well, if you believe that argumentation, then you should actually use.",
            "You could use something like this because here we actually making no assumptions of temporal independence.",
            "Actually no assumptions at all on the temporal on the time courses of the components.",
            "I think this would be this might be really important, especially if you analyze something like evoked responses.",
            "If you have your responses, then the response the responses in different components which correspond to sources in different areas of the brain might actually be very strongly very strongly dependent, even linearly correlated."
        ],
        [
            "OK. Then I will go to another topic.",
            "So ICA, which is about testing independent components.",
            "So as somebody already asked me at the during the break.",
            "So now if you just what happens, what happens if you just apply icy and some data and then actually the?",
            "The assumptions of the model are completely violated.",
            "How do you know that your algorithm actually did something meaningful?",
            "Well, that is actually of course very important question and that has not been answered very well in the research so far.",
            "So what I have been developed during the last couple of years is a method for actually testing the independent components.",
            "Being able to say something about whether the components are likely to correspond to something real or whether you should think that they are simply some kind of noise.",
            "So this is of course kind of.",
            "I think this situation has been rather.",
            "Rather funny from the viewpoint of classical statistical modeling, usually always in science.",
            "When you do a scientific analysis and you estimate something, you should also also do some kind of testing to see whether the test, whether the estimation results are just somehow trivial.",
            "Maybe they, whether they could be just due to noise.",
            "Maybe maybe your results could be just zero or M. That means it's completely if they would be, there would be the same in two different conditions, and so on.",
            "So you should always be assumed to do some kind of statistical testing.",
            "Before you can publish stuff.",
            "But then I see a that has not been the case.",
            "You just run the icy algorithm, get nice pictures, and published.",
            "It's of course nice for from the viewpoint of publishing a lot of papers.",
            "But of course there is a risk that many of the independent components are perhaps completely rubbish so.",
            "It would be nice if we could actually kind of basically do some testing and basically assign P values to components saying that this component is clearly it's likely to be something meaningful, but that component is does not seem to be anything.",
            "So.",
            "An in ICA, like in many machine learning algorithms.",
            "This problem is even more serious because that's because there are.",
            "Because there's always also the problem of getting stuck in local minima.",
            "So in classical statistical literature you always hang up, have the problem that you might have a finite that your data might be your data sample might be too small, or in general simply, or there might be too much randomness in your results.",
            "But in ICA and related methods you also have the problem that your algorithm might actually get stuck in local minima.",
            "Because the objective function is related to non gaussianity, highly complex and there is absolutely no guarantee for any algorithm that you will find some kind of a global maximum instead of getting stuck in some local minima.",
            "Sorry, I mixed up the word maximum minimum.",
            "Let's just say you can get stuck in local Optima."
        ],
        [
            "So.",
            "The idea here would be is based on the idea of intersubject consistency.",
            "Which I think is very relevant in this kind of a neuroscientific context, so the idea is simply that we do ICA separately on many subjects, and then we say that the component is significant if it appears in more than two of the subjects insufficiently similar for.",
            "So now of course the whole question then is how to quantify this idea of sufficiently similar.",
            "So what I did in our recent paper is to basically formulate a rigorous null hypothesis using classical statistical theory, formulate a null distribution and then B.",
            "Then we can actually quantify this idea or when when the components are sufficiently similar.",
            "So I think this, especially in the neuroscientific context you, I mean your image.",
            "In context, you want to have several subjects, but you can also use the same idea by just taking different parts.",
            "Different datasets from the same subjects.",
            "Ultimately, perhaps if you have just a single data set, you just divide it into two and say that, well, these are.",
            "These are basically two different datasets and you do I see separately on them.",
            "So, so this is for this is for EG and images really well let's say so this is this is for for the kind of temporal ICA which is usually applied on EG or images.",
            "Made it could also.",
            "It also well recently.",
            "Also people have started applying temporal.",
            "I see an fMRI so it could also be used in that context so we have a paper which is almost submitted.",
            "So many months ago, I pub promise that it will be submitted in two months, so I will not promise promise anything anymore, but I think it will be submitted in a couple of weeks where we also show how to do this on in the classical fMRI spatializer case.",
            "8 miss consistency of the mixing coefficients.",
            "So basically the anatomical location.",
            "Yes, I mean that is the basic distinction why we have to have different different methods.",
            "I mean the general principles are the same, but the all the details in the methods are different whether you do temporal, icy or needy fMRI or specialize in or.",
            "Sorry whether you do space temporal.",
            "I see an EGM EG.",
            "Or whether you do specialize in fMRI, that's yeah, you have to look at.",
            "In one case you have to look at the mixing coefficients.",
            "In the other case you have to look at the values of the independent components.",
            "But in both cases, so because it's resting state, the thing that should be consistent is is the spatial patterns not not a temporal time, not time courses."
        ],
        [
            "So here's an example of what we get from a mediator.",
            "So in this case we had 11 subjects.",
            "So what I'm showing here is 2 components to basic two components, which were basically considered significantly consistent by this method.",
            "And here I'm showing basically the the topic distributions in those subjects for which the component was was found in sufficiently similar form.",
            "So this is a classical merism, and as you can see it's found in pretty much the same form in most of the subjects.",
            "Actually, not all of them.",
            "999 of the 11 subjects.",
            "And then you can analyze that in more detail by looking at the Fourier spectrum, politics of which is, so you see that it's actually a classical music from an minimum estimate an.",
            "Actually, this was.",
            "Again, this was not actually a resting state data, this was something where we had different kinds of multimodal, kind of a multimodal data set with different kinds of modalities of stimulation.",
            "So we could also see that this one is particularly well.",
            "It's actually almost exclusively modulated by tactile stimulation.",
            "Here's another independent component.",
            "Which was again found in nine of the components, and it seems to be relatively classical occipitale Alpha rhythm with something like typically the frequency being around around 10 a little bit more than 10 and basically modulated by by visual stimulation.",
            "Well, sorry I don't have time to go into the details of the stimulation protocol.",
            "So yeah, so this.",
            "So basically, of course, without without, my claim would be that every time you apply ICA you should do some kind of testing like this.",
            "Of course I am not ecocentric enough to claim that you should always use my testing method, but I think you should do some kind of testing anyway."
        ],
        [
            "OK and then.",
            "The last topic of this talk.",
            "Is about causal analysis or effective connectivity?",
            "So.",
            "The point is now is to model connections between the measured variables, and especially actually.",
            "So this is about effective connectivity, so it's about directed connections between measured variables, not just saying that two variables are connected, but saying which direction the connection is.",
            "Actually, the effect is actually going.",
            "So there are basically two fundamental approaches for this kind of a analysis.",
            "Now, if the time resolution of the measurements is basically fast enough, then we can do something like autoregressive modeling.",
            "Now, the idea being that so if your measurements if the causal connections actually are very fast compared to the time resolution of your measurements, then you cannot use something like auto regressive modeling because auto regressive modeling strongly relies on the time information on the time precedents of your effect of your effects and also as has been recently pointed out by many people.",
            "In fMRI data, we also have the problem that.",
            "Not only that, time resolution is not fast enough, but also that we have these unknown hemodynamic lags.",
            "So that's another reason why, Granger, why this kind of a Granger causality might actually be rather difficult for that kind of date.",
            "So if that it basically it's I mean, the point is that auto regressive modeling using Granger causality is something rather simple.",
            "So we would like to use that if basically the conditions for using it are met.",
            "But if they are not, then what we knew need is something else.",
            "And what is typically useful in this case is the structural equation models.",
            "Now, if the measured variables are basically the rowie gmed channels, it doesn't actually make a lot of sense to look at the connectivities between the original channels, of course, because the channels are basically measuring nearby channels are measuring pretty much the same stuff, so you could somehow first separate or localize some sources or components and do the analysis on them.",
            "And another point is that.",
            "Now if we fish do ICA on this E Gen emaji well, the sources are actually that you get.",
            "I usually exactly rather exactly uncorrelated.",
            "So what we may not be meaningful to look at any kind of Granger causality methods, autoregressive modeling because the because because there may not be enough information in the data left.",
            "I mean in the sense that if the components often are just into independent that you would actually be able to estimate meaningful some kind of Granger causality.",
            "Auto regressive modeling.",
            "So one way to get around that is to look at then the dependent look at, then the envelopes of those components envelopes.",
            "Meaning basically kind of the local variances or the amplitudes of the signals.",
            "Is it?",
            "Yes.",
            "Yes, that is actually that is true.",
            "It is actually very confusing, yes?",
            "What should I say yes?",
            "Well, of course, if you believe that ICA will actually give you good sources, good localization of the sources, then you have to use it.",
            "It will give you individual components which are very independent, but you can't help it.",
            "So then what you need to do is to basically introduce some kind of a dependency in your data by look basic looking at what kind of statistics between the sources are as independent as dependent as possible.",
            "And just empirically, you would often find that it is the envelopes which are still strongly dependent between the independent components, while the wild any kind of linear correlations are difficult to find.",
            "I mean, well OK, looking from neuroscientific viewpoint, it's actually well people talk a lot about phase phase synchrony in a media e.g data.",
            "But, at least in my experience, an other peoples also experience.",
            "Also, it's very difficult to find any kind of phase synchrony in this kind of resting state or spontaneous spontaneous data.",
            "So you may perhaps find it in some, you know when you have some very specific tasks and perhaps like revoke responses and so on.",
            "But what you do find very clearly is strong dependency.",
            "Strong correlations of the envelopes, even resting state data.",
            "So yeah, it is a bit confusing.",
            "It may seem a bit contradictory.",
            "But but maybe that's just how you have to do it.",
            "You have to assume independence in order to get meaningful sources, but then you have to basically.",
            "Try to find the what kind of dependencies still remain in the data, and it turns out to be basically something related to envelopes."
        ],
        [
            "OK, so.",
            "OK, I'm running out of time so I will go rather quickly now through our our frame framework for structural equation modeling.",
            "So the idea in structural equation modeling is that is to model how an externally imposed change in one of the variables affects the others.",
            "So basically you build a model like this where each variable is expressed as a function of all the other variables.",
            "Plus a stochastic stochastic disturbance term.",
            "That case basically then models something like the external inputs to the system.",
            "Now.",
            "This method in this model here is in many ways analogous to in factor analysis model.",
            "In some senses it is.",
            "It's estimation is actually partly under some assumptions, it's estimation is equivalent to estimation of factor analysis model, so it has the same problems like factor analysis, which is basically that if your data is Gaussian, it cannot really be uniquely estimated.",
            "You can basically find.",
            "So you can basically find many different kinds of many sets of para meters BIJ here.",
            "Which give exactly the same distribution for the day.",
            "But we can basically borrow the theory of ice."
        ],
        [
            "EA.",
            "We can make assumptions similar to ICA.",
            "And show that then actually this model can be estimated.",
            "Basically we treat these stochastic disturbances very much like independent components in the theory of ICA.",
            "We assume that the stochastic disturbances are mutually independent and they are non Gaussian, for example sparse.",
            "And then we also must make one specific assumption, which is very specific to this kind of a structural equation modeling, which is that the BIJ form an acyclic graph, which means that there is an ordering of this of this variables where all the effects actually go forward.",
            "So under these assumptions, then we have shown that actually the model can be estimated.",
            "Actually it is.",
            "It can be transformed into an ICA model and estimation of the ICM model will enable estimation of this model here.",
            "So then we would get something like this graph.",
            "Here we would basically so the coefficients by J here basically then give the different strengths of the connections between the different variables.",
            "So the goal of this analysis is very different from ICA.",
            "We're not finding any kind of hidden variables or sources.",
            "We're finding connections between the variables, the object variables."
        ],
        [
            "Where, of course, the opposite variables might actually be results of an earlier ICA.",
            "The envelopes of an earlier ICA in particular.",
            "OK so I will just describe one kind of a basic approach for estimating this model, which actually at the same time attacks kind of a very deep problem in ICA in statistical theory.",
            "Longstanding problem I think.",
            "So suppose that we have we have two variables, just two variables X&Y.",
            "And we basically want to know whether a regression model where y = a coefficient times X is is is good or whether we should basically model the regression in the other direction.",
            "XXS is where X is a linear function of Y.",
            "Now again we get so that kind of gaussianity problem.",
            "It is very very well known that if the data is Gaussian then there is no way we can distinguish between these two models.",
            "They give exactly the same.",
            "They give equally good fits equally equal likelihoods.",
            "And everything in both cases.",
            "But again, if we assume that things are non Gaussian and independent, basically assume that we assume that the residuals are not reservoirs are non Gaussian and independent of the opposite variables which can also be non Gaussian.",
            "Then we can this is this is a special case of the structural equation modeling before and it can is actually closely related to an ICA.",
            "And what we can do is to approach is to look at the log likelihood ratio of the two models and make a first order approximation of that.",
            "And we get something very simple.",
            "We get basically a kind of.",
            "A difference of two nonlinear correlations between the two variables.",
            "So now we actually can use the nonlinear correlations that I talked about earlier, but in a funny way where we basically look at the difference of these two nonlinear correlations where we switch the non linearity from here.",
            "So here.",
            "And the non linearity here is very similar to those used in ICA.",
            "Well, actually I didn't treat the nonlinearities used in ICA, but these are closely related to the sparsity measures that we would use in IC estimation.",
            "So then if the under all these theoretical assumptions, then basically the sign of this kind of a statistique will tell you which of the two directions is the right one, which one generated the date.",
            "OK, and you can then use this framework for estimating a bigger model where you have many many variables by just looking at all the different pairs and so on."
        ],
        [
            "OK, and here's one result we have not using exactly these methods I explained, but something related.",
            "But this is kind of very preliminary results, but this is kind of what we would like to have.",
            "We basically like to have first to ICA an well here we represent each of the independent components by this kind of topic graphic plot, and then we want to have both those in a graph where we basically then see connections between all the different components.",
            "The strengths and signs of the connections being shown by the by the width and the colors of those arrows.",
            "So for example, here we might see that that, for example, this A.",
            "Here's a group of sources which are more like in the romantic areas.",
            "Presumably somatosensory motor motor components, which have positive connections between each other.",
            "And here we have more like visual or perhaps attentional components, and these two are kind of separated in the sense that most of the connections in between are negative means, which is shown as.",
            "As red or actually looks like purple here."
        ],
        [
            "OK, but so.",
            "So to conclude.",
            "Basically.",
            "Well, this is actually a conclusion of the latter part of the talk.",
            "So basically what we can do is do expert Tori data analysis by ICA.",
            "So it can basically give us information about the internal dynamics during rests or some possibly some related protocols, like when you are watching watching a movie without making really any assumptions on on on what kind of activities you might find an.",
            "Of course, without knowing anything about the stimulation protocol.",
            "And actually, but even you can actually.",
            "Even used this so you can use this when even in a in a kind of a in a case where you have stimulation when simply the stimulation is too complex, like watching a movie like I just said.",
            "But even in a more classical paradigm where you have rather simple stimulation, you might be interested in trying out this kind of methods because they can then basically look at activity which is not directly related to stimulation, or which is perhaps related to stimulation, but it, but in a complicated nonlinear way which is not detected by by ordinary analysis methods.",
            "So basically we had here 2."
        ],
        [
            "Ages of analysis.",
            "So first we find sources or actually I should say components.",
            "This kind of components, which presumably also called and corresponds to some active sources in the brain.",
            "Perhaps I should say like that that can be found used for that we have.",
            "We had different variants of ICA, like the space all forms of ICA.",
            "For me G time frequency decompositions, which leads to the spectral sparsity criteria and so on.",
            "And then after that, after finding the sources, we can then basically analyze their effective connectivity by looking at these non Gaussian versions or versions of structural equation model."
        ],
        [
            "But then at some point we should also analyze really something about the significant statistical significance of reliability of the components, for example by using this idea of intersubject consistency.",
            "OK, that is all.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm going to talk about is basically two things that kind of general theory, machine learning, theory of separating sources and then analyzing connectivity, particularly concentrate.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "EG and MG.",
                    "label": 0
                },
                {
                    "sent": "So basically the first part.",
                    "label": 0
                },
                {
                    "sent": "Well, the first half of my talk is about kind of a general introduction to ICA.",
                    "label": 1
                },
                {
                    "sent": "Basically assuming that you don't know anything about the theory of ICA.",
                    "label": 0
                },
                {
                    "sent": "So I will start with the introduction of the basic problem, light source separation.",
                    "label": 0
                },
                {
                    "sent": "Then I will consider the importance of non gaussianity in this kind of models, an especially when I want to emphasize is the difference to principal component analysis.",
                    "label": 1
                },
                {
                    "sent": "And then the second half of the talk will be about applying this kind of ideas to analysis of EG and MG, and in particular I will talk about spontaneous activity an which and the kind of kind of archetypal case of spontaneous activity is the resting state activity.",
                    "label": 1
                },
                {
                    "sent": "So I will first talk about some improvements to the basic theory of ICA for the purposes of analyzing this kind of resting state or spontaneous EG.",
                    "label": 0
                },
                {
                    "sent": "OMG by applying ice, it's time, frequency decompositions and also by using a spatial version of ICA.",
                    "label": 0
                },
                {
                    "sent": "Then I will go to another.",
                    "label": 0
                },
                {
                    "sent": "Rather different topic, well kind of different but linked topic which is about testing this independent components.",
                    "label": 1
                },
                {
                    "sent": "So basically when we do do ICA, how do we know that the components actually correspond to something real?",
                    "label": 0
                },
                {
                    "sent": "I mean usually always when you do some kind of estimation you should always also do some testing, but in I see it is kind of being pretty much neglected so far.",
                    "label": 1
                },
                {
                    "sent": "So I have recently been developing a framework for that using the idea of intersubject consistency and then in the end I will go to this vast topic of effective connectivity or causal analysis.",
                    "label": 0
                },
                {
                    "sent": "By basically introducing a framework for that kind of analysis which is closely related to ICA, it is it uses non gaussianity in structural equation modeling.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I will start so from the very beginning of ICA, and this is also kind of a historical background, so I see a.",
                    "label": 0
                },
                {
                    "sent": "So the point is that let's assume that we have a system where we have a number of source signals.",
                    "label": 1
                },
                {
                    "sent": "Well, in the context of brain imaging, especially GM, it's kind of easy to imagine this as being four different sources in the brain, but what often happens is that we can only observe linear mixtures of this kind of source signals.",
                    "label": 0
                },
                {
                    "sent": "That is obviously true for the when you have EG or image recordings.",
                    "label": 0
                },
                {
                    "sent": "Where you are, of course you measure some kind of complicated linear mixture, so many sources, but so in this case we for illustration purposes we assume we have just a small number of source signals for an.",
                    "label": 0
                },
                {
                    "sent": "What we observe is this.",
                    "label": 1
                },
                {
                    "sent": "For mixtures of the signals.",
                    "label": 0
                },
                {
                    "sent": "So how can we actually recover the original signals from these mixtures without actually knowing anything about the mixing system?",
                    "label": 0
                },
                {
                    "sent": "So that's why it is called blind.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some people used to think.",
                    "label": 0
                },
                {
                    "sent": "I hope nobody thinks that anymore is that you could do something like this with principal component analysis, but it just doesn't work.",
                    "label": 0
                },
                {
                    "sent": "There is actually no theoretical reason why principal component analysis would actually find the original signals, and this is actually what you get with.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sea.",
                    "label": 0
                },
                {
                    "sent": "But if we now use information on the statistical independence of these components.",
                    "label": 1
                },
                {
                    "sent": "Then we can actually recover the original signals.",
                    "label": 0
                },
                {
                    "sent": "So statistical independence basically means simply that we think consider the amplitudes of these signals.",
                    "label": 0
                },
                {
                    "sent": "So the values in each time point as a random variable, and then we say that these random variables before random variable should be rather independent.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Which may be a useful approximation, although of course we know.",
                    "label": 0
                },
                {
                    "sent": "Actually I realized that I'm this schedule of the summer School is a bit.",
                    "label": 0
                },
                {
                    "sent": "Bit reversed in the sense that yesterday Ricardo Vigario was basically talking about various kinds of problems with this kind of framework, while only now I'm actually introducing the basic framework.",
                    "label": 0
                },
                {
                    "sent": "But let me just say that, well, I think that in many cases this kind of basic assumption of independence is quite quite useful.",
                    "label": 0
                },
                {
                    "sent": "An approximately full field.",
                    "label": 0
                },
                {
                    "sent": "But OK so.",
                    "label": 0
                },
                {
                    "sent": "That's of course in an application dependent question.",
                    "label": 0
                },
                {
                    "sent": "In the general theoretical framework, we just assume that's the sources are independent.",
                    "label": 0
                },
                {
                    "sent": "So this then led to the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the model of independent component analysis in the late 80s.",
                    "label": 1
                },
                {
                    "sent": "So where the idea is that?",
                    "label": 1
                },
                {
                    "sent": "So we basically say that the opposite random variables denoted by XI, some linear combinations of some hidden variables, SJ called the independent components and they basically the observations are then a linear mixing of these original independent components by some mixing made by some mixing coefficients which we don't know.",
                    "label": 1
                },
                {
                    "sent": "So it's actually rather similar models factor analysis if you happen to know the classical theory of multivarious statistics.",
                    "label": 0
                },
                {
                    "sent": "Yes, and so then the problem really is to estimate both the Ace AI JS and the SJS by observation of XI only.",
                    "label": 0
                },
                {
                    "sent": "So it is a form of unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "In the terminology of machine learning, if we for example knew both X&S, then it would be a case a classical case of supervised learning, and if we knew both X and the AI JS, that would be kind of a classical inverse problem.",
                    "label": 0
                },
                {
                    "sent": "A linear inverse problem.",
                    "label": 0
                },
                {
                    "sent": "But here in this case we basically don't know neither 8th nor the SS Anne.",
                    "label": 0
                },
                {
                    "sent": "It's unsupervised learning then.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order for us to be able to estimate this model, it's actually enough to make just a couple of assumptions.",
                    "label": 0
                },
                {
                    "sent": "First of all, we assume that these.",
                    "label": 0
                },
                {
                    "sent": "It leaves hidden variables SJSIR mutual statistically independent and 2nd, which is the most important thing is that we assume that they have non Gaussian or non normal distributions and this is really the departure from the classical framework of factor analysis or principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "The idea of non Gaussian source signals or independent components.",
                    "label": 0
                },
                {
                    "sent": "It is usually always.",
                    "label": 0
                },
                {
                    "sent": "It usually also assumed in the theory that's the number of independent components is equal to the number of observed variables.",
                    "label": 1
                },
                {
                    "sent": "But that is not strictly necessary.",
                    "label": 0
                },
                {
                    "sent": "It's basically to simplify the theory.",
                    "label": 0
                },
                {
                    "sent": "It simplifies the theory because the mixing matrix and has a unique inverse.",
                    "label": 0
                },
                {
                    "sent": "The system can be can be uniquely inverted, but it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not strictly necessary actually.",
                    "label": 1
                },
                {
                    "sent": "OK, so then, well, basically it's usually the proof that in this case the mixing matrix and components can be identified is usually attributed to common, although of course there were some earlier results in the same in the same field.",
                    "label": 0
                },
                {
                    "sent": "So what it means is that we can, so we can basically recover both the a sun dresses up to certain throughout the trivial indeterminacies like scaling and signs of the components.",
                    "label": 0
                },
                {
                    "sent": "So this is actually, well, this is a very surprising results in the framework of classical factor analysis where people have been working for had been working for almost 100 years on this kind of a problem, and always they found that, well, the problem is basically just undetermined.",
                    "label": 0
                },
                {
                    "sent": "There is what they call a factor rotation that always remains undetermined.",
                    "label": 0
                },
                {
                    "sent": "But well, the idea non gaussianity really helps us quite a lot.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it is important to understand then the connection.",
                    "label": 0
                },
                {
                    "sent": "So classical ideas like classical methods like principal component analysis and factor analysis.",
                    "label": 0
                },
                {
                    "sent": "So I will go through them.",
                    "label": 0
                },
                {
                    "sent": "Well, very very briefly.",
                    "label": 0
                },
                {
                    "sent": "So in principal component analysis, well, the basic idea is that we look at linear combinations of the opposite variables that have maximum variance.",
                    "label": 1
                },
                {
                    "sent": "In order for this I have this problem to be well defined, we have to fix the norm of W2, something.",
                    "label": 1
                },
                {
                    "sent": "Otherwise W would simply become infinite.",
                    "label": 1
                },
                {
                    "sent": "And then we can find many principal components by iterating this so that we find always new components which are orthogonal to those components previously found 1 by 1.",
                    "label": 1
                },
                {
                    "sent": "An essentially classical factor analysis has, well, it's.",
                    "label": 0
                },
                {
                    "sent": "It's in certain ways equivalent to this, although the basic, although it is usually formulated in terms of a generative model abit like the ICA model, but in any case in both cases the really the basic idea is to.",
                    "label": 1
                },
                {
                    "sent": "The basic goal is really to explain Max the maximum amount of variance with a limited number of components, so this is actually if you look at kind of really at the classical theory this is you can see that this is what.",
                    "label": 0
                },
                {
                    "sent": "Factor analysis and PCI can actually do.",
                    "label": 0
                },
                {
                    "sent": "So some people then kind of think that in addition to this factor analysis or PCA would be actually able to do this kind of blind source separation, But actually that that is not the case.",
                    "label": 0
                },
                {
                    "sent": "It can easily be shown in simulations, and there is actually no theorie in factor analysis that would even claim that you can actually separate the original signals, but it is still a classical misunderstanding.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically what we can if well, this is for people who basically know that there's something about like the theory of cloud factor analysis.",
                    "label": 0
                },
                {
                    "sent": "So if you happen to know factor in the theory of factor analysis, you can simply see ICA as a non Gaussian version of factor analysis which is also simplified in the sense that you don't have any kind of noise or specific factors in your model.",
                    "label": 0
                },
                {
                    "sent": "Well, if you don't know this year of factor analysis, you can just perhaps ignore much of this slide.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "So in the theory of factor analysis, they talk about the problem of factor rotation, which means basically that the model is very strongly in determined.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of things that you cannot estimate in the model.",
                    "label": 0
                },
                {
                    "sent": "But well, basically in ICA we I see it can be seen as basically a method of factor rotation.",
                    "label": 0
                },
                {
                    "sent": "The identifiability result by common that I just cited actually shows that there is no factor rotation left undetermined like in ordinary factor analysis.",
                    "label": 1
                },
                {
                    "sent": "So that's why then in ICA the components really give those origianal so signals or underlying latent variables.",
                    "label": 0
                },
                {
                    "sent": "Of course, in the theoretical case, assuming that that's the model exactly holds.",
                    "label": 0
                },
                {
                    "sent": "So then important catch here is that well, I mean, the reason why why this is ICA?",
                    "label": 0
                },
                {
                    "sent": "Well, one of the main reasons why ICA has is a very recent method.",
                    "label": 0
                },
                {
                    "sent": "Very recent invention is really this idea of non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "It is certainly true that in many cases, in classical applications of statistics.",
                    "label": 0
                },
                {
                    "sent": "The variables also Gaussian, that you cannot actually do something like ICA like classically.",
                    "label": 0
                },
                {
                    "sent": "Perhaps for this audience, the most closest closest field would be Psycho metrics where you have psychological variables like intelligence for example, which is actually historically the very first factor that was estimated, and those may actually be Gaussian because they are as the classical argument goes, they are sums of of a large number of hidden hidden.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Independent variables which might be called factors, but that would be confusing in this case and by the central limit theorem when you have a sum of many independent variables that will be close to Gaussian.",
                    "label": 1
                },
                {
                    "sent": "So in the case of intelligence it is presumably a sum of many genetic factors.",
                    "label": 0
                },
                {
                    "sent": "Many environmental factors that perhaps come together more or less independently, so you might assume that some kind of general factor of intelligence is rather gauss.",
                    "label": 0
                },
                {
                    "sent": "But the point is that now if we go to more modern applications of statistics or machine learning like for an L analyse basically another way we analyze outputs of set of physical sensors like e.g image then this kind of logic simply doesn't hold at all and the signals can be actually very very very non Gaussian and they typically are.",
                    "label": 0
                },
                {
                    "sent": "So here's some examples of case.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is where we might have known goes easy to get non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "First of all, if you have a strong oscillatory signal like this, well then it is non Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So what I'm plotting here on the first top row is 3 signals and here I'm putting the history.",
                    "label": 0
                },
                {
                    "sent": "I'm plotting the histograms.",
                    "label": 0
                },
                {
                    "sent": "So always when we talk about non gaussianity in the case of basic ICA, we're really talking about the non gaussianity of simply a histogram of the signal.",
                    "label": 0
                },
                {
                    "sent": "So the values the distribution of the variable when you just think about.",
                    "label": 0
                },
                {
                    "sent": "The amplitudes as the as the value of the random variable.",
                    "label": 0
                },
                {
                    "sent": "So now if you plot the histogram of this kind of a signal, it will be strongly non Gaussian.",
                    "label": 0
                },
                {
                    "sent": "The closest Gaussian approximation that is the Gaussian distribution with the same mean and variance is actually.",
                    "label": 0
                },
                {
                    "sent": "Put it here as the red curve and clearly it's a rather bad approximation of this distribution.",
                    "label": 0
                },
                {
                    "sent": "Another case would be something where you basically have.",
                    "label": 0
                },
                {
                    "sent": "Periods of hardly any activity.",
                    "label": 0
                },
                {
                    "sent": "Activity in some kind of EG sense and then you have short periods of activity like here.",
                    "label": 0
                },
                {
                    "sent": "So you have basically just noise here and then you have something a bit like an evoked response, and this is also non Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It is actually as you can see here, it has basically a kind of a peak at zero.",
                    "label": 0
                },
                {
                    "sent": "This is what they call sparsity.",
                    "label": 0
                },
                {
                    "sent": "I will have a bit more on that a bit later and now here is another example where we have basically two periods of short activity and peak at zero is even higher.",
                    "label": 0
                },
                {
                    "sent": "So this kind of signals are kind of not very likely to be very Gaussian.",
                    "label": 0
                },
                {
                    "sent": "The noise maybe Gaussian, which kind of then confuses things and makes things a little bit more Gaussian, but the underlying signals typically are quite non Gaussian I would say.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, So what is wrong with things like PCA and factor analysis?",
                    "label": 0
                },
                {
                    "sent": "Well, basically there are many ways of looking at this.",
                    "label": 1
                },
                {
                    "sent": "One simple way of looking at it is that well in PCA and factor analysis.",
                    "label": 0
                },
                {
                    "sent": "The idea is that we basically find components which are uncorrelated and maximize the explained variance well.",
                    "label": 1
                },
                {
                    "sent": "Correlations or covariances.",
                    "label": 0
                },
                {
                    "sent": "An variances are basically all what they call 2nd order statistics.",
                    "label": 0
                },
                {
                    "sent": "They can all be derived from the matrix of covariances, so all the information used by these two by factor analysis or PCA.",
                    "label": 0
                },
                {
                    "sent": "It's basically contained in the covariance matrix of the data.",
                    "label": 0
                },
                {
                    "sent": "In fact, these methods fall into the larger category of what some people call the covariance analysis methods.",
                    "label": 0
                },
                {
                    "sent": "Now, however, it's rather easy to see why this kind of methods cannot actually determine the whole mixing matrix.",
                    "label": 0
                },
                {
                    "sent": "That is because this matrix of the covariance is symmetric and that's why it has approximately 1/2 of N squared well numbers.",
                    "label": 0
                },
                {
                    "sent": "But the number of elements in the mixing matrix or what I call here the factor loading matrix is basically N squared because it has no restriction of symmetric and it is an N * N matrix.",
                    "label": 1
                },
                {
                    "sent": "So basically then there is simply not enough information in this covariance matrix to solve all these N squared elements in the mixing matrix, because there's simply you would get something where you have a system where you have much more variables, twice as many variables as you have equations.",
                    "label": 0
                },
                {
                    "sent": "So we basically need more information than what is contained in the covariances.",
                    "label": 0
                },
                {
                    "sent": "And that's actually.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is the whole point in non gaussianity in non Gaussian variables we actually can then use the non gaussianity to get more information well and non gaussianity together with independence.",
                    "label": 1
                },
                {
                    "sent": "Well, one theory or in one theorem in probability says that basically if we have two random variables which are independent, Y1 and Y2, then we can basically take any nonlinear transformations or any only functions H1 and H2, and those transformations will always be uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "So well, this is 1 intuitive way of seeing how we could use the non gaussianity so we could look at some kind of nonlinear nonlinear covariances of the two variables.",
                    "label": 0
                },
                {
                    "sent": "Well, this is not.",
                    "label": 0
                },
                {
                    "sent": "Well, some it is not how most ICA methods actually work, but this is one way of looking at how you get more information by using non Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But this is simply now you might think that why don't?",
                    "label": 0
                },
                {
                    "sent": "Why can't you then take this nonlinear transformations for Gaussian variables as well?",
                    "label": 0
                },
                {
                    "sent": "Well, it's because those nonlinear transformations would actually not give you any more information because the Gaussian distribution is completely determined by the covariances and actually also the means.",
                    "label": 1
                },
                {
                    "sent": "So I'm actually actually.",
                    "label": 0
                },
                {
                    "sent": "I'm always here assuming.",
                    "label": 0
                },
                {
                    "sent": "That the variables are centered and the means are zero.",
                    "label": 0
                },
                {
                    "sent": "So then the only information that we have is in the covariance.",
                    "label": 0
                },
                {
                    "sent": "Another way of looking at this is that now if we find Gaussian variables which are uncorrelated, then they are already independent.",
                    "label": 0
                },
                {
                    "sent": "Now this is a deep theorem in probability theory.",
                    "label": 0
                },
                {
                    "sent": "If you have uncorrelated jointly Gaussian variables, then they are also independent.",
                    "label": 0
                },
                {
                    "sent": "So this independence doesn't bring you any any more information than what uncorrelatedness does.",
                    "label": 0
                },
                {
                    "sent": "So if you calculate it, all kinds of nonlinear transformations for cows have variables.",
                    "label": 0
                },
                {
                    "sent": "It would simply be the calculations would simply be pointless.",
                    "label": 0
                },
                {
                    "sent": "You could actually the nonlinear correlations are simply some complicated functions of the linear correlations.",
                    "label": 0
                },
                {
                    "sent": "So the Gaussian distribution is in that sense you might save apps that generate.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so this is basically because why the ICM model cannot be estimated for Gaussian data.",
                    "label": 1
                },
                {
                    "sent": "There is actually a third, a Third Point, third viewpoint.",
                    "label": 0
                },
                {
                    "sent": "So there are here many, many viewpoints on why girls share variables are kind of a no good in this kind of a factor analysis.",
                    "label": 0
                },
                {
                    "sent": "I say context.",
                    "label": 0
                },
                {
                    "sent": "A third one here is that when you have a distribution, when you look at the distribution of uncorrelated Gaussian variables which have the same variance, then it is.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's a symmetric distribution.",
                    "label": 0
                },
                {
                    "sent": "We will see that in and graphically abit late.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a simple illustration of what happens.",
                    "label": 0
                },
                {
                    "sent": "So here we have first a distribution of the of two independent components, which have uniform distributions.",
                    "label": 1
                },
                {
                    "sent": "So if the independent components the two axis have independent sort of uniform distributions and they are independent, then the joint distribution is simply a uniform distribution on this kind of a square.",
                    "label": 0
                },
                {
                    "sent": "So now if we take a linear transformation so we have a linear mixing like in the ICM model, we will get the distribution which looks something a bit like this.",
                    "label": 0
                },
                {
                    "sent": "Now if we do PCA.",
                    "label": 0
                },
                {
                    "sent": "Anne, and also standardize the variance each of the components.",
                    "label": 0
                },
                {
                    "sent": "For illustration, we will actually get something like this, so we see that PCA does not actually recover the original components because the square is.",
                    "label": 1
                },
                {
                    "sent": "The square is essentially rotated.",
                    "label": 0
                },
                {
                    "sent": "This is actually the factor rotation indeterminacy.",
                    "label": 0
                },
                {
                    "sent": "If you know that terminology.",
                    "label": 0
                },
                {
                    "sent": "But at the same time you will see that.",
                    "label": 0
                },
                {
                    "sent": "Well, there's intuitively speaking, there must be some way of figuring out the original independent components, because in this trivial illustration you could simply kind of locate the edges of the square and those will.",
                    "label": 0
                },
                {
                    "sent": "More or less obviously give you the right directions of the independent components.",
                    "label": 0
                },
                {
                    "sent": "And, well, that's actually if you run an IC algorithm, you will actually then get this distribution.",
                    "label": 0
                },
                {
                    "sent": "That is the original 1 and you recover the independent components.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why doesn't this work with with the Gaussian distribution?",
                    "label": 0
                },
                {
                    "sent": "Here I have a kind of a graphical graphical viewpoint on that on this problem.",
                    "label": 0
                },
                {
                    "sent": "So we have two independent Gaussian variables of unit variance and we get this kind of rather.",
                    "label": 0
                },
                {
                    "sent": "Non informative distribution an the observed mixtures after a linear mixing.",
                    "label": 1
                },
                {
                    "sent": "Then we'll have a distribution which is something like this.",
                    "label": 0
                },
                {
                    "sent": "So the opposite variables exercise.",
                    "label": 0
                },
                {
                    "sent": "Now if we do PCA and standardize the components to unit variance, we get uncorrelated Gaussian variables which have unit variance.",
                    "label": 0
                },
                {
                    "sent": "And by the theory I explained before, so because they aren't correlated, they are actually also independent.",
                    "label": 0
                },
                {
                    "sent": "So we actually have here exactly the same distribution.",
                    "label": 1
                },
                {
                    "sent": "As here 2 uncorrelated random, two uncorrelated Gaussian variables with unit variance?",
                    "label": 0
                },
                {
                    "sent": "So well from that viewpoint also, you would think that, well, you can't really recover the you don't actually know.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, from that viewpoint you might think that maybe you actually found the original components because you have here exactly the same distribution as here, but the point is that this distribution here is completely spherically symmetric, although it's a bit difficult to see in this kind of a scatter plot, so that's why I have plotted with the red curves the ISO contours of the probability distribution function.",
                    "label": 0
                },
                {
                    "sent": "So the probability distribution function of the of this data here has is spherical in the sense that it's it's constant on all spheres.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that you can basically then rotate this distribution anyway you like, and the distribution will not change at all.",
                    "label": 0
                },
                {
                    "sent": "Because of this spherical symmetry.",
                    "label": 0
                },
                {
                    "sent": "And so that's.",
                    "label": 1
                },
                {
                    "sent": "That's again, so that is actually the essence of the factor rotation problem.",
                    "label": 1
                },
                {
                    "sent": "So we can basically rotate this data anyway we like, and the distribution will not change at all, so there's basically no way we could figure out what is the right rotation.",
                    "label": 0
                },
                {
                    "sent": "But there is certainly a single right single rotation that will give you the original components.",
                    "label": 0
                },
                {
                    "sent": "That's what we would like to find, but there's no way we can find it from this data, because any rotation will give the same distribution for the data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we then proceed so we know that we have to use non gaussianity together with independence, but how?",
                    "label": 0
                },
                {
                    "sent": "So we had so simple intuitive ideas before, like locating these edges, which is really kind of trivial and doesn't certainly work in real data.",
                    "label": 0
                },
                {
                    "sent": "And then the idea of using non non non linear correlations.",
                    "label": 0
                },
                {
                    "sent": "But actually most it kind of the perhaps the best approach that actually will lead to simple algorithms for solving this problem is not based on those ideas, it is actually based on the idea of maximizing.",
                    "label": 0
                },
                {
                    "sent": "Of maximizing the non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "Well, the central limit theorem basically says tells you that if you have many independent random variables then and you take some kind of an average of them.",
                    "label": 0
                },
                {
                    "sent": "So maybe a weighted average, then the average will have a distribution that is closer to Gaussian than you original variance.",
                    "label": 1
                },
                {
                    "sent": "So well, the central limit theorem actually says this in the limit of an infinite number of random variables with weights that are constraint in certain ways, but kind of an intuitive, intuitive interpretation of the central limit theorem would be that maybe if we just have a finite number limited number of of independent components, then the sum will be closer to ghost.",
                    "label": 1
                },
                {
                    "sent": "So now let us consider a linear combination of the opposite variables.",
                    "label": 0
                },
                {
                    "sent": "So the linear combination being denoted by weights WI.",
                    "label": 0
                },
                {
                    "sent": "Now, because the because XI is a linear transformation of the original independent components, this linear combination here must also be a linear combination of the original independent components, although we don't know the way it's Q here because the weights Q depend not only on the W's here, but also on the mixing matrix A.",
                    "label": 0
                },
                {
                    "sent": "But now, because of this kind of the intuitive reasoning based on the central limit theorem.",
                    "label": 0
                },
                {
                    "sent": "This linear combination Qi of some of the qis I should actually be closer to Gaussian than any of the original independent variables.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that if we maximize the non gaussianity of this linear combination, it should be maximized at the points where this linear combination equals some one of the independent components.",
                    "label": 0
                },
                {
                    "sent": "That is the basic logic.",
                    "label": 0
                },
                {
                    "sent": "So even though we can't calculate these qis, that doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "We just look at all possible linear combinations of the opposite variables and the maximum of non gaussianity should then correspond to the original independent components.",
                    "label": 0
                },
                {
                    "sent": "Because if you don't have if you have anything that's not equal, so then so the so in the independent components that should be more non Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Sorry that sorry that would be more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Any mixture of the independent components would be more Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Well, what I'm saying here is kind of just kind of intuitive logic.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly true, but it can be shown that this method works by.",
                    "label": 0
                },
                {
                    "sent": "It can be rigorously shown under certain assumptions and certain measures of non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "Of course, what we haven't defined here is the measure of non gaussianity, which is what we need for the practical for any practical algorithm.",
                    "label": 0
                },
                {
                    "sent": "So actually this kind of an idea of finding a linear combination that maximizes non gaussianity was published already in the 70s under the heading of projection pursuit.",
                    "label": 0
                },
                {
                    "sent": "So in projection pursuit the idea is was not to solve this kind of a blind source separation problem to recover any or any underlying variables.",
                    "label": 0
                },
                {
                    "sent": "The idea was simply to find.",
                    "label": 0
                },
                {
                    "sent": "Projections of the data that enable enable some interesting visualizations.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a very interesting thing that, well, this the purpose of visualization in projection pursuit eventually ended up using exactly the same objective function that we can use here for ICA.",
                    "label": 0
                },
                {
                    "sent": "So now actually here we see clearly very very clearly.",
                    "label": 0
                },
                {
                    "sent": "I would say the difference between PCA and ICA.",
                    "label": 0
                },
                {
                    "sent": "So in PCA, what we do we look at linear combinations of the of the origin of the object variables and we maximize the variance.",
                    "label": 0
                },
                {
                    "sent": "So in ICA now we maximized the non gaussianity and these two of course completely different things.",
                    "label": 0
                },
                {
                    "sent": "They have nothing to do with each other.",
                    "label": 0
                },
                {
                    "sent": "You can have.",
                    "label": 0
                },
                {
                    "sent": "I mean basically changing the variance of a variable does not affect its non gaussianity at all.",
                    "label": 0
                },
                {
                    "sent": "And for any given variance you can have, you can have any variables of any any kind of non Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Do you ever lie?",
                    "label": 0
                },
                {
                    "sent": "So we see in this sense that ICN PCA actually completely different things, although they are sometimes confused because the acronyms are so similar.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a very simple illustration of this idea of changing non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "So we have independent components of with the uniform distribution as earlier.",
                    "label": 0
                },
                {
                    "sent": "Here's a 2 dimensional scatterplot.",
                    "label": 0
                },
                {
                    "sent": "Now if we look at the solution given by PCA in the previous slide or in, well, this could also be just seen as kind of a some kind of a random mixing, mixing with some arbitrary to arbitrarily chosen coefficients of the independent components.",
                    "label": 0
                },
                {
                    "sent": "Then what if you look at now the distribution of the obtained component components while they are something like this, so much closer to Gaussian than the original independent uniform independent components.",
                    "label": 0
                },
                {
                    "sent": "So you can of course figure to do this kind of simulations very very easily yourself.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this thing basically opens up.",
                    "label": 0
                },
                {
                    "sent": "One way of developing a large number of different.",
                    "label": 0
                },
                {
                    "sent": "I see algorithms, So what you basically need is now a measure of non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "It can certainly be measured in many different ways and then some method for optimizing it.",
                    "label": 0
                },
                {
                    "sent": "So regarding the measure, some non gaussianity I will not go here into details, but there are some rather classical methods of non girls, some rather classical measures or non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "The most logical one is kurtosis, which is basically a form of the fourth moment expectation of the 4th power of X.",
                    "label": 0
                },
                {
                    "sent": "It is very nice algebraically, that's why it is a very classical measure of non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "You can prove things like you can basically prove using those.",
                    "label": 0
                },
                {
                    "sent": "Is that this method of maximizing non gaussianity works, it will it will give you consistent estimates.",
                    "label": 0
                },
                {
                    "sent": "That means estimators that converge in the limit of an infinite sample size.",
                    "label": 0
                },
                {
                    "sent": "But it is actually.",
                    "label": 0
                },
                {
                    "sent": "It is actually rather bad statistical properties in the sense.",
                    "label": 0
                },
                {
                    "sent": "Well, one of them being that it is rather non robust against outliers.",
                    "label": 0
                },
                {
                    "sent": "Because if you calculate 4th moments, well, you're basically taking the 4th power of your data points and so that can be.",
                    "label": 0
                },
                {
                    "sent": "Can be very very strongly influenced by some outliers.",
                    "label": 0
                },
                {
                    "sent": "Well, from statistical criteria, people tend to find that the optimal way of measuring non gaussianity for this context and actually for many other purposes also is to use differential entropy, which is basically a continuous valued version of Shannon's entropy.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that this differential entropy is very, very difficult to compute and to estimate.",
                    "label": 1
                },
                {
                    "sent": "Well, it turns out that actually using differential entropy is essentially the same thing as much as formulating a kind of a likelihood of the ICA model.",
                    "label": 0
                },
                {
                    "sent": "And it is also related to this well known neural network idea of maximizing information flow in a neural network.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "But then if you want using differential entropy, then well, basically the question is that how do you actually calculate that?",
                    "label": 0
                },
                {
                    "sent": "Or how do you compute an estimate?",
                    "label": 0
                },
                {
                    "sent": "How do you estimate that?",
                    "label": 0
                },
                {
                    "sent": "How you how do you compute the estimators?",
                    "label": 0
                },
                {
                    "sent": "In practice?",
                    "label": 1
                },
                {
                    "sent": "What people would use this various kinds of rough approximations of entropy, which typically then go come relatively close to the idea inherent in keratosis, so they're kind of a compromise between the noise, noise, computational properties of kurtosis analyze.",
                    "label": 0
                },
                {
                    "sent": "Statistical properties of different length.",
                    "label": 0
                },
                {
                    "sent": "So once you have figured out your measuring non Gaussian then you need to optimize that.",
                    "label": 0
                },
                {
                    "sent": "Well there are basically two methods which are widely used.",
                    "label": 0
                },
                {
                    "sent": "One of them is a gradient method.",
                    "label": 0
                },
                {
                    "sent": "Well basically.",
                    "label": 0
                },
                {
                    "sent": "Is that is especially used with this info Max likelihood framework.",
                    "label": 0
                },
                {
                    "sent": "But basically there you can make kind of a rather funny funny trick and use what is called the natural gradient, which is a very simple modification of the gradient that simplifies the computation and also seems to have some some good some better statistical properties.",
                    "label": 0
                },
                {
                    "sent": "Or then you can use a fixed point algorithm which which I published some 10 years ago.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But actually I need to make here as a short remark on what kind of non gaussianity is.",
                    "label": 0
                },
                {
                    "sent": "Do you actually have you, you might actually expect to having your date.",
                    "label": 0
                },
                {
                    "sent": "That is also important because.",
                    "label": 0
                },
                {
                    "sent": "Well, often these if you want to have a simple measure of non gaussianity you can you can you can use prior information on the kinds of non gaussianity's that you have in your data.",
                    "label": 0
                },
                {
                    "sent": "If you know that the kind of non gaussianity that you have in your data is is for example this positive that I'm going to talk about then we can drastically simplify your measures from non gaussianity and your algorithms.",
                    "label": 0
                },
                {
                    "sent": "So actually this is kind of basically an empirical finding that, well, the typical form of non gaussianity that we find in most kinds of data analyzed by ICA is what is called sparsity.",
                    "label": 0
                },
                {
                    "sent": "So the idea is positive is basically that the probability densities have heavy tails and a peak at 0.",
                    "label": 1
                },
                {
                    "sent": "Which is classically illustrated by this law policy and distribution, which is plotted here in the solid curve and the best best Gaussian approximation of that is plotted as the dotted curve here.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, now the Laplacian distribution has a very sharp peak here, and what you can't really see very well in this plot is that it also has heavier tails than the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So much more data very far away from zero.",
                    "label": 0
                },
                {
                    "sent": "Perhaps another illustration of this is that well, here we have basically just drawn samples.",
                    "label": 0
                },
                {
                    "sent": "Observations from a Gaussian distribution and hear from his past distribution from exact of exactly the same mean and exactly the same variance, and So what you can see is that the sparse distribution is basically much of that.",
                    "label": 0
                },
                {
                    "sent": "I'm very close to 0, almost indistinguishable from zero, but then every now and then it obtains very large values, which is of course just another way of looking at this idea of peak at zero and heavy tails.",
                    "label": 0
                },
                {
                    "sent": "So many I see algorithms.",
                    "label": 1
                },
                {
                    "sent": "Actually kind of assume that the non gaussianity in your data is it takes the form of sparsity and then then that's how you get much simpler and faster algorithms.",
                    "label": 0
                },
                {
                    "sent": "Actually both the most of the classical.",
                    "label": 0
                },
                {
                    "sent": "Well, actually.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, both the most widely used natural gradient methods and fixed point algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make a bit an assumption which is a bit in this direction.",
                    "label": 0
                },
                {
                    "sent": "It is a bit more general, but it is essentially something it is essentially looking at.",
                    "label": 0
                },
                {
                    "sent": "In practice it is really looking at something like this as possible.",
                    "label": 0
                },
                {
                    "sent": "But of course part it is not the whole story.",
                    "label": 0
                },
                {
                    "sent": "Well, another important form of non gaussianity that you find in your data is typically skewness or asymmetry.",
                    "label": 1
                },
                {
                    "sent": "So simply the data is not symmetric around 0, but it is.",
                    "label": 0
                },
                {
                    "sent": "It is some kind of an asymmetric shape.",
                    "label": 0
                },
                {
                    "sent": "So that's perhaps the.",
                    "label": 0
                },
                {
                    "sent": "Well, let's say that a couple of years ago I used to think that, well, you just that's positive is really what you find in almost any kind of data.",
                    "label": 0
                },
                {
                    "sent": "But recently I have encountered also data where skewness may be important, where you would typically have both skewness and sparsity, so it may still be possible to just use sparsity, but it may your algorithms may improve if you if you also use the idea of skewness.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk I have been pretty much like bashing PCA and factor analysis saying that they are no good but OK, but that's not actually my purpose.",
                    "label": 0
                },
                {
                    "sent": "My purpose is to say that, well, PCM factor analysis do.",
                    "label": 0
                },
                {
                    "sent": "Very.",
                    "label": 0
                },
                {
                    "sent": "They are very well in doing what they are supposed to do.",
                    "label": 0
                },
                {
                    "sent": "But you have to understand that they only do what they're supposed to do, and they don't do I see a.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I see a does not do what PC and factor analysis are supposed to do.",
                    "label": 0
                },
                {
                    "sent": "So what PC and factor analysis are supposed to do is to find a subspace, so reduce the dimension of the data, find the subspace which explains the maximum amount of variance in your day.",
                    "label": 0
                },
                {
                    "sent": "And I see a supposed to find then the underlying variables in your data.",
                    "label": 0
                },
                {
                    "sent": "So of course then the obvious way of combining these two is that you first do PCA or factor analysis and then do ICA in that's in it stop subspace.",
                    "label": 0
                },
                {
                    "sent": "That should combine the both kind of the strong points of both methods.",
                    "label": 0
                },
                {
                    "sent": "And so in this case then again, if you happen to know the classical terminology of factor analysis in this case I see is really strictly a form on factor rotation.",
                    "label": 0
                },
                {
                    "sent": "Based on non gaussianity, so it's very different from classical methods for factor rotation, like very Max quality, Max and so on, because those classical methods do not use the statistical structure of your data.",
                    "label": 1
                },
                {
                    "sent": "In particular this non gaussianity.",
                    "label": 0
                },
                {
                    "sent": "They are basically concerned about about finding about the structure of your mixing matrix or the matrix of your factor loadings.",
                    "label": 0
                },
                {
                    "sent": "And so the good point about, I mean, the classical justification for this kind of PCA factor analysis.",
                    "label": 1
                },
                {
                    "sent": "Kind of doing PCA factor analysis in kind of signal processing context would be that they reduced noise in your signals.",
                    "label": 0
                },
                {
                    "sent": "The idea being that, well, the main signal is contained in the PCA subspace.",
                    "label": 0
                },
                {
                    "sent": "And also then they of course reduce the computation, because then you need to only do ICA in a smaller subspace.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this was actually the end of the first half of my talk.",
                    "label": 0
                },
                {
                    "sent": "That is the introduction to ICA.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I will then go to the start, the second half, which is about applying ICA on EG and MG, especially in the case of spontaneous activity.",
                    "label": 0
                },
                {
                    "sent": "So perhaps a short introduction on this idea of resting state analysis, which is very fashionable in brain imaging circles at the moment, so the idea being simply that, well, you measure a subject's brain while the subject has no task and no stimulation.",
                    "label": 1
                },
                {
                    "sent": "And presumably then, this object is at rest, although of course it's not.",
                    "label": 0
                },
                {
                    "sent": "There are some problems here in the definition, because it might be that that that that's actually the subject is thinking about something with great intensity, even though there is no experimental task.",
                    "label": 0
                },
                {
                    "sent": "But let's hope we don't have that kind of problems.",
                    "label": 0
                },
                {
                    "sent": "So then we can of course measure this kind of activity by any well known imaging method.",
                    "label": 0
                },
                {
                    "sent": "So why is this data so interesting?",
                    "label": 1
                },
                {
                    "sent": "Well, one reason why it's very interesting is that it's somehow kind of objective in the sense that it's not.",
                    "label": 1
                },
                {
                    "sent": "It's not dependent on the subjective choices in the experimental design, or maybe objective is not the right word.",
                    "label": 0
                },
                {
                    "sent": "Let's say perhaps it's kind of universal, so it can somehow reflects the function of your brain in kind of a very general way, and not just in the case of a particular task or stimulation.",
                    "label": 0
                },
                {
                    "sent": "Also, it's interesting because not much analysis for this kind of data has been done so far.",
                    "label": 1
                },
                {
                    "sent": "Well, in the fMRI literature, this is very fashionable and more and more is being done, but especially in the case of edion image, there is very little analysis on this kind of data.",
                    "label": 0
                },
                {
                    "sent": "An from a more philosophical viewpoint, it kind of gives you kind of an IUD viewpoint on or kind of it, as it is kind of a different philosophical assumptions on what the brain is doing.",
                    "label": 0
                },
                {
                    "sent": "So in a more classical neuroscientific context, the brain is more like a stimulus response machine where you give some stimuli and you get responses.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you have a task that slightly different, but in this case what we're talking about is some kind of rich internal dynamics which are there all the time, even if there is no.",
                    "label": 0
                },
                {
                    "sent": "Simulation are no task given from the outside.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So of course, the first thing that you might ask that is anything actually happening in the brain during rest.",
                    "label": 1
                },
                {
                    "sent": "Well, of course you might think that there is not much happening, but actually it turns out that some brain areas are more active at rest than during most most tasks, so this is what Reichel and others called.",
                    "label": 1
                },
                {
                    "sent": "Basically the default mode network based on their measurements on in pets, an later fMRI.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of an intrinsic brain activity.",
                    "label": 0
                },
                {
                    "sent": "Instead of being responses to any some kind of stimulation, so this is a figure of one of their papers and shows kind of the very classical form of this kind of a default mode network, which is really basically.",
                    "label": 0
                },
                {
                    "sent": "It is both at the at the medial frontal cortex, so this is a medial view of the brain.",
                    "label": 0
                },
                {
                    "sent": "Medial frontal cortex and also here at the place called.",
                    "label": 0
                },
                {
                    "sent": "Typically the posterior singlet cortex, or perhaps the precuneus.",
                    "label": 0
                },
                {
                    "sent": "And the point is that even though the kind of surprising thing is that even though these two areas are quite far from each other, their activities tend to be very correlated at rest.",
                    "label": 0
                },
                {
                    "sent": "So here those the activities are shown for the for this part in Redan for this part in.",
                    "label": 0
                },
                {
                    "sent": "So for this sorry for this area in yellow and for this area in red and they tend to be actually.",
                    "label": 0
                },
                {
                    "sent": "Extremely correlated in this.",
                    "label": 0
                },
                {
                    "sent": "In this fMRI result.",
                    "label": 1
                },
                {
                    "sent": "But OK, so how do we actually analyze?",
                    "label": 0
                },
                {
                    "sent": "How would we analyze this kind of resting state activity in more detail?",
                    "label": 0
                },
                {
                    "sent": "Well, one thing that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have done in ICA sorry in fMRI teacher is to basically just take this data and apply ICA on it.",
                    "label": 1
                },
                {
                    "sent": "One well known result was why Beckman and others where they basically had rather limited number of independent components which they considered to be somehow consistent over many over different subjects.",
                    "label": 0
                },
                {
                    "sent": "So what they had was a was basically components which corresponds to visual areas.",
                    "label": 1
                },
                {
                    "sent": "Typically you get a couple of them, then an auditory system and sensory motor system, and then of course this default mode network.",
                    "label": 0
                },
                {
                    "sent": "Here, pretty much like the posterior signet politics and the media frontal cortex, and then a couple of other other things which are perhaps like executive control and dorsal visual stream.",
                    "label": 1
                },
                {
                    "sent": "Well, the actual number.",
                    "label": 0
                },
                {
                    "sent": "Of course this is only one result using ICA, so the number of components and basically the resolution of the components depends on how much, how many, how many dimensions you basically leave after PCA.",
                    "label": 0
                },
                {
                    "sent": "So it might be for example that in some results the default mode network is split into.",
                    "label": 0
                },
                {
                    "sent": "Different components the visual area might be split into many more components than two, and so on.",
                    "label": 0
                },
                {
                    "sent": "But so the interesting thing you said you're basically even incomplete.",
                    "label": 0
                },
                {
                    "sent": "Resting state data.",
                    "label": 0
                },
                {
                    "sent": "You get this kind of this kind of a division into areas which corresponds to well known well known brain systems.",
                    "label": 0
                },
                {
                    "sent": "An is an even sensory systems, although there is actually no sensory stimulation coming in.",
                    "label": 0
                },
                {
                    "sent": "Another another interesting point is that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, you get very similar results if you if the subjects are stimulated.",
                    "label": 1
                },
                {
                    "sent": "If, for example, if their subjects are watching a movie and you apply ICA, you get actually a very similar decomposition.",
                    "label": 1
                },
                {
                    "sent": "So this seems to be something which is not only about the resting state, but some kind of a something about kind of very general properties of some kind of spontaneous activity in the brain.",
                    "label": 0
                },
                {
                    "sent": "Well, of course, using the red spontaneous when people are watching a movie maybe a little bit controversial, but I'm just using here the word spontaneous as opposed to some kind of a two responses to some kind of repetetive experimental protocol where you show the same thing many times face locked.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because of this.",
                    "label": 0
                },
                {
                    "sent": "Very rapidly expanding fMRI leeches, so there's a great interest these days to use the new similar kind of analysis using e.g an image 'cause he is a kind of a classical introductory slide.",
                    "label": 0
                },
                {
                    "sent": "So even me I will not go into details.",
                    "label": 0
                },
                {
                    "sent": "We I think we all know that.",
                    "label": 0
                },
                {
                    "sent": "Well Legion images simply basically have a very high temporal accuracy where slider slightly with temporal accuracy, because this kind of activity in spontaneous resting state activities basic characterized by various kinds of oscillations in different parts of the brain with different frequencies.",
                    "label": 1
                },
                {
                    "sent": "The most prominent ones being around 10 Hertz.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now I will then go.",
                    "label": 0
                },
                {
                    "sent": "To this.",
                    "label": 0
                },
                {
                    "sent": "I will explain a couple of ways of using ICA on e.g an image data.",
                    "label": 0
                },
                {
                    "sent": "This kind of spontaneous image engine image data.",
                    "label": 1
                },
                {
                    "sent": "Basically some of our most recent advances in this in this.",
                    "label": 0
                },
                {
                    "sent": "On this topic.",
                    "label": 0
                },
                {
                    "sent": "So the first question is that, well, actually.",
                    "label": 0
                },
                {
                    "sent": "We saw earlier that ICA basically finds components by maximizing non gaussianity, which typically called means maximizing sparsity, sparsity being the dominant form of non gaussianity.",
                    "label": 1
                },
                {
                    "sent": "But we have to ask that the sparsity of what is the sparsity.",
                    "label": 1
                },
                {
                    "sent": "I mean what?",
                    "label": 0
                },
                {
                    "sent": "What are we actually maximizing the sparsity?",
                    "label": 0
                },
                {
                    "sent": "Well, that depends basically on the pre processing and representation.",
                    "label": 0
                },
                {
                    "sent": "Now each specially this is a good question in EG and Amy G where we have also the oscillatory structure.",
                    "label": 0
                },
                {
                    "sent": "Which basically means that we can talk also about spectral.",
                    "label": 0
                },
                {
                    "sent": "The spectral properties of the data.",
                    "label": 0
                },
                {
                    "sent": "So basically we can talk about.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three different kinds of sparsity's.",
                    "label": 0
                },
                {
                    "sent": "First of all, we have the kind of very classical one.",
                    "label": 0
                },
                {
                    "sent": "I mean, which is basically implicitly in the introductory part, so I see I was implicitly always talking about this kind of this kind of sparsity on gaussianity, which means that we basically look at the temporal.",
                    "label": 0
                },
                {
                    "sent": "We basically look at these time series of the independent components and look at the histograms.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have this kind of a temporally modulated signal which is has has a certain time segment where it is simply.",
                    "label": 0
                },
                {
                    "sent": "Only much more strongly active than elsewhere.",
                    "label": 0
                },
                {
                    "sent": "Then you would actually get a very strongly sparse and non Gaussian signal and you can separate these by ICA.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But another thing is that we could actually also look at the sparsity in space on the cortical space, basically so this is what this is actually how people usually apply ICA on fMRI.",
                    "label": 1
                },
                {
                    "sent": "What they basically?",
                    "label": 0
                },
                {
                    "sent": "Kind of put the data into the ICAO ICAO algorithm in such a way that basically it is the cortical points.",
                    "label": 0
                },
                {
                    "sent": "Which take the place of observations.",
                    "label": 0
                },
                {
                    "sent": "So in some sense they are taking the transpose of the matrix of the data matrix before putting it in, putting into an icy algorithm.",
                    "label": 0
                },
                {
                    "sent": "So then basically the independent components will be some kind of some kind of cortical distributions, and you're maximizing the sparsity of those.",
                    "label": 1
                },
                {
                    "sent": "I will actually have a slide on that a bit late, but then.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the case of Ejen Emaji, we can also talk about sparsity in frequency.",
                    "label": 0
                },
                {
                    "sent": "Now, it might be that we have, for example, if we have this kind of a signal, it is actually not sparse.",
                    "label": 0
                },
                {
                    "sent": "Its histogram is actually not sparse, but if you go to the Fourier domain it will be sparse.",
                    "label": 0
                },
                {
                    "sent": "Well, I will have a slide on that later, so actually the idea of sparsity has then many different ways of applying it for, in which we're going to apply it on easy Remedy data.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first let me talk about this idea of spectral sparsity.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is that, well, if we have this kind of rhythmic sources also like the resources in the brain.",
                    "label": 0
                },
                {
                    "sent": "They may actually.",
                    "label": 0
                },
                {
                    "sent": "The distribution may actually not be very sparse.",
                    "label": 1
                },
                {
                    "sent": "Well or actually non Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So for example if we now take a kind of a.",
                    "label": 0
                },
                {
                    "sent": "And also at the resource where which originally would have oscillations like this.",
                    "label": 1
                },
                {
                    "sent": "And then we basically modulate its strength, like usually the strength of these oscillations is modulated as in the brain as a function of stimulation or task or kind of.",
                    "label": 0
                },
                {
                    "sent": "It is just some for some other reason it is stimulated at rest.",
                    "label": 0
                },
                {
                    "sent": "So now if we take for example this kind of an envelope and multiply this what we might call the carrier oscillation a carrier signal.",
                    "label": 0
                },
                {
                    "sent": "By this kind of this kind of an envelope, we get this kind of signal.",
                    "label": 0
                },
                {
                    "sent": "And actually, well, the distribution of this thing.",
                    "label": 0
                },
                {
                    "sent": "Well it is non Gaussian certainly, but it's kind of very funny it has.",
                    "label": 0
                },
                {
                    "sent": "It is actually.",
                    "label": 0
                },
                {
                    "sent": "It is not very sparse and it is not skewed.",
                    "label": 0
                },
                {
                    "sent": "It's non gaussian.",
                    "label": 0
                },
                {
                    "sent": "There may be very difficult so difficult to measure and especially if you input this.",
                    "label": 0
                },
                {
                    "sent": "If you have this kind of signals and you use a kind of a classical icy algorithm which is mainly looking at sparsity, you may run into big trouble.",
                    "label": 0
                },
                {
                    "sent": "Now if you have something in another envelope which is modulating the signal, much more spa much more strongly, so their modulation is going near closer to 0 and you get the signal like this.",
                    "label": 0
                },
                {
                    "sent": "Then the signal is perhaps a bit more, looks a bit more like sparse.",
                    "label": 0
                },
                {
                    "sent": "But actually if you measure the non gaussianity of this kind of a signal by keratosis, so conventional measures, it is still not not very Gaussian, is not still not very non Gaussian, not very sparse.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a solution for this kind of a problem might be now actually to go to the Fourier domain.",
                    "label": 0
                },
                {
                    "sent": "An look at the sparsity in this in the Fourier domain, so the sparsity of Spectra basically.",
                    "label": 0
                },
                {
                    "sent": "So this is what we proposed in your image 2010.",
                    "label": 0
                },
                {
                    "sent": "So the idea being that we basically take short time Fourier transforms of the data, we divide each channel so immediately channel into short time windows of maybe 1 second and then take a Fourier transform in each of those windows, and then we apply or I see and this kind of data.",
                    "label": 1
                },
                {
                    "sent": "The idea being that if you have like and also that there is signal like this and you do the short time Fourier transform, you transform it into something like this.",
                    "label": 0
                },
                {
                    "sent": "Where basically so like the wind in each window you have one peak like this, adds the frequency corresponding to the so the frequency of this oscillation and this signal here is sparse.",
                    "label": 0
                },
                {
                    "sent": "So ICA might work much better when we do this kind of a short time Fourier transform and then we which means then essentially that we're basically measuring sparsity in the spectral domain.",
                    "label": 0
                },
                {
                    "sent": "Well in the spectral domain.",
                    "label": 0
                },
                {
                    "sent": "But actually because we're taking a short time Fourier transform, we will also be measuring all the sparsity in the time domain as well.",
                    "label": 0
                },
                {
                    "sent": "Actually combining the two.",
                    "label": 0
                },
                {
                    "sent": "So another.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It would be to use then.",
                    "label": 0
                },
                {
                    "sent": "Look at a spatial sparsity, so this is as I told you, this is how basically almost always people apply.",
                    "label": 1
                },
                {
                    "sent": "I see an fMRI, so the idea being that now you look at the kind of images of the cortex which are here shown in this as this kind of caricatures of kind of a cortical sheet with some active areas.",
                    "label": 0
                },
                {
                    "sent": "You consider each of these observed Coop saved brain activity Maps as one object variable, and then you input this into an icy algorithm, which then essentially means that you have you are finding some kind of that independent components are some kind of a origonal activity patterns, presumably very simple activity patterns, which then are combined together to give the activity patterns in the object data typical.",
                    "label": 0
                },
                {
                    "sent": "This would be activity patterns in different time points.",
                    "label": 1
                },
                {
                    "sent": "So now so this basically then reverses the roles of the observations of the Varian variables to the compared to the classical case.",
                    "label": 1
                },
                {
                    "sent": "So now then, because we are maximizing the sparsity and independence of these independent components, it means that this origonal activity patterns should be somehow simple simple because of sparsity, they should be 0 most of the time and only occasionally only in some parts take non zero values.",
                    "label": 0
                },
                {
                    "sent": "Now, can we actually do so?",
                    "label": 0
                },
                {
                    "sent": "This is what how they do fMRI, can we actually?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do it on me G. Well, actually we can do it on energy, but what we need to do is then the first project data from the sensor space to the source space.",
                    "label": 0
                },
                {
                    "sent": "So on assault you some some inverse solver to get out to put our data into the into the cortical space.",
                    "label": 0
                },
                {
                    "sent": "And we also then combine this idea with this idea of short time Fourier transforms.",
                    "label": 1
                },
                {
                    "sent": "Because that seems to work better.",
                    "label": 1
                },
                {
                    "sent": "So basically then we are in this in our paper in human brain mapping, what we're doing is that we're basically maximizing at the same time spatial sparsity and spectral sparsity.",
                    "label": 0
                },
                {
                    "sent": "And well, actually it seems to find.",
                    "label": 0
                },
                {
                    "sent": "It seems to find pretty much the same kind of sources in this kind of resting state data as the temporal as temporal of temporal spectral ICA methods.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, it doesn't seem in this paper the results where in that sense not particularly interesting, but but the point here is that it actually relax is one of the main assumptions of ICA.",
                    "label": 1
                },
                {
                    "sent": "So basically now there is no assumption of temporal independence of the signals anymore because all the other statistical properties are now concerning concerning the spatial and spectral distributions.",
                    "label": 0
                },
                {
                    "sent": "So now now going back, referring back to some, some critics of ICA like I think maybe one lecture yesterday.",
                    "label": 0
                },
                {
                    "sent": "It might be claimed that perhaps in the brain the sources are actually not.",
                    "label": 0
                },
                {
                    "sent": "So this kind of brain sources.",
                    "label": 0
                },
                {
                    "sent": "Components they are actually not independent because there's all kinds of complicated interactions between between the brain sources.",
                    "label": 1
                },
                {
                    "sent": "Well, if you believe that argumentation, then you should actually use.",
                    "label": 0
                },
                {
                    "sent": "You could use something like this because here we actually making no assumptions of temporal independence.",
                    "label": 0
                },
                {
                    "sent": "Actually no assumptions at all on the temporal on the time courses of the components.",
                    "label": 0
                },
                {
                    "sent": "I think this would be this might be really important, especially if you analyze something like evoked responses.",
                    "label": 0
                },
                {
                    "sent": "If you have your responses, then the response the responses in different components which correspond to sources in different areas of the brain might actually be very strongly very strongly dependent, even linearly correlated.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Then I will go to another topic.",
                    "label": 0
                },
                {
                    "sent": "So ICA, which is about testing independent components.",
                    "label": 1
                },
                {
                    "sent": "So as somebody already asked me at the during the break.",
                    "label": 0
                },
                {
                    "sent": "So now if you just what happens, what happens if you just apply icy and some data and then actually the?",
                    "label": 0
                },
                {
                    "sent": "The assumptions of the model are completely violated.",
                    "label": 1
                },
                {
                    "sent": "How do you know that your algorithm actually did something meaningful?",
                    "label": 0
                },
                {
                    "sent": "Well, that is actually of course very important question and that has not been answered very well in the research so far.",
                    "label": 0
                },
                {
                    "sent": "So what I have been developed during the last couple of years is a method for actually testing the independent components.",
                    "label": 0
                },
                {
                    "sent": "Being able to say something about whether the components are likely to correspond to something real or whether you should think that they are simply some kind of noise.",
                    "label": 0
                },
                {
                    "sent": "So this is of course kind of.",
                    "label": 0
                },
                {
                    "sent": "I think this situation has been rather.",
                    "label": 0
                },
                {
                    "sent": "Rather funny from the viewpoint of classical statistical modeling, usually always in science.",
                    "label": 0
                },
                {
                    "sent": "When you do a scientific analysis and you estimate something, you should also also do some kind of testing to see whether the test, whether the estimation results are just somehow trivial.",
                    "label": 0
                },
                {
                    "sent": "Maybe they, whether they could be just due to noise.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe your results could be just zero or M. That means it's completely if they would be, there would be the same in two different conditions, and so on.",
                    "label": 0
                },
                {
                    "sent": "So you should always be assumed to do some kind of statistical testing.",
                    "label": 0
                },
                {
                    "sent": "Before you can publish stuff.",
                    "label": 0
                },
                {
                    "sent": "But then I see a that has not been the case.",
                    "label": 0
                },
                {
                    "sent": "You just run the icy algorithm, get nice pictures, and published.",
                    "label": 0
                },
                {
                    "sent": "It's of course nice for from the viewpoint of publishing a lot of papers.",
                    "label": 0
                },
                {
                    "sent": "But of course there is a risk that many of the independent components are perhaps completely rubbish so.",
                    "label": 0
                },
                {
                    "sent": "It would be nice if we could actually kind of basically do some testing and basically assign P values to components saying that this component is clearly it's likely to be something meaningful, but that component is does not seem to be anything.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "An in ICA, like in many machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "This problem is even more serious because that's because there are.",
                    "label": 0
                },
                {
                    "sent": "Because there's always also the problem of getting stuck in local minima.",
                    "label": 0
                },
                {
                    "sent": "So in classical statistical literature you always hang up, have the problem that you might have a finite that your data might be your data sample might be too small, or in general simply, or there might be too much randomness in your results.",
                    "label": 0
                },
                {
                    "sent": "But in ICA and related methods you also have the problem that your algorithm might actually get stuck in local minima.",
                    "label": 0
                },
                {
                    "sent": "Because the objective function is related to non gaussianity, highly complex and there is absolutely no guarantee for any algorithm that you will find some kind of a global maximum instead of getting stuck in some local minima.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I mixed up the word maximum minimum.",
                    "label": 0
                },
                {
                    "sent": "Let's just say you can get stuck in local Optima.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The idea here would be is based on the idea of intersubject consistency.",
                    "label": 1
                },
                {
                    "sent": "Which I think is very relevant in this kind of a neuroscientific context, so the idea is simply that we do ICA separately on many subjects, and then we say that the component is significant if it appears in more than two of the subjects insufficiently similar for.",
                    "label": 1
                },
                {
                    "sent": "So now of course the whole question then is how to quantify this idea of sufficiently similar.",
                    "label": 0
                },
                {
                    "sent": "So what I did in our recent paper is to basically formulate a rigorous null hypothesis using classical statistical theory, formulate a null distribution and then B.",
                    "label": 0
                },
                {
                    "sent": "Then we can actually quantify this idea or when when the components are sufficiently similar.",
                    "label": 0
                },
                {
                    "sent": "So I think this, especially in the neuroscientific context you, I mean your image.",
                    "label": 0
                },
                {
                    "sent": "In context, you want to have several subjects, but you can also use the same idea by just taking different parts.",
                    "label": 0
                },
                {
                    "sent": "Different datasets from the same subjects.",
                    "label": 0
                },
                {
                    "sent": "Ultimately, perhaps if you have just a single data set, you just divide it into two and say that, well, these are.",
                    "label": 0
                },
                {
                    "sent": "These are basically two different datasets and you do I see separately on them.",
                    "label": 0
                },
                {
                    "sent": "So, so this is for this is for EG and images really well let's say so this is this is for for the kind of temporal ICA which is usually applied on EG or images.",
                    "label": 0
                },
                {
                    "sent": "Made it could also.",
                    "label": 0
                },
                {
                    "sent": "It also well recently.",
                    "label": 0
                },
                {
                    "sent": "Also people have started applying temporal.",
                    "label": 0
                },
                {
                    "sent": "I see an fMRI so it could also be used in that context so we have a paper which is almost submitted.",
                    "label": 0
                },
                {
                    "sent": "So many months ago, I pub promise that it will be submitted in two months, so I will not promise promise anything anymore, but I think it will be submitted in a couple of weeks where we also show how to do this on in the classical fMRI spatializer case.",
                    "label": 0
                },
                {
                    "sent": "8 miss consistency of the mixing coefficients.",
                    "label": 0
                },
                {
                    "sent": "So basically the anatomical location.",
                    "label": 0
                },
                {
                    "sent": "Yes, I mean that is the basic distinction why we have to have different different methods.",
                    "label": 0
                },
                {
                    "sent": "I mean the general principles are the same, but the all the details in the methods are different whether you do temporal, icy or needy fMRI or specialize in or.",
                    "label": 0
                },
                {
                    "sent": "Sorry whether you do space temporal.",
                    "label": 0
                },
                {
                    "sent": "I see an EGM EG.",
                    "label": 0
                },
                {
                    "sent": "Or whether you do specialize in fMRI, that's yeah, you have to look at.",
                    "label": 0
                },
                {
                    "sent": "In one case you have to look at the mixing coefficients.",
                    "label": 0
                },
                {
                    "sent": "In the other case you have to look at the values of the independent components.",
                    "label": 0
                },
                {
                    "sent": "But in both cases, so because it's resting state, the thing that should be consistent is is the spatial patterns not not a temporal time, not time courses.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of what we get from a mediator.",
                    "label": 0
                },
                {
                    "sent": "So in this case we had 11 subjects.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here is 2 components to basic two components, which were basically considered significantly consistent by this method.",
                    "label": 0
                },
                {
                    "sent": "And here I'm showing basically the the topic distributions in those subjects for which the component was was found in sufficiently similar form.",
                    "label": 0
                },
                {
                    "sent": "So this is a classical merism, and as you can see it's found in pretty much the same form in most of the subjects.",
                    "label": 0
                },
                {
                    "sent": "Actually, not all of them.",
                    "label": 0
                },
                {
                    "sent": "999 of the 11 subjects.",
                    "label": 0
                },
                {
                    "sent": "And then you can analyze that in more detail by looking at the Fourier spectrum, politics of which is, so you see that it's actually a classical music from an minimum estimate an.",
                    "label": 0
                },
                {
                    "sent": "Actually, this was.",
                    "label": 0
                },
                {
                    "sent": "Again, this was not actually a resting state data, this was something where we had different kinds of multimodal, kind of a multimodal data set with different kinds of modalities of stimulation.",
                    "label": 0
                },
                {
                    "sent": "So we could also see that this one is particularly well.",
                    "label": 0
                },
                {
                    "sent": "It's actually almost exclusively modulated by tactile stimulation.",
                    "label": 0
                },
                {
                    "sent": "Here's another independent component.",
                    "label": 0
                },
                {
                    "sent": "Which was again found in nine of the components, and it seems to be relatively classical occipitale Alpha rhythm with something like typically the frequency being around around 10 a little bit more than 10 and basically modulated by by visual stimulation.",
                    "label": 0
                },
                {
                    "sent": "Well, sorry I don't have time to go into the details of the stimulation protocol.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so this.",
                    "label": 0
                },
                {
                    "sent": "So basically, of course, without without, my claim would be that every time you apply ICA you should do some kind of testing like this.",
                    "label": 0
                },
                {
                    "sent": "Of course I am not ecocentric enough to claim that you should always use my testing method, but I think you should do some kind of testing anyway.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK and then.",
                    "label": 0
                },
                {
                    "sent": "The last topic of this talk.",
                    "label": 0
                },
                {
                    "sent": "Is about causal analysis or effective connectivity?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The point is now is to model connections between the measured variables, and especially actually.",
                    "label": 1
                },
                {
                    "sent": "So this is about effective connectivity, so it's about directed connections between measured variables, not just saying that two variables are connected, but saying which direction the connection is.",
                    "label": 1
                },
                {
                    "sent": "Actually, the effect is actually going.",
                    "label": 1
                },
                {
                    "sent": "So there are basically two fundamental approaches for this kind of a analysis.",
                    "label": 0
                },
                {
                    "sent": "Now, if the time resolution of the measurements is basically fast enough, then we can do something like autoregressive modeling.",
                    "label": 0
                },
                {
                    "sent": "Now, the idea being that so if your measurements if the causal connections actually are very fast compared to the time resolution of your measurements, then you cannot use something like auto regressive modeling because auto regressive modeling strongly relies on the time information on the time precedents of your effect of your effects and also as has been recently pointed out by many people.",
                    "label": 0
                },
                {
                    "sent": "In fMRI data, we also have the problem that.",
                    "label": 0
                },
                {
                    "sent": "Not only that, time resolution is not fast enough, but also that we have these unknown hemodynamic lags.",
                    "label": 0
                },
                {
                    "sent": "So that's another reason why, Granger, why this kind of a Granger causality might actually be rather difficult for that kind of date.",
                    "label": 0
                },
                {
                    "sent": "So if that it basically it's I mean, the point is that auto regressive modeling using Granger causality is something rather simple.",
                    "label": 0
                },
                {
                    "sent": "So we would like to use that if basically the conditions for using it are met.",
                    "label": 1
                },
                {
                    "sent": "But if they are not, then what we knew need is something else.",
                    "label": 0
                },
                {
                    "sent": "And what is typically useful in this case is the structural equation models.",
                    "label": 0
                },
                {
                    "sent": "Now, if the measured variables are basically the rowie gmed channels, it doesn't actually make a lot of sense to look at the connectivities between the original channels, of course, because the channels are basically measuring nearby channels are measuring pretty much the same stuff, so you could somehow first separate or localize some sources or components and do the analysis on them.",
                    "label": 0
                },
                {
                    "sent": "And another point is that.",
                    "label": 0
                },
                {
                    "sent": "Now if we fish do ICA on this E Gen emaji well, the sources are actually that you get.",
                    "label": 0
                },
                {
                    "sent": "I usually exactly rather exactly uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "So what we may not be meaningful to look at any kind of Granger causality methods, autoregressive modeling because the because because there may not be enough information in the data left.",
                    "label": 0
                },
                {
                    "sent": "I mean in the sense that if the components often are just into independent that you would actually be able to estimate meaningful some kind of Granger causality.",
                    "label": 0
                },
                {
                    "sent": "Auto regressive modeling.",
                    "label": 0
                },
                {
                    "sent": "So one way to get around that is to look at then the dependent look at, then the envelopes of those components envelopes.",
                    "label": 0
                },
                {
                    "sent": "Meaning basically kind of the local variances or the amplitudes of the signals.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, that is actually that is true.",
                    "label": 0
                },
                {
                    "sent": "It is actually very confusing, yes?",
                    "label": 0
                },
                {
                    "sent": "What should I say yes?",
                    "label": 0
                },
                {
                    "sent": "Well, of course, if you believe that ICA will actually give you good sources, good localization of the sources, then you have to use it.",
                    "label": 0
                },
                {
                    "sent": "It will give you individual components which are very independent, but you can't help it.",
                    "label": 0
                },
                {
                    "sent": "So then what you need to do is to basically introduce some kind of a dependency in your data by look basic looking at what kind of statistics between the sources are as independent as dependent as possible.",
                    "label": 0
                },
                {
                    "sent": "And just empirically, you would often find that it is the envelopes which are still strongly dependent between the independent components, while the wild any kind of linear correlations are difficult to find.",
                    "label": 0
                },
                {
                    "sent": "I mean, well OK, looking from neuroscientific viewpoint, it's actually well people talk a lot about phase phase synchrony in a media e.g data.",
                    "label": 0
                },
                {
                    "sent": "But, at least in my experience, an other peoples also experience.",
                    "label": 0
                },
                {
                    "sent": "Also, it's very difficult to find any kind of phase synchrony in this kind of resting state or spontaneous spontaneous data.",
                    "label": 0
                },
                {
                    "sent": "So you may perhaps find it in some, you know when you have some very specific tasks and perhaps like revoke responses and so on.",
                    "label": 0
                },
                {
                    "sent": "But what you do find very clearly is strong dependency.",
                    "label": 0
                },
                {
                    "sent": "Strong correlations of the envelopes, even resting state data.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it is a bit confusing.",
                    "label": 0
                },
                {
                    "sent": "It may seem a bit contradictory.",
                    "label": 0
                },
                {
                    "sent": "But but maybe that's just how you have to do it.",
                    "label": 0
                },
                {
                    "sent": "You have to assume independence in order to get meaningful sources, but then you have to basically.",
                    "label": 0
                },
                {
                    "sent": "Try to find the what kind of dependencies still remain in the data, and it turns out to be basically something related to envelopes.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm running out of time so I will go rather quickly now through our our frame framework for structural equation modeling.",
                    "label": 0
                },
                {
                    "sent": "So the idea in structural equation modeling is that is to model how an externally imposed change in one of the variables affects the others.",
                    "label": 1
                },
                {
                    "sent": "So basically you build a model like this where each variable is expressed as a function of all the other variables.",
                    "label": 0
                },
                {
                    "sent": "Plus a stochastic stochastic disturbance term.",
                    "label": 0
                },
                {
                    "sent": "That case basically then models something like the external inputs to the system.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This method in this model here is in many ways analogous to in factor analysis model.",
                    "label": 0
                },
                {
                    "sent": "In some senses it is.",
                    "label": 0
                },
                {
                    "sent": "It's estimation is actually partly under some assumptions, it's estimation is equivalent to estimation of factor analysis model, so it has the same problems like factor analysis, which is basically that if your data is Gaussian, it cannot really be uniquely estimated.",
                    "label": 0
                },
                {
                    "sent": "You can basically find.",
                    "label": 0
                },
                {
                    "sent": "So you can basically find many different kinds of many sets of para meters BIJ here.",
                    "label": 0
                },
                {
                    "sent": "Which give exactly the same distribution for the day.",
                    "label": 0
                },
                {
                    "sent": "But we can basically borrow the theory of ice.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "EA.",
                    "label": 0
                },
                {
                    "sent": "We can make assumptions similar to ICA.",
                    "label": 1
                },
                {
                    "sent": "And show that then actually this model can be estimated.",
                    "label": 1
                },
                {
                    "sent": "Basically we treat these stochastic disturbances very much like independent components in the theory of ICA.",
                    "label": 1
                },
                {
                    "sent": "We assume that the stochastic disturbances are mutually independent and they are non Gaussian, for example sparse.",
                    "label": 0
                },
                {
                    "sent": "And then we also must make one specific assumption, which is very specific to this kind of a structural equation modeling, which is that the BIJ form an acyclic graph, which means that there is an ordering of this of this variables where all the effects actually go forward.",
                    "label": 1
                },
                {
                    "sent": "So under these assumptions, then we have shown that actually the model can be estimated.",
                    "label": 0
                },
                {
                    "sent": "Actually it is.",
                    "label": 0
                },
                {
                    "sent": "It can be transformed into an ICA model and estimation of the ICM model will enable estimation of this model here.",
                    "label": 0
                },
                {
                    "sent": "So then we would get something like this graph.",
                    "label": 0
                },
                {
                    "sent": "Here we would basically so the coefficients by J here basically then give the different strengths of the connections between the different variables.",
                    "label": 0
                },
                {
                    "sent": "So the goal of this analysis is very different from ICA.",
                    "label": 0
                },
                {
                    "sent": "We're not finding any kind of hidden variables or sources.",
                    "label": 0
                },
                {
                    "sent": "We're finding connections between the variables, the object variables.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where, of course, the opposite variables might actually be results of an earlier ICA.",
                    "label": 0
                },
                {
                    "sent": "The envelopes of an earlier ICA in particular.",
                    "label": 0
                },
                {
                    "sent": "OK so I will just describe one kind of a basic approach for estimating this model, which actually at the same time attacks kind of a very deep problem in ICA in statistical theory.",
                    "label": 0
                },
                {
                    "sent": "Longstanding problem I think.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we have we have two variables, just two variables X&Y.",
                    "label": 0
                },
                {
                    "sent": "And we basically want to know whether a regression model where y = a coefficient times X is is is good or whether we should basically model the regression in the other direction.",
                    "label": 0
                },
                {
                    "sent": "XXS is where X is a linear function of Y.",
                    "label": 0
                },
                {
                    "sent": "Now again we get so that kind of gaussianity problem.",
                    "label": 0
                },
                {
                    "sent": "It is very very well known that if the data is Gaussian then there is no way we can distinguish between these two models.",
                    "label": 1
                },
                {
                    "sent": "They give exactly the same.",
                    "label": 0
                },
                {
                    "sent": "They give equally good fits equally equal likelihoods.",
                    "label": 0
                },
                {
                    "sent": "And everything in both cases.",
                    "label": 0
                },
                {
                    "sent": "But again, if we assume that things are non Gaussian and independent, basically assume that we assume that the residuals are not reservoirs are non Gaussian and independent of the opposite variables which can also be non Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then we can this is this is a special case of the structural equation modeling before and it can is actually closely related to an ICA.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is to approach is to look at the log likelihood ratio of the two models and make a first order approximation of that.",
                    "label": 0
                },
                {
                    "sent": "And we get something very simple.",
                    "label": 0
                },
                {
                    "sent": "We get basically a kind of.",
                    "label": 0
                },
                {
                    "sent": "A difference of two nonlinear correlations between the two variables.",
                    "label": 0
                },
                {
                    "sent": "So now we actually can use the nonlinear correlations that I talked about earlier, but in a funny way where we basically look at the difference of these two nonlinear correlations where we switch the non linearity from here.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "And the non linearity here is very similar to those used in ICA.",
                    "label": 1
                },
                {
                    "sent": "Well, actually I didn't treat the nonlinearities used in ICA, but these are closely related to the sparsity measures that we would use in IC estimation.",
                    "label": 0
                },
                {
                    "sent": "So then if the under all these theoretical assumptions, then basically the sign of this kind of a statistique will tell you which of the two directions is the right one, which one generated the date.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can then use this framework for estimating a bigger model where you have many many variables by just looking at all the different pairs and so on.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and here's one result we have not using exactly these methods I explained, but something related.",
                    "label": 0
                },
                {
                    "sent": "But this is kind of very preliminary results, but this is kind of what we would like to have.",
                    "label": 0
                },
                {
                    "sent": "We basically like to have first to ICA an well here we represent each of the independent components by this kind of topic graphic plot, and then we want to have both those in a graph where we basically then see connections between all the different components.",
                    "label": 0
                },
                {
                    "sent": "The strengths and signs of the connections being shown by the by the width and the colors of those arrows.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we might see that that, for example, this A.",
                    "label": 0
                },
                {
                    "sent": "Here's a group of sources which are more like in the romantic areas.",
                    "label": 0
                },
                {
                    "sent": "Presumably somatosensory motor motor components, which have positive connections between each other.",
                    "label": 0
                },
                {
                    "sent": "And here we have more like visual or perhaps attentional components, and these two are kind of separated in the sense that most of the connections in between are negative means, which is shown as.",
                    "label": 0
                },
                {
                    "sent": "As red or actually looks like purple here.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but so.",
                    "label": 0
                },
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "Well, this is actually a conclusion of the latter part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So basically what we can do is do expert Tori data analysis by ICA.",
                    "label": 1
                },
                {
                    "sent": "So it can basically give us information about the internal dynamics during rests or some possibly some related protocols, like when you are watching watching a movie without making really any assumptions on on on what kind of activities you might find an.",
                    "label": 0
                },
                {
                    "sent": "Of course, without knowing anything about the stimulation protocol.",
                    "label": 0
                },
                {
                    "sent": "And actually, but even you can actually.",
                    "label": 0
                },
                {
                    "sent": "Even used this so you can use this when even in a in a kind of a in a case where you have stimulation when simply the stimulation is too complex, like watching a movie like I just said.",
                    "label": 0
                },
                {
                    "sent": "But even in a more classical paradigm where you have rather simple stimulation, you might be interested in trying out this kind of methods because they can then basically look at activity which is not directly related to stimulation, or which is perhaps related to stimulation, but it, but in a complicated nonlinear way which is not detected by by ordinary analysis methods.",
                    "label": 0
                },
                {
                    "sent": "So basically we had here 2.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ages of analysis.",
                    "label": 0
                },
                {
                    "sent": "So first we find sources or actually I should say components.",
                    "label": 0
                },
                {
                    "sent": "This kind of components, which presumably also called and corresponds to some active sources in the brain.",
                    "label": 0
                },
                {
                    "sent": "Perhaps I should say like that that can be found used for that we have.",
                    "label": 0
                },
                {
                    "sent": "We had different variants of ICA, like the space all forms of ICA.",
                    "label": 1
                },
                {
                    "sent": "For me G time frequency decompositions, which leads to the spectral sparsity criteria and so on.",
                    "label": 1
                },
                {
                    "sent": "And then after that, after finding the sources, we can then basically analyze their effective connectivity by looking at these non Gaussian versions or versions of structural equation model.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But then at some point we should also analyze really something about the significant statistical significance of reliability of the components, for example by using this idea of intersubject consistency.",
                    "label": 1
                },
                {
                    "sent": "OK, that is all.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}