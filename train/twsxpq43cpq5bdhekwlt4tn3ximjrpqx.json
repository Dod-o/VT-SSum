{
    "id": "twsxpq43cpq5bdhekwlt4tn3ximjrpqx",
    "title": "Linear Algebra and Machine Learning of Large Informatics Graphs",
    "info": {
        "author": [
            "Michael Mahoney, Department of Computer Science, Stanford University"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_mahoney_lam/",
    "segmentation": [
        [
            "So I'd like to tell you about.",
            "A couple particular directions that might be of interest as you sort of looking forward.",
            "For matrix methods in machine learning and related areas, and I think one of the problems with developing sort of qualitatively new methods is that you know it's always hard because there's a large body of infrastructure competing against an in in some application area domain, and so it's good to have sort of data that's very new and messy where you can say something about.",
            "So to get a toehold to work through the method and then of course it's going to be applicable more generally, so you can imagine there's a range of reasons people are interested in in large social and information graphs.",
            "Maybe not.",
            "The most common reason, but certainly think one legitimate reason is that so large social and information graphs are sort of bad in just about every way you can imagine that the data might be bad, and then some.",
            "So any idea that you might have about the way the data is nicer, smoother, regular or low rank or high rank or anything else probably isn't satisfied.",
            "And so from that perspective, there are very very nice test case for the development of algorithmic methods, and in particular because a lot of matrix and sort of spectral based techniques have some sort of robustness underlying them and these graphs you if you have a million nodes in 10,000,000 edges are pretty sparse, sort of implicitly getting some smoothing because of those techniques is very useful, so we've all sort of self selected to be at a workshop called numerical mathematics in machine learning, so I'll describe it a little bit more from the technical perspective."
        ],
        [
            "But the particular things we talk about will also have.",
            "Some downstream applications at a .2, so I thought what I might do is give a little bit of history of linear algebra machine learning.",
            "It will be biased and sort of everything that I say will be oversimplified a little bit, but I want to set the stage for describing a range of types of methods that people do, and hopefully you know all of those will make a point of contact and you'll certainly ahead and say, yeah, I can see how that would be something people do.",
            "And then I'd like to describe some properties of very wide class of large social and information graph so this line of work actually.",
            "Started while I was at at Yahoo.",
            "I was the resident matrix expert 'cause I had been doing a lot of matrix sampling, random sampling, random projection algorithms for range of large datasets and so two different business units came to us with problems that at the end of the boil down to finding good clusters.",
            "So one wanted to find markets economic.",
            "These are advertiser bid phrase graphs.",
            "So sets of advertiser phrases that you could reasonably think about as markets.",
            "In the sense of Vancouver flower market or something to pull out to do bucket testing and the other wanted to get clusters of words to do something called query expansion for advanced Match where you expand queries to nearby queries.",
            "So at the end of both those boiled down to clustering problems there getting some very strange results that didn't make so much sense, and so they came to us and they were right.",
            "They were getting a lot of results that didn't make so much sense, and so we use a range of matrix methods and manifold methods that are popular machine learning.",
            "That sort of are basically matrix methods, but a bit more descriptively flexibel a range of other techniques, and the particular structural properties that we saw were sufficiently bizarre and counterintuitive that I couldn't believe that they were peculiar to the two advertiser bid phrase graph.",
            "So we went back and looked at on the order of 70.",
            "7 zero large social information graphs and there was a couple key structural properties that were fairly ubiquitous but very, very different in than wide wide range of datasets that you'd see, and sufficiently subtle counterintuitive that a lot of the tools you would do would would just not be particularly relevant.",
            "So, for example, if you want to do a recursive partitioning, you split the graph in half, split into two, and recurse.",
            "If you happen to be nibbling off 100 nodes out of a million, you recursion depth is going to be pretty big, as opposed to half million half million.",
            "So building off 100 nodes out of a million that's not so good, and so there's a range of tools, and I'll describe the structural properties of the range of tools that would have implications like that, so the tools would just be an interesting or not particularly useful, and there's a few particular properties here, and basically what they have to do is that in a lot of cases there's good local clusters, so if you're familiar with all the stuff on locally linear methods in machine learning.",
            "In these graphs, there's lots of spots that look vaguely locally linear, whatever that means.",
            "You know if nothing else you can put them on a piece of paper and look at them, which means that there's sort of a 2 dimensional embedding, and think about them or something.",
            "What a lot of the manifold stuff does from there is, say, OK good, now I see these things.",
            "I'm going to posit some objective.",
            "Try and paste these together into something global and under extremely strong assumptions that works, but if the data is at all noisy, robust it you know it often times doesn't know what's going on.",
            "All of these graphs is you have these local clusters that are something meaningful, but at larger size scales there isn't any particularly good embedding in a very precise sense of the words.",
            "If you don't, expander is, these graphs are expanders at large size skills.",
            "They do not have good cuts.",
            "There's degree heterogeneity, which adds an extra wrinkle, which means that all is not quite hopeless.",
            "But to a first approximation there pretty bad at large scale, so you're not going to have good cuts.",
            "So what you need to do is, since low rank spaces are particularly nice in certain ways, the question is can you take a lot of the spectral machinery?",
            "That sort of implicitly uses low rank spaces and maybe get a local version local because you want to get the Vancouver flower market as opposed to something at the size scale of the world in a lot of these times, you make various sort of design decisions, we truncate something, or you do early stopping or range of these things.",
            "Can you and any other operational things that practitioners do?",
            "Can you understand in a more principled way with the implicit regularization in these or other approximation algorithms is?",
            "And you might imagine that a lot of the VC sorts of bounds if these graphs of this sort of bad simply don't extend, and they don't, and you might say, can I use other sorts of things like annealed entropy or so on to get some sort of capacity control for learning?",
            "So I'll describe the bad properties here just to give you an example, I could spend a whole hour on this, but I'd like to just give you the bottom line and then maybe give you 3 examples of new machine learning or new linear algebra.",
            "That will be motivated by this, and I'd be willing to bet something pretty close.",
            "My bottom dollar that you know if you workout the methods in something like here that these sorts of methods will be applicable in a range of other domains, because what happens in a range of other domains as people will cook the data to make global spectral partitioning or whatever applicable.",
            "And so if you have better methods developed in a class of datasets that are particularly bad, they'll be able to apply back more generally, and I can talk."
        ],
        [
            "Little bit more offline if anyone interested."
        ],
        [
            "So.",
            "Numerical linear algebra in the pre history you know way back when Eckart Young was not published in the Journal of Linear Algebra, right?",
            "It didn't exist at the time was probably psychometric our biometric us so close connections to data analysis and noise and statistics.",
            "I mean it was very much in the air.",
            "In 1950 or so, you got the computers on the machine and its cultural Brandon.",
            "This is more or less banished.",
            "You didn't see it again until about five years ago when Jonas and I and some others had very precise randomized algorithms, and so those have been implemented by a numerical analysts in a range of scientific computing applications, and they actually perform very, very well, and that's a very nontrivial statement, because the natural questions asking theoretical computer science don't perform well need to have slightly more refined ones.",
            "So no randomness.",
            "Then in the 1980s numerical linear algebra sometimes come up.",
            "Age because you get high quality codes that are written for computers, and they're written for high performance computing.",
            "So you want to do matrix vector multiplies fast and when you want to matrix vector multiplies fast, you want 10 digits of precision and you just want to be fast and then a lot of more.",
            "Data sort of applications that have arisen since then.",
            "Latent semantic indexing of doing something term document, spectral ranking, sorts of ideas, partitioning and normalized cuts, and so on that have arisen recently.",
            "You don't need that much precision.",
            "Oftentimes some you know that much precision is a liability, and so there's a lot of new problems that among."
        ],
        [
            "Things this workshops addressing so machine learning way back you might do that.",
            "I don't know what machine learning was back in the 40s before you did machines, but you know those statistical data analysis that you did by hand the computers or people in the 60s you had roots, maybe in artificial intelligence and neural networks, and perceptrons and so on.",
            "So forth.",
            "Anna, lot of combinatorial foundations in terms of PAC learning.",
            "VC theory, that sort of stuff.",
            "So it's not obvious with the material those combinatorial ideas have to do with vector spaces and so on, but it turns out in the 90s a lot of connections to vector space ideas.",
            "Kernels, manifold based methods, normalized cuts and so on and so forth.",
            "And so again you know the fact that you're able to work in a vector space here gives you a lot of power in terms of applying these things and robustly in a range of data applications, and so again, you're having a range of new prob."
        ],
        [
            "And one of those will be talking about today.",
            "So to give you a feel, sort of just a few examples and I hope these examples are also familiar to everyone you wanted.",
            "Spectral partitioning and this was made popular by Shi and Malik in the machine learning and computer vision community in the name normalized cuts.",
            "I hope this is familiar to you, but the notation might not be, so let me describe the notation.",
            "L is the Laplacian of the graph.",
            "It's roughly the diagonal minus the adjacency matrix and you normalize it.",
            "And what you want to do is minimize X. Transpose LSX is a dummy variable.",
            "You want to minimize this mixing.",
            "This has to do with the quadratic term unmixing such that X transpose X is 1, so there's a variance constraint.",
            "Index is perpendicular to the all zeros vector, so this is solvable via an eigenvalue problem, which isn't immediately obvious.",
            "But it turns out it is.",
            "You can get bounds on how good this is via something called the Cheeger's inequality, so if you're using this, you're able to say that this solution is good relative to the control thing, because if she goes inequality you have intractable graph partitioning problem.",
            "You relax at the spectral and you know you're quadratically close because of triggers inequality was used for years in parallel scientific computing, computer vision, and machine learning.",
            "And oftentimes the results are formulated such that if the data is drawn from somewhere nice, and if you can let the sampling the number of data points go to Infinity and so on and so forth, then the graph Laplacian will reconstruct some sort of manifold Laplacian, some nice underlying geometry.",
            "But you know what?",
            "If there's not a nice underlying geometry?",
            "What if there's not good, well balanced cuts?",
            "But if you're not in some low dimensional place, so the idea is, here's the adjacency matrix is a left half and right half.",
            "Botham are about 5050 or 6040.",
            "Look at the eigenvector you cut.",
            "Alright, so you know what if there's not good, well balanced cut.",
            "So what if these other bad things happened?"
        ],
        [
            "What is this algorithm?",
            "Because it will return an answer regardless of what you feed in.",
            "So long history of ideas, having with spectral ranking throughout the years, paid rank is sort of the most recent, most popular instantiation of it.",
            "So you can think of this as a damped spectral ranking of a normalized adjacency matrix of the web graph so that eigenvector computer on the previous slide you put all the data points down to that and you cut somewhere.",
            "But you can think of this as providing a ranking over the nodes.",
            "In.",
            "This idea has been invented again and again in a range of different areas.",
            "And from the perspective this is a great thing for people in numerical linear algebra, they got very excited because you know this involves matrix computations.",
            "But one thing that is a little bit on appealing to them was that when computed you oftentimes you approximated with three steps of the power method which you spend your life.",
            "Look dealing with long sales and all these sorts of things.",
            "That's a little unappealing.",
            "It's not particularly important for today's ranking functions.",
            "That's just an FY I.",
            "In case you don't know it, but the idea underlying it could be useful more generally, and we'll see an example later.",
            "In particularly, the personalized notion of this, which is yes to that local issue.",
            "An very strong connections to some of these other things.",
            "We're talking about a minute ago."
        ],
        [
            "So these things may manifest themselves in the following way, so hopefully you're familiar with something called latent semantic indexing or latent semantic analysis.",
            "You have a bunch of documents, you have a bunch of terms.",
            "You normalize them in some way.",
            "You have a big matrix, it's pretty sparse, and you say what I want to do is get better precision recall.",
            "So I'm going to get a sub K. I'm going to fix K according to some rules and model selection rule and I'm going to instead of doing precision recall and this thing, I'll do precision recall you.",
            "I'll do my prediction task on a sub K. So basically taking the matrix in your filtering it through a low dimensional space, which is a reasonable thing to do if you believe that.",
            "K concepts out in the world and maybe each document is a manifestation of a linear combination of one of those concepts.",
            "So what you have in the back of your mind when you're doing something like this is that is this roughly K concepts that are most important and the rest is basically noise.",
            "So basically you're in a low dimensional place, maybe with some other little structure, but the you know dimension, place and the other thing you're assuming is that there's no data point that's particularly important.",
            "There's not a single data point that is extremely important, and the simplest way to state this is, you know there's some Gaussian process going on underneath, and so all the nodes contribute a little bit, but doesn't need to be Gaussian particular.",
            "Anything where the information spread out on most of the nodes, so it's an empirical question to say, are these matrices well approximated by something low rank?",
            "Is it the case that the information is distributed?",
            "Or maybe are there localized hotspots?",
            "Not obvious right so?"
        ],
        [
            "Here's a theorem I don't have a plot, but the empirical results follow this sort of idea very closely, and if the theorem is, you know, the largest eigenvalues of the adjacency matrix of a gold graph with power law.",
            "Degree distributions are also parallel distributed, so if you have any social graph or you have term document graph, so you have whatever that are noisy in some sense of the word and in the limit you might, you might realize this by saying it's it's I should have random in here somewhere large bags of adjacency matrix of a random power law graph.",
            "Parallel distributed, so if you have bad degree variability and you're pretty noisy then then that's going to manifest itself in the spectrum that the spectrum is going to power law distributed.",
            "So what this says is that there are 10 nodes that are most important, but they do not capture most of the information.",
            "You know they capture 20% of the information that 20% of variance or whatever.",
            "Alright, to get 30% of the variance you don't need 11 or 12 directions, you need 100.",
            "And to get 40% of the information into 1050% need 10,000.",
            "So by now you densifying the graph exactly when you should be sparsifying it.",
            "And so in some sense, you're damaging the relevant structure over what's going on.",
            "So, So what this is saying is that there are a small number of most important things, but they do not capture most of the information.",
            "So the assumption that you're in the right place is not even close to quality.",
            "With satisfied in a lot of these datasets.",
            "The entries of the eigenvectors also.",
            "We'll get to that.",
            "Yeah."
        ],
        [
            "So the question here is, what about is the mass in the eigenvectors spread out?",
            "So I'll answer the question more refined way.",
            "But initially given M by N matrix.",
            "So how would you quantify localization?",
            "So you might say how localized or coherent or the left or the right singular vector.",
            "So there's a range of ways to do this in physics for reasons having to do with the Hamiltonian.",
            "It's called the inverse participation ratio, which is a variant of this.",
            "We use this to get relative error bounds.",
            "In the worst case matrix sampling algorithms, and it turns out to be a much more general concept that's useful for lot of these social graphs.",
            "So let me say that row is PU sub KA projection matrix on use of K. The left or right singular vectors I say bye.",
            "So take the K left singular vectors that define the subspace project onto it.",
            "You have diagonal elements and look at those diagonal elements.",
            "Alright, equivalently, with those diagonal elements are is if you have.",
            "This is your use of K. Your matrix of left singular vectors.",
            "That is, the Euclidean norms of these rows, right?",
            "The Euclidean norms of these things are one Euclidean norms of these.",
            "This could be a truncated identity.",
            "This could be a truncated Hadamard.",
            "These could be very uniform or very non uniform.",
            "But there's nothing special about the left singular vectors.",
            "Any basis spanning that space will have the same properties 'cause it's a function of the projection matrix.",
            "These are statistical leverage scores.",
            "If you're a statistician, these are the diagonal elements of that matrix, so we have a bunch of papers on the topic.",
            "You won't see that word until we had our PNS paper because we didn't know about the connection.",
            "And finally, you know you read the book of third Time and Dawn breaks on Marblehead and so the connection becomes obvious.",
            "So these are statistical leverage scores.",
            "Widely used classically in regression diagnostics.",
            "Basically for finding outliers.",
            "So these will quantify which rows have the most influence or leverage on the low rank fit.",
            "And as these are important for bridging the gap in terms of numerically implementable randomized algorithms.",
            "Not always, but often.",
            "Very often they're very nonuniform.",
            "In practice, this particular data set, not a social graph.",
            "This is DNA snip data.",
            "Social graphs would be much, much worse.",
            "I couldn't even put it.",
            "There would be that bad.",
            "This DNA snip data and this is the average value.",
            "There's a red line here.",
            "I don't know if you can see it and this is how non uniform they are.",
            "And it turns out if you keep the top 10 or 20 and you give those snips to the geneticist, those are particularly informative with a capital I informative as a particular measure that's of interest in the area.",
            "These correlate very strongly with the snips that are most informative, so they are in fact very non very nonuniform, and they often times correlate with things that are of interest.",
            "If you're looking at adjacency matrices, they tend to correlate with high degree nodes.",
            "If you're looking at Laplacians in particular, normalize well either.",
            "Normalized or not Laplacian.",
            "They tend to small clusters that will get back to in a minute, so very nonuniform so."
        ],
        [
            "You're not even close to satisfy either the assumptions that the low rank spaces SVD would be assuming.",
            "So why do these methods work at all so implicit?",
            "Effectively these methods work because low rank spaces are very, very structured places.",
            "Alright, I mean this is just not a lot of places to hide your mistakes as opposed to I mean graphs where bad things can happen.",
            "Graphs are relatively good hypergraphs, tensors.",
            "You know non negativity when you start taking a lot of those other stuff you give yourself much, much more descriptive flexibility.",
            "These are very structured places so you can only hide your mistakes in so many places in some sense of all models are wrong but some are useful.",
            "Those that are most useful, those that implicitly have capacity control.",
            "So I haven't act on capacity control anyway.",
            "There's no regulations on by the data modeling.",
            "I'm working in a structured place.",
            "Sort of implicitly give you sort of capacity control and basic.",
            "It's 'cause diffusions and L2 based methods aggregate information in a very particular way, and that factors associated pluses and minuses.",
            "So it means that information spread out eigenvectors if you're diffusing in a low rank place, but it means you get hot spots if you have very high degree nodes.",
            "If you have small clustering offer range of other."
        ],
        [
            "So.",
            "So the question might be, since I've given you a hint that what might be going on here is, we've seen sort of roughly why some of these methods might work.",
            "I've sort of argued why that might be the case.",
            "I'm going to show you how these are bad in a lot of ways.",
            "But the question is, can we get sort of local analogs of some of the traditional machinery and maybe apply them here?",
            "And so the answer is yes.",
            "Empirically and then I'll give you some.",
            "Some more theoretical things and this is I could talk for now and that's all in."
        ],
        [
            "Miracle, but I'll give you sort of hint of it.",
            "So lots of networks out there.",
            "Networks are very popular.",
            "Think of social graphs, collaboration, networks of friendship networks, maybe information networks with Citation blog cross postings.",
            "As I said, advertise a bid phrase graph.",
            "With these things grew out of an for all these things, you can model them in different ways, But let's not give ourselves too much descriptive flexibility.",
            "Let's just work with the so called interaction site model of a network where nodes represent some sort of entities.",
            "Advertisers, phrases, authors.",
            "You know whatever and edges will represent some sort of interaction between entities.",
            "I'm going or direction here 'cause this work grew out of asking for clusters directing.",
            "This is a non trivial extension of this weights.",
            "Don't worry about, everything will go through to any weight that a practitioner cares about.",
            "Meaning you could design weights that such that what I'm talking bout will not be the case, but when we went back and looked at 70 graphs we said give us your grass waited the way you like and will chew on them so they only about weights time evolution.",
            "We have ideas on but I'm not going to."
        ],
        [
            "Weather today, so to give you an example, live Journal, 4 million nodes, 40,000,000 edges.",
            "That's typical of the sparsity.",
            "I mean incredibly sparse.",
            "10 nodes on average.",
            "And since you have power law heavy tailed structure here, it's even worse in general, except for some high degree nodes.",
            "Epinions Flickr, delicious coauthorship graphs in physics and computer science.",
            "This is 10 of the 70 that was in the short version of the paper, not particularly largest to smallest.",
            "Everybody's representative information graphs a bunch of web snapshots, Athens to papers, the bipartite structure.",
            "There is a little bit like the."
        ],
        [
            "The term like the advertiser bid phrase properties.",
            "So what people want to do?",
            "As I said in the application was defined isolated clusters in markets with sufficient money or clicks and sufficient coherence.",
            "So want to be relatively isolated.",
            "You wanted to have sufficient size by whatever metric, and the Internet companies oftentimes something correlated with money because it has revenue downstream and sufficient coherence that you can pull it out and reason about it as an economic unit or whatever.",
            "So if you pull it out in any reason about the used car market versus the flower market or whatever, and the idea that people have in the back of their mind is that the world looks this way.",
            "You have gambling markets and you have sports markets and you might have some overlap and you might have some hierarchy between them and you want to find what is the clickthrough rate, or whatever of the sports gambling market when you draw the picture on a piece of paper like this, what you're saying is that gambling has a lot of stuff going on in particular relative to how much gambling interacts with the rest of the world.",
            "You've drawn a circle or square around gambling, and similarly with sports.",
            "And there's an interaction here, but in some sense, in terms of, you know that in some sense these things gambling and sports have more stuff going on compared to the outside relative to sports gambling.",
            "Bigger circles are better than smaller circles.",
            "Elephants dissipate heat in different ways than mice do, because their volume and surface area properties are very different.",
            "And you're saying that bigger things are better.",
            "Insofar as this internal versus external tension is talking, but that should have the back of your mind.",
            "What's usually not asked.",
            "Is this even possible?",
            "So the tools I mentioned in the first half actually addressed this question.",
            "I'm not going to go into detail about that, but will be able to actually test the hypothesis of the data looks this way the answer is no.",
            "And then I'll tell you what the data looks like, and then I'll give you some examples of new machine learning linear algebra that comes from sort of looking under the hood in terms of how these techniques are useful."
        ],
        [
            "So this is what people think they look like.",
            "This is what the graphs actually look like.",
            "When you put them on 2 dimensions, they look like a mess of hairballs.",
            "Dirty little secret in a lot of this visualization game, you'd like visualization to reveal something about the data.",
            "If you are familiar with state of the art in visualization algorithms, you could tell me what visualization algorithm we used to visualize this.",
            "'cause these rings and so on.",
            "If you were familiar with state of the art in publicly available networks, you'd probably be hard pressed to tell me which graph this was.",
            "So these visualization algorithms are revealing more about the inner workings of the algorithms and about the graphs they're trying to visualize.",
            "Right, so that's a little disturbing.",
            "If you're interesting visualization, but it suggests that you know you might be able to take advantage of the artifactual structure that this particular visualization does to reveal something about the inner workings of the algorithm.",
            "And I have a different visualization.",
            "I get another hairball.",
            "It's a mess, but it's slightly different artificial structure.",
            "You know, if I use something spectral like here, and it might not be obvious that it was, but if it makes certain local to global decisions or whatever, can I reveal insight about the graphs and so that sort of philosophy guided a lot of the."
        ],
        [
            "But not with respect to visualization with more generally.",
            "So I talked about the large elephants in the small mice.",
            "Let's quantify that in terms of conductance, this is the normalized cuts objective.",
            "Conductances defined for a graph don't think manifolds then graph conductance if the graphs of discrimination of a manifold, then things are good.",
            "But this is worlds for an arbitrary graph an it's a surface area term.",
            "The number of edges between the conductance of a set is the number of edges between that the set and its complement.",
            "So a surface area term, the numbers between this sentence compliment divided by volume term.",
            "And there's a couple of things you can put down here, but it's basically the number of edges.",
            "Inside your set.",
            "So the best conductance this is the conductance of a set of nodes.",
            "The best conductance in the entire graph.",
            "That's an intractable problem.",
            "One way to approximate it is to relax into it and solve a spectral thing a different way to approximate it is to relax in a different way and solving multi commodity flow problem.",
            "So there's a bunch of ways to approximate it.",
            "Let's ask a slightly more refined question.",
            "Let's define the network community profile plot to be the minimum over clusters or sets of nodes of size K of this quantity.",
            "So basically the size resolved version of conductance.",
            "Right and in the same way as conductors.",
            "Conductance captures a surface area to volume.",
            "I'm talking bout clusters, but you can think of them as this is, this is the motivation people have when they talking bout communities in networks.",
            "Even if they go on to find some other objectives.",
            "This is motivation.",
            "People in the same way this profile plot capture sort of size resolved version of that we can ask with the big things are better worse than smaller things and we'll see."
        ],
        [
            "It's important later, so there's two criteria here.",
            "Surface Area, terminal, volume term, and for some graphs, let's column space like graphs.",
            "Things that are available in a low dimensional space, random geometric graphs, discretizations of low dimensional spaces, you know, things that are sort of morally low dimensional.",
            "These these two quantities, the cut quality and the cut balance.",
            "How good the cut is by that measure and how balanced the cut is it 5050 or 99?",
            "One these two things work together basically because if you're living on a low dimensional space you have this issue with the elephants in the mice.",
            "The bigger things have more stuff in relative surface area.",
            "There are graphs such that this plot, looking at volume versus the surface area term, is flat.",
            "So for low dimensional things this plot goes down.",
            "So down is good and as you get larger and larger you get better and better 'cause you have more stuff going on inside you, like the elephant.",
            "There are graphs for which this is flat, those are known as expanders.",
            "Those are very bad properties.",
            "They're very, extremely in alot of ways.",
            "In particular, constant expanders that very, very useful in algorithms because they're good test case for a lot of different things will get back to that and what we'll see is that in a lot of cases this curve actually goes up, so the best possible cuts get worse and worse as a function of size.",
            "So at large size scales these things are expanders at small size skills you have good clusters alright?"
        ],
        [
            "So if you're interested in Zachary, this is a 34 node graph that the Community detection people like to work with.",
            "This plot goes down, local minima correspond to things that you might plausibly want to call clusters.",
            "This is Mark Newman's network science, which is a couple 100 nodes constructed from bibliography on.",
            "This thing goes down also, and the reason these two things go down basically is because if you deal with a chain or a grid, or a cube or whatever, for those isoperimetric reasons for the surface area to volume reasons, this thing will go down.",
            "And this graph embeds pretty well on the plane.",
            "You can visualize it and it goes down.",
            "And so if you want to do normalized cuts, you mean more or less, any method will cut you right there.",
            "I mean, an awful lot of work is done arguing about whether 9 should be on this side of the cut of that side of the cut, whatever.",
            "But there all the same they'll just put you there in the cut in half and similar.",
            "Here they will cut there, and maybe it'll cut there there.",
            "But the first approximation will be the same and the actual.",
            "We're dealing with an extremely statistic here, so you gotta be careful.",
            "The actual sets of nodes and achieve a minimum.",
            "That's a very non robust.",
            "But the qualitative property of going down is very, very robust, so more or less anything that lives on the Earth will go down.",
            "And if you expand or click or something, this thing will be flat.",
            "So if you're going down, you can do a bipartition and recurse and everything's you.",
            "Know, reasonably good.",
            "This corresponds to what I showed you before with the spectral partitioning we have two sides that are reason."
        ],
        [
            "Well balanced, so the downward sloping profile plot is important for small social graphs for validation, low dimensional things that inform your intuition.",
            "Hierarchical networks in a natural interpretation in terms of isoperimetry and K means and manifold, and so on and so forth.",
            "So they said we start very some very strange properties.",
            "We went back and looked at 70 large social information graphs.",
            "I'll give you the leading bit of information this summer.",
            "I'm looking at 10,000 feet.",
            "Every graph is an exception to simple story.",
            "I could tell in an hour, so everyone's going to be exception.",
            "We can talk offline, but the leading order piece of information is what do these things look like if you squint at him from 10,000 feet?",
            "Do they look like a hot dog that you can cut in half?",
            "Are they in expanded with no good cuts or is it something worse going on?"
        ],
        [
            "So the idea is that we're going to in order to answer that question.",
            "It's hard 'cause I showed you these graphs look like they're a mess.",
            "So in a sense we're going to probe large networks with approximation algorithms.",
            "We're going to use approximation algorithms for this NP hard graph partitioning problem says experimental probes of network structure.",
            "So the nice thing about this particular notion of conductance is that not only does it correspond to what you might think of as clusters, but there's a rich body body of theory and practice.",
            "I mean, this is normalized cut, something I'd mentioned on Slide 3.",
            "Rich body of theory and practice that tells how it behaves.",
            "So machine learners and scientific computers like spectral methods and spectral methods.",
            "Sugar gives you a quadratic approximation guarantee, and that's not an artifact analysis.",
            "There are graphs that are that bad, and those are graphs that in some sense confuse long pass with deep cuts.",
            "Multicommodity flow methods will give you a log and approximation that's not not effective.",
            "The analysis there are graphs that are that bad.",
            "Those are constant degree expanders and there's a bunch of other sorts of methods so these things you start with something intractable.",
            "You relax in different ways and you filter through different geometric places.",
            "The geometric place that multi commodity flow is effectively being filtered through is very different than spectral, but this sort of implicitly regularising in different ways.",
            "And so we might say, can we tear the graph apart using these complementary methods and play them off of each other in a little bit like an experimental probe?",
            "So you intuitively, if you want understand what protein looks like, how do people do it right?",
            "They send an NMR, they put in a liquid or crystal, they send it in tomorrow they sent an X Rays.",
            "They measure lots of garbage and then they look at all this stuff.",
            "Knowledge of the physics of what you put in, and trying to infer what these things look like.",
            "So we're doing exactly the same thing, right?",
            "We have this graph we can't visualize were tearing it apart with Spectra were tearing it apart with low.",
            "We know that spectral behave has certain problems on certain classes of graphs and perform very well on others.",
            "We know that flow has problems.",
            "Certain classes of graphs, and if he's very well in others, so tearing it apart with these things going to measure lots of junk and then we're going for with this thing looks like.",
            "And there's a lot of details that I'm going to gloss over, except to say the actual way we implement, say, for example, spectral is to do a local version of spectral, so special about eigenvectors.",
            "But one way to compute this is to say, do a random walk walk and run it forever.",
            "Right power method or something like this?",
            "Or heat kernel?",
            "So you might say, what if you put?",
            "All your mass, all your heat on one or a small number of nodes and diffuse just a few steps and maybe truncate a little bit and diffuse four or five more steps.",
            "So what you're doing something very local, you're truncating that you're doing that for algorithmic reasons, 'cause you don't touch a lot of nodes.",
            "It might have statistical side effects.",
            "And so you might ask, what are these things?",
            "So I'll show you what the empirical results are.",
            "But answering these questions is very much the new machine learning a new linear algebra that I mentioned.",
            "I'll give you an example of some of those."
        ],
        [
            "Alright, so we're not naively going to compute a vector, but we're going to operational steps that should you get all the guarantees that global spectral gives you.",
            "So here's a typical example of would find.",
            "This plot will go down and down.",
            "You'll hit a minimum, and then I'll go up and up and up over those of magnitude.",
            "All these plots are log log.",
            "And so you know minima correspond to things you might possibly want to call clusters.",
            "This plot will go up and up and up and up.",
            "The X axis is the size of the cluster.",
            "Considering the Y axis is the Community quality, score, the conductance down is good.",
            "'cause of the surface area volume.",
            "So the best possible clusters get better and better until you hit 50 nodes or something and then they get worse and worse and worse."
        ],
        [
            "And again, the details are different, but you see that with live Journal with opinions I could show you 68 more plots.",
            "The different colors correspond to different.",
            "The different algorithm proceeds.",
            "I'm not going to get into that.",
            "Trust me that I'm not cheating here.",
            "You know the different algorithms behave in ways you'd expect those lower bounds, and so on and so forth.",
            "But the property is that it goes down and down.",
            "It's a minimum, goes up and up.",
            "So what's the what would be explaining this?",
            "Right?",
            "So what's the structure of the graph and maybe what's the simplest model?",
            "It would reproduce something like this.",
            "'cause it's clearly not low dimensional, right?",
            "The low dimensional if you low dimensional.",
            "Any meaningful sense of the word, you go down and down and down.",
            "And if you really had no good cuts, you'll be flat.",
            "So."
        ],
        [
            "Going on here is if we define some sort of notion of whisker, here's one way to define.",
            "You could define otherwise maximal sub graphs that are attached to a network by removing a single edge.",
            "You might say those guys are nuisances.",
            "You just cut him off.",
            "If you do that, you're going to be cutting off 40% of the nodes in 20% on the edges of some huge fraction.",
            "And if you believe that you want to push mass out on the heavy tail, those are the things that you do not want to be cutting off 'cause everyone knows what you are searching for.",
            "If you search on Britney Spears, would you want to find a web page or the Wikipedia page?",
            "But if you site if you search on something that's farther out on the tail?",
            "This is exactly what you don't want to be cutting off.",
            "If you cut all those off, you get qualitatively the same plot, but you don't want to pay off and the core is the rest of the graph in empirical factors at the global minimum is a whisker.",
            "So some huge fraction of the nodes think of as ballpark 100 edges are connected by one node.",
            "If you cut all those off, you have a bunch of other whiskers that are now connected by two edges, and if you cut all those up a bunch of other whiskers, so you have a core periphery, recursive core, periphery structure.",
            "And that's sort of what's going on here.",
            "Now you might ask what's this?"
        ],
        [
            "Placenta model that would reproduce this, so here's a theorem.",
            "Power law, random graph.",
            "Let W be a parallel degree distribution to some assumptions.",
            "Here, connect the nodes in the usual way theorem you will have logarithmically deep cuts of log size and if you get a factor of 10 more you will not have any cuts below the basal level.",
            "So everyone beats up on their train yay right?",
            "Air Terranea explain, explains the fact that you have deep cuts that small size skills and know deep cuts at large size skills.",
            "You can't be naive about this.",
            "Because.",
            "Let me back up on steps.",
            "This is what you get.",
            "The power law degree distribution.",
            "You might want to blame the power laws.",
            "It's not the power law that's causing the problem.",
            "There is rain gives you exactly the same thing.",
            "Now for any, there's a parameter P which has to do with how connected you are.",
            "If you choose P to be greater than log N / N log in, usually the scale at which measure concentrates to, say, log squared N / N. Then the graph is fully connected, all the degrees are at their pyrrhic values.",
            "Everything is pretty flat if P is between 1 / N an login over, and so say constant over N. Then the same theorem holds.",
            "So air this training is as simple as possible model that explains these qualitative results.",
            "So sparsity coupled with extreme randomness is the issue.",
            "So when I said you take this graph, you filter through spectral and flow spaces.",
            "And you're getting some implicit regularization or playing them off of each other.",
            "The reason that's particularly important is 1, because the graphs are large enough that we don't want to be putting out one or whatever regularization just tacked on, because we will be making the problem harder and intractable.",
            "Problem with understanding of so working with sort of scalable algorithms, but the data sparse enough and noisy enough that although you don't have stringy things with spectral fails, and although you're not a constant expanded workflow as problems, these graphs have expanded like parts and long stringy parts, and so on and so forth, so understanding the implicit regularization and a lot of those things."
        ],
        [
            "Is a particularly important.",
            "And I guess I mentioned their training, so you should really think of these datasets as local pockets of structure on global noise, not noise on global structure.",
            "So don't think of this as some global structure, whether it's a low rank space or manifold, you know whatever with some noise.",
            "I mean these things if you squint at them, they look like you're trying extremely sparse right now, so you'd have little pockets of structure.",
            "You have some bad variance properties.",
            "Clearly triangles closing quadrangles close, and you get clusters of.",
            "So all those plots I showed you, you are well below the random basal level.",
            "I didn't emphasize this, but."
        ],
        [
            "All these plots had this thing.",
            "That's the random basal level, so you're well below that.",
            "There's a lot of local structure, but it's local structure on top of a sparse quasi ran."
        ],
        [
            "Scaffolding, and when you're on top of a sparse quasirandom scaffolding, you do not get good."
        ],
        [
            "Bipartitions and a lot of other things that are."
        ],
        [
            "In the back of your mind, in terms of how."
        ],
        [
            "These sorts of methods might behave.",
            "So I didn't walk through all the details that prove this, but I hope you at least believe that it's plausible that these graphs look that way, and you might say you know is all hope lost.",
            "Or can we have sort of maybe some new machine learning a new linear algebra?",
            "So the answer is yes, I think so.",
            "I wanted to talk about three different examples.",
            "One is the way we actually computed those plots was to sprinkle a seed set of nodes at a bunch of different places in the graph.",
            "Fix a bunch of size scales, which is basically a number of steps to iterate.",
            "And and and run that process again and again.",
            "So sprinkle, see note here, here.",
            "Here bunch of size scales etc etc.",
            "Compute all those stuff, all those things and take sort of a lower bound so the primitive there is that we're starting with no defusing a bit what we're doing.",
            "Other things like a pay drink analogue of it, or heat kernel analog of it.",
            "So there's a couple of different ways you can implement that, but think of it is doing sort of local diffusion, which is very much of a flavor of a local spectral thing.",
            "So Spielman, Tang, and sung laying had some theorems justifying why this might be reasonable, but the theorems are incredibly complicated in terms of their quantification.",
            "I mean, the results are extremely.",
            "Difficult to state and we're working it for years and it's fairly hard to understand.",
            "You know you started this at a seed nodes and for every set of nodes, if you choose a constant fraction of the nodes and you get a cut such that for all cuts of a reasonable size bounded near you that are correlated in some way, and by then you probably don't remember the theorem started so intuitively.",
            "What are these things doing and what's going on with these things is you're doing a very operational thing that you're running a bunch of steps.",
            "It's not clear what you're optimizing.",
            "Are you even optimizing an objective function?",
            "I mean, you might be approximately optimizing it, but are you exactly optimizing some objective function?",
            "So I want to give you an optimization view of a local version of spectral partitioning.",
            "It's going to be first cousin of of what they did and what we actually implemented.",
            "And in a lot of these cases, we weren't doing the power method, but I'll describe it in terms of the power method, just 'cause it's a little bit simpler.",
            "We were doing page personalized page rank and so either he Colonel variants of this, but we had to do things where there's an iteration parameter like the power method an we didn't compute an exact eigenvector name.",
            "We didn't.",
            "We didn't want a million steps of the power method.",
            "Get the exact language and we ran for three steps or 10 steps or something that had to do with the size scale over which we did.",
            "The local local clustering.",
            "How can we understand, in some principled sense, what regularization is going on there?",
            "Are we exactly solving some optimization problem?",
            "We approximately solving some optimization problem and then let's get beyond sort of VC bounds.",
            "Can we use these ideas in bad environments?",
            "Because when you start here and you defuse a bit, it turns out you're computing a vector an eigenvector.",
            "It's an approximate eigenvector for certain modified problem and all the masses localized here.",
            "So it's not the high degree node of the small cluster thing that I mentioned before, but you're computing an eigenvector and approximately convective some modified graph.",
            "So you have localization properties that are bad, exactly as I mentioned before.",
            "Can you get beyond VC bounds?",
            "And in particular maybe use ideas having to do with this distribution to get better learning results."
        ],
        [
            "Let me mention these three results in a little bit of detail so.",
            "Lessons learned on the local and global clustering properties of these datasets.",
            "Oftentimes, you'll have good clusters near a particular set of nodes.",
            "The Vancouver flower market, but no meaningful global clusters.",
            "Can you meaningfully pull those out in some sense on approximate computation and implicit regularization?",
            "I mean, there's a universe of easy to state problems.",
            "L1 regular Sale, 2, exactly optimized conductance, whatever.",
            "There's a universe of easy to say algorithms.",
            "Three steps of the power method, whatever you know, you make some.",
            "Yeah, if you Ristic truncation, do these things overlap at all?",
            "If you're running these things with practitioners, actually do what do you actually optimizing, and can you cut and paste these things together?",
            "What a lot of people do is specify some objective function.",
            "A priority might even be nonconvex, but even if it's convex, it might be hard to work with.",
            "So what's the interplay here so that I'll give you an example and answer that in one very special case, and on learning and inference in high variability data in terms of this issue about eigenvector localization and VC bounds?",
            "So the."
        ],
        [
            "Example would be the following.",
            "So I said the local methods will be probably good versions of the global spectral.",
            "Spielman Tang Anderson changlang.",
            "There's a bunch of versions with local random walks.",
            "Some locally biased, a personalized version of page rank.",
            "An approximate heat kernel and the idea is you start here.",
            "You defuse a bit and you get some local cluster.",
            "The best partition of the road network in the US might be here, but around this city at a certain size scale, that's the best one.",
            "And although these graphs don't have a nice global embedding like the road map might, locally it pull something out that might be meaningful."
        ],
        [
            "So.",
            "Recall the basic spectral partitioning problem that I had before.",
            "Minimize X transpose LX that you're perpendicular to the all ones vector or so you have a variance constraint in perpendicular lines vector.",
            "This is a relaxation of this combinatorial thing that's intractable.",
            "You can solve this via an eigenvalue problem.",
            "And you get a vector and a sweep cut of that eigenvector yields this thing that you know you bound below and above with the quadratic factor, so this is Cheeger and you don't need an exact eigenvector.",
            "Any Mahal has a variant where you can sweep cut any vector whose Rayleigh quotient is about right, so you never need an exact eigenvector.",
            "All you need is one that's approximate in the sense of the Rayleigh quotient being about right."
        ],
        [
            "So what we want to do is come up with an ansatz, basically for an optimization problem that these things might be solving.",
            "So, given a cut, let's define this vector.",
            "So this isn't exactly an indicator vector S for the for the set T, but think of it, it's sort of like an indicator vector.",
            "It's something like that projected away from the all ones vector.",
            "So SMT is a vector that is something like an indicator vector for that cut T and we can use this vector to define what I'll call a geometric notion of correlation between cuts.",
            "So this vector dot into the lens vector I said was zero.",
            "Which is, why isn't isn't exactly indicator vector.",
            "I've normalized and so the dot product into itself is 1.",
            "An SU dot into St is some correlation thing that if I wrote down would look to you like a correlation condition.",
            "And so given the way page rank is usually told, you have some guy diffusing around and you teleport and so on and so forth.",
            "But effectively it's going to have a structure like this.",
            "So let's say given a graph G, Anna, number Alpha and any vector assets that the seed set.",
            "Basically it's a function on the nodes.",
            "It doesn't need to be localized, will be using it, but it's a function on the nodes.",
            "Let's define a generalized personalized Pagerank vector to be vector of the following form Laplacian of your original graph minus Laplacian.",
            "The complete graph pseudo inverted dot into D. Dundas, so the usual story about the random Walker, the teleports around, and that's what this thing is.",
            "It may not be obvious, but that's what this thing is, and I'm calling it generalized because we can now allow negative values of the teleportation.",
            "I guess positive values of Alpha the way I've written it, so it's more general than the usual notion, but it's a straightforward extension.",
            "Alright.",
            "So we have for a set of nodes you define the geometric notion of coral."
        ],
        [
            "Ocean.",
            "And here's an ansatz for local spectral partitioning.",
            "Minimize X transpose LX such that the variance condition satisfied it didn't write down your also perpendicular to the ones vector and that X dot into S is greater than Kappa.",
            "So the DOT product between X, the vector you're looking for and the seed set of nodes is pretty high, so we're minimizing the same thing, except that we need to be well correlated the seed set.",
            "So there's a dual.",
            "The dual usually is Max Alpha such that LG you can sort of sit on top of the complete graph.",
            "Now you get another term that has to do with the complete graphs on the T and the T bar, and you don't want to cut through TNT bar, which is depending on how big data is.",
            "That's going to be more or less important, so the interpretation is here that you're going to find a cut that's well correlated.",
            "The seed vector S, and in particular.",
            "If S is a single node, you relaxing this quantity.",
            "Alright, so in the same way as you sort of embed the graph in a complete graph, that's what's going on under the hood when you do global spectral.",
            "This is what's going on here.",
            "When you're doing local spectral trade."
        ],
        [
            "Of these two things here, so theorem, if X is an optimal solution to this program.",
            "Then it's a generalized personalized Pagerank vector for the parameter Alpha and it can be computed as a set of linear equations.",
            "So I've written down an ugly program, but the solution you can solve a set of linear equations since it's a GPR vector.",
            "The proof is relatively straightforward.",
            "You relax a nonconvex problem to convex SDP.",
            "Strong duality holds here.",
            "The solution is going to be rank one from complementary slackness, and if you have a rank one matrix, that's a vector basically, and so that's that vector.",
            "So you can compute the solution."
        ],
        [
            "Quickly and in addition, you get upper and lower bounds that are cheaper like so I'm not going to go through the details of that, but you get up on lower bounds that are Chiga like.",
            "So this is the flavor of the results that Spielman, Tang and Chong Ling get, and so on so forth.",
            "This vector is still global in the sense that it touches all the nodes, the operational steps they have, sort of a hard truncation.",
            "That will introduce sort of an additional.",
            "An additional form of 1 type regularization that I didn't get into 'cause we don't have formalized, but the idea here is that we have an optimization form that these local spectral things are."
        ],
        [
            "Optimizing.",
            "So I told you we did all this stuff.",
            "A million node graphs.",
            "Here's a 300 or 600 node graph where I want to show a pretty picture just to sort of give you show you roughly how it behaves if we start with seed nodes here and run it a little bit depending on where you want to cut, you can cut the red.",
            "You can cut the orange, you can cut the ground.",
            "You can cut the yellow.",
            "And if you start with the set of nodes out here, you can cut in the same way at a range of different levels.",
            "And we saw the profile plots before.",
            "This is a local version of the profile plots, and although usually you'd.",
            "I think of the seed vectors is localized in a small number of nodes."
        ],
        [
            "It can be any function on the nodes, so in particular it could be a function that's correlated with high degree nodes or correlated with low degree nodes, and so if you think that you want to find a soft version of of a cut that's correlated with high degree nodes or anti Corley Corley low degree nodes, you enter into that into your vector S you run that machinery and you get something like this that you can cut at the red or the Brown or the yellow."
        ],
        [
            "Alright, so this is what these local spectral methods are doing.",
            "They're optimizing the modified objective like that form.",
            "But but as I said, you know the way I formulated that I'm computing an exact eigenvector with those methods are doing is computing approximate eigenvector 'cause they run a few steps of the page rank or a few steps of the heat kernel?",
            "They don't want it forever.",
            "And so in a lot of situations in linear algebra, machine learning, and so on, these will involve approximate computations.",
            "Power method truncate apartment.",
            "The heat kernel truncated version truncated page rank diffusion kernels, interest rate concern, and so forth, and often they'll come with some sort of generative story about random surfers and so on, but you know you might ask what are these procedures actually computing 'cause if you want to start cutting and pasting the spectral and flow things to probe these large graphs in a more principled way, or absolutely more refined, the 10,000 foot view that I just mentioned answering these questions that you'll need to be able to do, and that's not something that's historically been done in.",
            "Linear algebra, other area."
        ],
        [
            "So it's involved.",
            "There's nothing new here.",
            "I mean, it's also the classical linear algebra optimization ideas, but you sort of reformulating.",
            "I'm in this sort of new directions to ask based on these sorts of applications, so regularization is a general method for computing a smoother or nicer and more regular solution, and it's useful for inference in a range of other things.",
            "And regularization is usually implemented by adding some sort of regularization penalty.",
            "So if I want to optimize F of X, you decide a priority on some G of X.",
            "So that could be a Euclidean norm regression and you put an L1L2 or something there.",
            "So you minimize F of X plus Lambda G of X.",
            "An empirical observation is lots of times heuristics like binning or early stopping.",
            "You know on this, or conjugate gradients or variants of this.",
            "You often do better in practice, where better means more useful, smoother, nicer.",
            "For some downstream analyst.",
            "So the question is can we can approximate computations implicitly lead to more regular solutions?",
            "Because I told you we're starting with tractable graph partitioning problem, we're filtering through two different places depending on whether we're doing spectral or flow.",
            "Can we understand the extent to which the nature of the approximate computation itself is implicitly regularising?",
            "So we have a lot of evidence how this can be done for graph algorithms, but I'm going to talk about one particular clean example of the matrix algorithm."
        ],
        [
            "That's computing an eigenvector, so three common procedures are heat kernel, which you can formally write this way.",
            "And paid rank which you can formally right this way and a Q step lazy random walk which you can formally right this way.",
            "So if you stop after three steps.",
            "You aren't exactly computing the vector, but can you make?",
            "Can you say in a very precise sense that this approximation procedure?",
            "This truncated thing exactly optimizes some regularised objective, so the Rayleigh quotient is what you get when you run this thing forever.",
            "Are you implicitly optimizing some regularised objective here?"
        ],
        [
            "So this is a vector program we saw before and a regularised version of this vector program would be this thing.",
            "Minimize X, transpose X plus Lambda F of X. Alright, this vector program, it turns out, is equivalent to an SDP semidefinite program.",
            "And this semidefinite program is of the form minimize L dotted into X when X is now capital X. X is an S PSD matrix such that the Laplacian Dot in the Capital X is one that generalizes this thing.",
            "You don't need to worry about the rank.",
            "There will be a rank one solution to this and that rank.",
            "One solution is the same as that, so the rank one solution to this.",
            "And that rank one solution to Capital X is lower case X, lower case X transpose in that lower case X is that.",
            "And you can regularize this and tack on some Lambda capital F of X.",
            "Now clearly when you regularize, you might not be rank one, but you can do this.",
            "So it might be believable that if you've done three steps of the power method, you haven't washed out the rest of the residual space, so it might be hard to write it or not possible to write it.",
            "The regularization in this way as a function of that vector.",
            "But here we have a lot of other information.",
            "He would have the correlation matrix and we haven't washed out the full residual space."
        ],
        [
            "So here's a here's a theorem.",
            "That let's define the F, ADA semidefinite program.",
            "To be basically the same thing we had before, minimize L dot X such that you know I got XI is the same as that Laplacian thing I had on the previous page.",
            "In X is greater than zero, but we tack on ADA 1 / 8 F of X. Alright, so modification of the usual SDP to have regularization.",
            "But on the correlation matrix X, not on the vector X. Theorem If G is the.",
            "A graph with Losian al.",
            "Then these following conditions are sufficient.",
            "For ex.",
            "Start to be an optimal solution this regularizer program.",
            "And the conditions are the text art takes this particular form and that you have these two things.",
            "Alright, once you have this result.",
            "We have Dell F inverse."
        ],
        [
            "Then it's a relatively immediate corollary that if you happen to choose F of X to be a generalized entropy, then you get the scaled heat kernel with T equals ETA.",
            "If you happen to choose F of X equals logdet, this gives you the page rank.",
            "And if you choose F of X to be certain matrix P norm, you get this truncated iterated lazy random walk.",
            "Alright, now you can solve the SDP for any value of the regularization parameter you like by calling a black box solver, or you can call it for a particular set of discrete values that happens to correspond to one Step 2 step, three steps, and if you choose to solve it for those particular set of values, you can solve the SDP by doing 3 steps of the power method or of a truncated lazy random walk, or or of a approximate personalized rate page rank.",
            "So these approximation algorithms that we were using to tear these graphs apart and probe their structure, these approximation algorithms are basically computing regularised versions of the Fiedler vector.",
            "And doing so very quickly so we haven't act on any regularization terms, but by the steps of the approximation algorithms we."
        ],
        [
            "Implicitly implementing that regularization so that holds true.",
            "More generally, I think for graph approximations we haven't formalized that, so that's actually working on and you should know that a lot of work on large scale data applications sort of already uses this stuff implicitly, so there's a range of papers.",
            "Here is just three.",
            "That sort of informed our understanding.",
            "We're thinking about this, but there's a bunch of ones where you know you do random walks on, say, a query quick graph in order to generate some keywords, and you need to be pretty careful about the seed set and the size scale of which you operate, and so on.",
            "So these methods are a little bit finicky because they're basically doing a spectral thing.",
            "They're doing a diffusion because they're doing a random walk.",
            "And if you start to touch high degree nodes, you're dead.",
            "Right, so you might ask, could I do a local diffusion and then couple that with local spectral?",
            "A local sort of flow sort of thing to make these a little bit more robust?",
            "So that's something we don't have a final answer too, but the answer is probably yes, and we're thinking about that.",
            "So more generally, can you formalize when these sorts of heuristics that people actually do work or fail for matrix a graph approximation albums if you want to start cutting and pasting these things together?",
            "One way is to specify an objective and then trying to approximate it a different way to say what are people actually doing on very large scale graphs and understand the implicit if."
        ],
        [
            "Acts of what they're doing.",
            "So let me wrap up with the third example having to do with the classification bounds so supervised binary classification you observe XY samples from some unknown distribution you want to construct a classifier Alpha drawn from some family Lambda.",
            "So for example hyperplanes after you see some number K samples question, how big must K be in order to get good prediction in order to get low error.",
            "So you define the risk.",
            "You don't know what that is.",
            "You define an empirical risk, which is the risk on the observed data, and we want to bound these two.",
            "And so one way to do it is to bound the VC dimension.",
            "And that's really the most well known.",
            "And it's nice 'cause its distribution independent a second way and not the only other way.",
            "But Secondly, it's bound was known as the annealed entropy.",
            "So this will depend on the distribution of the data is drawn from, so it's less general in that sense.",
            "But in a lot of cases you get much finer balance so."
        ],
        [
            "Sample complexity of distribution free learner depends on the ambient dimension variance.",
            "Satisfactory for formerly high dimensional data, and so on and so forth.",
            "The annealed entropy is this sort of expression.",
            "I'm just I'm not going to explain, I'm just going to write it down to sort of show you that make you believe that it exists, hopefully, but the point is, you have an expectation of the distribution.",
            "There is not just accounting thing, but you have an expectation of the distribution here.",
            "And the theorem is that you know the risk minus empirical risk you can bound in terms of annealed entropy.",
            "So if you're familiar with the bounds, the structure of that claim is very similar to what you see."
        ],
        [
            "BC bounds and that will hold for some number of samples.",
            "Allen any error parameter epsilon so distribution independent sample complexity bounds for high variability environment and what I'll say is approximately low dimensional environments, meaning you're not exactly on a low dimensional space, but you have noise which you shouldn't even think of his Gaussian.",
            "It might be very, very bad, so think of you doing LSI, so you want there to be 10 dimensions.",
            "But there's really heavy tailed noise, very bad variability, data, high variability environments, more generally.",
            "So I'm going to say Tord learning on informatics graphs 'cause we really want to do here, is think about the graph as a single.",
            "Piece of data.",
            "You know this is not these aren't feature vectors.",
            "We can say it's feature vectors and every you know every node links to every other node.",
            "To find that to be a feature vector.",
            "But in a lot of ways more meaningful to think about the graph as a single entity operate on it.",
            "So a lot of this learning is formulated in vector spaces, and so you know you're not directly on graphs, and the particular statement of the results we have here wouldn't be immediately available for the graphs, but they're in vector space analogs of that, so it'll be tored learning information for mixcraft, so we'd like to do it directly on a graph.",
            "High variability environments.",
            "Let's say the probability that the feature is non zero decays as a power law.",
            "Or the magnitude of the feature decays.",
            "A power law, and so we can slightly different results.",
            "Exact learning or gap calling depending on how exactly quantify it.",
            "Approximately low dimensional environments William Bounds and say the covering number when using diffusion based kernels and if using diffusion based kernels.",
            "The idea is that if you really know dimensional space, eigen values are going to be vectors are going to be pretty localized and so good things might happen.",
            "But if you're doing it on arbitrary graph you might have these hot spots and the hot spots are going to be problematic for you."
        ],
        [
            "So.",
            "Are you the diffusion Maps is that hopefully is Laplacian Eigen Maps is is a variant of it and you might ask when do eigenvectors localized.",
            "So as I said, the locals and high degree nodes at articulation points between clusters points that stick out a lot very sparse random graphs.",
            "So this is seen a lot of datasets when eigenvector based methods are chosen for algorithmica not particularly statistical reasons right?",
            "Use LSI, not because it's really K components, but because the low dimensional space might be implicitly providing some sort of capacity control.",
            "So you actually see this sort of localization."
        ],
        [
            "So this localization, just as an aside, this localization I call them leverage scores before.",
            "But if you're familiar with stuff on matrix reconstruction and all this sort of stuff, they talk about a notion of coherence and incoherence.",
            "Coherence and incoherence is sort of an Infinity norm over these leverage scores.",
            "If the Max of these scores is high, things are very incoherent.",
            "If the Max of these things is flat, things are coherent, or maybe vice versa when the leverage scores have bad are very when you have bad variability over the leverage scores, things are coherent, which is a bad case for them, and in a wide, wide range of datasets, that's going to be the case, and so the approximation that the data are reasonably coherent is not even approximately satisfied."
        ],
        [
            "You know, if you have some idea of what the distribution might look like, can you get learning bounds?",
            "So let P be some sort of probability distribution over RN, and suppose the probability that the value is non 0.",
            "So the one of the two models I mentioned before decays a power law.",
            "So think of this as the degree of localization on the eigenvector.",
            "You there's a small number of components that have a lot of mass and a bunch of components that have less a bunch more than just a lesson.",
            "A bunch that have very very, very little or none.",
            "So theorem.",
            "In this model you can bound the annealed entropy, and I'm not going to go into details except to say that you can do it, and so you'll need L, which is some ugly expression that is independent of D, which could be infinite.",
            "So the sample complexity finite, even if D is going to be infinite.",
            "So you can get a theoretical result.",
            "I'm not going to say that this."
        ],
        [
            "Immediately applicable social graphs, but you can get that sort of bound more generally with gap tolerant classifiers.",
            "If you say P is a probability measure in Hilbert space and Delta is greater than zero, let the expectation over P under P of the norm of XBR R-squared and let that be less than Infinity.",
            "Then the annealed entropy of the gap calling classifiers is bounded by this and all the learning results go through.",
            "Now I want to say the expectation here rather than, say the maximum, and this is the sort of thing that you probably see in a lot of cases people sweep under the rug.",
            "They say I'm going to have a technical assumption, and the technical assumption is that I'm going to bounce some Max.",
            "You can always bound to Max.",
            "But then if the main gets arbitrarily small, you'd have a problem, and it's a technical result in the sense that if you just familiar with manifold and so on, you know it's a technical result.",
            "But if you're dealing with arbitrarily bad graphs, it's far from a technical result.",
            "It will render a lot of the results that gloss over this and bound the Max irrelevant for bad very bad variability environments."
        ],
        [
            "So.",
            "This application for learning with spectral in diffusion based kernels, learning with heavy tailed data.",
            "As I mentioned you have a new proof.",
            "It actually generalizes to Banach spaces if you're interested in that which wasn't known before.",
            "But more generally, you know the question is and we don't have answers to these questions.",
            "Can you control the negative effect of outliers in other data models, or can you do?",
            "Learning in some sense, if the measure never concentrates the point of this very sparse regime of various Ronnie, is that you're not in some low dimension, plus noise, and you're not in high dimensions or measure concentrates.",
            "You're exactly at that.",
            "Pre asymptotic state and that creates some topics that you have very bad variability properties and you see that in a wide range of datasets in 99 plus percent of statistical theory ignores that, and so that's sort of the dominant effect here, that's."
        ],
        [
            "Sponsible for the local global issues, so that let me wrap up large informatics graphs you may imagine and believe and not be surprised their important practice there.",
            "Also, I think important in theory because they starkly illustrate that many common assumptions that people bring to the analysis are very inappropriate, or they're actually very good.",
            "Hydrogen Atom for I think, method development.",
            "You should think of the data as little local pockets of structure on top of a sparse quasirandom global scaffolding.",
            "So empirical noise.",
            "Let's call it, and I think that's very significant implications for clustering community detection.",
            "And the use of a lot of machine learning and data analysis tools more generally.",
            "And so I gave you a couple of particular examples and I could talk offline about a bunch of others, but these are examples that we sort of work through in detail, so I think there's a lot of other examples where sort of if you look under the hood in terms of what is done numerically, eigenvector localization or other sort of very basic ideas that people usually assume away.",
            "If you look under the hood, there's a lot of new directions for.",
            "Numerical and matrix based methods and machine learning so that let me wrap up and thank you.",
            "Questions.",
            "Very simple.",
            "Adam Walker where do you start?",
            "If you start to me.",
            "Right, so where do you start the random walk when you start a random walk?",
            "For those, it depends what you want to do.",
            "If you want to find the Vancouver flower market, you should start with seed nodes in Vancouver, not in Zimbabwe, right?",
            "If you want to compute the plots we computed, you start with seed nodes at all sorts of different places, and you look at a bunch of different size skills, and so if you're going to look at it at a small size scale, then do 2 steps or three steps, you might say that Vancouver and Seattle markets are different.",
            "If you're going to look at 10 size scales, then you might say, well, they never mind.",
            "This is an underlying geometry to this to the Earth, but you'd see similar things as you get more expanded like that if you start here and here and you defuse around a bit on small size scales will be different at larger size scales you might believe they're going to be about the same, and so you could just start with one of 'em.",
            "And so when we computed that particular plot, we started with a bunch of different places, bunch of different size scales, trying not to naively have sort of gratuitous computation there.",
            "And then took the minimum.",
            "But if you want to ask more fine questions like what's going on around here, also other things.",
            "You could do that too, so that really depends on the question you want to ask.",
            "Yeah, for the local spectral partitioning, so you are really solve the problem by the SDP or using some other powermaster or type skill.",
            "I mean really screw up.",
            "So what we do is put mass here.",
            "We actually used to pay drank version because it's certain better results, but I'll describe the diffusion version because we've tried that to put your mess here.",
            "Diffusa step diffuse 2 steps if the probability is less than something, truncate, diffuse another step and iterate that process a few times.",
            "That's what you actually do.",
            "Approximation so yeah, so we never touch an SDP.",
            "Those SDP's are what we actually did.",
            "If you want to know, sort of what we were optimizing that.",
            "So we were optimizing, but we optimized it by doing these few steps.",
            "So that's the key point that this is a massive graph.",
            "This SDP is going to be hopeless.",
            "Yeah, cancel that SDP in a million node graph for 10 million node graph.",
            "But you know this is effectively what you're optimizing when you run these operational procedures very locally.",
            "Then can we say how far you are from this open optimum of this SDP?",
            "After these three steps?",
            "So there's a couple.",
            "I mean, there's the truncation issue, which I did in the direction that regularization, but it can be, but we haven't done it in this cleaner format, but that's what's going on there.",
            "There's a particular way stated it was in terms of a global thing, but it will extend the local.",
            "So modulo those two, that SDP is exactly what was going on with the methods.",
            "Well, also I had another comment, maybe I misunderstood, you know, suppose you run just three iterations of some method.",
            "Yeah, whatever you get after three iterations could be the solution to potentially an infinite number of different problems.",
            "Why do I look at the particular ones that say, yeah, I mean, it could be a solution.",
            "I mean the solution to spectral.",
            "You can write as a vector program, but you can also write it as this SDP programs.",
            "I mean, there's probably other ones, yeah, so but when you do three steps of spectral, you're solving the STB for a particular value of the regularization parameter an.",
            "If you do 4 steps.",
            "You're solving it for a different value, and if you do 5 steps you solving for different value.",
            "So that's my question comes back to, you know, because you can have potentially an infinite number of optimization problems.",
            "Let us solve by these three steps what inside you know when I do three steps, I solve a particular when I do three steps.",
            "I solve a particular SDP for."
        ],
        [
            "Ocular value of the of the regularization parameter.",
            "The simplest one is heat kernel, where there's inequality, the other, the other two are little bit more complicated, but you know when I do three steps of this heat kernel thing.",
            "You know it's that's the value of the regularization parameter.",
            "If I chose to do 4 steps.",
            "That will be the value of the regularization parameter, so it's not like there's a bunch of I'm not solving a bunch of different things.",
            "I'm selling this particular STP.",
            "I could have chosen a regularization parameter in between those two and called an SDP solver.",
            "That would be a hard thing to do.",
            "So the question is, what is the what is the problem that I'm actually solving because we told these graphs apart and a lot of intelligent heuristic things.",
            "If you want to ask more refined questions about these graphs, like doing the keyword expansion in the diffusion, you understanding what you're actually doing when you introduce these structured approximations is going to be important.",
            "If you want to sort of.",
            "Richer analytics on large scale graphs.",
            "So the question was what are we solving when we do certain things were not saying I want to give some complicated objective and hope that I can solve that.",
            "The question is I have a few sort of primitives that I know how to solve and I cut and paste them in various ways and what I'm actually solving when I do that.",
            "Even better, I mean, if you thought about there are many optimization methods today, whatever they call.",
            "Problems that we can't really solve in every step in every step, right?",
            "Every step and SVD.",
            "Yeah, well if I only run many steps of my power method, I can tell you what you're really doing.",
            "You then take that together and tell me what's the iterative method around it really doing?",
            "I mean, for example, doesn't.",
            "Yeah.",
            "Can't tell you yet.",
            "So the question is if I paste things together and this is one step of a bigger machine where I'm doing iterations on, can I tell you what that bigger machinery is solving?",
            "I suspect the answer is yes, and we're thinking about that, but I don't have the answer yet.",
            "I mean, if you run something and you sell is convex, and then you do something in between, it might just destroy convexity.",
            "Yeah it could, yeah.",
            "Safeway only running steps.",
            "Possibly, but usually when you do these things here, you're smoothing something out your regularising, so I would guess that I don't know.",
            "I would guess that things wouldn't go pathologically bad like that, but.",
            "I'm sure you can find an example where they do, but in a lot of cases they wouldn't, so I don't think it's so much that you're going to preserve convexity as to scale up things that you would be too small to do otherwise.",
            "These results dependent on what the matrix is very strongly or no.",
            "I mean, the matrix is is.",
            "This is the problem solving.",
            "So the matrix enters there.",
            "Oh so the results are pending on the Matrix, except that enters into the the penalty, but the regularization term it doesn't depend on that.",
            "Now if the matrix is more or less than when you're doing the page rank, you diffusing around your hopping.",
            "If it's a global page rank of its local when you're doing the same thing to a local spot, you're hopping anywhere.",
            "So you might imagine you're doing a very global sort of regularization, so the logged it should have a different structure than the generalized entropy, or this matrix P norm.",
            "Because with these two things are doing is a very local thing.",
            "You know just diffusing around and maybe you're truncating a little bit, whereas the page rank because of the hop you doing much more global thing, so that particular particular regularization function will illustrate that difference.",
            "Metaphor?",
            "Basically a few steps.",
            "Yeah, so historically the there was a couple different variants of what people did.",
            "They are pretty close and the page rank actually forms better on the large graphs because of the global.",
            "Because of the range of reasons, but the was historically done was well, I don't have it here, but.",
            "You know this this Spielman Tang the Andersen Chung Chung.",
            "We're doing 3 versions of basically these three.",
            "So we just wanted we could we could.",
            "We could derive this result for all three and not just too.",
            "So we did it for all three, but but these two are going to be very similar because it has to do with the formula has to do with the way you know you arrange the some of the terms.",
            "You know how they decay implementation alee.",
            "This performs worse than that.",
            "We've never implemented that advice.",
            "We've implemented that in that.",
            "Not that, but yeah, that probably is a sort of very similar to that, so I wouldn't want to.",
            "I mean, I don't know a practical example of that.",
            "Matters, I mean they might.",
            "I'm sure you could construct one, but certainly in the situations we've looked at, those tool gives similar results, I suspect, so the point isn't that I want it.",
            "I mean, they're not very different things, it's just that.",
            "You know that you're doing slightly different things and you can Venn diagrams of easy to state problems and easy to state algorithms.",
            "Three steps of any of these you know overlaps here in at least one case, and we can make it precise statement about the optimization problem.",
            "These particular procedures are optimizing, but yeah.",
            "All messages for me.",
            "I mean, I have to run it basically until roughly process mixing and then I can converse.",
            "But if you did three steps, what would you be doing?",
            "Well basically report iterative.",
            "OK then I'll call it that too.",
            "If you like.",
            "I mean that's not a debate.",
            "I mean that's not.",
            "Yeah, I mean call those two types of random walks.",
            "I mean I don't, that's you can call that yeah, so just coming back to my point nonuniqueness, if you go back one slide.",
            "For example.",
            "That you write.",
            "I can.",
            "Because it's on the form, you know something plus something.",
            "So the reason it's not unique.",
            "I can add something to the first term and subtract it up in the second term I have.",
            "Different problem.",
            "OK, so there may be that sort of non uniqueness, yeah?",
            "Sort of a gauge issue or something?",
            "Yeah, that may be the case.",
            "So let this or any of it's just, but I think we can unify some.",
            "Who is local?",
            "Please run as a local version of the first eigenvalue or there are also local versions of the second and third.",
            "OK, so don't think of it as a local version of the first eigenvalue.",
            "'cause everyone talks about local eigenvalues and eigen vectors.",
            "Eigenvectors sometimes localized, but sometimes they're not, and if they're not, there is not.",
            "Roughly what you should think of of this is I have my graph G. You could probably make this precise.",
            "We haven't, but roughly I have my graph G. And that has whatever eigenvalues it has.",
            "I can replace G with G prime where G prime is now G+.",
            "Something having to do with the rank one teleportation jumps.",
            "So G prime is a different adjacency matrix.",
            "G as a Laplacian G prime is Laplacian.",
            "Geez, Laplacian doesn't need to be localized 'cause I can get local spectral methods on that Rd network of the US, right?",
            "But if I paid if I do the teleportation jump jump back and that teleportation jumps back to a small set of localized nodes now have G prime so these things are approximations to the eigenvectors of G prime, not G, 'cause I could localize I could jump back to Florida, I could jump back to me, I can jump back to Vancouver, I can jump back anywhere.",
            "So these things you should think of as approximate eigenvectors of this modified graph.",
            "Correct, yeah, yeah, you might be able to extend it, but it gets.",
            "It gets trickier as you March farther out on the spectrum, so this is just about the 1st.",
            "Alright."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'd like to tell you about.",
                    "label": 0
                },
                {
                    "sent": "A couple particular directions that might be of interest as you sort of looking forward.",
                    "label": 0
                },
                {
                    "sent": "For matrix methods in machine learning and related areas, and I think one of the problems with developing sort of qualitatively new methods is that you know it's always hard because there's a large body of infrastructure competing against an in in some application area domain, and so it's good to have sort of data that's very new and messy where you can say something about.",
                    "label": 0
                },
                {
                    "sent": "So to get a toehold to work through the method and then of course it's going to be applicable more generally, so you can imagine there's a range of reasons people are interested in in large social and information graphs.",
                    "label": 0
                },
                {
                    "sent": "Maybe not.",
                    "label": 0
                },
                {
                    "sent": "The most common reason, but certainly think one legitimate reason is that so large social and information graphs are sort of bad in just about every way you can imagine that the data might be bad, and then some.",
                    "label": 0
                },
                {
                    "sent": "So any idea that you might have about the way the data is nicer, smoother, regular or low rank or high rank or anything else probably isn't satisfied.",
                    "label": 0
                },
                {
                    "sent": "And so from that perspective, there are very very nice test case for the development of algorithmic methods, and in particular because a lot of matrix and sort of spectral based techniques have some sort of robustness underlying them and these graphs you if you have a million nodes in 10,000,000 edges are pretty sparse, sort of implicitly getting some smoothing because of those techniques is very useful, so we've all sort of self selected to be at a workshop called numerical mathematics in machine learning, so I'll describe it a little bit more from the technical perspective.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the particular things we talk about will also have.",
                    "label": 0
                },
                {
                    "sent": "Some downstream applications at a .2, so I thought what I might do is give a little bit of history of linear algebra machine learning.",
                    "label": 1
                },
                {
                    "sent": "It will be biased and sort of everything that I say will be oversimplified a little bit, but I want to set the stage for describing a range of types of methods that people do, and hopefully you know all of those will make a point of contact and you'll certainly ahead and say, yeah, I can see how that would be something people do.",
                    "label": 0
                },
                {
                    "sent": "And then I'd like to describe some properties of very wide class of large social and information graph so this line of work actually.",
                    "label": 0
                },
                {
                    "sent": "Started while I was at at Yahoo.",
                    "label": 0
                },
                {
                    "sent": "I was the resident matrix expert 'cause I had been doing a lot of matrix sampling, random sampling, random projection algorithms for range of large datasets and so two different business units came to us with problems that at the end of the boil down to finding good clusters.",
                    "label": 0
                },
                {
                    "sent": "So one wanted to find markets economic.",
                    "label": 0
                },
                {
                    "sent": "These are advertiser bid phrase graphs.",
                    "label": 0
                },
                {
                    "sent": "So sets of advertiser phrases that you could reasonably think about as markets.",
                    "label": 0
                },
                {
                    "sent": "In the sense of Vancouver flower market or something to pull out to do bucket testing and the other wanted to get clusters of words to do something called query expansion for advanced Match where you expand queries to nearby queries.",
                    "label": 0
                },
                {
                    "sent": "So at the end of both those boiled down to clustering problems there getting some very strange results that didn't make so much sense, and so they came to us and they were right.",
                    "label": 0
                },
                {
                    "sent": "They were getting a lot of results that didn't make so much sense, and so we use a range of matrix methods and manifold methods that are popular machine learning.",
                    "label": 0
                },
                {
                    "sent": "That sort of are basically matrix methods, but a bit more descriptively flexibel a range of other techniques, and the particular structural properties that we saw were sufficiently bizarre and counterintuitive that I couldn't believe that they were peculiar to the two advertiser bid phrase graph.",
                    "label": 0
                },
                {
                    "sent": "So we went back and looked at on the order of 70.",
                    "label": 0
                },
                {
                    "sent": "7 zero large social information graphs and there was a couple key structural properties that were fairly ubiquitous but very, very different in than wide wide range of datasets that you'd see, and sufficiently subtle counterintuitive that a lot of the tools you would do would would just not be particularly relevant.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you want to do a recursive partitioning, you split the graph in half, split into two, and recurse.",
                    "label": 0
                },
                {
                    "sent": "If you happen to be nibbling off 100 nodes out of a million, you recursion depth is going to be pretty big, as opposed to half million half million.",
                    "label": 0
                },
                {
                    "sent": "So building off 100 nodes out of a million that's not so good, and so there's a range of tools, and I'll describe the structural properties of the range of tools that would have implications like that, so the tools would just be an interesting or not particularly useful, and there's a few particular properties here, and basically what they have to do is that in a lot of cases there's good local clusters, so if you're familiar with all the stuff on locally linear methods in machine learning.",
                    "label": 1
                },
                {
                    "sent": "In these graphs, there's lots of spots that look vaguely locally linear, whatever that means.",
                    "label": 0
                },
                {
                    "sent": "You know if nothing else you can put them on a piece of paper and look at them, which means that there's sort of a 2 dimensional embedding, and think about them or something.",
                    "label": 0
                },
                {
                    "sent": "What a lot of the manifold stuff does from there is, say, OK good, now I see these things.",
                    "label": 0
                },
                {
                    "sent": "I'm going to posit some objective.",
                    "label": 0
                },
                {
                    "sent": "Try and paste these together into something global and under extremely strong assumptions that works, but if the data is at all noisy, robust it you know it often times doesn't know what's going on.",
                    "label": 0
                },
                {
                    "sent": "All of these graphs is you have these local clusters that are something meaningful, but at larger size scales there isn't any particularly good embedding in a very precise sense of the words.",
                    "label": 0
                },
                {
                    "sent": "If you don't, expander is, these graphs are expanders at large size skills.",
                    "label": 0
                },
                {
                    "sent": "They do not have good cuts.",
                    "label": 0
                },
                {
                    "sent": "There's degree heterogeneity, which adds an extra wrinkle, which means that all is not quite hopeless.",
                    "label": 0
                },
                {
                    "sent": "But to a first approximation there pretty bad at large scale, so you're not going to have good cuts.",
                    "label": 0
                },
                {
                    "sent": "So what you need to do is, since low rank spaces are particularly nice in certain ways, the question is can you take a lot of the spectral machinery?",
                    "label": 0
                },
                {
                    "sent": "That sort of implicitly uses low rank spaces and maybe get a local version local because you want to get the Vancouver flower market as opposed to something at the size scale of the world in a lot of these times, you make various sort of design decisions, we truncate something, or you do early stopping or range of these things.",
                    "label": 0
                },
                {
                    "sent": "Can you and any other operational things that practitioners do?",
                    "label": 0
                },
                {
                    "sent": "Can you understand in a more principled way with the implicit regularization in these or other approximation algorithms is?",
                    "label": 0
                },
                {
                    "sent": "And you might imagine that a lot of the VC sorts of bounds if these graphs of this sort of bad simply don't extend, and they don't, and you might say, can I use other sorts of things like annealed entropy or so on to get some sort of capacity control for learning?",
                    "label": 0
                },
                {
                    "sent": "So I'll describe the bad properties here just to give you an example, I could spend a whole hour on this, but I'd like to just give you the bottom line and then maybe give you 3 examples of new machine learning or new linear algebra.",
                    "label": 0
                },
                {
                    "sent": "That will be motivated by this, and I'd be willing to bet something pretty close.",
                    "label": 0
                },
                {
                    "sent": "My bottom dollar that you know if you workout the methods in something like here that these sorts of methods will be applicable in a range of other domains, because what happens in a range of other domains as people will cook the data to make global spectral partitioning or whatever applicable.",
                    "label": 0
                },
                {
                    "sent": "And so if you have better methods developed in a class of datasets that are particularly bad, they'll be able to apply back more generally, and I can talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit more offline if anyone interested.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Numerical linear algebra in the pre history you know way back when Eckart Young was not published in the Journal of Linear Algebra, right?",
                    "label": 0
                },
                {
                    "sent": "It didn't exist at the time was probably psychometric our biometric us so close connections to data analysis and noise and statistics.",
                    "label": 1
                },
                {
                    "sent": "I mean it was very much in the air.",
                    "label": 0
                },
                {
                    "sent": "In 1950 or so, you got the computers on the machine and its cultural Brandon.",
                    "label": 0
                },
                {
                    "sent": "This is more or less banished.",
                    "label": 0
                },
                {
                    "sent": "You didn't see it again until about five years ago when Jonas and I and some others had very precise randomized algorithms, and so those have been implemented by a numerical analysts in a range of scientific computing applications, and they actually perform very, very well, and that's a very nontrivial statement, because the natural questions asking theoretical computer science don't perform well need to have slightly more refined ones.",
                    "label": 0
                },
                {
                    "sent": "So no randomness.",
                    "label": 0
                },
                {
                    "sent": "Then in the 1980s numerical linear algebra sometimes come up.",
                    "label": 0
                },
                {
                    "sent": "Age because you get high quality codes that are written for computers, and they're written for high performance computing.",
                    "label": 0
                },
                {
                    "sent": "So you want to do matrix vector multiplies fast and when you want to matrix vector multiplies fast, you want 10 digits of precision and you just want to be fast and then a lot of more.",
                    "label": 0
                },
                {
                    "sent": "Data sort of applications that have arisen since then.",
                    "label": 0
                },
                {
                    "sent": "Latent semantic indexing of doing something term document, spectral ranking, sorts of ideas, partitioning and normalized cuts, and so on that have arisen recently.",
                    "label": 0
                },
                {
                    "sent": "You don't need that much precision.",
                    "label": 1
                },
                {
                    "sent": "Oftentimes some you know that much precision is a liability, and so there's a lot of new problems that among.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things this workshops addressing so machine learning way back you might do that.",
                    "label": 0
                },
                {
                    "sent": "I don't know what machine learning was back in the 40s before you did machines, but you know those statistical data analysis that you did by hand the computers or people in the 60s you had roots, maybe in artificial intelligence and neural networks, and perceptrons and so on.",
                    "label": 1
                },
                {
                    "sent": "So forth.",
                    "label": 1
                },
                {
                    "sent": "Anna, lot of combinatorial foundations in terms of PAC learning.",
                    "label": 0
                },
                {
                    "sent": "VC theory, that sort of stuff.",
                    "label": 1
                },
                {
                    "sent": "So it's not obvious with the material those combinatorial ideas have to do with vector spaces and so on, but it turns out in the 90s a lot of connections to vector space ideas.",
                    "label": 0
                },
                {
                    "sent": "Kernels, manifold based methods, normalized cuts and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And so again you know the fact that you're able to work in a vector space here gives you a lot of power in terms of applying these things and robustly in a range of data applications, and so again, you're having a range of new prob.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of those will be talking about today.",
                    "label": 0
                },
                {
                    "sent": "So to give you a feel, sort of just a few examples and I hope these examples are also familiar to everyone you wanted.",
                    "label": 0
                },
                {
                    "sent": "Spectral partitioning and this was made popular by Shi and Malik in the machine learning and computer vision community in the name normalized cuts.",
                    "label": 0
                },
                {
                    "sent": "I hope this is familiar to you, but the notation might not be, so let me describe the notation.",
                    "label": 0
                },
                {
                    "sent": "L is the Laplacian of the graph.",
                    "label": 0
                },
                {
                    "sent": "It's roughly the diagonal minus the adjacency matrix and you normalize it.",
                    "label": 0
                },
                {
                    "sent": "And what you want to do is minimize X. Transpose LSX is a dummy variable.",
                    "label": 0
                },
                {
                    "sent": "You want to minimize this mixing.",
                    "label": 0
                },
                {
                    "sent": "This has to do with the quadratic term unmixing such that X transpose X is 1, so there's a variance constraint.",
                    "label": 0
                },
                {
                    "sent": "Index is perpendicular to the all zeros vector, so this is solvable via an eigenvalue problem, which isn't immediately obvious.",
                    "label": 0
                },
                {
                    "sent": "But it turns out it is.",
                    "label": 0
                },
                {
                    "sent": "You can get bounds on how good this is via something called the Cheeger's inequality, so if you're using this, you're able to say that this solution is good relative to the control thing, because if she goes inequality you have intractable graph partitioning problem.",
                    "label": 0
                },
                {
                    "sent": "You relax at the spectral and you know you're quadratically close because of triggers inequality was used for years in parallel scientific computing, computer vision, and machine learning.",
                    "label": 1
                },
                {
                    "sent": "And oftentimes the results are formulated such that if the data is drawn from somewhere nice, and if you can let the sampling the number of data points go to Infinity and so on and so forth, then the graph Laplacian will reconstruct some sort of manifold Laplacian, some nice underlying geometry.",
                    "label": 0
                },
                {
                    "sent": "But you know what?",
                    "label": 0
                },
                {
                    "sent": "If there's not a nice underlying geometry?",
                    "label": 1
                },
                {
                    "sent": "What if there's not good, well balanced cuts?",
                    "label": 0
                },
                {
                    "sent": "But if you're not in some low dimensional place, so the idea is, here's the adjacency matrix is a left half and right half.",
                    "label": 0
                },
                {
                    "sent": "Botham are about 5050 or 6040.",
                    "label": 0
                },
                {
                    "sent": "Look at the eigenvector you cut.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you know what if there's not good, well balanced cut.",
                    "label": 0
                },
                {
                    "sent": "So what if these other bad things happened?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is this algorithm?",
                    "label": 0
                },
                {
                    "sent": "Because it will return an answer regardless of what you feed in.",
                    "label": 0
                },
                {
                    "sent": "So long history of ideas, having with spectral ranking throughout the years, paid rank is sort of the most recent, most popular instantiation of it.",
                    "label": 1
                },
                {
                    "sent": "So you can think of this as a damped spectral ranking of a normalized adjacency matrix of the web graph so that eigenvector computer on the previous slide you put all the data points down to that and you cut somewhere.",
                    "label": 1
                },
                {
                    "sent": "But you can think of this as providing a ranking over the nodes.",
                    "label": 0
                },
                {
                    "sent": "In.",
                    "label": 0
                },
                {
                    "sent": "This idea has been invented again and again in a range of different areas.",
                    "label": 1
                },
                {
                    "sent": "And from the perspective this is a great thing for people in numerical linear algebra, they got very excited because you know this involves matrix computations.",
                    "label": 0
                },
                {
                    "sent": "But one thing that is a little bit on appealing to them was that when computed you oftentimes you approximated with three steps of the power method which you spend your life.",
                    "label": 0
                },
                {
                    "sent": "Look dealing with long sales and all these sorts of things.",
                    "label": 0
                },
                {
                    "sent": "That's a little unappealing.",
                    "label": 1
                },
                {
                    "sent": "It's not particularly important for today's ranking functions.",
                    "label": 1
                },
                {
                    "sent": "That's just an FY I.",
                    "label": 0
                },
                {
                    "sent": "In case you don't know it, but the idea underlying it could be useful more generally, and we'll see an example later.",
                    "label": 0
                },
                {
                    "sent": "In particularly, the personalized notion of this, which is yes to that local issue.",
                    "label": 0
                },
                {
                    "sent": "An very strong connections to some of these other things.",
                    "label": 0
                },
                {
                    "sent": "We're talking about a minute ago.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these things may manifest themselves in the following way, so hopefully you're familiar with something called latent semantic indexing or latent semantic analysis.",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of documents, you have a bunch of terms.",
                    "label": 0
                },
                {
                    "sent": "You normalize them in some way.",
                    "label": 0
                },
                {
                    "sent": "You have a big matrix, it's pretty sparse, and you say what I want to do is get better precision recall.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to get a sub K. I'm going to fix K according to some rules and model selection rule and I'm going to instead of doing precision recall and this thing, I'll do precision recall you.",
                    "label": 0
                },
                {
                    "sent": "I'll do my prediction task on a sub K. So basically taking the matrix in your filtering it through a low dimensional space, which is a reasonable thing to do if you believe that.",
                    "label": 0
                },
                {
                    "sent": "K concepts out in the world and maybe each document is a manifestation of a linear combination of one of those concepts.",
                    "label": 0
                },
                {
                    "sent": "So what you have in the back of your mind when you're doing something like this is that is this roughly K concepts that are most important and the rest is basically noise.",
                    "label": 0
                },
                {
                    "sent": "So basically you're in a low dimensional place, maybe with some other little structure, but the you know dimension, place and the other thing you're assuming is that there's no data point that's particularly important.",
                    "label": 0
                },
                {
                    "sent": "There's not a single data point that is extremely important, and the simplest way to state this is, you know there's some Gaussian process going on underneath, and so all the nodes contribute a little bit, but doesn't need to be Gaussian particular.",
                    "label": 0
                },
                {
                    "sent": "Anything where the information spread out on most of the nodes, so it's an empirical question to say, are these matrices well approximated by something low rank?",
                    "label": 0
                },
                {
                    "sent": "Is it the case that the information is distributed?",
                    "label": 0
                },
                {
                    "sent": "Or maybe are there localized hotspots?",
                    "label": 0
                },
                {
                    "sent": "Not obvious right so?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a theorem I don't have a plot, but the empirical results follow this sort of idea very closely, and if the theorem is, you know, the largest eigenvalues of the adjacency matrix of a gold graph with power law.",
                    "label": 1
                },
                {
                    "sent": "Degree distributions are also parallel distributed, so if you have any social graph or you have term document graph, so you have whatever that are noisy in some sense of the word and in the limit you might, you might realize this by saying it's it's I should have random in here somewhere large bags of adjacency matrix of a random power law graph.",
                    "label": 0
                },
                {
                    "sent": "Parallel distributed, so if you have bad degree variability and you're pretty noisy then then that's going to manifest itself in the spectrum that the spectrum is going to power law distributed.",
                    "label": 0
                },
                {
                    "sent": "So what this says is that there are 10 nodes that are most important, but they do not capture most of the information.",
                    "label": 0
                },
                {
                    "sent": "You know they capture 20% of the information that 20% of variance or whatever.",
                    "label": 1
                },
                {
                    "sent": "Alright, to get 30% of the variance you don't need 11 or 12 directions, you need 100.",
                    "label": 0
                },
                {
                    "sent": "And to get 40% of the information into 1050% need 10,000.",
                    "label": 0
                },
                {
                    "sent": "So by now you densifying the graph exactly when you should be sparsifying it.",
                    "label": 0
                },
                {
                    "sent": "And so in some sense, you're damaging the relevant structure over what's going on.",
                    "label": 0
                },
                {
                    "sent": "So, So what this is saying is that there are a small number of most important things, but they do not capture most of the information.",
                    "label": 0
                },
                {
                    "sent": "So the assumption that you're in the right place is not even close to quality.",
                    "label": 0
                },
                {
                    "sent": "With satisfied in a lot of these datasets.",
                    "label": 0
                },
                {
                    "sent": "The entries of the eigenvectors also.",
                    "label": 0
                },
                {
                    "sent": "We'll get to that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question here is, what about is the mass in the eigenvectors spread out?",
                    "label": 0
                },
                {
                    "sent": "So I'll answer the question more refined way.",
                    "label": 0
                },
                {
                    "sent": "But initially given M by N matrix.",
                    "label": 0
                },
                {
                    "sent": "So how would you quantify localization?",
                    "label": 0
                },
                {
                    "sent": "So you might say how localized or coherent or the left or the right singular vector.",
                    "label": 1
                },
                {
                    "sent": "So there's a range of ways to do this in physics for reasons having to do with the Hamiltonian.",
                    "label": 0
                },
                {
                    "sent": "It's called the inverse participation ratio, which is a variant of this.",
                    "label": 0
                },
                {
                    "sent": "We use this to get relative error bounds.",
                    "label": 0
                },
                {
                    "sent": "In the worst case matrix sampling algorithms, and it turns out to be a much more general concept that's useful for lot of these social graphs.",
                    "label": 0
                },
                {
                    "sent": "So let me say that row is PU sub KA projection matrix on use of K. The left or right singular vectors I say bye.",
                    "label": 0
                },
                {
                    "sent": "So take the K left singular vectors that define the subspace project onto it.",
                    "label": 0
                },
                {
                    "sent": "You have diagonal elements and look at those diagonal elements.",
                    "label": 0
                },
                {
                    "sent": "Alright, equivalently, with those diagonal elements are is if you have.",
                    "label": 0
                },
                {
                    "sent": "This is your use of K. Your matrix of left singular vectors.",
                    "label": 0
                },
                {
                    "sent": "That is, the Euclidean norms of these rows, right?",
                    "label": 0
                },
                {
                    "sent": "The Euclidean norms of these things are one Euclidean norms of these.",
                    "label": 0
                },
                {
                    "sent": "This could be a truncated identity.",
                    "label": 0
                },
                {
                    "sent": "This could be a truncated Hadamard.",
                    "label": 0
                },
                {
                    "sent": "These could be very uniform or very non uniform.",
                    "label": 1
                },
                {
                    "sent": "But there's nothing special about the left singular vectors.",
                    "label": 0
                },
                {
                    "sent": "Any basis spanning that space will have the same properties 'cause it's a function of the projection matrix.",
                    "label": 1
                },
                {
                    "sent": "These are statistical leverage scores.",
                    "label": 0
                },
                {
                    "sent": "If you're a statistician, these are the diagonal elements of that matrix, so we have a bunch of papers on the topic.",
                    "label": 0
                },
                {
                    "sent": "You won't see that word until we had our PNS paper because we didn't know about the connection.",
                    "label": 0
                },
                {
                    "sent": "And finally, you know you read the book of third Time and Dawn breaks on Marblehead and so the connection becomes obvious.",
                    "label": 0
                },
                {
                    "sent": "So these are statistical leverage scores.",
                    "label": 1
                },
                {
                    "sent": "Widely used classically in regression diagnostics.",
                    "label": 0
                },
                {
                    "sent": "Basically for finding outliers.",
                    "label": 1
                },
                {
                    "sent": "So these will quantify which rows have the most influence or leverage on the low rank fit.",
                    "label": 1
                },
                {
                    "sent": "And as these are important for bridging the gap in terms of numerically implementable randomized algorithms.",
                    "label": 0
                },
                {
                    "sent": "Not always, but often.",
                    "label": 0
                },
                {
                    "sent": "Very often they're very nonuniform.",
                    "label": 0
                },
                {
                    "sent": "In practice, this particular data set, not a social graph.",
                    "label": 0
                },
                {
                    "sent": "This is DNA snip data.",
                    "label": 0
                },
                {
                    "sent": "Social graphs would be much, much worse.",
                    "label": 0
                },
                {
                    "sent": "I couldn't even put it.",
                    "label": 0
                },
                {
                    "sent": "There would be that bad.",
                    "label": 0
                },
                {
                    "sent": "This DNA snip data and this is the average value.",
                    "label": 0
                },
                {
                    "sent": "There's a red line here.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can see it and this is how non uniform they are.",
                    "label": 0
                },
                {
                    "sent": "And it turns out if you keep the top 10 or 20 and you give those snips to the geneticist, those are particularly informative with a capital I informative as a particular measure that's of interest in the area.",
                    "label": 0
                },
                {
                    "sent": "These correlate very strongly with the snips that are most informative, so they are in fact very non very nonuniform, and they often times correlate with things that are of interest.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at adjacency matrices, they tend to correlate with high degree nodes.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at Laplacians in particular, normalize well either.",
                    "label": 0
                },
                {
                    "sent": "Normalized or not Laplacian.",
                    "label": 0
                },
                {
                    "sent": "They tend to small clusters that will get back to in a minute, so very nonuniform so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You're not even close to satisfy either the assumptions that the low rank spaces SVD would be assuming.",
                    "label": 1
                },
                {
                    "sent": "So why do these methods work at all so implicit?",
                    "label": 1
                },
                {
                    "sent": "Effectively these methods work because low rank spaces are very, very structured places.",
                    "label": 0
                },
                {
                    "sent": "Alright, I mean this is just not a lot of places to hide your mistakes as opposed to I mean graphs where bad things can happen.",
                    "label": 0
                },
                {
                    "sent": "Graphs are relatively good hypergraphs, tensors.",
                    "label": 0
                },
                {
                    "sent": "You know non negativity when you start taking a lot of those other stuff you give yourself much, much more descriptive flexibility.",
                    "label": 0
                },
                {
                    "sent": "These are very structured places so you can only hide your mistakes in so many places in some sense of all models are wrong but some are useful.",
                    "label": 1
                },
                {
                    "sent": "Those that are most useful, those that implicitly have capacity control.",
                    "label": 0
                },
                {
                    "sent": "So I haven't act on capacity control anyway.",
                    "label": 0
                },
                {
                    "sent": "There's no regulations on by the data modeling.",
                    "label": 0
                },
                {
                    "sent": "I'm working in a structured place.",
                    "label": 0
                },
                {
                    "sent": "Sort of implicitly give you sort of capacity control and basic.",
                    "label": 1
                },
                {
                    "sent": "It's 'cause diffusions and L2 based methods aggregate information in a very particular way, and that factors associated pluses and minuses.",
                    "label": 0
                },
                {
                    "sent": "So it means that information spread out eigenvectors if you're diffusing in a low rank place, but it means you get hot spots if you have very high degree nodes.",
                    "label": 0
                },
                {
                    "sent": "If you have small clustering offer range of other.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the question might be, since I've given you a hint that what might be going on here is, we've seen sort of roughly why some of these methods might work.",
                    "label": 0
                },
                {
                    "sent": "I've sort of argued why that might be the case.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you how these are bad in a lot of ways.",
                    "label": 0
                },
                {
                    "sent": "But the question is, can we get sort of local analogs of some of the traditional machinery and maybe apply them here?",
                    "label": 0
                },
                {
                    "sent": "And so the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "Empirically and then I'll give you some.",
                    "label": 0
                },
                {
                    "sent": "Some more theoretical things and this is I could talk for now and that's all in.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Miracle, but I'll give you sort of hint of it.",
                    "label": 0
                },
                {
                    "sent": "So lots of networks out there.",
                    "label": 1
                },
                {
                    "sent": "Networks are very popular.",
                    "label": 1
                },
                {
                    "sent": "Think of social graphs, collaboration, networks of friendship networks, maybe information networks with Citation blog cross postings.",
                    "label": 0
                },
                {
                    "sent": "As I said, advertise a bid phrase graph.",
                    "label": 0
                },
                {
                    "sent": "With these things grew out of an for all these things, you can model them in different ways, But let's not give ourselves too much descriptive flexibility.",
                    "label": 1
                },
                {
                    "sent": "Let's just work with the so called interaction site model of a network where nodes represent some sort of entities.",
                    "label": 1
                },
                {
                    "sent": "Advertisers, phrases, authors.",
                    "label": 0
                },
                {
                    "sent": "You know whatever and edges will represent some sort of interaction between entities.",
                    "label": 0
                },
                {
                    "sent": "I'm going or direction here 'cause this work grew out of asking for clusters directing.",
                    "label": 0
                },
                {
                    "sent": "This is a non trivial extension of this weights.",
                    "label": 0
                },
                {
                    "sent": "Don't worry about, everything will go through to any weight that a practitioner cares about.",
                    "label": 0
                },
                {
                    "sent": "Meaning you could design weights that such that what I'm talking bout will not be the case, but when we went back and looked at 70 graphs we said give us your grass waited the way you like and will chew on them so they only about weights time evolution.",
                    "label": 0
                },
                {
                    "sent": "We have ideas on but I'm not going to.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weather today, so to give you an example, live Journal, 4 million nodes, 40,000,000 edges.",
                    "label": 0
                },
                {
                    "sent": "That's typical of the sparsity.",
                    "label": 0
                },
                {
                    "sent": "I mean incredibly sparse.",
                    "label": 0
                },
                {
                    "sent": "10 nodes on average.",
                    "label": 0
                },
                {
                    "sent": "And since you have power law heavy tailed structure here, it's even worse in general, except for some high degree nodes.",
                    "label": 0
                },
                {
                    "sent": "Epinions Flickr, delicious coauthorship graphs in physics and computer science.",
                    "label": 0
                },
                {
                    "sent": "This is 10 of the 70 that was in the short version of the paper, not particularly largest to smallest.",
                    "label": 0
                },
                {
                    "sent": "Everybody's representative information graphs a bunch of web snapshots, Athens to papers, the bipartite structure.",
                    "label": 0
                },
                {
                    "sent": "There is a little bit like the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The term like the advertiser bid phrase properties.",
                    "label": 0
                },
                {
                    "sent": "So what people want to do?",
                    "label": 0
                },
                {
                    "sent": "As I said in the application was defined isolated clusters in markets with sufficient money or clicks and sufficient coherence.",
                    "label": 1
                },
                {
                    "sent": "So want to be relatively isolated.",
                    "label": 0
                },
                {
                    "sent": "You wanted to have sufficient size by whatever metric, and the Internet companies oftentimes something correlated with money because it has revenue downstream and sufficient coherence that you can pull it out and reason about it as an economic unit or whatever.",
                    "label": 0
                },
                {
                    "sent": "So if you pull it out in any reason about the used car market versus the flower market or whatever, and the idea that people have in the back of their mind is that the world looks this way.",
                    "label": 1
                },
                {
                    "sent": "You have gambling markets and you have sports markets and you might have some overlap and you might have some hierarchy between them and you want to find what is the clickthrough rate, or whatever of the sports gambling market when you draw the picture on a piece of paper like this, what you're saying is that gambling has a lot of stuff going on in particular relative to how much gambling interacts with the rest of the world.",
                    "label": 0
                },
                {
                    "sent": "You've drawn a circle or square around gambling, and similarly with sports.",
                    "label": 0
                },
                {
                    "sent": "And there's an interaction here, but in some sense, in terms of, you know that in some sense these things gambling and sports have more stuff going on compared to the outside relative to sports gambling.",
                    "label": 0
                },
                {
                    "sent": "Bigger circles are better than smaller circles.",
                    "label": 0
                },
                {
                    "sent": "Elephants dissipate heat in different ways than mice do, because their volume and surface area properties are very different.",
                    "label": 0
                },
                {
                    "sent": "And you're saying that bigger things are better.",
                    "label": 0
                },
                {
                    "sent": "Insofar as this internal versus external tension is talking, but that should have the back of your mind.",
                    "label": 0
                },
                {
                    "sent": "What's usually not asked.",
                    "label": 0
                },
                {
                    "sent": "Is this even possible?",
                    "label": 0
                },
                {
                    "sent": "So the tools I mentioned in the first half actually addressed this question.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into detail about that, but will be able to actually test the hypothesis of the data looks this way the answer is no.",
                    "label": 0
                },
                {
                    "sent": "And then I'll tell you what the data looks like, and then I'll give you some examples of new machine learning linear algebra that comes from sort of looking under the hood in terms of how these techniques are useful.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is what people think they look like.",
                    "label": 0
                },
                {
                    "sent": "This is what the graphs actually look like.",
                    "label": 1
                },
                {
                    "sent": "When you put them on 2 dimensions, they look like a mess of hairballs.",
                    "label": 0
                },
                {
                    "sent": "Dirty little secret in a lot of this visualization game, you'd like visualization to reveal something about the data.",
                    "label": 0
                },
                {
                    "sent": "If you are familiar with state of the art in visualization algorithms, you could tell me what visualization algorithm we used to visualize this.",
                    "label": 0
                },
                {
                    "sent": "'cause these rings and so on.",
                    "label": 0
                },
                {
                    "sent": "If you were familiar with state of the art in publicly available networks, you'd probably be hard pressed to tell me which graph this was.",
                    "label": 0
                },
                {
                    "sent": "So these visualization algorithms are revealing more about the inner workings of the algorithms and about the graphs they're trying to visualize.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's a little disturbing.",
                    "label": 0
                },
                {
                    "sent": "If you're interesting visualization, but it suggests that you know you might be able to take advantage of the artifactual structure that this particular visualization does to reveal something about the inner workings of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And I have a different visualization.",
                    "label": 0
                },
                {
                    "sent": "I get another hairball.",
                    "label": 0
                },
                {
                    "sent": "It's a mess, but it's slightly different artificial structure.",
                    "label": 0
                },
                {
                    "sent": "You know, if I use something spectral like here, and it might not be obvious that it was, but if it makes certain local to global decisions or whatever, can I reveal insight about the graphs and so that sort of philosophy guided a lot of the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But not with respect to visualization with more generally.",
                    "label": 0
                },
                {
                    "sent": "So I talked about the large elephants in the small mice.",
                    "label": 0
                },
                {
                    "sent": "Let's quantify that in terms of conductance, this is the normalized cuts objective.",
                    "label": 0
                },
                {
                    "sent": "Conductances defined for a graph don't think manifolds then graph conductance if the graphs of discrimination of a manifold, then things are good.",
                    "label": 0
                },
                {
                    "sent": "But this is worlds for an arbitrary graph an it's a surface area term.",
                    "label": 0
                },
                {
                    "sent": "The number of edges between the conductance of a set is the number of edges between that the set and its complement.",
                    "label": 0
                },
                {
                    "sent": "So a surface area term, the numbers between this sentence compliment divided by volume term.",
                    "label": 0
                },
                {
                    "sent": "And there's a couple of things you can put down here, but it's basically the number of edges.",
                    "label": 0
                },
                {
                    "sent": "Inside your set.",
                    "label": 0
                },
                {
                    "sent": "So the best conductance this is the conductance of a set of nodes.",
                    "label": 1
                },
                {
                    "sent": "The best conductance in the entire graph.",
                    "label": 0
                },
                {
                    "sent": "That's an intractable problem.",
                    "label": 0
                },
                {
                    "sent": "One way to approximate it is to relax into it and solve a spectral thing a different way to approximate it is to relax in a different way and solving multi commodity flow problem.",
                    "label": 0
                },
                {
                    "sent": "So there's a bunch of ways to approximate it.",
                    "label": 0
                },
                {
                    "sent": "Let's ask a slightly more refined question.",
                    "label": 1
                },
                {
                    "sent": "Let's define the network community profile plot to be the minimum over clusters or sets of nodes of size K of this quantity.",
                    "label": 0
                },
                {
                    "sent": "So basically the size resolved version of conductance.",
                    "label": 1
                },
                {
                    "sent": "Right and in the same way as conductors.",
                    "label": 0
                },
                {
                    "sent": "Conductance captures a surface area to volume.",
                    "label": 0
                },
                {
                    "sent": "I'm talking bout clusters, but you can think of them as this is, this is the motivation people have when they talking bout communities in networks.",
                    "label": 0
                },
                {
                    "sent": "Even if they go on to find some other objectives.",
                    "label": 0
                },
                {
                    "sent": "This is motivation.",
                    "label": 0
                },
                {
                    "sent": "People in the same way this profile plot capture sort of size resolved version of that we can ask with the big things are better worse than smaller things and we'll see.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's important later, so there's two criteria here.",
                    "label": 0
                },
                {
                    "sent": "Surface Area, terminal, volume term, and for some graphs, let's column space like graphs.",
                    "label": 0
                },
                {
                    "sent": "Things that are available in a low dimensional space, random geometric graphs, discretizations of low dimensional spaces, you know, things that are sort of morally low dimensional.",
                    "label": 1
                },
                {
                    "sent": "These these two quantities, the cut quality and the cut balance.",
                    "label": 1
                },
                {
                    "sent": "How good the cut is by that measure and how balanced the cut is it 5050 or 99?",
                    "label": 0
                },
                {
                    "sent": "One these two things work together basically because if you're living on a low dimensional space you have this issue with the elephants in the mice.",
                    "label": 1
                },
                {
                    "sent": "The bigger things have more stuff in relative surface area.",
                    "label": 0
                },
                {
                    "sent": "There are graphs such that this plot, looking at volume versus the surface area term, is flat.",
                    "label": 0
                },
                {
                    "sent": "So for low dimensional things this plot goes down.",
                    "label": 1
                },
                {
                    "sent": "So down is good and as you get larger and larger you get better and better 'cause you have more stuff going on inside you, like the elephant.",
                    "label": 0
                },
                {
                    "sent": "There are graphs for which this is flat, those are known as expanders.",
                    "label": 0
                },
                {
                    "sent": "Those are very bad properties.",
                    "label": 0
                },
                {
                    "sent": "They're very, extremely in alot of ways.",
                    "label": 0
                },
                {
                    "sent": "In particular, constant expanders that very, very useful in algorithms because they're good test case for a lot of different things will get back to that and what we'll see is that in a lot of cases this curve actually goes up, so the best possible cuts get worse and worse as a function of size.",
                    "label": 0
                },
                {
                    "sent": "So at large size scales these things are expanders at small size skills you have good clusters alright?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you're interested in Zachary, this is a 34 node graph that the Community detection people like to work with.",
                    "label": 0
                },
                {
                    "sent": "This plot goes down, local minima correspond to things that you might plausibly want to call clusters.",
                    "label": 0
                },
                {
                    "sent": "This is Mark Newman's network science, which is a couple 100 nodes constructed from bibliography on.",
                    "label": 1
                },
                {
                    "sent": "This thing goes down also, and the reason these two things go down basically is because if you deal with a chain or a grid, or a cube or whatever, for those isoperimetric reasons for the surface area to volume reasons, this thing will go down.",
                    "label": 0
                },
                {
                    "sent": "And this graph embeds pretty well on the plane.",
                    "label": 0
                },
                {
                    "sent": "You can visualize it and it goes down.",
                    "label": 0
                },
                {
                    "sent": "And so if you want to do normalized cuts, you mean more or less, any method will cut you right there.",
                    "label": 0
                },
                {
                    "sent": "I mean, an awful lot of work is done arguing about whether 9 should be on this side of the cut of that side of the cut, whatever.",
                    "label": 0
                },
                {
                    "sent": "But there all the same they'll just put you there in the cut in half and similar.",
                    "label": 0
                },
                {
                    "sent": "Here they will cut there, and maybe it'll cut there there.",
                    "label": 0
                },
                {
                    "sent": "But the first approximation will be the same and the actual.",
                    "label": 0
                },
                {
                    "sent": "We're dealing with an extremely statistic here, so you gotta be careful.",
                    "label": 0
                },
                {
                    "sent": "The actual sets of nodes and achieve a minimum.",
                    "label": 0
                },
                {
                    "sent": "That's a very non robust.",
                    "label": 0
                },
                {
                    "sent": "But the qualitative property of going down is very, very robust, so more or less anything that lives on the Earth will go down.",
                    "label": 0
                },
                {
                    "sent": "And if you expand or click or something, this thing will be flat.",
                    "label": 0
                },
                {
                    "sent": "So if you're going down, you can do a bipartition and recurse and everything's you.",
                    "label": 0
                },
                {
                    "sent": "Know, reasonably good.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to what I showed you before with the spectral partitioning we have two sides that are reason.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well balanced, so the downward sloping profile plot is important for small social graphs for validation, low dimensional things that inform your intuition.",
                    "label": 1
                },
                {
                    "sent": "Hierarchical networks in a natural interpretation in terms of isoperimetry and K means and manifold, and so on and so forth.",
                    "label": 1
                },
                {
                    "sent": "So they said we start very some very strange properties.",
                    "label": 1
                },
                {
                    "sent": "We went back and looked at 70 large social information graphs.",
                    "label": 0
                },
                {
                    "sent": "I'll give you the leading bit of information this summer.",
                    "label": 0
                },
                {
                    "sent": "I'm looking at 10,000 feet.",
                    "label": 1
                },
                {
                    "sent": "Every graph is an exception to simple story.",
                    "label": 0
                },
                {
                    "sent": "I could tell in an hour, so everyone's going to be exception.",
                    "label": 0
                },
                {
                    "sent": "We can talk offline, but the leading order piece of information is what do these things look like if you squint at him from 10,000 feet?",
                    "label": 0
                },
                {
                    "sent": "Do they look like a hot dog that you can cut in half?",
                    "label": 0
                },
                {
                    "sent": "Are they in expanded with no good cuts or is it something worse going on?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea is that we're going to in order to answer that question.",
                    "label": 0
                },
                {
                    "sent": "It's hard 'cause I showed you these graphs look like they're a mess.",
                    "label": 0
                },
                {
                    "sent": "So in a sense we're going to probe large networks with approximation algorithms.",
                    "label": 1
                },
                {
                    "sent": "We're going to use approximation algorithms for this NP hard graph partitioning problem says experimental probes of network structure.",
                    "label": 1
                },
                {
                    "sent": "So the nice thing about this particular notion of conductance is that not only does it correspond to what you might think of as clusters, but there's a rich body body of theory and practice.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is normalized cut, something I'd mentioned on Slide 3.",
                    "label": 0
                },
                {
                    "sent": "Rich body of theory and practice that tells how it behaves.",
                    "label": 0
                },
                {
                    "sent": "So machine learners and scientific computers like spectral methods and spectral methods.",
                    "label": 0
                },
                {
                    "sent": "Sugar gives you a quadratic approximation guarantee, and that's not an artifact analysis.",
                    "label": 0
                },
                {
                    "sent": "There are graphs that are that bad, and those are graphs that in some sense confuse long pass with deep cuts.",
                    "label": 0
                },
                {
                    "sent": "Multicommodity flow methods will give you a log and approximation that's not not effective.",
                    "label": 0
                },
                {
                    "sent": "The analysis there are graphs that are that bad.",
                    "label": 0
                },
                {
                    "sent": "Those are constant degree expanders and there's a bunch of other sorts of methods so these things you start with something intractable.",
                    "label": 0
                },
                {
                    "sent": "You relax in different ways and you filter through different geometric places.",
                    "label": 0
                },
                {
                    "sent": "The geometric place that multi commodity flow is effectively being filtered through is very different than spectral, but this sort of implicitly regularising in different ways.",
                    "label": 0
                },
                {
                    "sent": "And so we might say, can we tear the graph apart using these complementary methods and play them off of each other in a little bit like an experimental probe?",
                    "label": 0
                },
                {
                    "sent": "So you intuitively, if you want understand what protein looks like, how do people do it right?",
                    "label": 0
                },
                {
                    "sent": "They send an NMR, they put in a liquid or crystal, they send it in tomorrow they sent an X Rays.",
                    "label": 0
                },
                {
                    "sent": "They measure lots of garbage and then they look at all this stuff.",
                    "label": 0
                },
                {
                    "sent": "Knowledge of the physics of what you put in, and trying to infer what these things look like.",
                    "label": 0
                },
                {
                    "sent": "So we're doing exactly the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "We have this graph we can't visualize were tearing it apart with Spectra were tearing it apart with low.",
                    "label": 0
                },
                {
                    "sent": "We know that spectral behave has certain problems on certain classes of graphs and perform very well on others.",
                    "label": 0
                },
                {
                    "sent": "We know that flow has problems.",
                    "label": 0
                },
                {
                    "sent": "Certain classes of graphs, and if he's very well in others, so tearing it apart with these things going to measure lots of junk and then we're going for with this thing looks like.",
                    "label": 0
                },
                {
                    "sent": "And there's a lot of details that I'm going to gloss over, except to say the actual way we implement, say, for example, spectral is to do a local version of spectral, so special about eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "But one way to compute this is to say, do a random walk walk and run it forever.",
                    "label": 0
                },
                {
                    "sent": "Right power method or something like this?",
                    "label": 0
                },
                {
                    "sent": "Or heat kernel?",
                    "label": 0
                },
                {
                    "sent": "So you might say, what if you put?",
                    "label": 0
                },
                {
                    "sent": "All your mass, all your heat on one or a small number of nodes and diffuse just a few steps and maybe truncate a little bit and diffuse four or five more steps.",
                    "label": 0
                },
                {
                    "sent": "So what you're doing something very local, you're truncating that you're doing that for algorithmic reasons, 'cause you don't touch a lot of nodes.",
                    "label": 0
                },
                {
                    "sent": "It might have statistical side effects.",
                    "label": 0
                },
                {
                    "sent": "And so you might ask, what are these things?",
                    "label": 0
                },
                {
                    "sent": "So I'll show you what the empirical results are.",
                    "label": 0
                },
                {
                    "sent": "But answering these questions is very much the new machine learning a new linear algebra that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "I'll give you an example of some of those.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so we're not naively going to compute a vector, but we're going to operational steps that should you get all the guarantees that global spectral gives you.",
                    "label": 0
                },
                {
                    "sent": "So here's a typical example of would find.",
                    "label": 1
                },
                {
                    "sent": "This plot will go down and down.",
                    "label": 0
                },
                {
                    "sent": "You'll hit a minimum, and then I'll go up and up and up over those of magnitude.",
                    "label": 0
                },
                {
                    "sent": "All these plots are log log.",
                    "label": 0
                },
                {
                    "sent": "And so you know minima correspond to things you might possibly want to call clusters.",
                    "label": 0
                },
                {
                    "sent": "This plot will go up and up and up and up.",
                    "label": 0
                },
                {
                    "sent": "The X axis is the size of the cluster.",
                    "label": 0
                },
                {
                    "sent": "Considering the Y axis is the Community quality, score, the conductance down is good.",
                    "label": 0
                },
                {
                    "sent": "'cause of the surface area volume.",
                    "label": 0
                },
                {
                    "sent": "So the best possible clusters get better and better until you hit 50 nodes or something and then they get worse and worse and worse.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, the details are different, but you see that with live Journal with opinions I could show you 68 more plots.",
                    "label": 0
                },
                {
                    "sent": "The different colors correspond to different.",
                    "label": 0
                },
                {
                    "sent": "The different algorithm proceeds.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get into that.",
                    "label": 0
                },
                {
                    "sent": "Trust me that I'm not cheating here.",
                    "label": 0
                },
                {
                    "sent": "You know the different algorithms behave in ways you'd expect those lower bounds, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "But the property is that it goes down and down.",
                    "label": 0
                },
                {
                    "sent": "It's a minimum, goes up and up.",
                    "label": 0
                },
                {
                    "sent": "So what's the what would be explaining this?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So what's the structure of the graph and maybe what's the simplest model?",
                    "label": 0
                },
                {
                    "sent": "It would reproduce something like this.",
                    "label": 0
                },
                {
                    "sent": "'cause it's clearly not low dimensional, right?",
                    "label": 0
                },
                {
                    "sent": "The low dimensional if you low dimensional.",
                    "label": 0
                },
                {
                    "sent": "Any meaningful sense of the word, you go down and down and down.",
                    "label": 0
                },
                {
                    "sent": "And if you really had no good cuts, you'll be flat.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going on here is if we define some sort of notion of whisker, here's one way to define.",
                    "label": 0
                },
                {
                    "sent": "You could define otherwise maximal sub graphs that are attached to a network by removing a single edge.",
                    "label": 1
                },
                {
                    "sent": "You might say those guys are nuisances.",
                    "label": 0
                },
                {
                    "sent": "You just cut him off.",
                    "label": 0
                },
                {
                    "sent": "If you do that, you're going to be cutting off 40% of the nodes in 20% on the edges of some huge fraction.",
                    "label": 0
                },
                {
                    "sent": "And if you believe that you want to push mass out on the heavy tail, those are the things that you do not want to be cutting off 'cause everyone knows what you are searching for.",
                    "label": 0
                },
                {
                    "sent": "If you search on Britney Spears, would you want to find a web page or the Wikipedia page?",
                    "label": 0
                },
                {
                    "sent": "But if you site if you search on something that's farther out on the tail?",
                    "label": 0
                },
                {
                    "sent": "This is exactly what you don't want to be cutting off.",
                    "label": 0
                },
                {
                    "sent": "If you cut all those off, you get qualitatively the same plot, but you don't want to pay off and the core is the rest of the graph in empirical factors at the global minimum is a whisker.",
                    "label": 1
                },
                {
                    "sent": "So some huge fraction of the nodes think of as ballpark 100 edges are connected by one node.",
                    "label": 0
                },
                {
                    "sent": "If you cut all those off, you have a bunch of other whiskers that are now connected by two edges, and if you cut all those up a bunch of other whiskers, so you have a core periphery, recursive core, periphery structure.",
                    "label": 0
                },
                {
                    "sent": "And that's sort of what's going on here.",
                    "label": 0
                },
                {
                    "sent": "Now you might ask what's this?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Placenta model that would reproduce this, so here's a theorem.",
                    "label": 0
                },
                {
                    "sent": "Power law, random graph.",
                    "label": 0
                },
                {
                    "sent": "Let W be a parallel degree distribution to some assumptions.",
                    "label": 0
                },
                {
                    "sent": "Here, connect the nodes in the usual way theorem you will have logarithmically deep cuts of log size and if you get a factor of 10 more you will not have any cuts below the basal level.",
                    "label": 0
                },
                {
                    "sent": "So everyone beats up on their train yay right?",
                    "label": 0
                },
                {
                    "sent": "Air Terranea explain, explains the fact that you have deep cuts that small size skills and know deep cuts at large size skills.",
                    "label": 0
                },
                {
                    "sent": "You can't be naive about this.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "Let me back up on steps.",
                    "label": 0
                },
                {
                    "sent": "This is what you get.",
                    "label": 0
                },
                {
                    "sent": "The power law degree distribution.",
                    "label": 0
                },
                {
                    "sent": "You might want to blame the power laws.",
                    "label": 1
                },
                {
                    "sent": "It's not the power law that's causing the problem.",
                    "label": 0
                },
                {
                    "sent": "There is rain gives you exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "Now for any, there's a parameter P which has to do with how connected you are.",
                    "label": 0
                },
                {
                    "sent": "If you choose P to be greater than log N / N log in, usually the scale at which measure concentrates to, say, log squared N / N. Then the graph is fully connected, all the degrees are at their pyrrhic values.",
                    "label": 0
                },
                {
                    "sent": "Everything is pretty flat if P is between 1 / N an login over, and so say constant over N. Then the same theorem holds.",
                    "label": 0
                },
                {
                    "sent": "So air this training is as simple as possible model that explains these qualitative results.",
                    "label": 0
                },
                {
                    "sent": "So sparsity coupled with extreme randomness is the issue.",
                    "label": 1
                },
                {
                    "sent": "So when I said you take this graph, you filter through spectral and flow spaces.",
                    "label": 0
                },
                {
                    "sent": "And you're getting some implicit regularization or playing them off of each other.",
                    "label": 0
                },
                {
                    "sent": "The reason that's particularly important is 1, because the graphs are large enough that we don't want to be putting out one or whatever regularization just tacked on, because we will be making the problem harder and intractable.",
                    "label": 0
                },
                {
                    "sent": "Problem with understanding of so working with sort of scalable algorithms, but the data sparse enough and noisy enough that although you don't have stringy things with spectral fails, and although you're not a constant expanded workflow as problems, these graphs have expanded like parts and long stringy parts, and so on and so forth, so understanding the implicit regularization and a lot of those things.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a particularly important.",
                    "label": 0
                },
                {
                    "sent": "And I guess I mentioned their training, so you should really think of these datasets as local pockets of structure on global noise, not noise on global structure.",
                    "label": 1
                },
                {
                    "sent": "So don't think of this as some global structure, whether it's a low rank space or manifold, you know whatever with some noise.",
                    "label": 0
                },
                {
                    "sent": "I mean these things if you squint at them, they look like you're trying extremely sparse right now, so you'd have little pockets of structure.",
                    "label": 0
                },
                {
                    "sent": "You have some bad variance properties.",
                    "label": 0
                },
                {
                    "sent": "Clearly triangles closing quadrangles close, and you get clusters of.",
                    "label": 0
                },
                {
                    "sent": "So all those plots I showed you, you are well below the random basal level.",
                    "label": 0
                },
                {
                    "sent": "I didn't emphasize this, but.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these plots had this thing.",
                    "label": 0
                },
                {
                    "sent": "That's the random basal level, so you're well below that.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of local structure, but it's local structure on top of a sparse quasi ran.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scaffolding, and when you're on top of a sparse quasirandom scaffolding, you do not get good.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bipartitions and a lot of other things that are.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the back of your mind, in terms of how.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These sorts of methods might behave.",
                    "label": 0
                },
                {
                    "sent": "So I didn't walk through all the details that prove this, but I hope you at least believe that it's plausible that these graphs look that way, and you might say you know is all hope lost.",
                    "label": 0
                },
                {
                    "sent": "Or can we have sort of maybe some new machine learning a new linear algebra?",
                    "label": 1
                },
                {
                    "sent": "So the answer is yes, I think so.",
                    "label": 0
                },
                {
                    "sent": "I wanted to talk about three different examples.",
                    "label": 0
                },
                {
                    "sent": "One is the way we actually computed those plots was to sprinkle a seed set of nodes at a bunch of different places in the graph.",
                    "label": 0
                },
                {
                    "sent": "Fix a bunch of size scales, which is basically a number of steps to iterate.",
                    "label": 0
                },
                {
                    "sent": "And and and run that process again and again.",
                    "label": 0
                },
                {
                    "sent": "So sprinkle, see note here, here.",
                    "label": 0
                },
                {
                    "sent": "Here bunch of size scales etc etc.",
                    "label": 0
                },
                {
                    "sent": "Compute all those stuff, all those things and take sort of a lower bound so the primitive there is that we're starting with no defusing a bit what we're doing.",
                    "label": 0
                },
                {
                    "sent": "Other things like a pay drink analogue of it, or heat kernel analog of it.",
                    "label": 0
                },
                {
                    "sent": "So there's a couple of different ways you can implement that, but think of it is doing sort of local diffusion, which is very much of a flavor of a local spectral thing.",
                    "label": 0
                },
                {
                    "sent": "So Spielman, Tang, and sung laying had some theorems justifying why this might be reasonable, but the theorems are incredibly complicated in terms of their quantification.",
                    "label": 0
                },
                {
                    "sent": "I mean, the results are extremely.",
                    "label": 0
                },
                {
                    "sent": "Difficult to state and we're working it for years and it's fairly hard to understand.",
                    "label": 0
                },
                {
                    "sent": "You know you started this at a seed nodes and for every set of nodes, if you choose a constant fraction of the nodes and you get a cut such that for all cuts of a reasonable size bounded near you that are correlated in some way, and by then you probably don't remember the theorem started so intuitively.",
                    "label": 0
                },
                {
                    "sent": "What are these things doing and what's going on with these things is you're doing a very operational thing that you're running a bunch of steps.",
                    "label": 0
                },
                {
                    "sent": "It's not clear what you're optimizing.",
                    "label": 0
                },
                {
                    "sent": "Are you even optimizing an objective function?",
                    "label": 0
                },
                {
                    "sent": "I mean, you might be approximately optimizing it, but are you exactly optimizing some objective function?",
                    "label": 0
                },
                {
                    "sent": "So I want to give you an optimization view of a local version of spectral partitioning.",
                    "label": 1
                },
                {
                    "sent": "It's going to be first cousin of of what they did and what we actually implemented.",
                    "label": 0
                },
                {
                    "sent": "And in a lot of these cases, we weren't doing the power method, but I'll describe it in terms of the power method, just 'cause it's a little bit simpler.",
                    "label": 0
                },
                {
                    "sent": "We were doing page personalized page rank and so either he Colonel variants of this, but we had to do things where there's an iteration parameter like the power method an we didn't compute an exact eigenvector name.",
                    "label": 0
                },
                {
                    "sent": "We didn't.",
                    "label": 0
                },
                {
                    "sent": "We didn't want a million steps of the power method.",
                    "label": 0
                },
                {
                    "sent": "Get the exact language and we ran for three steps or 10 steps or something that had to do with the size scale over which we did.",
                    "label": 0
                },
                {
                    "sent": "The local local clustering.",
                    "label": 0
                },
                {
                    "sent": "How can we understand, in some principled sense, what regularization is going on there?",
                    "label": 0
                },
                {
                    "sent": "Are we exactly solving some optimization problem?",
                    "label": 0
                },
                {
                    "sent": "We approximately solving some optimization problem and then let's get beyond sort of VC bounds.",
                    "label": 0
                },
                {
                    "sent": "Can we use these ideas in bad environments?",
                    "label": 0
                },
                {
                    "sent": "Because when you start here and you defuse a bit, it turns out you're computing a vector an eigenvector.",
                    "label": 0
                },
                {
                    "sent": "It's an approximate eigenvector for certain modified problem and all the masses localized here.",
                    "label": 0
                },
                {
                    "sent": "So it's not the high degree node of the small cluster thing that I mentioned before, but you're computing an eigenvector and approximately convective some modified graph.",
                    "label": 1
                },
                {
                    "sent": "So you have localization properties that are bad, exactly as I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "Can you get beyond VC bounds?",
                    "label": 0
                },
                {
                    "sent": "And in particular maybe use ideas having to do with this distribution to get better learning results.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me mention these three results in a little bit of detail so.",
                    "label": 0
                },
                {
                    "sent": "Lessons learned on the local and global clustering properties of these datasets.",
                    "label": 1
                },
                {
                    "sent": "Oftentimes, you'll have good clusters near a particular set of nodes.",
                    "label": 1
                },
                {
                    "sent": "The Vancouver flower market, but no meaningful global clusters.",
                    "label": 1
                },
                {
                    "sent": "Can you meaningfully pull those out in some sense on approximate computation and implicit regularization?",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a universe of easy to state problems.",
                    "label": 0
                },
                {
                    "sent": "L1 regular Sale, 2, exactly optimized conductance, whatever.",
                    "label": 0
                },
                {
                    "sent": "There's a universe of easy to say algorithms.",
                    "label": 0
                },
                {
                    "sent": "Three steps of the power method, whatever you know, you make some.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you Ristic truncation, do these things overlap at all?",
                    "label": 0
                },
                {
                    "sent": "If you're running these things with practitioners, actually do what do you actually optimizing, and can you cut and paste these things together?",
                    "label": 0
                },
                {
                    "sent": "What a lot of people do is specify some objective function.",
                    "label": 0
                },
                {
                    "sent": "A priority might even be nonconvex, but even if it's convex, it might be hard to work with.",
                    "label": 0
                },
                {
                    "sent": "So what's the interplay here so that I'll give you an example and answer that in one very special case, and on learning and inference in high variability data in terms of this issue about eigenvector localization and VC bounds?",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example would be the following.",
                    "label": 0
                },
                {
                    "sent": "So I said the local methods will be probably good versions of the global spectral.",
                    "label": 1
                },
                {
                    "sent": "Spielman Tang Anderson changlang.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of versions with local random walks.",
                    "label": 1
                },
                {
                    "sent": "Some locally biased, a personalized version of page rank.",
                    "label": 0
                },
                {
                    "sent": "An approximate heat kernel and the idea is you start here.",
                    "label": 0
                },
                {
                    "sent": "You defuse a bit and you get some local cluster.",
                    "label": 0
                },
                {
                    "sent": "The best partition of the road network in the US might be here, but around this city at a certain size scale, that's the best one.",
                    "label": 0
                },
                {
                    "sent": "And although these graphs don't have a nice global embedding like the road map might, locally it pull something out that might be meaningful.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Recall the basic spectral partitioning problem that I had before.",
                    "label": 1
                },
                {
                    "sent": "Minimize X transpose LX that you're perpendicular to the all ones vector or so you have a variance constraint in perpendicular lines vector.",
                    "label": 1
                },
                {
                    "sent": "This is a relaxation of this combinatorial thing that's intractable.",
                    "label": 0
                },
                {
                    "sent": "You can solve this via an eigenvalue problem.",
                    "label": 1
                },
                {
                    "sent": "And you get a vector and a sweep cut of that eigenvector yields this thing that you know you bound below and above with the quadratic factor, so this is Cheeger and you don't need an exact eigenvector.",
                    "label": 1
                },
                {
                    "sent": "Any Mahal has a variant where you can sweep cut any vector whose Rayleigh quotient is about right, so you never need an exact eigenvector.",
                    "label": 0
                },
                {
                    "sent": "All you need is one that's approximate in the sense of the Rayleigh quotient being about right.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want to do is come up with an ansatz, basically for an optimization problem that these things might be solving.",
                    "label": 0
                },
                {
                    "sent": "So, given a cut, let's define this vector.",
                    "label": 0
                },
                {
                    "sent": "So this isn't exactly an indicator vector S for the for the set T, but think of it, it's sort of like an indicator vector.",
                    "label": 0
                },
                {
                    "sent": "It's something like that projected away from the all ones vector.",
                    "label": 0
                },
                {
                    "sent": "So SMT is a vector that is something like an indicator vector for that cut T and we can use this vector to define what I'll call a geometric notion of correlation between cuts.",
                    "label": 1
                },
                {
                    "sent": "So this vector dot into the lens vector I said was zero.",
                    "label": 0
                },
                {
                    "sent": "Which is, why isn't isn't exactly indicator vector.",
                    "label": 0
                },
                {
                    "sent": "I've normalized and so the dot product into itself is 1.",
                    "label": 0
                },
                {
                    "sent": "An SU dot into St is some correlation thing that if I wrote down would look to you like a correlation condition.",
                    "label": 0
                },
                {
                    "sent": "And so given the way page rank is usually told, you have some guy diffusing around and you teleport and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "But effectively it's going to have a structure like this.",
                    "label": 0
                },
                {
                    "sent": "So let's say given a graph G, Anna, number Alpha and any vector assets that the seed set.",
                    "label": 0
                },
                {
                    "sent": "Basically it's a function on the nodes.",
                    "label": 0
                },
                {
                    "sent": "It doesn't need to be localized, will be using it, but it's a function on the nodes.",
                    "label": 0
                },
                {
                    "sent": "Let's define a generalized personalized Pagerank vector to be vector of the following form Laplacian of your original graph minus Laplacian.",
                    "label": 0
                },
                {
                    "sent": "The complete graph pseudo inverted dot into D. Dundas, so the usual story about the random Walker, the teleports around, and that's what this thing is.",
                    "label": 0
                },
                {
                    "sent": "It may not be obvious, but that's what this thing is, and I'm calling it generalized because we can now allow negative values of the teleportation.",
                    "label": 0
                },
                {
                    "sent": "I guess positive values of Alpha the way I've written it, so it's more general than the usual notion, but it's a straightforward extension.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So we have for a set of nodes you define the geometric notion of coral.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "And here's an ansatz for local spectral partitioning.",
                    "label": 0
                },
                {
                    "sent": "Minimize X transpose LX such that the variance condition satisfied it didn't write down your also perpendicular to the ones vector and that X dot into S is greater than Kappa.",
                    "label": 0
                },
                {
                    "sent": "So the DOT product between X, the vector you're looking for and the seed set of nodes is pretty high, so we're minimizing the same thing, except that we need to be well correlated the seed set.",
                    "label": 0
                },
                {
                    "sent": "So there's a dual.",
                    "label": 0
                },
                {
                    "sent": "The dual usually is Max Alpha such that LG you can sort of sit on top of the complete graph.",
                    "label": 0
                },
                {
                    "sent": "Now you get another term that has to do with the complete graphs on the T and the T bar, and you don't want to cut through TNT bar, which is depending on how big data is.",
                    "label": 0
                },
                {
                    "sent": "That's going to be more or less important, so the interpretation is here that you're going to find a cut that's well correlated.",
                    "label": 0
                },
                {
                    "sent": "The seed vector S, and in particular.",
                    "label": 1
                },
                {
                    "sent": "If S is a single node, you relaxing this quantity.",
                    "label": 1
                },
                {
                    "sent": "Alright, so in the same way as you sort of embed the graph in a complete graph, that's what's going on under the hood when you do global spectral.",
                    "label": 0
                },
                {
                    "sent": "This is what's going on here.",
                    "label": 0
                },
                {
                    "sent": "When you're doing local spectral trade.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of these two things here, so theorem, if X is an optimal solution to this program.",
                    "label": 1
                },
                {
                    "sent": "Then it's a generalized personalized Pagerank vector for the parameter Alpha and it can be computed as a set of linear equations.",
                    "label": 1
                },
                {
                    "sent": "So I've written down an ugly program, but the solution you can solve a set of linear equations since it's a GPR vector.",
                    "label": 1
                },
                {
                    "sent": "The proof is relatively straightforward.",
                    "label": 0
                },
                {
                    "sent": "You relax a nonconvex problem to convex SDP.",
                    "label": 0
                },
                {
                    "sent": "Strong duality holds here.",
                    "label": 0
                },
                {
                    "sent": "The solution is going to be rank one from complementary slackness, and if you have a rank one matrix, that's a vector basically, and so that's that vector.",
                    "label": 0
                },
                {
                    "sent": "So you can compute the solution.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quickly and in addition, you get upper and lower bounds that are cheaper like so I'm not going to go through the details of that, but you get up on lower bounds that are Chiga like.",
                    "label": 0
                },
                {
                    "sent": "So this is the flavor of the results that Spielman, Tang and Chong Ling get, and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "This vector is still global in the sense that it touches all the nodes, the operational steps they have, sort of a hard truncation.",
                    "label": 0
                },
                {
                    "sent": "That will introduce sort of an additional.",
                    "label": 0
                },
                {
                    "sent": "An additional form of 1 type regularization that I didn't get into 'cause we don't have formalized, but the idea here is that we have an optimization form that these local spectral things are.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimizing.",
                    "label": 0
                },
                {
                    "sent": "So I told you we did all this stuff.",
                    "label": 0
                },
                {
                    "sent": "A million node graphs.",
                    "label": 0
                },
                {
                    "sent": "Here's a 300 or 600 node graph where I want to show a pretty picture just to sort of give you show you roughly how it behaves if we start with seed nodes here and run it a little bit depending on where you want to cut, you can cut the red.",
                    "label": 0
                },
                {
                    "sent": "You can cut the orange, you can cut the ground.",
                    "label": 0
                },
                {
                    "sent": "You can cut the yellow.",
                    "label": 0
                },
                {
                    "sent": "And if you start with the set of nodes out here, you can cut in the same way at a range of different levels.",
                    "label": 0
                },
                {
                    "sent": "And we saw the profile plots before.",
                    "label": 0
                },
                {
                    "sent": "This is a local version of the profile plots, and although usually you'd.",
                    "label": 0
                },
                {
                    "sent": "I think of the seed vectors is localized in a small number of nodes.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It can be any function on the nodes, so in particular it could be a function that's correlated with high degree nodes or correlated with low degree nodes, and so if you think that you want to find a soft version of of a cut that's correlated with high degree nodes or anti Corley Corley low degree nodes, you enter into that into your vector S you run that machinery and you get something like this that you can cut at the red or the Brown or the yellow.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this is what these local spectral methods are doing.",
                    "label": 0
                },
                {
                    "sent": "They're optimizing the modified objective like that form.",
                    "label": 0
                },
                {
                    "sent": "But but as I said, you know the way I formulated that I'm computing an exact eigenvector with those methods are doing is computing approximate eigenvector 'cause they run a few steps of the page rank or a few steps of the heat kernel?",
                    "label": 0
                },
                {
                    "sent": "They don't want it forever.",
                    "label": 0
                },
                {
                    "sent": "And so in a lot of situations in linear algebra, machine learning, and so on, these will involve approximate computations.",
                    "label": 1
                },
                {
                    "sent": "Power method truncate apartment.",
                    "label": 0
                },
                {
                    "sent": "The heat kernel truncated version truncated page rank diffusion kernels, interest rate concern, and so forth, and often they'll come with some sort of generative story about random surfers and so on, but you know you might ask what are these procedures actually computing 'cause if you want to start cutting and pasting the spectral and flow things to probe these large graphs in a more principled way, or absolutely more refined, the 10,000 foot view that I just mentioned answering these questions that you'll need to be able to do, and that's not something that's historically been done in.",
                    "label": 1
                },
                {
                    "sent": "Linear algebra, other area.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's involved.",
                    "label": 0
                },
                {
                    "sent": "There's nothing new here.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's also the classical linear algebra optimization ideas, but you sort of reformulating.",
                    "label": 0
                },
                {
                    "sent": "I'm in this sort of new directions to ask based on these sorts of applications, so regularization is a general method for computing a smoother or nicer and more regular solution, and it's useful for inference in a range of other things.",
                    "label": 1
                },
                {
                    "sent": "And regularization is usually implemented by adding some sort of regularization penalty.",
                    "label": 0
                },
                {
                    "sent": "So if I want to optimize F of X, you decide a priority on some G of X.",
                    "label": 0
                },
                {
                    "sent": "So that could be a Euclidean norm regression and you put an L1L2 or something there.",
                    "label": 0
                },
                {
                    "sent": "So you minimize F of X plus Lambda G of X.",
                    "label": 0
                },
                {
                    "sent": "An empirical observation is lots of times heuristics like binning or early stopping.",
                    "label": 0
                },
                {
                    "sent": "You know on this, or conjugate gradients or variants of this.",
                    "label": 0
                },
                {
                    "sent": "You often do better in practice, where better means more useful, smoother, nicer.",
                    "label": 0
                },
                {
                    "sent": "For some downstream analyst.",
                    "label": 1
                },
                {
                    "sent": "So the question is can we can approximate computations implicitly lead to more regular solutions?",
                    "label": 0
                },
                {
                    "sent": "Because I told you we're starting with tractable graph partitioning problem, we're filtering through two different places depending on whether we're doing spectral or flow.",
                    "label": 0
                },
                {
                    "sent": "Can we understand the extent to which the nature of the approximate computation itself is implicitly regularising?",
                    "label": 0
                },
                {
                    "sent": "So we have a lot of evidence how this can be done for graph algorithms, but I'm going to talk about one particular clean example of the matrix algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's computing an eigenvector, so three common procedures are heat kernel, which you can formally write this way.",
                    "label": 1
                },
                {
                    "sent": "And paid rank which you can formally right this way and a Q step lazy random walk which you can formally right this way.",
                    "label": 0
                },
                {
                    "sent": "So if you stop after three steps.",
                    "label": 0
                },
                {
                    "sent": "You aren't exactly computing the vector, but can you make?",
                    "label": 0
                },
                {
                    "sent": "Can you say in a very precise sense that this approximation procedure?",
                    "label": 0
                },
                {
                    "sent": "This truncated thing exactly optimizes some regularised objective, so the Rayleigh quotient is what you get when you run this thing forever.",
                    "label": 1
                },
                {
                    "sent": "Are you implicitly optimizing some regularised objective here?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a vector program we saw before and a regularised version of this vector program would be this thing.",
                    "label": 0
                },
                {
                    "sent": "Minimize X, transpose X plus Lambda F of X. Alright, this vector program, it turns out, is equivalent to an SDP semidefinite program.",
                    "label": 0
                },
                {
                    "sent": "And this semidefinite program is of the form minimize L dotted into X when X is now capital X. X is an S PSD matrix such that the Laplacian Dot in the Capital X is one that generalizes this thing.",
                    "label": 0
                },
                {
                    "sent": "You don't need to worry about the rank.",
                    "label": 0
                },
                {
                    "sent": "There will be a rank one solution to this and that rank.",
                    "label": 0
                },
                {
                    "sent": "One solution is the same as that, so the rank one solution to this.",
                    "label": 0
                },
                {
                    "sent": "And that rank one solution to Capital X is lower case X, lower case X transpose in that lower case X is that.",
                    "label": 0
                },
                {
                    "sent": "And you can regularize this and tack on some Lambda capital F of X.",
                    "label": 0
                },
                {
                    "sent": "Now clearly when you regularize, you might not be rank one, but you can do this.",
                    "label": 0
                },
                {
                    "sent": "So it might be believable that if you've done three steps of the power method, you haven't washed out the rest of the residual space, so it might be hard to write it or not possible to write it.",
                    "label": 0
                },
                {
                    "sent": "The regularization in this way as a function of that vector.",
                    "label": 0
                },
                {
                    "sent": "But here we have a lot of other information.",
                    "label": 0
                },
                {
                    "sent": "He would have the correlation matrix and we haven't washed out the full residual space.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a here's a theorem.",
                    "label": 0
                },
                {
                    "sent": "That let's define the F, ADA semidefinite program.",
                    "label": 0
                },
                {
                    "sent": "To be basically the same thing we had before, minimize L dot X such that you know I got XI is the same as that Laplacian thing I had on the previous page.",
                    "label": 0
                },
                {
                    "sent": "In X is greater than zero, but we tack on ADA 1 / 8 F of X. Alright, so modification of the usual SDP to have regularization.",
                    "label": 0
                },
                {
                    "sent": "But on the correlation matrix X, not on the vector X. Theorem If G is the.",
                    "label": 1
                },
                {
                    "sent": "A graph with Losian al.",
                    "label": 0
                },
                {
                    "sent": "Then these following conditions are sufficient.",
                    "label": 0
                },
                {
                    "sent": "For ex.",
                    "label": 0
                },
                {
                    "sent": "Start to be an optimal solution this regularizer program.",
                    "label": 0
                },
                {
                    "sent": "And the conditions are the text art takes this particular form and that you have these two things.",
                    "label": 0
                },
                {
                    "sent": "Alright, once you have this result.",
                    "label": 0
                },
                {
                    "sent": "We have Dell F inverse.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then it's a relatively immediate corollary that if you happen to choose F of X to be a generalized entropy, then you get the scaled heat kernel with T equals ETA.",
                    "label": 1
                },
                {
                    "sent": "If you happen to choose F of X equals logdet, this gives you the page rank.",
                    "label": 0
                },
                {
                    "sent": "And if you choose F of X to be certain matrix P norm, you get this truncated iterated lazy random walk.",
                    "label": 0
                },
                {
                    "sent": "Alright, now you can solve the SDP for any value of the regularization parameter you like by calling a black box solver, or you can call it for a particular set of discrete values that happens to correspond to one Step 2 step, three steps, and if you choose to solve it for those particular set of values, you can solve the SDP by doing 3 steps of the power method or of a truncated lazy random walk, or or of a approximate personalized rate page rank.",
                    "label": 0
                },
                {
                    "sent": "So these approximation algorithms that we were using to tear these graphs apart and probe their structure, these approximation algorithms are basically computing regularised versions of the Fiedler vector.",
                    "label": 1
                },
                {
                    "sent": "And doing so very quickly so we haven't act on any regularization terms, but by the steps of the approximation algorithms we.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Implicitly implementing that regularization so that holds true.",
                    "label": 0
                },
                {
                    "sent": "More generally, I think for graph approximations we haven't formalized that, so that's actually working on and you should know that a lot of work on large scale data applications sort of already uses this stuff implicitly, so there's a range of papers.",
                    "label": 1
                },
                {
                    "sent": "Here is just three.",
                    "label": 0
                },
                {
                    "sent": "That sort of informed our understanding.",
                    "label": 1
                },
                {
                    "sent": "We're thinking about this, but there's a bunch of ones where you know you do random walks on, say, a query quick graph in order to generate some keywords, and you need to be pretty careful about the seed set and the size scale of which you operate, and so on.",
                    "label": 0
                },
                {
                    "sent": "So these methods are a little bit finicky because they're basically doing a spectral thing.",
                    "label": 0
                },
                {
                    "sent": "They're doing a diffusion because they're doing a random walk.",
                    "label": 0
                },
                {
                    "sent": "And if you start to touch high degree nodes, you're dead.",
                    "label": 0
                },
                {
                    "sent": "Right, so you might ask, could I do a local diffusion and then couple that with local spectral?",
                    "label": 0
                },
                {
                    "sent": "A local sort of flow sort of thing to make these a little bit more robust?",
                    "label": 0
                },
                {
                    "sent": "So that's something we don't have a final answer too, but the answer is probably yes, and we're thinking about that.",
                    "label": 0
                },
                {
                    "sent": "So more generally, can you formalize when these sorts of heuristics that people actually do work or fail for matrix a graph approximation albums if you want to start cutting and pasting these things together?",
                    "label": 0
                },
                {
                    "sent": "One way is to specify an objective and then trying to approximate it a different way to say what are people actually doing on very large scale graphs and understand the implicit if.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Acts of what they're doing.",
                    "label": 0
                },
                {
                    "sent": "So let me wrap up with the third example having to do with the classification bounds so supervised binary classification you observe XY samples from some unknown distribution you want to construct a classifier Alpha drawn from some family Lambda.",
                    "label": 1
                },
                {
                    "sent": "So for example hyperplanes after you see some number K samples question, how big must K be in order to get good prediction in order to get low error.",
                    "label": 1
                },
                {
                    "sent": "So you define the risk.",
                    "label": 1
                },
                {
                    "sent": "You don't know what that is.",
                    "label": 0
                },
                {
                    "sent": "You define an empirical risk, which is the risk on the observed data, and we want to bound these two.",
                    "label": 0
                },
                {
                    "sent": "And so one way to do it is to bound the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "And that's really the most well known.",
                    "label": 0
                },
                {
                    "sent": "And it's nice 'cause its distribution independent a second way and not the only other way.",
                    "label": 1
                },
                {
                    "sent": "But Secondly, it's bound was known as the annealed entropy.",
                    "label": 0
                },
                {
                    "sent": "So this will depend on the distribution of the data is drawn from, so it's less general in that sense.",
                    "label": 0
                },
                {
                    "sent": "But in a lot of cases you get much finer balance so.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sample complexity of distribution free learner depends on the ambient dimension variance.",
                    "label": 1
                },
                {
                    "sent": "Satisfactory for formerly high dimensional data, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "The annealed entropy is this sort of expression.",
                    "label": 0
                },
                {
                    "sent": "I'm just I'm not going to explain, I'm just going to write it down to sort of show you that make you believe that it exists, hopefully, but the point is, you have an expectation of the distribution.",
                    "label": 0
                },
                {
                    "sent": "There is not just accounting thing, but you have an expectation of the distribution here.",
                    "label": 0
                },
                {
                    "sent": "And the theorem is that you know the risk minus empirical risk you can bound in terms of annealed entropy.",
                    "label": 0
                },
                {
                    "sent": "So if you're familiar with the bounds, the structure of that claim is very similar to what you see.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "BC bounds and that will hold for some number of samples.",
                    "label": 0
                },
                {
                    "sent": "Allen any error parameter epsilon so distribution independent sample complexity bounds for high variability environment and what I'll say is approximately low dimensional environments, meaning you're not exactly on a low dimensional space, but you have noise which you shouldn't even think of his Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It might be very, very bad, so think of you doing LSI, so you want there to be 10 dimensions.",
                    "label": 0
                },
                {
                    "sent": "But there's really heavy tailed noise, very bad variability, data, high variability environments, more generally.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to say Tord learning on informatics graphs 'cause we really want to do here, is think about the graph as a single.",
                    "label": 0
                },
                {
                    "sent": "Piece of data.",
                    "label": 0
                },
                {
                    "sent": "You know this is not these aren't feature vectors.",
                    "label": 0
                },
                {
                    "sent": "We can say it's feature vectors and every you know every node links to every other node.",
                    "label": 0
                },
                {
                    "sent": "To find that to be a feature vector.",
                    "label": 0
                },
                {
                    "sent": "But in a lot of ways more meaningful to think about the graph as a single entity operate on it.",
                    "label": 0
                },
                {
                    "sent": "So a lot of this learning is formulated in vector spaces, and so you know you're not directly on graphs, and the particular statement of the results we have here wouldn't be immediately available for the graphs, but they're in vector space analogs of that, so it'll be tored learning information for mixcraft, so we'd like to do it directly on a graph.",
                    "label": 0
                },
                {
                    "sent": "High variability environments.",
                    "label": 0
                },
                {
                    "sent": "Let's say the probability that the feature is non zero decays as a power law.",
                    "label": 0
                },
                {
                    "sent": "Or the magnitude of the feature decays.",
                    "label": 0
                },
                {
                    "sent": "A power law, and so we can slightly different results.",
                    "label": 0
                },
                {
                    "sent": "Exact learning or gap calling depending on how exactly quantify it.",
                    "label": 0
                },
                {
                    "sent": "Approximately low dimensional environments William Bounds and say the covering number when using diffusion based kernels and if using diffusion based kernels.",
                    "label": 0
                },
                {
                    "sent": "The idea is that if you really know dimensional space, eigen values are going to be vectors are going to be pretty localized and so good things might happen.",
                    "label": 0
                },
                {
                    "sent": "But if you're doing it on arbitrary graph you might have these hot spots and the hot spots are going to be problematic for you.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Are you the diffusion Maps is that hopefully is Laplacian Eigen Maps is is a variant of it and you might ask when do eigenvectors localized.",
                    "label": 0
                },
                {
                    "sent": "So as I said, the locals and high degree nodes at articulation points between clusters points that stick out a lot very sparse random graphs.",
                    "label": 0
                },
                {
                    "sent": "So this is seen a lot of datasets when eigenvector based methods are chosen for algorithmica not particularly statistical reasons right?",
                    "label": 0
                },
                {
                    "sent": "Use LSI, not because it's really K components, but because the low dimensional space might be implicitly providing some sort of capacity control.",
                    "label": 0
                },
                {
                    "sent": "So you actually see this sort of localization.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this localization, just as an aside, this localization I call them leverage scores before.",
                    "label": 0
                },
                {
                    "sent": "But if you're familiar with stuff on matrix reconstruction and all this sort of stuff, they talk about a notion of coherence and incoherence.",
                    "label": 0
                },
                {
                    "sent": "Coherence and incoherence is sort of an Infinity norm over these leverage scores.",
                    "label": 0
                },
                {
                    "sent": "If the Max of these scores is high, things are very incoherent.",
                    "label": 0
                },
                {
                    "sent": "If the Max of these things is flat, things are coherent, or maybe vice versa when the leverage scores have bad are very when you have bad variability over the leverage scores, things are coherent, which is a bad case for them, and in a wide, wide range of datasets, that's going to be the case, and so the approximation that the data are reasonably coherent is not even approximately satisfied.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, if you have some idea of what the distribution might look like, can you get learning bounds?",
                    "label": 0
                },
                {
                    "sent": "So let P be some sort of probability distribution over RN, and suppose the probability that the value is non 0.",
                    "label": 0
                },
                {
                    "sent": "So the one of the two models I mentioned before decays a power law.",
                    "label": 0
                },
                {
                    "sent": "So think of this as the degree of localization on the eigenvector.",
                    "label": 0
                },
                {
                    "sent": "You there's a small number of components that have a lot of mass and a bunch of components that have less a bunch more than just a lesson.",
                    "label": 0
                },
                {
                    "sent": "A bunch that have very very, very little or none.",
                    "label": 0
                },
                {
                    "sent": "So theorem.",
                    "label": 0
                },
                {
                    "sent": "In this model you can bound the annealed entropy, and I'm not going to go into details except to say that you can do it, and so you'll need L, which is some ugly expression that is independent of D, which could be infinite.",
                    "label": 0
                },
                {
                    "sent": "So the sample complexity finite, even if D is going to be infinite.",
                    "label": 0
                },
                {
                    "sent": "So you can get a theoretical result.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to say that this.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Immediately applicable social graphs, but you can get that sort of bound more generally with gap tolerant classifiers.",
                    "label": 1
                },
                {
                    "sent": "If you say P is a probability measure in Hilbert space and Delta is greater than zero, let the expectation over P under P of the norm of XBR R-squared and let that be less than Infinity.",
                    "label": 1
                },
                {
                    "sent": "Then the annealed entropy of the gap calling classifiers is bounded by this and all the learning results go through.",
                    "label": 0
                },
                {
                    "sent": "Now I want to say the expectation here rather than, say the maximum, and this is the sort of thing that you probably see in a lot of cases people sweep under the rug.",
                    "label": 0
                },
                {
                    "sent": "They say I'm going to have a technical assumption, and the technical assumption is that I'm going to bounce some Max.",
                    "label": 0
                },
                {
                    "sent": "You can always bound to Max.",
                    "label": 0
                },
                {
                    "sent": "But then if the main gets arbitrarily small, you'd have a problem, and it's a technical result in the sense that if you just familiar with manifold and so on, you know it's a technical result.",
                    "label": 0
                },
                {
                    "sent": "But if you're dealing with arbitrarily bad graphs, it's far from a technical result.",
                    "label": 0
                },
                {
                    "sent": "It will render a lot of the results that gloss over this and bound the Max irrelevant for bad very bad variability environments.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This application for learning with spectral in diffusion based kernels, learning with heavy tailed data.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned you have a new proof.",
                    "label": 0
                },
                {
                    "sent": "It actually generalizes to Banach spaces if you're interested in that which wasn't known before.",
                    "label": 1
                },
                {
                    "sent": "But more generally, you know the question is and we don't have answers to these questions.",
                    "label": 0
                },
                {
                    "sent": "Can you control the negative effect of outliers in other data models, or can you do?",
                    "label": 1
                },
                {
                    "sent": "Learning in some sense, if the measure never concentrates the point of this very sparse regime of various Ronnie, is that you're not in some low dimension, plus noise, and you're not in high dimensions or measure concentrates.",
                    "label": 0
                },
                {
                    "sent": "You're exactly at that.",
                    "label": 0
                },
                {
                    "sent": "Pre asymptotic state and that creates some topics that you have very bad variability properties and you see that in a wide range of datasets in 99 plus percent of statistical theory ignores that, and so that's sort of the dominant effect here, that's.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sponsible for the local global issues, so that let me wrap up large informatics graphs you may imagine and believe and not be surprised their important practice there.",
                    "label": 0
                },
                {
                    "sent": "Also, I think important in theory because they starkly illustrate that many common assumptions that people bring to the analysis are very inappropriate, or they're actually very good.",
                    "label": 1
                },
                {
                    "sent": "Hydrogen Atom for I think, method development.",
                    "label": 0
                },
                {
                    "sent": "You should think of the data as little local pockets of structure on top of a sparse quasirandom global scaffolding.",
                    "label": 0
                },
                {
                    "sent": "So empirical noise.",
                    "label": 0
                },
                {
                    "sent": "Let's call it, and I think that's very significant implications for clustering community detection.",
                    "label": 0
                },
                {
                    "sent": "And the use of a lot of machine learning and data analysis tools more generally.",
                    "label": 0
                },
                {
                    "sent": "And so I gave you a couple of particular examples and I could talk offline about a bunch of others, but these are examples that we sort of work through in detail, so I think there's a lot of other examples where sort of if you look under the hood in terms of what is done numerically, eigenvector localization or other sort of very basic ideas that people usually assume away.",
                    "label": 0
                },
                {
                    "sent": "If you look under the hood, there's a lot of new directions for.",
                    "label": 0
                },
                {
                    "sent": "Numerical and matrix based methods and machine learning so that let me wrap up and thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "Adam Walker where do you start?",
                    "label": 0
                },
                {
                    "sent": "If you start to me.",
                    "label": 0
                },
                {
                    "sent": "Right, so where do you start the random walk when you start a random walk?",
                    "label": 0
                },
                {
                    "sent": "For those, it depends what you want to do.",
                    "label": 0
                },
                {
                    "sent": "If you want to find the Vancouver flower market, you should start with seed nodes in Vancouver, not in Zimbabwe, right?",
                    "label": 0
                },
                {
                    "sent": "If you want to compute the plots we computed, you start with seed nodes at all sorts of different places, and you look at a bunch of different size skills, and so if you're going to look at it at a small size scale, then do 2 steps or three steps, you might say that Vancouver and Seattle markets are different.",
                    "label": 0
                },
                {
                    "sent": "If you're going to look at 10 size scales, then you might say, well, they never mind.",
                    "label": 0
                },
                {
                    "sent": "This is an underlying geometry to this to the Earth, but you'd see similar things as you get more expanded like that if you start here and here and you defuse around a bit on small size scales will be different at larger size scales you might believe they're going to be about the same, and so you could just start with one of 'em.",
                    "label": 0
                },
                {
                    "sent": "And so when we computed that particular plot, we started with a bunch of different places, bunch of different size scales, trying not to naively have sort of gratuitous computation there.",
                    "label": 0
                },
                {
                    "sent": "And then took the minimum.",
                    "label": 0
                },
                {
                    "sent": "But if you want to ask more fine questions like what's going on around here, also other things.",
                    "label": 0
                },
                {
                    "sent": "You could do that too, so that really depends on the question you want to ask.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for the local spectral partitioning, so you are really solve the problem by the SDP or using some other powermaster or type skill.",
                    "label": 0
                },
                {
                    "sent": "I mean really screw up.",
                    "label": 0
                },
                {
                    "sent": "So what we do is put mass here.",
                    "label": 0
                },
                {
                    "sent": "We actually used to pay drank version because it's certain better results, but I'll describe the diffusion version because we've tried that to put your mess here.",
                    "label": 0
                },
                {
                    "sent": "Diffusa step diffuse 2 steps if the probability is less than something, truncate, diffuse another step and iterate that process a few times.",
                    "label": 0
                },
                {
                    "sent": "That's what you actually do.",
                    "label": 0
                },
                {
                    "sent": "Approximation so yeah, so we never touch an SDP.",
                    "label": 0
                },
                {
                    "sent": "Those SDP's are what we actually did.",
                    "label": 0
                },
                {
                    "sent": "If you want to know, sort of what we were optimizing that.",
                    "label": 0
                },
                {
                    "sent": "So we were optimizing, but we optimized it by doing these few steps.",
                    "label": 0
                },
                {
                    "sent": "So that's the key point that this is a massive graph.",
                    "label": 0
                },
                {
                    "sent": "This SDP is going to be hopeless.",
                    "label": 0
                },
                {
                    "sent": "Yeah, cancel that SDP in a million node graph for 10 million node graph.",
                    "label": 0
                },
                {
                    "sent": "But you know this is effectively what you're optimizing when you run these operational procedures very locally.",
                    "label": 0
                },
                {
                    "sent": "Then can we say how far you are from this open optimum of this SDP?",
                    "label": 0
                },
                {
                    "sent": "After these three steps?",
                    "label": 0
                },
                {
                    "sent": "So there's a couple.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's the truncation issue, which I did in the direction that regularization, but it can be, but we haven't done it in this cleaner format, but that's what's going on there.",
                    "label": 0
                },
                {
                    "sent": "There's a particular way stated it was in terms of a global thing, but it will extend the local.",
                    "label": 0
                },
                {
                    "sent": "So modulo those two, that SDP is exactly what was going on with the methods.",
                    "label": 0
                },
                {
                    "sent": "Well, also I had another comment, maybe I misunderstood, you know, suppose you run just three iterations of some method.",
                    "label": 0
                },
                {
                    "sent": "Yeah, whatever you get after three iterations could be the solution to potentially an infinite number of different problems.",
                    "label": 0
                },
                {
                    "sent": "Why do I look at the particular ones that say, yeah, I mean, it could be a solution.",
                    "label": 0
                },
                {
                    "sent": "I mean the solution to spectral.",
                    "label": 0
                },
                {
                    "sent": "You can write as a vector program, but you can also write it as this SDP programs.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's probably other ones, yeah, so but when you do three steps of spectral, you're solving the STB for a particular value of the regularization parameter an.",
                    "label": 0
                },
                {
                    "sent": "If you do 4 steps.",
                    "label": 0
                },
                {
                    "sent": "You're solving it for a different value, and if you do 5 steps you solving for different value.",
                    "label": 0
                },
                {
                    "sent": "So that's my question comes back to, you know, because you can have potentially an infinite number of optimization problems.",
                    "label": 0
                },
                {
                    "sent": "Let us solve by these three steps what inside you know when I do three steps, I solve a particular when I do three steps.",
                    "label": 0
                },
                {
                    "sent": "I solve a particular SDP for.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ocular value of the of the regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "The simplest one is heat kernel, where there's inequality, the other, the other two are little bit more complicated, but you know when I do three steps of this heat kernel thing.",
                    "label": 1
                },
                {
                    "sent": "You know it's that's the value of the regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "If I chose to do 4 steps.",
                    "label": 0
                },
                {
                    "sent": "That will be the value of the regularization parameter, so it's not like there's a bunch of I'm not solving a bunch of different things.",
                    "label": 0
                },
                {
                    "sent": "I'm selling this particular STP.",
                    "label": 0
                },
                {
                    "sent": "I could have chosen a regularization parameter in between those two and called an SDP solver.",
                    "label": 0
                },
                {
                    "sent": "That would be a hard thing to do.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what is the what is the problem that I'm actually solving because we told these graphs apart and a lot of intelligent heuristic things.",
                    "label": 0
                },
                {
                    "sent": "If you want to ask more refined questions about these graphs, like doing the keyword expansion in the diffusion, you understanding what you're actually doing when you introduce these structured approximations is going to be important.",
                    "label": 0
                },
                {
                    "sent": "If you want to sort of.",
                    "label": 0
                },
                {
                    "sent": "Richer analytics on large scale graphs.",
                    "label": 0
                },
                {
                    "sent": "So the question was what are we solving when we do certain things were not saying I want to give some complicated objective and hope that I can solve that.",
                    "label": 0
                },
                {
                    "sent": "The question is I have a few sort of primitives that I know how to solve and I cut and paste them in various ways and what I'm actually solving when I do that.",
                    "label": 0
                },
                {
                    "sent": "Even better, I mean, if you thought about there are many optimization methods today, whatever they call.",
                    "label": 0
                },
                {
                    "sent": "Problems that we can't really solve in every step in every step, right?",
                    "label": 0
                },
                {
                    "sent": "Every step and SVD.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well if I only run many steps of my power method, I can tell you what you're really doing.",
                    "label": 0
                },
                {
                    "sent": "You then take that together and tell me what's the iterative method around it really doing?",
                    "label": 0
                },
                {
                    "sent": "I mean, for example, doesn't.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Can't tell you yet.",
                    "label": 0
                },
                {
                    "sent": "So the question is if I paste things together and this is one step of a bigger machine where I'm doing iterations on, can I tell you what that bigger machinery is solving?",
                    "label": 0
                },
                {
                    "sent": "I suspect the answer is yes, and we're thinking about that, but I don't have the answer yet.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you run something and you sell is convex, and then you do something in between, it might just destroy convexity.",
                    "label": 0
                },
                {
                    "sent": "Yeah it could, yeah.",
                    "label": 0
                },
                {
                    "sent": "Safeway only running steps.",
                    "label": 0
                },
                {
                    "sent": "Possibly, but usually when you do these things here, you're smoothing something out your regularising, so I would guess that I don't know.",
                    "label": 0
                },
                {
                    "sent": "I would guess that things wouldn't go pathologically bad like that, but.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you can find an example where they do, but in a lot of cases they wouldn't, so I don't think it's so much that you're going to preserve convexity as to scale up things that you would be too small to do otherwise.",
                    "label": 0
                },
                {
                    "sent": "These results dependent on what the matrix is very strongly or no.",
                    "label": 0
                },
                {
                    "sent": "I mean, the matrix is is.",
                    "label": 0
                },
                {
                    "sent": "This is the problem solving.",
                    "label": 0
                },
                {
                    "sent": "So the matrix enters there.",
                    "label": 0
                },
                {
                    "sent": "Oh so the results are pending on the Matrix, except that enters into the the penalty, but the regularization term it doesn't depend on that.",
                    "label": 0
                },
                {
                    "sent": "Now if the matrix is more or less than when you're doing the page rank, you diffusing around your hopping.",
                    "label": 0
                },
                {
                    "sent": "If it's a global page rank of its local when you're doing the same thing to a local spot, you're hopping anywhere.",
                    "label": 0
                },
                {
                    "sent": "So you might imagine you're doing a very global sort of regularization, so the logged it should have a different structure than the generalized entropy, or this matrix P norm.",
                    "label": 1
                },
                {
                    "sent": "Because with these two things are doing is a very local thing.",
                    "label": 0
                },
                {
                    "sent": "You know just diffusing around and maybe you're truncating a little bit, whereas the page rank because of the hop you doing much more global thing, so that particular particular regularization function will illustrate that difference.",
                    "label": 0
                },
                {
                    "sent": "Metaphor?",
                    "label": 0
                },
                {
                    "sent": "Basically a few steps.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so historically the there was a couple different variants of what people did.",
                    "label": 0
                },
                {
                    "sent": "They are pretty close and the page rank actually forms better on the large graphs because of the global.",
                    "label": 0
                },
                {
                    "sent": "Because of the range of reasons, but the was historically done was well, I don't have it here, but.",
                    "label": 0
                },
                {
                    "sent": "You know this this Spielman Tang the Andersen Chung Chung.",
                    "label": 0
                },
                {
                    "sent": "We're doing 3 versions of basically these three.",
                    "label": 0
                },
                {
                    "sent": "So we just wanted we could we could.",
                    "label": 0
                },
                {
                    "sent": "We could derive this result for all three and not just too.",
                    "label": 0
                },
                {
                    "sent": "So we did it for all three, but but these two are going to be very similar because it has to do with the formula has to do with the way you know you arrange the some of the terms.",
                    "label": 0
                },
                {
                    "sent": "You know how they decay implementation alee.",
                    "label": 0
                },
                {
                    "sent": "This performs worse than that.",
                    "label": 0
                },
                {
                    "sent": "We've never implemented that advice.",
                    "label": 0
                },
                {
                    "sent": "We've implemented that in that.",
                    "label": 0
                },
                {
                    "sent": "Not that, but yeah, that probably is a sort of very similar to that, so I wouldn't want to.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't know a practical example of that.",
                    "label": 0
                },
                {
                    "sent": "Matters, I mean they might.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you could construct one, but certainly in the situations we've looked at, those tool gives similar results, I suspect, so the point isn't that I want it.",
                    "label": 0
                },
                {
                    "sent": "I mean, they're not very different things, it's just that.",
                    "label": 0
                },
                {
                    "sent": "You know that you're doing slightly different things and you can Venn diagrams of easy to state problems and easy to state algorithms.",
                    "label": 0
                },
                {
                    "sent": "Three steps of any of these you know overlaps here in at least one case, and we can make it precise statement about the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "These particular procedures are optimizing, but yeah.",
                    "label": 0
                },
                {
                    "sent": "All messages for me.",
                    "label": 0
                },
                {
                    "sent": "I mean, I have to run it basically until roughly process mixing and then I can converse.",
                    "label": 0
                },
                {
                    "sent": "But if you did three steps, what would you be doing?",
                    "label": 0
                },
                {
                    "sent": "Well basically report iterative.",
                    "label": 0
                },
                {
                    "sent": "OK then I'll call it that too.",
                    "label": 0
                },
                {
                    "sent": "If you like.",
                    "label": 0
                },
                {
                    "sent": "I mean that's not a debate.",
                    "label": 0
                },
                {
                    "sent": "I mean that's not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean call those two types of random walks.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't, that's you can call that yeah, so just coming back to my point nonuniqueness, if you go back one slide.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "That you write.",
                    "label": 0
                },
                {
                    "sent": "I can.",
                    "label": 0
                },
                {
                    "sent": "Because it's on the form, you know something plus something.",
                    "label": 0
                },
                {
                    "sent": "So the reason it's not unique.",
                    "label": 0
                },
                {
                    "sent": "I can add something to the first term and subtract it up in the second term I have.",
                    "label": 0
                },
                {
                    "sent": "Different problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so there may be that sort of non uniqueness, yeah?",
                    "label": 0
                },
                {
                    "sent": "Sort of a gauge issue or something?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that may be the case.",
                    "label": 0
                },
                {
                    "sent": "So let this or any of it's just, but I think we can unify some.",
                    "label": 0
                },
                {
                    "sent": "Who is local?",
                    "label": 0
                },
                {
                    "sent": "Please run as a local version of the first eigenvalue or there are also local versions of the second and third.",
                    "label": 1
                },
                {
                    "sent": "OK, so don't think of it as a local version of the first eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "'cause everyone talks about local eigenvalues and eigen vectors.",
                    "label": 0
                },
                {
                    "sent": "Eigenvectors sometimes localized, but sometimes they're not, and if they're not, there is not.",
                    "label": 0
                },
                {
                    "sent": "Roughly what you should think of of this is I have my graph G. You could probably make this precise.",
                    "label": 0
                },
                {
                    "sent": "We haven't, but roughly I have my graph G. And that has whatever eigenvalues it has.",
                    "label": 0
                },
                {
                    "sent": "I can replace G with G prime where G prime is now G+.",
                    "label": 0
                },
                {
                    "sent": "Something having to do with the rank one teleportation jumps.",
                    "label": 0
                },
                {
                    "sent": "So G prime is a different adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "G as a Laplacian G prime is Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Geez, Laplacian doesn't need to be localized 'cause I can get local spectral methods on that Rd network of the US, right?",
                    "label": 0
                },
                {
                    "sent": "But if I paid if I do the teleportation jump jump back and that teleportation jumps back to a small set of localized nodes now have G prime so these things are approximations to the eigenvectors of G prime, not G, 'cause I could localize I could jump back to Florida, I could jump back to me, I can jump back to Vancouver, I can jump back anywhere.",
                    "label": 0
                },
                {
                    "sent": "So these things you should think of as approximate eigenvectors of this modified graph.",
                    "label": 0
                },
                {
                    "sent": "Correct, yeah, yeah, you might be able to extend it, but it gets.",
                    "label": 0
                },
                {
                    "sent": "It gets trickier as you March farther out on the spectrum, so this is just about the 1st.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        }
    }
}