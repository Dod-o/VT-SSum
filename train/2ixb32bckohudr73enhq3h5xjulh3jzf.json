{
    "id": "2ixb32bckohudr73enhq3h5xjulh3jzf",
    "title": "Tricks of the trade for training SVMs",
    "info": {
        "author": [
            "G\u00f6khan H. Bakir, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "March 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/mlsvmlso05_bakir_ttts/",
    "segmentation": [
        [
            "The Max Planck Institute for Biological Cybernetics into people and he's going to talk about the tricks of the trade.",
            "1st infantry.",
            "Thanks very much.",
            "Then taxing OK.",
            "So this is joint work with Olivia pair and.",
            "I'm going to tell you some strategies and shortcuts you're allowed to take to speed up your conventional support vector machine training.",
            "OK, and.",
            "Now we should get the next slide.",
            "Yeah, OK so."
        ],
        [
            "I know it's the last talk so.",
            "Everything I hope is in a way that it's easy digestible, so you will hopefully see nothing very special, new and partially.",
            "I will refer to the excellent talks in the morning from Kinsler and Sunny.",
            "OK, so I'll start with.",
            "I will start with work which was done by Mangasarian and which was the first who came up with the support vector machine formalism and trained it on a problem which is far larger than what we could do until that point.",
            "So actually so this was the first guy will ever trained on a data set bigger of data size 1 million OK and then we will discuss.",
            "So there are some limitations and then we will discuss how can this be extendable in a way that you can use as a nonlinear as productive machines.",
            "And here I'm going to refer to principles which you know from balance talk.",
            "It's the only regular early stopping.",
            "Afterwards I will talk about this and I will refer to the decomposition techniques and I will show you why you get that much support, support vectors and what can you do about it.",
            "And why it's bad for you actually?",
            "And OK.",
            "So now the last part is so I changed it a bit becausw.",
            "I have, so I'm using this opportunity to speak because I have some strong opinions about the link of machine learning through optimization and I hope I will make some, let's say, philosophical statements.",
            "No one is to agree, but I hope it's like something which later on will be picked up in the discussion.",
            "Right, so OK, let me."
        ],
        [
            "Start with citation.",
            "There is one guy.",
            "So why we are?",
            "Why is it a problem?",
            "Large scale and support vector machine?",
            "Yeah, if you think about it then the extreme point is actually this perfect statement made by a llama too.",
            "So if you get a lot of points and actually you take it to the limit so you get data stream, we cannot use support vector machines at the moment, right?",
            "And somehow this guy or some guys in so this is a machine learner using more class using as well classical strategies.",
            "They claim that machine learning is going backwards in time, right?",
            "So anyway, the datasets we get is data capacity is growing and growing, but our techniques they actually go the opposite way, becauses we're going back to standard optimization techniques.",
            "So such that we cannot handle this huge data size anymore.",
            "And before, let's say 15 years ago with this neural network, no, this is this is possible with the neural network.",
            "I'm not talking if it's reasonable or something, but The thing is.",
            "Bigger datasets is a real problem for us right now.",
            "OK, and now why?"
        ],
        [
            "Is it a problem?",
            "So there is this beauty with the kernel framework.",
            "OK, and The thing is, there's one 2 rim which allows you to switch the view between.",
            "Do you want to minimize?",
            "So are we looking for a function?",
            "It's an indicator function or regression functional.",
            "Our future expect are.",
            "So do we want to solve this or do we want to solve a problem and N variables?",
            "OK, so this is very very difficult in general 'cause you have to optimize over function.",
            "But then because of the structure of the problem such that you can say I know my function is living in representable space people, so it's easy to switch to this representation, and this is the cache actually cause.",
            "So here it's like for, let me just say in comments you can see from the pointer that my hand is like this actually, so I hope it doesn't confuse it that much.",
            "OK, so this is a problem cause.",
            "To express this guy, you need the data points we are relying on data points.",
            "So now in special cases and this will be the case for support vector machines.",
            "The number you need to express this to express the minimum of your optimization is dependent on how many patterns you see.",
            "So from a practical viewpoint this is very bad because I give you points.",
            "OK, so you should make some predictions.",
            "I give you more points and your predictions.",
            "You're going to make will be accurate and more slower and slower and slower, so this is bad.",
            "OK, so this is nice 'cause we can do standard optimization on real variables and it's bad 'cause the number of points we need to express our solution depends on the data.",
            "Alright."
        ],
        [
            "OK, so now.",
            "Let us go back in history an start with.",
            "So let's forget about kernels or moments and go to the original problem."
        ],
        [
            "OK, here The thing is.",
            "So yeah, we know support vector machines.",
            "You have the L1 loss and then if.",
            "OK, some of you might know.",
            "Ideally you know we want to minimize the 01 loss very, very difficult, so the upper bound shop amount is the L1 loss, but another upper bound is the L2 loss and L2 loss, which this is actually you can think about this.",
            "You write your standard SVM, linear inequalities and your slacks.",
            "But we take now this'll X squared and people said oh, this is very very nice big cause if you do this.",
            "Then we can take gradients and we can calculate actions and we can utilize standard optimization technique.",
            "So this is the primal formulation for squared Flex and.",
            "Um?",
            "We're actually going to use this now to solve the primal problem, and there's here we don't need any concept about support vectors about dual problems or nothing.",
            "It's just an unconstrained optimization problem.",
            "OK, so yeah, it's very good today.",
            "I will let me just let me just say this of course, so the complexity of this of 1 Newton step is.",
            "Squared in the dimension and then N in the number of points and N since N is bigger than we have here.",
            "At least cubic complexity, so nothing new for you.",
            "OK, so now and."
        ],
        [
            "The first guy I wanted, the first.",
            "I think the 1st Post Purges, but then really and a big data in a.",
            "Big size scenario.",
            "It wasn't necessary and he took this formulation.",
            "He didn't look at gradients and at the Hessian matrix, but he derived a fixed point formula and use and it's called Lagrangian support vector machines.",
            "And this thing works in the primer formulation and it's very, very well designed for training linear schemes.",
            "Alright, and so you can say.",
            "So this is an alternative way to optimize the final formulation.",
            "Let's see how we actually compare how we.",
            "If we are doing the same as CDOs, so.",
            "You can see that the complexity actually in training time is scales the same As for the lagrangians public machine, which is good.",
            "And it's a small city I have to say.",
            "This small C makes something like this value.",
            "If you see this, it makes no sense for the L1 loss.",
            "OK, yeah, go ahead.",
            "Yeah, OK, so here I'm plotting the training set size.",
            "Which I use and to use time.",
            "OK, and you can see it's so it's linear.",
            "It's actually log plot, so the complexity is the same As for the Lagrangian product machine.",
            "Which is.",
            "I mean, I think up to date.",
            "No reasonable way to really assess quickly big size datasets.",
            "Everything is in primal linear.",
            "Yeah and that's the thing.",
            "Oh yeah, so I.",
            "So Lagrangians perfect machine has a fixed point rule and The thing is it can only work on big problems.",
            "Becausw I care about the dimension.",
            "So this is something fixed."
        ],
        [
            "And this only this N is in terms of data points.",
            "But the unconstrained optimization problem is fixed OK if you have.",
            "If you have this then you can also train on 60,000,000.",
            "Points as was asked yesterday.",
            "OK.",
            "So."
        ],
        [
            "And instead of a fixed point we have now the classical that another textbook approach, the Newton step.",
            "OK, but The thing is that the LaGrange public machine it's actually quite sensitive on this parimeter.",
            "OK, so if you change this and you'll see how much you take to converge, then you see that this is a very critical parameter for lagrangians perfecting machine, but not for do not.",
            "If you really do directly Newton steps.",
            "Which is the best it can take?",
            "And this phenomena we will see later again.",
            "OK. Alright."
        ],
        [
            "So as I said.",
            "We don't need military and.",
            "Simpler is better.",
            "Yeah, and this is actually sort of the first message is if your number of points.",
            "So let's say primal.",
            "Yeah, you can work in the primal if your number of points is larger than the dimension.",
            "There is no need to switch to the dual formulation.",
            "OK, so now what's happening."
        ],
        [
            "We use if you want to do use kernels, cause the indicator function we need is a nonlinear rule.",
            "Alright, we can?",
            "We know from the representative in you know that we can just switch the alphas there.",
            "So if you remember the expansion coefficients in the kernels, they're not necessarily Lagrangian multipliers.",
            "I mean, that's only an artifact which comes in from the optimization.",
            "It can be anything you are free to choose, and what one can utilizes.",
            "Since I know my function has this shape, I can directly plug it in in the primer.",
            "And OK, so now you see the beat as they don't have a physical rule now as you would have if you have Lagrangian multipliers.",
            "But I mean I don't care, you know you want to solve this problem machine with some coefficients.",
            "Then you can go for it directly.",
            "OK, so the problem here.",
            "This.",
            "That's OK, no hold on.",
            "Yeah OK.",
            "So."
        ],
        [
            "So let's assume we are at a position with N support vectors or any support vectors OK, and we want to calculate the gradient for our optimization.",
            "Problem and anyway the hash and of course you have to update because he has a new point.",
            "You can use the work by formula to invert, so that's quick.",
            "But nevertheless.",
            "Anyway the complexity we have is something CUBICIN in the number of support vectors and this so this is the same complexity if as if you would, for example do lip SVM or SVM light.",
            "So you train use the composition method for to solve the dual formula.",
            "So anyway.",
            "If you have kernels then.",
            "And in the setting right now it doesn't matter.",
            "You have to pay the same price.",
            "OK, so we will see that the number of support vectors it's it depends in a very special way on your characteristics of your data and it will grow in terms of it's dependent on the data size.",
            "But I will comment this later on.",
            "OK, so."
        ],
        [
            "Here's the trick.",
            "Um?",
            "What I do is so this is called I think.",
            "It's a reduced rank approximate if you like to call matrix as I guess, and the idea is.",
            "We train the SVM, but only on a subset of points.",
            "OK, so to express the rule you can use the subset.",
            "However, you use all the points to to somehow assess the rule.",
            "So what I want ideally is I want to go back to the original formulation in the primal such that my dimension is fixed.",
            "Exactly, so The thing is.",
            "Um?",
            "The number of patterns I use for my indicator function.",
            "They are fixed, but the number of loss.",
            "So the number of patterns I have to the number of constraints I get is variable.",
            "OK, so in here.",
            "So we get something similar as in the linear case, yeah, and.",
            "And let's say a variation of this is also proposed for monastery and it's called the reduced support vector machine and it's just like you train is select a random subset of your training set and then you train actually regarding this subset.",
            "OK.",
            "So this is good 'cause it now I break the link between.",
            "The number of points I need to a number of variables.",
            "I need to find and the number of data points.",
            "OK.",
            "So yeah."
        ],
        [
            "So yeah, and it's actually so.",
            "That's equivalent.",
            "Then taking all your points projecting on the subspace spanned by your by this particular set, and then training a linear SVM so OK and then.",
            "Um?",
            "So this is actually very interesting.",
            "We had yesterday the question with Gaussian.",
            "If you use Gaussian you get infinitely feature that space.",
            "And here The thing is in practice, so this is absolutely true.",
            "In theory, the more points you see by I mean theoretically you have always full rank gram matrix.",
            "But since finite how you say finite precision?",
            "You will see often sharp drop off in the spectrum of your cloud matrix and for this reason somehow if you want to, if you go to a real machine then there's something like an affective dimension.",
            "OK, and this is like this depends on the data set and if if you can anyway only.",
            "If the real numerically calculated the rank of a kernel matrix is K, then.",
            "This is actually.",
            "And you choose something.",
            "You choose a subspace, the subspace of dimension K. Then you can buy this projection.",
            "You don't lose anything, and you can still use the trick in the Primal XP.",
            "Alright, yeah.",
            "But on the other hand, you remember you have this K ^2 * N, so the more the bigger K is.",
            "Yeah, the more you have to pay doing a new Newton step.",
            "So there's some tradeoff here.",
            "OK, and this is The thing is often in some applications maybe.",
            "If there's if there's a lot of loss also, you want, not the high precision answer from your from.",
            "One which you would get by solving the classical optimization formula.",
            "Now you can say I'm already happy if it's like.",
            "It's fine until some accuracy, so you can really now actively choose if you want high precision, but then you do have the tradeoff that it takes long, or you say, yeah, optimize very quickly and.",
            "Just to get an impression what your data looks like.",
            "OK, so I think that's very nice feature for data miners.",
            "OK, so yeah, alright now."
        ],
        [
            "The nice thing of the primal formulation as well is.",
            "You know you, I really only care about the primal if I have some kernels and Alpha, and I see how far are the Alpha changing.",
            "It doesn't tell me something 'cause my hyperplane is defined by W and not, and if you just look to the dual formulation you have Alpha and then the kernel.",
            "So are you somehow this Alpha as they are in the way multiplied by something nonlinear and that's why it's hard to come up with them.",
            "If you say my Alpha vector changed in the norm.",
            "Let's say 10 to the minus one.",
            "It doesn't mean anything.",
            "You know in the primal space it could change rapidly are dramatically, but if you look to this primal formulation, then you always know changed in the W directly.",
            "So in the variables you optimize directly corresponds to the test error, because this is what you can read this.",
            "Oh sorry OK very good.",
            "Are there a lot of great stuff here?",
            "So now the upper curve?",
            "Actually, that's the objective function of the primal.",
            "And the lower curve is the test error.",
            "So once you're here I test it on the.",
            "On the test that actually OK and then and this we see it's dependent, so the X axis is the number of the size of the subset we use.",
            "Yeah, so the bigger subset size.",
            "Obviously you can minimize or approach the real solution more and somehow your solution is more accurate.",
            "This is what I said.",
            "OK, and so this size this number, here you know can you?",
            "Do you know it's a priority in a way?",
            "And so I told you, if you look.",
            "If you could calculate the spectrum of the kernel matrix.",
            "You often observe something like theirs.",
            "So there's.",
            "One subspace, or, let's say of dimension K, and it has all the most of the variance, and then the rest.",
            "Actually it's marginal like the eigen values up to the first 100 eigenvalues.",
            "They have 89% of the whole spectrum.",
            "OK, so I don't need to look at all points.",
            "Anyway, if I just can construct the subspace of this size 'cause they will lift anyway, endpoints will lift anyway in a subspace of maximally 100.",
            "I mean approximately OK and the rest is, you know, the rest is 11%, so it's tiny fraction.",
            "This is clear.",
            "OK. Alright, and now."
        ],
        [
            "So I told you about the tradeoff.",
            "I really like this.",
            "It gives me the chance to say let's let's do standard let's let's train with the decomposition technique.",
            "OK, and I don't have any choice.",
            "You know, I just trained with some fixed Chrissy and it gives me here.",
            "But now if I just control the subset size, for example, I have away some excuse me some means to say how far I mean how it's very quickly answers me the question, how good is your?",
            "How good is the solution?",
            "And then I can.",
            "I can see something like you can create this, plot it.",
            "You know you see your time versus test.",
            "And so very quickly you get already reasonable results.",
            "And The thing is, for some datasets, which is this one adult?",
            "So maybe you know it's already a lot of noise, so why do you want to really go to the minimum?",
            "While there is no need for high accuracy?",
            "So what you would like if you would very quickly get an answer and then let's say you get you go 1 accuracy level further and if you see or there's a strong drop off, yeah now it makes sense to go on and to desire more OK, but then after some time and this is very quickly this is a lock again very quickly it will tell you something like.",
            "OK, now there's doesn't change much anymore.",
            "So there's no need to go further.",
            "Yes.",
            "Stop.",
            "Yeah, yeah.",
            "So so exactly.",
            "Yeah and OK.",
            "So I will tell Mark The thing is how can you innovate?",
            "Smartly extend the solution to a further increase the subset size.",
            "I mean, how can you smartly extend?",
            "And it's very easy actually.",
            "OK. Good.",
            "So now yeah, I told you I want.",
            "Ideally you want."
        ],
        [
            "Have K growing and Meanwhile you want to see.",
            "So this is the subset size and.",
            "And you want to see if your objective function doesn't change anymore.",
            "There's no need to go on, it's OK, and this objective function in the primal.",
            "It really relates to the test error.",
            "So Fleck objective function there means that you're done in a way, right, which is not the case in the dual.",
            "So small change in the dual can make dramatic change in the primal.",
            "OK, so yeah, so the first thing to do is actually you take all points OK and then you make you do you.",
            "You minimize, but not.",
            "You don't take it.",
            "And I mean you don't use the minimization to express the solution.",
            "You just check how far using the point.",
            "How far can I go and then?",
            "So this is very expensive, but nevertheless.",
            "It will tell you, so that's the best you can do.",
            "You know it's the best one guy which will bring you which somehow contributes most objective function.",
            "And the other way is 1.",
            "One thing just subsample from the data and then only select the best of the pool actually and we will do this.",
            "But this is not the only."
        ],
        [
            "And now you are somehow in the domain of active learning.",
            "You say?",
            "What's the pattern I should use which is which contributes to the most and they're def different strategies.",
            "So one thing is you know you don't matter, you just roll.",
            "You flip a coin and just take one.",
            "So that's the random approach and something which is very interesting is if you take the one where which.",
            "So yeah, if you take a pattern which is as most independent to your current subset, OK, it adds a dimension, obviously.",
            "Then it's actually almost like random, in a way.",
            "That's actually excuse me, yeah, so here, but I mean this.",
            "I don't know if it's this is significant.",
            "Anyway, yeah, So what does it mean?",
            "OK so this.",
            "You know this is just one strategy you could try and now The thing is we did this and now the interpretation would I would say.",
            "And this is a hypothesis so statement in probability means.",
            "Actually, you know I choose a pattern which is as diverse from what I have so far, but somehow the pattern I chose.",
            "It doesn't have nothing to do with my classification rule.",
            "No uh-huh.",
            "Oh yeah, so that's the so OK, you can calculate given a point and given a set of points, I can calculate something like how far, what's the distance of this point to spend of this point?",
            "OK?",
            "And then you can say I'll take the one which is furthest away actually first I mean.",
            "Since in the way it will contribute most to the span, but The thing is, the span is not really what you want 'cause what you want is you want to classify.",
            "OK. Yeah, no, that doesn't mean it's.",
            "It means only that I checked that I checked the points which are farthest away from my current expressive subset, but it can be that the farthest point isn't away good for classification as well.",
            "I mean, there's no, there's no really critical thing it can be.",
            "Fun.",
            "Yeah, you can do this as well and.",
            "Actually so.",
            "So this is.",
            "The problem with this is.",
            "I don't think that's a good thing because what you do is you will keep adding points which are maybe mislabeled anyway, so I don't know if you really want to take the maximum error directly.",
            "Maybe you want?",
            "I mean, maybe it's a good thing.",
            "What I mean, what we did is, I mean there's something like.",
            "What is the next in a way to the margin?",
            "So where are you most uncertain?",
            "It does not need to be the one where you making the biggest error.",
            "Let's say assume you have an outlier.",
            "OK, so let's say 99% of your data.",
            "This describe your classification rule, and there is one big Gaussian block in the opposite side, so it will have the biggest gradient.",
            "But somehow it won't contribute most to classifying.",
            "If I just count 01.",
            "Yeah, I mean."
        ],
        [
            "So actually yeah, this one I don't know.",
            "I mean you know The thing is, this is one of four, it's not clear."
        ],
        [
            "So it will directly go down to the minimum, but The thing is, you can.",
            "I mean it's up to discussion.",
            "It's not what I mean.",
            "I don't know if this is really good or if this one is better, etc.",
            "It depends so, but if you think yeah you want to go direct, minimize then of course the one with the maximum error.",
            "It will do this, but it's expensive.",
            "Right, so it's up to discussion.",
            "I mean, I don't know which one is best.",
            "Just a remark, yeah.",
            "Vector machine E. Lawrence yeah and he uses it.",
            "The the entropy reduction of the cost.",
            "Which is next one.",
            "That might be something interesting, yeah, maybe.",
            "I know that's very interesting, but it's somehow out of my knowledge thing.",
            "Station.",
            "Incomplete.",
            "Constantly.",
            "The yeah yeah.",
            "OK.",
            "I see I have nice, but nevertheless I mean it's not really what I mean.",
            "It's in a way, there's no link to the wise in the sense I don't know.",
            "I don't know.",
            "Yeah yeah yeah I will collect everything afterwards.",
            "OK, so is there any other comments on this?",
            "OK, so why don't you?"
        ],
        [
            "Just discuss it afterwards and I can go on this later.",
            "OK, so and The thing is.",
            "If you So what am I doing?",
            "I'm doing the student steps and I know it's very quick to make rank 1 updates to my passion and.",
            "So if you don't.",
            "So if I assume that the number of patterns in a way everything is support vector which I have.",
            "Then it's like.",
            "Ridge regression and so.",
            "So I incremented the update points for my rich regression solution.",
            "OK, so I can do this updates an only sometimes.",
            "You can actually train.",
            "You can reiterate, retrain everything.",
            "OK, using the subset.",
            "So The thing is here you keep the old direction information of the Hessian and of the gradient and your update with the new point anyway.",
            "But obviously this is not correct.",
            "So yeah, because the old part, the old directional information is wrong once you have updated one guy, but nevertheless.",
            "Making just sometimes corrections to direction information gives you a further speedup.",
            "OK."
        ],
        [
            "And yeah, and yeah you can use multi class and it's again the same phenomenon that the primal objective gives you an idea about how I mean is directly linked to the test error and you can so going step by step you can check if it's worth to extend the subset or not.",
            "And to this end you can during optimization it's you have somehow control.",
            "If it's worth to go on or not.",
            "If you go, if you solve the best optimization directly and you so you ask for the minimum directly, that's the optimization approach.",
            "Then you end up here, but you might not worth it.",
            "So OK, so in a way to summarize this is.",
            "I think it's important to have somehow enough where you can choose.",
            "How much you want for the currency and it has you trade off.",
            "Actually the time you need to get the answer.",
            "Alright, OK. Now."
        ],
        [
            "Um?",
            "Will do.",
            "So this is nice, but you know there is a crucial thing is the square slack.",
            "So we take another loss and then you get some problems, right?",
            "So the dual never delay.",
            "Its dual formulation has its existence existence right?",
            "And you're interested.",
            "How can you really speed up?",
            "What are the strategies there?",
            "OK, so now on the dual problem."
        ],
        [
            "It's like.",
            "I mean, there are several things going on.",
            "One thing is the composition techniques in general, say, give me this point in this point, and then you have to provide the column of this particular kernel matrix at that part, right?",
            "So what is very important is things like caching.",
            "And further, and what's also important is the strategy how you choose points.",
            "And let us see the state of the art, the composite techniques.",
            "What happens if we just get a lot of noise?",
            "OK, So what ends if you have noise?",
            "And what if we increase the number of patterns for training?",
            "So there's great theorem."
        ],
        [
            "From Stainbach from 2004 it tells you it directly tells you that.",
            "The link between the number of patterns you're going to have, the number of support vectors are going to have.",
            "Which is the direct indicator of the cache size and everything and.",
            "Relates this quantity 2 how many patterns for any patterns you have and about the noise ratio.",
            "So this.",
            "Tell, I mean this.",
            "Actually it says.",
            "If you go up with the base rate, so if you get a problem, OK, it has some assume you would know the base risk then.",
            "And you get another problem and use the same optimizer and there's higher noise then.",
            "Actually there is a relation among these quantities, and these quantities are here in the case.",
            "The rank of econometrics, so it tells you what are the puts information in the patterns.",
            "This is how many patterns don't hit the bound so our non boundaries protect us and this is how many.",
            "Alpha, so variables hit the bounds actually and you can see the the direct thing once we still growing is the number of.",
            "Patterns which hit the bounds OK so.",
            "If you are OK, so I mean equivalently, you know, just fix this and keep and make this variety so it's the same effect.",
            "So the number of training patterns to see only the ones that the balance will actually grow, and this is nice because you can often if you design for example caching strategies, there is no.",
            "I mean, you can consider a very noisy problem.",
            "You will have the same effects.",
            "Then in a problem where you have a lot of where you have a lot of patterns.",
            "OK, so.",
            "It's going.",
            "So why is this?",
            "Why is this?"
        ],
        [
            "This issue a problem, so I told you you need somehow too.",
            "Once you have a pattern you need to provide the kernel matrix.",
            "OK, now if we see if we make this decomposition approach like small then you can see.",
            "The following so let's make this type pattern where we have just very clear noise region.",
            "OK and.",
            "We make a histogram over the cache Miss which end over the patterns.",
            "So ideally if you have a lot of RAM you can catch everything.",
            "You calculate everything and you're done.",
            "But if you have less, if not that much RAM, you will have cache misses and then this means Kashmir miss means delete the ultra completely, calculate the neuro and we can make a histogram about where are the most cash methods and what you see is.",
            "For 30% of the time, the cache misses exactly at the point at, say at the region where there is noise actually.",
            "It's also interesting Lee.",
            "Here outside, I don't know.",
            "Is this?",
            "I mean it has to be there.",
            "Precisely so this is yeah, so this.",
            "The margin actually is like this, but instead of the noise you have stuff there, so that's fine.",
            "OK, so this tells me now.",
            "I have noise and this noise makes me paying a lot of money 'cause most of the time I'm busy here recalculating stuff right?",
            "So what can I do about this?",
            "Yes.",
            "OK so yeah.",
            "So this."
        ],
        [
            "Almost direct relation of standards, results and that's what I told you.",
            "Number of cache misses are linear related to the noise level and so this is again as before the base.",
            "So problems with varying based risk or varying size problems with fixed base risk.",
            "And you see that the number of questions are linear.",
            "I told you this.",
            "Yes.",
            "Sorry guys.",
            "It's new so."
        ],
        [
            "OK, so The thing is how do I, you know, I don't want this.",
            "I know I'm paying money for the noise actually, so how can you carefully choose patterns in a very, very quick way?",
            "Such as you can say this is a noise level and this is a good point.",
            "OK, and there's."
        ],
        [
            "Very, very old trick.",
            "It's from nearest neighbors.",
            "It's like a news neighbors, you just keep every point and if you want to predict it doesn't matter how many points you have, right?",
            "It's the same thing and people so people thought about what can we do to speed up nearest neighbors.",
            "And there's so one strategy is called condensation.",
            "It's it does.",
            "It just looks at so it splits the data and trains and you randomly then trains the nearest neighbor classifier.",
            "And makes predictions over the other split OK and then if it's if I am uncertain about a pattern, so several splits say something different about this guy.",
            "I dropped this guy actually.",
            "So here the nearest neighbors will say that this is a red dye, but it was a blue guy, so I delete this.",
            "Although I'm sorry I missed it up, it's actually the other way around.",
            "So sorry, sorry, sorry.",
            "So yeah, exactly condensation is you keep this point.",
            "So if you are uncertain you keep it.",
            "If you if you're very certain, then somehow it does not tell you anything about your problem so you drop it that way around and somehow the symmetry.",
            "I was confused.",
            "It's editing there you delete everything where you are uncertain about the patterns.",
            "So this if you train an L1 SCM, actually you have this already in a way, but we don't have this.",
            "Right and how can we implement this very quickly?",
            "It's very simple, so this is."
        ],
        [
            "Yeah, OK, I will come back later this commit.",
            "So this is used cross validation.",
            "You just make you know SVM training is cubic in the number of support vectors so it makes sense to train on a very small fraction.",
            "So and this is quick so you can use cross validation and you train on the subsets and then you can vote about patterns OK?",
            "So you can say, let's.",
            "Let's say FS is the rule which you support vector machine rule which you got by training on a set S and this is the corresponding pattern of.",
            "A test point XI.",
            "So now if you average over the splits, you can say so this is bigger.",
            "1 means I'm very certain about it.",
            "OK, my so all the splits say give the same label then this point has no information.",
            "You can drop it so this conversation and editing is like this smaller so smaller one would be uncertain, but smaller zero would be.",
            "It's actually wrong classified.",
            "So and then you remove this and this is very quickly.",
            "I mean, this is very very easy to do.",
            "We train independence VM's you don't need to implement anything, you just need to do some preprocessing by splitting your datasets.",
            "So you do this, you split datasets, training your SCM with favorite solver and afterwards you somehow vote about labels.",
            "And then you can say you know you can.",
            "We can drop points which are very far away and where we are very certain about and we can drop points where we are really certain that they are misclassified.",
            "OK, so once you have done this, you repeat your training.",
            "Sounds very easy and dubious.",
            "But"
        ],
        [
            "The interesting thing is.",
            "So.",
            "So this is several standard test sets used in machine learning, from the UCI data set, and it does remarkably well.",
            "Actually it's it produces rules which are much sparser OK because you don't have these points at the upper bound anymore.",
            "I mean several of them.",
            "A lot of them will be actually dropped from the training set, so we don't need to worry about them anymore, and it allows you to scale up.",
            "The data set size OK because you can if you say I have end points and you have an idea about the noise and a lot of you can drop a lot of points without sacrificing something.",
            "OK so nice, but actually there is a problem.",
            "So I mean what?"
        ],
        [
            "By doing here I say yeah this point I like.",
            "In this point I don't like OK so.",
            "But The thing is, since I'm I'm doing this cross validation, there is no really.",
            "I mean I don't have the density, so there is something like we don't have the real control and for this reason if I give you a data set, so like what I plot is.",
            "What you see here is the histogram over the margins.",
            "And this is the original data set is with thin line and then the edited data set is a thick line.",
            "You can see that by doing this voting and editing, you somehow really changed problem nature in a way.",
            "Sometimes I really flip actually the for example.",
            "For example, here I had more blue guys in red guys and now I totally flipped it around.",
            "So this is there is a problem here.",
            "So there's still work going on.",
            "OK, nevertheless.",
            "There is a spin."
        ],
        [
            "And that's the question from yesterday, so doing simply cross validation and then estimating the margin location.",
            "It can be used to initialize lagrangians.",
            "I know directly if something is far in interior.",
            "I know it's zero and I know if I'm very uncertain I know it's at the bound.",
            "OK, so and this is actually very good 'cause now it's like a very cheap hot start to your real.",
            "Optimization procedure and Furthermore it allows you to say yeah, this.",
            "This patterns I will need in the cache directly.",
            "Uh huh.",
            "Everybody.",
            "Yeah, but if I do this the problem is I mean I also have to calculate others.",
            "I mean then I will only catch the error points.",
            "So actually the problem is that's the cache miss right?",
            "If you do the composition scheme.",
            "Yeah, OK, but then somehow that means to all never going to cash parts which you might need later on, right?",
            "So I mean it's not.",
            "I mean, it's I don't know if you want if your favorite through list only cash good points or only cash back points.",
            "In a way, it's tricky.",
            "Nevertheless, you can.",
            "Some of them you can lock directly because you know you're going to need them.",
            "That's true, you can do this.",
            "So OK, this we can do.",
            "And the nice thing is, if you do start decomposition, you start at 00 OK and then you rule the W is changing during optimization, but I already used this changing about my decision if I should.",
            "Cash a column or not, so by just simply doing this hot start I match.",
            "I have something which is much more.",
            "It's much more reliable about the actual state of the alphas then using this intermediate solutions.",
            "So that's very nice, but nevertheless I mean ideally I would like to have the same what I had in the primer, so I would have something like control choice through which if I increase the subset size, gives me a better generalization and it does not destroy the statistical properties of the problem, which I can't ensure right now.",
            "OK.",
            "So."
        ],
        [
            "Now, yeah, we actually almost done.",
            "So come to conditions and at the end so there there is again citation which regard which is regarding the discussion yesterday.",
            "So what I claim is."
        ],
        [
            "Um?",
            "Machine learning is not equal optimization.",
            "The demands are different and this way.",
            "So what I say the vicinity of a solution OK is already sufficient in away so.",
            "And then Furthermore, it's actually from application point important.",
            "If you do not want.",
            "That much.",
            "OK, so you demand less inaccuracy.",
            "OK, you can do a lot of more things away.",
            "I don't know how you would do this with.",
            "Look for the minimum I have.",
            "I don't know how you would encode this and here we have a mechanism very old, but we can use this.",
            "It's early stopping.",
            "We can use this mechanism to really control if you should go further or not.",
            "And last statement is, you know convexity is good.",
            "Duality is great, but I mean.",
            "Some, I mean, maybe you know.",
            "Just because it's there, you don't need to use it.",
            "OK, OK so this is the thing.",
            "If you have to do approximations, do it in with the physical quantities of interest and let me finish my talk with.",
            "One cetacean.",
            "So this is children in the author of."
        ],
        [
            "SCM, which is state of the art server.",
            "So this is a coauthor, so.",
            "Citation is wrong.",
            "This is Michael Kerns.",
            "Well, I mean I have them from personal communications, so I think is right.",
            "They have.",
            "This is a citation problem that I don't know.",
            "I mean, I just maybe I. I mean, maybe yeah, anyway is something which I would also say I guess so it says if someone puts a gun on my head and asked me to do model selection, you know yesterday that discussion by minimizing the cost function an extra day refer to about like we also we had yesterday then or I choose cross validation.",
            "The answers I choose cross relation.",
            "So optimization is great but this one gives me the real question.",
            "Interest, so I do this one.",
            "OK, thank you very much."
        ],
        [
            "Alright."
        ],
        [
            "Personal statements are there.",
            "The discussions later though.",
            "Conversion.",
            "Non smooth.",
            "So The thing is that smoothness will probably cut off the Max.",
            "Yes, but you know the Max.",
            "So somehow I wrote it down as this 'cause it just tells me for gradient and for Hessian calculation select patterns.",
            "So, so it's not like I'm not calculating next there.",
            "Ideally you would have a lot of inequality's and a slack, and then you have.",
            "I mean, I can write it down and then sum over all the slack squared.",
            "It's the same thing.",
            "I just rewrote it in one line, which I can do only four squares legs.",
            "I have a constraint problem.",
            "No.",
            "Which one?",
            "My sample.",
            "I mean, there's no, there's no constant, it's always bigger than 0, right?",
            "I knew it was.",
            "Your second did you enter doesn't exist.",
            "Alright, OK, but so.",
            "So this continues the same, but I mean I just met this.",
            "So.",
            "OK OK OK. That is a question.",
            "OK, I agree.",
            "I mean X video.",
            "The news about this is great.",
            "Your friend.",
            "Other questions or comments?",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Max Planck Institute for Biological Cybernetics into people and he's going to talk about the tricks of the trade.",
                    "label": 1
                },
                {
                    "sent": "1st infantry.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                },
                {
                    "sent": "Then taxing OK.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with Olivia pair and.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you some strategies and shortcuts you're allowed to take to speed up your conventional support vector machine training.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "Now we should get the next slide.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I know it's the last talk so.",
                    "label": 0
                },
                {
                    "sent": "Everything I hope is in a way that it's easy digestible, so you will hopefully see nothing very special, new and partially.",
                    "label": 0
                },
                {
                    "sent": "I will refer to the excellent talks in the morning from Kinsler and Sunny.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'll start with.",
                    "label": 0
                },
                {
                    "sent": "I will start with work which was done by Mangasarian and which was the first who came up with the support vector machine formalism and trained it on a problem which is far larger than what we could do until that point.",
                    "label": 0
                },
                {
                    "sent": "So actually so this was the first guy will ever trained on a data set bigger of data size 1 million OK and then we will discuss.",
                    "label": 0
                },
                {
                    "sent": "So there are some limitations and then we will discuss how can this be extendable in a way that you can use as a nonlinear as productive machines.",
                    "label": 0
                },
                {
                    "sent": "And here I'm going to refer to principles which you know from balance talk.",
                    "label": 0
                },
                {
                    "sent": "It's the only regular early stopping.",
                    "label": 0
                },
                {
                    "sent": "Afterwards I will talk about this and I will refer to the decomposition techniques and I will show you why you get that much support, support vectors and what can you do about it.",
                    "label": 0
                },
                {
                    "sent": "And why it's bad for you actually?",
                    "label": 0
                },
                {
                    "sent": "And OK.",
                    "label": 0
                },
                {
                    "sent": "So now the last part is so I changed it a bit becausw.",
                    "label": 0
                },
                {
                    "sent": "I have, so I'm using this opportunity to speak because I have some strong opinions about the link of machine learning through optimization and I hope I will make some, let's say, philosophical statements.",
                    "label": 1
                },
                {
                    "sent": "No one is to agree, but I hope it's like something which later on will be picked up in the discussion.",
                    "label": 0
                },
                {
                    "sent": "Right, so OK, let me.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start with citation.",
                    "label": 0
                },
                {
                    "sent": "There is one guy.",
                    "label": 0
                },
                {
                    "sent": "So why we are?",
                    "label": 0
                },
                {
                    "sent": "Why is it a problem?",
                    "label": 0
                },
                {
                    "sent": "Large scale and support vector machine?",
                    "label": 1
                },
                {
                    "sent": "Yeah, if you think about it then the extreme point is actually this perfect statement made by a llama too.",
                    "label": 0
                },
                {
                    "sent": "So if you get a lot of points and actually you take it to the limit so you get data stream, we cannot use support vector machines at the moment, right?",
                    "label": 0
                },
                {
                    "sent": "And somehow this guy or some guys in so this is a machine learner using more class using as well classical strategies.",
                    "label": 0
                },
                {
                    "sent": "They claim that machine learning is going backwards in time, right?",
                    "label": 0
                },
                {
                    "sent": "So anyway, the datasets we get is data capacity is growing and growing, but our techniques they actually go the opposite way, becauses we're going back to standard optimization techniques.",
                    "label": 0
                },
                {
                    "sent": "So such that we cannot handle this huge data size anymore.",
                    "label": 0
                },
                {
                    "sent": "And before, let's say 15 years ago with this neural network, no, this is this is possible with the neural network.",
                    "label": 0
                },
                {
                    "sent": "I'm not talking if it's reasonable or something, but The thing is.",
                    "label": 0
                },
                {
                    "sent": "Bigger datasets is a real problem for us right now.",
                    "label": 0
                },
                {
                    "sent": "OK, and now why?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is it a problem?",
                    "label": 0
                },
                {
                    "sent": "So there is this beauty with the kernel framework.",
                    "label": 0
                },
                {
                    "sent": "OK, and The thing is, there's one 2 rim which allows you to switch the view between.",
                    "label": 0
                },
                {
                    "sent": "Do you want to minimize?",
                    "label": 0
                },
                {
                    "sent": "So are we looking for a function?",
                    "label": 0
                },
                {
                    "sent": "It's an indicator function or regression functional.",
                    "label": 0
                },
                {
                    "sent": "Our future expect are.",
                    "label": 0
                },
                {
                    "sent": "So do we want to solve this or do we want to solve a problem and N variables?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very very difficult in general 'cause you have to optimize over function.",
                    "label": 0
                },
                {
                    "sent": "But then because of the structure of the problem such that you can say I know my function is living in representable space people, so it's easy to switch to this representation, and this is the cache actually cause.",
                    "label": 1
                },
                {
                    "sent": "So here it's like for, let me just say in comments you can see from the pointer that my hand is like this actually, so I hope it doesn't confuse it that much.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a problem cause.",
                    "label": 0
                },
                {
                    "sent": "To express this guy, you need the data points we are relying on data points.",
                    "label": 0
                },
                {
                    "sent": "So now in special cases and this will be the case for support vector machines.",
                    "label": 1
                },
                {
                    "sent": "The number you need to express this to express the minimum of your optimization is dependent on how many patterns you see.",
                    "label": 0
                },
                {
                    "sent": "So from a practical viewpoint this is very bad because I give you points.",
                    "label": 1
                },
                {
                    "sent": "OK, so you should make some predictions.",
                    "label": 0
                },
                {
                    "sent": "I give you more points and your predictions.",
                    "label": 0
                },
                {
                    "sent": "You're going to make will be accurate and more slower and slower and slower, so this is bad.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is nice 'cause we can do standard optimization on real variables and it's bad 'cause the number of points we need to express our solution depends on the data.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "Let us go back in history an start with.",
                    "label": 0
                },
                {
                    "sent": "So let's forget about kernels or moments and go to the original problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here The thing is.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we know support vector machines.",
                    "label": 1
                },
                {
                    "sent": "You have the L1 loss and then if.",
                    "label": 0
                },
                {
                    "sent": "OK, some of you might know.",
                    "label": 0
                },
                {
                    "sent": "Ideally you know we want to minimize the 01 loss very, very difficult, so the upper bound shop amount is the L1 loss, but another upper bound is the L2 loss and L2 loss, which this is actually you can think about this.",
                    "label": 0
                },
                {
                    "sent": "You write your standard SVM, linear inequalities and your slacks.",
                    "label": 0
                },
                {
                    "sent": "But we take now this'll X squared and people said oh, this is very very nice big cause if you do this.",
                    "label": 0
                },
                {
                    "sent": "Then we can take gradients and we can calculate actions and we can utilize standard optimization technique.",
                    "label": 1
                },
                {
                    "sent": "So this is the primal formulation for squared Flex and.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We're actually going to use this now to solve the primal problem, and there's here we don't need any concept about support vectors about dual problems or nothing.",
                    "label": 0
                },
                {
                    "sent": "It's just an unconstrained optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, it's very good today.",
                    "label": 0
                },
                {
                    "sent": "I will let me just let me just say this of course, so the complexity of this of 1 Newton step is.",
                    "label": 0
                },
                {
                    "sent": "Squared in the dimension and then N in the number of points and N since N is bigger than we have here.",
                    "label": 1
                },
                {
                    "sent": "At least cubic complexity, so nothing new for you.",
                    "label": 0
                },
                {
                    "sent": "OK, so now and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first guy I wanted, the first.",
                    "label": 0
                },
                {
                    "sent": "I think the 1st Post Purges, but then really and a big data in a.",
                    "label": 0
                },
                {
                    "sent": "Big size scenario.",
                    "label": 0
                },
                {
                    "sent": "It wasn't necessary and he took this formulation.",
                    "label": 0
                },
                {
                    "sent": "He didn't look at gradients and at the Hessian matrix, but he derived a fixed point formula and use and it's called Lagrangian support vector machines.",
                    "label": 1
                },
                {
                    "sent": "And this thing works in the primer formulation and it's very, very well designed for training linear schemes.",
                    "label": 0
                },
                {
                    "sent": "Alright, and so you can say.",
                    "label": 0
                },
                {
                    "sent": "So this is an alternative way to optimize the final formulation.",
                    "label": 0
                },
                {
                    "sent": "Let's see how we actually compare how we.",
                    "label": 0
                },
                {
                    "sent": "If we are doing the same as CDOs, so.",
                    "label": 0
                },
                {
                    "sent": "You can see that the complexity actually in training time is scales the same As for the lagrangians public machine, which is good.",
                    "label": 0
                },
                {
                    "sent": "And it's a small city I have to say.",
                    "label": 0
                },
                {
                    "sent": "This small C makes something like this value.",
                    "label": 0
                },
                {
                    "sent": "If you see this, it makes no sense for the L1 loss.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, go ahead.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so here I'm plotting the training set size.",
                    "label": 0
                },
                {
                    "sent": "Which I use and to use time.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can see it's so it's linear.",
                    "label": 1
                },
                {
                    "sent": "It's actually log plot, so the complexity is the same As for the Lagrangian product machine.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think up to date.",
                    "label": 0
                },
                {
                    "sent": "No reasonable way to really assess quickly big size datasets.",
                    "label": 0
                },
                {
                    "sent": "Everything is in primal linear.",
                    "label": 0
                },
                {
                    "sent": "Yeah and that's the thing.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so I.",
                    "label": 0
                },
                {
                    "sent": "So Lagrangians perfect machine has a fixed point rule and The thing is it can only work on big problems.",
                    "label": 1
                },
                {
                    "sent": "Becausw I care about the dimension.",
                    "label": 0
                },
                {
                    "sent": "So this is something fixed.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this only this N is in terms of data points.",
                    "label": 0
                },
                {
                    "sent": "But the unconstrained optimization problem is fixed OK if you have.",
                    "label": 0
                },
                {
                    "sent": "If you have this then you can also train on 60,000,000.",
                    "label": 0
                },
                {
                    "sent": "Points as was asked yesterday.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And instead of a fixed point we have now the classical that another textbook approach, the Newton step.",
                    "label": 0
                },
                {
                    "sent": "OK, but The thing is that the LaGrange public machine it's actually quite sensitive on this parimeter.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you change this and you'll see how much you take to converge, then you see that this is a very critical parameter for lagrangians perfecting machine, but not for do not.",
                    "label": 0
                },
                {
                    "sent": "If you really do directly Newton steps.",
                    "label": 0
                },
                {
                    "sent": "Which is the best it can take?",
                    "label": 1
                },
                {
                    "sent": "And this phenomena we will see later again.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said.",
                    "label": 0
                },
                {
                    "sent": "We don't need military and.",
                    "label": 0
                },
                {
                    "sent": "Simpler is better.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and this is actually sort of the first message is if your number of points.",
                    "label": 0
                },
                {
                    "sent": "So let's say primal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can work in the primal if your number of points is larger than the dimension.",
                    "label": 1
                },
                {
                    "sent": "There is no need to switch to the dual formulation.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what's happening.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use if you want to do use kernels, cause the indicator function we need is a nonlinear rule.",
                    "label": 0
                },
                {
                    "sent": "Alright, we can?",
                    "label": 0
                },
                {
                    "sent": "We know from the representative in you know that we can just switch the alphas there.",
                    "label": 1
                },
                {
                    "sent": "So if you remember the expansion coefficients in the kernels, they're not necessarily Lagrangian multipliers.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's only an artifact which comes in from the optimization.",
                    "label": 0
                },
                {
                    "sent": "It can be anything you are free to choose, and what one can utilizes.",
                    "label": 0
                },
                {
                    "sent": "Since I know my function has this shape, I can directly plug it in in the primer.",
                    "label": 0
                },
                {
                    "sent": "And OK, so now you see the beat as they don't have a physical rule now as you would have if you have Lagrangian multipliers.",
                    "label": 0
                },
                {
                    "sent": "But I mean I don't care, you know you want to solve this problem machine with some coefficients.",
                    "label": 0
                },
                {
                    "sent": "Then you can go for it directly.",
                    "label": 0
                },
                {
                    "sent": "OK, so the problem here.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "That's OK, no hold on.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's assume we are at a position with N support vectors or any support vectors OK, and we want to calculate the gradient for our optimization.",
                    "label": 0
                },
                {
                    "sent": "Problem and anyway the hash and of course you have to update because he has a new point.",
                    "label": 0
                },
                {
                    "sent": "You can use the work by formula to invert, so that's quick.",
                    "label": 1
                },
                {
                    "sent": "But nevertheless.",
                    "label": 0
                },
                {
                    "sent": "Anyway the complexity we have is something CUBICIN in the number of support vectors and this so this is the same complexity if as if you would, for example do lip SVM or SVM light.",
                    "label": 1
                },
                {
                    "sent": "So you train use the composition method for to solve the dual formula.",
                    "label": 0
                },
                {
                    "sent": "So anyway.",
                    "label": 0
                },
                {
                    "sent": "If you have kernels then.",
                    "label": 0
                },
                {
                    "sent": "And in the setting right now it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You have to pay the same price.",
                    "label": 0
                },
                {
                    "sent": "OK, so we will see that the number of support vectors it's it depends in a very special way on your characteristics of your data and it will grow in terms of it's dependent on the data size.",
                    "label": 0
                },
                {
                    "sent": "But I will comment this later on.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the trick.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What I do is so this is called I think.",
                    "label": 0
                },
                {
                    "sent": "It's a reduced rank approximate if you like to call matrix as I guess, and the idea is.",
                    "label": 0
                },
                {
                    "sent": "We train the SVM, but only on a subset of points.",
                    "label": 0
                },
                {
                    "sent": "OK, so to express the rule you can use the subset.",
                    "label": 0
                },
                {
                    "sent": "However, you use all the points to to somehow assess the rule.",
                    "label": 0
                },
                {
                    "sent": "So what I want ideally is I want to go back to the original formulation in the primal such that my dimension is fixed.",
                    "label": 0
                },
                {
                    "sent": "Exactly, so The thing is.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The number of patterns I use for my indicator function.",
                    "label": 0
                },
                {
                    "sent": "They are fixed, but the number of loss.",
                    "label": 0
                },
                {
                    "sent": "So the number of patterns I have to the number of constraints I get is variable.",
                    "label": 0
                },
                {
                    "sent": "OK, so in here.",
                    "label": 0
                },
                {
                    "sent": "So we get something similar as in the linear case, yeah, and.",
                    "label": 0
                },
                {
                    "sent": "And let's say a variation of this is also proposed for monastery and it's called the reduced support vector machine and it's just like you train is select a random subset of your training set and then you train actually regarding this subset.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is good 'cause it now I break the link between.",
                    "label": 0
                },
                {
                    "sent": "The number of points I need to a number of variables.",
                    "label": 0
                },
                {
                    "sent": "I need to find and the number of data points.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, and it's actually so.",
                    "label": 0
                },
                {
                    "sent": "That's equivalent.",
                    "label": 0
                },
                {
                    "sent": "Then taking all your points projecting on the subspace spanned by your by this particular set, and then training a linear SVM so OK and then.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is actually very interesting.",
                    "label": 0
                },
                {
                    "sent": "We had yesterday the question with Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If you use Gaussian you get infinitely feature that space.",
                    "label": 0
                },
                {
                    "sent": "And here The thing is in practice, so this is absolutely true.",
                    "label": 0
                },
                {
                    "sent": "In theory, the more points you see by I mean theoretically you have always full rank gram matrix.",
                    "label": 0
                },
                {
                    "sent": "But since finite how you say finite precision?",
                    "label": 0
                },
                {
                    "sent": "You will see often sharp drop off in the spectrum of your cloud matrix and for this reason somehow if you want to, if you go to a real machine then there's something like an affective dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is like this depends on the data set and if if you can anyway only.",
                    "label": 0
                },
                {
                    "sent": "If the real numerically calculated the rank of a kernel matrix is K, then.",
                    "label": 0
                },
                {
                    "sent": "This is actually.",
                    "label": 0
                },
                {
                    "sent": "And you choose something.",
                    "label": 0
                },
                {
                    "sent": "You choose a subspace, the subspace of dimension K. Then you can buy this projection.",
                    "label": 0
                },
                {
                    "sent": "You don't lose anything, and you can still use the trick in the Primal XP.",
                    "label": 0
                },
                {
                    "sent": "Alright, yeah.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, you remember you have this K ^2 * N, so the more the bigger K is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the more you have to pay doing a new Newton step.",
                    "label": 0
                },
                {
                    "sent": "So there's some tradeoff here.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is The thing is often in some applications maybe.",
                    "label": 0
                },
                {
                    "sent": "If there's if there's a lot of loss also, you want, not the high precision answer from your from.",
                    "label": 0
                },
                {
                    "sent": "One which you would get by solving the classical optimization formula.",
                    "label": 0
                },
                {
                    "sent": "Now you can say I'm already happy if it's like.",
                    "label": 0
                },
                {
                    "sent": "It's fine until some accuracy, so you can really now actively choose if you want high precision, but then you do have the tradeoff that it takes long, or you say, yeah, optimize very quickly and.",
                    "label": 0
                },
                {
                    "sent": "Just to get an impression what your data looks like.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think that's very nice feature for data miners.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, alright now.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The nice thing of the primal formulation as well is.",
                    "label": 0
                },
                {
                    "sent": "You know you, I really only care about the primal if I have some kernels and Alpha, and I see how far are the Alpha changing.",
                    "label": 0
                },
                {
                    "sent": "It doesn't tell me something 'cause my hyperplane is defined by W and not, and if you just look to the dual formulation you have Alpha and then the kernel.",
                    "label": 0
                },
                {
                    "sent": "So are you somehow this Alpha as they are in the way multiplied by something nonlinear and that's why it's hard to come up with them.",
                    "label": 0
                },
                {
                    "sent": "If you say my Alpha vector changed in the norm.",
                    "label": 0
                },
                {
                    "sent": "Let's say 10 to the minus one.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean anything.",
                    "label": 0
                },
                {
                    "sent": "You know in the primal space it could change rapidly are dramatically, but if you look to this primal formulation, then you always know changed in the W directly.",
                    "label": 0
                },
                {
                    "sent": "So in the variables you optimize directly corresponds to the test error, because this is what you can read this.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry OK very good.",
                    "label": 0
                },
                {
                    "sent": "Are there a lot of great stuff here?",
                    "label": 0
                },
                {
                    "sent": "So now the upper curve?",
                    "label": 0
                },
                {
                    "sent": "Actually, that's the objective function of the primal.",
                    "label": 0
                },
                {
                    "sent": "And the lower curve is the test error.",
                    "label": 0
                },
                {
                    "sent": "So once you're here I test it on the.",
                    "label": 0
                },
                {
                    "sent": "On the test that actually OK and then and this we see it's dependent, so the X axis is the number of the size of the subset we use.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the bigger subset size.",
                    "label": 0
                },
                {
                    "sent": "Obviously you can minimize or approach the real solution more and somehow your solution is more accurate.",
                    "label": 0
                },
                {
                    "sent": "This is what I said.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this size this number, here you know can you?",
                    "label": 0
                },
                {
                    "sent": "Do you know it's a priority in a way?",
                    "label": 0
                },
                {
                    "sent": "And so I told you, if you look.",
                    "label": 0
                },
                {
                    "sent": "If you could calculate the spectrum of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "You often observe something like theirs.",
                    "label": 0
                },
                {
                    "sent": "So there's.",
                    "label": 0
                },
                {
                    "sent": "One subspace, or, let's say of dimension K, and it has all the most of the variance, and then the rest.",
                    "label": 0
                },
                {
                    "sent": "Actually it's marginal like the eigen values up to the first 100 eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "They have 89% of the whole spectrum.",
                    "label": 0
                },
                {
                    "sent": "OK, so I don't need to look at all points.",
                    "label": 0
                },
                {
                    "sent": "Anyway, if I just can construct the subspace of this size 'cause they will lift anyway, endpoints will lift anyway in a subspace of maximally 100.",
                    "label": 0
                },
                {
                    "sent": "I mean approximately OK and the rest is, you know, the rest is 11%, so it's tiny fraction.",
                    "label": 0
                },
                {
                    "sent": "This is clear.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, and now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I told you about the tradeoff.",
                    "label": 0
                },
                {
                    "sent": "I really like this.",
                    "label": 0
                },
                {
                    "sent": "It gives me the chance to say let's let's do standard let's let's train with the decomposition technique.",
                    "label": 0
                },
                {
                    "sent": "OK, and I don't have any choice.",
                    "label": 0
                },
                {
                    "sent": "You know, I just trained with some fixed Chrissy and it gives me here.",
                    "label": 0
                },
                {
                    "sent": "But now if I just control the subset size, for example, I have away some excuse me some means to say how far I mean how it's very quickly answers me the question, how good is your?",
                    "label": 0
                },
                {
                    "sent": "How good is the solution?",
                    "label": 0
                },
                {
                    "sent": "And then I can.",
                    "label": 0
                },
                {
                    "sent": "I can see something like you can create this, plot it.",
                    "label": 0
                },
                {
                    "sent": "You know you see your time versus test.",
                    "label": 0
                },
                {
                    "sent": "And so very quickly you get already reasonable results.",
                    "label": 0
                },
                {
                    "sent": "And The thing is, for some datasets, which is this one adult?",
                    "label": 0
                },
                {
                    "sent": "So maybe you know it's already a lot of noise, so why do you want to really go to the minimum?",
                    "label": 0
                },
                {
                    "sent": "While there is no need for high accuracy?",
                    "label": 0
                },
                {
                    "sent": "So what you would like if you would very quickly get an answer and then let's say you get you go 1 accuracy level further and if you see or there's a strong drop off, yeah now it makes sense to go on and to desire more OK, but then after some time and this is very quickly this is a lock again very quickly it will tell you something like.",
                    "label": 0
                },
                {
                    "sent": "OK, now there's doesn't change much anymore.",
                    "label": 0
                },
                {
                    "sent": "So there's no need to go further.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Stop.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So so exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah and OK.",
                    "label": 0
                },
                {
                    "sent": "So I will tell Mark The thing is how can you innovate?",
                    "label": 0
                },
                {
                    "sent": "Smartly extend the solution to a further increase the subset size.",
                    "label": 0
                },
                {
                    "sent": "I mean, how can you smartly extend?",
                    "label": 0
                },
                {
                    "sent": "And it's very easy actually.",
                    "label": 0
                },
                {
                    "sent": "OK. Good.",
                    "label": 0
                },
                {
                    "sent": "So now yeah, I told you I want.",
                    "label": 0
                },
                {
                    "sent": "Ideally you want.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have K growing and Meanwhile you want to see.",
                    "label": 0
                },
                {
                    "sent": "So this is the subset size and.",
                    "label": 0
                },
                {
                    "sent": "And you want to see if your objective function doesn't change anymore.",
                    "label": 0
                },
                {
                    "sent": "There's no need to go on, it's OK, and this objective function in the primal.",
                    "label": 0
                },
                {
                    "sent": "It really relates to the test error.",
                    "label": 0
                },
                {
                    "sent": "So Fleck objective function there means that you're done in a way, right, which is not the case in the dual.",
                    "label": 0
                },
                {
                    "sent": "So small change in the dual can make dramatic change in the primal.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, so the first thing to do is actually you take all points OK and then you make you do you.",
                    "label": 0
                },
                {
                    "sent": "You minimize, but not.",
                    "label": 0
                },
                {
                    "sent": "You don't take it.",
                    "label": 0
                },
                {
                    "sent": "And I mean you don't use the minimization to express the solution.",
                    "label": 0
                },
                {
                    "sent": "You just check how far using the point.",
                    "label": 0
                },
                {
                    "sent": "How far can I go and then?",
                    "label": 0
                },
                {
                    "sent": "So this is very expensive, but nevertheless.",
                    "label": 0
                },
                {
                    "sent": "It will tell you, so that's the best you can do.",
                    "label": 0
                },
                {
                    "sent": "You know it's the best one guy which will bring you which somehow contributes most objective function.",
                    "label": 0
                },
                {
                    "sent": "And the other way is 1.",
                    "label": 0
                },
                {
                    "sent": "One thing just subsample from the data and then only select the best of the pool actually and we will do this.",
                    "label": 0
                },
                {
                    "sent": "But this is not the only.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now you are somehow in the domain of active learning.",
                    "label": 0
                },
                {
                    "sent": "You say?",
                    "label": 0
                },
                {
                    "sent": "What's the pattern I should use which is which contributes to the most and they're def different strategies.",
                    "label": 0
                },
                {
                    "sent": "So one thing is you know you don't matter, you just roll.",
                    "label": 0
                },
                {
                    "sent": "You flip a coin and just take one.",
                    "label": 0
                },
                {
                    "sent": "So that's the random approach and something which is very interesting is if you take the one where which.",
                    "label": 0
                },
                {
                    "sent": "So yeah, if you take a pattern which is as most independent to your current subset, OK, it adds a dimension, obviously.",
                    "label": 0
                },
                {
                    "sent": "Then it's actually almost like random, in a way.",
                    "label": 0
                },
                {
                    "sent": "That's actually excuse me, yeah, so here, but I mean this.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's this is significant.",
                    "label": 0
                },
                {
                    "sent": "Anyway, yeah, So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "OK so this.",
                    "label": 0
                },
                {
                    "sent": "You know this is just one strategy you could try and now The thing is we did this and now the interpretation would I would say.",
                    "label": 0
                },
                {
                    "sent": "And this is a hypothesis so statement in probability means.",
                    "label": 0
                },
                {
                    "sent": "Actually, you know I choose a pattern which is as diverse from what I have so far, but somehow the pattern I chose.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have nothing to do with my classification rule.",
                    "label": 0
                },
                {
                    "sent": "No uh-huh.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so that's the so OK, you can calculate given a point and given a set of points, I can calculate something like how far, what's the distance of this point to spend of this point?",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "And then you can say I'll take the one which is furthest away actually first I mean.",
                    "label": 0
                },
                {
                    "sent": "Since in the way it will contribute most to the span, but The thing is, the span is not really what you want 'cause what you want is you want to classify.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah, no, that doesn't mean it's.",
                    "label": 0
                },
                {
                    "sent": "It means only that I checked that I checked the points which are farthest away from my current expressive subset, but it can be that the farthest point isn't away good for classification as well.",
                    "label": 1
                },
                {
                    "sent": "I mean, there's no, there's no really critical thing it can be.",
                    "label": 0
                },
                {
                    "sent": "Fun.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can do this as well and.",
                    "label": 0
                },
                {
                    "sent": "Actually so.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "The problem with this is.",
                    "label": 0
                },
                {
                    "sent": "I don't think that's a good thing because what you do is you will keep adding points which are maybe mislabeled anyway, so I don't know if you really want to take the maximum error directly.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want?",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe it's a good thing.",
                    "label": 0
                },
                {
                    "sent": "What I mean, what we did is, I mean there's something like.",
                    "label": 0
                },
                {
                    "sent": "What is the next in a way to the margin?",
                    "label": 0
                },
                {
                    "sent": "So where are you most uncertain?",
                    "label": 1
                },
                {
                    "sent": "It does not need to be the one where you making the biggest error.",
                    "label": 0
                },
                {
                    "sent": "Let's say assume you have an outlier.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say 99% of your data.",
                    "label": 0
                },
                {
                    "sent": "This describe your classification rule, and there is one big Gaussian block in the opposite side, so it will have the biggest gradient.",
                    "label": 1
                },
                {
                    "sent": "But somehow it won't contribute most to classifying.",
                    "label": 0
                },
                {
                    "sent": "If I just count 01.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually yeah, this one I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean you know The thing is, this is one of four, it's not clear.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it will directly go down to the minimum, but The thing is, you can.",
                    "label": 0
                },
                {
                    "sent": "I mean it's up to discussion.",
                    "label": 0
                },
                {
                    "sent": "It's not what I mean.",
                    "label": 0
                },
                {
                    "sent": "I don't know if this is really good or if this one is better, etc.",
                    "label": 0
                },
                {
                    "sent": "It depends so, but if you think yeah you want to go direct, minimize then of course the one with the maximum error.",
                    "label": 0
                },
                {
                    "sent": "It will do this, but it's expensive.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's up to discussion.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't know which one is best.",
                    "label": 0
                },
                {
                    "sent": "Just a remark, yeah.",
                    "label": 0
                },
                {
                    "sent": "Vector machine E. Lawrence yeah and he uses it.",
                    "label": 0
                },
                {
                    "sent": "The the entropy reduction of the cost.",
                    "label": 0
                },
                {
                    "sent": "Which is next one.",
                    "label": 0
                },
                {
                    "sent": "That might be something interesting, yeah, maybe.",
                    "label": 0
                },
                {
                    "sent": "I know that's very interesting, but it's somehow out of my knowledge thing.",
                    "label": 0
                },
                {
                    "sent": "Station.",
                    "label": 0
                },
                {
                    "sent": "Incomplete.",
                    "label": 0
                },
                {
                    "sent": "Constantly.",
                    "label": 0
                },
                {
                    "sent": "The yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I see I have nice, but nevertheless I mean it's not really what I mean.",
                    "label": 0
                },
                {
                    "sent": "It's in a way, there's no link to the wise in the sense I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah I will collect everything afterwards.",
                    "label": 0
                },
                {
                    "sent": "OK, so is there any other comments on this?",
                    "label": 0
                },
                {
                    "sent": "OK, so why don't you?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just discuss it afterwards and I can go on this later.",
                    "label": 0
                },
                {
                    "sent": "OK, so and The thing is.",
                    "label": 0
                },
                {
                    "sent": "If you So what am I doing?",
                    "label": 0
                },
                {
                    "sent": "I'm doing the student steps and I know it's very quick to make rank 1 updates to my passion and.",
                    "label": 0
                },
                {
                    "sent": "So if you don't.",
                    "label": 0
                },
                {
                    "sent": "So if I assume that the number of patterns in a way everything is support vector which I have.",
                    "label": 0
                },
                {
                    "sent": "Then it's like.",
                    "label": 0
                },
                {
                    "sent": "Ridge regression and so.",
                    "label": 0
                },
                {
                    "sent": "So I incremented the update points for my rich regression solution.",
                    "label": 0
                },
                {
                    "sent": "OK, so I can do this updates an only sometimes.",
                    "label": 0
                },
                {
                    "sent": "You can actually train.",
                    "label": 0
                },
                {
                    "sent": "You can reiterate, retrain everything.",
                    "label": 0
                },
                {
                    "sent": "OK, using the subset.",
                    "label": 0
                },
                {
                    "sent": "So The thing is here you keep the old direction information of the Hessian and of the gradient and your update with the new point anyway.",
                    "label": 0
                },
                {
                    "sent": "But obviously this is not correct.",
                    "label": 0
                },
                {
                    "sent": "So yeah, because the old part, the old directional information is wrong once you have updated one guy, but nevertheless.",
                    "label": 0
                },
                {
                    "sent": "Making just sometimes corrections to direction information gives you a further speedup.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And yeah, and yeah you can use multi class and it's again the same phenomenon that the primal objective gives you an idea about how I mean is directly linked to the test error and you can so going step by step you can check if it's worth to extend the subset or not.",
                    "label": 1
                },
                {
                    "sent": "And to this end you can during optimization it's you have somehow control.",
                    "label": 0
                },
                {
                    "sent": "If it's worth to go on or not.",
                    "label": 0
                },
                {
                    "sent": "If you go, if you solve the best optimization directly and you so you ask for the minimum directly, that's the optimization approach.",
                    "label": 0
                },
                {
                    "sent": "Then you end up here, but you might not worth it.",
                    "label": 0
                },
                {
                    "sent": "So OK, so in a way to summarize this is.",
                    "label": 0
                },
                {
                    "sent": "I think it's important to have somehow enough where you can choose.",
                    "label": 1
                },
                {
                    "sent": "How much you want for the currency and it has you trade off.",
                    "label": 0
                },
                {
                    "sent": "Actually the time you need to get the answer.",
                    "label": 0
                },
                {
                    "sent": "Alright, OK. Now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Will do.",
                    "label": 0
                },
                {
                    "sent": "So this is nice, but you know there is a crucial thing is the square slack.",
                    "label": 0
                },
                {
                    "sent": "So we take another loss and then you get some problems, right?",
                    "label": 0
                },
                {
                    "sent": "So the dual never delay.",
                    "label": 1
                },
                {
                    "sent": "Its dual formulation has its existence existence right?",
                    "label": 0
                },
                {
                    "sent": "And you're interested.",
                    "label": 0
                },
                {
                    "sent": "How can you really speed up?",
                    "label": 0
                },
                {
                    "sent": "What are the strategies there?",
                    "label": 0
                },
                {
                    "sent": "OK, so now on the dual problem.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's like.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are several things going on.",
                    "label": 0
                },
                {
                    "sent": "One thing is the composition techniques in general, say, give me this point in this point, and then you have to provide the column of this particular kernel matrix at that part, right?",
                    "label": 0
                },
                {
                    "sent": "So what is very important is things like caching.",
                    "label": 0
                },
                {
                    "sent": "And further, and what's also important is the strategy how you choose points.",
                    "label": 0
                },
                {
                    "sent": "And let us see the state of the art, the composite techniques.",
                    "label": 0
                },
                {
                    "sent": "What happens if we just get a lot of noise?",
                    "label": 0
                },
                {
                    "sent": "OK, So what ends if you have noise?",
                    "label": 0
                },
                {
                    "sent": "And what if we increase the number of patterns for training?",
                    "label": 0
                },
                {
                    "sent": "So there's great theorem.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From Stainbach from 2004 it tells you it directly tells you that.",
                    "label": 0
                },
                {
                    "sent": "The link between the number of patterns you're going to have, the number of support vectors are going to have.",
                    "label": 0
                },
                {
                    "sent": "Which is the direct indicator of the cache size and everything and.",
                    "label": 0
                },
                {
                    "sent": "Relates this quantity 2 how many patterns for any patterns you have and about the noise ratio.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Tell, I mean this.",
                    "label": 0
                },
                {
                    "sent": "Actually it says.",
                    "label": 0
                },
                {
                    "sent": "If you go up with the base rate, so if you get a problem, OK, it has some assume you would know the base risk then.",
                    "label": 0
                },
                {
                    "sent": "And you get another problem and use the same optimizer and there's higher noise then.",
                    "label": 0
                },
                {
                    "sent": "Actually there is a relation among these quantities, and these quantities are here in the case.",
                    "label": 0
                },
                {
                    "sent": "The rank of econometrics, so it tells you what are the puts information in the patterns.",
                    "label": 0
                },
                {
                    "sent": "This is how many patterns don't hit the bound so our non boundaries protect us and this is how many.",
                    "label": 0
                },
                {
                    "sent": "Alpha, so variables hit the bounds actually and you can see the the direct thing once we still growing is the number of.",
                    "label": 0
                },
                {
                    "sent": "Patterns which hit the bounds OK so.",
                    "label": 0
                },
                {
                    "sent": "If you are OK, so I mean equivalently, you know, just fix this and keep and make this variety so it's the same effect.",
                    "label": 0
                },
                {
                    "sent": "So the number of training patterns to see only the ones that the balance will actually grow, and this is nice because you can often if you design for example caching strategies, there is no.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can consider a very noisy problem.",
                    "label": 0
                },
                {
                    "sent": "You will have the same effects.",
                    "label": 0
                },
                {
                    "sent": "Then in a problem where you have a lot of where you have a lot of patterns.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "It's going.",
                    "label": 0
                },
                {
                    "sent": "So why is this?",
                    "label": 0
                },
                {
                    "sent": "Why is this?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This issue a problem, so I told you you need somehow too.",
                    "label": 0
                },
                {
                    "sent": "Once you have a pattern you need to provide the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, now if we see if we make this decomposition approach like small then you can see.",
                    "label": 0
                },
                {
                    "sent": "The following so let's make this type pattern where we have just very clear noise region.",
                    "label": 0
                },
                {
                    "sent": "OK and.",
                    "label": 0
                },
                {
                    "sent": "We make a histogram over the cache Miss which end over the patterns.",
                    "label": 0
                },
                {
                    "sent": "So ideally if you have a lot of RAM you can catch everything.",
                    "label": 0
                },
                {
                    "sent": "You calculate everything and you're done.",
                    "label": 0
                },
                {
                    "sent": "But if you have less, if not that much RAM, you will have cache misses and then this means Kashmir miss means delete the ultra completely, calculate the neuro and we can make a histogram about where are the most cash methods and what you see is.",
                    "label": 0
                },
                {
                    "sent": "For 30% of the time, the cache misses exactly at the point at, say at the region where there is noise actually.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting Lee.",
                    "label": 0
                },
                {
                    "sent": "Here outside, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "I mean it has to be there.",
                    "label": 0
                },
                {
                    "sent": "Precisely so this is yeah, so this.",
                    "label": 0
                },
                {
                    "sent": "The margin actually is like this, but instead of the noise you have stuff there, so that's fine.",
                    "label": 0
                },
                {
                    "sent": "OK, so this tells me now.",
                    "label": 0
                },
                {
                    "sent": "I have noise and this noise makes me paying a lot of money 'cause most of the time I'm busy here recalculating stuff right?",
                    "label": 0
                },
                {
                    "sent": "So what can I do about this?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK so yeah.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Almost direct relation of standards, results and that's what I told you.",
                    "label": 0
                },
                {
                    "sent": "Number of cache misses are linear related to the noise level and so this is again as before the base.",
                    "label": 1
                },
                {
                    "sent": "So problems with varying based risk or varying size problems with fixed base risk.",
                    "label": 0
                },
                {
                    "sent": "And you see that the number of questions are linear.",
                    "label": 0
                },
                {
                    "sent": "I told you this.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Sorry guys.",
                    "label": 0
                },
                {
                    "sent": "It's new so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so The thing is how do I, you know, I don't want this.",
                    "label": 0
                },
                {
                    "sent": "I know I'm paying money for the noise actually, so how can you carefully choose patterns in a very, very quick way?",
                    "label": 0
                },
                {
                    "sent": "Such as you can say this is a noise level and this is a good point.",
                    "label": 0
                },
                {
                    "sent": "OK, and there's.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very, very old trick.",
                    "label": 0
                },
                {
                    "sent": "It's from nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "It's like a news neighbors, you just keep every point and if you want to predict it doesn't matter how many points you have, right?",
                    "label": 0
                },
                {
                    "sent": "It's the same thing and people so people thought about what can we do to speed up nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "And there's so one strategy is called condensation.",
                    "label": 0
                },
                {
                    "sent": "It's it does.",
                    "label": 0
                },
                {
                    "sent": "It just looks at so it splits the data and trains and you randomly then trains the nearest neighbor classifier.",
                    "label": 0
                },
                {
                    "sent": "And makes predictions over the other split OK and then if it's if I am uncertain about a pattern, so several splits say something different about this guy.",
                    "label": 0
                },
                {
                    "sent": "I dropped this guy actually.",
                    "label": 0
                },
                {
                    "sent": "So here the nearest neighbors will say that this is a red dye, but it was a blue guy, so I delete this.",
                    "label": 0
                },
                {
                    "sent": "Although I'm sorry I missed it up, it's actually the other way around.",
                    "label": 0
                },
                {
                    "sent": "So sorry, sorry, sorry.",
                    "label": 0
                },
                {
                    "sent": "So yeah, exactly condensation is you keep this point.",
                    "label": 0
                },
                {
                    "sent": "So if you are uncertain you keep it.",
                    "label": 0
                },
                {
                    "sent": "If you if you're very certain, then somehow it does not tell you anything about your problem so you drop it that way around and somehow the symmetry.",
                    "label": 0
                },
                {
                    "sent": "I was confused.",
                    "label": 0
                },
                {
                    "sent": "It's editing there you delete everything where you are uncertain about the patterns.",
                    "label": 0
                },
                {
                    "sent": "So this if you train an L1 SCM, actually you have this already in a way, but we don't have this.",
                    "label": 0
                },
                {
                    "sent": "Right and how can we implement this very quickly?",
                    "label": 0
                },
                {
                    "sent": "It's very simple, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, OK, I will come back later this commit.",
                    "label": 0
                },
                {
                    "sent": "So this is used cross validation.",
                    "label": 0
                },
                {
                    "sent": "You just make you know SVM training is cubic in the number of support vectors so it makes sense to train on a very small fraction.",
                    "label": 0
                },
                {
                    "sent": "So and this is quick so you can use cross validation and you train on the subsets and then you can vote about patterns OK?",
                    "label": 0
                },
                {
                    "sent": "So you can say, let's.",
                    "label": 0
                },
                {
                    "sent": "Let's say FS is the rule which you support vector machine rule which you got by training on a set S and this is the corresponding pattern of.",
                    "label": 0
                },
                {
                    "sent": "A test point XI.",
                    "label": 0
                },
                {
                    "sent": "So now if you average over the splits, you can say so this is bigger.",
                    "label": 0
                },
                {
                    "sent": "1 means I'm very certain about it.",
                    "label": 0
                },
                {
                    "sent": "OK, my so all the splits say give the same label then this point has no information.",
                    "label": 0
                },
                {
                    "sent": "You can drop it so this conversation and editing is like this smaller so smaller one would be uncertain, but smaller zero would be.",
                    "label": 0
                },
                {
                    "sent": "It's actually wrong classified.",
                    "label": 0
                },
                {
                    "sent": "So and then you remove this and this is very quickly.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is very very easy to do.",
                    "label": 0
                },
                {
                    "sent": "We train independence VM's you don't need to implement anything, you just need to do some preprocessing by splitting your datasets.",
                    "label": 0
                },
                {
                    "sent": "So you do this, you split datasets, training your SCM with favorite solver and afterwards you somehow vote about labels.",
                    "label": 0
                },
                {
                    "sent": "And then you can say you know you can.",
                    "label": 0
                },
                {
                    "sent": "We can drop points which are very far away and where we are very certain about and we can drop points where we are really certain that they are misclassified.",
                    "label": 0
                },
                {
                    "sent": "OK, so once you have done this, you repeat your training.",
                    "label": 0
                },
                {
                    "sent": "Sounds very easy and dubious.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The interesting thing is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is several standard test sets used in machine learning, from the UCI data set, and it does remarkably well.",
                    "label": 0
                },
                {
                    "sent": "Actually it's it produces rules which are much sparser OK because you don't have these points at the upper bound anymore.",
                    "label": 0
                },
                {
                    "sent": "I mean several of them.",
                    "label": 0
                },
                {
                    "sent": "A lot of them will be actually dropped from the training set, so we don't need to worry about them anymore, and it allows you to scale up.",
                    "label": 0
                },
                {
                    "sent": "The data set size OK because you can if you say I have end points and you have an idea about the noise and a lot of you can drop a lot of points without sacrificing something.",
                    "label": 0
                },
                {
                    "sent": "OK so nice, but actually there is a problem.",
                    "label": 0
                },
                {
                    "sent": "So I mean what?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By doing here I say yeah this point I like.",
                    "label": 0
                },
                {
                    "sent": "In this point I don't like OK so.",
                    "label": 0
                },
                {
                    "sent": "But The thing is, since I'm I'm doing this cross validation, there is no really.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't have the density, so there is something like we don't have the real control and for this reason if I give you a data set, so like what I plot is.",
                    "label": 0
                },
                {
                    "sent": "What you see here is the histogram over the margins.",
                    "label": 0
                },
                {
                    "sent": "And this is the original data set is with thin line and then the edited data set is a thick line.",
                    "label": 0
                },
                {
                    "sent": "You can see that by doing this voting and editing, you somehow really changed problem nature in a way.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I really flip actually the for example.",
                    "label": 0
                },
                {
                    "sent": "For example, here I had more blue guys in red guys and now I totally flipped it around.",
                    "label": 0
                },
                {
                    "sent": "So this is there is a problem here.",
                    "label": 0
                },
                {
                    "sent": "So there's still work going on.",
                    "label": 0
                },
                {
                    "sent": "OK, nevertheless.",
                    "label": 0
                },
                {
                    "sent": "There is a spin.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's the question from yesterday, so doing simply cross validation and then estimating the margin location.",
                    "label": 0
                },
                {
                    "sent": "It can be used to initialize lagrangians.",
                    "label": 1
                },
                {
                    "sent": "I know directly if something is far in interior.",
                    "label": 0
                },
                {
                    "sent": "I know it's zero and I know if I'm very uncertain I know it's at the bound.",
                    "label": 0
                },
                {
                    "sent": "OK, so and this is actually very good 'cause now it's like a very cheap hot start to your real.",
                    "label": 0
                },
                {
                    "sent": "Optimization procedure and Furthermore it allows you to say yeah, this.",
                    "label": 0
                },
                {
                    "sent": "This patterns I will need in the cache directly.",
                    "label": 0
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "Everybody.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but if I do this the problem is I mean I also have to calculate others.",
                    "label": 0
                },
                {
                    "sent": "I mean then I will only catch the error points.",
                    "label": 0
                },
                {
                    "sent": "So actually the problem is that's the cache miss right?",
                    "label": 0
                },
                {
                    "sent": "If you do the composition scheme.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, but then somehow that means to all never going to cash parts which you might need later on, right?",
                    "label": 0
                },
                {
                    "sent": "So I mean it's not.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's I don't know if you want if your favorite through list only cash good points or only cash back points.",
                    "label": 0
                },
                {
                    "sent": "In a way, it's tricky.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, you can.",
                    "label": 0
                },
                {
                    "sent": "Some of them you can lock directly because you know you're going to need them.",
                    "label": 0
                },
                {
                    "sent": "That's true, you can do this.",
                    "label": 0
                },
                {
                    "sent": "So OK, this we can do.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is, if you do start decomposition, you start at 00 OK and then you rule the W is changing during optimization, but I already used this changing about my decision if I should.",
                    "label": 0
                },
                {
                    "sent": "Cash a column or not, so by just simply doing this hot start I match.",
                    "label": 0
                },
                {
                    "sent": "I have something which is much more.",
                    "label": 0
                },
                {
                    "sent": "It's much more reliable about the actual state of the alphas then using this intermediate solutions.",
                    "label": 0
                },
                {
                    "sent": "So that's very nice, but nevertheless I mean ideally I would like to have the same what I had in the primer, so I would have something like control choice through which if I increase the subset size, gives me a better generalization and it does not destroy the statistical properties of the problem, which I can't ensure right now.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, yeah, we actually almost done.",
                    "label": 0
                },
                {
                    "sent": "So come to conditions and at the end so there there is again citation which regard which is regarding the discussion yesterday.",
                    "label": 0
                },
                {
                    "sent": "So what I claim is.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Machine learning is not equal optimization.",
                    "label": 1
                },
                {
                    "sent": "The demands are different and this way.",
                    "label": 0
                },
                {
                    "sent": "So what I say the vicinity of a solution OK is already sufficient in away so.",
                    "label": 0
                },
                {
                    "sent": "And then Furthermore, it's actually from application point important.",
                    "label": 1
                },
                {
                    "sent": "If you do not want.",
                    "label": 0
                },
                {
                    "sent": "That much.",
                    "label": 1
                },
                {
                    "sent": "OK, so you demand less inaccuracy.",
                    "label": 0
                },
                {
                    "sent": "OK, you can do a lot of more things away.",
                    "label": 0
                },
                {
                    "sent": "I don't know how you would do this with.",
                    "label": 0
                },
                {
                    "sent": "Look for the minimum I have.",
                    "label": 0
                },
                {
                    "sent": "I don't know how you would encode this and here we have a mechanism very old, but we can use this.",
                    "label": 0
                },
                {
                    "sent": "It's early stopping.",
                    "label": 0
                },
                {
                    "sent": "We can use this mechanism to really control if you should go further or not.",
                    "label": 0
                },
                {
                    "sent": "And last statement is, you know convexity is good.",
                    "label": 0
                },
                {
                    "sent": "Duality is great, but I mean.",
                    "label": 0
                },
                {
                    "sent": "Some, I mean, maybe you know.",
                    "label": 0
                },
                {
                    "sent": "Just because it's there, you don't need to use it.",
                    "label": 0
                },
                {
                    "sent": "OK, OK so this is the thing.",
                    "label": 0
                },
                {
                    "sent": "If you have to do approximations, do it in with the physical quantities of interest and let me finish my talk with.",
                    "label": 1
                },
                {
                    "sent": "One cetacean.",
                    "label": 0
                },
                {
                    "sent": "So this is children in the author of.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "SCM, which is state of the art server.",
                    "label": 0
                },
                {
                    "sent": "So this is a coauthor, so.",
                    "label": 0
                },
                {
                    "sent": "Citation is wrong.",
                    "label": 0
                },
                {
                    "sent": "This is Michael Kerns.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean I have them from personal communications, so I think is right.",
                    "label": 0
                },
                {
                    "sent": "They have.",
                    "label": 0
                },
                {
                    "sent": "This is a citation problem that I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, I just maybe I. I mean, maybe yeah, anyway is something which I would also say I guess so it says if someone puts a gun on my head and asked me to do model selection, you know yesterday that discussion by minimizing the cost function an extra day refer to about like we also we had yesterday then or I choose cross validation.",
                    "label": 1
                },
                {
                    "sent": "The answers I choose cross relation.",
                    "label": 0
                },
                {
                    "sent": "So optimization is great but this one gives me the real question.",
                    "label": 0
                },
                {
                    "sent": "Interest, so I do this one.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Personal statements are there.",
                    "label": 0
                },
                {
                    "sent": "The discussions later though.",
                    "label": 0
                },
                {
                    "sent": "Conversion.",
                    "label": 0
                },
                {
                    "sent": "Non smooth.",
                    "label": 0
                },
                {
                    "sent": "So The thing is that smoothness will probably cut off the Max.",
                    "label": 0
                },
                {
                    "sent": "Yes, but you know the Max.",
                    "label": 0
                },
                {
                    "sent": "So somehow I wrote it down as this 'cause it just tells me for gradient and for Hessian calculation select patterns.",
                    "label": 0
                },
                {
                    "sent": "So, so it's not like I'm not calculating next there.",
                    "label": 0
                },
                {
                    "sent": "Ideally you would have a lot of inequality's and a slack, and then you have.",
                    "label": 0
                },
                {
                    "sent": "I mean, I can write it down and then sum over all the slack squared.",
                    "label": 0
                },
                {
                    "sent": "It's the same thing.",
                    "label": 0
                },
                {
                    "sent": "I just rewrote it in one line, which I can do only four squares legs.",
                    "label": 0
                },
                {
                    "sent": "I have a constraint problem.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Which one?",
                    "label": 0
                },
                {
                    "sent": "My sample.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's no, there's no constant, it's always bigger than 0, right?",
                    "label": 0
                },
                {
                    "sent": "I knew it was.",
                    "label": 0
                },
                {
                    "sent": "Your second did you enter doesn't exist.",
                    "label": 0
                },
                {
                    "sent": "Alright, OK, but so.",
                    "label": 0
                },
                {
                    "sent": "So this continues the same, but I mean I just met this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK OK OK. That is a question.",
                    "label": 0
                },
                {
                    "sent": "OK, I agree.",
                    "label": 0
                },
                {
                    "sent": "I mean X video.",
                    "label": 0
                },
                {
                    "sent": "The news about this is great.",
                    "label": 0
                },
                {
                    "sent": "Your friend.",
                    "label": 0
                },
                {
                    "sent": "Other questions or comments?",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}