{
    "id": "oy5lfwy4vjcr25ywsnnzb2kprwnx2wkn",
    "title": "Gaussian Process Product Models for Nonparametric Nonstationarity",
    "info": {
        "author": [
            "Oliver Stegle, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "July 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/icml08_stegle_gpp/",
    "segmentation": [
        [
            "So the next speaker is Oliver Stable and this is joint work with Bryan Adams and he's going to be telling us about Gaussian process product models for nonparametric nonstationarity.",
            "OK, good afternoon everybody.",
            "This notice introduced I'm talking about customer support model from stationarity.",
            "And since Ryan can't be here today, I'm giving a talk about this work.",
            "So before I jump directly into the model that we're proposing, let me just give you an intuitive description that motivates the work."
        ],
        [
            "Trying to achieve here, if you look at the very Canonical problem of Gaussian process or what were the first intuitive application would be, namely the regression problem where we have a data set with an input output pairs and we basically what we want to do.",
            "We want to learn the latent function this data."
        ],
        [
            "Comes from where the GP comes into this problem is a prior over functions, so we encode our a priori belief about these functions that might give rise into this data by means of production process, and those beliefs can incorporate link skills amplitudes.",
            "Also whether it's differentiable or other properties."
        ],
        [
            "Of the process and if we do that, we get the well known picture.",
            "We can get marginal predictive means and error bars here represented by 1 standard deviation and can be solved."
        ],
        [
            "Problem.",
            "So what I want to talk a little bit later on about is sort of what are the beliefs of encode in this covariance function.",
            "So what the current functions in naive sensing code is?",
            "How outputs are two input locations?",
            "X&X prime covary, meaning how similar they are natively speaking at the current with their more correlated then we would expect that there is a stronger bond between them and that incorporates a lot of our smoothness assumption."
        ],
        [
            "In the regression problem.",
            "So.",
            "A very, very short slight wrapping up the GP machinery.",
            "Really.",
            "I'm not going into this.",
            "Let's assume we have chosen the covariance function.",
            "We have made.",
            "This assumption is then we can really get an elliptic solutions for the predictive distribution at a new input location X star.",
            "Given our data for the output Y star, which then will be a Gaussian with mean mu storenvy star.",
            "But the mean is really just the covariance matrix evaluated on the target outputs and the cross reference between the test input and the training data.",
            "And the predicted variance is based on the prior variance minus the reduced variance due to the data that we have been observed."
        ],
        [
            "And similarly we want to do optimization of hyperparameters, which is also well known.",
            "We can do this selection problem by optimizing the log marginal likelihood, the probability of the data under the model, and this is also in closed form in this vanilla GP model, so to speak.",
            "So what?"
        ],
        [
            "I want to be focusing now is a little bit more the covariance function.",
            "What are the beliefs that we can encode in this model in this coherence function?",
            "So very popular choice of covariances, this squared exponential covariance function?",
            "We basically assume that covariance between two inputs drops off exponentially with their distance, and for simplicity of just shows the single length scale for this joint dimensions and there are few hyperparameters.",
            "In this case it's the Lynx skill that gives the relative lengths on where we think these correlations significant and amplitude, and the observation noise level reference."
        ],
        [
            "During.",
            "And such covariance function that only depend on the distance that could be also any other distance metrics.",
            "I just generalize that in a second.",
            "Those are called stationary because they're basically translation invariant.",
            "It only depends on the distance of the input and not in their absolute positions.",
            "In this space of data that we're modeling."
        ],
        [
            "So why'd stationarity?",
            "Well, the key point and I hope that just convince you by telling you these small details that station occurrences are very, very intuitive.",
            "You can interpret them in terms of length scales in terms of amplitudes and noise and Moreover they are also easy to construct this person's theorem.",
            "We can look at them in terms of the spectral decomposition, it's just."
        ],
        [
            "Very, very handy representation, but they're also down to that.",
            "It's a strong assumption that we make in the data and the questions really does real data look like that?",
            "Do we believe that's the truth?",
            "That's a good model."
        ],
        [
            "For many datasets, one way out of this which has been discussed previously in other workers, we could predefine a nonstationary covariance function.",
            "In principle, we are free to just write down any covariance functions of X prime, as long as ideals and different covariance matrix in the end."
        ],
        [
            "So this is a really really difficult task, and it's also not intuitive how to do that and and Moreover, we also make assumptions about that.",
            "So we need a lot of domain specific knowledge about this problem domain."
        ],
        [
            "Select the work we are proposing in this regime is really to go down another alternative route which is to lower the nonstationarity to infer it to not make assumptions.",
            "Brother introduced light and processes that represent this nonstationarity."
        ],
        [
            "Implicitly.",
            "So let's go back to the scrap metal fence function.",
            "It shows a little bit of different representation where we just have a general matrix which also can account for spatial informations if you wish, and several extensions to this simple model has been proposed that at nonstationarity."
        ],
        [
            "One of them, for instance, with no hanging 2003, but they did.",
            "They made this matrix here nonstationary, or they introduced basically input dependence in it by modeling the components that with a variety of processes, keeping it positive net which is required at this development process."
        ],
        [
            "Another extension that has been discussed very early on is to extend the input noise.",
            "The observation noise as a nonstationary process.",
            "So make this input dependent and reference go back."
        ],
        [
            "1998 for that one, and the work we're considering here is in this part of looking at the amplitude C of zero.",
            "And if you look at this problem, literature gets much more sparse.",
            "There's a little bit of work, for instance, turn on Tony last year at NIPS proposed a model for Cascade Process, where also products and applications come into play.",
            "But this is a little bit different.",
            "Application domain.",
            "It's one of the few links this so now, having more."
        ],
        [
            "Motivated this work, let me just give you a brief outline of the main over."
        ],
        [
            "Talk, so we motivate the problem.",
            "The next step will be I discuss or introduce the Gaussian process product model to solve exactly this problem.",
            "Domain I talk about inference, apologize in this model, and then some reason."
        ],
        [
            "So the question process product model is a very simple intuitive extension to what we've been discussing so far.",
            "We model the observed outputs by a pointwise product of two functions, F of X&NG of XN this summer."
        ],
        [
            "Ocean noise and we then place independent priors on both of these functions F."
        ],
        [
            "The.",
            "And is used to notice that one of them is exponentiated, and this is really just to catch potential problems too.",
            "Multimodality, we would otherwise this sign ambiguity because potentially either both function could be positive or both negative, which gives us the same solutions but might be difficult to infer in the posterior."
        ],
        [
            "Because look at this model and a graphical representation.",
            "So this is a graphical representation.",
            "We have inputs that you observe.",
            "We have observed outputs.",
            "We have these two Gaussian processes that all depend on the inputs.",
            "One of them gives exponentiated and the product yields the outputs where it's important to note that these genes in Athens or not be coupled.",
            "The Gaussian process couples those ones with the smoothness assumption in time domain, which is not represented here.",
            "And both processors of hyperparameters directly to G, and we have this observation was over here and the Convention from now in this talk will be that we pick hyperparameters or hyper priors such that the FTP represents near stationary variations.",
            "So it's basically a shorter length scale process and the GGP really looks at these longer length scale nonstationary variations of amplitude."
        ],
        [
            "OK, so to give you a little bit of flavor how this model performance will look at some sample from it.",
            "So here we see samples from the joint model in blue, which is the product Fe to the G and the FTP and the GTP.",
            "And what we can see here is the amplitude is low.",
            "Basically the resulting function is very very close to 0, because this is your main function again, and if there is high, we modulate this FTP significantly in its exponentiated enhanced."
        ],
        [
            "Another interesting setting that I thought I show here is 1 setting where the FTP is actually very very noisy process.",
            "So in this case it is close to IID noise.",
            "There's not very much structure in this in this process, and we can sort of think about the GPS modulating this noise process here very low, and then it gets sort of more significantly amplified.",
            "So there's a really rich class of different assumptions and models that we can look at depending on how parameter settings, and I hope that you convinced that such data really is out there in reality, and this might be a good model to consider."
        ],
        [
            "So having specified the model, we want to look at inference, how do we can use this model to get posteriors out of our interest?"
        ],
        [
            "Processes.",
            "So first of all, let's look at the joint posterior of F&G of this two Gaussian processes.",
            "We have a few terms."
        ],
        [
            "Have a Gaussian process prior on F which is 0 mean with the covariance defined by the province."
        ],
        [
            "Parameters we have."
        ],
        [
            "Brown sheep and we have likely return the couples these two processes in this rather complicated way to the observations."
        ],
        [
            "And as you can guess and already see, this is serious, pretty intractable.",
            "The nice analytic integrating out properties of the render logically don't apply here anymore, otherwise it will not be very interesting."
        ],
        [
            "So what affects?",
            "Or what properties affect our choice of approximation for this problem?",
            "First of all, we expect that the posterior is near to be Gaussian simply because with these two Gaussian process priors and so there's a natural assumption to make that we might assume that any abortion properties in the posterior also the likely factors in these end independent terms and sort of all the alarm rings role that this has been seen.",
            "Another approximate problems in GP, and that is very important to notice here is that these likelihood term introduces complicated couplings.",
            "Between the two functions, so F energy and cannot be well approximated by a factor of distribution, for instance because they are very very tightly coupled by these.",
            "Nice model."
        ],
        [
            "So one approximation is very well suited for this information, which is in variational method and has to be applied to Gaussian process approximate inference quite a few times this success and it's well suited for such factorized likelihoods and can conserve these."
        ],
        [
            "Relations.",
            "So let me just review EP.",
            "Very very briefly one slide.",
            "I hope that is enough to remind everybody on where we are here.",
            "So the idea really is assume we have a complicated posterior over parameters theater given data.",
            "Or in this case is infinite infinite objects malfunctions, and this is basically the prior of the theater times the product over an independent and complicated like kilograms, and the idea here really is that the approximate this complicated posterior biosimilar one, where we replace the likely terms.",
            "This simple approximate likelihood terms."
        ],
        [
            "And once we have done this this choice, we have chosen our approximation.",
            "We can update these approximately term iteratively and this update is done by minimizing the KL diversions of diversions measure, which must the difference between distributions between the product of the prior and all the approximate factors.",
            "Sorry, except for the NTH one that we are considering, and the exact factor.",
            "And the product of the prior and all the approximate factor and also the approximate factor."
        ],
        [
            "And it boils down if we consider this model in more detail, that is equivalent to moment matching.",
            "So we only have to calculate the moments of the left expression and modify parameters to match those moments in our approximate term."
        ],
        [
            "So applying this to the GPM model.",
            "We have to choose this approximate distributions for our approximate factors.",
            "So if you look at the approximate posterior, we have these two GP priors over F&G and an approximate likelihood term of F&G."
        ],
        [
            "And jointly what we do now is we summarize these two priors in just one prior, where we have a covariance structure that block structured.",
            "Basically, we remain the."
        ],
        [
            "Of the priors.",
            "At the next step is that to choose a local approximation and we just follow the literature which has always, and there's the convenient idea here.",
            "Chosen Goshen for this approximation, and in this case we have a 2 dimensional Gulshan over F&G and jointly vectorial mean and covariance matrix that also covers the couplings between these two objects.",
            "And this term supposed to approximate the true likelihood that it was printed here for reference, that this complicated structure that we can."
        ],
        [
            "Dealer and just to make this explicit, these are the three parameters that are basically optimate via update via moment matching.",
            "Well, we only would need the 1st two.",
            "We keep this for other reasons that I come back later.",
            "So this is basically equivalent to the right moment and those two concocted from the 1st and 2nd moment so well known."
        ],
        [
            "So.",
            "Where a few details about the covariance structure over the approximation that we get, and I would like to point these out and we have discussed the priors block like so we have these two block like structures of the prior of C. Angie and I also emphasize that the approximate likely that we get there not be coupled.",
            "We have strong couplings between any element for data .1 in the F domain and one in the G domain which is with these little blocks here.",
            "So we multiply in the complete covariance structure with product over blockwise covariance structure.",
            "And this coupled likelihood, which then gives a global approximation.",
            "And that's an important point that this global approximation is able to solve this problem."
        ],
        [
            "A few more details about the implementation, an one important detail that enough time to go into is that we can't compute these moments analytically.",
            "That's not tractable, so we have to revert to 2 dimensional Gaussian quadrature for numerical integration, which is rather complicated and computation involved, but 50 product points are fine in this case, and we need on the order of 10 EP iterations, which is more than for other problems But still handle in the scheme.",
            "In practice, is sort of feasible up to about 1000 points.",
            "In the data space, which is rather large."
        ],
        [
            "Another note, because we would like to carry over all the things we do with energy to this model is how do we optimize hyperparameters, how to reset them, and ideally we'd like to use the likelihood the probability of the high performers given the data."
        ],
        [
            "And nice, the nice thing is that expectation propagation provides an approximation to this, so we can use these zeros moment."
        ],
        [
            "That be explicitly representatives earlier to calculate an Xbox Mid term of this likely."
        ],
        [
            "And how this works?",
            "I'm just briefly deriving that this is one way to split up the log marginal likelihood for the presentation.",
            "We can fill up in global approximation term that involves the joint posterior over these two latent functions there means."
        ],
        [
            "We have a prior term in there from the Gaussian process prior and all."
        ],
        [
            "So these local approximations with their mean and variance come into play and last not least."
        ],
        [
            "The term that we took care of, namely the zeros moment that we need to get this completed, so using this term it is a.",
            "Is it possible to optimize hyperparameters As for the standard GP, there's a small catch and that is that the neural integration proves not to be stable enough to calculate gradients on this expression.",
            "So we have to revert to gradient free optimization methods, which is certainly not visit."
        ],
        [
            "What is with this method?",
            "So one thing we haven't solved yet is how to make predictions in this model.",
            "What we can get easily out is a joint prediction over F. Angie, given you input, I skip over this step, but this is basically completely in line with the vanilla Gaussian process for the."
        ],
        [
            "Exclamation, but now we can do.",
            "We can give him this approximation, which is a joint caution.",
            "Again, I want to emphasize that there's this correlation was captured in the covariance matrix.",
            "We can derive approximate solutions for the posterior on the outputs, and one possibility is we could just create a mixture of caution.",
            "We can sample from the GP then condition on G results in the joint distribution over F&G and then create a mixture over all these terms which would then properly represent the hepatitis of the Gaussian times a lot normal."
        ],
        [
            "It's also possible in works very well in practice is to just linearize one of them.",
            "The exponential term 1st order, and then we get a predicted caution distribution in the handout that we use for the results actually."
        ],
        [
            "OK, that leads me to show you results of this model."
        ],
        [
            "Try to some data, but we've been looking at is comparing three models the vanilla Gaussian process sparse custom for this model introduced by Snails in Germany and our model, and the reason why we picked the PGP for comparison is because it's one of the few demonistic methods that don't use MCMC schemes that yield nonstationarity.",
            "In this case, it's a side effect.",
            "But still interesting model and we wanted these models on three datasets.",
            "The motorhome datasets with traditional data set for non stationarity and some stock data and human heart rate data sets and happy parameters were optimized for possible, namely in SP GP and vanilla GPV used gradient based maximum likely Type 2 commiseration for the GPM we used to grid to set the hyperparameters."
        ],
        [
            "So this is the first data set.",
            "This is the motorhome and data set is like where we have that the data points in red and the line up here is the invert gossan predicted distribution for the geovax process.",
            "And we can see in areas where there is not much variation, like here.",
            "GFX is slow and elevation increases.",
            "It goes up.",
            "Said really models the difference in amplitude variations in this data set quite nicely."
        ],
        [
            "That was also the case if you look for objective measures, for instance here the mean log probability with multiple filling tests, you can see that slightly higher for the GPM then for the other two models and the mean squared error where low is good sort of the inverse.",
            "We also get a slight improvement."
        ],
        [
            "The second data set we apply this to is this stop data.",
            "We really don't have much signal.",
            "I mean there is no mean really picked up.",
            "It's uncorrelated noise.",
            "But what happens here is that basically this noise processes modulated and this modulation is quite nicely picked up by the G of X process as well.",
            "Anne."
        ],
        [
            "As we can expect, in this case, the means good error for all three models all the same, because the mean is 0, but predictive distribution is significantly better for this model."
        ],
        [
            "And last not least, we looked at this human data set where it represents hardware database sample from an human.",
            "Here in the beginning the person is sleeping.",
            "There's not much happening.",
            "Then the activity starts and that is an obvious example for non stationarity in aptitude and we can see that this is represented by the posterior over G quite."
        ],
        [
            "Nicely and again the same picture, slight improvements.",
            "It's not dramatic, but certainly noticeable."
        ],
        [
            "OK, that leads me to summarize this talk.",
            "We introduced the GBM as a principled way too.",
            "Enable GPS for smoothly varying nonstationary amplitude variations.",
            "Expectation propagation deals very efficient inference in this model and future work on our side.",
            "To refine the quadrature AP to also get reasonably good grades out and to overcome this one."
        ],
        [
            "Outside thank you for listening.",
            "Question.",
            "Cortana text girlfriend for mixer.",
            "Hey.",
            "156 Yeah, it's a good question.",
            "I mean, I think the mixture of caution processes principle in more general model because it could also handle on the stationarity in amplitude.",
            "Sorry in length scales and other processes.",
            "I think what's nice about this model, if you have a strong belief that really the data that we've seen comes from this product, then we can model it here because we can really take care of this long range correlation.",
            "This GP really captures also correlations between areas that are far apart compared to these other process that we modulate Ng.",
            "And if that's true in the data that we look at for instance in hardware data we know.",
            "That humans have the same activity pattern 24 hours later and we really know there is this other structure in it.",
            "Then this model might be advantages.",
            "I think there's another issue there that in the mixture of GPS, if you move from one region to another, you can get discontinuity's as you transfer between the regions relying on the data to make things smooth or you have to do something else.",
            "Whereas in this model it looks to me like it's always going to be smooth.",
            "It's a slowly changing this nonstationarity rather than a rapid change in non stationarity which the mixture of GPS implies.",
            "Next yeah, but the gating function doesn't force.",
            "You can get a discontinuity, which could be a good thing, but in this model you're not going to get a discontinuity, which could also.",
            "Be a good thing.",
            "The casting process is always good.",
            "That was John, did you?",
            "Yeah, I have my head.",
            "Better.",
            "Model sending this update.",
            "There's nowhere to go from dictionary, so.",
            "Pop up alpaca talk about how does it end?",
            "Yeah, very very quick note.",
            "I mean basically you can get in quite a few cases.",
            "I just want to go back to this one slide here basically also uncertain Arity in noise amplitude noise.",
            "There is a it performs to some extent nonstationary noise modeling as well by the amplitude.",
            "But you're right, definitely skill not considering at all in this model at the moment, but we were thinking of is basically if you think about the general credit special function, how we can incorporate other nonstationary processes describing unstated.",
            "Out of these parameters, like long scale this model as well, the moment that's not tractable, simply because that would mean we don't even only have two processors to integrate our approximation, but three or four, but there's no principle reason why that's not possible as rather computational at the moment, but it's definitely something to consider for future work.",
            "So I have a related question to that that you talked about.",
            "You said characterized extensively the amplitude function as slowly varying, which makes sense because obviously there will be.",
            "It will try to fit the length scale itself if it gets fast varying, so I presume it's sufficient just to initialize it as slowly varying versus the.",
            "Function for it to find that you don't get problems with local minima.",
            "No, it's in practice it's not that big of a deal, but you sort of have to make the assumption that the length scale of the chichi processes, say, let's twice as big as the F process.",
            "Otherwise it gets confused by this identifiability issue.",
            "I mean, if there's no structure to be picked up, then this doesn't work.",
            "As I said, if you initialize it correspondingly, then it's.",
            "This works fine.",
            "And I had another question, so it's not really direct about this work, but you mentioned smitten O'hagan's work on varying length scales.",
            "I wasn't so familiar with that as I am with Mark Gibbs.",
            "Is it possible to characterize how that work on varying length scales differs from Mark Gibbs work on varying length scales?",
            "Or you're not so familiar with that?",
            "I'm less familiar with the second there, but.",
            "So does markets also consider different deformations of the space?",
            "I mean they basically considered well.",
            "I think he.",
            "I think the difference, maybe that they considered deforming the space in a non linear way before putting it in the Gaussian process.",
            "Whereas Mark sort of considered considered defamation.",
            "I don't know how to characterize that property, but it's a definition within the GP and actually causes a term to come out the front of the RBF kernel which has some weird effects they consider it.",
            "Nonstationary parameterisation of this deformation?",
            "So basically the parameters of this information and non stationary model violating processes?",
            "Maybe that's the difference because I thought to remember that markets was parametric.",
            "Is that yes?",
            "So sorry that's true, but you could work was non parametric.",
            "They had the latent processes that characterize this deformation matrix and this mapping.",
            "OK, so if there's no further questions if you just think all over again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next speaker is Oliver Stable and this is joint work with Bryan Adams and he's going to be telling us about Gaussian process product models for nonparametric nonstationarity.",
                    "label": 1
                },
                {
                    "sent": "OK, good afternoon everybody.",
                    "label": 0
                },
                {
                    "sent": "This notice introduced I'm talking about customer support model from stationarity.",
                    "label": 0
                },
                {
                    "sent": "And since Ryan can't be here today, I'm giving a talk about this work.",
                    "label": 0
                },
                {
                    "sent": "So before I jump directly into the model that we're proposing, let me just give you an intuitive description that motivates the work.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trying to achieve here, if you look at the very Canonical problem of Gaussian process or what were the first intuitive application would be, namely the regression problem where we have a data set with an input output pairs and we basically what we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to learn the latent function this data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comes from where the GP comes into this problem is a prior over functions, so we encode our a priori belief about these functions that might give rise into this data by means of production process, and those beliefs can incorporate link skills amplitudes.",
                    "label": 0
                },
                {
                    "sent": "Also whether it's differentiable or other properties.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the process and if we do that, we get the well known picture.",
                    "label": 0
                },
                {
                    "sent": "We can get marginal predictive means and error bars here represented by 1 standard deviation and can be solved.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So what I want to talk a little bit later on about is sort of what are the beliefs of encode in this covariance function.",
                    "label": 0
                },
                {
                    "sent": "So what the current functions in naive sensing code is?",
                    "label": 0
                },
                {
                    "sent": "How outputs are two input locations?",
                    "label": 0
                },
                {
                    "sent": "X&X prime covary, meaning how similar they are natively speaking at the current with their more correlated then we would expect that there is a stronger bond between them and that incorporates a lot of our smoothness assumption.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the regression problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A very, very short slight wrapping up the GP machinery.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "I'm not going into this.",
                    "label": 0
                },
                {
                    "sent": "Let's assume we have chosen the covariance function.",
                    "label": 0
                },
                {
                    "sent": "We have made.",
                    "label": 0
                },
                {
                    "sent": "This assumption is then we can really get an elliptic solutions for the predictive distribution at a new input location X star.",
                    "label": 0
                },
                {
                    "sent": "Given our data for the output Y star, which then will be a Gaussian with mean mu storenvy star.",
                    "label": 0
                },
                {
                    "sent": "But the mean is really just the covariance matrix evaluated on the target outputs and the cross reference between the test input and the training data.",
                    "label": 0
                },
                {
                    "sent": "And the predicted variance is based on the prior variance minus the reduced variance due to the data that we have been observed.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And similarly we want to do optimization of hyperparameters, which is also well known.",
                    "label": 0
                },
                {
                    "sent": "We can do this selection problem by optimizing the log marginal likelihood, the probability of the data under the model, and this is also in closed form in this vanilla GP model, so to speak.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to be focusing now is a little bit more the covariance function.",
                    "label": 0
                },
                {
                    "sent": "What are the beliefs that we can encode in this model in this coherence function?",
                    "label": 0
                },
                {
                    "sent": "So very popular choice of covariances, this squared exponential covariance function?",
                    "label": 0
                },
                {
                    "sent": "We basically assume that covariance between two inputs drops off exponentially with their distance, and for simplicity of just shows the single length scale for this joint dimensions and there are few hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "In this case it's the Lynx skill that gives the relative lengths on where we think these correlations significant and amplitude, and the observation noise level reference.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "During.",
                    "label": 0
                },
                {
                    "sent": "And such covariance function that only depend on the distance that could be also any other distance metrics.",
                    "label": 1
                },
                {
                    "sent": "I just generalize that in a second.",
                    "label": 1
                },
                {
                    "sent": "Those are called stationary because they're basically translation invariant.",
                    "label": 0
                },
                {
                    "sent": "It only depends on the distance of the input and not in their absolute positions.",
                    "label": 0
                },
                {
                    "sent": "In this space of data that we're modeling.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why'd stationarity?",
                    "label": 0
                },
                {
                    "sent": "Well, the key point and I hope that just convince you by telling you these small details that station occurrences are very, very intuitive.",
                    "label": 0
                },
                {
                    "sent": "You can interpret them in terms of length scales in terms of amplitudes and noise and Moreover they are also easy to construct this person's theorem.",
                    "label": 0
                },
                {
                    "sent": "We can look at them in terms of the spectral decomposition, it's just.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very, very handy representation, but they're also down to that.",
                    "label": 0
                },
                {
                    "sent": "It's a strong assumption that we make in the data and the questions really does real data look like that?",
                    "label": 1
                },
                {
                    "sent": "Do we believe that's the truth?",
                    "label": 0
                },
                {
                    "sent": "That's a good model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For many datasets, one way out of this which has been discussed previously in other workers, we could predefine a nonstationary covariance function.",
                    "label": 0
                },
                {
                    "sent": "In principle, we are free to just write down any covariance functions of X prime, as long as ideals and different covariance matrix in the end.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a really really difficult task, and it's also not intuitive how to do that and and Moreover, we also make assumptions about that.",
                    "label": 0
                },
                {
                    "sent": "So we need a lot of domain specific knowledge about this problem domain.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Select the work we are proposing in this regime is really to go down another alternative route which is to lower the nonstationarity to infer it to not make assumptions.",
                    "label": 0
                },
                {
                    "sent": "Brother introduced light and processes that represent this nonstationarity.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Implicitly.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to the scrap metal fence function.",
                    "label": 0
                },
                {
                    "sent": "It shows a little bit of different representation where we just have a general matrix which also can account for spatial informations if you wish, and several extensions to this simple model has been proposed that at nonstationarity.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of them, for instance, with no hanging 2003, but they did.",
                    "label": 0
                },
                {
                    "sent": "They made this matrix here nonstationary, or they introduced basically input dependence in it by modeling the components that with a variety of processes, keeping it positive net which is required at this development process.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another extension that has been discussed very early on is to extend the input noise.",
                    "label": 0
                },
                {
                    "sent": "The observation noise as a nonstationary process.",
                    "label": 1
                },
                {
                    "sent": "So make this input dependent and reference go back.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1998 for that one, and the work we're considering here is in this part of looking at the amplitude C of zero.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this problem, literature gets much more sparse.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of work, for instance, turn on Tony last year at NIPS proposed a model for Cascade Process, where also products and applications come into play.",
                    "label": 0
                },
                {
                    "sent": "But this is a little bit different.",
                    "label": 0
                },
                {
                    "sent": "Application domain.",
                    "label": 0
                },
                {
                    "sent": "It's one of the few links this so now, having more.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motivated this work, let me just give you a brief outline of the main over.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk, so we motivate the problem.",
                    "label": 0
                },
                {
                    "sent": "The next step will be I discuss or introduce the Gaussian process product model to solve exactly this problem.",
                    "label": 1
                },
                {
                    "sent": "Domain I talk about inference, apologize in this model, and then some reason.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question process product model is a very simple intuitive extension to what we've been discussing so far.",
                    "label": 0
                },
                {
                    "sent": "We model the observed outputs by a pointwise product of two functions, F of X&NG of XN this summer.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean noise and we then place independent priors on both of these functions F.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "And is used to notice that one of them is exponentiated, and this is really just to catch potential problems too.",
                    "label": 0
                },
                {
                    "sent": "Multimodality, we would otherwise this sign ambiguity because potentially either both function could be positive or both negative, which gives us the same solutions but might be difficult to infer in the posterior.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because look at this model and a graphical representation.",
                    "label": 0
                },
                {
                    "sent": "So this is a graphical representation.",
                    "label": 0
                },
                {
                    "sent": "We have inputs that you observe.",
                    "label": 0
                },
                {
                    "sent": "We have observed outputs.",
                    "label": 0
                },
                {
                    "sent": "We have these two Gaussian processes that all depend on the inputs.",
                    "label": 0
                },
                {
                    "sent": "One of them gives exponentiated and the product yields the outputs where it's important to note that these genes in Athens or not be coupled.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian process couples those ones with the smoothness assumption in time domain, which is not represented here.",
                    "label": 1
                },
                {
                    "sent": "And both processors of hyperparameters directly to G, and we have this observation was over here and the Convention from now in this talk will be that we pick hyperparameters or hyper priors such that the FTP represents near stationary variations.",
                    "label": 0
                },
                {
                    "sent": "So it's basically a shorter length scale process and the GGP really looks at these longer length scale nonstationary variations of amplitude.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to give you a little bit of flavor how this model performance will look at some sample from it.",
                    "label": 0
                },
                {
                    "sent": "So here we see samples from the joint model in blue, which is the product Fe to the G and the FTP and the GTP.",
                    "label": 1
                },
                {
                    "sent": "And what we can see here is the amplitude is low.",
                    "label": 0
                },
                {
                    "sent": "Basically the resulting function is very very close to 0, because this is your main function again, and if there is high, we modulate this FTP significantly in its exponentiated enhanced.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another interesting setting that I thought I show here is 1 setting where the FTP is actually very very noisy process.",
                    "label": 0
                },
                {
                    "sent": "So in this case it is close to IID noise.",
                    "label": 0
                },
                {
                    "sent": "There's not very much structure in this in this process, and we can sort of think about the GPS modulating this noise process here very low, and then it gets sort of more significantly amplified.",
                    "label": 0
                },
                {
                    "sent": "So there's a really rich class of different assumptions and models that we can look at depending on how parameter settings, and I hope that you convinced that such data really is out there in reality, and this might be a good model to consider.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So having specified the model, we want to look at inference, how do we can use this model to get posteriors out of our interest?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Processes.",
                    "label": 0
                },
                {
                    "sent": "So first of all, let's look at the joint posterior of F&G of this two Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "We have a few terms.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have a Gaussian process prior on F which is 0 mean with the covariance defined by the province.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameters we have.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Brown sheep and we have likely return the couples these two processes in this rather complicated way to the observations.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as you can guess and already see, this is serious, pretty intractable.",
                    "label": 0
                },
                {
                    "sent": "The nice analytic integrating out properties of the render logically don't apply here anymore, otherwise it will not be very interesting.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what affects?",
                    "label": 0
                },
                {
                    "sent": "Or what properties affect our choice of approximation for this problem?",
                    "label": 1
                },
                {
                    "sent": "First of all, we expect that the posterior is near to be Gaussian simply because with these two Gaussian process priors and so there's a natural assumption to make that we might assume that any abortion properties in the posterior also the likely factors in these end independent terms and sort of all the alarm rings role that this has been seen.",
                    "label": 1
                },
                {
                    "sent": "Another approximate problems in GP, and that is very important to notice here is that these likelihood term introduces complicated couplings.",
                    "label": 0
                },
                {
                    "sent": "Between the two functions, so F energy and cannot be well approximated by a factor of distribution, for instance because they are very very tightly coupled by these.",
                    "label": 0
                },
                {
                    "sent": "Nice model.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one approximation is very well suited for this information, which is in variational method and has to be applied to Gaussian process approximate inference quite a few times this success and it's well suited for such factorized likelihoods and can conserve these.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Relations.",
                    "label": 0
                },
                {
                    "sent": "So let me just review EP.",
                    "label": 0
                },
                {
                    "sent": "Very very briefly one slide.",
                    "label": 0
                },
                {
                    "sent": "I hope that is enough to remind everybody on where we are here.",
                    "label": 0
                },
                {
                    "sent": "So the idea really is assume we have a complicated posterior over parameters theater given data.",
                    "label": 0
                },
                {
                    "sent": "Or in this case is infinite infinite objects malfunctions, and this is basically the prior of the theater times the product over an independent and complicated like kilograms, and the idea here really is that the approximate this complicated posterior biosimilar one, where we replace the likely terms.",
                    "label": 0
                },
                {
                    "sent": "This simple approximate likelihood terms.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And once we have done this this choice, we have chosen our approximation.",
                    "label": 0
                },
                {
                    "sent": "We can update these approximately term iteratively and this update is done by minimizing the KL diversions of diversions measure, which must the difference between distributions between the product of the prior and all the approximate factors.",
                    "label": 1
                },
                {
                    "sent": "Sorry, except for the NTH one that we are considering, and the exact factor.",
                    "label": 0
                },
                {
                    "sent": "And the product of the prior and all the approximate factor and also the approximate factor.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it boils down if we consider this model in more detail, that is equivalent to moment matching.",
                    "label": 0
                },
                {
                    "sent": "So we only have to calculate the moments of the left expression and modify parameters to match those moments in our approximate term.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So applying this to the GPM model.",
                    "label": 0
                },
                {
                    "sent": "We have to choose this approximate distributions for our approximate factors.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the approximate posterior, we have these two GP priors over F&G and an approximate likelihood term of F&G.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And jointly what we do now is we summarize these two priors in just one prior, where we have a covariance structure that block structured.",
                    "label": 0
                },
                {
                    "sent": "Basically, we remain the.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the priors.",
                    "label": 0
                },
                {
                    "sent": "At the next step is that to choose a local approximation and we just follow the literature which has always, and there's the convenient idea here.",
                    "label": 0
                },
                {
                    "sent": "Chosen Goshen for this approximation, and in this case we have a 2 dimensional Gulshan over F&G and jointly vectorial mean and covariance matrix that also covers the couplings between these two objects.",
                    "label": 0
                },
                {
                    "sent": "And this term supposed to approximate the true likelihood that it was printed here for reference, that this complicated structure that we can.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dealer and just to make this explicit, these are the three parameters that are basically optimate via update via moment matching.",
                    "label": 0
                },
                {
                    "sent": "Well, we only would need the 1st two.",
                    "label": 0
                },
                {
                    "sent": "We keep this for other reasons that I come back later.",
                    "label": 0
                },
                {
                    "sent": "So this is basically equivalent to the right moment and those two concocted from the 1st and 2nd moment so well known.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Where a few details about the covariance structure over the approximation that we get, and I would like to point these out and we have discussed the priors block like so we have these two block like structures of the prior of C. Angie and I also emphasize that the approximate likely that we get there not be coupled.",
                    "label": 1
                },
                {
                    "sent": "We have strong couplings between any element for data .1 in the F domain and one in the G domain which is with these little blocks here.",
                    "label": 1
                },
                {
                    "sent": "So we multiply in the complete covariance structure with product over blockwise covariance structure.",
                    "label": 0
                },
                {
                    "sent": "And this coupled likelihood, which then gives a global approximation.",
                    "label": 0
                },
                {
                    "sent": "And that's an important point that this global approximation is able to solve this problem.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A few more details about the implementation, an one important detail that enough time to go into is that we can't compute these moments analytically.",
                    "label": 0
                },
                {
                    "sent": "That's not tractable, so we have to revert to 2 dimensional Gaussian quadrature for numerical integration, which is rather complicated and computation involved, but 50 product points are fine in this case, and we need on the order of 10 EP iterations, which is more than for other problems But still handle in the scheme.",
                    "label": 1
                },
                {
                    "sent": "In practice, is sort of feasible up to about 1000 points.",
                    "label": 0
                },
                {
                    "sent": "In the data space, which is rather large.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another note, because we would like to carry over all the things we do with energy to this model is how do we optimize hyperparameters, how to reset them, and ideally we'd like to use the likelihood the probability of the high performers given the data.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And nice, the nice thing is that expectation propagation provides an approximation to this, so we can use these zeros moment.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That be explicitly representatives earlier to calculate an Xbox Mid term of this likely.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how this works?",
                    "label": 0
                },
                {
                    "sent": "I'm just briefly deriving that this is one way to split up the log marginal likelihood for the presentation.",
                    "label": 0
                },
                {
                    "sent": "We can fill up in global approximation term that involves the joint posterior over these two latent functions there means.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a prior term in there from the Gaussian process prior and all.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these local approximations with their mean and variance come into play and last not least.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The term that we took care of, namely the zeros moment that we need to get this completed, so using this term it is a.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to optimize hyperparameters As for the standard GP, there's a small catch and that is that the neural integration proves not to be stable enough to calculate gradients on this expression.",
                    "label": 0
                },
                {
                    "sent": "So we have to revert to gradient free optimization methods, which is certainly not visit.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is with this method?",
                    "label": 0
                },
                {
                    "sent": "So one thing we haven't solved yet is how to make predictions in this model.",
                    "label": 0
                },
                {
                    "sent": "What we can get easily out is a joint prediction over F. Angie, given you input, I skip over this step, but this is basically completely in line with the vanilla Gaussian process for the.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exclamation, but now we can do.",
                    "label": 0
                },
                {
                    "sent": "We can give him this approximation, which is a joint caution.",
                    "label": 0
                },
                {
                    "sent": "Again, I want to emphasize that there's this correlation was captured in the covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "We can derive approximate solutions for the posterior on the outputs, and one possibility is we could just create a mixture of caution.",
                    "label": 1
                },
                {
                    "sent": "We can sample from the GP then condition on G results in the joint distribution over F&G and then create a mixture over all these terms which would then properly represent the hepatitis of the Gaussian times a lot normal.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's also possible in works very well in practice is to just linearize one of them.",
                    "label": 0
                },
                {
                    "sent": "The exponential term 1st order, and then we get a predicted caution distribution in the handout that we use for the results actually.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that leads me to show you results of this model.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try to some data, but we've been looking at is comparing three models the vanilla Gaussian process sparse custom for this model introduced by Snails in Germany and our model, and the reason why we picked the PGP for comparison is because it's one of the few demonistic methods that don't use MCMC schemes that yield nonstationarity.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's a side effect.",
                    "label": 0
                },
                {
                    "sent": "But still interesting model and we wanted these models on three datasets.",
                    "label": 0
                },
                {
                    "sent": "The motorhome datasets with traditional data set for non stationarity and some stock data and human heart rate data sets and happy parameters were optimized for possible, namely in SP GP and vanilla GPV used gradient based maximum likely Type 2 commiseration for the GPM we used to grid to set the hyperparameters.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the first data set.",
                    "label": 0
                },
                {
                    "sent": "This is the motorhome and data set is like where we have that the data points in red and the line up here is the invert gossan predicted distribution for the geovax process.",
                    "label": 0
                },
                {
                    "sent": "And we can see in areas where there is not much variation, like here.",
                    "label": 0
                },
                {
                    "sent": "GFX is slow and elevation increases.",
                    "label": 0
                },
                {
                    "sent": "It goes up.",
                    "label": 0
                },
                {
                    "sent": "Said really models the difference in amplitude variations in this data set quite nicely.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That was also the case if you look for objective measures, for instance here the mean log probability with multiple filling tests, you can see that slightly higher for the GPM then for the other two models and the mean squared error where low is good sort of the inverse.",
                    "label": 0
                },
                {
                    "sent": "We also get a slight improvement.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second data set we apply this to is this stop data.",
                    "label": 0
                },
                {
                    "sent": "We really don't have much signal.",
                    "label": 0
                },
                {
                    "sent": "I mean there is no mean really picked up.",
                    "label": 0
                },
                {
                    "sent": "It's uncorrelated noise.",
                    "label": 0
                },
                {
                    "sent": "But what happens here is that basically this noise processes modulated and this modulation is quite nicely picked up by the G of X process as well.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we can expect, in this case, the means good error for all three models all the same, because the mean is 0, but predictive distribution is significantly better for this model.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And last not least, we looked at this human data set where it represents hardware database sample from an human.",
                    "label": 0
                },
                {
                    "sent": "Here in the beginning the person is sleeping.",
                    "label": 0
                },
                {
                    "sent": "There's not much happening.",
                    "label": 0
                },
                {
                    "sent": "Then the activity starts and that is an obvious example for non stationarity in aptitude and we can see that this is represented by the posterior over G quite.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nicely and again the same picture, slight improvements.",
                    "label": 0
                },
                {
                    "sent": "It's not dramatic, but certainly noticeable.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that leads me to summarize this talk.",
                    "label": 0
                },
                {
                    "sent": "We introduced the GBM as a principled way too.",
                    "label": 1
                },
                {
                    "sent": "Enable GPS for smoothly varying nonstationary amplitude variations.",
                    "label": 1
                },
                {
                    "sent": "Expectation propagation deals very efficient inference in this model and future work on our side.",
                    "label": 1
                },
                {
                    "sent": "To refine the quadrature AP to also get reasonably good grades out and to overcome this one.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Outside thank you for listening.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Cortana text girlfriend for mixer.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "156 Yeah, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think the mixture of caution processes principle in more general model because it could also handle on the stationarity in amplitude.",
                    "label": 0
                },
                {
                    "sent": "Sorry in length scales and other processes.",
                    "label": 0
                },
                {
                    "sent": "I think what's nice about this model, if you have a strong belief that really the data that we've seen comes from this product, then we can model it here because we can really take care of this long range correlation.",
                    "label": 0
                },
                {
                    "sent": "This GP really captures also correlations between areas that are far apart compared to these other process that we modulate Ng.",
                    "label": 0
                },
                {
                    "sent": "And if that's true in the data that we look at for instance in hardware data we know.",
                    "label": 0
                },
                {
                    "sent": "That humans have the same activity pattern 24 hours later and we really know there is this other structure in it.",
                    "label": 0
                },
                {
                    "sent": "Then this model might be advantages.",
                    "label": 0
                },
                {
                    "sent": "I think there's another issue there that in the mixture of GPS, if you move from one region to another, you can get discontinuity's as you transfer between the regions relying on the data to make things smooth or you have to do something else.",
                    "label": 0
                },
                {
                    "sent": "Whereas in this model it looks to me like it's always going to be smooth.",
                    "label": 1
                },
                {
                    "sent": "It's a slowly changing this nonstationarity rather than a rapid change in non stationarity which the mixture of GPS implies.",
                    "label": 0
                },
                {
                    "sent": "Next yeah, but the gating function doesn't force.",
                    "label": 0
                },
                {
                    "sent": "You can get a discontinuity, which could be a good thing, but in this model you're not going to get a discontinuity, which could also.",
                    "label": 0
                },
                {
                    "sent": "Be a good thing.",
                    "label": 0
                },
                {
                    "sent": "The casting process is always good.",
                    "label": 0
                },
                {
                    "sent": "That was John, did you?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I have my head.",
                    "label": 0
                },
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "Model sending this update.",
                    "label": 0
                },
                {
                    "sent": "There's nowhere to go from dictionary, so.",
                    "label": 0
                },
                {
                    "sent": "Pop up alpaca talk about how does it end?",
                    "label": 0
                },
                {
                    "sent": "Yeah, very very quick note.",
                    "label": 0
                },
                {
                    "sent": "I mean basically you can get in quite a few cases.",
                    "label": 0
                },
                {
                    "sent": "I just want to go back to this one slide here basically also uncertain Arity in noise amplitude noise.",
                    "label": 0
                },
                {
                    "sent": "There is a it performs to some extent nonstationary noise modeling as well by the amplitude.",
                    "label": 0
                },
                {
                    "sent": "But you're right, definitely skill not considering at all in this model at the moment, but we were thinking of is basically if you think about the general credit special function, how we can incorporate other nonstationary processes describing unstated.",
                    "label": 0
                },
                {
                    "sent": "Out of these parameters, like long scale this model as well, the moment that's not tractable, simply because that would mean we don't even only have two processors to integrate our approximation, but three or four, but there's no principle reason why that's not possible as rather computational at the moment, but it's definitely something to consider for future work.",
                    "label": 0
                },
                {
                    "sent": "So I have a related question to that that you talked about.",
                    "label": 0
                },
                {
                    "sent": "You said characterized extensively the amplitude function as slowly varying, which makes sense because obviously there will be.",
                    "label": 0
                },
                {
                    "sent": "It will try to fit the length scale itself if it gets fast varying, so I presume it's sufficient just to initialize it as slowly varying versus the.",
                    "label": 0
                },
                {
                    "sent": "Function for it to find that you don't get problems with local minima.",
                    "label": 0
                },
                {
                    "sent": "No, it's in practice it's not that big of a deal, but you sort of have to make the assumption that the length scale of the chichi processes, say, let's twice as big as the F process.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it gets confused by this identifiability issue.",
                    "label": 0
                },
                {
                    "sent": "I mean, if there's no structure to be picked up, then this doesn't work.",
                    "label": 0
                },
                {
                    "sent": "As I said, if you initialize it correspondingly, then it's.",
                    "label": 0
                },
                {
                    "sent": "This works fine.",
                    "label": 0
                },
                {
                    "sent": "And I had another question, so it's not really direct about this work, but you mentioned smitten O'hagan's work on varying length scales.",
                    "label": 0
                },
                {
                    "sent": "I wasn't so familiar with that as I am with Mark Gibbs.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to characterize how that work on varying length scales differs from Mark Gibbs work on varying length scales?",
                    "label": 0
                },
                {
                    "sent": "Or you're not so familiar with that?",
                    "label": 0
                },
                {
                    "sent": "I'm less familiar with the second there, but.",
                    "label": 0
                },
                {
                    "sent": "So does markets also consider different deformations of the space?",
                    "label": 0
                },
                {
                    "sent": "I mean they basically considered well.",
                    "label": 0
                },
                {
                    "sent": "I think he.",
                    "label": 1
                },
                {
                    "sent": "I think the difference, maybe that they considered deforming the space in a non linear way before putting it in the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Whereas Mark sort of considered considered defamation.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to characterize that property, but it's a definition within the GP and actually causes a term to come out the front of the RBF kernel which has some weird effects they consider it.",
                    "label": 0
                },
                {
                    "sent": "Nonstationary parameterisation of this deformation?",
                    "label": 0
                },
                {
                    "sent": "So basically the parameters of this information and non stationary model violating processes?",
                    "label": 0
                },
                {
                    "sent": "Maybe that's the difference because I thought to remember that markets was parametric.",
                    "label": 0
                },
                {
                    "sent": "Is that yes?",
                    "label": 0
                },
                {
                    "sent": "So sorry that's true, but you could work was non parametric.",
                    "label": 0
                },
                {
                    "sent": "They had the latent processes that characterize this deformation matrix and this mapping.",
                    "label": 0
                },
                {
                    "sent": "OK, so if there's no further questions if you just think all over again.",
                    "label": 0
                }
            ]
        }
    }
}