{
    "id": "7wckzjx4cdqwsgnyfjt4zf3pdxyw6spn",
    "title": "Nonparametric Learning of Switching Autoregressive Processes",
    "info": {
        "author": [
            "Emily Fox, Department of Statistics, University of Washington"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes",
            "Top->Computer Science->Speech Analysis",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_fox_nls/",
    "segmentation": [
        [
            "OK, so this will be quite a bit of change of pace here, but um.",
            "Many nonlinear dynamical phenomenon can be described as switching between a set linear dynamic systems."
        ],
        [
            "For example, switching linear dynamic systems.",
            "Scribe, let me just check.",
            "Changes in the volatility of stock indices, the dynamics of a maneuvering target, and the dance of honey bees as they change from a turn right dance to waggle, dance to turn left, dance in communicating the location of foods."
        ],
        [
            "This is.",
            "However, there's often uncertainty about the number of modes that should be used to describe the observed behavior, and also one would like to be able to add new modes to the system when new behaviors are exhibited.",
            "So, as such, we want to examine a non parametric.",
            "Good morning switching dynamical processes and specifically we place a hierarchical dearsley process prior on our mode space, and then we aim to cluster the observations based on the observed dynamics."
        ],
        [
            "OK, so we'll start by just presenting some background on switching dynamic processes and a prior that can be placed on the dynamic parameters.",
            "We then present the sticky HTP HMM formulation, which is a nonparametric Bayesian approach to clean hidden Markov models.",
            "And we show how we can extend this model to these switching dynamical processes.",
            "We develop a gift sampler.",
            "Static data the Ibovespa stock index and sequences of dancing honey bees."
        ],
        [
            "OK, so the states based model just consists of an underlying state X that's observed via set of.",
            "Why?",
            "An linear time invariant model assumes that the dynamics of the state property.",
            "Yeah, fixed linear dynamic matrix A plus some additive Gaussian noise, and that the observations are noisy.",
            "Measurement of a linear transformation of your through a measurement matrix C corrupted by Gaussian noise as well.",
            "One key property of this model is the fact that DISH and on the state sequence your observations are conditionally independent.",
            "On the other hand, vector autoregressive processes placed the dynamics directly on the observations, so an order orvar process assumes that each observation is some linear combination of the previous.",
            "Our observation vectors plus yet again, some additive Gaussian noise."
        ],
        [
            "But bar processes can be put in state space form via the following transformation, just as one example.",
            "And so we see that the class of all of our processes is a subclass of all state space models."
        ],
        [
            "OK, so switch.",
            "Extend the state space formulation to more complex dynamical phenomenon.",
            "This model consists of an underlying discrete mode sequence represented by these random variables.",
            "There's this Markov structure with transition distribution \u03c0.",
            "And then conditioned on the mode sequence, you have these conditionally linear dynamic matrix dynamic systems with mode specific dynamic matrix and process noise covariance.",
            "So.",
            "Building model is actually nonlinear.",
            "Here, we're just assuming that the measurement matrix and the measurement noise are the same for all modes of our system.",
            "The switching fire process similarly extends the standard Model where now each mode has its own unique set of dynamic matrices."
        ],
        [
            "OK, let's just start by assuming that the mode sequence is fixed and we'd like to do posterior inference over the parameters of our switching bar process.",
            "Well, we can rewrite the model in this matrix form here.",
            "Then we can look at all of the observations that were generated from the case mode.",
            "We group these observations together.",
            "And find some more specific set of matrices and from this we see that the problem just decouples K different linear regression problems.",
            "Capital K is just the number of unique modes in this sequence.",
            "So we can place.",
            "And that's when we put the posterior distribution condition on all the observations from that mode.",
            "The distribution remains matrix normal inverse Wishart.",
            "And that's just a matrix generalization of the normal inverse Wishart distribution."
        ],
        [
            "OK, so.",
            "View these switching processes.",
            "It's just a hidden Markov model where instead of having Additionally independent observations each.",
            "Linear dynamics.",
            "Air dynamics.",
            "OK, so in order to start analyzing nonparametric.",
            "Approaches for learning switching dynamical processes.",
            "It's useful to part.",
            "First, start examining these models for hidden Markov models.",
            "So this model is referred to as.",
            "I hmm or the HTP, hmm?",
            "And what this model does is it defines these infinite transition distributions so that there's an unbounded mode space.",
            "I just kind of graphically depict these transition distributions here in this mode versus time lattice, and I highlight the chosen mode at time steps one and time steps to you see a countably infinite set of possible transitions and the problem one of those transitions.",
            "Indicated by the weight of the arrow.",
            "OK, so the dearsley process part of the HDP is what allows for this unbounded mode space and what encourages the use of a sparse subset of these modes.",
            "Hierarchical layering.",
            "Of these dearsley processes is what ties together all these transition distributions so that there's a shared sparse subset that's better visited.",
            "OK, so this additional sticky parameter.",
            "Here of the sticky HTP.",
            "Hmm, just allows remote have a bias towards self transitions."
        ],
        [
            "Just a bit more formally, we start with a global transition distribution beta drawn according to the stick breaking construction.",
            "What are sticky parameter?",
            "Kappa and we use that to define the infinite set of mode specific transition distributions, each of which is permitted according to Adir ishly process with beta as the base measure plus this additional weight on the component corresponding to self transition.",
            "And from this we see that the expected set of weights for each one of these transition distributions is just a convex combination of the global weights defined by beta plus some mode specific weights.",
            "Use in beta from the stick.",
            "Breaking construction is then shared by.",
            "Each mode is able to have an additional bias towards self transitions."
        ],
        [
            "OK, so we can then.",
            "The sticky HTP HMM model to these switching dynamical processes for the bar process or the switching part process, we define what's called the HTP AR HMM.",
            "App.",
            "This is an underlying mode sequence and transition distributions that are defined exactly after the sticky HTP.",
            "Hmm, where now instead of having conditionally independent observations, observations are conditionally bar R. OK, so for this application.",
            "This set of unique mode parameters consists of the dynamic keys for that var process.",
            "For that mode specific part process plus that process noise covariance.",
            "We can similarly extend the switching linear dynamic system by placing this sticky HTP.",
            "Hmm, prior on the most space.",
            "For now, each mode has the this state space representation for generating observations, so.",
            "In this case.",
            "The set of mode specific parameters here.",
            "Consist of the dynamic matrix a, the process noise covariance plus the measurement noise covariance."
        ],
        [
            "OK. We do is we use a block IP which just takes a dynamic.",
            "In order to improve the sampling.",
            "But for this method, it actually relies on instantiating.",
            "Distributions and the parameters.",
            "So since the position distributions are each countably infinite, we need a finite approximation to them and we choose to use.",
            "Limit approximation to the HTP, which simply takes a symmetric Dirichlet distribution.",
            "Data which then induced finite seriously distribution for each of the modes.",
            "Specific transition distributions.",
            "Then on state sequences we can look at the posterior distribution of these transition distributions and Additionally we can sample our parameters from that posterior distribution.",
            "LD couple linear regression problems.",
            "We saw that that was just that matrix normal inverse Wishart distribution."
        ],
        [
            "OK, we then condition on the set of transition distributions and parameters and we treat the sampled state sequence pseudo observations from the hidden Markov model.",
            "And once we do that, we can block sample the entire boat sequence using a variant.",
            "We simply pass messages backwards and then conditionally sample forwards."
        ],
        [
            "OK, now that we've sampled all of our parameters, an R mode sequence, we just have a time varying linear dynamic system.",
            "OK, so we can.",
            "Also sample the entire state sequence conditioned on these other parameters and to do this we use.",
            "The mode sequence except here are backwards.",
            "Message passing is equivalent to.",
            "Pre determined by this sample mode sequence and then we conditionally sample this state forward from the following distribution and what's important to note here is the fact that.",
            "Distributions.",
            "Is Gaussian and thus the messages are Gaussian.",
            "This backwards message passing and forward sampling technique is actually feasible."
        ],
        [
            "OK, so in addition to sampling all the transition distributions parameters and the mode and state sequence, we also place weakly informative priors on our hyperparameters and sample them from the data as well.",
            "So for all the results that we're going to show, we use the same set for these hyper priors."
        ],
        [
            "So we first present just some results on synthetic data.",
            "And we.",
            "Of an order one in order to HD par hmm to the HTTPS LDS and we use an HTP HMM or sticky HTP.",
            "Hmm, as a baseline comparison using first difference observations because first difference observations just imitate a random walk.",
            "OK, so.",
            "Here we generated data from A5.",
            "One process so order autoregressive process the mode sequence is shown in magenta and I've just offset the different components of the observation vector.",
            "Just clarity here.",
            "And what we do is we run 100 initialization so that Gibbs sampler for 1000 iterations and at Federation we compute the Hamming distance between the true and estimated mode sequences.",
            "And we plot the 10th and 90th quantiles in red in the median performance in blue, and we see for the matched order one HP par.",
            "Hmm, we have very good performance.",
            "However, we can get equivalent performance by considering the order or the H. Because these models in the class of the order one bar process.",
            "You see?",
            "Slower mixing rate for the HTTPS LDS because we're sampling that state sequence which doesn't exist for.",
            "And if we look at our baseline performance, this is on 1st difference observations.",
            "We see significantly worse performance, and that's because what does not really capture the dynamics in this data."
        ],
        [
            "OK so here is some data generated from a three mode order to autoregressive process.",
            "And we see as we would expect, that the order one month cannot capture everything that the order to model captures.",
            "But if you define a switching linear dynamics.",
            "Order to bar process.",
            "Here's our."
        ],
        [
            "Baseline performance.",
            "And finally we present results from a three mode SL DS.",
            "And here are the results for the HTTPS LDS.",
            "And for both.",
            "Much worse performance, but that shouldn't be surprising since we showed at the beginning of the talk that the.",
            "And once again, the baseline performance."
        ],
        [
            "Is much worse.",
            "OK. Now present some results on the Ivo Vespa stock Index using the HTTPS LDS model.",
            "And.",
            "Here the goal is to take these changes in this.",
            "Of the daily returns over the period from 1997 to 2001.",
            "An hour by car polo and Lopez.",
            "They cited key world events that affected this emerging Brazilian market over this time period, and we show those 10 events.",
            "But the blue lines are posterior.",
            "In Ferd by the HDPS LDS and so we see that they align quite well with these key event dates and.",
            "If you compare them to the results presented here, there even closer to those infer change points in that paper.",
            "Um, I guess 11.",
            "Key thing to note is that.",
            "Always in Ferd 3 loads of volatility.",
            "Since they assume two mode stochastic volatility models.",
            "OK, we can run the same experiment using on sticky HTTPS LDS, so that's.",
            "Setting a parameter to zero that's.",
            "Fire an asshole, yes.",
            "And we see many more inferred change points, and that's because they're biased towards self transitions.",
            "Revenge is plot in Roc Curve comparing the GPS LDS with the sticky parameter and without the sticky parameter to an order one.",
            "In order to HTP, hmm, we declare detection if there's an inferred changepoint within a small window around the key date and so from this we see the proportions of using this LDS model as well as the capital."
        ],
        [
            "Finally, we're going to present some results on some sequences of dancing honey bees.",
            "And there are six sequences that have been segmented, labeled.",
            "I don't really know what kind of job that is, but they segment the data into turn right, dances waggle dances, and turn left dances.",
            "And these dancers are used to communicate the location of food sources.",
            "Our observation vector consists of the head angle of the honeybee as well as the XY body position, and we show these observations as a time series for sequence 6."
        ],
        [
            "OK, so here's a little movie.",
            "If the quality is OK for the be doing this dance in the hype and this is labeled by that honey bee expert, so you see a turn left, hands to waggle dance.",
            "Other term left hands.",
            "Changing to turn right dance."
        ],
        [
            "OK.",
            "So we compare our segmentation performance of OH and his colleagues presented in the AJC paper.",
            "Just as an overview, we use an order One HD par HMM model and we consider a fully unsupervised approach where we can.",
            "I always do.",
            "And from that we aim to infer the number of modes.",
            "Those approach is much more supervised and they use.",
            "Fixed the number of modes to three.",
            "This leave one out training where they fixed the label sequences for five out of six of the sequences and then they test on the left out sequence.",
            "Is this data driven MCM?",
            "Proposed label sequences or segmentations of this data using learn cues that are very specific to this application such as head angle since head angle tends to be very predictive of which dance the honeybee is doing."
        ],
        [
            "So on sequences 4, five, and six, we get kind of surprising good segmentations.",
            "So on the top row I show the segmentations as a time series for each one of these sequences an on the bottom room, which is the bottom half of each of these plants.",
            "I should learn segmentation, which is represented.",
            "Initializations of the Gibbs sampler.",
            "And just to show some numbers.",
            "On sequence four or almost comparable to the results planted in the Oper sequence five, we beat them sequence 6."
        ],
        [
            "We're comfortable.",
            "If you look at sequences 1, two and three are learned.",
            "Segmentations are much worse and.",
            "I guess the hypothesis for this is just the irregularity you see in these dances.",
            "If we go back.",
            "To look at the other dancers, see very nice symmetry between the turn left and then turn right dances.",
            "And very large waggle dances.",
            "On whereas these dances are much more regular, especially if you look at sequence three.",
            "It's kind of hard to tell sometimes between the turning dance is an Idol dances and one thing that's not plotted here is the headache will.",
            "Mentioned was very indicative of the dances and sequences.",
            "Very consistent throughout the day.",
            "His head is all over the place."
        ],
        [
            "OK, so in conclusion, we examine the parametric Bayesian approach for learning, switching dynamical processes, and we presented an efficient Gibbs sampler and results on both synthetic data and a number of real datasets.",
            "What I think is interesting is the fact that we're using.",
            "Parameter settings and in one case we provide the Ibovespa stock index daily returns and its segments into different regions of stochastic volatility, and in another case we provide these dancing honeybee sequences and is able to learn these different honey bee dances and we haven't told it anything about the application or the number of modes."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this will be quite a bit of change of pace here, but um.",
                    "label": 0
                },
                {
                    "sent": "Many nonlinear dynamical phenomenon can be described as switching between a set linear dynamic systems.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, switching linear dynamic systems.",
                    "label": 0
                },
                {
                    "sent": "Scribe, let me just check.",
                    "label": 0
                },
                {
                    "sent": "Changes in the volatility of stock indices, the dynamics of a maneuvering target, and the dance of honey bees as they change from a turn right dance to waggle, dance to turn left, dance in communicating the location of foods.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "However, there's often uncertainty about the number of modes that should be used to describe the observed behavior, and also one would like to be able to add new modes to the system when new behaviors are exhibited.",
                    "label": 0
                },
                {
                    "sent": "So, as such, we want to examine a non parametric.",
                    "label": 0
                },
                {
                    "sent": "Good morning switching dynamical processes and specifically we place a hierarchical dearsley process prior on our mode space, and then we aim to cluster the observations based on the observed dynamics.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we'll start by just presenting some background on switching dynamic processes and a prior that can be placed on the dynamic parameters.",
                    "label": 0
                },
                {
                    "sent": "We then present the sticky HTP HMM formulation, which is a nonparametric Bayesian approach to clean hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "And we show how we can extend this model to these switching dynamical processes.",
                    "label": 0
                },
                {
                    "sent": "We develop a gift sampler.",
                    "label": 0
                },
                {
                    "sent": "Static data the Ibovespa stock index and sequences of dancing honey bees.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the states based model just consists of an underlying state X that's observed via set of.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "An linear time invariant model assumes that the dynamics of the state property.",
                    "label": 0
                },
                {
                    "sent": "Yeah, fixed linear dynamic matrix A plus some additive Gaussian noise, and that the observations are noisy.",
                    "label": 0
                },
                {
                    "sent": "Measurement of a linear transformation of your through a measurement matrix C corrupted by Gaussian noise as well.",
                    "label": 0
                },
                {
                    "sent": "One key property of this model is the fact that DISH and on the state sequence your observations are conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, vector autoregressive processes placed the dynamics directly on the observations, so an order orvar process assumes that each observation is some linear combination of the previous.",
                    "label": 0
                },
                {
                    "sent": "Our observation vectors plus yet again, some additive Gaussian noise.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But bar processes can be put in state space form via the following transformation, just as one example.",
                    "label": 0
                },
                {
                    "sent": "And so we see that the class of all of our processes is a subclass of all state space models.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so switch.",
                    "label": 0
                },
                {
                    "sent": "Extend the state space formulation to more complex dynamical phenomenon.",
                    "label": 0
                },
                {
                    "sent": "This model consists of an underlying discrete mode sequence represented by these random variables.",
                    "label": 0
                },
                {
                    "sent": "There's this Markov structure with transition distribution \u03c0.",
                    "label": 0
                },
                {
                    "sent": "And then conditioned on the mode sequence, you have these conditionally linear dynamic matrix dynamic systems with mode specific dynamic matrix and process noise covariance.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Building model is actually nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Here, we're just assuming that the measurement matrix and the measurement noise are the same for all modes of our system.",
                    "label": 0
                },
                {
                    "sent": "The switching fire process similarly extends the standard Model where now each mode has its own unique set of dynamic matrices.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's just start by assuming that the mode sequence is fixed and we'd like to do posterior inference over the parameters of our switching bar process.",
                    "label": 0
                },
                {
                    "sent": "Well, we can rewrite the model in this matrix form here.",
                    "label": 0
                },
                {
                    "sent": "Then we can look at all of the observations that were generated from the case mode.",
                    "label": 0
                },
                {
                    "sent": "We group these observations together.",
                    "label": 0
                },
                {
                    "sent": "And find some more specific set of matrices and from this we see that the problem just decouples K different linear regression problems.",
                    "label": 1
                },
                {
                    "sent": "Capital K is just the number of unique modes in this sequence.",
                    "label": 0
                },
                {
                    "sent": "So we can place.",
                    "label": 0
                },
                {
                    "sent": "And that's when we put the posterior distribution condition on all the observations from that mode.",
                    "label": 1
                },
                {
                    "sent": "The distribution remains matrix normal inverse Wishart.",
                    "label": 0
                },
                {
                    "sent": "And that's just a matrix generalization of the normal inverse Wishart distribution.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "View these switching processes.",
                    "label": 0
                },
                {
                    "sent": "It's just a hidden Markov model where instead of having Additionally independent observations each.",
                    "label": 0
                },
                {
                    "sent": "Linear dynamics.",
                    "label": 0
                },
                {
                    "sent": "Air dynamics.",
                    "label": 0
                },
                {
                    "sent": "OK, so in order to start analyzing nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Approaches for learning switching dynamical processes.",
                    "label": 0
                },
                {
                    "sent": "It's useful to part.",
                    "label": 0
                },
                {
                    "sent": "First, start examining these models for hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "So this model is referred to as.",
                    "label": 0
                },
                {
                    "sent": "I hmm or the HTP, hmm?",
                    "label": 0
                },
                {
                    "sent": "And what this model does is it defines these infinite transition distributions so that there's an unbounded mode space.",
                    "label": 1
                },
                {
                    "sent": "I just kind of graphically depict these transition distributions here in this mode versus time lattice, and I highlight the chosen mode at time steps one and time steps to you see a countably infinite set of possible transitions and the problem one of those transitions.",
                    "label": 0
                },
                {
                    "sent": "Indicated by the weight of the arrow.",
                    "label": 0
                },
                {
                    "sent": "OK, so the dearsley process part of the HDP is what allows for this unbounded mode space and what encourages the use of a sparse subset of these modes.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical layering.",
                    "label": 1
                },
                {
                    "sent": "Of these dearsley processes is what ties together all these transition distributions so that there's a shared sparse subset that's better visited.",
                    "label": 0
                },
                {
                    "sent": "OK, so this additional sticky parameter.",
                    "label": 0
                },
                {
                    "sent": "Here of the sticky HTP.",
                    "label": 0
                },
                {
                    "sent": "Hmm, just allows remote have a bias towards self transitions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just a bit more formally, we start with a global transition distribution beta drawn according to the stick breaking construction.",
                    "label": 1
                },
                {
                    "sent": "What are sticky parameter?",
                    "label": 0
                },
                {
                    "sent": "Kappa and we use that to define the infinite set of mode specific transition distributions, each of which is permitted according to Adir ishly process with beta as the base measure plus this additional weight on the component corresponding to self transition.",
                    "label": 0
                },
                {
                    "sent": "And from this we see that the expected set of weights for each one of these transition distributions is just a convex combination of the global weights defined by beta plus some mode specific weights.",
                    "label": 0
                },
                {
                    "sent": "Use in beta from the stick.",
                    "label": 0
                },
                {
                    "sent": "Breaking construction is then shared by.",
                    "label": 0
                },
                {
                    "sent": "Each mode is able to have an additional bias towards self transitions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we can then.",
                    "label": 0
                },
                {
                    "sent": "The sticky HTP HMM model to these switching dynamical processes for the bar process or the switching part process, we define what's called the HTP AR HMM.",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "This is an underlying mode sequence and transition distributions that are defined exactly after the sticky HTP.",
                    "label": 0
                },
                {
                    "sent": "Hmm, where now instead of having conditionally independent observations, observations are conditionally bar R. OK, so for this application.",
                    "label": 0
                },
                {
                    "sent": "This set of unique mode parameters consists of the dynamic keys for that var process.",
                    "label": 0
                },
                {
                    "sent": "For that mode specific part process plus that process noise covariance.",
                    "label": 0
                },
                {
                    "sent": "We can similarly extend the switching linear dynamic system by placing this sticky HTP.",
                    "label": 0
                },
                {
                    "sent": "Hmm, prior on the most space.",
                    "label": 0
                },
                {
                    "sent": "For now, each mode has the this state space representation for generating observations, so.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "The set of mode specific parameters here.",
                    "label": 0
                },
                {
                    "sent": "Consist of the dynamic matrix a, the process noise covariance plus the measurement noise covariance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. We do is we use a block IP which just takes a dynamic.",
                    "label": 0
                },
                {
                    "sent": "In order to improve the sampling.",
                    "label": 0
                },
                {
                    "sent": "But for this method, it actually relies on instantiating.",
                    "label": 0
                },
                {
                    "sent": "Distributions and the parameters.",
                    "label": 0
                },
                {
                    "sent": "So since the position distributions are each countably infinite, we need a finite approximation to them and we choose to use.",
                    "label": 0
                },
                {
                    "sent": "Limit approximation to the HTP, which simply takes a symmetric Dirichlet distribution.",
                    "label": 0
                },
                {
                    "sent": "Data which then induced finite seriously distribution for each of the modes.",
                    "label": 0
                },
                {
                    "sent": "Specific transition distributions.",
                    "label": 0
                },
                {
                    "sent": "Then on state sequences we can look at the posterior distribution of these transition distributions and Additionally we can sample our parameters from that posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "LD couple linear regression problems.",
                    "label": 0
                },
                {
                    "sent": "We saw that that was just that matrix normal inverse Wishart distribution.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we then condition on the set of transition distributions and parameters and we treat the sampled state sequence pseudo observations from the hidden Markov model.",
                    "label": 1
                },
                {
                    "sent": "And once we do that, we can block sample the entire boat sequence using a variant.",
                    "label": 0
                },
                {
                    "sent": "We simply pass messages backwards and then conditionally sample forwards.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now that we've sampled all of our parameters, an R mode sequence, we just have a time varying linear dynamic system.",
                    "label": 1
                },
                {
                    "sent": "OK, so we can.",
                    "label": 0
                },
                {
                    "sent": "Also sample the entire state sequence conditioned on these other parameters and to do this we use.",
                    "label": 1
                },
                {
                    "sent": "The mode sequence except here are backwards.",
                    "label": 0
                },
                {
                    "sent": "Message passing is equivalent to.",
                    "label": 1
                },
                {
                    "sent": "Pre determined by this sample mode sequence and then we conditionally sample this state forward from the following distribution and what's important to note here is the fact that.",
                    "label": 0
                },
                {
                    "sent": "Distributions.",
                    "label": 0
                },
                {
                    "sent": "Is Gaussian and thus the messages are Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This backwards message passing and forward sampling technique is actually feasible.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in addition to sampling all the transition distributions parameters and the mode and state sequence, we also place weakly informative priors on our hyperparameters and sample them from the data as well.",
                    "label": 0
                },
                {
                    "sent": "So for all the results that we're going to show, we use the same set for these hyper priors.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we first present just some results on synthetic data.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                },
                {
                    "sent": "Of an order one in order to HD par hmm to the HTTPS LDS and we use an HTP HMM or sticky HTP.",
                    "label": 0
                },
                {
                    "sent": "Hmm, as a baseline comparison using first difference observations because first difference observations just imitate a random walk.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here we generated data from A5.",
                    "label": 0
                },
                {
                    "sent": "One process so order autoregressive process the mode sequence is shown in magenta and I've just offset the different components of the observation vector.",
                    "label": 0
                },
                {
                    "sent": "Just clarity here.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we run 100 initialization so that Gibbs sampler for 1000 iterations and at Federation we compute the Hamming distance between the true and estimated mode sequences.",
                    "label": 0
                },
                {
                    "sent": "And we plot the 10th and 90th quantiles in red in the median performance in blue, and we see for the matched order one HP par.",
                    "label": 0
                },
                {
                    "sent": "Hmm, we have very good performance.",
                    "label": 0
                },
                {
                    "sent": "However, we can get equivalent performance by considering the order or the H. Because these models in the class of the order one bar process.",
                    "label": 0
                },
                {
                    "sent": "You see?",
                    "label": 0
                },
                {
                    "sent": "Slower mixing rate for the HTTPS LDS because we're sampling that state sequence which doesn't exist for.",
                    "label": 0
                },
                {
                    "sent": "And if we look at our baseline performance, this is on 1st difference observations.",
                    "label": 0
                },
                {
                    "sent": "We see significantly worse performance, and that's because what does not really capture the dynamics in this data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here is some data generated from a three mode order to autoregressive process.",
                    "label": 0
                },
                {
                    "sent": "And we see as we would expect, that the order one month cannot capture everything that the order to model captures.",
                    "label": 0
                },
                {
                    "sent": "But if you define a switching linear dynamics.",
                    "label": 0
                },
                {
                    "sent": "Order to bar process.",
                    "label": 0
                },
                {
                    "sent": "Here's our.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Baseline performance.",
                    "label": 0
                },
                {
                    "sent": "And finally we present results from a three mode SL DS.",
                    "label": 0
                },
                {
                    "sent": "And here are the results for the HTTPS LDS.",
                    "label": 0
                },
                {
                    "sent": "And for both.",
                    "label": 0
                },
                {
                    "sent": "Much worse performance, but that shouldn't be surprising since we showed at the beginning of the talk that the.",
                    "label": 0
                },
                {
                    "sent": "And once again, the baseline performance.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is much worse.",
                    "label": 0
                },
                {
                    "sent": "OK. Now present some results on the Ivo Vespa stock Index using the HTTPS LDS model.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here the goal is to take these changes in this.",
                    "label": 1
                },
                {
                    "sent": "Of the daily returns over the period from 1997 to 2001.",
                    "label": 0
                },
                {
                    "sent": "An hour by car polo and Lopez.",
                    "label": 0
                },
                {
                    "sent": "They cited key world events that affected this emerging Brazilian market over this time period, and we show those 10 events.",
                    "label": 0
                },
                {
                    "sent": "But the blue lines are posterior.",
                    "label": 0
                },
                {
                    "sent": "In Ferd by the HDPS LDS and so we see that they align quite well with these key event dates and.",
                    "label": 0
                },
                {
                    "sent": "If you compare them to the results presented here, there even closer to those infer change points in that paper.",
                    "label": 0
                },
                {
                    "sent": "Um, I guess 11.",
                    "label": 0
                },
                {
                    "sent": "Key thing to note is that.",
                    "label": 0
                },
                {
                    "sent": "Always in Ferd 3 loads of volatility.",
                    "label": 0
                },
                {
                    "sent": "Since they assume two mode stochastic volatility models.",
                    "label": 0
                },
                {
                    "sent": "OK, we can run the same experiment using on sticky HTTPS LDS, so that's.",
                    "label": 0
                },
                {
                    "sent": "Setting a parameter to zero that's.",
                    "label": 0
                },
                {
                    "sent": "Fire an asshole, yes.",
                    "label": 0
                },
                {
                    "sent": "And we see many more inferred change points, and that's because they're biased towards self transitions.",
                    "label": 0
                },
                {
                    "sent": "Revenge is plot in Roc Curve comparing the GPS LDS with the sticky parameter and without the sticky parameter to an order one.",
                    "label": 0
                },
                {
                    "sent": "In order to HTP, hmm, we declare detection if there's an inferred changepoint within a small window around the key date and so from this we see the proportions of using this LDS model as well as the capital.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, we're going to present some results on some sequences of dancing honey bees.",
                    "label": 0
                },
                {
                    "sent": "And there are six sequences that have been segmented, labeled.",
                    "label": 0
                },
                {
                    "sent": "I don't really know what kind of job that is, but they segment the data into turn right, dances waggle dances, and turn left dances.",
                    "label": 0
                },
                {
                    "sent": "And these dancers are used to communicate the location of food sources.",
                    "label": 0
                },
                {
                    "sent": "Our observation vector consists of the head angle of the honeybee as well as the XY body position, and we show these observations as a time series for sequence 6.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's a little movie.",
                    "label": 0
                },
                {
                    "sent": "If the quality is OK for the be doing this dance in the hype and this is labeled by that honey bee expert, so you see a turn left, hands to waggle dance.",
                    "label": 0
                },
                {
                    "sent": "Other term left hands.",
                    "label": 0
                },
                {
                    "sent": "Changing to turn right dance.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we compare our segmentation performance of OH and his colleagues presented in the AJC paper.",
                    "label": 0
                },
                {
                    "sent": "Just as an overview, we use an order One HD par HMM model and we consider a fully unsupervised approach where we can.",
                    "label": 0
                },
                {
                    "sent": "I always do.",
                    "label": 0
                },
                {
                    "sent": "And from that we aim to infer the number of modes.",
                    "label": 1
                },
                {
                    "sent": "Those approach is much more supervised and they use.",
                    "label": 0
                },
                {
                    "sent": "Fixed the number of modes to three.",
                    "label": 1
                },
                {
                    "sent": "This leave one out training where they fixed the label sequences for five out of six of the sequences and then they test on the left out sequence.",
                    "label": 1
                },
                {
                    "sent": "Is this data driven MCM?",
                    "label": 0
                },
                {
                    "sent": "Proposed label sequences or segmentations of this data using learn cues that are very specific to this application such as head angle since head angle tends to be very predictive of which dance the honeybee is doing.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on sequences 4, five, and six, we get kind of surprising good segmentations.",
                    "label": 0
                },
                {
                    "sent": "So on the top row I show the segmentations as a time series for each one of these sequences an on the bottom room, which is the bottom half of each of these plants.",
                    "label": 0
                },
                {
                    "sent": "I should learn segmentation, which is represented.",
                    "label": 0
                },
                {
                    "sent": "Initializations of the Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "And just to show some numbers.",
                    "label": 0
                },
                {
                    "sent": "On sequence four or almost comparable to the results planted in the Oper sequence five, we beat them sequence 6.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're comfortable.",
                    "label": 0
                },
                {
                    "sent": "If you look at sequences 1, two and three are learned.",
                    "label": 0
                },
                {
                    "sent": "Segmentations are much worse and.",
                    "label": 0
                },
                {
                    "sent": "I guess the hypothesis for this is just the irregularity you see in these dances.",
                    "label": 0
                },
                {
                    "sent": "If we go back.",
                    "label": 0
                },
                {
                    "sent": "To look at the other dancers, see very nice symmetry between the turn left and then turn right dances.",
                    "label": 0
                },
                {
                    "sent": "And very large waggle dances.",
                    "label": 0
                },
                {
                    "sent": "On whereas these dances are much more regular, especially if you look at sequence three.",
                    "label": 0
                },
                {
                    "sent": "It's kind of hard to tell sometimes between the turning dance is an Idol dances and one thing that's not plotted here is the headache will.",
                    "label": 0
                },
                {
                    "sent": "Mentioned was very indicative of the dances and sequences.",
                    "label": 0
                },
                {
                    "sent": "Very consistent throughout the day.",
                    "label": 0
                },
                {
                    "sent": "His head is all over the place.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in conclusion, we examine the parametric Bayesian approach for learning, switching dynamical processes, and we presented an efficient Gibbs sampler and results on both synthetic data and a number of real datasets.",
                    "label": 1
                },
                {
                    "sent": "What I think is interesting is the fact that we're using.",
                    "label": 0
                },
                {
                    "sent": "Parameter settings and in one case we provide the Ibovespa stock index daily returns and its segments into different regions of stochastic volatility, and in another case we provide these dancing honeybee sequences and is able to learn these different honey bee dances and we haven't told it anything about the application or the number of modes.",
                    "label": 0
                }
            ]
        }
    }
}