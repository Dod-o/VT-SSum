{
    "id": "kijgnxaox4w3dkv4n66ofjcjj3icaj2f",
    "title": "Noise Thresholds for Spectral Clustering",
    "info": {
        "author": [
            "Sivaraman Balakrishnan, Carnegie Mellon University"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/nips2011_balakrishnan_clustering/",
    "segmentation": [
        [
            "So we study."
        ],
        [
            "Essentially, noise thresholds for spectral clustering.",
            "So spectral clustering is a very popular clustering algorithm.",
            "It works the following way.",
            "You're given a bunch of data points, you compute what is called a similarity matrix, typically between the data points, and then you compute a Laplacian from the similarity matrix.",
            "You then use it, stop few eigenvectors to define what is called a spectral embedding, and then run K means on this spectral embedding to recover the clusters.",
            "So there are many high level ways to justify spectral clustering, including its deep connections to graph cuts, random walks, even certain differential operators on the data manifold.",
            "However, somewhat closer to our own work is the analysis or the justification of spectral clustering via perturbation theory.",
            "So here what you say is that in the absence of noisier similarity, matrix has eigenvectors which perfectly encode the cluster structure, and now these eigenvectors are known via perturbation theory to be stable in an L2 cents.",
            "When you add noise to this matrix and so even after you add noise, maybe you can use these eigenvectors to recover the cluster structure.",
            "So this is the justification for spectral clustering via perturbation theory.",
            "So here are contribu."
        ],
        [
            "Just this.",
            "So firstly, we study hierarchical spectralon kwei clustering algorithms.",
            "So the first thing that we do is to show that there's actually a very broad class of hierarchically structured similarity matrices for which the eigenvectors without noise reflect the true cluster structure.",
            "So this is without noise statement, and then we show something stronger, which is that when you add noise, these eigenvectors are stable in an L Infinity sense.",
            "So what I mean is that their coordinate wise stable.",
            "Why this is interesting is that this now allows us to make precise statements about how many mistakes the K means algorithm will make when run on the spectral embedding.",
            "So this is somewhat new in the sense that we can say that if you add less than a certain amount of noise when you run K means on the spectral embedding.",
            "You won't make any mistakes, so essentially we can characterize as a function of noise variance and the number of objects and a certain notion of signal strength, which is essentially the gap between Inter and intra cluster similarities.",
            "We can characterize the relation between these three quantities."
        ],
        [
            "Our final set of contributions is basically an information theoretic analysis, so the first thing is that we showed there some minimax lower bound which says that no clustering algorithm can succeed if there's more than a certain amount of noise.",
            "While we aren't able to show that spectral clustering actually achieves this bond, we do then study these combinatorial graph cut objectives so spectral clustering can be viewed as a relaxation of certain graph cut objectives, and So what we show is that these combinatorial algorithms do in fact achieve this information theoretic bond and spectral clustering comes quite close to this an yeah, so it's an open question to improve the analysis to show that spectral clustering actually achieves the information theoretic bond we conjectured.",
            "This is actually true, so please come to our poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we study.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Essentially, noise thresholds for spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "So spectral clustering is a very popular clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "It works the following way.",
                    "label": 0
                },
                {
                    "sent": "You're given a bunch of data points, you compute what is called a similarity matrix, typically between the data points, and then you compute a Laplacian from the similarity matrix.",
                    "label": 0
                },
                {
                    "sent": "You then use it, stop few eigenvectors to define what is called a spectral embedding, and then run K means on this spectral embedding to recover the clusters.",
                    "label": 0
                },
                {
                    "sent": "So there are many high level ways to justify spectral clustering, including its deep connections to graph cuts, random walks, even certain differential operators on the data manifold.",
                    "label": 1
                },
                {
                    "sent": "However, somewhat closer to our own work is the analysis or the justification of spectral clustering via perturbation theory.",
                    "label": 0
                },
                {
                    "sent": "So here what you say is that in the absence of noisier similarity, matrix has eigenvectors which perfectly encode the cluster structure, and now these eigenvectors are known via perturbation theory to be stable in an L2 cents.",
                    "label": 0
                },
                {
                    "sent": "When you add noise to this matrix and so even after you add noise, maybe you can use these eigenvectors to recover the cluster structure.",
                    "label": 0
                },
                {
                    "sent": "So this is the justification for spectral clustering via perturbation theory.",
                    "label": 0
                },
                {
                    "sent": "So here are contribu.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just this.",
                    "label": 0
                },
                {
                    "sent": "So firstly, we study hierarchical spectralon kwei clustering algorithms.",
                    "label": 1
                },
                {
                    "sent": "So the first thing that we do is to show that there's actually a very broad class of hierarchically structured similarity matrices for which the eigenvectors without noise reflect the true cluster structure.",
                    "label": 0
                },
                {
                    "sent": "So this is without noise statement, and then we show something stronger, which is that when you add noise, these eigenvectors are stable in an L Infinity sense.",
                    "label": 0
                },
                {
                    "sent": "So what I mean is that their coordinate wise stable.",
                    "label": 0
                },
                {
                    "sent": "Why this is interesting is that this now allows us to make precise statements about how many mistakes the K means algorithm will make when run on the spectral embedding.",
                    "label": 0
                },
                {
                    "sent": "So this is somewhat new in the sense that we can say that if you add less than a certain amount of noise when you run K means on the spectral embedding.",
                    "label": 0
                },
                {
                    "sent": "You won't make any mistakes, so essentially we can characterize as a function of noise variance and the number of objects and a certain notion of signal strength, which is essentially the gap between Inter and intra cluster similarities.",
                    "label": 1
                },
                {
                    "sent": "We can characterize the relation between these three quantities.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our final set of contributions is basically an information theoretic analysis, so the first thing is that we showed there some minimax lower bound which says that no clustering algorithm can succeed if there's more than a certain amount of noise.",
                    "label": 1
                },
                {
                    "sent": "While we aren't able to show that spectral clustering actually achieves this bond, we do then study these combinatorial graph cut objectives so spectral clustering can be viewed as a relaxation of certain graph cut objectives, and So what we show is that these combinatorial algorithms do in fact achieve this information theoretic bond and spectral clustering comes quite close to this an yeah, so it's an open question to improve the analysis to show that spectral clustering actually achieves the information theoretic bond we conjectured.",
                    "label": 0
                },
                {
                    "sent": "This is actually true, so please come to our poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}