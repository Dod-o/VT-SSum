{
    "id": "aash3nhrht6kqvzlhss742hlbm6fmcsy",
    "title": "Ontology-Driven Sentiment Analysis of Product and Service Aspects",
    "info": {
        "author": [
            "Kim Schouten, Erasmus School of Economics, Erasmus University Rotterdam"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_schouten_service_aspects/",
    "segmentation": [
        [
            "OK, so ontology driven sentiment, analysis of product and service aspects.",
            "It's a bit of a mouthful.",
            "So what are we trying?"
        ],
        [
            "So to do here.",
            "So basically it's aspect based sentiment analysis.",
            "So what sentiment is expressed about which aspect for a given entity?",
            "And in this case, we're often only looking at polarity of the sentiment, right?",
            "So you can measure a sentiment in different ways.",
            "You could look at emotions or other things and we just look at polarity.",
            "So if it's positive or negative to keep it a bit easy.",
            "So the data that kind of determines the task is the sum of all data from the aspect based sentiment analysis task from two years ago and there.",
            "So what we get from the.",
            "The data set basically is a set of reviews which are already split in sentences, so we don't have to do that, and the sentences are annotated with aspects, and each aspect has a polarity.",
            "So either positive, neutral or negative.",
            "So the aspects are given in our case, so we only focus on the sentiment analysis part.",
            "So then we need to save.",
            "It's what the right polarity is.",
            "So our idea was to see how far we could get by doing this using an ontology instead of using a fancy machine learning classifier.",
            "So there are some."
        ],
        [
            "And for that, so previously we did have a classifier and we tried to add some ontology features to it.",
            "It gave a somewhat better results, but the results are really hard to interpret and we don't really know what the ontology is really adding apart from the little performance boost.",
            "So we thought, OK, what if we just use an ontology and use that for sentiment prediction as much as possible?",
            "Well, in that case you know exactly what's going on, right?",
            "Because there are words in the text, there are some concepts in ontology.",
            "You know exactly how you get through your prediction, which is nice.",
            "However, the ontology needs to be big enough to cover enough cases.",
            "Spoiler ours isn't, so that's an issue, but we'll get to that.",
            "So to cover for the lack of size, we have a simple bag of words classifier that we can use as a sort of a backup.",
            "Algorithm, so if the ontology is not able to fear it out, then we can use this bag of words classifier to help out a bit.",
            "Alright, so."
        ],
        [
            "So about the ontology.",
            "So what's the purpose?",
            "So at the core, of course it needs to be somewhat of a sentiment lexicon because you need to have those concepts in there.",
            "So we have aspect and sentiment concepts and these have lexicalization's to link them to words in the text.",
            "So we find some words.",
            "We know what concepts there are because we are limited to the restaurant domain.",
            "That's the data set from the semi file that we are using.",
            "We don't do any word sense disfiguration because given the domain there are not many words which have really multiple meanings here.",
            "So we also annotate some of the high level aspects concepts in ontology with the aspects category, which is a label from the data set.",
            "So just to show you."
        ],
        [
            "So this is a part of the data, so you see the polarity that we want to predict.",
            "And here are the aspects categories which are given so we can use this label here to basically link some of the concepts in the ontology to some of the things we want to find in data.",
            "Which is helpful, which we'll see later."
        ],
        [
            "So there are some words in the ontology which are always of a certain sentiments like good.",
            "Good is always a positive thing, regardless of how you use it, where you use it, it's very nice and simple words and there are a lot of words like that and those concepts that represent these words are subclasses of positive or negative.",
            "If there are negative words an in the paper we call this type one sentiment words there just nice and easy.",
            "We do not have a neutral class in the ontology, which is a bit of a shortcoming in one hand, because the data set does have instances which are neutral, but we found it pretty hard to say OK, this word really denotes a neutral instance, so we skip that for now.",
            "So in most cases, what you would say is a neutral word often is also a negative words, because it's not, let's say good enough, right?",
            "If the food is average.",
            "You're not really that happy to go to that restaurant, so that's why we left it out to you because it was a bit too difficult to model that.",
            "Alright, so then."
        ],
        [
            "The second purpose of the ontology apart from having just the words in there and the sentiment is to help with the scoping.",
            "So there are some sentiment words which basically imply a certain aspect category, let's say.",
            "Have an example noisy.",
            "If you have finally word noisy, which is a bit of a negative words, then you know it has to be about the ambiance, which is one of the aspects categories in the datasets, so it cannot be about food, right?",
            "Your food is not noisy, hopefully.",
            "Otherwise something's really wrong, but normally speaking it's about the ambiance, not the foods.",
            "So because we annotated like we make the aspect superclass of this noisy words in this case would be the ambiance class.",
            "We can use that to say OK, every other aspect that we're trying to find a sentiment forward that is not a subclass of this ambiance closet then noisy is not applicable, so we just ignore it when finding the sentiment so we can ignore irrelevant words.",
            "Basically using this setup and we call these Type 2 sentiment words.",
            "Then there is a third purpose."
        ],
        [
            "Rich opposition obviously Maps to the Type 3 sentiment words where we look at the context dependent sentiment words, so that's the last type where basically the same words has a different polarity in different contexts.",
            "So when it's about one aspect, it can be positive when it's about another aspect that can be negative, which is tricky of course.",
            "But the ontology can help here.",
            "So the examples.",
            "Let's say we have a high price and a high-quality so high is positive for one, but negative for the other.",
            "An in the ontology.",
            "We modeled that using class axiom, so we see if we find quality and high that is then a subclass of positive model.",
            "If we find price and high it's a subclass of negative.",
            "So the way it works is that if we find these words close together or directly related to each other actually, then we create a new class, let's say high quality an then hopefully there is a class axiom.",
            "In this case that will trigger an also make that new class subclass of.",
            "Positive.",
            "And we can infer the sentiment like that.",
            "Alright, so that's the basically the idea of the ontology.",
            "So how do we use it for sentiment classification?",
            "Because just having the ontology is not really enough and you have to do some some code around it."
        ],
        [
            "So the the classification."
        ],
        [
            "Uses a simpler method as we could possibly think of.",
            "So we for each aspect that is given in the datasets we are looking at all of the concepts.",
            "All of the sentiment concepts in the sentence where that aspect is in.",
            "So that's all given in the data.",
            "And then for each of these sentiment concepts, we're looking at what type it is, right?",
            "We have these three types.",
            "Then we check the type of each sentiment words.",
            "So if it's type one, then we just save all of the Super classes where we have a method.",
            "So OK, give me all your super classes, so that's easy.",
            "So all of the superclasses are stored in a set for later.",
            "If it's type 2, then we do the same, but only if the aspect matches with the ontology and the category annotation that we have for that aspect so.",
            "The noisy will not be used to classify food, let's say.",
            "And then for the type three, we check each directly related words, so that's related on the dependency graph.",
            "Which is also an aspect, so it has to be directly related and it has to be an aspect.",
            "And then we check by making a new class basically of which is a subclass of both to see if there is maybe a class action that will be triggered, and then we again just save all of the Super classes to a set.",
            "So if there is a class axiom, these superclasses of the new one will include either positive or negative.",
            "If there's no class axiom, then they're just the same as just a combination of the two.",
            "Superclasses and nothing special happens really.",
            "If there is a negation, we do check for that by looking at the negator words in the preceding context and by checking the dependency graph for negation relation.",
            "In that case, we just flip around the sentiment because we're doing no neutral cases that you can just swap and then we have this set of superclasses we can just check if there is a positive or a negative class in there.",
            "So."
        ],
        [
            "So in this case, if we only have a positive class, maybe one or more, but it's a set.",
            "So just once, if it's there, we say, OK, this is a positive aspect.",
            "If there is only negative in there, we say, well, it has to be negative.",
            "If there is both or there is none, then the ontology doesn't really help, so there are too many signals or conflicting signals or no signals at all.",
            "So in that case we don't really know what to do.",
            "So we tried looking at counting how many positive and negatives there are an even discounting with the distance towards the aspect that the things we tried in that regard didn't really help out much.",
            "So we just kept it like that, and for the pure ontology methods we just predict the majority class which is positive for this data set.",
            "But our backup method, as mentioned is a bag of words model.",
            "So in the evaluation we have the ONS method, which is just a pure ontology with the majority class.",
            "If there is no.",
            "Information really.",
            "And the end plus bow is the ontology method plus the bag of words as a backup methods.",
            "So it is better."
        ],
        [
            "Kids model just uses the presence of words, but in the whole review we found that that's actually worked a bit better than looking at only the sentence.",
            "So it gave a bit better results.",
            "We have the aspect category as a feature because that's given in the data, and then because we use the core NLP library for the dependency parsing, we just as easily could use this sentiment module that they have in there.",
            "So we have the sentiment of this sentence as determined by the sentiment module.",
            "MLP as a feature as well.",
            "Numerical feature, and then it's just standard waka SVM with standard kernel, basically a standard as possible.",
            "And then, like."
        ],
        [
            "Is 1/4 methods for the evaluation.",
            "We also tried adding the output of the ontology model to the bag of words model so we have one additional feature, basically saying positive or negative, which was the outcome of the ontology methods.",
            "Then we add that as an additional feature to the bag of words model just basically to see what happens."
        ],
        [
            "Alright, so the results I just want to show quickly the distribution.",
            "So to give an idea of the how hard the problem is.",
            "So this is for the 2016 data.",
            "We also tried the 2015 data which is basically a subset of this data set.",
            "So as you can see it's not really balanced so most of the instances are actually positive, even more so on the test data.",
            "Then on the training data a little bit and there are not many neutral so it was not.",
            "That bats to just skip them, although the bag of words model does predicts neutral, so it's trained on all three classes.",
            "OK, so the 2015 data was a little bit less unbalanced though, but still unbalanced."
        ],
        [
            "So these are the results for the 5th, 2015 data.",
            "So the ontology methods on its own does not perform supergroups.",
            "To be honest, that's how it is.",
            "The bag of words.",
            "The models either with or without ontology.",
            "Things work quite a bit better.",
            "So did the two stage approach where we used the ontology method with the bag of words as a backup.",
            "Actually slightly outperforms the other way around.",
            "Where we add the ontology to the bag of words feature set, which was interesting to see."
        ],
        [
            "Anne.",
            "This results are somewhat the same for 2016.",
            "Do well, it was more unbalanced, so that's maybe why the ontology method works a bit better.",
            "I can imagine the majority vote being maybe a bit better.",
            "I don't know.",
            "So All in all, the performance was a bit higher and but you know the ordering of the methods is still the same.",
            "So one of the things we also are also interested in was because the ontology method doesn't need any training data, right?",
            "All of the information is basically put in the ontology, so we were interested to see if you'd use the ontology based method if it needs less training data, then the bag of words model.",
            "So we did a data size sensitivity analysis where we bid."
        ],
        [
            "Only kept the tests at the same but artificially shrunk the training training set to see what happens.",
            "So this is for the 2015 data an what we wanted to see was basically the gap here becoming wider between the bag of words model, which is the orange dotted line here and the two with ontology based methods.",
            "So yeah, that happened, so that was good to see.",
            "Unfortunately for the 26."
        ],
        [
            "In data you see, the difference is really pretty much the same, so we wanted to see this gap increase here as well, but it didn't.",
            "So that was a.",
            "There's a bit of a shame, though.",
            "So.",
            "The last analysis that we did was to look at the different scenarios where we use each algorithm in right.",
            "Because we say if we find only positive or only negative, then we use the ontology methods.",
            "Otherwise we use either this majority vote or Bank of words.",
            "So we checked for."
        ],
        [
            "Various scenarios to see how each of the methods is performing.",
            "To get a bit more insight.",
            "So.",
            "In case we find just a positive class that would be in 43% of the cases and then the ontology will predict positive of course, and it would be right in 88% of the cases, which is pretty high in a bit higher than the bag of words model an for negative, which is a lot slower, smaller though, so it doesn't happen that often that it will say there's just the negative concept in here.",
            "But if it does, then it is usually right.",
            "However, I did.",
            "Interesting part is in the bottom, so the top two are basically the easy cases, right?",
            "So the ontology is able to figure it out.",
            "The bag of words model actually is also pretty high, so those are not the very interesting cases, but the bottom two are more interesting I think.",
            "So in the third line we see where there are both positive and negative instances in the sentence.",
            "Then the ontology will do the majority vote, which is sort of even.",
            "Like almost 5050.",
            "Same for bag of words, but a little bit higher, but the performance is so much lower than the other two that you know both actually have issues or the bag of words model also doesn't really know what's going on, right?",
            "So it's a 5050 thing.",
            "And the lower lowest line is where there is nothing, so the ontology does not have any information about the words in the sentence.",
            "So you can see that as basically a failure of ontology size, right?",
            "So it's not big enough to cover those cases because the bag of words model does have some information here, so the performance is pretty OK. 77 for just a simple bag of words model.",
            "So apparently there are some signals in there, but they're just not in the ontology, so we cannot use them.",
            "Interestingly, I actually looked over my slides this morning and I thought, OK, if we're doing majority votes, how can the performance be only 33%?",
            "So maybe the majority is not the majority anymore for this subset that we are arriving at but never thought of that."
        ],
        [
            "So in conclusions.",
            "So we have this ontology method and a bag of words which combine pretty OK, so they complement each other to a certain extent, which gives pretty good performance actually.",
            "So we think the performance of the Pure Ontology method is low because it doesn't cover most cases, so it's about 50% of the time that you can actually use it given the current size and the other 50%, it's either majority vote or bag of words.",
            "However, when it is used it gives good performance an.",
            "Explainable results and you don't need any training data, however of course you know domain ontologies don't fall from the Sky, so you can say we don't need training data, but you will need to knology which is also a lot of work to get.",
            "So in a sense that's that's a bit of a thing."
        ],
        [
            "So for future work, we're looking into doing more automatic ontology population.",
            "So we do have a sort of semi automatic approach which gives suggestions to add to the ontology now.",
            "So that helps with the save some time, but ideally you would do that on completely automatically, but we don't have such methods yet.",
            "Other things more in terms of the quality, is to include multiple words expression.",
            "So currently all of the concepts are really linked to a single word like good or bad, but there are also expressions like out of this world which is actually in the data and the separate words do not have any sentiment right out in the world.",
            "It's just regular words, but together they actually have the sentiment which is pretty positive.",
            "So it would be interesting to see if we can.",
            "Somehow get that in there as well.",
            "And then maybe it's interesting to look at the sentiment in this distance information.",
            "So for type three we just do the directly related aspects to a given sentiment words just one step on the dependency graph.",
            "But we didn't really investigate if maybe we needed two steps or three, or maybe just over different certain kinds of grammatical relations and not others.",
            "We just do one step regardless of relation type, which is pretty simplistic.",
            "And last, there is a quite a bit of a speed issue, at least in our current implementation.",
            "It is pretty slow, so we're looking how to solve it, but I don't really know how yet.",
            "So we use this general library in Java to use the ontology to get information out, but that is pretty slow and if we add the class so for the Type 3 sentiment where it's basically it will rebuild the complete inference model and the larger your ontology, the more time it takes.",
            "It is nasty.",
            "So yeah, those are the things we're looking to improve."
        ],
        [
            "If you have any questions then I'm happy to answer.",
            "The code is available on GitHub so you can have a look at it, but yeah, it is somewhat slow, so that's that's a warning.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so ontology driven sentiment, analysis of product and service aspects.",
                    "label": 1
                },
                {
                    "sent": "It's a bit of a mouthful.",
                    "label": 0
                },
                {
                    "sent": "So what are we trying?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do here.",
                    "label": 0
                },
                {
                    "sent": "So basically it's aspect based sentiment analysis.",
                    "label": 0
                },
                {
                    "sent": "So what sentiment is expressed about which aspect for a given entity?",
                    "label": 1
                },
                {
                    "sent": "And in this case, we're often only looking at polarity of the sentiment, right?",
                    "label": 0
                },
                {
                    "sent": "So you can measure a sentiment in different ways.",
                    "label": 0
                },
                {
                    "sent": "You could look at emotions or other things and we just look at polarity.",
                    "label": 0
                },
                {
                    "sent": "So if it's positive or negative to keep it a bit easy.",
                    "label": 0
                },
                {
                    "sent": "So the data that kind of determines the task is the sum of all data from the aspect based sentiment analysis task from two years ago and there.",
                    "label": 0
                },
                {
                    "sent": "So what we get from the.",
                    "label": 1
                },
                {
                    "sent": "The data set basically is a set of reviews which are already split in sentences, so we don't have to do that, and the sentences are annotated with aspects, and each aspect has a polarity.",
                    "label": 1
                },
                {
                    "sent": "So either positive, neutral or negative.",
                    "label": 1
                },
                {
                    "sent": "So the aspects are given in our case, so we only focus on the sentiment analysis part.",
                    "label": 0
                },
                {
                    "sent": "So then we need to save.",
                    "label": 0
                },
                {
                    "sent": "It's what the right polarity is.",
                    "label": 0
                },
                {
                    "sent": "So our idea was to see how far we could get by doing this using an ontology instead of using a fancy machine learning classifier.",
                    "label": 0
                },
                {
                    "sent": "So there are some.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for that, so previously we did have a classifier and we tried to add some ontology features to it.",
                    "label": 1
                },
                {
                    "sent": "It gave a somewhat better results, but the results are really hard to interpret and we don't really know what the ontology is really adding apart from the little performance boost.",
                    "label": 1
                },
                {
                    "sent": "So we thought, OK, what if we just use an ontology and use that for sentiment prediction as much as possible?",
                    "label": 0
                },
                {
                    "sent": "Well, in that case you know exactly what's going on, right?",
                    "label": 1
                },
                {
                    "sent": "Because there are words in the text, there are some concepts in ontology.",
                    "label": 0
                },
                {
                    "sent": "You know exactly how you get through your prediction, which is nice.",
                    "label": 0
                },
                {
                    "sent": "However, the ontology needs to be big enough to cover enough cases.",
                    "label": 0
                },
                {
                    "sent": "Spoiler ours isn't, so that's an issue, but we'll get to that.",
                    "label": 0
                },
                {
                    "sent": "So to cover for the lack of size, we have a simple bag of words classifier that we can use as a sort of a backup.",
                    "label": 1
                },
                {
                    "sent": "Algorithm, so if the ontology is not able to fear it out, then we can use this bag of words classifier to help out a bit.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So about the ontology.",
                    "label": 0
                },
                {
                    "sent": "So what's the purpose?",
                    "label": 0
                },
                {
                    "sent": "So at the core, of course it needs to be somewhat of a sentiment lexicon because you need to have those concepts in there.",
                    "label": 0
                },
                {
                    "sent": "So we have aspect and sentiment concepts and these have lexicalization's to link them to words in the text.",
                    "label": 1
                },
                {
                    "sent": "So we find some words.",
                    "label": 0
                },
                {
                    "sent": "We know what concepts there are because we are limited to the restaurant domain.",
                    "label": 1
                },
                {
                    "sent": "That's the data set from the semi file that we are using.",
                    "label": 0
                },
                {
                    "sent": "We don't do any word sense disfiguration because given the domain there are not many words which have really multiple meanings here.",
                    "label": 0
                },
                {
                    "sent": "So we also annotate some of the high level aspects concepts in ontology with the aspects category, which is a label from the data set.",
                    "label": 0
                },
                {
                    "sent": "So just to show you.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a part of the data, so you see the polarity that we want to predict.",
                    "label": 0
                },
                {
                    "sent": "And here are the aspects categories which are given so we can use this label here to basically link some of the concepts in the ontology to some of the things we want to find in data.",
                    "label": 0
                },
                {
                    "sent": "Which is helpful, which we'll see later.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are some words in the ontology which are always of a certain sentiments like good.",
                    "label": 1
                },
                {
                    "sent": "Good is always a positive thing, regardless of how you use it, where you use it, it's very nice and simple words and there are a lot of words like that and those concepts that represent these words are subclasses of positive or negative.",
                    "label": 1
                },
                {
                    "sent": "If there are negative words an in the paper we call this type one sentiment words there just nice and easy.",
                    "label": 0
                },
                {
                    "sent": "We do not have a neutral class in the ontology, which is a bit of a shortcoming in one hand, because the data set does have instances which are neutral, but we found it pretty hard to say OK, this word really denotes a neutral instance, so we skip that for now.",
                    "label": 0
                },
                {
                    "sent": "So in most cases, what you would say is a neutral word often is also a negative words, because it's not, let's say good enough, right?",
                    "label": 0
                },
                {
                    "sent": "If the food is average.",
                    "label": 0
                },
                {
                    "sent": "You're not really that happy to go to that restaurant, so that's why we left it out to you because it was a bit too difficult to model that.",
                    "label": 0
                },
                {
                    "sent": "Alright, so then.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second purpose of the ontology apart from having just the words in there and the sentiment is to help with the scoping.",
                    "label": 1
                },
                {
                    "sent": "So there are some sentiment words which basically imply a certain aspect category, let's say.",
                    "label": 1
                },
                {
                    "sent": "Have an example noisy.",
                    "label": 0
                },
                {
                    "sent": "If you have finally word noisy, which is a bit of a negative words, then you know it has to be about the ambiance, which is one of the aspects categories in the datasets, so it cannot be about food, right?",
                    "label": 0
                },
                {
                    "sent": "Your food is not noisy, hopefully.",
                    "label": 0
                },
                {
                    "sent": "Otherwise something's really wrong, but normally speaking it's about the ambiance, not the foods.",
                    "label": 0
                },
                {
                    "sent": "So because we annotated like we make the aspect superclass of this noisy words in this case would be the ambiance class.",
                    "label": 0
                },
                {
                    "sent": "We can use that to say OK, every other aspect that we're trying to find a sentiment forward that is not a subclass of this ambiance closet then noisy is not applicable, so we just ignore it when finding the sentiment so we can ignore irrelevant words.",
                    "label": 1
                },
                {
                    "sent": "Basically using this setup and we call these Type 2 sentiment words.",
                    "label": 0
                },
                {
                    "sent": "Then there is a third purpose.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rich opposition obviously Maps to the Type 3 sentiment words where we look at the context dependent sentiment words, so that's the last type where basically the same words has a different polarity in different contexts.",
                    "label": 1
                },
                {
                    "sent": "So when it's about one aspect, it can be positive when it's about another aspect that can be negative, which is tricky of course.",
                    "label": 0
                },
                {
                    "sent": "But the ontology can help here.",
                    "label": 1
                },
                {
                    "sent": "So the examples.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have a high price and a high-quality so high is positive for one, but negative for the other.",
                    "label": 1
                },
                {
                    "sent": "An in the ontology.",
                    "label": 0
                },
                {
                    "sent": "We modeled that using class axiom, so we see if we find quality and high that is then a subclass of positive model.",
                    "label": 1
                },
                {
                    "sent": "If we find price and high it's a subclass of negative.",
                    "label": 0
                },
                {
                    "sent": "So the way it works is that if we find these words close together or directly related to each other actually, then we create a new class, let's say high quality an then hopefully there is a class axiom.",
                    "label": 1
                },
                {
                    "sent": "In this case that will trigger an also make that new class subclass of.",
                    "label": 0
                },
                {
                    "sent": "Positive.",
                    "label": 0
                },
                {
                    "sent": "And we can infer the sentiment like that.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the basically the idea of the ontology.",
                    "label": 0
                },
                {
                    "sent": "So how do we use it for sentiment classification?",
                    "label": 0
                },
                {
                    "sent": "Because just having the ontology is not really enough and you have to do some some code around it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the the classification.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uses a simpler method as we could possibly think of.",
                    "label": 1
                },
                {
                    "sent": "So we for each aspect that is given in the datasets we are looking at all of the concepts.",
                    "label": 1
                },
                {
                    "sent": "All of the sentiment concepts in the sentence where that aspect is in.",
                    "label": 1
                },
                {
                    "sent": "So that's all given in the data.",
                    "label": 0
                },
                {
                    "sent": "And then for each of these sentiment concepts, we're looking at what type it is, right?",
                    "label": 1
                },
                {
                    "sent": "We have these three types.",
                    "label": 0
                },
                {
                    "sent": "Then we check the type of each sentiment words.",
                    "label": 0
                },
                {
                    "sent": "So if it's type one, then we just save all of the Super classes where we have a method.",
                    "label": 0
                },
                {
                    "sent": "So OK, give me all your super classes, so that's easy.",
                    "label": 0
                },
                {
                    "sent": "So all of the superclasses are stored in a set for later.",
                    "label": 0
                },
                {
                    "sent": "If it's type 2, then we do the same, but only if the aspect matches with the ontology and the category annotation that we have for that aspect so.",
                    "label": 1
                },
                {
                    "sent": "The noisy will not be used to classify food, let's say.",
                    "label": 0
                },
                {
                    "sent": "And then for the type three, we check each directly related words, so that's related on the dependency graph.",
                    "label": 1
                },
                {
                    "sent": "Which is also an aspect, so it has to be directly related and it has to be an aspect.",
                    "label": 0
                },
                {
                    "sent": "And then we check by making a new class basically of which is a subclass of both to see if there is maybe a class action that will be triggered, and then we again just save all of the Super classes to a set.",
                    "label": 0
                },
                {
                    "sent": "So if there is a class axiom, these superclasses of the new one will include either positive or negative.",
                    "label": 0
                },
                {
                    "sent": "If there's no class axiom, then they're just the same as just a combination of the two.",
                    "label": 0
                },
                {
                    "sent": "Superclasses and nothing special happens really.",
                    "label": 0
                },
                {
                    "sent": "If there is a negation, we do check for that by looking at the negator words in the preceding context and by checking the dependency graph for negation relation.",
                    "label": 0
                },
                {
                    "sent": "In that case, we just flip around the sentiment because we're doing no neutral cases that you can just swap and then we have this set of superclasses we can just check if there is a positive or a negative class in there.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this case, if we only have a positive class, maybe one or more, but it's a set.",
                    "label": 0
                },
                {
                    "sent": "So just once, if it's there, we say, OK, this is a positive aspect.",
                    "label": 0
                },
                {
                    "sent": "If there is only negative in there, we say, well, it has to be negative.",
                    "label": 1
                },
                {
                    "sent": "If there is both or there is none, then the ontology doesn't really help, so there are too many signals or conflicting signals or no signals at all.",
                    "label": 0
                },
                {
                    "sent": "So in that case we don't really know what to do.",
                    "label": 0
                },
                {
                    "sent": "So we tried looking at counting how many positive and negatives there are an even discounting with the distance towards the aspect that the things we tried in that regard didn't really help out much.",
                    "label": 0
                },
                {
                    "sent": "So we just kept it like that, and for the pure ontology methods we just predict the majority class which is positive for this data set.",
                    "label": 0
                },
                {
                    "sent": "But our backup method, as mentioned is a bag of words model.",
                    "label": 0
                },
                {
                    "sent": "So in the evaluation we have the ONS method, which is just a pure ontology with the majority class.",
                    "label": 0
                },
                {
                    "sent": "If there is no.",
                    "label": 0
                },
                {
                    "sent": "Information really.",
                    "label": 0
                },
                {
                    "sent": "And the end plus bow is the ontology method plus the bag of words as a backup methods.",
                    "label": 1
                },
                {
                    "sent": "So it is better.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kids model just uses the presence of words, but in the whole review we found that that's actually worked a bit better than looking at only the sentence.",
                    "label": 1
                },
                {
                    "sent": "So it gave a bit better results.",
                    "label": 0
                },
                {
                    "sent": "We have the aspect category as a feature because that's given in the data, and then because we use the core NLP library for the dependency parsing, we just as easily could use this sentiment module that they have in there.",
                    "label": 1
                },
                {
                    "sent": "So we have the sentiment of this sentence as determined by the sentiment module.",
                    "label": 0
                },
                {
                    "sent": "MLP as a feature as well.",
                    "label": 0
                },
                {
                    "sent": "Numerical feature, and then it's just standard waka SVM with standard kernel, basically a standard as possible.",
                    "label": 0
                },
                {
                    "sent": "And then, like.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is 1/4 methods for the evaluation.",
                    "label": 0
                },
                {
                    "sent": "We also tried adding the output of the ontology model to the bag of words model so we have one additional feature, basically saying positive or negative, which was the outcome of the ontology methods.",
                    "label": 0
                },
                {
                    "sent": "Then we add that as an additional feature to the bag of words model just basically to see what happens.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so the results I just want to show quickly the distribution.",
                    "label": 0
                },
                {
                    "sent": "So to give an idea of the how hard the problem is.",
                    "label": 0
                },
                {
                    "sent": "So this is for the 2016 data.",
                    "label": 1
                },
                {
                    "sent": "We also tried the 2015 data which is basically a subset of this data set.",
                    "label": 0
                },
                {
                    "sent": "So as you can see it's not really balanced so most of the instances are actually positive, even more so on the test data.",
                    "label": 0
                },
                {
                    "sent": "Then on the training data a little bit and there are not many neutral so it was not.",
                    "label": 0
                },
                {
                    "sent": "That bats to just skip them, although the bag of words model does predicts neutral, so it's trained on all three classes.",
                    "label": 0
                },
                {
                    "sent": "OK, so the 2015 data was a little bit less unbalanced though, but still unbalanced.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are the results for the 5th, 2015 data.",
                    "label": 0
                },
                {
                    "sent": "So the ontology methods on its own does not perform supergroups.",
                    "label": 0
                },
                {
                    "sent": "To be honest, that's how it is.",
                    "label": 0
                },
                {
                    "sent": "The bag of words.",
                    "label": 0
                },
                {
                    "sent": "The models either with or without ontology.",
                    "label": 0
                },
                {
                    "sent": "Things work quite a bit better.",
                    "label": 0
                },
                {
                    "sent": "So did the two stage approach where we used the ontology method with the bag of words as a backup.",
                    "label": 0
                },
                {
                    "sent": "Actually slightly outperforms the other way around.",
                    "label": 0
                },
                {
                    "sent": "Where we add the ontology to the bag of words feature set, which was interesting to see.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This results are somewhat the same for 2016.",
                    "label": 0
                },
                {
                    "sent": "Do well, it was more unbalanced, so that's maybe why the ontology method works a bit better.",
                    "label": 0
                },
                {
                    "sent": "I can imagine the majority vote being maybe a bit better.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "So All in all, the performance was a bit higher and but you know the ordering of the methods is still the same.",
                    "label": 0
                },
                {
                    "sent": "So one of the things we also are also interested in was because the ontology method doesn't need any training data, right?",
                    "label": 0
                },
                {
                    "sent": "All of the information is basically put in the ontology, so we were interested to see if you'd use the ontology based method if it needs less training data, then the bag of words model.",
                    "label": 0
                },
                {
                    "sent": "So we did a data size sensitivity analysis where we bid.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only kept the tests at the same but artificially shrunk the training training set to see what happens.",
                    "label": 0
                },
                {
                    "sent": "So this is for the 2015 data an what we wanted to see was basically the gap here becoming wider between the bag of words model, which is the orange dotted line here and the two with ontology based methods.",
                    "label": 0
                },
                {
                    "sent": "So yeah, that happened, so that was good to see.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately for the 26.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In data you see, the difference is really pretty much the same, so we wanted to see this gap increase here as well, but it didn't.",
                    "label": 0
                },
                {
                    "sent": "So that was a.",
                    "label": 0
                },
                {
                    "sent": "There's a bit of a shame, though.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The last analysis that we did was to look at the different scenarios where we use each algorithm in right.",
                    "label": 0
                },
                {
                    "sent": "Because we say if we find only positive or only negative, then we use the ontology methods.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we use either this majority vote or Bank of words.",
                    "label": 0
                },
                {
                    "sent": "So we checked for.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Various scenarios to see how each of the methods is performing.",
                    "label": 1
                },
                {
                    "sent": "To get a bit more insight.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In case we find just a positive class that would be in 43% of the cases and then the ontology will predict positive of course, and it would be right in 88% of the cases, which is pretty high in a bit higher than the bag of words model an for negative, which is a lot slower, smaller though, so it doesn't happen that often that it will say there's just the negative concept in here.",
                    "label": 0
                },
                {
                    "sent": "But if it does, then it is usually right.",
                    "label": 0
                },
                {
                    "sent": "However, I did.",
                    "label": 1
                },
                {
                    "sent": "Interesting part is in the bottom, so the top two are basically the easy cases, right?",
                    "label": 0
                },
                {
                    "sent": "So the ontology is able to figure it out.",
                    "label": 0
                },
                {
                    "sent": "The bag of words model actually is also pretty high, so those are not the very interesting cases, but the bottom two are more interesting I think.",
                    "label": 0
                },
                {
                    "sent": "So in the third line we see where there are both positive and negative instances in the sentence.",
                    "label": 0
                },
                {
                    "sent": "Then the ontology will do the majority vote, which is sort of even.",
                    "label": 0
                },
                {
                    "sent": "Like almost 5050.",
                    "label": 0
                },
                {
                    "sent": "Same for bag of words, but a little bit higher, but the performance is so much lower than the other two that you know both actually have issues or the bag of words model also doesn't really know what's going on, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a 5050 thing.",
                    "label": 0
                },
                {
                    "sent": "And the lower lowest line is where there is nothing, so the ontology does not have any information about the words in the sentence.",
                    "label": 0
                },
                {
                    "sent": "So you can see that as basically a failure of ontology size, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not big enough to cover those cases because the bag of words model does have some information here, so the performance is pretty OK. 77 for just a simple bag of words model.",
                    "label": 0
                },
                {
                    "sent": "So apparently there are some signals in there, but they're just not in the ontology, so we cannot use them.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, I actually looked over my slides this morning and I thought, OK, if we're doing majority votes, how can the performance be only 33%?",
                    "label": 0
                },
                {
                    "sent": "So maybe the majority is not the majority anymore for this subset that we are arriving at but never thought of that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusions.",
                    "label": 0
                },
                {
                    "sent": "So we have this ontology method and a bag of words which combine pretty OK, so they complement each other to a certain extent, which gives pretty good performance actually.",
                    "label": 1
                },
                {
                    "sent": "So we think the performance of the Pure Ontology method is low because it doesn't cover most cases, so it's about 50% of the time that you can actually use it given the current size and the other 50%, it's either majority vote or bag of words.",
                    "label": 1
                },
                {
                    "sent": "However, when it is used it gives good performance an.",
                    "label": 0
                },
                {
                    "sent": "Explainable results and you don't need any training data, however of course you know domain ontologies don't fall from the Sky, so you can say we don't need training data, but you will need to knology which is also a lot of work to get.",
                    "label": 0
                },
                {
                    "sent": "So in a sense that's that's a bit of a thing.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for future work, we're looking into doing more automatic ontology population.",
                    "label": 1
                },
                {
                    "sent": "So we do have a sort of semi automatic approach which gives suggestions to add to the ontology now.",
                    "label": 0
                },
                {
                    "sent": "So that helps with the save some time, but ideally you would do that on completely automatically, but we don't have such methods yet.",
                    "label": 0
                },
                {
                    "sent": "Other things more in terms of the quality, is to include multiple words expression.",
                    "label": 1
                },
                {
                    "sent": "So currently all of the concepts are really linked to a single word like good or bad, but there are also expressions like out of this world which is actually in the data and the separate words do not have any sentiment right out in the world.",
                    "label": 0
                },
                {
                    "sent": "It's just regular words, but together they actually have the sentiment which is pretty positive.",
                    "label": 0
                },
                {
                    "sent": "So it would be interesting to see if we can.",
                    "label": 1
                },
                {
                    "sent": "Somehow get that in there as well.",
                    "label": 0
                },
                {
                    "sent": "And then maybe it's interesting to look at the sentiment in this distance information.",
                    "label": 1
                },
                {
                    "sent": "So for type three we just do the directly related aspects to a given sentiment words just one step on the dependency graph.",
                    "label": 1
                },
                {
                    "sent": "But we didn't really investigate if maybe we needed two steps or three, or maybe just over different certain kinds of grammatical relations and not others.",
                    "label": 0
                },
                {
                    "sent": "We just do one step regardless of relation type, which is pretty simplistic.",
                    "label": 0
                },
                {
                    "sent": "And last, there is a quite a bit of a speed issue, at least in our current implementation.",
                    "label": 0
                },
                {
                    "sent": "It is pretty slow, so we're looking how to solve it, but I don't really know how yet.",
                    "label": 0
                },
                {
                    "sent": "So we use this general library in Java to use the ontology to get information out, but that is pretty slow and if we add the class so for the Type 3 sentiment where it's basically it will rebuild the complete inference model and the larger your ontology, the more time it takes.",
                    "label": 0
                },
                {
                    "sent": "It is nasty.",
                    "label": 0
                },
                {
                    "sent": "So yeah, those are the things we're looking to improve.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you have any questions then I'm happy to answer.",
                    "label": 0
                },
                {
                    "sent": "The code is available on GitHub so you can have a look at it, but yeah, it is somewhat slow, so that's that's a warning.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}