{
    "id": "6awfvmwv2ocao266i4id3zvre34vsye4",
    "title": "Improving Text Simplification Language Modeling Using Unsimplified Text Data",
    "info": {
        "author": [
            "David Kauchak, Department of Computer Science, Middlebury College"
        ],
        "published": "Oct. 2, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/acl2013_kauchak_improving/",
    "segmentation": [
        [
            "So how many people have heard of text simplification before?",
            "OK, good.",
            "I notice it's sort of a growing field, so I always ask, you know, just to make sure on the same."
        ],
        [
            "So I thought I'd start out with a with a quote from.",
            "I'm not sure, but Internet attributes it to Albert Einstein.",
            "You can never quite tell.",
            "So I think it's a nice motivation for why we actually want to look at simplification.",
            "You know a lot of motivations for assess Accessibility and other things like that which motivated, and it also gives me a nice example to show an example of simplification.",
            "So if I were to simplify this may be a very simple way.",
            "Is it simpler?",
            "Is better so to put it sort of more concretely though."
        ],
        [
            "The goal for simplification is that we would like what we'd like to do is take some text and reduce the reading complexity of the text.",
            "By either reducing, you know the simplicity of the cab, you Larry, the structure structure could be sentence structure.",
            "You know the discourse structure, something like that.",
            "And the key ideas that we'd like to keep the content this same.",
            "So we're trying to reduce the reading level.",
            "Keep the content the same."
        ],
        [
            "So a lot of the approaches, particularly approaches that people have been looking at saying the last five years.",
            "So to follow the traditional sort of translation type approach.",
            "And given that now ACL seems to be like 1/4 translation, hopefully everybody sort of roughly familiar with that.",
            "But the basic idea is you have some sentence or text or document that comes in.",
            "You have some sort of model that you're working with an the output that you'd like to get out of.",
            "That is some sort of simplified version that still maintains roughly the content, but.",
            "Is easier to understand, say for a broader range of audiences.",
            "Any guesses as to what the simplification is?",
            "Yeah, so I do not like green eggs and ham, so this is the basic idea, right?",
            "More complicated comes in simplification, so if we look under the covers, there's a lot of different approaches, right that people have taken for simplification in general, sort of translation, and inside the model there's often common components are some sort of translation model.",
            "How this simplification actually happens?",
            "There's some language model, maybe a model of length, and pick your favorite models features that you want to include there.",
            "So what I want to focus on today is this is the language model and hopefully everybody sort of familiar with the general idea of a language model models the likelihood of the output text in this case.",
            "Simplified text.",
            "And you know one of the reasons we wanted to focus on this is, you know, we started originally building simplification systems and we were really sure what to do for language model.",
            "So we tried a few things and it worked OK.",
            "But for this work we wanted to dive in a little bit more and see what we could do here."
        ],
        [
            "So how much?",
            "Anybody know, ballpark how much data is available for training a simple English language model?",
            "So how much sort of simple text in English is available?",
            "Yeah, they're simple English Wikipedia exactly, and that seems to be the main resource, and if you look at it nowadays, I mean, this is as of a few months ago.",
            "I think it's about 1/2 a million sentences, so it's not.",
            "It's not, you know.",
            "It's not T if you compare to, say, compression or some of these other domains, there's not.",
            "This is a lot more data, but if you compare it to other domains like, say machine translation, how much data is available just to train an English language model?",
            "Yeah."
        ],
        [
            "Yeah, lots right?",
            "So basically there's a lot of data out there.",
            "I mean, there's the.",
            "You know the Google corpus.",
            "There's the giga word corpus.",
            "There's all sorts of Wikipedia, other web data, so there's lots and lots of data here.",
            "And so the question is really."
        ],
        [
            "Can we utilized this data so the first idea when you're trying this language model is maybe we can just use this unsimplified data so there's lots of this data out there, we have some that's simplified already, but there's a lot more that son simplified, and in fact if you look at some other domains, like say compression, this is basically what they do, right?",
            "So you don't have enough compressed data, so you just basically train it on an uncompressed model, and there's some issues with that, But that's the data that you have.",
            "So just to see if this is feasible, right?",
            "What are the ramifications of that?",
            "We did sort of 1st introductory.",
            "Investigation in this so.",
            "We took English Wikipedia and simple English Wikipedia an we looked at.",
            "There's a sentence aligned corpus that we put together a few years ago and has 137 thousand sentences and this is, you know, pretty good alignment.",
            "Something like a quarter of these sentences are actually identical, and then the other 3/4 are pretty good translation.",
            "There's a few mistakes in there, but it's not too bad, so we wanted to look at how much overlap is there between the sort of simple text Ann.",
            "The normal text is all call it, so throughout the rest I use simple for simple English and normal for sort of normal.",
            "Some people call it complex.",
            "Those sorts of things, so we looked at the engram overlap."
        ],
        [
            "And the good news is, you know if you look at.",
            "Right, if you look at the lower lower order N grams, there's actually pretty reasonable overlap, and this is not surprising, particularly since this is sentence alliance in the same language, so this is sort of encouraging.",
            "There's there is this day that we may be able to use in this pretty good overlap the problem."
        ],
        [
            "That may be bad news.",
            "Right is there if you look at the higher order N grams, the overlaps not as good right?",
            "It may or may not be an issue, right?",
            "These are two different corpora on some level.",
            "If you look at this a little."
        ],
        [
            "Closer.",
            "So here are percentages from before the number of engrams from the simple corpus found in the normal.",
            "We flip it around and ask how many of the engrams found in the normal are in this simple.",
            "What you notice is that the distributions are different.",
            "So in particular.",
            "On the normal side, there's just more engrams.",
            "The sensors tend to be a little bit longer.",
            "They do.",
            "The vocabulary is a little bit more varied, so you get more varied.",
            "Engrams so?",
            "So there may be some benefit.",
            "It is English, but the vocabulary distribution over the language is slightly different than, say, the simplified English."
        ],
        [
            "So given this, the big questions that we really wanted to ask with this study was First off, you know how do these distribution differences affect language modeling performance?",
            "So there are slight differences.",
            "What does that actually mean when we start to use it for a language model in a simple English or simplification task?",
            "Can we use this data at all for simple language modeling?",
            "And then, given that, what is the best way to actually use it?"
        ],
        [
            "So to investigate this, what we did is we started with a document aligned corpus.",
            "We wanted to sort of keep the data set as sort of stable as possible, remove any variation for content etc.",
            "So anybody who worked with her computer, one of the nice things, is document aligning is very easy because they are very strict about title and therefore article naming.",
            "So we basically took all of this simple English Wikipedia articles, which this was a couple years ago.",
            "There's maybe about 10 or 20,000 more now, so this was six 60,000 articles, and we found their corresponding article article.",
            "In English Wikipedia and this was our data set that we that we did the experiments on, so it's around 60,000.",
            "It's 60,000 articles sentences you can see again, there's this disparity.",
            "The traditional English Wikipedia articles just tend to be longer.",
            "Sentences are longer, and there's just more data there, so if you look both sentences and words about an order of magnitude larger already, and this is just in the aligned set, right?",
            "There's other data beyond this.",
            "And again, if you look at vocabulary size over this data set, you can see about four or five times as much so, but this is the data set we decided to play with to experiment with language modeling."
        ],
        [
            "So the first task we looked at is sort of, you know, straightforward task for evaluating language modeling, which is just perplexity.",
            "So we take whatever day that we're going to train the language model on.",
            "We use the Sri Language Modeling Toolkit, trigram language model fairly standard.",
            "Smoothing techniques and then we used to close vocabulary.",
            "Since we're measuring with perplexity means the distributions need to be over the exact same thing.",
            "We tried a number of different vocabulary things and the results came out roughly the same, and then to test it.",
            "Basically we held out 2000 articles.",
            "And tested the perplexity right, so how likely the language models thought, or how confused they were on those simple articles?",
            "This is fairly standard setup for intrinsic evaluation of language model.",
            "And the models that we decided to evaluate.",
            "So one is the simple only articles or language model.",
            "So language model trained just one simple English Wikipedia articles the normal only.",
            "So if we trained on the opposite right the English Wikipedia articles and then a language model trained on a concatenation of the two.",
            "So take some number of sentences from simple English Wikipedia and concatenate on some normal Wikipedia articles and then train the language model on that."
        ],
        [
            "So here's our first 1st result.",
            "So again, here we have perplexity.",
            "Here lower is better and we have increasing number of sentences there.",
            "So the first thing that sort of encouraging about this is, it does.",
            "If you look at the results, they're sort of reasonable.",
            "With what you'd expect.",
            "So in particular what you see is the red is the simple only model performs better right for given amount of data for a fixed amount of data, the simple model performs better than the normal model.",
            "That's sort of what you'd expect to see in a particularly see.",
            "It grows much quicker here.",
            "This tends to taper off.",
            "So that's encouraging that this is doing at least something sort of reasonable.",
            "Both of them do tend to get better overtime, so as you add more sentences, results get better.",
            "That's intuitive, so.",
            "The few interesting things to note, so this is the best you can do right here.",
            "It's a member.",
            "It's about 1:30.",
            "Something perplexity with all of the basically simple English data available.",
            "What you see here is as you start to add additional normal sentences, right non simplified sentences.",
            "We still get an increase in perplexity.",
            "So this isn't, you know, very nice thing to see.",
            "In fact, if you look at the tail end it takes a lot more sentence.",
            "You can see that starts to taper off very quickly, but in the end we get a 23% improvement in perplexity by adding that normal data.",
            "One of the other interesting things to note.",
            "If you it's not as obvious, but this even if you add enough normal data, you can actually get perplexity results that are better than just using simple language.",
            "So this is, you know again because it's still English.",
            "You had enough data you can compensate for the fact that it's slightly out of domain."
        ],
        [
            "So we also wanted to see.",
            "What happens is you have different amounts of simplified data that you start with.",
            "Right, so in this domain you know, presumably you want to use all of the data you have.",
            "Maybe another domains, either in simplification in a specific domain, say medicine or something like that.",
            "Or you know in in a similar monolingual translation task we wanted to see you know what's the impact.",
            "So one of the things that sort of interesting is to see is if you have less simple data.",
            "The actual impact of adding the out of domain the normal data is even even more substantial.",
            "So here is just 50,000 sentences starting.",
            "You can see a fairly large improvement there by adding the normal data."
        ],
        [
            "So far, all we've done basically to combine the symbol in the normal data is just concatenate them together.",
            "You can think of is aggregating accounts, or just concatenating 2 corporate together.",
            "There are much much smarter ways of actually taking two language models that are, you know, I say in different domains, so simple versus normal different domains and combining them in a more reasonable way.",
            "So one of the simplest things that people do is an interpolated linearly interpolated language model.",
            "So basically you have the one model, another model, and you interpolate their probabilities together into a combined model.",
            "Now there are many other more advanced ways for doing language model adaptation, but we just wanted to again push it a little more and see if this could be useful.",
            "So we."
        ],
        [
            "This, and again for perplexity.",
            "What we have here perplexity, and here on this side, is the simple only model.",
            "So this is actually again, you see around 1:30 is the performance of the simple only there.",
            "It makes it more obvious than normal model is always on the other side, and as you start to mix the two models together.",
            "You start to see the improvement an at about 5050 right match between 5050 blending of the normal in a simple model you get a slight improvement only over just concatenating them, so there are, you know adaptation helps a little bit there."
        ],
        [
            "So just to summarize, task one in the end, if you you know if you use a linearly interpreted model you get a 24% increase in perplexity.",
            "Improvement in perplexity over just using all of the simple data that's currently available, so that's a nice sort of result.",
            "So."
        ],
        [
            "It was an intrinsic evaluation, right language model, or perplexities intrinsic model.",
            "We also wanted to do an XX trinsic model.",
            "How does it actually affect simplification performance?",
            "So right now where the systems and evaluation are for tax implication, there really aren't standardized ways of evaluating text application systems.",
            "Are there really only a handful of systems out there?",
            "So instead we looked at a different extrinsic task and this is just the sum eval 2012 lexical simplification task.",
            "So the basic ideas you're given a sentence and a particular word in that sentence.",
            "And they used the data, so these are phrasal substitutions for that word.",
            "They all fit contextually perfectly fine, and then they asked a bunch of humans to actually rank those based on simplicity.",
            "So your task is, given these candidates to learner anchor that ranks the words the candidate substitutions by simplicity.",
            "So this was the task that we decided to look at for an extrinsic task."
        ],
        [
            "And the way we post or use the language model to do this ranking was sort of straightforward way.",
            "You basically consider all the substitutions in the context of the sentence, evaluate them with the language model, and then you rank them based on the language model score."
        ],
        [
            "For evaluation we use the same evaluation metric that they use for the semi valve 2012 tasks, and it roughly is how how close does the ranking here correlate?",
            "The system ranking correlate to the human ranking the way they do that is basically a pairwise comparison of the rank and then you gotta basically Kappa score for that.",
            "So there between zero and one higher being better."
        ],
        [
            "So same graph that we looked at before for perplexity.",
            "Now for the lexical simplification task, here we have the cap rank score better, higher is better.",
            "And again we have the simple only model.",
            "The normal only model and then the normal where we appended the data.",
            "So again, just like we saw before, you tend to see this nice sort of increase as you add more data, the scores get better.",
            "That's encouraging and you also see this simple performs better than just the normal only model.",
            "One difference though, with the versus.",
            "The perplexity data you notice here, this is about .345, something like that.",
            "As we add more normal data, the results actually don't get much better.",
            "In fact, they kind of swivel around, but they really don't get any better, so so far.",
            "Doing adding extra data didn't actually help."
        ],
        [
            "Now just like before, if we look at limiting the amount of simple data that we have.",
            "And then adding more normal sentences.",
            "We do actually start to see the increase.",
            "So here at the top is that same one we saw before starting with all the simple data, the Orange one you see we don't get an increase, but so if we start with something like only 100,000 sentences, so maybe another domains if you are limited.",
            "Then adding normal data does help, and you know even up here when you have something like 250,000 sentences, you still do get an increase in adding that, so that's a nice benefit.",
            "Now let's look at."
        ],
        [
            "Language model adaptation.",
            "So we did the linear linearly interpolated adaptation model, just like we did for the perplexity.",
            "Here what I'm showing is as we increase the number of data.",
            "So here when we do adaptation, we can get a benefit so we can get an improvement.",
            "So this is basically where the simple only model and basically where we were before.",
            "If we just took a simple only model and concatenated all the normal data, so around 35 if we use language model adaptation to blend the simple data in the normal data so that those two models we again can see a nontrivial improvement in the performance of the lexical ranking task.",
            "And actually we didn't design this for the semi Val task.",
            "But with this language model adaptation system performs relatively well.",
            "I remember it's like ties for 3rd or 4th out of the result, so this is actually.",
            "Very interesting result.",
            "So in the end we end up with again about a 23% improvement over the simple only model on this lexical simplification task."
        ],
        [
            "So it seems at least on a couple of different tasks.",
            "Right, that adding normal data does help.",
            "We can utilize that to improve our performance, so the question is why does it help?",
            "I'm not sure this is still something we're thinking about, but my best guess is that, well, you have it's more data, so you have more engrams.",
            "So to slightly to investigate this a little bit, we looked at basically how many more engrams do we have in the normal data than in the simple data."
        ],
        [
            "So in particular, we looked at the test data that we use for the perplexity, and the lexical simplification task, and we looked at basically the coverage of the engrams.",
            "And so if you look at the unigrams bigrams and trigrams across the board.",
            "The normal data has better coverage.",
            "It has more of those engrams now it's out of domain, but it does have better coverage.",
            "So when you blend the two together.",
            "You can get better coverage overall if you do something intelligent with that.",
            "So to put this in context, for example, in this one the 9.4% increase.",
            "This is actually a decrease in about half of the out of vocabulary.",
            "Words that you end up with if you just use the simple data.",
            "So I mean this is a non trivial improvement for the for your language model and the other benefit this is just you know grams you haven't seen even for sort of rare engrams that maybe it just means seen once or twice adding this normal data may help you estimate their probabilities even better.",
            "So this is our conjecture."
        ],
        [
            "One other thing to note is the application did matter, so we looked at this linearly interpolated model.",
            "If you look at the perplexity task and this partially, I think explains why you can just add that the data together.",
            "The best Lambda.",
            "Is about .5.",
            "So 5050 blending of the data?",
            "Interestingly, on the lexical simplification task.",
            "The best blending of the models was basically to use the simplified data for almost all the cases, right?",
            "So you have a very strong waiting here, so only when the simplification the simplified model doesn't have a strong.",
            "Vote or strong indicator.",
            "Strong difference between the choices.",
            "Do you then rely on the normal model?"
        ],
        [
            "So to wrap up, so First off, I mean I think given these couple of experiments, there's still some more.",
            "Then I'll talk about a minute, but I think there is using unsimplified data for training a simple English language model is useful.",
            "You get improvements on these two tasks and non trivial improvements by about 2324%.",
            "Right, so on both intrinsic and extrinsic extrinsic evaluation in both cases domain adaptation helped right so we didn't do anything particularly found on this front, but they did help.",
            "But caveat is be careful if you know depending on the domain we did see large variations in the actual adaptation parameters.",
            "We did generate some data that's available for this, so there's a new aligned corpus that we have available, plus all of the document aligned data.",
            "So people are curious, you could just do a search for me, or there's the.",
            "The link in links also in the paper."
        ],
        [
            "To finish up, there are a lot of sort of interesting questions.",
            "I think that I would still like to investigate, or maybe other people would like to investigate.",
            "So first up again, what we looked at here was the document aligned data set so it's only 60,000 articles.",
            "And was I think 2.5 million sentences in normal English.",
            "So the question is, how much more data can we utilize?",
            "So can we utilized say?",
            "Corpus or the giga word corpus.",
            "And you know how does that affect our performance?",
            "How does this source so these were basically in the same domain that they were article aligned?",
            "So how does the source effect the performance on the simple English?",
            "And then what happens on sort of more realistic?",
            "So this is was half realistic simplification task.",
            "But what happens when we actually try to use a safer full sentence simplification systems or other lexical simplification tasks?",
            "And then obviously investigating better language model adaptation techniques?",
            "So that's all I have.",
            "In your evaluation you will look at a particular type of lexical simplification, obviously related to N gram models in quite a simple way, and I wonder.",
            "If you think that made a big difference somehow.",
            "And it might be different with other kinds of things.",
            "Yeah, I mean, it would be interesting to try it on other types of models.",
            "So in the same eval task, probably 2/3 of the approaches that people use did have a language model as a feature as a feature, and that tended to be the strongest indicator.",
            "But there are other features that will actually go into it.",
            "So one of the things that were sort of interesting looking at further is what happens is you start to incorporate this language model and sort of more sophisticated either lexical simplification or sentence level tasks.",
            "Hi there, I'm at Schadler University of Manchester.",
            "It's really encouraging to see what you're doing and how about you to say thank you for.",
            "Yeah presenting great.",
            "I was wondering if you'd looked at controlling which of the English Wikipedia articles and sentence is are actually going into your additive model, like controlling for the simplest sentence is.",
            "Yeah, so we didn't yet, so I did some.",
            "We did some experiments with, basically randomizing which ones were getting and the results seem to be the same, but it would be interesting.",
            "Like you said, to investigate more how content or other things like that, so we didn't.",
            "Look at it yet, but I think that is sort of interesting.",
            "Would be interesting to see, thanks.",
            "Hi, thank you for the talk.",
            "It is.",
            "I think that certain amount of simple sentence is simplified.",
            "Sentences are exactly the same as the unsupervised sentences, right?",
            "Have you end in the Wikipedia in the simple English Wikipedia in the guidelines, they write that there is a certain corpus from which you should try to choose the words.",
            "If you want to write in simple way, so have you tried to kind of filter out the unsimplified data?",
            "Based on this vocabulary and maybe like this to extend the out of domain.",
            "Yeah, no, that's a good idea.",
            "So one thing we did try is.",
            "Some people other people have done this but basically classifying sentence is as simple and not simple because it is kind of hard.",
            "Like you said, there is some overlap so you can't try to figure out which ones are simple or not.",
            "So we actually tried to learn a classifier based on a handful of features to predict whether a sentence was simple or not.",
            "And you accuracy is reasonable.",
            "I think around 80% and then when we tried to use those the results weren't quite as strong as you hope to see, but that is I think something to investigate would be to basically throw out the really complex ones and try and keep it.",
            "But we didn't see a lot of benefit but.",
            "Sort of a preliminary experiment.",
            "I think we better leave it there 'cause we need time.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how many people have heard of text simplification before?",
                    "label": 1
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "I notice it's sort of a growing field, so I always ask, you know, just to make sure on the same.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I thought I'd start out with a with a quote from.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure, but Internet attributes it to Albert Einstein.",
                    "label": 1
                },
                {
                    "sent": "You can never quite tell.",
                    "label": 0
                },
                {
                    "sent": "So I think it's a nice motivation for why we actually want to look at simplification.",
                    "label": 0
                },
                {
                    "sent": "You know a lot of motivations for assess Accessibility and other things like that which motivated, and it also gives me a nice example to show an example of simplification.",
                    "label": 1
                },
                {
                    "sent": "So if I were to simplify this may be a very simple way.",
                    "label": 0
                },
                {
                    "sent": "Is it simpler?",
                    "label": 0
                },
                {
                    "sent": "Is better so to put it sort of more concretely though.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The goal for simplification is that we would like what we'd like to do is take some text and reduce the reading complexity of the text.",
                    "label": 1
                },
                {
                    "sent": "By either reducing, you know the simplicity of the cab, you Larry, the structure structure could be sentence structure.",
                    "label": 0
                },
                {
                    "sent": "You know the discourse structure, something like that.",
                    "label": 0
                },
                {
                    "sent": "And the key ideas that we'd like to keep the content this same.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to reduce the reading level.",
                    "label": 1
                },
                {
                    "sent": "Keep the content the same.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a lot of the approaches, particularly approaches that people have been looking at saying the last five years.",
                    "label": 0
                },
                {
                    "sent": "So to follow the traditional sort of translation type approach.",
                    "label": 0
                },
                {
                    "sent": "And given that now ACL seems to be like 1/4 translation, hopefully everybody sort of roughly familiar with that.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is you have some sentence or text or document that comes in.",
                    "label": 0
                },
                {
                    "sent": "You have some sort of model that you're working with an the output that you'd like to get out of.",
                    "label": 0
                },
                {
                    "sent": "That is some sort of simplified version that still maintains roughly the content, but.",
                    "label": 0
                },
                {
                    "sent": "Is easier to understand, say for a broader range of audiences.",
                    "label": 0
                },
                {
                    "sent": "Any guesses as to what the simplification is?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I do not like green eggs and ham, so this is the basic idea, right?",
                    "label": 1
                },
                {
                    "sent": "More complicated comes in simplification, so if we look under the covers, there's a lot of different approaches, right that people have taken for simplification in general, sort of translation, and inside the model there's often common components are some sort of translation model.",
                    "label": 0
                },
                {
                    "sent": "How this simplification actually happens?",
                    "label": 0
                },
                {
                    "sent": "There's some language model, maybe a model of length, and pick your favorite models features that you want to include there.",
                    "label": 1
                },
                {
                    "sent": "So what I want to focus on today is this is the language model and hopefully everybody sort of familiar with the general idea of a language model models the likelihood of the output text in this case.",
                    "label": 0
                },
                {
                    "sent": "Simplified text.",
                    "label": 0
                },
                {
                    "sent": "And you know one of the reasons we wanted to focus on this is, you know, we started originally building simplification systems and we were really sure what to do for language model.",
                    "label": 0
                },
                {
                    "sent": "So we tried a few things and it worked OK.",
                    "label": 0
                },
                {
                    "sent": "But for this work we wanted to dive in a little bit more and see what we could do here.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how much?",
                    "label": 0
                },
                {
                    "sent": "Anybody know, ballpark how much data is available for training a simple English language model?",
                    "label": 1
                },
                {
                    "sent": "So how much sort of simple text in English is available?",
                    "label": 0
                },
                {
                    "sent": "Yeah, they're simple English Wikipedia exactly, and that seems to be the main resource, and if you look at it nowadays, I mean, this is as of a few months ago.",
                    "label": 0
                },
                {
                    "sent": "I think it's about 1/2 a million sentences, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not, you know.",
                    "label": 0
                },
                {
                    "sent": "It's not T if you compare to, say, compression or some of these other domains, there's not.",
                    "label": 0
                },
                {
                    "sent": "This is a lot more data, but if you compare it to other domains like, say machine translation, how much data is available just to train an English language model?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, lots right?",
                    "label": 0
                },
                {
                    "sent": "So basically there's a lot of data out there.",
                    "label": 1
                },
                {
                    "sent": "I mean, there's the.",
                    "label": 0
                },
                {
                    "sent": "You know the Google corpus.",
                    "label": 0
                },
                {
                    "sent": "There's the giga word corpus.",
                    "label": 0
                },
                {
                    "sent": "There's all sorts of Wikipedia, other web data, so there's lots and lots of data here.",
                    "label": 0
                },
                {
                    "sent": "And so the question is really.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can we utilized this data so the first idea when you're trying this language model is maybe we can just use this unsimplified data so there's lots of this data out there, we have some that's simplified already, but there's a lot more that son simplified, and in fact if you look at some other domains, like say compression, this is basically what they do, right?",
                    "label": 0
                },
                {
                    "sent": "So you don't have enough compressed data, so you just basically train it on an uncompressed model, and there's some issues with that, But that's the data that you have.",
                    "label": 0
                },
                {
                    "sent": "So just to see if this is feasible, right?",
                    "label": 0
                },
                {
                    "sent": "What are the ramifications of that?",
                    "label": 0
                },
                {
                    "sent": "We did sort of 1st introductory.",
                    "label": 0
                },
                {
                    "sent": "Investigation in this so.",
                    "label": 0
                },
                {
                    "sent": "We took English Wikipedia and simple English Wikipedia an we looked at.",
                    "label": 0
                },
                {
                    "sent": "There's a sentence aligned corpus that we put together a few years ago and has 137 thousand sentences and this is, you know, pretty good alignment.",
                    "label": 1
                },
                {
                    "sent": "Something like a quarter of these sentences are actually identical, and then the other 3/4 are pretty good translation.",
                    "label": 0
                },
                {
                    "sent": "There's a few mistakes in there, but it's not too bad, so we wanted to look at how much overlap is there between the sort of simple text Ann.",
                    "label": 0
                },
                {
                    "sent": "The normal text is all call it, so throughout the rest I use simple for simple English and normal for sort of normal.",
                    "label": 0
                },
                {
                    "sent": "Some people call it complex.",
                    "label": 0
                },
                {
                    "sent": "Those sorts of things, so we looked at the engram overlap.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the good news is, you know if you look at.",
                    "label": 0
                },
                {
                    "sent": "Right, if you look at the lower lower order N grams, there's actually pretty reasonable overlap, and this is not surprising, particularly since this is sentence alliance in the same language, so this is sort of encouraging.",
                    "label": 0
                },
                {
                    "sent": "There's there is this day that we may be able to use in this pretty good overlap the problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That may be bad news.",
                    "label": 0
                },
                {
                    "sent": "Right is there if you look at the higher order N grams, the overlaps not as good right?",
                    "label": 0
                },
                {
                    "sent": "It may or may not be an issue, right?",
                    "label": 0
                },
                {
                    "sent": "These are two different corpora on some level.",
                    "label": 0
                },
                {
                    "sent": "If you look at this a little.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Closer.",
                    "label": 0
                },
                {
                    "sent": "So here are percentages from before the number of engrams from the simple corpus found in the normal.",
                    "label": 0
                },
                {
                    "sent": "We flip it around and ask how many of the engrams found in the normal are in this simple.",
                    "label": 0
                },
                {
                    "sent": "What you notice is that the distributions are different.",
                    "label": 0
                },
                {
                    "sent": "So in particular.",
                    "label": 0
                },
                {
                    "sent": "On the normal side, there's just more engrams.",
                    "label": 0
                },
                {
                    "sent": "The sensors tend to be a little bit longer.",
                    "label": 0
                },
                {
                    "sent": "They do.",
                    "label": 0
                },
                {
                    "sent": "The vocabulary is a little bit more varied, so you get more varied.",
                    "label": 0
                },
                {
                    "sent": "Engrams so?",
                    "label": 0
                },
                {
                    "sent": "So there may be some benefit.",
                    "label": 0
                },
                {
                    "sent": "It is English, but the vocabulary distribution over the language is slightly different than, say, the simplified English.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So given this, the big questions that we really wanted to ask with this study was First off, you know how do these distribution differences affect language modeling performance?",
                    "label": 1
                },
                {
                    "sent": "So there are slight differences.",
                    "label": 0
                },
                {
                    "sent": "What does that actually mean when we start to use it for a language model in a simple English or simplification task?",
                    "label": 1
                },
                {
                    "sent": "Can we use this data at all for simple language modeling?",
                    "label": 1
                },
                {
                    "sent": "And then, given that, what is the best way to actually use it?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to investigate this, what we did is we started with a document aligned corpus.",
                    "label": 1
                },
                {
                    "sent": "We wanted to sort of keep the data set as sort of stable as possible, remove any variation for content etc.",
                    "label": 0
                },
                {
                    "sent": "So anybody who worked with her computer, one of the nice things, is document aligning is very easy because they are very strict about title and therefore article naming.",
                    "label": 0
                },
                {
                    "sent": "So we basically took all of this simple English Wikipedia articles, which this was a couple years ago.",
                    "label": 0
                },
                {
                    "sent": "There's maybe about 10 or 20,000 more now, so this was six 60,000 articles, and we found their corresponding article article.",
                    "label": 0
                },
                {
                    "sent": "In English Wikipedia and this was our data set that we that we did the experiments on, so it's around 60,000.",
                    "label": 1
                },
                {
                    "sent": "It's 60,000 articles sentences you can see again, there's this disparity.",
                    "label": 0
                },
                {
                    "sent": "The traditional English Wikipedia articles just tend to be longer.",
                    "label": 1
                },
                {
                    "sent": "Sentences are longer, and there's just more data there, so if you look both sentences and words about an order of magnitude larger already, and this is just in the aligned set, right?",
                    "label": 0
                },
                {
                    "sent": "There's other data beyond this.",
                    "label": 0
                },
                {
                    "sent": "And again, if you look at vocabulary size over this data set, you can see about four or five times as much so, but this is the data set we decided to play with to experiment with language modeling.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first task we looked at is sort of, you know, straightforward task for evaluating language modeling, which is just perplexity.",
                    "label": 0
                },
                {
                    "sent": "So we take whatever day that we're going to train the language model on.",
                    "label": 0
                },
                {
                    "sent": "We use the Sri Language Modeling Toolkit, trigram language model fairly standard.",
                    "label": 0
                },
                {
                    "sent": "Smoothing techniques and then we used to close vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Since we're measuring with perplexity means the distributions need to be over the exact same thing.",
                    "label": 0
                },
                {
                    "sent": "We tried a number of different vocabulary things and the results came out roughly the same, and then to test it.",
                    "label": 0
                },
                {
                    "sent": "Basically we held out 2000 articles.",
                    "label": 0
                },
                {
                    "sent": "And tested the perplexity right, so how likely the language models thought, or how confused they were on those simple articles?",
                    "label": 0
                },
                {
                    "sent": "This is fairly standard setup for intrinsic evaluation of language model.",
                    "label": 0
                },
                {
                    "sent": "And the models that we decided to evaluate.",
                    "label": 0
                },
                {
                    "sent": "So one is the simple only articles or language model.",
                    "label": 0
                },
                {
                    "sent": "So language model trained just one simple English Wikipedia articles the normal only.",
                    "label": 1
                },
                {
                    "sent": "So if we trained on the opposite right the English Wikipedia articles and then a language model trained on a concatenation of the two.",
                    "label": 0
                },
                {
                    "sent": "So take some number of sentences from simple English Wikipedia and concatenate on some normal Wikipedia articles and then train the language model on that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's our first 1st result.",
                    "label": 0
                },
                {
                    "sent": "So again, here we have perplexity.",
                    "label": 0
                },
                {
                    "sent": "Here lower is better and we have increasing number of sentences there.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that sort of encouraging about this is, it does.",
                    "label": 0
                },
                {
                    "sent": "If you look at the results, they're sort of reasonable.",
                    "label": 0
                },
                {
                    "sent": "With what you'd expect.",
                    "label": 0
                },
                {
                    "sent": "So in particular what you see is the red is the simple only model performs better right for given amount of data for a fixed amount of data, the simple model performs better than the normal model.",
                    "label": 0
                },
                {
                    "sent": "That's sort of what you'd expect to see in a particularly see.",
                    "label": 0
                },
                {
                    "sent": "It grows much quicker here.",
                    "label": 0
                },
                {
                    "sent": "This tends to taper off.",
                    "label": 0
                },
                {
                    "sent": "So that's encouraging that this is doing at least something sort of reasonable.",
                    "label": 0
                },
                {
                    "sent": "Both of them do tend to get better overtime, so as you add more sentences, results get better.",
                    "label": 0
                },
                {
                    "sent": "That's intuitive, so.",
                    "label": 0
                },
                {
                    "sent": "The few interesting things to note, so this is the best you can do right here.",
                    "label": 0
                },
                {
                    "sent": "It's a member.",
                    "label": 0
                },
                {
                    "sent": "It's about 1:30.",
                    "label": 0
                },
                {
                    "sent": "Something perplexity with all of the basically simple English data available.",
                    "label": 0
                },
                {
                    "sent": "What you see here is as you start to add additional normal sentences, right non simplified sentences.",
                    "label": 0
                },
                {
                    "sent": "We still get an increase in perplexity.",
                    "label": 0
                },
                {
                    "sent": "So this isn't, you know, very nice thing to see.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you look at the tail end it takes a lot more sentence.",
                    "label": 0
                },
                {
                    "sent": "You can see that starts to taper off very quickly, but in the end we get a 23% improvement in perplexity by adding that normal data.",
                    "label": 1
                },
                {
                    "sent": "One of the other interesting things to note.",
                    "label": 0
                },
                {
                    "sent": "If you it's not as obvious, but this even if you add enough normal data, you can actually get perplexity results that are better than just using simple language.",
                    "label": 0
                },
                {
                    "sent": "So this is, you know again because it's still English.",
                    "label": 0
                },
                {
                    "sent": "You had enough data you can compensate for the fact that it's slightly out of domain.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we also wanted to see.",
                    "label": 0
                },
                {
                    "sent": "What happens is you have different amounts of simplified data that you start with.",
                    "label": 0
                },
                {
                    "sent": "Right, so in this domain you know, presumably you want to use all of the data you have.",
                    "label": 0
                },
                {
                    "sent": "Maybe another domains, either in simplification in a specific domain, say medicine or something like that.",
                    "label": 0
                },
                {
                    "sent": "Or you know in in a similar monolingual translation task we wanted to see you know what's the impact.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that sort of interesting is to see is if you have less simple data.",
                    "label": 0
                },
                {
                    "sent": "The actual impact of adding the out of domain the normal data is even even more substantial.",
                    "label": 1
                },
                {
                    "sent": "So here is just 50,000 sentences starting.",
                    "label": 0
                },
                {
                    "sent": "You can see a fairly large improvement there by adding the normal data.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So far, all we've done basically to combine the symbol in the normal data is just concatenate them together.",
                    "label": 0
                },
                {
                    "sent": "You can think of is aggregating accounts, or just concatenating 2 corporate together.",
                    "label": 0
                },
                {
                    "sent": "There are much much smarter ways of actually taking two language models that are, you know, I say in different domains, so simple versus normal different domains and combining them in a more reasonable way.",
                    "label": 0
                },
                {
                    "sent": "So one of the simplest things that people do is an interpolated linearly interpolated language model.",
                    "label": 1
                },
                {
                    "sent": "So basically you have the one model, another model, and you interpolate their probabilities together into a combined model.",
                    "label": 0
                },
                {
                    "sent": "Now there are many other more advanced ways for doing language model adaptation, but we just wanted to again push it a little more and see if this could be useful.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This, and again for perplexity.",
                    "label": 0
                },
                {
                    "sent": "What we have here perplexity, and here on this side, is the simple only model.",
                    "label": 0
                },
                {
                    "sent": "So this is actually again, you see around 1:30 is the performance of the simple only there.",
                    "label": 0
                },
                {
                    "sent": "It makes it more obvious than normal model is always on the other side, and as you start to mix the two models together.",
                    "label": 0
                },
                {
                    "sent": "You start to see the improvement an at about 5050 right match between 5050 blending of the normal in a simple model you get a slight improvement only over just concatenating them, so there are, you know adaptation helps a little bit there.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to summarize, task one in the end, if you you know if you use a linearly interpreted model you get a 24% increase in perplexity.",
                    "label": 0
                },
                {
                    "sent": "Improvement in perplexity over just using all of the simple data that's currently available, so that's a nice sort of result.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was an intrinsic evaluation, right language model, or perplexities intrinsic model.",
                    "label": 0
                },
                {
                    "sent": "We also wanted to do an XX trinsic model.",
                    "label": 0
                },
                {
                    "sent": "How does it actually affect simplification performance?",
                    "label": 0
                },
                {
                    "sent": "So right now where the systems and evaluation are for tax implication, there really aren't standardized ways of evaluating text application systems.",
                    "label": 0
                },
                {
                    "sent": "Are there really only a handful of systems out there?",
                    "label": 0
                },
                {
                    "sent": "So instead we looked at a different extrinsic task and this is just the sum eval 2012 lexical simplification task.",
                    "label": 0
                },
                {
                    "sent": "So the basic ideas you're given a sentence and a particular word in that sentence.",
                    "label": 0
                },
                {
                    "sent": "And they used the data, so these are phrasal substitutions for that word.",
                    "label": 0
                },
                {
                    "sent": "They all fit contextually perfectly fine, and then they asked a bunch of humans to actually rank those based on simplicity.",
                    "label": 0
                },
                {
                    "sent": "So your task is, given these candidates to learner anchor that ranks the words the candidate substitutions by simplicity.",
                    "label": 0
                },
                {
                    "sent": "So this was the task that we decided to look at for an extrinsic task.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the way we post or use the language model to do this ranking was sort of straightforward way.",
                    "label": 0
                },
                {
                    "sent": "You basically consider all the substitutions in the context of the sentence, evaluate them with the language model, and then you rank them based on the language model score.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For evaluation we use the same evaluation metric that they use for the semi valve 2012 tasks, and it roughly is how how close does the ranking here correlate?",
                    "label": 0
                },
                {
                    "sent": "The system ranking correlate to the human ranking the way they do that is basically a pairwise comparison of the rank and then you gotta basically Kappa score for that.",
                    "label": 1
                },
                {
                    "sent": "So there between zero and one higher being better.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So same graph that we looked at before for perplexity.",
                    "label": 0
                },
                {
                    "sent": "Now for the lexical simplification task, here we have the cap rank score better, higher is better.",
                    "label": 0
                },
                {
                    "sent": "And again we have the simple only model.",
                    "label": 0
                },
                {
                    "sent": "The normal only model and then the normal where we appended the data.",
                    "label": 0
                },
                {
                    "sent": "So again, just like we saw before, you tend to see this nice sort of increase as you add more data, the scores get better.",
                    "label": 0
                },
                {
                    "sent": "That's encouraging and you also see this simple performs better than just the normal only model.",
                    "label": 0
                },
                {
                    "sent": "One difference though, with the versus.",
                    "label": 0
                },
                {
                    "sent": "The perplexity data you notice here, this is about .345, something like that.",
                    "label": 0
                },
                {
                    "sent": "As we add more normal data, the results actually don't get much better.",
                    "label": 0
                },
                {
                    "sent": "In fact, they kind of swivel around, but they really don't get any better, so so far.",
                    "label": 0
                },
                {
                    "sent": "Doing adding extra data didn't actually help.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now just like before, if we look at limiting the amount of simple data that we have.",
                    "label": 0
                },
                {
                    "sent": "And then adding more normal sentences.",
                    "label": 0
                },
                {
                    "sent": "We do actually start to see the increase.",
                    "label": 0
                },
                {
                    "sent": "So here at the top is that same one we saw before starting with all the simple data, the Orange one you see we don't get an increase, but so if we start with something like only 100,000 sentences, so maybe another domains if you are limited.",
                    "label": 0
                },
                {
                    "sent": "Then adding normal data does help, and you know even up here when you have something like 250,000 sentences, you still do get an increase in adding that, so that's a nice benefit.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Language model adaptation.",
                    "label": 0
                },
                {
                    "sent": "So we did the linear linearly interpolated adaptation model, just like we did for the perplexity.",
                    "label": 0
                },
                {
                    "sent": "Here what I'm showing is as we increase the number of data.",
                    "label": 0
                },
                {
                    "sent": "So here when we do adaptation, we can get a benefit so we can get an improvement.",
                    "label": 0
                },
                {
                    "sent": "So this is basically where the simple only model and basically where we were before.",
                    "label": 0
                },
                {
                    "sent": "If we just took a simple only model and concatenated all the normal data, so around 35 if we use language model adaptation to blend the simple data in the normal data so that those two models we again can see a nontrivial improvement in the performance of the lexical ranking task.",
                    "label": 0
                },
                {
                    "sent": "And actually we didn't design this for the semi Val task.",
                    "label": 0
                },
                {
                    "sent": "But with this language model adaptation system performs relatively well.",
                    "label": 1
                },
                {
                    "sent": "I remember it's like ties for 3rd or 4th out of the result, so this is actually.",
                    "label": 0
                },
                {
                    "sent": "Very interesting result.",
                    "label": 0
                },
                {
                    "sent": "So in the end we end up with again about a 23% improvement over the simple only model on this lexical simplification task.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it seems at least on a couple of different tasks.",
                    "label": 0
                },
                {
                    "sent": "Right, that adding normal data does help.",
                    "label": 1
                },
                {
                    "sent": "We can utilize that to improve our performance, so the question is why does it help?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure this is still something we're thinking about, but my best guess is that, well, you have it's more data, so you have more engrams.",
                    "label": 0
                },
                {
                    "sent": "So to slightly to investigate this a little bit, we looked at basically how many more engrams do we have in the normal data than in the simple data.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in particular, we looked at the test data that we use for the perplexity, and the lexical simplification task, and we looked at basically the coverage of the engrams.",
                    "label": 1
                },
                {
                    "sent": "And so if you look at the unigrams bigrams and trigrams across the board.",
                    "label": 1
                },
                {
                    "sent": "The normal data has better coverage.",
                    "label": 0
                },
                {
                    "sent": "It has more of those engrams now it's out of domain, but it does have better coverage.",
                    "label": 0
                },
                {
                    "sent": "So when you blend the two together.",
                    "label": 0
                },
                {
                    "sent": "You can get better coverage overall if you do something intelligent with that.",
                    "label": 0
                },
                {
                    "sent": "So to put this in context, for example, in this one the 9.4% increase.",
                    "label": 0
                },
                {
                    "sent": "This is actually a decrease in about half of the out of vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Words that you end up with if you just use the simple data.",
                    "label": 0
                },
                {
                    "sent": "So I mean this is a non trivial improvement for the for your language model and the other benefit this is just you know grams you haven't seen even for sort of rare engrams that maybe it just means seen once or twice adding this normal data may help you estimate their probabilities even better.",
                    "label": 0
                },
                {
                    "sent": "So this is our conjecture.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One other thing to note is the application did matter, so we looked at this linearly interpolated model.",
                    "label": 0
                },
                {
                    "sent": "If you look at the perplexity task and this partially, I think explains why you can just add that the data together.",
                    "label": 0
                },
                {
                    "sent": "The best Lambda.",
                    "label": 0
                },
                {
                    "sent": "Is about .5.",
                    "label": 0
                },
                {
                    "sent": "So 5050 blending of the data?",
                    "label": 0
                },
                {
                    "sent": "Interestingly, on the lexical simplification task.",
                    "label": 1
                },
                {
                    "sent": "The best blending of the models was basically to use the simplified data for almost all the cases, right?",
                    "label": 1
                },
                {
                    "sent": "So you have a very strong waiting here, so only when the simplification the simplified model doesn't have a strong.",
                    "label": 0
                },
                {
                    "sent": "Vote or strong indicator.",
                    "label": 0
                },
                {
                    "sent": "Strong difference between the choices.",
                    "label": 0
                },
                {
                    "sent": "Do you then rely on the normal model?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to wrap up, so First off, I mean I think given these couple of experiments, there's still some more.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk about a minute, but I think there is using unsimplified data for training a simple English language model is useful.",
                    "label": 1
                },
                {
                    "sent": "You get improvements on these two tasks and non trivial improvements by about 2324%.",
                    "label": 0
                },
                {
                    "sent": "Right, so on both intrinsic and extrinsic extrinsic evaluation in both cases domain adaptation helped right so we didn't do anything particularly found on this front, but they did help.",
                    "label": 0
                },
                {
                    "sent": "But caveat is be careful if you know depending on the domain we did see large variations in the actual adaptation parameters.",
                    "label": 0
                },
                {
                    "sent": "We did generate some data that's available for this, so there's a new aligned corpus that we have available, plus all of the document aligned data.",
                    "label": 0
                },
                {
                    "sent": "So people are curious, you could just do a search for me, or there's the.",
                    "label": 0
                },
                {
                    "sent": "The link in links also in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To finish up, there are a lot of sort of interesting questions.",
                    "label": 0
                },
                {
                    "sent": "I think that I would still like to investigate, or maybe other people would like to investigate.",
                    "label": 0
                },
                {
                    "sent": "So first up again, what we looked at here was the document aligned data set so it's only 60,000 articles.",
                    "label": 0
                },
                {
                    "sent": "And was I think 2.5 million sentences in normal English.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how much more data can we utilize?",
                    "label": 1
                },
                {
                    "sent": "So can we utilized say?",
                    "label": 0
                },
                {
                    "sent": "Corpus or the giga word corpus.",
                    "label": 0
                },
                {
                    "sent": "And you know how does that affect our performance?",
                    "label": 0
                },
                {
                    "sent": "How does this source so these were basically in the same domain that they were article aligned?",
                    "label": 1
                },
                {
                    "sent": "So how does the source effect the performance on the simple English?",
                    "label": 0
                },
                {
                    "sent": "And then what happens on sort of more realistic?",
                    "label": 0
                },
                {
                    "sent": "So this is was half realistic simplification task.",
                    "label": 0
                },
                {
                    "sent": "But what happens when we actually try to use a safer full sentence simplification systems or other lexical simplification tasks?",
                    "label": 0
                },
                {
                    "sent": "And then obviously investigating better language model adaptation techniques?",
                    "label": 0
                },
                {
                    "sent": "So that's all I have.",
                    "label": 0
                },
                {
                    "sent": "In your evaluation you will look at a particular type of lexical simplification, obviously related to N gram models in quite a simple way, and I wonder.",
                    "label": 0
                },
                {
                    "sent": "If you think that made a big difference somehow.",
                    "label": 0
                },
                {
                    "sent": "And it might be different with other kinds of things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, it would be interesting to try it on other types of models.",
                    "label": 0
                },
                {
                    "sent": "So in the same eval task, probably 2/3 of the approaches that people use did have a language model as a feature as a feature, and that tended to be the strongest indicator.",
                    "label": 0
                },
                {
                    "sent": "But there are other features that will actually go into it.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that were sort of interesting looking at further is what happens is you start to incorporate this language model and sort of more sophisticated either lexical simplification or sentence level tasks.",
                    "label": 0
                },
                {
                    "sent": "Hi there, I'm at Schadler University of Manchester.",
                    "label": 0
                },
                {
                    "sent": "It's really encouraging to see what you're doing and how about you to say thank you for.",
                    "label": 0
                },
                {
                    "sent": "Yeah presenting great.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if you'd looked at controlling which of the English Wikipedia articles and sentence is are actually going into your additive model, like controlling for the simplest sentence is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we didn't yet, so I did some.",
                    "label": 0
                },
                {
                    "sent": "We did some experiments with, basically randomizing which ones were getting and the results seem to be the same, but it would be interesting.",
                    "label": 0
                },
                {
                    "sent": "Like you said, to investigate more how content or other things like that, so we didn't.",
                    "label": 0
                },
                {
                    "sent": "Look at it yet, but I think that is sort of interesting.",
                    "label": 0
                },
                {
                    "sent": "Would be interesting to see, thanks.",
                    "label": 0
                },
                {
                    "sent": "Hi, thank you for the talk.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "I think that certain amount of simple sentence is simplified.",
                    "label": 0
                },
                {
                    "sent": "Sentences are exactly the same as the unsupervised sentences, right?",
                    "label": 0
                },
                {
                    "sent": "Have you end in the Wikipedia in the simple English Wikipedia in the guidelines, they write that there is a certain corpus from which you should try to choose the words.",
                    "label": 0
                },
                {
                    "sent": "If you want to write in simple way, so have you tried to kind of filter out the unsimplified data?",
                    "label": 0
                },
                {
                    "sent": "Based on this vocabulary and maybe like this to extend the out of domain.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, that's a good idea.",
                    "label": 0
                },
                {
                    "sent": "So one thing we did try is.",
                    "label": 0
                },
                {
                    "sent": "Some people other people have done this but basically classifying sentence is as simple and not simple because it is kind of hard.",
                    "label": 0
                },
                {
                    "sent": "Like you said, there is some overlap so you can't try to figure out which ones are simple or not.",
                    "label": 0
                },
                {
                    "sent": "So we actually tried to learn a classifier based on a handful of features to predict whether a sentence was simple or not.",
                    "label": 0
                },
                {
                    "sent": "And you accuracy is reasonable.",
                    "label": 0
                },
                {
                    "sent": "I think around 80% and then when we tried to use those the results weren't quite as strong as you hope to see, but that is I think something to investigate would be to basically throw out the really complex ones and try and keep it.",
                    "label": 0
                },
                {
                    "sent": "But we didn't see a lot of benefit but.",
                    "label": 0
                },
                {
                    "sent": "Sort of a preliminary experiment.",
                    "label": 0
                },
                {
                    "sent": "I think we better leave it there 'cause we need time.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}