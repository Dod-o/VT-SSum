{
    "id": "2ucy7efexdle5apavmswez2hnxhvnlr5",
    "title": "Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization",
    "info": {
        "author": [
            "Ohad Samir, Microsoft Research New England, Microsoft Research"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Optimization Methods->Stochastic Optimization"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_shamir_convex/",
    "segmentation": [
        [
            "So."
        ],
        [
            "What I'm going to talk about is stochastic convex optimization, which probably most of you already know.",
            "We want to optimize some convex function over some convex domain, and we assume that we have access to unbiased estimates of the gradients or sub gradients in case the function is not differentiable and a very popular and very simple method to do it to stochastic gradient descent, where each time unbiased estimate of the gradient, say by sampling from our training set, do a gradient step project if needed.",
            "And under pretty minimal assumptions, if you take the stepsize appropriately and you take the average of the predictions, you have convergence at a rate of 1 / sqrt T."
        ],
        [
            "Now it might talk.",
            "I'm going to focus on the strongly convex case, so I'm assuming that F is strongly convex.",
            "Again, I assume most of you familiar with these things, basically that we can a lower bound or function everywhere by a parabola.",
            "This is the kind of thing that you meet when you say do regularize learning, support vector machines and so on."
        ],
        [
            "No um.",
            "If you want to analyze the convergence rate in this case an you don't want to make any additional assumptions such as smoothness.",
            "Kind of currently known result is based on the work which came from the online learning community, so we know that if F is Lambda strongly convex and you assume that the gradient estimates are bounded, then you by taking the step size appropriately and the average the convergence rate is log T / A Lambda T and the proof goes through a setting which is actually harder than stochastic learning, it's an.",
            "Online learning setting where the gradients are assumed to be provided by some adversary."
        ],
        [
            "Now in two recent papers in 2010, you did scale Nesterov and Husband and Claire showed independently that actually this is not the best thing you can do in this setting, and they propose a virtually the same algorithm where they get an optimal one over Lambda T rate, where the log factor is removed.",
            "So of course you can argue that they just logarithmic factor.",
            "It's not that important, but you know maybe this difference is real so.",
            "Maybe these new this new algorithm actually performs better than the standard gradient descent that everyone uses, and in that case, maybe we should drop the stochastic gradient descent and move to this new algorithm.",
            "And this is a motivating question for our work.",
            "Are these new algorithms really better than stochastic gradient?"
        ],
        [
            "Sent.",
            "Now of course, they should emphasize that the issue of the convergence of stochastic gradient descent is extremely well studied for over 50 years, and also more recent work works address the non aseptic regime where really interested in some sort of finite sample a bound.",
            "However, at least as far as I know, almost these results make some sort of smoothness assumption on F, which in many cases it doesn't really hold.",
            "So for instance, if we want to solve support vector machines by.",
            "Gradient descent, so support vector machines use the hinge loss which is not smooth loss function.",
            "So overall we might be optimizing announcement function here, so these kind of kind of standard analysis that we have doesn't all here."
        ],
        [
            "OK, so in this work we want to understand what is the convergence rate of stochastic gradient descent for strongly convex problems, but possibly non smooth ones.",
            "And we have the following findings."
        ],
        [
            "So the first one is a very small one.",
            "It's actually mostly known.",
            "We basically stated to contrast their other findings.",
            "So if we do assume smoothness then stochastic gradient descent achieves the optimal 1 / T rate without a log factor.",
            "And this happens both with and without averaging.",
            "So without averaging it's a classical result.",
            "So bend in his lecture talked about works of numerous key in the 80s which showed it, but actually.",
            "There are even works in the 50s and I don't know, maybe even earlier, which implies this and our very small contribution is just establishing that doing averaging also maintains the same kind of rate."
        ],
        [
            "And now we move to the more interesting case of the nonsmooth functions.",
            "So we showed that if it is non smooth, there actually might be cases where doing SVG with averaging really give you a log T / 2 rates, so this logarithmic factor is not some artifact of the analysis, or doing it through an online learning route, it's."
        ],
        [
            "Really there.",
            "On the flip side, we show that actually if you make a very small modification to the averaging step, you get back the 1 / T rate for stochastic gradient descent, and you don't need any new algorithm like those proposed."
        ],
        [
            "Recently and we also did some experiments which accorded with our theory."
        ],
        [
            "And one other nice side result is that it, by avoiding this sort of online learning analysis, we get a somewhat more general result.",
            "So the original online analysis assumed the very specific step size of one over Lambda T, and with our analysis you can also see what happens if you take another step."
        ],
        [
            "Sizes.",
            "OK, So what do I mean by smoothness?",
            "So we're going to make a very actually.",
            "Turns out you need the very minimal smoothness assumption.",
            "So just need smoothness with respect to the optimum W star.",
            "Basically that you can upper bound in your objective function by."
        ],
        [
            "Abla and as I said, there is this very classical analysis that if you assume both smoothness and strong convexity, and you take the step size to scale like one over Lambda T and not too small otherwise things breakdown, then you get basically the optimal 1 / T rate with the smoothness and strong parameter.",
            "A parameters inside."
        ],
        [
            "And the proof is very, very simple.",
            "I'm just showing you this because some elements of it would be important later on.",
            "So by strong convexity we get this sort of recursive inequality where we upper bound the expected distance from the optimum at iteration T + 1.",
            "By the distance expected distance squared.",
            "Iteration tee times in fact are smaller than one and some additional stuff.",
            "Here we then we solve the recursion and get the expected distance squared.",
            "Scales like 1 / T so up to now we didn't assume any smoothness.",
            "We use smoothness now to argue that this thing implies that you're expected Suboptimality would also scale like 1 / T. And as I said earlier, we can do a variant of this analysis, showing that for the for the average predictor you also get a 1 / T rate.",
            "Actually.",
            "Interestingly, you have here a bit more robustness in terms of the parameters.",
            "So in the without averaging you get this C over Lambda term.",
            "Here at square sqrt C + 1 over Lambda.",
            "So it's a bit more robust in terms of the constants, but overall."
        ],
        [
            "Rate is the same.",
            "OK, so now we turn to the case of the non smooth F so I just said earlier with nonsmooth you can still show that your business squared from the optimum would be 1 / T, but now it's no longer enough to give you 1 / 2 suboptimality.",
            "On the other hand, we can use these online learning techniques to show that the suboptimality is log T / T. And what I'm going to show you is that this log T is real.",
            "It's not just some artifact of the analysis."
        ],
        [
            "So to show this, I'm going to start with a very, very, very simple problem an so suppose that your domain is.",
            "It's a 1 dimensional problem on 01, and the function is this extremely simple quadratic function which is non smooth with respect to the optimal W star, right?",
            "Because you can't upper bound this function at the optimum by parabola.",
            "And suppose that your gradient estimates are going to be the actual gradient plus some.",
            "Uniform nodes noise, and in this case it's quite straightforward to show that the expected value of the WT would be at least 1 / T. It's basically because you're taking steps 1 / T, and you can't go beyond zero, so you'll be most of the time in the 1 / T regime.",
            "And now if you do averaging so average of T = 1, two TF 1 / T You get a log T term.",
            "So this is a very simple example, but it's not really satisfying.",
            "It's not satisfying because we crucially use the fact that the optimum W star is at the edge of the domain, and usually when we do strongly convex optimization, the optimum lies in the domain.",
            "Or maybe we don't even constrain the domain at all, so we want something a bit more realistic."
        ],
        [
            "So here is a slightly more just a slightly more involved example, so will still stick with the 1 dimensional setting, but this can be easily extended to Rd.",
            "And the function looks like the graph here, so it looks like the original example on for positive values affects, but for negative values we are going to make it to have this large negative slope, so it's strongly convex and non smooth.",
            "That's important and we do the same sort of fit stochastic model and the idea here is because the gradient for negative values is so large then this thing acts effectively as some sort of barrier so.",
            "The WS as you run the stochastic gradient descent can't really push into the negative side and its behavior surf reduces to what we've seen in the previous example where we had an explicit constraint.",
            "And again, doing the same kind of analysis, you get the log T term.",
            "Showing that."
        ],
        [
            "It's real.",
            "OK, so we've seen that log T / T is really inevitable and from the examples I showed you, it's quite evident that the problem is the averaging steps effectively average overall predictors an, so it would have been very nice to prove something for the last predictor.",
            "Unfortunately, I'm not aware of any good analysis of the last predictor in the non smooth case, so reasonable solution is to do something in between so."
        ],
        [
            "What you can do is something which.",
            "I believe people have already considered in past work that you instead of taking the average over everything, you just take an average over some suffix of the predictors so it can be like the last 1/2 of predictors 1/3.",
            "For some constants you're just going to take the last Alpha T predictors and average on that, and for this predictor using what we call Alpha suffix averaging, you can show that you get a 1 / T A rate.",
            "Again, if the step size is not too small and the Alpha is strictly between a zero and one an, I won't go into the proof details, but it's some sort of combination of the way that you prove these online regret bounds and the stochastic convergence guarantees that we have for the expected distance squared from the optimum."
        ],
        [
            "And finally we did a few experiments.",
            "So this is just an artificial leg day to the top.",
            "One is for a smooth and strongly convex function, and the bottom is for non smooth one.",
            "The blue line is for a stochastic gradient descent with averaging over all predictors.",
            "A red line is on just the last 1/2 of the predictors.",
            "Green line is on the last one and for comparison we also ran this algorithm proposed in 2010 called EPOC a gradient descent.",
            "So you can end the X axis is log of T, so we're taking logarithmic scaling and the Y axis is the suboptimality multiplied by T because we're interested in these terms which other than the 1 / T. So in the smooth case, we see that all of them are more or less the same.",
            "The SGD with averaging is slightly worse in terms of constants.",
            "For the non smooth case, like in the examples we've seen earlier, then the stochastic gradient with averaging has this positive slope, which in our scaling means that you really have this log T term, yes?",
            "In which area is it measured with respect to clear it spread to WTI or the average or the is what it is it was ever for that particular algorithm, so it's at each iteration with respect to the averaging up to that point.",
            "The average is defined differently in each algorithm, right?",
            "Yes, so that's the difference between the blue, red and green lines.",
            "Whether you take all 1/2 or just the last.",
            "An actually in this case, the Epoc GD algorithm was somewhat worse, so it still had a 1 / T rate, but somewhat worse than the SGD, with suffix averaging, and that's one at least in terms of the constant."
        ],
        [
            "So these are some experiments on real data.",
            "We also did a couple of others an.",
            "And in all the cases that we consider, then indeed the success of gradient descent with full averaging was significantly worse than the other algorithms.",
            "And doing Alpha suffix averaging or even just taking the last predictor was fully competitive with the new Epoc gradient descent algorithm for all the datasets we considered."
        ],
        [
            "OK, so just to conclude, so in the smooth cases, stochastic gradient descent performs optimally.",
            "That's not really new, a result.",
            "We've mostly focused on the non smooth case, showing that in this case doing just simple averaging can really hurt you.",
            "It can really lead to worse rate and it's an artifact of the algorithm, not of the analysis.",
            "On the flip side, you can easily fix it just by taking an averaging over some.",
            "Constant proportion for predictors, the last one, and we don't need some new algorithm to achieve it or experiments accord with theory and there are still a few open questions, so one very nagging one is experimentally.",
            "We've seen that the last predictor actually performed as well as doing the suffix to averaging, and this is something that most people actually do in practice, but we don't know how to show that this achieves the optimal thing announcement case.",
            "Just just W capital T. Yes, there's also the issue of our results are in expectation and you might wonder whether you can get a high probability bound.",
            "So for the log two over to, you can get a high probability and it's not really trivial doing some standard martingale argument wouldn't work, and currently you need something different for it, so that's it.",
            "Thank you very much.",
            "Could you instead do the common thing that people also do like oh point 9 of the previous iteration and open one of the new IT written?",
            "I mean, you can compute it online once you decide they like it, iterate T / 2 years start to average, then you can do it online.",
            "It's not a problem.",
            "Additional costs with the factor of double how many iterations you are going to make right, but assuming that you know what capital T is going to be, then, then yeah, I agree that if you want to do it online then.",
            "Cold as well.",
            "The rates hold for the case where you do this averaging of enormous called.",
            "Actually you take take a next combinations.",
            "Yeah, comics emanation of the previous estimates 4.9 previous SPN plus open.",
            "One knew phased array.",
            "Moving average.",
            "Then could be in case you need to check the analysis.",
            "Before the last predicted, the idea is that you don't want to depend too much on the predictors at the beginning because they are the ones that really hurt you.",
            "I mean, that's equivalent to just taking a geometric decreasing weight sequence, right?",
            "Which you don't have to store anything for.",
            "And maybe yeah, that might work as well.",
            "Is it possible that the law goes away if you are strong convex at the optimal solution mean that you can get that upper bound at the officers.",
            "So in the examples we showed, you are strongly convex everywhere, so I took.",
            "So if you're smoother, the optimal solution that's the smooth case that I discussed.",
            "You just need smoothness at the optimum.",
            "Yes, because you just need this single step that.",
            "Yeah, like if you're doing support vector machines with hinge loss, it can be a bit of a risky assumption.",
            "Look, thanks seems to come from 1 sided convenience.",
            "We have very different gradients or something stopping the gating.",
            "Right?",
            "I'm not sure if you had some condition that you have a disk.",
            "Someone equal gradient, so your step size you know it was late and then you get along together.",
            "So it might be possible, but this sort of inequality between the different size is really what characterizes non smoothness.",
            "So like in a certain direction it can be like very sharp gradient in other directions it's very much a smoother.",
            "So maybe we would come up.",
            "You can come up with some condition but I would suspect it would be some strong.",
            "Condition somehow implying that it's not really nonsmooth in some sense.",
            "Vision.",
            "Then in that in this case is taking the averaging seems to be dangerous, yeah?",
            "Because you're not really hovering around your optimum, you're really, as you say, more to one side, and yeah, and averaging won't help you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'm going to talk about is stochastic convex optimization, which probably most of you already know.",
                    "label": 1
                },
                {
                    "sent": "We want to optimize some convex function over some convex domain, and we assume that we have access to unbiased estimates of the gradients or sub gradients in case the function is not differentiable and a very popular and very simple method to do it to stochastic gradient descent, where each time unbiased estimate of the gradient, say by sampling from our training set, do a gradient step project if needed.",
                    "label": 1
                },
                {
                    "sent": "And under pretty minimal assumptions, if you take the stepsize appropriately and you take the average of the predictions, you have convergence at a rate of 1 / sqrt T.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now it might talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to focus on the strongly convex case, so I'm assuming that F is strongly convex.",
                    "label": 1
                },
                {
                    "sent": "Again, I assume most of you familiar with these things, basically that we can a lower bound or function everywhere by a parabola.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of thing that you meet when you say do regularize learning, support vector machines and so on.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No um.",
                    "label": 0
                },
                {
                    "sent": "If you want to analyze the convergence rate in this case an you don't want to make any additional assumptions such as smoothness.",
                    "label": 0
                },
                {
                    "sent": "Kind of currently known result is based on the work which came from the online learning community, so we know that if F is Lambda strongly convex and you assume that the gradient estimates are bounded, then you by taking the step size appropriately and the average the convergence rate is log T / A Lambda T and the proof goes through a setting which is actually harder than stochastic learning, it's an.",
                    "label": 0
                },
                {
                    "sent": "Online learning setting where the gradients are assumed to be provided by some adversary.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now in two recent papers in 2010, you did scale Nesterov and Husband and Claire showed independently that actually this is not the best thing you can do in this setting, and they propose a virtually the same algorithm where they get an optimal one over Lambda T rate, where the log factor is removed.",
                    "label": 0
                },
                {
                    "sent": "So of course you can argue that they just logarithmic factor.",
                    "label": 0
                },
                {
                    "sent": "It's not that important, but you know maybe this difference is real so.",
                    "label": 0
                },
                {
                    "sent": "Maybe these new this new algorithm actually performs better than the standard gradient descent that everyone uses, and in that case, maybe we should drop the stochastic gradient descent and move to this new algorithm.",
                    "label": 0
                },
                {
                    "sent": "And this is a motivating question for our work.",
                    "label": 0
                },
                {
                    "sent": "Are these new algorithms really better than stochastic gradient?",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sent.",
                    "label": 0
                },
                {
                    "sent": "Now of course, they should emphasize that the issue of the convergence of stochastic gradient descent is extremely well studied for over 50 years, and also more recent work works address the non aseptic regime where really interested in some sort of finite sample a bound.",
                    "label": 1
                },
                {
                    "sent": "However, at least as far as I know, almost these results make some sort of smoothness assumption on F, which in many cases it doesn't really hold.",
                    "label": 1
                },
                {
                    "sent": "So for instance, if we want to solve support vector machines by.",
                    "label": 0
                },
                {
                    "sent": "Gradient descent, so support vector machines use the hinge loss which is not smooth loss function.",
                    "label": 0
                },
                {
                    "sent": "So overall we might be optimizing announcement function here, so these kind of kind of standard analysis that we have doesn't all here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in this work we want to understand what is the convergence rate of stochastic gradient descent for strongly convex problems, but possibly non smooth ones.",
                    "label": 0
                },
                {
                    "sent": "And we have the following findings.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first one is a very small one.",
                    "label": 0
                },
                {
                    "sent": "It's actually mostly known.",
                    "label": 0
                },
                {
                    "sent": "We basically stated to contrast their other findings.",
                    "label": 0
                },
                {
                    "sent": "So if we do assume smoothness then stochastic gradient descent achieves the optimal 1 / T rate without a log factor.",
                    "label": 0
                },
                {
                    "sent": "And this happens both with and without averaging.",
                    "label": 1
                },
                {
                    "sent": "So without averaging it's a classical result.",
                    "label": 0
                },
                {
                    "sent": "So bend in his lecture talked about works of numerous key in the 80s which showed it, but actually.",
                    "label": 0
                },
                {
                    "sent": "There are even works in the 50s and I don't know, maybe even earlier, which implies this and our very small contribution is just establishing that doing averaging also maintains the same kind of rate.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we move to the more interesting case of the nonsmooth functions.",
                    "label": 0
                },
                {
                    "sent": "So we showed that if it is non smooth, there actually might be cases where doing SVG with averaging really give you a log T / 2 rates, so this logarithmic factor is not some artifact of the analysis, or doing it through an online learning route, it's.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really there.",
                    "label": 0
                },
                {
                    "sent": "On the flip side, we show that actually if you make a very small modification to the averaging step, you get back the 1 / T rate for stochastic gradient descent, and you don't need any new algorithm like those proposed.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recently and we also did some experiments which accorded with our theory.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one other nice side result is that it, by avoiding this sort of online learning analysis, we get a somewhat more general result.",
                    "label": 0
                },
                {
                    "sent": "So the original online analysis assumed the very specific step size of one over Lambda T, and with our analysis you can also see what happens if you take another step.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sizes.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do I mean by smoothness?",
                    "label": 0
                },
                {
                    "sent": "So we're going to make a very actually.",
                    "label": 0
                },
                {
                    "sent": "Turns out you need the very minimal smoothness assumption.",
                    "label": 0
                },
                {
                    "sent": "So just need smoothness with respect to the optimum W star.",
                    "label": 0
                },
                {
                    "sent": "Basically that you can upper bound in your objective function by.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Abla and as I said, there is this very classical analysis that if you assume both smoothness and strong convexity, and you take the step size to scale like one over Lambda T and not too small otherwise things breakdown, then you get basically the optimal 1 / T rate with the smoothness and strong parameter.",
                    "label": 0
                },
                {
                    "sent": "A parameters inside.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the proof is very, very simple.",
                    "label": 0
                },
                {
                    "sent": "I'm just showing you this because some elements of it would be important later on.",
                    "label": 0
                },
                {
                    "sent": "So by strong convexity we get this sort of recursive inequality where we upper bound the expected distance from the optimum at iteration T + 1.",
                    "label": 1
                },
                {
                    "sent": "By the distance expected distance squared.",
                    "label": 0
                },
                {
                    "sent": "Iteration tee times in fact are smaller than one and some additional stuff.",
                    "label": 0
                },
                {
                    "sent": "Here we then we solve the recursion and get the expected distance squared.",
                    "label": 0
                },
                {
                    "sent": "Scales like 1 / T so up to now we didn't assume any smoothness.",
                    "label": 1
                },
                {
                    "sent": "We use smoothness now to argue that this thing implies that you're expected Suboptimality would also scale like 1 / T. And as I said earlier, we can do a variant of this analysis, showing that for the for the average predictor you also get a 1 / T rate.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, you have here a bit more robustness in terms of the parameters.",
                    "label": 0
                },
                {
                    "sent": "So in the without averaging you get this C over Lambda term.",
                    "label": 0
                },
                {
                    "sent": "Here at square sqrt C + 1 over Lambda.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit more robust in terms of the constants, but overall.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rate is the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we turn to the case of the non smooth F so I just said earlier with nonsmooth you can still show that your business squared from the optimum would be 1 / T, but now it's no longer enough to give you 1 / 2 suboptimality.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, we can use these online learning techniques to show that the suboptimality is log T / T. And what I'm going to show you is that this log T is real.",
                    "label": 1
                },
                {
                    "sent": "It's not just some artifact of the analysis.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to show this, I'm going to start with a very, very, very simple problem an so suppose that your domain is.",
                    "label": 0
                },
                {
                    "sent": "It's a 1 dimensional problem on 01, and the function is this extremely simple quadratic function which is non smooth with respect to the optimal W star, right?",
                    "label": 0
                },
                {
                    "sent": "Because you can't upper bound this function at the optimum by parabola.",
                    "label": 0
                },
                {
                    "sent": "And suppose that your gradient estimates are going to be the actual gradient plus some.",
                    "label": 0
                },
                {
                    "sent": "Uniform nodes noise, and in this case it's quite straightforward to show that the expected value of the WT would be at least 1 / T. It's basically because you're taking steps 1 / T, and you can't go beyond zero, so you'll be most of the time in the 1 / T regime.",
                    "label": 1
                },
                {
                    "sent": "And now if you do averaging so average of T = 1, two TF 1 / T You get a log T term.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple example, but it's not really satisfying.",
                    "label": 0
                },
                {
                    "sent": "It's not satisfying because we crucially use the fact that the optimum W star is at the edge of the domain, and usually when we do strongly convex optimization, the optimum lies in the domain.",
                    "label": 0
                },
                {
                    "sent": "Or maybe we don't even constrain the domain at all, so we want something a bit more realistic.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a slightly more just a slightly more involved example, so will still stick with the 1 dimensional setting, but this can be easily extended to Rd.",
                    "label": 1
                },
                {
                    "sent": "And the function looks like the graph here, so it looks like the original example on for positive values affects, but for negative values we are going to make it to have this large negative slope, so it's strongly convex and non smooth.",
                    "label": 0
                },
                {
                    "sent": "That's important and we do the same sort of fit stochastic model and the idea here is because the gradient for negative values is so large then this thing acts effectively as some sort of barrier so.",
                    "label": 0
                },
                {
                    "sent": "The WS as you run the stochastic gradient descent can't really push into the negative side and its behavior surf reduces to what we've seen in the previous example where we had an explicit constraint.",
                    "label": 0
                },
                {
                    "sent": "And again, doing the same kind of analysis, you get the log T term.",
                    "label": 0
                },
                {
                    "sent": "Showing that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's real.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've seen that log T / T is really inevitable and from the examples I showed you, it's quite evident that the problem is the averaging steps effectively average overall predictors an, so it would have been very nice to prove something for the last predictor.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I'm not aware of any good analysis of the last predictor in the non smooth case, so reasonable solution is to do something in between so.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you can do is something which.",
                    "label": 0
                },
                {
                    "sent": "I believe people have already considered in past work that you instead of taking the average over everything, you just take an average over some suffix of the predictors so it can be like the last 1/2 of predictors 1/3.",
                    "label": 0
                },
                {
                    "sent": "For some constants you're just going to take the last Alpha T predictors and average on that, and for this predictor using what we call Alpha suffix averaging, you can show that you get a 1 / T A rate.",
                    "label": 0
                },
                {
                    "sent": "Again, if the step size is not too small and the Alpha is strictly between a zero and one an, I won't go into the proof details, but it's some sort of combination of the way that you prove these online regret bounds and the stochastic convergence guarantees that we have for the expected distance squared from the optimum.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally we did a few experiments.",
                    "label": 0
                },
                {
                    "sent": "So this is just an artificial leg day to the top.",
                    "label": 0
                },
                {
                    "sent": "One is for a smooth and strongly convex function, and the bottom is for non smooth one.",
                    "label": 1
                },
                {
                    "sent": "The blue line is for a stochastic gradient descent with averaging over all predictors.",
                    "label": 0
                },
                {
                    "sent": "A red line is on just the last 1/2 of the predictors.",
                    "label": 0
                },
                {
                    "sent": "Green line is on the last one and for comparison we also ran this algorithm proposed in 2010 called EPOC a gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So you can end the X axis is log of T, so we're taking logarithmic scaling and the Y axis is the suboptimality multiplied by T because we're interested in these terms which other than the 1 / T. So in the smooth case, we see that all of them are more or less the same.",
                    "label": 0
                },
                {
                    "sent": "The SGD with averaging is slightly worse in terms of constants.",
                    "label": 0
                },
                {
                    "sent": "For the non smooth case, like in the examples we've seen earlier, then the stochastic gradient with averaging has this positive slope, which in our scaling means that you really have this log T term, yes?",
                    "label": 0
                },
                {
                    "sent": "In which area is it measured with respect to clear it spread to WTI or the average or the is what it is it was ever for that particular algorithm, so it's at each iteration with respect to the averaging up to that point.",
                    "label": 0
                },
                {
                    "sent": "The average is defined differently in each algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, so that's the difference between the blue, red and green lines.",
                    "label": 0
                },
                {
                    "sent": "Whether you take all 1/2 or just the last.",
                    "label": 0
                },
                {
                    "sent": "An actually in this case, the Epoc GD algorithm was somewhat worse, so it still had a 1 / T rate, but somewhat worse than the SGD, with suffix averaging, and that's one at least in terms of the constant.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are some experiments on real data.",
                    "label": 0
                },
                {
                    "sent": "We also did a couple of others an.",
                    "label": 0
                },
                {
                    "sent": "And in all the cases that we consider, then indeed the success of gradient descent with full averaging was significantly worse than the other algorithms.",
                    "label": 0
                },
                {
                    "sent": "And doing Alpha suffix averaging or even just taking the last predictor was fully competitive with the new Epoc gradient descent algorithm for all the datasets we considered.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to conclude, so in the smooth cases, stochastic gradient descent performs optimally.",
                    "label": 1
                },
                {
                    "sent": "That's not really new, a result.",
                    "label": 0
                },
                {
                    "sent": "We've mostly focused on the non smooth case, showing that in this case doing just simple averaging can really hurt you.",
                    "label": 1
                },
                {
                    "sent": "It can really lead to worse rate and it's an artifact of the algorithm, not of the analysis.",
                    "label": 0
                },
                {
                    "sent": "On the flip side, you can easily fix it just by taking an averaging over some.",
                    "label": 1
                },
                {
                    "sent": "Constant proportion for predictors, the last one, and we don't need some new algorithm to achieve it or experiments accord with theory and there are still a few open questions, so one very nagging one is experimentally.",
                    "label": 0
                },
                {
                    "sent": "We've seen that the last predictor actually performed as well as doing the suffix to averaging, and this is something that most people actually do in practice, but we don't know how to show that this achieves the optimal thing announcement case.",
                    "label": 0
                },
                {
                    "sent": "Just just W capital T. Yes, there's also the issue of our results are in expectation and you might wonder whether you can get a high probability bound.",
                    "label": 0
                },
                {
                    "sent": "So for the log two over to, you can get a high probability and it's not really trivial doing some standard martingale argument wouldn't work, and currently you need something different for it, so that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Could you instead do the common thing that people also do like oh point 9 of the previous iteration and open one of the new IT written?",
                    "label": 0
                },
                {
                    "sent": "I mean, you can compute it online once you decide they like it, iterate T / 2 years start to average, then you can do it online.",
                    "label": 0
                },
                {
                    "sent": "It's not a problem.",
                    "label": 0
                },
                {
                    "sent": "Additional costs with the factor of double how many iterations you are going to make right, but assuming that you know what capital T is going to be, then, then yeah, I agree that if you want to do it online then.",
                    "label": 0
                },
                {
                    "sent": "Cold as well.",
                    "label": 0
                },
                {
                    "sent": "The rates hold for the case where you do this averaging of enormous called.",
                    "label": 0
                },
                {
                    "sent": "Actually you take take a next combinations.",
                    "label": 0
                },
                {
                    "sent": "Yeah, comics emanation of the previous estimates 4.9 previous SPN plus open.",
                    "label": 0
                },
                {
                    "sent": "One knew phased array.",
                    "label": 0
                },
                {
                    "sent": "Moving average.",
                    "label": 0
                },
                {
                    "sent": "Then could be in case you need to check the analysis.",
                    "label": 0
                },
                {
                    "sent": "Before the last predicted, the idea is that you don't want to depend too much on the predictors at the beginning because they are the ones that really hurt you.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's equivalent to just taking a geometric decreasing weight sequence, right?",
                    "label": 0
                },
                {
                    "sent": "Which you don't have to store anything for.",
                    "label": 0
                },
                {
                    "sent": "And maybe yeah, that might work as well.",
                    "label": 0
                },
                {
                    "sent": "Is it possible that the law goes away if you are strong convex at the optimal solution mean that you can get that upper bound at the officers.",
                    "label": 0
                },
                {
                    "sent": "So in the examples we showed, you are strongly convex everywhere, so I took.",
                    "label": 0
                },
                {
                    "sent": "So if you're smoother, the optimal solution that's the smooth case that I discussed.",
                    "label": 0
                },
                {
                    "sent": "You just need smoothness at the optimum.",
                    "label": 0
                },
                {
                    "sent": "Yes, because you just need this single step that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, like if you're doing support vector machines with hinge loss, it can be a bit of a risky assumption.",
                    "label": 0
                },
                {
                    "sent": "Look, thanks seems to come from 1 sided convenience.",
                    "label": 0
                },
                {
                    "sent": "We have very different gradients or something stopping the gating.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if you had some condition that you have a disk.",
                    "label": 0
                },
                {
                    "sent": "Someone equal gradient, so your step size you know it was late and then you get along together.",
                    "label": 0
                },
                {
                    "sent": "So it might be possible, but this sort of inequality between the different size is really what characterizes non smoothness.",
                    "label": 0
                },
                {
                    "sent": "So like in a certain direction it can be like very sharp gradient in other directions it's very much a smoother.",
                    "label": 0
                },
                {
                    "sent": "So maybe we would come up.",
                    "label": 0
                },
                {
                    "sent": "You can come up with some condition but I would suspect it would be some strong.",
                    "label": 0
                },
                {
                    "sent": "Condition somehow implying that it's not really nonsmooth in some sense.",
                    "label": 0
                },
                {
                    "sent": "Vision.",
                    "label": 0
                },
                {
                    "sent": "Then in that in this case is taking the averaging seems to be dangerous, yeah?",
                    "label": 0
                },
                {
                    "sent": "Because you're not really hovering around your optimum, you're really, as you say, more to one side, and yeah, and averaging won't help you.",
                    "label": 0
                }
            ]
        }
    }
}