{
    "id": "j4mkngquovbhcva5jbr3rsn7tuk4vsto",
    "title": "Regression-Based Latent Factor Models",
    "info": {
        "author": [
            "Deepak Agarwal, LinkedIn Corporation"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Recommender Systems"
        ]
    },
    "url": "http://videolectures.net/kdd09_agarwal_rblfm/",
    "segmentation": [
        [
            "OK thanks everyone.",
            "So first just to point out the difference of this work from previous work.",
            "So this work was motivated mainly by application web applications at Yahoo things like online advertising, content optimization.",
            "So there are two differences between this work and what is traditionally done in movie recommendation number one.",
            "We assume that we have a lot of metadata about users and movies and #2 the cold start problem is very important here so.",
            "In web applications you have new users and items coming every hour and it is important to have a model that can do a good job with both cold start and warm started in a single framework, right?",
            "So these are the two main motive."
        ],
        [
            "Asians, so I'm going to begin with the problem definition.",
            "This will be brief based on the previous talks, and again we'll use the factorization models to work with our applications.",
            "So I'm going to give a brief overview of factorization.",
            "Again, just to basically set up the notation so you already know about factorization from the previous talks.",
            "Then I'm going to show how we could incorporate covariate or meta information through regressions and come up with a model that can address both cold start and warm start.",
            "Through a single principle model, I'm going to look at the correlations that are induced by this model in the data itself.",
            "So whenever you have a model, you can calculate the correlation that is induced by the model in directly on the data, and that's going to lead to some interesting insights into what's going on.",
            "And then I'm going to talk about the fitting algorithms which we use and finally and with experiments and summary."
        ],
        [
            "So again, here again, you have a bunch of user, so users are going to be denoted by the letter I.",
            "Movies will be denoted by letter J and you have ratings YIJ for movie recommendation.",
            "It would be rating on the scale of 1 to 5 for online advertising.",
            "Users would be queries, movies would be ads and the ratings would be binary indicator whether the user clicked on the ad or not, right?",
            "So again, very simple general set up.",
            "You know you have any dyadic data you could apply these methods.",
            "The main assumption here, which was not present in previous talk is you have a lot of metadata on the diet.",
            "So for users you have a covariant vector given by the given by W subway, so that might include the age, gender, browse behavior of the user.",
            "For movies you have another covariate vector given by G of Z Jesus.",
            "So that might include the movie general, genre, release date and other things.",
            "And then you might also have diet specific covariates like is Tom Cruise part of this movie?",
            "Only later so you know does is the is the user's favorite actor.",
            "Part of this movie.",
            "And that's another covariate.",
            "So that would be all included in Xig, right?",
            "So this is the setting."
        ],
        [
            "Right so and we're going to come up with models to predict ratings for new dietze for new user movie pairs.",
            "Now if the user movie pair for which you are doing predictions already occurred in the training data, then it's a warm start problem.",
            "If the user, if any one of the user movie pair for which you want to do a prediction is not in the training data, then that's a cold start problem.",
            "And again, the challenges in the area in this area.",
            "Whenever you're dealing with data like this, you have a highly incomplete matrix, right?",
            "So you have a user cross movie ratings, but 99% of the entries are missing you.",
            "Not observed data on 99% of the entries.",
            "That's a typical scenario.",
            "The data is not missing at random, so a small fraction of users and movies account for 80% of the ratings, right?",
            "So the 8020 rule?",
            "So that's another challenge.",
            "And Thirdly, we want to handle both cold start and warm start problem in a single framework.",
            "We don't want to do ad hoc things like OK for one start.",
            "I have model A for cold start I have model B.",
            "Now let me come up with some heuristic which is going to switch from order late Model B. OK, if I have more than five ratings, let me use this model otherwise, so you could do those things, but those are not very smooth ways of transitioning from warm start to go start and does not lead to good performance.",
            "Or so can we?"
        ],
        [
            "Do this in a more principled way, so one approach you might argue is why don't you build a large scale regression, right?",
            "You have covariates on users, movies.",
            "Take the cross product and build a big regression.",
            "Now you could do that, but that's not going to do a good job of estimating ratings for heavy users and movies, right?",
            "So for users where you have a lot of ratings, you should be able to do a user centric model as you do in collaborative filtering.",
            "Regressions are going to miss that, right?",
            "So you want the best of both worlds, and that's why you can do that collaborative filtering.",
            "Yeah, you can do neighborhood based factorization, which is what we're going to do in this paper, so these are good for warm start situations, but not that good for call start you have to cook up something else for cold start.",
            "Right, so can you do both together in a single model, but that's what we're going to do.",
            "Can Asingle model do both of these together and the key is.",
            "So for heavy users you want a user specific model.",
            "For light users you want to fall back on the regression model and the keys.",
            "Can you actually come up with a smooth mechanism of doing this fall back right?",
            "The data should tell you when to fall back on what model.",
            "You should not be."
        ],
        [
            "Clicking this so again a brief overview of factorization just to set up the notations so you have users.",
            "Every user has a bunch of latent factors.",
            "Alpha Alpha is the bias for the user, the popularity, whatever you think is more appropriate, and then these are the test vectors or user profiles, movie profiles and generally you know when you capture interaction by adding them up together right?",
            "So you add up the biases and you take the inner product of the profiles to get the interaction and if there are capital N users, capital and movies.",
            "Immediately have that many number of parameters and this will start overfitting the data even for moderate values of our, because the matrix is highly incomplete.",
            "As I said in the beginning, 99% of the cells don't have any data at all, so this is going to start fitting overfitting very quickly.",
            "For small values of R, and so the key technical issue in this problem is really how to regularize these factors.",
            "That's really the crux of the problem.",
            "Writing down this model is not a big deal, right, it's OK. A little bit of thought.",
            "You could write down the model, but regularization is the key thing, and generally what is done is in the literature.",
            "You assume the UI's and the reason the alphas and betas they're drawn from zero mean Gaussian prior, and that's kind of equivalent to the L2 regularization, just a different way."
        ],
        [
            "Writing the penalty function right so this is what is done.",
            "Again, I'm writing it in a more generative model framework because that's what we're going to use.",
            "But you know, you have an observation.",
            "Equations have ratings that are assumed to come from a Gaussian distribution with some mean, and the mean is modeled by this regression function.",
            "And again in the state equation, you assume these latent variables are coming from some zero mean Gaussian priors, and that's you take this equation, put it into the training data, come up with estimates and you have a new diet.",
            "That's how you predict if you've not seen the dieting.",
            "If the diet involves a new user movie, you plug a zero over there, because that's what your prior told you to do.",
            "So if you have a new user you don't know his profile, assume it's a 0.",
            "So that's what is done."
        ],
        [
            "Now let's see if we can.",
            "So this is a simple modification you need to do to incorporate metadata in this scenario.",
            "So instead of assuming a zero mean Gaussian prior, you add a regression.",
            "To each of these terms, right?",
            "So the biases instead of the instead of the bias being drawn from a zero mean prior.",
            "Now it's going to have a regression term here, so you use all the metadata about the user, put a regression model over there.",
            "You can do that for all the terms and that immediately starts giving you that immediately gives you a model that can handle both cold start and warm start in a single framework.",
            "So intuitively the idea is if there is an you user you use this regression to predict his factors, rather than predicting a factor of 0, you predict a factor of G * W. And this regression has been estimated from training data, so you get much better estimates of user factors when they aren't."
        ],
        [
            "Doing the system right.",
            "Right, so first of all, so the regression does two things.",
            "First, it does a better job of regularising the factors you get better regularization than what you would get by using a zero mean prior #2 it addresses the cold start and warm start in the single framework, right?",
            "So the cold start so the fall back regression model, which is going to accrue due to that model which we wrote down earlier is going to be this right?",
            "So this is the regression model that is going to be used for an you user and new movie.",
            "This is the prediction which you're going to get in these parameters, mind you.",
            "Estimated all from the training data, which involves both heavy users and light users, so you're using the heavy users metadata to come up with this relationship and then extrapolating it to new users and new movies.",
            "So that's the beauty of this model."
        ],
        [
            "OK, so for those of you like graphical representation, this is what the graphical representation looks like.",
            "You have the ratings, you have the covariates and these are some parameters that goes into the first part of the regression.",
            "Then you have the user factors, movie factors, biases and these are learned through these covariate functions, Wis and CJ.",
            "So this is the new contribution from what you've seen in the literature before an then you have the factorization model connecting them all together.",
            "You get a good patient."
        ],
        [
            "Particle model and you can do computations.",
            "So just to give you an intuitive idea what happens.",
            "So this is an example where I'm showing the predicted value of the first factor on some data set, which we're going to describe later.",
            "So on the Y axis you have the predicted factor that uses only the features, no factorization at all, just the features, no user centric model on the.",
            "Sorry on the X axis and the Y axis, you have predictions from the model.",
            "So in the first graph you can see that.",
            "These are FM is the model which we are proposing in this paper.",
            "So the RLF.",
            "So this is the prediction which you're going to get by using the covariates and on the Y axis the predictions we're going to get out of the new model that regularizes using the features and this is the one that regularizes without the feature.",
            "So you could see that.",
            "It is a much better anchoring of predictions around the feature based model you use the new technique.",
            "These are things for light users, so for light users where you don't have a lot of data, the factorization model which we proposed falls back on the regression model, right?",
            "Everything clusters around the straight line y = X.",
            "If you don't do that.",
            "If you just do the usual factor model, everything clusters around 0 which is not right at all.",
            "That's that seems like a better model in this one, so that's the intuitive idea of how this model is working."
        ],
        [
            "And that's why there is a smooth transition between cold start and warm start, right?",
            "So that's what's happening.",
            "You can also interpret that as a hierarchical random effects model and."
        ],
        [
            "OK, I'm going to skip that.",
            "So again, model fitting.",
            "We do model fitting by using Monte Carlo M and also by using conditional mode.",
            "And it turns out the Monte Carlo game doesn't much better job of model fitting than the iterated conditional modes."
        ],
        [
            "So let me actually go."
        ],
        [
            "Go to the results."
        ],
        [
            "'cause I don't have much time.",
            "These details are all in the paper, so this is the first experiment on movielens and again here we use the usual movielens data that has five splits and you can see if you use the new model versus the old model, you get better regularization.",
            "So there is no cold start problem in this data at all because we use the usual splits and all the user movies that occur in the training also occur in the test.",
            "But yet by using them."
        ],
        [
            "The data you get better numbers due to better regularization.",
            "This experiment basically show for this experiment we split the data by timestamps and so in the test data you have a lot of new users and movies that were not seen in the training data.",
            "And if you use the new technique again you get better results.",
            "Again, I'm not going through the numbers, but you do get better results and we also compared it with several other cold start problem problems in the recommendation system literature."
        ],
        [
            "Experiment three again.",
            "We also added some online update scheme to the model, similar to what you have described and."
        ],
        [
            "And that improves the results further, right?",
            "So if you do an online update then this this improves the results further.",
            "So this is what you will get without any online update on the training data.",
            "If you start doing online updates, you improve the results further from 4, nine, 3.9363.",
            "So I think it started late."
        ],
        [
            "Running a bit out of time is only 10 minutes.",
            "OK, so so this is the last day to set.",
            "This is the Yahoo front page data.",
            "So the goal here is so this is the module which is of interest to us.",
            "the Today module that today module it has got four tabs featured entertainment, sports, video and the feature displays.",
            "So this is the so we have a feature tab where we display four stories and."
        ],
        [
            "The goal here is to select articles that are to be displayed on the F1 position.",
            "So this is the F1 position.",
            "The most prominent position on the model.",
            "To maximize click rates right so we have a bunch of articles coming into the system constantly and our goal is to look at click feedback that are obtained from users and try to personalize items for that F1 slot."
        ],
        [
            "Right, so this is experiment four and so the user features we used here were age, gender, Geo and browse behavior.",
            "So we have a lot of data about users browsing history and that provides a very rich information that characterizes the users interaction with content.",
            "So this is something that you don't get in movie lengths, for instance, or even Netflix.",
            "So this was actually very helpful.",
            "And then we also know things about articles, so we know whether the article is about sports or entertainment.",
            "We also have keywords associated with the article on the title, so so we use this data and."
        ],
        [
            "Again, we fitted several models, so this is the dynamic model dynamic version of our model.",
            "This is the non dynamic version of a model.",
            "This is the usual factorization model.",
            "This is a model that only uses features plane regression model, no factorization at all right?",
            "And as you heard in the talk today it is dangerous to report area under the arosi curve, so I'm not going to report the area under the RC car.",
            "I'm going to show you the whole RC car and they are all uniformly dominant over others, so that's good.",
            "There's no ambiguity here at all, so.",
            "The black curve is the one that is the dynamic model dynamic version of our model and that uniformly dominates all other three curves.",
            "The red one dominates the one that is based only on features, and it actually is better than the factorization model, so this method seems to work if you have a lot of metadata about users and movies, as was the case."
        ],
        [
            "Our application here.",
            "OK, so I'm going to skip."
        ],
        [
            "That so to summarize.",
            "We showed that if you regularize your factors by using metadata, covariate information on users and movies, it leads to better regularization.",
            "We presented a regression based factor model that regularizes wait better and also deals with both cold start and Mom start in a single framework in a seamless way.",
            "So this transition from cold start to warm start is smooth.",
            "You don't have to cook up rules like more than five go to coast or less than five come to one.",
            "You don't have to do that at all.",
            "It will automatically figure out when.",
            "To fall back on the regression model, how much to fall back on the regression model.",
            "This we did not talk about a lot in the talk, but the details are all in the paper, so our fitting methods are scalable.",
            "In particular, we have a deep sampling method that seems to perform better than using something like optimization method and.",
            "The so the method consists of two parts, the eastep and the M step.",
            "Y step is done by doing Gibbs sampling M4M step.",
            "You can actually take any of the shelf regression software.",
            "So any any linear regression routine can be used to do the M step.",
            "So what we're doing in the M step is essentially solving a bunch of linear regressions and you can take any of the shelf regression routine and use it.",
            "It could be lost, so it could be, you know, AT regression whatever is your favorite, you could use it over here, so that's one good advantage of doing the EM kind of algorithm rather than doing a full Gibbs sampler where you have to cook up your own regression routines using the Gibbs sampler.",
            "And again we have obtained good results on benchmark data, which was the movie length and I showed you.",
            "We also got good results on a new Yahoo front page data.",
            "So yeah, that.",
            "I'll end my talk.",
            "I think I'm out of time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK thanks everyone.",
                    "label": 0
                },
                {
                    "sent": "So first just to point out the difference of this work from previous work.",
                    "label": 0
                },
                {
                    "sent": "So this work was motivated mainly by application web applications at Yahoo things like online advertising, content optimization.",
                    "label": 0
                },
                {
                    "sent": "So there are two differences between this work and what is traditionally done in movie recommendation number one.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have a lot of metadata about users and movies and #2 the cold start problem is very important here so.",
                    "label": 0
                },
                {
                    "sent": "In web applications you have new users and items coming every hour and it is important to have a model that can do a good job with both cold start and warm started in a single framework, right?",
                    "label": 0
                },
                {
                    "sent": "So these are the two main motive.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asians, so I'm going to begin with the problem definition.",
                    "label": 0
                },
                {
                    "sent": "This will be brief based on the previous talks, and again we'll use the factorization models to work with our applications.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give a brief overview of factorization.",
                    "label": 0
                },
                {
                    "sent": "Again, just to basically set up the notation so you already know about factorization from the previous talks.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to show how we could incorporate covariate or meta information through regressions and come up with a model that can address both cold start and warm start.",
                    "label": 1
                },
                {
                    "sent": "Through a single principle model, I'm going to look at the correlations that are induced by this model in the data itself.",
                    "label": 0
                },
                {
                    "sent": "So whenever you have a model, you can calculate the correlation that is induced by the model in directly on the data, and that's going to lead to some interesting insights into what's going on.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to talk about the fitting algorithms which we use and finally and with experiments and summary.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, here again, you have a bunch of user, so users are going to be denoted by the letter I.",
                    "label": 0
                },
                {
                    "sent": "Movies will be denoted by letter J and you have ratings YIJ for movie recommendation.",
                    "label": 0
                },
                {
                    "sent": "It would be rating on the scale of 1 to 5 for online advertising.",
                    "label": 0
                },
                {
                    "sent": "Users would be queries, movies would be ads and the ratings would be binary indicator whether the user clicked on the ad or not, right?",
                    "label": 0
                },
                {
                    "sent": "So again, very simple general set up.",
                    "label": 0
                },
                {
                    "sent": "You know you have any dyadic data you could apply these methods.",
                    "label": 1
                },
                {
                    "sent": "The main assumption here, which was not present in previous talk is you have a lot of metadata on the diet.",
                    "label": 1
                },
                {
                    "sent": "So for users you have a covariant vector given by the given by W subway, so that might include the age, gender, browse behavior of the user.",
                    "label": 0
                },
                {
                    "sent": "For movies you have another covariate vector given by G of Z Jesus.",
                    "label": 0
                },
                {
                    "sent": "So that might include the movie general, genre, release date and other things.",
                    "label": 0
                },
                {
                    "sent": "And then you might also have diet specific covariates like is Tom Cruise part of this movie?",
                    "label": 0
                },
                {
                    "sent": "Only later so you know does is the is the user's favorite actor.",
                    "label": 0
                },
                {
                    "sent": "Part of this movie.",
                    "label": 0
                },
                {
                    "sent": "And that's another covariate.",
                    "label": 0
                },
                {
                    "sent": "So that would be all included in Xig, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the setting.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right so and we're going to come up with models to predict ratings for new dietze for new user movie pairs.",
                    "label": 1
                },
                {
                    "sent": "Now if the user movie pair for which you are doing predictions already occurred in the training data, then it's a warm start problem.",
                    "label": 1
                },
                {
                    "sent": "If the user, if any one of the user movie pair for which you want to do a prediction is not in the training data, then that's a cold start problem.",
                    "label": 0
                },
                {
                    "sent": "And again, the challenges in the area in this area.",
                    "label": 0
                },
                {
                    "sent": "Whenever you're dealing with data like this, you have a highly incomplete matrix, right?",
                    "label": 0
                },
                {
                    "sent": "So you have a user cross movie ratings, but 99% of the entries are missing you.",
                    "label": 0
                },
                {
                    "sent": "Not observed data on 99% of the entries.",
                    "label": 0
                },
                {
                    "sent": "That's a typical scenario.",
                    "label": 0
                },
                {
                    "sent": "The data is not missing at random, so a small fraction of users and movies account for 80% of the ratings, right?",
                    "label": 0
                },
                {
                    "sent": "So the 8020 rule?",
                    "label": 0
                },
                {
                    "sent": "So that's another challenge.",
                    "label": 0
                },
                {
                    "sent": "And Thirdly, we want to handle both cold start and warm start problem in a single framework.",
                    "label": 0
                },
                {
                    "sent": "We don't want to do ad hoc things like OK for one start.",
                    "label": 0
                },
                {
                    "sent": "I have model A for cold start I have model B.",
                    "label": 0
                },
                {
                    "sent": "Now let me come up with some heuristic which is going to switch from order late Model B. OK, if I have more than five ratings, let me use this model otherwise, so you could do those things, but those are not very smooth ways of transitioning from warm start to go start and does not lead to good performance.",
                    "label": 0
                },
                {
                    "sent": "Or so can we?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do this in a more principled way, so one approach you might argue is why don't you build a large scale regression, right?",
                    "label": 1
                },
                {
                    "sent": "You have covariates on users, movies.",
                    "label": 0
                },
                {
                    "sent": "Take the cross product and build a big regression.",
                    "label": 0
                },
                {
                    "sent": "Now you could do that, but that's not going to do a good job of estimating ratings for heavy users and movies, right?",
                    "label": 0
                },
                {
                    "sent": "So for users where you have a lot of ratings, you should be able to do a user centric model as you do in collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "Regressions are going to miss that, right?",
                    "label": 0
                },
                {
                    "sent": "So you want the best of both worlds, and that's why you can do that collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can do neighborhood based factorization, which is what we're going to do in this paper, so these are good for warm start situations, but not that good for call start you have to cook up something else for cold start.",
                    "label": 1
                },
                {
                    "sent": "Right, so can you do both together in a single model, but that's what we're going to do.",
                    "label": 1
                },
                {
                    "sent": "Can Asingle model do both of these together and the key is.",
                    "label": 1
                },
                {
                    "sent": "So for heavy users you want a user specific model.",
                    "label": 0
                },
                {
                    "sent": "For light users you want to fall back on the regression model and the keys.",
                    "label": 0
                },
                {
                    "sent": "Can you actually come up with a smooth mechanism of doing this fall back right?",
                    "label": 0
                },
                {
                    "sent": "The data should tell you when to fall back on what model.",
                    "label": 0
                },
                {
                    "sent": "You should not be.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clicking this so again a brief overview of factorization just to set up the notations so you have users.",
                    "label": 0
                },
                {
                    "sent": "Every user has a bunch of latent factors.",
                    "label": 0
                },
                {
                    "sent": "Alpha Alpha is the bias for the user, the popularity, whatever you think is more appropriate, and then these are the test vectors or user profiles, movie profiles and generally you know when you capture interaction by adding them up together right?",
                    "label": 0
                },
                {
                    "sent": "So you add up the biases and you take the inner product of the profiles to get the interaction and if there are capital N users, capital and movies.",
                    "label": 0
                },
                {
                    "sent": "Immediately have that many number of parameters and this will start overfitting the data even for moderate values of our, because the matrix is highly incomplete.",
                    "label": 0
                },
                {
                    "sent": "As I said in the beginning, 99% of the cells don't have any data at all, so this is going to start fitting overfitting very quickly.",
                    "label": 0
                },
                {
                    "sent": "For small values of R, and so the key technical issue in this problem is really how to regularize these factors.",
                    "label": 1
                },
                {
                    "sent": "That's really the crux of the problem.",
                    "label": 0
                },
                {
                    "sent": "Writing down this model is not a big deal, right, it's OK. A little bit of thought.",
                    "label": 0
                },
                {
                    "sent": "You could write down the model, but regularization is the key thing, and generally what is done is in the literature.",
                    "label": 0
                },
                {
                    "sent": "You assume the UI's and the reason the alphas and betas they're drawn from zero mean Gaussian prior, and that's kind of equivalent to the L2 regularization, just a different way.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Writing the penalty function right so this is what is done.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm writing it in a more generative model framework because that's what we're going to use.",
                    "label": 0
                },
                {
                    "sent": "But you know, you have an observation.",
                    "label": 0
                },
                {
                    "sent": "Equations have ratings that are assumed to come from a Gaussian distribution with some mean, and the mean is modeled by this regression function.",
                    "label": 0
                },
                {
                    "sent": "And again in the state equation, you assume these latent variables are coming from some zero mean Gaussian priors, and that's you take this equation, put it into the training data, come up with estimates and you have a new diet.",
                    "label": 0
                },
                {
                    "sent": "That's how you predict if you've not seen the dieting.",
                    "label": 0
                },
                {
                    "sent": "If the diet involves a new user movie, you plug a zero over there, because that's what your prior told you to do.",
                    "label": 0
                },
                {
                    "sent": "So if you have a new user you don't know his profile, assume it's a 0.",
                    "label": 0
                },
                {
                    "sent": "So that's what is done.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's see if we can.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple modification you need to do to incorporate metadata in this scenario.",
                    "label": 1
                },
                {
                    "sent": "So instead of assuming a zero mean Gaussian prior, you add a regression.",
                    "label": 0
                },
                {
                    "sent": "To each of these terms, right?",
                    "label": 0
                },
                {
                    "sent": "So the biases instead of the instead of the bias being drawn from a zero mean prior.",
                    "label": 0
                },
                {
                    "sent": "Now it's going to have a regression term here, so you use all the metadata about the user, put a regression model over there.",
                    "label": 0
                },
                {
                    "sent": "You can do that for all the terms and that immediately starts giving you that immediately gives you a model that can handle both cold start and warm start in a single framework.",
                    "label": 0
                },
                {
                    "sent": "So intuitively the idea is if there is an you user you use this regression to predict his factors, rather than predicting a factor of 0, you predict a factor of G * W. And this regression has been estimated from training data, so you get much better estimates of user factors when they aren't.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing the system right.",
                    "label": 0
                },
                {
                    "sent": "Right, so first of all, so the regression does two things.",
                    "label": 0
                },
                {
                    "sent": "First, it does a better job of regularising the factors you get better regularization than what you would get by using a zero mean prior #2 it addresses the cold start and warm start in the single framework, right?",
                    "label": 1
                },
                {
                    "sent": "So the cold start so the fall back regression model, which is going to accrue due to that model which we wrote down earlier is going to be this right?",
                    "label": 1
                },
                {
                    "sent": "So this is the regression model that is going to be used for an you user and new movie.",
                    "label": 0
                },
                {
                    "sent": "This is the prediction which you're going to get in these parameters, mind you.",
                    "label": 0
                },
                {
                    "sent": "Estimated all from the training data, which involves both heavy users and light users, so you're using the heavy users metadata to come up with this relationship and then extrapolating it to new users and new movies.",
                    "label": 0
                },
                {
                    "sent": "So that's the beauty of this model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so for those of you like graphical representation, this is what the graphical representation looks like.",
                    "label": 1
                },
                {
                    "sent": "You have the ratings, you have the covariates and these are some parameters that goes into the first part of the regression.",
                    "label": 0
                },
                {
                    "sent": "Then you have the user factors, movie factors, biases and these are learned through these covariate functions, Wis and CJ.",
                    "label": 0
                },
                {
                    "sent": "So this is the new contribution from what you've seen in the literature before an then you have the factorization model connecting them all together.",
                    "label": 0
                },
                {
                    "sent": "You get a good patient.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Particle model and you can do computations.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an intuitive idea what happens.",
                    "label": 0
                },
                {
                    "sent": "So this is an example where I'm showing the predicted value of the first factor on some data set, which we're going to describe later.",
                    "label": 0
                },
                {
                    "sent": "So on the Y axis you have the predicted factor that uses only the features, no factorization at all, just the features, no user centric model on the.",
                    "label": 0
                },
                {
                    "sent": "Sorry on the X axis and the Y axis, you have predictions from the model.",
                    "label": 0
                },
                {
                    "sent": "So in the first graph you can see that.",
                    "label": 1
                },
                {
                    "sent": "These are FM is the model which we are proposing in this paper.",
                    "label": 0
                },
                {
                    "sent": "So the RLF.",
                    "label": 0
                },
                {
                    "sent": "So this is the prediction which you're going to get by using the covariates and on the Y axis the predictions we're going to get out of the new model that regularizes using the features and this is the one that regularizes without the feature.",
                    "label": 0
                },
                {
                    "sent": "So you could see that.",
                    "label": 0
                },
                {
                    "sent": "It is a much better anchoring of predictions around the feature based model you use the new technique.",
                    "label": 0
                },
                {
                    "sent": "These are things for light users, so for light users where you don't have a lot of data, the factorization model which we proposed falls back on the regression model, right?",
                    "label": 0
                },
                {
                    "sent": "Everything clusters around the straight line y = X.",
                    "label": 0
                },
                {
                    "sent": "If you don't do that.",
                    "label": 0
                },
                {
                    "sent": "If you just do the usual factor model, everything clusters around 0 which is not right at all.",
                    "label": 0
                },
                {
                    "sent": "That's that seems like a better model in this one, so that's the intuitive idea of how this model is working.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's why there is a smooth transition between cold start and warm start, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what's happening.",
                    "label": 0
                },
                {
                    "sent": "You can also interpret that as a hierarchical random effects model and.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to skip that.",
                    "label": 0
                },
                {
                    "sent": "So again, model fitting.",
                    "label": 0
                },
                {
                    "sent": "We do model fitting by using Monte Carlo M and also by using conditional mode.",
                    "label": 1
                },
                {
                    "sent": "And it turns out the Monte Carlo game doesn't much better job of model fitting than the iterated conditional modes.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me actually go.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go to the results.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause I don't have much time.",
                    "label": 0
                },
                {
                    "sent": "These details are all in the paper, so this is the first experiment on movielens and again here we use the usual movielens data that has five splits and you can see if you use the new model versus the old model, you get better regularization.",
                    "label": 0
                },
                {
                    "sent": "So there is no cold start problem in this data at all because we use the usual splits and all the user movies that occur in the training also occur in the test.",
                    "label": 0
                },
                {
                    "sent": "But yet by using them.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data you get better numbers due to better regularization.",
                    "label": 0
                },
                {
                    "sent": "This experiment basically show for this experiment we split the data by timestamps and so in the test data you have a lot of new users and movies that were not seen in the training data.",
                    "label": 0
                },
                {
                    "sent": "And if you use the new technique again you get better results.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm not going through the numbers, but you do get better results and we also compared it with several other cold start problem problems in the recommendation system literature.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiment three again.",
                    "label": 0
                },
                {
                    "sent": "We also added some online update scheme to the model, similar to what you have described and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that improves the results further, right?",
                    "label": 0
                },
                {
                    "sent": "So if you do an online update then this this improves the results further.",
                    "label": 0
                },
                {
                    "sent": "So this is what you will get without any online update on the training data.",
                    "label": 0
                },
                {
                    "sent": "If you start doing online updates, you improve the results further from 4, nine, 3.9363.",
                    "label": 0
                },
                {
                    "sent": "So I think it started late.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Running a bit out of time is only 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is the last day to set.",
                    "label": 0
                },
                {
                    "sent": "This is the Yahoo front page data.",
                    "label": 0
                },
                {
                    "sent": "So the goal here is so this is the module which is of interest to us.",
                    "label": 0
                },
                {
                    "sent": "the Today module that today module it has got four tabs featured entertainment, sports, video and the feature displays.",
                    "label": 1
                },
                {
                    "sent": "So this is the so we have a feature tab where we display four stories and.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The goal here is to select articles that are to be displayed on the F1 position.",
                    "label": 0
                },
                {
                    "sent": "So this is the F1 position.",
                    "label": 0
                },
                {
                    "sent": "The most prominent position on the model.",
                    "label": 0
                },
                {
                    "sent": "To maximize click rates right so we have a bunch of articles coming into the system constantly and our goal is to look at click feedback that are obtained from users and try to personalize items for that F1 slot.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so this is experiment four and so the user features we used here were age, gender, Geo and browse behavior.",
                    "label": 1
                },
                {
                    "sent": "So we have a lot of data about users browsing history and that provides a very rich information that characterizes the users interaction with content.",
                    "label": 0
                },
                {
                    "sent": "So this is something that you don't get in movie lengths, for instance, or even Netflix.",
                    "label": 0
                },
                {
                    "sent": "So this was actually very helpful.",
                    "label": 0
                },
                {
                    "sent": "And then we also know things about articles, so we know whether the article is about sports or entertainment.",
                    "label": 0
                },
                {
                    "sent": "We also have keywords associated with the article on the title, so so we use this data and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we fitted several models, so this is the dynamic model dynamic version of our model.",
                    "label": 0
                },
                {
                    "sent": "This is the non dynamic version of a model.",
                    "label": 0
                },
                {
                    "sent": "This is the usual factorization model.",
                    "label": 0
                },
                {
                    "sent": "This is a model that only uses features plane regression model, no factorization at all right?",
                    "label": 0
                },
                {
                    "sent": "And as you heard in the talk today it is dangerous to report area under the arosi curve, so I'm not going to report the area under the RC car.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you the whole RC car and they are all uniformly dominant over others, so that's good.",
                    "label": 0
                },
                {
                    "sent": "There's no ambiguity here at all, so.",
                    "label": 0
                },
                {
                    "sent": "The black curve is the one that is the dynamic model dynamic version of our model and that uniformly dominates all other three curves.",
                    "label": 0
                },
                {
                    "sent": "The red one dominates the one that is based only on features, and it actually is better than the factorization model, so this method seems to work if you have a lot of metadata about users and movies, as was the case.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our application here.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to skip.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That so to summarize.",
                    "label": 0
                },
                {
                    "sent": "We showed that if you regularize your factors by using metadata, covariate information on users and movies, it leads to better regularization.",
                    "label": 0
                },
                {
                    "sent": "We presented a regression based factor model that regularizes wait better and also deals with both cold start and Mom start in a single framework in a seamless way.",
                    "label": 1
                },
                {
                    "sent": "So this transition from cold start to warm start is smooth.",
                    "label": 0
                },
                {
                    "sent": "You don't have to cook up rules like more than five go to coast or less than five come to one.",
                    "label": 0
                },
                {
                    "sent": "You don't have to do that at all.",
                    "label": 0
                },
                {
                    "sent": "It will automatically figure out when.",
                    "label": 0
                },
                {
                    "sent": "To fall back on the regression model, how much to fall back on the regression model.",
                    "label": 0
                },
                {
                    "sent": "This we did not talk about a lot in the talk, but the details are all in the paper, so our fitting methods are scalable.",
                    "label": 0
                },
                {
                    "sent": "In particular, we have a deep sampling method that seems to perform better than using something like optimization method and.",
                    "label": 0
                },
                {
                    "sent": "The so the method consists of two parts, the eastep and the M step.",
                    "label": 0
                },
                {
                    "sent": "Y step is done by doing Gibbs sampling M4M step.",
                    "label": 0
                },
                {
                    "sent": "You can actually take any of the shelf regression software.",
                    "label": 0
                },
                {
                    "sent": "So any any linear regression routine can be used to do the M step.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing in the M step is essentially solving a bunch of linear regressions and you can take any of the shelf regression routine and use it.",
                    "label": 0
                },
                {
                    "sent": "It could be lost, so it could be, you know, AT regression whatever is your favorite, you could use it over here, so that's one good advantage of doing the EM kind of algorithm rather than doing a full Gibbs sampler where you have to cook up your own regression routines using the Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "And again we have obtained good results on benchmark data, which was the movie length and I showed you.",
                    "label": 0
                },
                {
                    "sent": "We also got good results on a new Yahoo front page data.",
                    "label": 0
                },
                {
                    "sent": "So yeah, that.",
                    "label": 0
                },
                {
                    "sent": "I'll end my talk.",
                    "label": 0
                },
                {
                    "sent": "I think I'm out of time.",
                    "label": 0
                }
            ]
        }
    }
}