{
    "id": "bjalsdtjodlybov3xzoiexydkdgt7lu7",
    "title": "Decision Tree and Instance-Based Learning for Label Ranking",
    "info": {
        "author": [
            "Weiwei Cheng, Mathematik und Informatik, Philipps-Universit\u00e4t Marburg"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_cheng_dtibllflr/",
    "segmentation": [
        [
            "Hello everybody.",
            "Our second speaker is weicheng on decision tree.",
            "An instance based learning for label ranking.",
            "OK, how everybody and here now talking about different kind of learning problem and namely I want to use this local learning approach to solve labor ranking problem.",
            "I will talk about how to use Decision tree and instant based learning for label ranking OK and this is joint work with my colleague yes here and my advisor Michael Myers.",
            "OK, let's get started.",
            "So consider you have our Costco annual selling different type of cars."
        ],
        [
            "And you have different customers and different customers.",
            "We have different preference or different type of cars.",
            "OK, now obviously that you have a new customer you would like to know that what is his preference on this different type of cars?",
            "OK, so here's the output.",
            "Is the ranking OK?",
            "It's not like classification which we are only interested in the Top Rank, but we're interested in a total ranking.",
            "Right, and sometimes it's more convenient."
        ],
        [
            "So present this problem in terms of computation.",
            "Right?",
            "An formally this label ranking problem can define exist way OK, we have a set of training instance and set up."
        ],
        [
            "Predefined finite set of labels and for each training instance we have a set of pairwise preference that tells you which label is better than which label.",
            "Alright, then the goal of this problem is we want to find a ranking function which is a mapping from this feature space to the ranking space that can Maps each instance to a ranking of the labels.",
            "And of course we would like to generalize well in terms of a predefined loss function, no rankings.",
            "OK, we do this talk.",
            "We concentrate ourselves on this kind of distance."
        ],
        [
            "And this is rather interesting problem and there are already a lot of work on this problem, so here is a list of representative approaches.",
            "And I want to go to details to all these metals.",
            "I want to say is that all these metals works, so the performance of comparably.",
            "And essentially all this approaches reduces labor ranking problem to classification.",
            "OK, this is quite efficient.",
            "But they may have some problems.",
            "For example, it comes along with the loss information due to this transformation.",
            "And.",
            "Most of these models, most of these approaches has somehow strong model assumptions.",
            "So it's come along with some how strong buyers OK due to these buyers you may lack of flexibility.",
            "Last but not least, that.",
            "Most of the approaches is a black box.",
            "OK, so tomorrow is a black box or sometimes you have problem to interpret this whole thing.",
            "Alright.",
            "Today I would like to talk how to use the local approach to solving."
        ],
        [
            "Problem.",
            "Uh, we talk about nearest labor approach and as well as this decision tree approach.",
            "We don't create a global model.",
            "Instead of that, just target function is estimated in a local way.",
            "Because we only consider a local region, so we can assume that the distribution of the ranking in that region is constant.",
            "OK, now the problem is how do you estimate this locally constant model?",
            "So to be more precise and we consider the output, the ranking is a.",
            "For instance is generated according to our underlying distribution."
        ],
        [
            "And this distribution is at least approximately constant within this local region.",
            "Because the other preference nearby.",
            "Are also generated by the same distribution.",
            "So basically we can just using the principle of maximum likelihood principle to solve this problem.",
            "OK."
        ],
        [
            "I have a sad that there is a probability distribution of rankings, but what should this distribution looks like?",
            "OK, so here I will be more explicitly that to show there's one particular way to model this distribution.",
            "Here we use this so called models model.",
            "This is quite popular in statistics to model this ranking data.",
            "Achieving permutation or say giving ranking Sigma.",
            "The probability can be computed in this way.",
            "OK is this model has two parameters, namely the central ranking.",
            "Pie is a ranking and spread parameter which is rare value which is bigger than there.",
            "Oh OK. And this model's model.",
            "Belongs to this exponential family, so it has a lot of nice properties.",
            "OK, so you can look at this model in a similar way with this Gaussian distribution.",
            "OK, just central ranking corresponds to the mean value and square parameter.",
            "Sita is conformed to this standard duration.",
            "OK, and of course you still need a distant functional rankings.",
            "OK, you have to measure the distance of the rankings so we have this D. Here is a distance function and this kind of distant function should satisfy a certain kind of property.",
            "For example this right invariant property.",
            "Which means if you got 2 rankings and you re number them in the same way, it doesn't change distance right sequential?",
            "In fact, a lot of this distance on permutation or rankings fulfills such kind of property.",
            "Just kind of talk distance we consider.",
            "Here is also running variant.",
            "Alright."
        ],
        [
            "OK, so now since we have the model that we can now do this inference OK. Basically based on this maximum likelihood principle.",
            "For now, let's just consider a complete ranking case, which means all the observation in the local region is a full ranking OK, so now let's do this inference.",
            "OK, so some mathematical stuff we have.",
            "This observation is K rankings.",
            "Is that the probability of this observation due to this independent assumption?",
            "Because every observation is different independent, so we can read up using this big product.",
            "OK, now we plug in the models model, do some calculation, then we end up with this phone.",
            "Alright, let's look a little crazy, but in fact if you look close to it, you find out that the center ranking Parimeter Pi only appears in the numerator.",
            "OK, and the normalization constants is only a function of the center.",
            "OK, so this is really nice because then the maximum likelihood of the central ranking Pi is just the ranking which can minimize the sum of the distance.",
            "And once you got this central ranking, you can just computer this setup parameter using this equation.",
            "Is this equation.",
            "It turns out that you just take derivative.",
            "You'll find that is monitoring system since it is monotone, we can just using standardized search, we just go to the city.",
            "OK, so everything is very nice, but.",
            "We are."
        ],
        [
            "Somehow more interesting case is, what if the ranking is incomplete?",
            "For example, there is a customer, he comes, he says OK.",
            "I like BMW better than Toyota.",
            "But he doesn't give you any information or meaning.",
            "OK, so in this case the observation is incomplete.",
            "Work you do.",
            "Speaking using the probabilistic model, then to deal with the problem is rather easy, because if you have a observation like this, tell you a is better than B, then the probability of this happened is just equal to all these three different cases happens OK, this is all say consistent extension of this incomplete ranking.",
            "OK, we're a is better than B.",
            "Right, you just summed it up that you got the probability of this event.",
            "Alright.",
            "So.",
            "Are just as beef."
        ],
        [
            "So we plug in the formula.",
            "And.",
            "We end up with this one.",
            "OK, this formula looks a little bit similar with what we got in the Paris slide, but this is much more difficult.",
            "There are at least two reasons.",
            "First, we saw there is a big sum here.",
            "Right, we all know that if you want to take a derivative, then you can pick some here, just totally screwed up right in the second reason.",
            "Even more important is you have to consider all the consistent extensions.",
            "This is in a permutation space, right?",
            "We all know this permutation space.",
            "The ranking space is really large.",
            "You do the sum.",
            "So.",
            "Now to have this exact muscle language estimation is rather difficult, so we have to do some approximation.",
            "OK, there are a lot of different ways to do this, but in this paper will present the following approach.",
            "To solve this problem.",
            "Namely, we're using a emlak approach to solve this problem we replace."
        ],
        [
            "So each tab with another maximization step OK.",
            "This is also commonly done in learning Markov model and K means clustering.",
            "Alright, so this works in the following way.",
            "For example you have this red dot here and you want to know what is ranking for that and then you check the local region we got the observation like this.",
            "The first thing to do is you have to start with a reasonable initial guess.",
            "And we're using this generalizable account to guard this initial guess, but I won't go into detail here.",
            "You could check the paper.",
            "Let's just say we start with a reasonable guess.",
            "For example, we got here, so I guess it's 1234 for this.",
            "Then we replace all the incomplete observations with this most probable extension in this case.",
            "We replace this.",
            "And again, replace this alright.",
            "So this can be done efficiently.",
            "We have shown the paper if you're interested in that, you can check the paper.",
            "So this is the first step.",
            "OK, so now all the observation are full rankings so you can now do this whole things get the estimation just as in the complete ranking case.",
            "So then you got and you say estimation for the central ranking.",
            "So then you consider the.",
            "Neighborhood again, then you do the whole things once again, again again until the central ranking.",
            "Here never changed.",
            "Alright.",
            "OK, so."
        ],
        [
            "It's not holding the central ranking is interested interesting, but also the spread parameter set up.",
            "Right?",
            "Because this setup parameter actually tells you that how certain you are about one particular prediction.",
            "Right, it's just you can just imagine the Gaussian distribution right this if you have very high variance and you know distribution is rather flat.",
            "But if he were slower variants and you know that everything just picked to the mean value, right?",
            "So if we have a big Theta here, then that means that you're pretty much certain about what you got.",
            "OK, the prediction is rather reliable.",
            "OK. Once you have this kind of.",
            "Spell parameters set up.",
            "Then you can do a lot of things, for example."
        ],
        [
            "We can now modify this conventional regression tree to solving this labor ranking problems.",
            "There are two major modifications.",
            "First of all is about the split criteria, right?",
            "How do you speak the tree?",
            "So in the conventional regression tree, this is based on this standard operation and here we can just using the spare parameter instead.",
            "OK, you can define a goodness of fit in terms of the setup parameter and then you just split the tree.",
            "Using this goodness of fit.",
            "A second modification is about this storming catiria for partitioning the tree OK. We now so far.",
            "Currently we stop to partition the tree.",
            "If first the train is pure OK or the number of labels in a node is too small.",
            "OK because in either case is making no sense to particular anymore.",
            "OK, that was stopped in the tree."
        ],
        [
            "So after you do the whole training, then label ranking tree looks approximately look like this.",
            "OK, this look quite similar as a regression tree, so differences in the node in the leaves you got rankings instead of a real value number.",
            "OK.",
            "So now I will show you some experimental results."
        ],
        [
            "So here is the data site.",
            "All the resulting one big table.",
            "We compare our local approaches, namely this local approach.",
            "With this label approach and this label, ranking trees.",
            "Compare them with this constraint classification.",
            "OK.",
            "There are two men messages here.",
            "First, we found out that if the data side is rather good, namely if the rankings are complete or the mislabels and also high.",
            "Then we found out this Eastern based label ranking is significantly better.",
            "The second observation is we found that we found out that the size of the label ranking tree is not that big.",
            "Here you see the column.",
            "After this labor entry, it is the relative size of the neighboring country compared with the conventional decision tree which is trained only on the top label.",
            "OK, we found the size is not that big this.",
            "Rather go see.",
            "So here."
        ],
        [
            "It is a typical learning curve of the local approach compared with this constraint classification.",
            "OK. We found out that our local approaches, our local metals are more flexible.",
            "So it can exploit more information preference information.",
            "Compared with this model based approach, while this model based approach has somehow stronger buyers, you see this so lack of flexibility.",
            "OK. And I pray."
        ],
        [
            "Down here, here is some man conclusion.",
            "We have introduced an instant based approach for label ranking where we using public model, namely the models model and once you give this label ranking problem full probabilistic treatment then you can gather a lot of useful information.",
            "For example this setup parameter.",
            "OK, so this setup parameter can tells you how reliable your prediction this.",
            "OK, and our approach can deal with complete and as well this incoming ranking as well.",
            "We can deal with both case.",
            "There are of course some filter work we want to do, namely is this.",
            "Approximation for this in company ranking case we want to make this inference more efficient.",
            "This is rather complex problem.",
            "There's always space to improve.",
            "The second is to approach.",
            "I proposed here.",
            "Is not only useful for labor ranking problem, but it can solve other type of problems as well.",
            "OK, so we also would like to see what we can do in the future for other type of problems.",
            "OK, so basically."
        ],
        [
            "This is all from my side and if you have any problems I would like to answer now, thanks."
        ],
        [
            "Time for questions.",
            "And since we're videotaping, can we just make sure the speaker that you repeat the questions so that I will try that if I understand?",
            "So there are two ingredients in your methods.",
            "That one is maximum likelihood and over the other one is Decision Tree, which are often considered to be proved overfitting.",
            "So I was wondering whether you observe that or whether the Spectre problem kind of avoided.",
            "In fact, that's where beginning because we don't have our explicit pruning strategy for the tree, so we somehow expect that the size of the tree is rather big.",
            "OK, I think it is a question right?",
            "Well that was for the decision.",
            "OK, but but in fact it turns out that the size of the tree is not as big as we imagine.",
            "Around here you see here this relative size of the tree is rather say, compatible with the normal decision tree.",
            "OK, so the overfitting is not obvious.",
            "Not that obvious in the experiments.",
            "As well As for the nearest label approaches, because in the nearest labor approaches you just have to control how big your neighborhood is.",
            "OK, if you have good control of the neighborhood, then is also not that likely you over fit your data.",
            "Does that also question?",
            "Right?",
            "Now please.",
            "So I was wondering why, why did you choose to use a mellophone?",
            "Why do you think that's a good?",
            "OK so question is why God uses models model?",
            "Well, first of all that this models model is kind of the standard or say Golden standard in statistics to dealing with the ranking data and there are already a lot of work had been done on this models model.",
            "This is the most important reason, but of course you could also use some other type of models for dealing.",
            "Ranking data is not a problem here.",
            "You could also try some other model of course as well.",
            "OK, so is that OK, cool.",
            "Clarifying your right.",
            "Requirements basically come back.",
            "Any loss metric that you have can't be spending on the right position, so you can actually have a model that where you want to optimize something or the higher rates for more.",
            "Well, this.",
            "This is fairly interesting question.",
            "The question is if we is it possible to use some other type of loss function right?",
            "Is that you?",
            "OK, you mean it means that it possible that higher rank object has a high importancy OK?",
            "So so far we don't have considered such kind of loss function.",
            "Because as you'll see here in this model's model.",
            "We have used this kind of distance.",
            "So this kind of distance doesn't have any pretending importancy on any particular rank, but this is definitely something we want.",
            "We want to look into, right?",
            "If you have another loss function.",
            "Which yeah, I think this works fine, but so far we treat all the rancor over position equally.",
            "I had a question.",
            "Yeah please.",
            "In your results you you mentioned that if you go outside you mentioned that the for the when there's a lot of data, not much missing data.",
            "IDL method performs.",
            "Well yeah, but just glancing at the slide, there's a lot of variation.",
            "There are some datasets that's true, some datasets it's not true.",
            "I have any thoughts about what are the properties of the domain or the data that might cause well.",
            "Work better than another if you are talking about a particular algorithm is definitely that.",
            "Maybe on some data set is better.",
            "Some datasets good.",
            "So when I say this local approach is significantly better is in terms of statistical tests OK?",
            "We pick a lot of random data set and we just do this freedom test.",
            "So question is if I have a particular data set, is there a way I can tell for this kind of data?",
            "Maybe it's sparser, maybe it's different kinds of features.",
            "My guess is here that, for example, if you have using construct application and if you have because this concerns classification is strongly say media like model.",
            "So if you have a very non linear data side our expect that our models works much better than just see.",
            "So it also depends on how lonely it is.",
            "That is OK, but especially I guess normally in the real world application the real data site is rather nonlinear.",
            "Is my guess.",
            "OK, so any other questions?",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everybody.",
                    "label": 0
                },
                {
                    "sent": "Our second speaker is weicheng on decision tree.",
                    "label": 0
                },
                {
                    "sent": "An instance based learning for label ranking.",
                    "label": 0
                },
                {
                    "sent": "OK, how everybody and here now talking about different kind of learning problem and namely I want to use this local learning approach to solve labor ranking problem.",
                    "label": 0
                },
                {
                    "sent": "I will talk about how to use Decision tree and instant based learning for label ranking OK and this is joint work with my colleague yes here and my advisor Michael Myers.",
                    "label": 0
                },
                {
                    "sent": "OK, let's get started.",
                    "label": 0
                },
                {
                    "sent": "So consider you have our Costco annual selling different type of cars.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you have different customers and different customers.",
                    "label": 0
                },
                {
                    "sent": "We have different preference or different type of cars.",
                    "label": 0
                },
                {
                    "sent": "OK, now obviously that you have a new customer you would like to know that what is his preference on this different type of cars?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the output.",
                    "label": 0
                },
                {
                    "sent": "Is the ranking OK?",
                    "label": 0
                },
                {
                    "sent": "It's not like classification which we are only interested in the Top Rank, but we're interested in a total ranking.",
                    "label": 0
                },
                {
                    "sent": "Right, and sometimes it's more convenient.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So present this problem in terms of computation.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "An formally this label ranking problem can define exist way OK, we have a set of training instance and set up.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Predefined finite set of labels and for each training instance we have a set of pairwise preference that tells you which label is better than which label.",
                    "label": 0
                },
                {
                    "sent": "Alright, then the goal of this problem is we want to find a ranking function which is a mapping from this feature space to the ranking space that can Maps each instance to a ranking of the labels.",
                    "label": 0
                },
                {
                    "sent": "And of course we would like to generalize well in terms of a predefined loss function, no rankings.",
                    "label": 0
                },
                {
                    "sent": "OK, we do this talk.",
                    "label": 0
                },
                {
                    "sent": "We concentrate ourselves on this kind of distance.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is rather interesting problem and there are already a lot of work on this problem, so here is a list of representative approaches.",
                    "label": 0
                },
                {
                    "sent": "And I want to go to details to all these metals.",
                    "label": 0
                },
                {
                    "sent": "I want to say is that all these metals works, so the performance of comparably.",
                    "label": 0
                },
                {
                    "sent": "And essentially all this approaches reduces labor ranking problem to classification.",
                    "label": 0
                },
                {
                    "sent": "OK, this is quite efficient.",
                    "label": 0
                },
                {
                    "sent": "But they may have some problems.",
                    "label": 0
                },
                {
                    "sent": "For example, it comes along with the loss information due to this transformation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Most of these models, most of these approaches has somehow strong model assumptions.",
                    "label": 0
                },
                {
                    "sent": "So it's come along with some how strong buyers OK due to these buyers you may lack of flexibility.",
                    "label": 0
                },
                {
                    "sent": "Last but not least, that.",
                    "label": 0
                },
                {
                    "sent": "Most of the approaches is a black box.",
                    "label": 0
                },
                {
                    "sent": "OK, so tomorrow is a black box or sometimes you have problem to interpret this whole thing.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Today I would like to talk how to use the local approach to solving.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "Uh, we talk about nearest labor approach and as well as this decision tree approach.",
                    "label": 0
                },
                {
                    "sent": "We don't create a global model.",
                    "label": 0
                },
                {
                    "sent": "Instead of that, just target function is estimated in a local way.",
                    "label": 1
                },
                {
                    "sent": "Because we only consider a local region, so we can assume that the distribution of the ranking in that region is constant.",
                    "label": 0
                },
                {
                    "sent": "OK, now the problem is how do you estimate this locally constant model?",
                    "label": 0
                },
                {
                    "sent": "So to be more precise and we consider the output, the ranking is a.",
                    "label": 0
                },
                {
                    "sent": "For instance is generated according to our underlying distribution.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this distribution is at least approximately constant within this local region.",
                    "label": 0
                },
                {
                    "sent": "Because the other preference nearby.",
                    "label": 0
                },
                {
                    "sent": "Are also generated by the same distribution.",
                    "label": 0
                },
                {
                    "sent": "So basically we can just using the principle of maximum likelihood principle to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a sad that there is a probability distribution of rankings, but what should this distribution looks like?",
                    "label": 0
                },
                {
                    "sent": "OK, so here I will be more explicitly that to show there's one particular way to model this distribution.",
                    "label": 0
                },
                {
                    "sent": "Here we use this so called models model.",
                    "label": 0
                },
                {
                    "sent": "This is quite popular in statistics to model this ranking data.",
                    "label": 0
                },
                {
                    "sent": "Achieving permutation or say giving ranking Sigma.",
                    "label": 0
                },
                {
                    "sent": "The probability can be computed in this way.",
                    "label": 0
                },
                {
                    "sent": "OK is this model has two parameters, namely the central ranking.",
                    "label": 0
                },
                {
                    "sent": "Pie is a ranking and spread parameter which is rare value which is bigger than there.",
                    "label": 0
                },
                {
                    "sent": "Oh OK. And this model's model.",
                    "label": 0
                },
                {
                    "sent": "Belongs to this exponential family, so it has a lot of nice properties.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can look at this model in a similar way with this Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, just central ranking corresponds to the mean value and square parameter.",
                    "label": 0
                },
                {
                    "sent": "Sita is conformed to this standard duration.",
                    "label": 0
                },
                {
                    "sent": "OK, and of course you still need a distant functional rankings.",
                    "label": 0
                },
                {
                    "sent": "OK, you have to measure the distance of the rankings so we have this D. Here is a distance function and this kind of distant function should satisfy a certain kind of property.",
                    "label": 0
                },
                {
                    "sent": "For example this right invariant property.",
                    "label": 0
                },
                {
                    "sent": "Which means if you got 2 rankings and you re number them in the same way, it doesn't change distance right sequential?",
                    "label": 0
                },
                {
                    "sent": "In fact, a lot of this distance on permutation or rankings fulfills such kind of property.",
                    "label": 0
                },
                {
                    "sent": "Just kind of talk distance we consider.",
                    "label": 0
                },
                {
                    "sent": "Here is also running variant.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now since we have the model that we can now do this inference OK. Basically based on this maximum likelihood principle.",
                    "label": 0
                },
                {
                    "sent": "For now, let's just consider a complete ranking case, which means all the observation in the local region is a full ranking OK, so now let's do this inference.",
                    "label": 0
                },
                {
                    "sent": "OK, so some mathematical stuff we have.",
                    "label": 0
                },
                {
                    "sent": "This observation is K rankings.",
                    "label": 0
                },
                {
                    "sent": "Is that the probability of this observation due to this independent assumption?",
                    "label": 0
                },
                {
                    "sent": "Because every observation is different independent, so we can read up using this big product.",
                    "label": 0
                },
                {
                    "sent": "OK, now we plug in the models model, do some calculation, then we end up with this phone.",
                    "label": 0
                },
                {
                    "sent": "Alright, let's look a little crazy, but in fact if you look close to it, you find out that the center ranking Parimeter Pi only appears in the numerator.",
                    "label": 0
                },
                {
                    "sent": "OK, and the normalization constants is only a function of the center.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is really nice because then the maximum likelihood of the central ranking Pi is just the ranking which can minimize the sum of the distance.",
                    "label": 0
                },
                {
                    "sent": "And once you got this central ranking, you can just computer this setup parameter using this equation.",
                    "label": 0
                },
                {
                    "sent": "Is this equation.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you just take derivative.",
                    "label": 0
                },
                {
                    "sent": "You'll find that is monitoring system since it is monotone, we can just using standardized search, we just go to the city.",
                    "label": 0
                },
                {
                    "sent": "OK, so everything is very nice, but.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Somehow more interesting case is, what if the ranking is incomplete?",
                    "label": 0
                },
                {
                    "sent": "For example, there is a customer, he comes, he says OK.",
                    "label": 0
                },
                {
                    "sent": "I like BMW better than Toyota.",
                    "label": 0
                },
                {
                    "sent": "But he doesn't give you any information or meaning.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case the observation is incomplete.",
                    "label": 0
                },
                {
                    "sent": "Work you do.",
                    "label": 0
                },
                {
                    "sent": "Speaking using the probabilistic model, then to deal with the problem is rather easy, because if you have a observation like this, tell you a is better than B, then the probability of this happened is just equal to all these three different cases happens OK, this is all say consistent extension of this incomplete ranking.",
                    "label": 0
                },
                {
                    "sent": "OK, we're a is better than B.",
                    "label": 0
                },
                {
                    "sent": "Right, you just summed it up that you got the probability of this event.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Are just as beef.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we plug in the formula.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We end up with this one.",
                    "label": 0
                },
                {
                    "sent": "OK, this formula looks a little bit similar with what we got in the Paris slide, but this is much more difficult.",
                    "label": 0
                },
                {
                    "sent": "There are at least two reasons.",
                    "label": 0
                },
                {
                    "sent": "First, we saw there is a big sum here.",
                    "label": 0
                },
                {
                    "sent": "Right, we all know that if you want to take a derivative, then you can pick some here, just totally screwed up right in the second reason.",
                    "label": 0
                },
                {
                    "sent": "Even more important is you have to consider all the consistent extensions.",
                    "label": 0
                },
                {
                    "sent": "This is in a permutation space, right?",
                    "label": 0
                },
                {
                    "sent": "We all know this permutation space.",
                    "label": 0
                },
                {
                    "sent": "The ranking space is really large.",
                    "label": 0
                },
                {
                    "sent": "You do the sum.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now to have this exact muscle language estimation is rather difficult, so we have to do some approximation.",
                    "label": 0
                },
                {
                    "sent": "OK, there are a lot of different ways to do this, but in this paper will present the following approach.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Namely, we're using a emlak approach to solve this problem we replace.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So each tab with another maximization step OK.",
                    "label": 0
                },
                {
                    "sent": "This is also commonly done in learning Markov model and K means clustering.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this works in the following way.",
                    "label": 0
                },
                {
                    "sent": "For example you have this red dot here and you want to know what is ranking for that and then you check the local region we got the observation like this.",
                    "label": 0
                },
                {
                    "sent": "The first thing to do is you have to start with a reasonable initial guess.",
                    "label": 0
                },
                {
                    "sent": "And we're using this generalizable account to guard this initial guess, but I won't go into detail here.",
                    "label": 0
                },
                {
                    "sent": "You could check the paper.",
                    "label": 0
                },
                {
                    "sent": "Let's just say we start with a reasonable guess.",
                    "label": 0
                },
                {
                    "sent": "For example, we got here, so I guess it's 1234 for this.",
                    "label": 0
                },
                {
                    "sent": "Then we replace all the incomplete observations with this most probable extension in this case.",
                    "label": 0
                },
                {
                    "sent": "We replace this.",
                    "label": 0
                },
                {
                    "sent": "And again, replace this alright.",
                    "label": 0
                },
                {
                    "sent": "So this can be done efficiently.",
                    "label": 0
                },
                {
                    "sent": "We have shown the paper if you're interested in that, you can check the paper.",
                    "label": 0
                },
                {
                    "sent": "So this is the first step.",
                    "label": 0
                },
                {
                    "sent": "OK, so now all the observation are full rankings so you can now do this whole things get the estimation just as in the complete ranking case.",
                    "label": 0
                },
                {
                    "sent": "So then you got and you say estimation for the central ranking.",
                    "label": 0
                },
                {
                    "sent": "So then you consider the.",
                    "label": 0
                },
                {
                    "sent": "Neighborhood again, then you do the whole things once again, again again until the central ranking.",
                    "label": 0
                },
                {
                    "sent": "Here never changed.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not holding the central ranking is interested interesting, but also the spread parameter set up.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Because this setup parameter actually tells you that how certain you are about one particular prediction.",
                    "label": 0
                },
                {
                    "sent": "Right, it's just you can just imagine the Gaussian distribution right this if you have very high variance and you know distribution is rather flat.",
                    "label": 0
                },
                {
                    "sent": "But if he were slower variants and you know that everything just picked to the mean value, right?",
                    "label": 0
                },
                {
                    "sent": "So if we have a big Theta here, then that means that you're pretty much certain about what you got.",
                    "label": 0
                },
                {
                    "sent": "OK, the prediction is rather reliable.",
                    "label": 0
                },
                {
                    "sent": "OK. Once you have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Spell parameters set up.",
                    "label": 0
                },
                {
                    "sent": "Then you can do a lot of things, for example.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can now modify this conventional regression tree to solving this labor ranking problems.",
                    "label": 0
                },
                {
                    "sent": "There are two major modifications.",
                    "label": 0
                },
                {
                    "sent": "First of all is about the split criteria, right?",
                    "label": 0
                },
                {
                    "sent": "How do you speak the tree?",
                    "label": 0
                },
                {
                    "sent": "So in the conventional regression tree, this is based on this standard operation and here we can just using the spare parameter instead.",
                    "label": 0
                },
                {
                    "sent": "OK, you can define a goodness of fit in terms of the setup parameter and then you just split the tree.",
                    "label": 0
                },
                {
                    "sent": "Using this goodness of fit.",
                    "label": 0
                },
                {
                    "sent": "A second modification is about this storming catiria for partitioning the tree OK. We now so far.",
                    "label": 0
                },
                {
                    "sent": "Currently we stop to partition the tree.",
                    "label": 0
                },
                {
                    "sent": "If first the train is pure OK or the number of labels in a node is too small.",
                    "label": 0
                },
                {
                    "sent": "OK because in either case is making no sense to particular anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, that was stopped in the tree.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So after you do the whole training, then label ranking tree looks approximately look like this.",
                    "label": 0
                },
                {
                    "sent": "OK, this look quite similar as a regression tree, so differences in the node in the leaves you got rankings instead of a real value number.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now I will show you some experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the data site.",
                    "label": 0
                },
                {
                    "sent": "All the resulting one big table.",
                    "label": 0
                },
                {
                    "sent": "We compare our local approaches, namely this local approach.",
                    "label": 0
                },
                {
                    "sent": "With this label approach and this label, ranking trees.",
                    "label": 0
                },
                {
                    "sent": "Compare them with this constraint classification.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There are two men messages here.",
                    "label": 0
                },
                {
                    "sent": "First, we found out that if the data side is rather good, namely if the rankings are complete or the mislabels and also high.",
                    "label": 0
                },
                {
                    "sent": "Then we found out this Eastern based label ranking is significantly better.",
                    "label": 0
                },
                {
                    "sent": "The second observation is we found that we found out that the size of the label ranking tree is not that big.",
                    "label": 0
                },
                {
                    "sent": "Here you see the column.",
                    "label": 0
                },
                {
                    "sent": "After this labor entry, it is the relative size of the neighboring country compared with the conventional decision tree which is trained only on the top label.",
                    "label": 0
                },
                {
                    "sent": "OK, we found the size is not that big this.",
                    "label": 0
                },
                {
                    "sent": "Rather go see.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is a typical learning curve of the local approach compared with this constraint classification.",
                    "label": 0
                },
                {
                    "sent": "OK. We found out that our local approaches, our local metals are more flexible.",
                    "label": 0
                },
                {
                    "sent": "So it can exploit more information preference information.",
                    "label": 0
                },
                {
                    "sent": "Compared with this model based approach, while this model based approach has somehow stronger buyers, you see this so lack of flexibility.",
                    "label": 0
                },
                {
                    "sent": "OK. And I pray.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down here, here is some man conclusion.",
                    "label": 0
                },
                {
                    "sent": "We have introduced an instant based approach for label ranking where we using public model, namely the models model and once you give this label ranking problem full probabilistic treatment then you can gather a lot of useful information.",
                    "label": 0
                },
                {
                    "sent": "For example this setup parameter.",
                    "label": 0
                },
                {
                    "sent": "OK, so this setup parameter can tells you how reliable your prediction this.",
                    "label": 0
                },
                {
                    "sent": "OK, and our approach can deal with complete and as well this incoming ranking as well.",
                    "label": 0
                },
                {
                    "sent": "We can deal with both case.",
                    "label": 0
                },
                {
                    "sent": "There are of course some filter work we want to do, namely is this.",
                    "label": 0
                },
                {
                    "sent": "Approximation for this in company ranking case we want to make this inference more efficient.",
                    "label": 0
                },
                {
                    "sent": "This is rather complex problem.",
                    "label": 0
                },
                {
                    "sent": "There's always space to improve.",
                    "label": 0
                },
                {
                    "sent": "The second is to approach.",
                    "label": 0
                },
                {
                    "sent": "I proposed here.",
                    "label": 0
                },
                {
                    "sent": "Is not only useful for labor ranking problem, but it can solve other type of problems as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so we also would like to see what we can do in the future for other type of problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is all from my side and if you have any problems I would like to answer now, thanks.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time for questions.",
                    "label": 0
                },
                {
                    "sent": "And since we're videotaping, can we just make sure the speaker that you repeat the questions so that I will try that if I understand?",
                    "label": 0
                },
                {
                    "sent": "So there are two ingredients in your methods.",
                    "label": 0
                },
                {
                    "sent": "That one is maximum likelihood and over the other one is Decision Tree, which are often considered to be proved overfitting.",
                    "label": 0
                },
                {
                    "sent": "So I was wondering whether you observe that or whether the Spectre problem kind of avoided.",
                    "label": 0
                },
                {
                    "sent": "In fact, that's where beginning because we don't have our explicit pruning strategy for the tree, so we somehow expect that the size of the tree is rather big.",
                    "label": 0
                },
                {
                    "sent": "OK, I think it is a question right?",
                    "label": 0
                },
                {
                    "sent": "Well that was for the decision.",
                    "label": 0
                },
                {
                    "sent": "OK, but but in fact it turns out that the size of the tree is not as big as we imagine.",
                    "label": 0
                },
                {
                    "sent": "Around here you see here this relative size of the tree is rather say, compatible with the normal decision tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so the overfitting is not obvious.",
                    "label": 0
                },
                {
                    "sent": "Not that obvious in the experiments.",
                    "label": 0
                },
                {
                    "sent": "As well As for the nearest label approaches, because in the nearest labor approaches you just have to control how big your neighborhood is.",
                    "label": 0
                },
                {
                    "sent": "OK, if you have good control of the neighborhood, then is also not that likely you over fit your data.",
                    "label": 0
                },
                {
                    "sent": "Does that also question?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now please.",
                    "label": 0
                },
                {
                    "sent": "So I was wondering why, why did you choose to use a mellophone?",
                    "label": 0
                },
                {
                    "sent": "Why do you think that's a good?",
                    "label": 0
                },
                {
                    "sent": "OK so question is why God uses models model?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all that this models model is kind of the standard or say Golden standard in statistics to dealing with the ranking data and there are already a lot of work had been done on this models model.",
                    "label": 0
                },
                {
                    "sent": "This is the most important reason, but of course you could also use some other type of models for dealing.",
                    "label": 0
                },
                {
                    "sent": "Ranking data is not a problem here.",
                    "label": 0
                },
                {
                    "sent": "You could also try some other model of course as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so is that OK, cool.",
                    "label": 0
                },
                {
                    "sent": "Clarifying your right.",
                    "label": 0
                },
                {
                    "sent": "Requirements basically come back.",
                    "label": 0
                },
                {
                    "sent": "Any loss metric that you have can't be spending on the right position, so you can actually have a model that where you want to optimize something or the higher rates for more.",
                    "label": 0
                },
                {
                    "sent": "Well, this.",
                    "label": 0
                },
                {
                    "sent": "This is fairly interesting question.",
                    "label": 0
                },
                {
                    "sent": "The question is if we is it possible to use some other type of loss function right?",
                    "label": 0
                },
                {
                    "sent": "Is that you?",
                    "label": 0
                },
                {
                    "sent": "OK, you mean it means that it possible that higher rank object has a high importancy OK?",
                    "label": 0
                },
                {
                    "sent": "So so far we don't have considered such kind of loss function.",
                    "label": 0
                },
                {
                    "sent": "Because as you'll see here in this model's model.",
                    "label": 0
                },
                {
                    "sent": "We have used this kind of distance.",
                    "label": 0
                },
                {
                    "sent": "So this kind of distance doesn't have any pretending importancy on any particular rank, but this is definitely something we want.",
                    "label": 0
                },
                {
                    "sent": "We want to look into, right?",
                    "label": 0
                },
                {
                    "sent": "If you have another loss function.",
                    "label": 0
                },
                {
                    "sent": "Which yeah, I think this works fine, but so far we treat all the rancor over position equally.",
                    "label": 0
                },
                {
                    "sent": "I had a question.",
                    "label": 0
                },
                {
                    "sent": "Yeah please.",
                    "label": 0
                },
                {
                    "sent": "In your results you you mentioned that if you go outside you mentioned that the for the when there's a lot of data, not much missing data.",
                    "label": 0
                },
                {
                    "sent": "IDL method performs.",
                    "label": 0
                },
                {
                    "sent": "Well yeah, but just glancing at the slide, there's a lot of variation.",
                    "label": 0
                },
                {
                    "sent": "There are some datasets that's true, some datasets it's not true.",
                    "label": 0
                },
                {
                    "sent": "I have any thoughts about what are the properties of the domain or the data that might cause well.",
                    "label": 0
                },
                {
                    "sent": "Work better than another if you are talking about a particular algorithm is definitely that.",
                    "label": 0
                },
                {
                    "sent": "Maybe on some data set is better.",
                    "label": 0
                },
                {
                    "sent": "Some datasets good.",
                    "label": 0
                },
                {
                    "sent": "So when I say this local approach is significantly better is in terms of statistical tests OK?",
                    "label": 0
                },
                {
                    "sent": "We pick a lot of random data set and we just do this freedom test.",
                    "label": 0
                },
                {
                    "sent": "So question is if I have a particular data set, is there a way I can tell for this kind of data?",
                    "label": 0
                },
                {
                    "sent": "Maybe it's sparser, maybe it's different kinds of features.",
                    "label": 0
                },
                {
                    "sent": "My guess is here that, for example, if you have using construct application and if you have because this concerns classification is strongly say media like model.",
                    "label": 0
                },
                {
                    "sent": "So if you have a very non linear data side our expect that our models works much better than just see.",
                    "label": 0
                },
                {
                    "sent": "So it also depends on how lonely it is.",
                    "label": 0
                },
                {
                    "sent": "That is OK, but especially I guess normally in the real world application the real data site is rather nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Is my guess.",
                    "label": 0
                },
                {
                    "sent": "OK, so any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}