{
    "id": "aqxcmqx64ge3g7z7d6k6lyqr3ezcqzyx",
    "title": "Mining All Non-Derivable Frequent Itemsets",
    "info": {
        "author": [
            "Toon Calders, Department of Mathematics and Computer Science, Eindhoven University of Technology"
        ],
        "published": "Oct. 29, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Data Modeling",
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2012_calders_frequent_itemsets/",
    "segmentation": [
        [
            "I will give an overview of this work and also tell a little bit about what other work has come after."
        ],
        [
            "About non driveable item sets.",
            "But before I go into the content of our paper, first I will sketch a little bit.",
            "The area of frequent itemset mining in 2002.",
            "So the year when the paper.",
            "Was published and there are two important aspects I will talk about.",
            "The first one is a pattern explosion problem that we realized back then was it was becoming a real problem in frequent itemset mining and the other one is about condensed representations.",
            "So to counter this pattern explosion problem, condensed representations have been proposed.",
            "After that I will go into the non derivable item sets so that our content of our paper actually and some extensions that we made to it in KDE.",
            "At 2005 and in same data mining conference, an after that I hope there will be still some time left.",
            "I will also tell something about some more recent approaches towards non redundant pattern mining."
        ],
        [
            "So first it all started out in 90, three with the definition of the Association rules by these three people.",
            "And you can see the paper below.",
            "As you can see, it's cited by almost 12,000 people, and in this paper and the Association Mindrolling problem was first introduced.",
            "I will just very quickly go over it, because I assume all of you know this problem, so you assume that there is a database with transactions.",
            "The transaction is just a set of items.",
            "So here we have four transactions.",
            "For example transaction 300 as items 123 and five.",
            "And now we want to find combinations of items that are frequent.",
            "Now and not in their 93 paper, but one year later they introduced the Apriori algorithm.",
            "Which basically first looks for L1, the set of large item sets of size 1, so the frequent items, all four of them are selected.",
            "Then based on these L2 is made and after that L 3 and then we have found all frequent itemsets, in this case with the threshold of two.",
            "After this step will go to Association rules, so we split it up into.",
            "Then we split up the item sets into rules with that meet support and confidence thresholds."
        ],
        [
            "So the situation in early 2000s, so just before the paper is Association rule mining gained enormous popularity.",
            "And here you can see a graph about the citations of this 93 paper.",
            "And how it's going up during all these years so that it became more and more popular.",
            "And there are literally hundreds of algorithms being proposed.",
            "There are some listed there.",
            "Some of those actually are from after after 2000 with anyway, just to give you an idea, there was a huge explosion of algorithms for mining the frequent itemsets."
        ],
        [
            "There were even competitions, so this is of the Femi competition.",
            "Frequent itemset mining implementations.",
            "I think it's actually from 2003, which was a workshop that was organized Co organized by, but hotels for example, and there there the goal was to make the fastest items at minor, so given certain minimal support threshold, and here you see it on the horizontal axis, you see the minimal support and the goal was to mine all frequent itemsets with that threshold as quickly as possible.",
            "Here you see the different algorithms and algorithms are listed in the Legend on the right and on the vertical axis you see on a logarithmic scale.",
            "The total time that it takes.",
            "Now if you look at this type of competitions, this is actually from the mushroom data set is one of the graphs.",
            "Then you see that on the left hand side with the threshold of 2002 thousand, there are actually 50,000 patterns being generated.",
            "On the rightmost threshold there are over 10,000,000 patterns being generated.",
            "Now, if you look at the mushroom data set and this data set has 8124 transactions and the transaction length of 23.",
            "Now if you think about your original goal in original 93 paper, finding these small Nuggets of gold in this huge amount of data, and then we're actually doing exactly the opposite, there were other small data set and we're finding a huge amount of patterns in our data set."
        ],
        [
            "And this was this is called the pattern explosion problem and clearly this is a huge problem because the output is 10,000,000 patterns without any post processing or actually useless.",
            "Now what is the reason for that?",
            "As you probably meant of many of you already know, is that many of the items had so redundant, it has to be purely on information theoretic for purely for information theoretic reasons, and you have given some examples for Association rules and where you can see some redundancy going on 1st one.",
            "If smoker implies lung cancer.",
            "So if you're a smoker, have a high probability of getting lung cancer if this is.",
            "A frequent rule with high support and confidence, the most likely also the rule smoker and Bolt implies lung cancer.",
            "Just because you added one extra item to the antecedent of Ja Rule doesn't make the confidence of your rule lower if they are not correlated to confidence of your rule remains the same with clearly this is redundant.",
            "Another type of redundancy, if someone is pregnant then it's a woman, and this also usually holds, but we know this of course.",
            "This is this background knowledge that we already have, but also infrequent items that mining the most frequent and confident rules will usually be exactly those type of rules.",
            "And then of course you also get combinations of different rules that hold if someone is pregnant.",
            "Anna Smoker, then it's probably a woman and there's a high probability that a higher increase probability that the person will get lung cancer."
        ],
        [
            "So this.",
            "Python explosion problem leads to the development of condensed representations.",
            "And the first one is the closed itemsets representation.",
            "Now look at this particular data set, which is very well structured.",
            "We have actually 3 tiles or three blocks of once in our data set.",
            "If you look at the frequent itemsets, if the threshold is 1, the number of frequent itemsets here is 21, and so clearly we need some kind of compact representation of these items.",
            "It's because actually you can just describe them by their three blocks of once in our data set.",
            "And this is exactly but in 1999, the first condensed presentation, as far as I know, proposed and the closed itemsets mining discovery discovering frequent closed itemsets paper by passkey and others.",
            "At this they proposed to commence representation that solved this particular problem, and so this is this is a scheduling situation.",
            "In 2002 we have this huge explosion pattern explosion we want to solve it one way or another and one way is to come up with condensed representations."
        ],
        [
            "Our physicalness representation is actually just a compressed version of the collection of all frequent itemsets, and usually we also required that they were lossless.",
            "So this means that they allow for lossless regeneration of all frequent itemsets, so they still contain another information to recover all the frequencies.",
            "So some proposals that existed at that time, so the close item sets free item sets, Politico and others disjunction free item sets of bikowski and rigotti.",
            "It was proposed that database conference spots."
        ],
        [
            "So this is a sketch of the situation back then, and this is also if you want to judge our paper and this is the situation you have to judge it against."
        ],
        [
            "So let's now go to the paper for this.",
            "In 2002 this was the cover of our presentation back then and how we looked back then 10 years ago.",
            "Centuries."
        ],
        [
            "And so this is some of these slides are actually from the 2002 presentation.",
            "So the questions that we asked Dennis how do supports interact with each other?",
            "You have on the 100 the multiplicity principle, so you know if you extend an item set with some items it supports will go down.",
            "So that's one way of interacting.",
            "If you have a smaller item set and a larger one, you always know that the larger one will have lower support then the smaller one.",
            "A second question is what information about unknown supports can we derive from known support?",
            "So suppose you're mining your data set.",
            "You've already found the frequency of some item sets, and now you want to find out.",
            "OK, what can I derive for the frequency of some other item sets that I have not counted yet in my database?",
            "Based on this, we want to make a condensed representation or concise representation by only storing the relevant part of the support, only storing some support of some item sets, and not storing those items sets of which you can derive the support perfectly."
        ],
        [
            "So first we studied a little bit.",
            "What type of redundancies were already exploited in other papers and the first one is obviously the monotonicity principle already introduced in 90.",
            "Three by Rakesh Agrawal and orders so that the support if you extend an item set with an extra item you support will go down.",
            "There will always be less people that buy A and X.",
            "Then there are people that buy a then if you look at the closed and the free item set representation.",
            "Then the rule they are actually using is this one with the support of A equals the support of a B.",
            "Then the support of a X will also equal the support of axb.",
            "So how can you see this?",
            "If every transaction that contains a also contains B?",
            "Because that's actually what it means that they have equal support.",
            "Then also every transaction that contains A and X will also contain B, so that's why and this second rule also holds.",
            "So in other words, if in your data set while mining you find out the support of A equals support of a B.",
            "Then there is no longer a need to count the support of axb.",
            "For example, and you can just count the support of a X and then add B to it in.",
            "The support will remain the same and then the closed itemsets will exactly be those.",
            "The ones with the maximal number of items in an equivalence class, but in equivalence classes that they have the same support."
        ],
        [
            "Then we also had Max Miner algorithm of biardeau.",
            "And he was using the the rule that is given here.",
            "Support of ABX is always larger than or equal to the support of a X minus the support of X plus the support of BX.",
            "So how can you see this actually is lossed he called the drop X, B.",
            "So if you have an item set X and you add item be to it, how much will IT support drop?",
            "How many transactions do contain X but do not contain B and that's exactly the support of X minus the support of BX.",
            "So if you now look at the support of a X, how much will IT support decrease if I add item B to it?",
            "The number of transactions that contain A and X but do not contain B?",
            "Is smaller than the number of transactions that contain X, but do not contain B, so if you subtract the drop from it actually get a lower bound on the support of ABX and this was exploited in the Max miner algorithm because they want to mine the maximal frequent itemsets and so finding a lower bound on the frequency or the support of itemsets was very useful, and because then if you know that items that will be frequent or not mining all frequent itemsets you're not interested in the frequency.",
            "If item sensors are not maximal, and then you can use this just to jump ahead and to go already one level further up.",
            "Then the last.",
            "Paper the last one about the reasoning about frequencies that existed was of Bikowski and rigotti, and they used that if the support of ABC and this is the disjunction free item sets in the support of ABC equals the support of a B + a C minus in support of A, then you can also find a similar relation for the support of ABCX.",
            "So how can you use this if you're mining all frequent itemsets and in every step you just check if this equality holds?",
            "If it holds, then there will no longer be a need to counter support of ABC X, because you can derive it from its perfectly from its subset.",
            "So if you're applying in a pre like algorithm just check for item sets.",
            "Once you find this relation, once a set is no longer disjunction free, you can skip.",
            "Counting all its super sets in the database."
        ],
        [
            "So actually the paper started out as being a theoretical study about this redundancies in between item sets, and as you can see it there already existed alot of scattered rules that were being exploited successfully to make condensed representations or to mine the maximal itemsets and we actually wanted to see started out my PhD research by trying to find out how can we find something more general that emcompasses.",
            "All these different rules, all these different proposals into one set action motivation, one set of deduction rules for the frequency of item sets, and that's where the non derivable item sets in the end came from.",
            "And we saw applicability in frequent item set mining because if you can derive perfectly the support of an item set without going to the database, you can skip.",
            "Maybe some costly scans of your database.",
            "And also condense concise representations.",
            "'cause if an item set, if you know IT support perfectly based on support of some other item sets, there is no longer a need to store them and you because you can just derive them and we will not present them to the user.",
            "So that was our solution to the pattern explosion problem."
        ],
        [
            "So we found deduction rules for the inclusion exclusion principles.",
            "From this we come to the rival item sets the algorithm to find non drivable item sets, and also show some evaluation and some conclusions and further work."
        ],
        [
            "So the inclusion exclusion principle, or at least one of the benefits from it, is as follows.",
            "Suppose here we have our database D and now I took a slightly different representation.",
            "I have three items in my database item AB&C, so these are the products that people come by.",
            "And now represented them as a set.",
            "So A is a set of all transactions that do contain item AB.",
            "Everything contains item B, ansi.",
            "Everything that contains items C. Now in this representation, the support of ABC is a number of transactions that here or here in the middle of my diagram, and these are all the transactions that contain both items AB&C.",
            "As you look at this particular fragment of a diagram, all the elements that are in a but not in B and not in C. These are all the transactions that contain item A, but not item B and not item C. Now the size of this set we can estimate.",
            "Or we can.",
            "Computers follows the support of A not B, not C. It's just a number of elements that are in the set A.",
            "Then we subtract the number of elements in a B which is this set.",
            "We should track the number of elements in a C as this set and but then we subtract it ABC twice.",
            "So we need to compensate and add ABC to it again and so here you have this relation between the supports and this what we called a generalized item set.",
            "Now.",
            "This part needs to be larger or equal to 0.",
            "Then there cannot be a negative number of elements in this partition.",
            "And so actually end by take it by making the right inside larger than or equal to 0 and shifting the ABC or shifting the other terms to the other side.",
            "We actually get this relation between the supports of the items.",
            "It's always in every data set.",
            "The support of ABC will be larger equal to the support of a B plus the support of a C minus the support of a.",
            "And is actually already has some similarity with this rule of bikowski and others where they had the disjunction free item sets, and there this set of this one isn't equality.",
            "Then for my larger item sets this one will also be inequality.",
            "Let's now see what happens if we would extend ABC even element D. So if this is an equality.",
            "It means that this one is not larger or equal to 0.",
            "It is 0.",
            "So if we have a not be not see, we extend it with not D. The number of transactions that contain a not be not seen or D will be smaller than or equal to the number of transactions containing a not B, not C. So this will also be zero and so also at the next level.",
            "For ABCD this equality will hold.",
            "This one will be 0 and we will get the same equality.",
            "So we already succeeded in finding another way of deriving this rule for the disjunction.",
            "Free item sets.",
            "Now you can do this, not only.",
            "With this partition you can do it with all of them with all eight of them."
        ],
        [
            "So let's look at the number of transactions that do not contain a, not contain B, not contain C. So then we just take all transactions the support of the empty set.",
            "We subtract everything that contains a everything that contains B.",
            "Everything that contains C. But then we subtracted too much.",
            "We subtracted AB twice, AC twice BC twice, so we need to compensate for that.",
            "But after doing that, actually we have.",
            "ABC.",
            "One time too many, so we need to subtract it.",
            "So here you see the inclusion exclusion some going on.",
            "Again, this one is larger than equal to 0.",
            "And again we get a rule, and this is actually a new rule that was not known before that just by applying the same principle but for another generalized item set.",
            "And we could find this new rule about how frequencies, how supports interact in our database.",
            "Again, we can exploit it in exactly the same way as disjunction free item sets.",
            "If this rule is an equality, and this is really zero, then for larger item sets, this one will also be zero, and so if we find that this equality that this is perfect.",
            "And then we know that we do not need to explore the supersets anymore."
        ],
        [
            "So one more one last, one support of a B, not C equals the support of a B minus the support of ABC.",
            "Larger equal to 0.",
            "And the support of ABC is larger than or equal to the support of a B and this we known know very well as the monotonicity principle.",
            "So."
        ],
        [
            "As I told you ABC, we have three items, so in our diagram have eight places, so this results in eight times that we can play the same game and eight rules that make for an item set.",
            "ABC.",
            "As you can see, if your if your items that grows the number of rules that you have for these items that will grow exponentially with the size of the item set.",
            "So these are all the rules for ABC.",
            "First one is trivial and it needs to be larger or equal to 0.",
            "It is also follows from that.",
            "Then we have.",
            "The monotonicity principle at the next level we have the rules that are being used for this disjunction.",
            "Free item sets and then in the answer with this last rule in this new rule.",
            "Now if you go to item size 4567 and then at every level here we have level 0 one and two, those are known the rules at level three were not known at that time.",
            "If you go to larger items that you also get rules at level 4567, and these were all new rules for deduction of the support.",
            "Now this is very nice, of course, so we found a whole lot of new rules that we could exploit to make condensed representations."
        ],
        [
            "And so, here is an example of how we could we could use it.",
            "So we're just doing the monotonicity, but just applying the Apiri algorithm first accounts and I will just go until a certain level.",
            "We counted the empty set."
        ],
        [
            "The six ABC"
        ],
        [
            "ABA CBC so now before we go and count the support of ABC."
        ],
        [
            "We can."
        ],
        [
            "Apply the rules.",
            "So we find a lower and upper bound for the support on ABC.",
            "Now in this particular case, a lower bound is 0, upper bound is 1, so in this case actually we cannot use it because our upper bound is still exceeding or minimal support because we accepted AC.",
            "So our minimal support is 1, so we cannot use it to prune our search space, but you can imagine if your upper bound is below your threshold.",
            "You can exclude the rule already on before hand, little bit like the multiplicity principle with an extended if your lower bound exceeds the minimum support threshold, they can do the other way around.",
            "You don't need to count it because you know for sure that will be frequent, and if the lower bound equals upper bound there is also no need to count it any longer, because then it's perfectly derivable."
        ],
        [
            "So then we come to the main theorem over paper.",
            "So suppose that you're in this situation in which you are when you're doing a priority, so we are interested in items at J and we know the support of all its subsets.",
            "So for ABC we know the support of all its subsets, but we do not know set ABC itself in that case, or deduction rules are sound complete, unknown, redundant for deducing upper and lower bounds on the support of J.",
            "So what does it mean?",
            "And they sound OK, so there all the rules are correct.",
            "And with that we already saw, by the way we derived the rules.",
            "They are complete.",
            "So this means that if the bounds that we find by the rules or LUOL is a lower bound and use upper bound, then there does exist a database which is completely consistent with all the supports of the subsets and the support of our side J is equal to L. And we can also find the database DU which is fully consistent with all the with all the support that we know and the support of J is equal to you.",
            "So no matter what you do and there is no more space for improving upon these derivation rules, in the context that you know the support of all subsets of an item set J.",
            "So unlike all the previous works where they proposed some of the rules and there might be some more rules out and here we have proven that in this particular situation there are no other.",
            "Rules there.",
            "The rules are nonredundant, and by this I mean that you really need every rule.",
            "Because you can always think of situations for every rule you can think of situations where you really need this particular rule where no other rules will give you a better lower or an as good, lower or as good upper bounds."
        ],
        [
            "So here's a small example of the completeness, so here this is the same example we had the lower bound to zero and an upper bound of 1.",
            "So this means that there must exist databases which are completely consistent with the information that we know into which the support of ABC is zero, and also a database in which is support of ABC is 1."
        ],
        [
            "Clearly this is our date or upper database.",
            "Support of ABC is equal to 1 and all the other supports or satisfied."
        ],
        [
            "And if we change a little bit attribute, see, it's still consistent with all the information that we have.",
            "So given the information that we have both DL or DU could be could be the underlying database.",
            "We don't know based on these supports.",
            "And here ABC has a support of 0, so our bounds are tides and based on this information we cannot better bound to support of ABC."
        ],
        [
            "So from this set of consulting complete deduction rules, we went to the derivable itemsets.",
            "So again, had this condition that you saw in the theorem, we suppose that we know the support of I for all either strict subset of J.",
            "So we can compute with our deduction rules lower bound on the support of J in an upper bound lnu.",
            "So then without counting we know that the support of J must be in this interval and an item set is called the derivable item set.",
            "FL equals U, so we know the support of J without having to count it in the database."
        ],
        [
            "So there is no need to count the support of J in such a situation, and this week all the derivable item set there is no need to store it.",
            "Enter concise representation.",
            "That we proposed is."
        ],
        [
            "Like this?",
            "So our set contains all the items that she and their support for which it holds that J is not derivable at those we cannot derive, so we need them in our representation.",
            "They are not derivable from all their subsets.",
            "And you can prove that this is a lossless representation of all frequent itemsets."
        ],
        [
            "Again.",
            "Let me speak an example."
        ],
        [
            "Where the lower and upper bounds are equal.",
            "So in this particular case, what you see here, we have a lower bound and upper bound to support of ABC smaller than or equal to 1, and this because of this one monotonicity principle, we also have a lower bound.",
            "Support of ABC is large in area code 21 plus is a so called overlap rule.",
            "It's the support of a B plus AC minus support of B, so we also get a lower bound of one on the support of ABC.",
            "So in this particular situation.",
            "No need to count ABC in our database."
        ],
        [
            "So then algorithmics, how can we find this nondurable items efficiently in a database?",
            "For this we had two additional results.",
            "The first one is monotonicity.",
            "If Jay is a subset of key and Jason derivable item set, then K is also derivable and clearly this is a very useful theorem because this means that if you find and derivable items that you can stop exploring the whole 3 above.",
            "This derivable item sets and the reason for that is obviously a little bit like I told you, with the disjunction free item sets Eva set is derivable.",
            "It means that one of these generalized item sets and for example a not B not C. Is zero and also the other direction holds.",
            "Then if you go to a larger item set and by adding items.",
            "So actually you're splitting up a cell which has zero transactions in it, you're splitting a not B, not C. Splitting it in a not being on CD and a not be not seen on D, so those two will also be zero, and so that's why this derivability or this will be monotone.",
            "So being derivable is anti monotone.",
            "If you find the rival item set.",
            "You can stop exploring there.",
            "And the second theorem.",
            "And because you can.",
            "OK, it's very nice.",
            "All these reduction rules with how much does it actually prune.",
            "So there we had this whole thing theorem and this says that the width of the interval for the items that J unit when you add item 82 it is at most half of the size of the interval for J.",
            "So at every step.",
            "So in the first step for the items you don't know anything about the database except that it has any items, so the lower bound is 0, the upper bound is North.",
            "Four items of item sets of size 2.",
            "You know that the size of this interval will have at least, so the difference between lower bound and upper bound will be at most N / 2 Level 3 and over 4 an over 8 and so on.",
            "So in other words the linked of non derivable item sets is at most the logarithm of the size of your database.",
            "So if you have a small database the size of your items non derivable item sets will be limited.",
            "Whis is also useful.",
            "As I told you before, the number of derivation rules increases exponentially with the size of your non derivable item set.",
            "So you can easily imagine a situation where evaluating the rules will become more complex than just counting the items in the database.",
            "Now this theorem is actually saying that this will never happen because the size of your items it is at most the logarithm of the database, they the exponent of that.",
            "So the effort of computing and arrival itemsets and computing the support.",
            "Of a derivable item set will always be at most the.",
            "The effort of counting it in the database and in all cases in reality is far less."
        ],
        [
            "So then the algorithm this is based on 2.",
            "Principles and non derivability is monotone.",
            "So we just choose used level wise appear like search algorithm.",
            "We also made the depth first version later on that we proposed at Siam Data Mining Conference in 2005.",
            "The card reader is that OK for computing whether another set is derivable or not?",
            "Arrival you need to support of all its subsets.",
            "Now if you do it in a priority this is trivial to get to have the support of all subsets.",
            "When you're evaluating an item set in depth first traversal, this is less trivial, but doing reverse depth first traversal, we can actually guarantee that even doing depth first that we evaluate a set only after all its subsets have been evaluated.",
            "So that's just another way to do it that we proposed in Siam data Mining conference.",
            "And then we will only count an item set in the database if all its subsets or frequent otherwise.",
            "We can prune it because of frequency threshold and the bounce cannot derive the support exactly."
        ],
        [
            "So then there was just a small optimization that we could still do.",
            "If you're bound to an item sets, I equals either the lower bound or the upper bound, then we know that all the supersets or derivable, so it can very well be that an item set itself is not an on derivable, but it's true support after if counting and database.",
            "It turns out that the support is equal to its lower bound or it's upper bound, and in that case you can also stop exploring the.",
            "Super fits because they will all be derivable."
        ],
        [
            "So then one of the bottlenecks of the algorithm remained this computation of all these lower and upper bounds is exponential number of lower and upper bounds.",
            "So later on in 2005, we came up with the quick inclusion exclusion principle, and this principle is actually based on the observation, and this is this is not a new observation or the other people saw it before machine learning.",
            "There is also a paper that exploits this relationship in order to compute.",
            "The support of all these generalized item sets, but if you look at these sums that are used to generate our rules and they share a lot of terms.",
            "So if you look at the support of a B not seen on D, it's equal to this.",
            "Some support of a not be not seen or D it's equal to this sun and it contains this some internally it contains this.",
            "If you first compute the first one, use this result.",
            "In the second one then you can save 3 operations.",
            "Yeah, in computing the support of a not B not seen or D. Um?"
        ],
        [
            "So the quick inclusion exclusion principle is based on this to more quickly compute the lower and upper bounds is actually based on this principle.",
            "So if you look at the support of a generalized item set, the item sets not a G, where G is just any journalized item set is equal to the support of G minus the support of AG.",
            "So what we're actually doing in the quick inclusion exclusion principle is we make an array that contains first in the first step just contains the frequency of all item sets.",
            "Then in the second step, we will negate item A.",
            "Item C for all items sets that do not contain item C, So here we do not have a B.",
            "So after the first step the support that will be stored here is the support of a B, not C. How do we compute it by from this cell we subtract what is in this cell.",
            "So then we get the support of a B, not C. What is in this?",
            "So from this we subtract for this in this cell to get a Nazi and again here we not see and hear Nazi.",
            "So after one step.",
            "And doing one operation for half of the entries and we have added the negating the negation of C. So the next step we do the same but now for B.",
            "So half of the items that do half of the sets of the entries do not contain item B, so there will not be in the same way.",
            "So for this cell it's a C minus ABC look at a not BC and so on.",
            "So here we get the negation of all the bees and then in the last step we also add the negation for a.",
            "So this is a fast way in theory."
        ],
        [
            "Is a graphical representation of the operations that we need to perform.",
            "If you have N items, we will have N. Cycles.",
            "How many steps do we need to take?",
            "How many operations is every step?",
            "I just half of the entries need to be updated so it's 2 to the power N -- 1.",
            "So if an algorithm of complexity N * 2 to the power N -- 1 versus if you would do it, brute force, you would get an algorithm of complexity 3 to the power of N. So it's it's not dramatic, but it's an improvement.",
            "So in this way, and this algorithm also allows us to compute this lower and upper bounds much more efficiently."
        ],
        [
            "So let's now move to the evaluation.",
            "So how good is it actually in practice?",
            "So we did extensive comparisons with other condensed representations, and we also looked at the influence of the rule depth, so there were many rules an exponential number of rules for every item set, but obviously you do not need to compute every rule.",
            "You can also say, OK, I'm happy with only doing the monotonicity principle and only doing the disjunction free.",
            "Optimization, then you get to discern truth free.",
            "Condensed representation or you can go to Level 3 level 4 in order to get rid of this exponential blowup of the number of rules.",
            "Because if you have very large item sets that are non derivable and it will be a whole lot of rules that you need to evaluate so you can restrict to a certain depth and there the question was OK. How much influence do we get from rules of larger depths?",
            "Maybe you can just stick with the smaller rules."
        ],
        [
            "So first the first one this I took from the Journal version of our email PKD paper.",
            "It's appeared in data mining Journal in 2007, so it took some time for us to make the Journal version.",
            "Yeah, what you actually can see here is that the closed itemsets in this particular graph on the horizontal axis you see the minimal support that is being very right on the vertical axis.",
            "You see the number of frequent item sets in a different representations at hearing this particular datasets that BM, SFU one, and closed itemsets are here.",
            "And the non driveable item sets.",
            "You're actually here in this junction free item sets.",
            "They are here.",
            "So why is there there is difference between the different representations?",
            "This actually comes from an because we just explained you that we have all the rules that exist.",
            "Also, the rules that were used in your other algorithms.",
            "So why aren't we better?",
            "And this has to do with the way that we explore that we exploit the different rules.",
            "So for us we do not store an item set, only if it can be perfectly derived.",
            "And in this in the sense of.",
            "Meaning the logical sense it cannot be.",
            "There cannot exist the database that is consistent with our condensed representation and to support is different.",
            "Now, in this order representations, they take some other rules.",
            "They say OK, we do not include an item set in our representation if IT support is equal to the upper bound.",
            "For example, if you're interested in that in how this relation between the different condensed representations is and how we can extend the non derivable item sets.",
            "And then you need to look at our paper one year later at EML PKD 2003 where we also exploit this.",
            "Adding these extra assumptions.",
            "To further reduce our collection.",
            "Well, so here in this case, closed itemsets sort of clear winner."
        ],
        [
            "Here is another data set.",
            "Connect four and here we perform best with an undeniable item sets in the closed itemsets are much worse."
        ],
        [
            "NT is another data set and we also perform better than the closed itemsets, so in general, how is the relation between the different condensed representations sometimes a closed itemsets are better sometimes, and unreliable item sets are better.",
            "They are just performing equally well later on.",
            "Also, at least not Wikipedia.",
            "I think.",
            "I think even two.",
            "I don't know.",
            "The year 2008 2009 someone also proposed the non drivable closed Itemsets, which exploits both Nondurable itemsets deduction rules, ends.",
            "The closed itemsets and that perform better every day.",
            "You can show that are always better than than any of the two."
        ],
        [
            "Then the influence of the rule depth is just one particular data set where we looked at rules of step one.",
            "This is only using monotonicity principle rules of depth.",
            "Two disjuncture free item sets, rules of depth 3, four and so on.",
            "And here you actually see that the more complex the rule become.",
            "In these datasets.",
            "The less the reduction in number of frequently unreliable itemsets becomes.",
            "So actually we can stick to rules of limited complexity and still gets the same reduction of our collection."
        ],
        [
            "So the conclusion of the evaluation, the number of frequent non derivable item sets is considerably smaller than the number of frequent itemsets.",
            "I didn't throw a graph here because for all the experience that I did, it was possible that I showed here.",
            "It was possible to make the condensed representations, but it is not possible to mine the frequent itemsets at such low frequency thresholds.",
            "Most of the work is done by rules of limited depth.",
            "So we don't actually usually do not need the more complex rules, although you can show in theory that there can exist cases in which you might need them.",
            "We are in.",
            "The algorithm is efficient so calculating the non derivable itemsets plus deducing derivable itemsets is in many cases outperforms apriori.",
            "The apriori algorithm, although in 2012 this is not such a strong argument.",
            "Anymore I guess.",
            "So."
        ],
        [
            "So let me go to the next one.",
            "I still have like 10 minutes left, so this was our work for trying to reduce the collection of frequent itemsets.",
            "OK, 5 minutes left.",
            "So let's now go to some more recent approaches towards non redundant patron mining because after this non drivable itemsets and after the extensions that we made also other people have been proposed.",
            "Different ways to reduce this collection, or at least to access or to remove the redundancy from this collection.",
            "Now the overview that I will very quickly give now is extremely biased and this is because I restrict myself mainly two proposals that in my feeling also have this flavor of trying to understand how supports interact and how learns this.",
            "What does it learn us about the support of other items?",
            "It's."
        ],
        [
            "So first I want to give analysts illustrative example of tiles.",
            "So now I've been talking about frequent itemsets all along.",
            "Let's now move to tiles, finding large tiles in a data set.",
            "Is just like you can we have 01 matrix Now we're allowed to reorder rows and columns after reordering rows and columns.",
            "We want to have large box of ones.",
            "Actually, for this particular data set, so usually I let public search a little bit to see if they can find them.",
            "Actually, this data set on the left if you rearrange rows and columns slightly and actually you see that this is that data, basically change the order of the products you change the order of the customers in your transaction database well, and that is the tile you can find in this particular example.",
            "Now here it is very easy if you find this result clearly, it is well, it is significant.",
            "You don't need statistical test to see that.",
            "It's highly likely that this will pass your test that this is not random this structure."
        ],
        [
            "But in general, I suppose that we find a four by three tile and it is just an example.",
            "So the area of this style is 12.",
            "Is this significant?",
            "If our database has 22 + 6 items and all items have a probability of 0.3, so just have random data set and then it's finding such a tile.",
            "Is this actually significant?",
            "The one approach that we recently seized that so we can do is we can characterize the distribution of the maximal maximum area of tiles in random data, then compute in this distribution, how likely is it?",
            "To have a tile of size 12 or more and this then the so called P value and how extreme is the observed value under a null model expressing the background knowledge that we already have.",
            "So now you can start seeing the relation with this frequent item sets on the 100 for an old model we have a model of things expressing what we already know, maybe some frequencies that we already know some rows or column marginals in our data set that we already know.",
            "We make a distribution.",
            "That satisfies our background knowledge that expresses our background knowledge and I want to see.",
            "OK, I have this new item set.",
            "I have this new tile, how likely or unlikely is it to see this style in my data set given the background knowledge that I already have is it's it's extremely likely to see it and then you can skip it.",
            "You do not need to take it into your condensed representation at, otherwise they take it in and your updates you know model."
        ],
        [
            "So in this particular example, we can rely rely to simulation.",
            "I did not do it analytically, so I sampled over all databases that satisfy this condition.",
            "Computed empirical P value and here you see that actually a style of size 12 or more given this data is a almost 11% of our random databases.",
            "This occurs, so if you're willing to accept this as a significant result, then it means that in random data.",
            "In 11% of the cases he will be accepting.",
            "A random.",
            "Random pattern you're accepting a pattern that has a likelihood of appearing in random data 11% of the time."
        ],
        [
            "So our new model expresses our prior belief or the prior believe the user.",
            "We can compute it for all the items sets of roll the tiles.",
            "So if you look at the different works and where the difference is, it's in how to select the normal compute P values, free item sets and then rank the items that according to P value or information gain of information or information that you would gain if you would also take this.",
            "Access it into account given the time constraints, will just go quickly."
        ],
        [
            "Over different approaches, first one is of the honest that I found like really a milestone or that was really a milestone for me.",
            "The swap randomization model, where we assume that the background knowledge trees OK here, if your data set and your background knowledge is that you know how much every item was sold, how many times, and this one is sold 2 * 3 * 2 * 1 time, it's very realistic to assume that an analyst has this information and you also know how many products different the different people by.",
            "You also have a distribution over how many products do people buy and then a guy honest actually sampled uniformly over all databases with the same number of rows and columns in the same row and column marginals.",
            "So you can read this paper if you want.",
            "I'll just go to."
        ],
        [
            "Another second class that or the maximal entropy based models, or just very quickly explaining before the chair stops me.",
            "So there it is supposed that the background knowledge is that you know the frequency of some item sets.",
            "Or you know that there are some tiles in your database, so you know that there is a tile of once for certain rows and columns.",
            "Then you can make a distribution over different databases.",
            "Or you can consider your databases distribution itself.",
            "Now which distribution will you pick?",
            "There are two different proposals.",
            "One is to consider your database as a distribution.",
            "And then of all distributions that satisfy your frequency constraints, you pick the one that has a maximal entropy and why the one that has maximal entropy.",
            "That is because that is the one that contains the least amount of information actually.",
            "Or he assume that there is a distribution over databases and you pick that distribution of databases that maximizes the entropy, and that's more like what tell the Beast proposal is.",
            "The first one is more like Nikolaj Tatti.",
            "If you're interested in that.",
            "Anne.",
            "And that he considered distribution over databases where on average your constraints need to be satisfied."
        ],
        [
            "While and then third recent work and after that I stopped by from US and is there based on minimal description linked and there if you want to assess how useful a set of items sets is, you actually consider your collection of item sets as being a model for your database so you know some things about your database.",
            "And if your model is good, if the item says that you have in your collection are really relevant nonredundant item sets, and then you assume that they are also very good for compressing the database.",
            "So every items that you have in your collection you just give a code number.",
            "And if you see this item set up pairing your transaction, you replace this item set in this transaction you replace it just by this code.",
            "And if you can use this code many times, then the size.",
            "Of your model, so your collection of frequent item sets that you decided to keep as your significant collection.",
            "Plus the size of your database that you have now compressed using this model and the sum of these two will actually be smaller than your original database.",
            "So actually you're trying to find a model, a collection of items sets that minimizes this sum and.",
            "The sides to the coding length of your model plus according length of your data given the model.",
            "So here there is an explicit tradeoff that you have.",
            "If you increase the size of your model, you add some extra item sets to your significant collection at this size growth that's bad, but it might also decrease the description of your data set given the model.",
            "Enter."
        ],
        [
            "The paper that I want to mention there is the crimp algorithm.",
            "The paper proposing the crimp algorithm by Euless Matteson are know that.",
            "First proposal long before this data.",
            "Money Journal paper."
        ],
        [
            "So general conclusion, frequent pattern mining was introduced in 1993, we had hundreds of algorithms, but the pattern explosion problem, so actually in some sense you can say mining all frequent itemsets.",
            "It's somewhat wrong problem that you were studying and we were just doing it as if it wasn't all operation trying to do it as efficiently as possible, but then generating a load of patterns, many of which are actually not that interesting.",
            "So in early 2000.",
            "So a lot of condensed representations coming up nondurable items.",
            "It was one of them at the contribution of our epic ATD paper was a sound complete and non redundant set of rules to derive support of items.",
            "It's a unifying framework and a competitive condensed representation."
        ],
        [
            "And then we have more recent approaches which are more going to the statistical way which are based on the minimal description, link principle or other based on statistics with no models expressing expectation."
        ],
        [
            "So what is the future?",
            "If you want to work on this is to make these approaches more practical, because right now most of these statistically based methods and also the encoding based methods.",
            "They work only on toy examples.",
            "Now maybe I'm a little bit too.",
            "Too harsh here.",
            "But there is a performance problem.",
            "You cannot apply it on industry size datasets.",
            "Extending it to other pattern Amaze is also an interesting way to go to sequences.",
            "There already are some proposals.",
            "There's two graphs, or to dynamic graphs where we have, for example, the networks that are evolving and now you want to describe the evolution of networks have patterns for that, and then it becomes far less obvious what would be the.",
            "I like the maximal entropy distribution in the last remark very personal remark from me as a reviewer.",
            "And please please please stop making new algorithms for mining all frequent itemsets, because I simply think that we already have very good algorithms.",
            "We already have hit the bottom there, and I don't think there is any need for more algorithms for mining.",
            "Oh frequent itemsets, that's about it.",
            "So suppose I want to apply this to all the data from Safeway.",
            "All the products people buy.",
            "There's two things that I want in addition to the the inclusion rules.",
            "One is that there's an SKU for every single little stupid product.",
            "You know.",
            "The Dannon yogurt, pineapple flavor, light size, 8 ounce, and the 16 ounce, etc.",
            "So mining these is too many, so there's a hierarchy over these, so I can generalize it by Dan.",
            "And I can generalize it by size of yogurt milk.",
            "Etc.",
            "So there's a whole bunch of inclusion sets there that you might be able to leverage.",
            "Secondly, it's about profit, so there's different profit margins and all these things, so I want to have you know a real value associated with profit margin on these different subsets.",
            "Have someone already done this?",
            "Is this great future work?",
            "Is it just an application well?",
            "There has been quite some work on generalizing frequently set mining to this higher keys as you certain to mine Association rules between the higher keys.",
            "Um?",
            "So definitely there and also these different profit margins.",
            "Also there some work has been done, so most of the time it's either so for the first one if it's higher key is more in the way patterns are being generated that most work is there.",
            "So how can I mine efficiently at different levels in my hierarchy and probably also with different support thresholds?",
            "And for the second one, there have been some proposals of utility based data mining where you would actually not compute the support of the item sets which would impose some kind of utility constraint on your item sets, not express how useful your items that is.",
            "For example the the costs of the total cost of your item set.",
            "And then you try to mine.",
            "All those items that satisfies a minimal utility threshold.",
            "So there have been some works if you talk about the combination of the two and make it condensed representations for this utility based item sets.",
            "I'm not aware of any works there and I don't know if it will be easy to generalize the frameworks there, but it could actually be.",
            "A nice way to future research.",
            "There were, yeah, so mining frequent itemsets is somehow related to privacy considerations.",
            "Say for example K anonymity requires you have at least K objects which you don't know where you are not able to separate them.",
            "There are some connections, or is there some research on that topic combining itemset mining with privacy issues?",
            "Well, in one of my papers I've mentioned this is a.",
            "Possible application of this derivation rules and obtaining the sets in the sense of OK.",
            "Suppose you do mining on an item set and you want to publish the results.",
            "For example, the Federal Bureau of Statistics want to publish some of the statistics about their data, which corresponds roughly to the frequencies of item sets.",
            "And now you want to know by publishing.",
            "These statistics, but to actually reveal about my data set.",
            "So in that sense, if you look at outputs anonymity, how much does my output reveal from my data set itself?",
            "There it can vary in that context, it can be helpful, yes.",
            "OK, thanks for the nice talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will give an overview of this work and also tell a little bit about what other work has come after.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About non driveable item sets.",
                    "label": 0
                },
                {
                    "sent": "But before I go into the content of our paper, first I will sketch a little bit.",
                    "label": 0
                },
                {
                    "sent": "The area of frequent itemset mining in 2002.",
                    "label": 0
                },
                {
                    "sent": "So the year when the paper.",
                    "label": 0
                },
                {
                    "sent": "Was published and there are two important aspects I will talk about.",
                    "label": 0
                },
                {
                    "sent": "The first one is a pattern explosion problem that we realized back then was it was becoming a real problem in frequent itemset mining and the other one is about condensed representations.",
                    "label": 0
                },
                {
                    "sent": "So to counter this pattern explosion problem, condensed representations have been proposed.",
                    "label": 0
                },
                {
                    "sent": "After that I will go into the non derivable item sets so that our content of our paper actually and some extensions that we made to it in KDE.",
                    "label": 0
                },
                {
                    "sent": "At 2005 and in same data mining conference, an after that I hope there will be still some time left.",
                    "label": 0
                },
                {
                    "sent": "I will also tell something about some more recent approaches towards non redundant pattern mining.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first it all started out in 90, three with the definition of the Association rules by these three people.",
                    "label": 0
                },
                {
                    "sent": "And you can see the paper below.",
                    "label": 0
                },
                {
                    "sent": "As you can see, it's cited by almost 12,000 people, and in this paper and the Association Mindrolling problem was first introduced.",
                    "label": 0
                },
                {
                    "sent": "I will just very quickly go over it, because I assume all of you know this problem, so you assume that there is a database with transactions.",
                    "label": 0
                },
                {
                    "sent": "The transaction is just a set of items.",
                    "label": 0
                },
                {
                    "sent": "So here we have four transactions.",
                    "label": 0
                },
                {
                    "sent": "For example transaction 300 as items 123 and five.",
                    "label": 0
                },
                {
                    "sent": "And now we want to find combinations of items that are frequent.",
                    "label": 0
                },
                {
                    "sent": "Now and not in their 93 paper, but one year later they introduced the Apriori algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which basically first looks for L1, the set of large item sets of size 1, so the frequent items, all four of them are selected.",
                    "label": 0
                },
                {
                    "sent": "Then based on these L2 is made and after that L 3 and then we have found all frequent itemsets, in this case with the threshold of two.",
                    "label": 0
                },
                {
                    "sent": "After this step will go to Association rules, so we split it up into.",
                    "label": 0
                },
                {
                    "sent": "Then we split up the item sets into rules with that meet support and confidence thresholds.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the situation in early 2000s, so just before the paper is Association rule mining gained enormous popularity.",
                    "label": 0
                },
                {
                    "sent": "And here you can see a graph about the citations of this 93 paper.",
                    "label": 0
                },
                {
                    "sent": "And how it's going up during all these years so that it became more and more popular.",
                    "label": 0
                },
                {
                    "sent": "And there are literally hundreds of algorithms being proposed.",
                    "label": 0
                },
                {
                    "sent": "There are some listed there.",
                    "label": 0
                },
                {
                    "sent": "Some of those actually are from after after 2000 with anyway, just to give you an idea, there was a huge explosion of algorithms for mining the frequent itemsets.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There were even competitions, so this is of the Femi competition.",
                    "label": 0
                },
                {
                    "sent": "Frequent itemset mining implementations.",
                    "label": 0
                },
                {
                    "sent": "I think it's actually from 2003, which was a workshop that was organized Co organized by, but hotels for example, and there there the goal was to make the fastest items at minor, so given certain minimal support threshold, and here you see it on the horizontal axis, you see the minimal support and the goal was to mine all frequent itemsets with that threshold as quickly as possible.",
                    "label": 0
                },
                {
                    "sent": "Here you see the different algorithms and algorithms are listed in the Legend on the right and on the vertical axis you see on a logarithmic scale.",
                    "label": 0
                },
                {
                    "sent": "The total time that it takes.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at this type of competitions, this is actually from the mushroom data set is one of the graphs.",
                    "label": 0
                },
                {
                    "sent": "Then you see that on the left hand side with the threshold of 2002 thousand, there are actually 50,000 patterns being generated.",
                    "label": 0
                },
                {
                    "sent": "On the rightmost threshold there are over 10,000,000 patterns being generated.",
                    "label": 0
                },
                {
                    "sent": "Now, if you look at the mushroom data set and this data set has 8124 transactions and the transaction length of 23.",
                    "label": 1
                },
                {
                    "sent": "Now if you think about your original goal in original 93 paper, finding these small Nuggets of gold in this huge amount of data, and then we're actually doing exactly the opposite, there were other small data set and we're finding a huge amount of patterns in our data set.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this was this is called the pattern explosion problem and clearly this is a huge problem because the output is 10,000,000 patterns without any post processing or actually useless.",
                    "label": 0
                },
                {
                    "sent": "Now what is the reason for that?",
                    "label": 0
                },
                {
                    "sent": "As you probably meant of many of you already know, is that many of the items had so redundant, it has to be purely on information theoretic for purely for information theoretic reasons, and you have given some examples for Association rules and where you can see some redundancy going on 1st one.",
                    "label": 0
                },
                {
                    "sent": "If smoker implies lung cancer.",
                    "label": 0
                },
                {
                    "sent": "So if you're a smoker, have a high probability of getting lung cancer if this is.",
                    "label": 0
                },
                {
                    "sent": "A frequent rule with high support and confidence, the most likely also the rule smoker and Bolt implies lung cancer.",
                    "label": 0
                },
                {
                    "sent": "Just because you added one extra item to the antecedent of Ja Rule doesn't make the confidence of your rule lower if they are not correlated to confidence of your rule remains the same with clearly this is redundant.",
                    "label": 0
                },
                {
                    "sent": "Another type of redundancy, if someone is pregnant then it's a woman, and this also usually holds, but we know this of course.",
                    "label": 0
                },
                {
                    "sent": "This is this background knowledge that we already have, but also infrequent items that mining the most frequent and confident rules will usually be exactly those type of rules.",
                    "label": 0
                },
                {
                    "sent": "And then of course you also get combinations of different rules that hold if someone is pregnant.",
                    "label": 0
                },
                {
                    "sent": "Anna Smoker, then it's probably a woman and there's a high probability that a higher increase probability that the person will get lung cancer.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Python explosion problem leads to the development of condensed representations.",
                    "label": 0
                },
                {
                    "sent": "And the first one is the closed itemsets representation.",
                    "label": 0
                },
                {
                    "sent": "Now look at this particular data set, which is very well structured.",
                    "label": 0
                },
                {
                    "sent": "We have actually 3 tiles or three blocks of once in our data set.",
                    "label": 0
                },
                {
                    "sent": "If you look at the frequent itemsets, if the threshold is 1, the number of frequent itemsets here is 21, and so clearly we need some kind of compact representation of these items.",
                    "label": 0
                },
                {
                    "sent": "It's because actually you can just describe them by their three blocks of once in our data set.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly but in 1999, the first condensed presentation, as far as I know, proposed and the closed itemsets mining discovery discovering frequent closed itemsets paper by passkey and others.",
                    "label": 0
                },
                {
                    "sent": "At this they proposed to commence representation that solved this particular problem, and so this is this is a scheduling situation.",
                    "label": 0
                },
                {
                    "sent": "In 2002 we have this huge explosion pattern explosion we want to solve it one way or another and one way is to come up with condensed representations.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our physicalness representation is actually just a compressed version of the collection of all frequent itemsets, and usually we also required that they were lossless.",
                    "label": 0
                },
                {
                    "sent": "So this means that they allow for lossless regeneration of all frequent itemsets, so they still contain another information to recover all the frequencies.",
                    "label": 0
                },
                {
                    "sent": "So some proposals that existed at that time, so the close item sets free item sets, Politico and others disjunction free item sets of bikowski and rigotti.",
                    "label": 0
                },
                {
                    "sent": "It was proposed that database conference spots.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a sketch of the situation back then, and this is also if you want to judge our paper and this is the situation you have to judge it against.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's now go to the paper for this.",
                    "label": 0
                },
                {
                    "sent": "In 2002 this was the cover of our presentation back then and how we looked back then 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "Centuries.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is some of these slides are actually from the 2002 presentation.",
                    "label": 0
                },
                {
                    "sent": "So the questions that we asked Dennis how do supports interact with each other?",
                    "label": 0
                },
                {
                    "sent": "You have on the 100 the multiplicity principle, so you know if you extend an item set with some items it supports will go down.",
                    "label": 0
                },
                {
                    "sent": "So that's one way of interacting.",
                    "label": 0
                },
                {
                    "sent": "If you have a smaller item set and a larger one, you always know that the larger one will have lower support then the smaller one.",
                    "label": 0
                },
                {
                    "sent": "A second question is what information about unknown supports can we derive from known support?",
                    "label": 0
                },
                {
                    "sent": "So suppose you're mining your data set.",
                    "label": 0
                },
                {
                    "sent": "You've already found the frequency of some item sets, and now you want to find out.",
                    "label": 0
                },
                {
                    "sent": "OK, what can I derive for the frequency of some other item sets that I have not counted yet in my database?",
                    "label": 0
                },
                {
                    "sent": "Based on this, we want to make a condensed representation or concise representation by only storing the relevant part of the support, only storing some support of some item sets, and not storing those items sets of which you can derive the support perfectly.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first we studied a little bit.",
                    "label": 0
                },
                {
                    "sent": "What type of redundancies were already exploited in other papers and the first one is obviously the monotonicity principle already introduced in 90.",
                    "label": 0
                },
                {
                    "sent": "Three by Rakesh Agrawal and orders so that the support if you extend an item set with an extra item you support will go down.",
                    "label": 0
                },
                {
                    "sent": "There will always be less people that buy A and X.",
                    "label": 0
                },
                {
                    "sent": "Then there are people that buy a then if you look at the closed and the free item set representation.",
                    "label": 0
                },
                {
                    "sent": "Then the rule they are actually using is this one with the support of A equals the support of a B.",
                    "label": 0
                },
                {
                    "sent": "Then the support of a X will also equal the support of axb.",
                    "label": 0
                },
                {
                    "sent": "So how can you see this?",
                    "label": 0
                },
                {
                    "sent": "If every transaction that contains a also contains B?",
                    "label": 0
                },
                {
                    "sent": "Because that's actually what it means that they have equal support.",
                    "label": 0
                },
                {
                    "sent": "Then also every transaction that contains A and X will also contain B, so that's why and this second rule also holds.",
                    "label": 0
                },
                {
                    "sent": "So in other words, if in your data set while mining you find out the support of A equals support of a B.",
                    "label": 0
                },
                {
                    "sent": "Then there is no longer a need to count the support of axb.",
                    "label": 0
                },
                {
                    "sent": "For example, and you can just count the support of a X and then add B to it in.",
                    "label": 0
                },
                {
                    "sent": "The support will remain the same and then the closed itemsets will exactly be those.",
                    "label": 0
                },
                {
                    "sent": "The ones with the maximal number of items in an equivalence class, but in equivalence classes that they have the same support.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we also had Max Miner algorithm of biardeau.",
                    "label": 0
                },
                {
                    "sent": "And he was using the the rule that is given here.",
                    "label": 0
                },
                {
                    "sent": "Support of ABX is always larger than or equal to the support of a X minus the support of X plus the support of BX.",
                    "label": 0
                },
                {
                    "sent": "So how can you see this actually is lossed he called the drop X, B.",
                    "label": 1
                },
                {
                    "sent": "So if you have an item set X and you add item be to it, how much will IT support drop?",
                    "label": 0
                },
                {
                    "sent": "How many transactions do contain X but do not contain B and that's exactly the support of X minus the support of BX.",
                    "label": 0
                },
                {
                    "sent": "So if you now look at the support of a X, how much will IT support decrease if I add item B to it?",
                    "label": 0
                },
                {
                    "sent": "The number of transactions that contain A and X but do not contain B?",
                    "label": 0
                },
                {
                    "sent": "Is smaller than the number of transactions that contain X, but do not contain B, so if you subtract the drop from it actually get a lower bound on the support of ABX and this was exploited in the Max miner algorithm because they want to mine the maximal frequent itemsets and so finding a lower bound on the frequency or the support of itemsets was very useful, and because then if you know that items that will be frequent or not mining all frequent itemsets you're not interested in the frequency.",
                    "label": 0
                },
                {
                    "sent": "If item sensors are not maximal, and then you can use this just to jump ahead and to go already one level further up.",
                    "label": 0
                },
                {
                    "sent": "Then the last.",
                    "label": 0
                },
                {
                    "sent": "Paper the last one about the reasoning about frequencies that existed was of Bikowski and rigotti, and they used that if the support of ABC and this is the disjunction free item sets in the support of ABC equals the support of a B + a C minus in support of A, then you can also find a similar relation for the support of ABCX.",
                    "label": 0
                },
                {
                    "sent": "So how can you use this if you're mining all frequent itemsets and in every step you just check if this equality holds?",
                    "label": 0
                },
                {
                    "sent": "If it holds, then there will no longer be a need to counter support of ABC X, because you can derive it from its perfectly from its subset.",
                    "label": 0
                },
                {
                    "sent": "So if you're applying in a pre like algorithm just check for item sets.",
                    "label": 0
                },
                {
                    "sent": "Once you find this relation, once a set is no longer disjunction free, you can skip.",
                    "label": 0
                },
                {
                    "sent": "Counting all its super sets in the database.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually the paper started out as being a theoretical study about this redundancies in between item sets, and as you can see it there already existed alot of scattered rules that were being exploited successfully to make condensed representations or to mine the maximal itemsets and we actually wanted to see started out my PhD research by trying to find out how can we find something more general that emcompasses.",
                    "label": 0
                },
                {
                    "sent": "All these different rules, all these different proposals into one set action motivation, one set of deduction rules for the frequency of item sets, and that's where the non derivable item sets in the end came from.",
                    "label": 0
                },
                {
                    "sent": "And we saw applicability in frequent item set mining because if you can derive perfectly the support of an item set without going to the database, you can skip.",
                    "label": 0
                },
                {
                    "sent": "Maybe some costly scans of your database.",
                    "label": 0
                },
                {
                    "sent": "And also condense concise representations.",
                    "label": 0
                },
                {
                    "sent": "'cause if an item set, if you know IT support perfectly based on support of some other item sets, there is no longer a need to store them and you because you can just derive them and we will not present them to the user.",
                    "label": 0
                },
                {
                    "sent": "So that was our solution to the pattern explosion problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we found deduction rules for the inclusion exclusion principles.",
                    "label": 0
                },
                {
                    "sent": "From this we come to the rival item sets the algorithm to find non drivable item sets, and also show some evaluation and some conclusions and further work.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the inclusion exclusion principle, or at least one of the benefits from it, is as follows.",
                    "label": 0
                },
                {
                    "sent": "Suppose here we have our database D and now I took a slightly different representation.",
                    "label": 0
                },
                {
                    "sent": "I have three items in my database item AB&C, so these are the products that people come by.",
                    "label": 0
                },
                {
                    "sent": "And now represented them as a set.",
                    "label": 0
                },
                {
                    "sent": "So A is a set of all transactions that do contain item AB.",
                    "label": 0
                },
                {
                    "sent": "Everything contains item B, ansi.",
                    "label": 0
                },
                {
                    "sent": "Everything that contains items C. Now in this representation, the support of ABC is a number of transactions that here or here in the middle of my diagram, and these are all the transactions that contain both items AB&C.",
                    "label": 0
                },
                {
                    "sent": "As you look at this particular fragment of a diagram, all the elements that are in a but not in B and not in C. These are all the transactions that contain item A, but not item B and not item C. Now the size of this set we can estimate.",
                    "label": 0
                },
                {
                    "sent": "Or we can.",
                    "label": 0
                },
                {
                    "sent": "Computers follows the support of A not B, not C. It's just a number of elements that are in the set A.",
                    "label": 0
                },
                {
                    "sent": "Then we subtract the number of elements in a B which is this set.",
                    "label": 0
                },
                {
                    "sent": "We should track the number of elements in a C as this set and but then we subtract it ABC twice.",
                    "label": 0
                },
                {
                    "sent": "So we need to compensate and add ABC to it again and so here you have this relation between the supports and this what we called a generalized item set.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This part needs to be larger or equal to 0.",
                    "label": 0
                },
                {
                    "sent": "Then there cannot be a negative number of elements in this partition.",
                    "label": 0
                },
                {
                    "sent": "And so actually end by take it by making the right inside larger than or equal to 0 and shifting the ABC or shifting the other terms to the other side.",
                    "label": 0
                },
                {
                    "sent": "We actually get this relation between the supports of the items.",
                    "label": 0
                },
                {
                    "sent": "It's always in every data set.",
                    "label": 0
                },
                {
                    "sent": "The support of ABC will be larger equal to the support of a B plus the support of a C minus the support of a.",
                    "label": 0
                },
                {
                    "sent": "And is actually already has some similarity with this rule of bikowski and others where they had the disjunction free item sets, and there this set of this one isn't equality.",
                    "label": 0
                },
                {
                    "sent": "Then for my larger item sets this one will also be inequality.",
                    "label": 0
                },
                {
                    "sent": "Let's now see what happens if we would extend ABC even element D. So if this is an equality.",
                    "label": 0
                },
                {
                    "sent": "It means that this one is not larger or equal to 0.",
                    "label": 0
                },
                {
                    "sent": "It is 0.",
                    "label": 0
                },
                {
                    "sent": "So if we have a not be not see, we extend it with not D. The number of transactions that contain a not be not seen or D will be smaller than or equal to the number of transactions containing a not B, not C. So this will also be zero and so also at the next level.",
                    "label": 0
                },
                {
                    "sent": "For ABCD this equality will hold.",
                    "label": 0
                },
                {
                    "sent": "This one will be 0 and we will get the same equality.",
                    "label": 0
                },
                {
                    "sent": "So we already succeeded in finding another way of deriving this rule for the disjunction.",
                    "label": 0
                },
                {
                    "sent": "Free item sets.",
                    "label": 0
                },
                {
                    "sent": "Now you can do this, not only.",
                    "label": 0
                },
                {
                    "sent": "With this partition you can do it with all of them with all eight of them.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at the number of transactions that do not contain a, not contain B, not contain C. So then we just take all transactions the support of the empty set.",
                    "label": 0
                },
                {
                    "sent": "We subtract everything that contains a everything that contains B.",
                    "label": 0
                },
                {
                    "sent": "Everything that contains C. But then we subtracted too much.",
                    "label": 0
                },
                {
                    "sent": "We subtracted AB twice, AC twice BC twice, so we need to compensate for that.",
                    "label": 0
                },
                {
                    "sent": "But after doing that, actually we have.",
                    "label": 0
                },
                {
                    "sent": "ABC.",
                    "label": 0
                },
                {
                    "sent": "One time too many, so we need to subtract it.",
                    "label": 0
                },
                {
                    "sent": "So here you see the inclusion exclusion some going on.",
                    "label": 0
                },
                {
                    "sent": "Again, this one is larger than equal to 0.",
                    "label": 0
                },
                {
                    "sent": "And again we get a rule, and this is actually a new rule that was not known before that just by applying the same principle but for another generalized item set.",
                    "label": 0
                },
                {
                    "sent": "And we could find this new rule about how frequencies, how supports interact in our database.",
                    "label": 0
                },
                {
                    "sent": "Again, we can exploit it in exactly the same way as disjunction free item sets.",
                    "label": 0
                },
                {
                    "sent": "If this rule is an equality, and this is really zero, then for larger item sets, this one will also be zero, and so if we find that this equality that this is perfect.",
                    "label": 0
                },
                {
                    "sent": "And then we know that we do not need to explore the supersets anymore.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one more one last, one support of a B, not C equals the support of a B minus the support of ABC.",
                    "label": 0
                },
                {
                    "sent": "Larger equal to 0.",
                    "label": 0
                },
                {
                    "sent": "And the support of ABC is larger than or equal to the support of a B and this we known know very well as the monotonicity principle.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I told you ABC, we have three items, so in our diagram have eight places, so this results in eight times that we can play the same game and eight rules that make for an item set.",
                    "label": 0
                },
                {
                    "sent": "ABC.",
                    "label": 0
                },
                {
                    "sent": "As you can see, if your if your items that grows the number of rules that you have for these items that will grow exponentially with the size of the item set.",
                    "label": 0
                },
                {
                    "sent": "So these are all the rules for ABC.",
                    "label": 0
                },
                {
                    "sent": "First one is trivial and it needs to be larger or equal to 0.",
                    "label": 0
                },
                {
                    "sent": "It is also follows from that.",
                    "label": 0
                },
                {
                    "sent": "Then we have.",
                    "label": 0
                },
                {
                    "sent": "The monotonicity principle at the next level we have the rules that are being used for this disjunction.",
                    "label": 0
                },
                {
                    "sent": "Free item sets and then in the answer with this last rule in this new rule.",
                    "label": 0
                },
                {
                    "sent": "Now if you go to item size 4567 and then at every level here we have level 0 one and two, those are known the rules at level three were not known at that time.",
                    "label": 0
                },
                {
                    "sent": "If you go to larger items that you also get rules at level 4567, and these were all new rules for deduction of the support.",
                    "label": 0
                },
                {
                    "sent": "Now this is very nice, of course, so we found a whole lot of new rules that we could exploit to make condensed representations.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, here is an example of how we could we could use it.",
                    "label": 0
                },
                {
                    "sent": "So we're just doing the monotonicity, but just applying the Apiri algorithm first accounts and I will just go until a certain level.",
                    "label": 0
                },
                {
                    "sent": "We counted the empty set.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The six ABC",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "ABA CBC so now before we go and count the support of ABC.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apply the rules.",
                    "label": 0
                },
                {
                    "sent": "So we find a lower and upper bound for the support on ABC.",
                    "label": 0
                },
                {
                    "sent": "Now in this particular case, a lower bound is 0, upper bound is 1, so in this case actually we cannot use it because our upper bound is still exceeding or minimal support because we accepted AC.",
                    "label": 0
                },
                {
                    "sent": "So our minimal support is 1, so we cannot use it to prune our search space, but you can imagine if your upper bound is below your threshold.",
                    "label": 0
                },
                {
                    "sent": "You can exclude the rule already on before hand, little bit like the multiplicity principle with an extended if your lower bound exceeds the minimum support threshold, they can do the other way around.",
                    "label": 0
                },
                {
                    "sent": "You don't need to count it because you know for sure that will be frequent, and if the lower bound equals upper bound there is also no need to count it any longer, because then it's perfectly derivable.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we come to the main theorem over paper.",
                    "label": 0
                },
                {
                    "sent": "So suppose that you're in this situation in which you are when you're doing a priority, so we are interested in items at J and we know the support of all its subsets.",
                    "label": 0
                },
                {
                    "sent": "So for ABC we know the support of all its subsets, but we do not know set ABC itself in that case, or deduction rules are sound complete, unknown, redundant for deducing upper and lower bounds on the support of J.",
                    "label": 0
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "And they sound OK, so there all the rules are correct.",
                    "label": 0
                },
                {
                    "sent": "And with that we already saw, by the way we derived the rules.",
                    "label": 0
                },
                {
                    "sent": "They are complete.",
                    "label": 0
                },
                {
                    "sent": "So this means that if the bounds that we find by the rules or LUOL is a lower bound and use upper bound, then there does exist a database which is completely consistent with all the supports of the subsets and the support of our side J is equal to L. And we can also find the database DU which is fully consistent with all the with all the support that we know and the support of J is equal to you.",
                    "label": 0
                },
                {
                    "sent": "So no matter what you do and there is no more space for improving upon these derivation rules, in the context that you know the support of all subsets of an item set J.",
                    "label": 0
                },
                {
                    "sent": "So unlike all the previous works where they proposed some of the rules and there might be some more rules out and here we have proven that in this particular situation there are no other.",
                    "label": 0
                },
                {
                    "sent": "Rules there.",
                    "label": 0
                },
                {
                    "sent": "The rules are nonredundant, and by this I mean that you really need every rule.",
                    "label": 0
                },
                {
                    "sent": "Because you can always think of situations for every rule you can think of situations where you really need this particular rule where no other rules will give you a better lower or an as good, lower or as good upper bounds.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a small example of the completeness, so here this is the same example we had the lower bound to zero and an upper bound of 1.",
                    "label": 0
                },
                {
                    "sent": "So this means that there must exist databases which are completely consistent with the information that we know into which the support of ABC is zero, and also a database in which is support of ABC is 1.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clearly this is our date or upper database.",
                    "label": 0
                },
                {
                    "sent": "Support of ABC is equal to 1 and all the other supports or satisfied.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we change a little bit attribute, see, it's still consistent with all the information that we have.",
                    "label": 0
                },
                {
                    "sent": "So given the information that we have both DL or DU could be could be the underlying database.",
                    "label": 0
                },
                {
                    "sent": "We don't know based on these supports.",
                    "label": 0
                },
                {
                    "sent": "And here ABC has a support of 0, so our bounds are tides and based on this information we cannot better bound to support of ABC.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from this set of consulting complete deduction rules, we went to the derivable itemsets.",
                    "label": 0
                },
                {
                    "sent": "So again, had this condition that you saw in the theorem, we suppose that we know the support of I for all either strict subset of J.",
                    "label": 0
                },
                {
                    "sent": "So we can compute with our deduction rules lower bound on the support of J in an upper bound lnu.",
                    "label": 0
                },
                {
                    "sent": "So then without counting we know that the support of J must be in this interval and an item set is called the derivable item set.",
                    "label": 0
                },
                {
                    "sent": "FL equals U, so we know the support of J without having to count it in the database.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is no need to count the support of J in such a situation, and this week all the derivable item set there is no need to store it.",
                    "label": 0
                },
                {
                    "sent": "Enter concise representation.",
                    "label": 0
                },
                {
                    "sent": "That we proposed is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "So our set contains all the items that she and their support for which it holds that J is not derivable at those we cannot derive, so we need them in our representation.",
                    "label": 0
                },
                {
                    "sent": "They are not derivable from all their subsets.",
                    "label": 0
                },
                {
                    "sent": "And you can prove that this is a lossless representation of all frequent itemsets.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "Let me speak an example.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the lower and upper bounds are equal.",
                    "label": 0
                },
                {
                    "sent": "So in this particular case, what you see here, we have a lower bound and upper bound to support of ABC smaller than or equal to 1, and this because of this one monotonicity principle, we also have a lower bound.",
                    "label": 0
                },
                {
                    "sent": "Support of ABC is large in area code 21 plus is a so called overlap rule.",
                    "label": 0
                },
                {
                    "sent": "It's the support of a B plus AC minus support of B, so we also get a lower bound of one on the support of ABC.",
                    "label": 0
                },
                {
                    "sent": "So in this particular situation.",
                    "label": 0
                },
                {
                    "sent": "No need to count ABC in our database.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then algorithmics, how can we find this nondurable items efficiently in a database?",
                    "label": 0
                },
                {
                    "sent": "For this we had two additional results.",
                    "label": 0
                },
                {
                    "sent": "The first one is monotonicity.",
                    "label": 0
                },
                {
                    "sent": "If Jay is a subset of key and Jason derivable item set, then K is also derivable and clearly this is a very useful theorem because this means that if you find and derivable items that you can stop exploring the whole 3 above.",
                    "label": 0
                },
                {
                    "sent": "This derivable item sets and the reason for that is obviously a little bit like I told you, with the disjunction free item sets Eva set is derivable.",
                    "label": 0
                },
                {
                    "sent": "It means that one of these generalized item sets and for example a not B not C. Is zero and also the other direction holds.",
                    "label": 0
                },
                {
                    "sent": "Then if you go to a larger item set and by adding items.",
                    "label": 0
                },
                {
                    "sent": "So actually you're splitting up a cell which has zero transactions in it, you're splitting a not B, not C. Splitting it in a not being on CD and a not be not seen on D, so those two will also be zero, and so that's why this derivability or this will be monotone.",
                    "label": 0
                },
                {
                    "sent": "So being derivable is anti monotone.",
                    "label": 0
                },
                {
                    "sent": "If you find the rival item set.",
                    "label": 0
                },
                {
                    "sent": "You can stop exploring there.",
                    "label": 0
                },
                {
                    "sent": "And the second theorem.",
                    "label": 0
                },
                {
                    "sent": "And because you can.",
                    "label": 0
                },
                {
                    "sent": "OK, it's very nice.",
                    "label": 0
                },
                {
                    "sent": "All these reduction rules with how much does it actually prune.",
                    "label": 0
                },
                {
                    "sent": "So there we had this whole thing theorem and this says that the width of the interval for the items that J unit when you add item 82 it is at most half of the size of the interval for J.",
                    "label": 0
                },
                {
                    "sent": "So at every step.",
                    "label": 0
                },
                {
                    "sent": "So in the first step for the items you don't know anything about the database except that it has any items, so the lower bound is 0, the upper bound is North.",
                    "label": 0
                },
                {
                    "sent": "Four items of item sets of size 2.",
                    "label": 0
                },
                {
                    "sent": "You know that the size of this interval will have at least, so the difference between lower bound and upper bound will be at most N / 2 Level 3 and over 4 an over 8 and so on.",
                    "label": 0
                },
                {
                    "sent": "So in other words the linked of non derivable item sets is at most the logarithm of the size of your database.",
                    "label": 0
                },
                {
                    "sent": "So if you have a small database the size of your items non derivable item sets will be limited.",
                    "label": 0
                },
                {
                    "sent": "Whis is also useful.",
                    "label": 0
                },
                {
                    "sent": "As I told you before, the number of derivation rules increases exponentially with the size of your non derivable item set.",
                    "label": 0
                },
                {
                    "sent": "So you can easily imagine a situation where evaluating the rules will become more complex than just counting the items in the database.",
                    "label": 0
                },
                {
                    "sent": "Now this theorem is actually saying that this will never happen because the size of your items it is at most the logarithm of the database, they the exponent of that.",
                    "label": 0
                },
                {
                    "sent": "So the effort of computing and arrival itemsets and computing the support.",
                    "label": 0
                },
                {
                    "sent": "Of a derivable item set will always be at most the.",
                    "label": 0
                },
                {
                    "sent": "The effort of counting it in the database and in all cases in reality is far less.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then the algorithm this is based on 2.",
                    "label": 0
                },
                {
                    "sent": "Principles and non derivability is monotone.",
                    "label": 0
                },
                {
                    "sent": "So we just choose used level wise appear like search algorithm.",
                    "label": 0
                },
                {
                    "sent": "We also made the depth first version later on that we proposed at Siam Data Mining Conference in 2005.",
                    "label": 0
                },
                {
                    "sent": "The card reader is that OK for computing whether another set is derivable or not?",
                    "label": 0
                },
                {
                    "sent": "Arrival you need to support of all its subsets.",
                    "label": 0
                },
                {
                    "sent": "Now if you do it in a priority this is trivial to get to have the support of all subsets.",
                    "label": 0
                },
                {
                    "sent": "When you're evaluating an item set in depth first traversal, this is less trivial, but doing reverse depth first traversal, we can actually guarantee that even doing depth first that we evaluate a set only after all its subsets have been evaluated.",
                    "label": 0
                },
                {
                    "sent": "So that's just another way to do it that we proposed in Siam data Mining conference.",
                    "label": 0
                },
                {
                    "sent": "And then we will only count an item set in the database if all its subsets or frequent otherwise.",
                    "label": 0
                },
                {
                    "sent": "We can prune it because of frequency threshold and the bounce cannot derive the support exactly.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then there was just a small optimization that we could still do.",
                    "label": 0
                },
                {
                    "sent": "If you're bound to an item sets, I equals either the lower bound or the upper bound, then we know that all the supersets or derivable, so it can very well be that an item set itself is not an on derivable, but it's true support after if counting and database.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the support is equal to its lower bound or it's upper bound, and in that case you can also stop exploring the.",
                    "label": 0
                },
                {
                    "sent": "Super fits because they will all be derivable.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then one of the bottlenecks of the algorithm remained this computation of all these lower and upper bounds is exponential number of lower and upper bounds.",
                    "label": 0
                },
                {
                    "sent": "So later on in 2005, we came up with the quick inclusion exclusion principle, and this principle is actually based on the observation, and this is this is not a new observation or the other people saw it before machine learning.",
                    "label": 0
                },
                {
                    "sent": "There is also a paper that exploits this relationship in order to compute.",
                    "label": 0
                },
                {
                    "sent": "The support of all these generalized item sets, but if you look at these sums that are used to generate our rules and they share a lot of terms.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the support of a B not seen on D, it's equal to this.",
                    "label": 0
                },
                {
                    "sent": "Some support of a not be not seen or D it's equal to this sun and it contains this some internally it contains this.",
                    "label": 0
                },
                {
                    "sent": "If you first compute the first one, use this result.",
                    "label": 0
                },
                {
                    "sent": "In the second one then you can save 3 operations.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in computing the support of a not B not seen or D. Um?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the quick inclusion exclusion principle is based on this to more quickly compute the lower and upper bounds is actually based on this principle.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the support of a generalized item set, the item sets not a G, where G is just any journalized item set is equal to the support of G minus the support of AG.",
                    "label": 0
                },
                {
                    "sent": "So what we're actually doing in the quick inclusion exclusion principle is we make an array that contains first in the first step just contains the frequency of all item sets.",
                    "label": 0
                },
                {
                    "sent": "Then in the second step, we will negate item A.",
                    "label": 0
                },
                {
                    "sent": "Item C for all items sets that do not contain item C, So here we do not have a B.",
                    "label": 0
                },
                {
                    "sent": "So after the first step the support that will be stored here is the support of a B, not C. How do we compute it by from this cell we subtract what is in this cell.",
                    "label": 0
                },
                {
                    "sent": "So then we get the support of a B, not C. What is in this?",
                    "label": 0
                },
                {
                    "sent": "So from this we subtract for this in this cell to get a Nazi and again here we not see and hear Nazi.",
                    "label": 0
                },
                {
                    "sent": "So after one step.",
                    "label": 0
                },
                {
                    "sent": "And doing one operation for half of the entries and we have added the negating the negation of C. So the next step we do the same but now for B.",
                    "label": 0
                },
                {
                    "sent": "So half of the items that do half of the sets of the entries do not contain item B, so there will not be in the same way.",
                    "label": 0
                },
                {
                    "sent": "So for this cell it's a C minus ABC look at a not BC and so on.",
                    "label": 0
                },
                {
                    "sent": "So here we get the negation of all the bees and then in the last step we also add the negation for a.",
                    "label": 0
                },
                {
                    "sent": "So this is a fast way in theory.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a graphical representation of the operations that we need to perform.",
                    "label": 0
                },
                {
                    "sent": "If you have N items, we will have N. Cycles.",
                    "label": 0
                },
                {
                    "sent": "How many steps do we need to take?",
                    "label": 0
                },
                {
                    "sent": "How many operations is every step?",
                    "label": 0
                },
                {
                    "sent": "I just half of the entries need to be updated so it's 2 to the power N -- 1.",
                    "label": 0
                },
                {
                    "sent": "So if an algorithm of complexity N * 2 to the power N -- 1 versus if you would do it, brute force, you would get an algorithm of complexity 3 to the power of N. So it's it's not dramatic, but it's an improvement.",
                    "label": 0
                },
                {
                    "sent": "So in this way, and this algorithm also allows us to compute this lower and upper bounds much more efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's now move to the evaluation.",
                    "label": 0
                },
                {
                    "sent": "So how good is it actually in practice?",
                    "label": 0
                },
                {
                    "sent": "So we did extensive comparisons with other condensed representations, and we also looked at the influence of the rule depth, so there were many rules an exponential number of rules for every item set, but obviously you do not need to compute every rule.",
                    "label": 0
                },
                {
                    "sent": "You can also say, OK, I'm happy with only doing the monotonicity principle and only doing the disjunction free.",
                    "label": 0
                },
                {
                    "sent": "Optimization, then you get to discern truth free.",
                    "label": 0
                },
                {
                    "sent": "Condensed representation or you can go to Level 3 level 4 in order to get rid of this exponential blowup of the number of rules.",
                    "label": 0
                },
                {
                    "sent": "Because if you have very large item sets that are non derivable and it will be a whole lot of rules that you need to evaluate so you can restrict to a certain depth and there the question was OK. How much influence do we get from rules of larger depths?",
                    "label": 0
                },
                {
                    "sent": "Maybe you can just stick with the smaller rules.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first the first one this I took from the Journal version of our email PKD paper.",
                    "label": 0
                },
                {
                    "sent": "It's appeared in data mining Journal in 2007, so it took some time for us to make the Journal version.",
                    "label": 0
                },
                {
                    "sent": "Yeah, what you actually can see here is that the closed itemsets in this particular graph on the horizontal axis you see the minimal support that is being very right on the vertical axis.",
                    "label": 0
                },
                {
                    "sent": "You see the number of frequent item sets in a different representations at hearing this particular datasets that BM, SFU one, and closed itemsets are here.",
                    "label": 0
                },
                {
                    "sent": "And the non driveable item sets.",
                    "label": 0
                },
                {
                    "sent": "You're actually here in this junction free item sets.",
                    "label": 0
                },
                {
                    "sent": "They are here.",
                    "label": 0
                },
                {
                    "sent": "So why is there there is difference between the different representations?",
                    "label": 0
                },
                {
                    "sent": "This actually comes from an because we just explained you that we have all the rules that exist.",
                    "label": 0
                },
                {
                    "sent": "Also, the rules that were used in your other algorithms.",
                    "label": 0
                },
                {
                    "sent": "So why aren't we better?",
                    "label": 0
                },
                {
                    "sent": "And this has to do with the way that we explore that we exploit the different rules.",
                    "label": 0
                },
                {
                    "sent": "So for us we do not store an item set, only if it can be perfectly derived.",
                    "label": 0
                },
                {
                    "sent": "And in this in the sense of.",
                    "label": 0
                },
                {
                    "sent": "Meaning the logical sense it cannot be.",
                    "label": 0
                },
                {
                    "sent": "There cannot exist the database that is consistent with our condensed representation and to support is different.",
                    "label": 0
                },
                {
                    "sent": "Now, in this order representations, they take some other rules.",
                    "label": 0
                },
                {
                    "sent": "They say OK, we do not include an item set in our representation if IT support is equal to the upper bound.",
                    "label": 0
                },
                {
                    "sent": "For example, if you're interested in that in how this relation between the different condensed representations is and how we can extend the non derivable item sets.",
                    "label": 0
                },
                {
                    "sent": "And then you need to look at our paper one year later at EML PKD 2003 where we also exploit this.",
                    "label": 0
                },
                {
                    "sent": "Adding these extra assumptions.",
                    "label": 0
                },
                {
                    "sent": "To further reduce our collection.",
                    "label": 0
                },
                {
                    "sent": "Well, so here in this case, closed itemsets sort of clear winner.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is another data set.",
                    "label": 0
                },
                {
                    "sent": "Connect four and here we perform best with an undeniable item sets in the closed itemsets are much worse.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "NT is another data set and we also perform better than the closed itemsets, so in general, how is the relation between the different condensed representations sometimes a closed itemsets are better sometimes, and unreliable item sets are better.",
                    "label": 0
                },
                {
                    "sent": "They are just performing equally well later on.",
                    "label": 0
                },
                {
                    "sent": "Also, at least not Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "I think even two.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "The year 2008 2009 someone also proposed the non drivable closed Itemsets, which exploits both Nondurable itemsets deduction rules, ends.",
                    "label": 0
                },
                {
                    "sent": "The closed itemsets and that perform better every day.",
                    "label": 0
                },
                {
                    "sent": "You can show that are always better than than any of the two.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the influence of the rule depth is just one particular data set where we looked at rules of step one.",
                    "label": 0
                },
                {
                    "sent": "This is only using monotonicity principle rules of depth.",
                    "label": 0
                },
                {
                    "sent": "Two disjuncture free item sets, rules of depth 3, four and so on.",
                    "label": 0
                },
                {
                    "sent": "And here you actually see that the more complex the rule become.",
                    "label": 0
                },
                {
                    "sent": "In these datasets.",
                    "label": 0
                },
                {
                    "sent": "The less the reduction in number of frequently unreliable itemsets becomes.",
                    "label": 0
                },
                {
                    "sent": "So actually we can stick to rules of limited complexity and still gets the same reduction of our collection.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the conclusion of the evaluation, the number of frequent non derivable item sets is considerably smaller than the number of frequent itemsets.",
                    "label": 0
                },
                {
                    "sent": "I didn't throw a graph here because for all the experience that I did, it was possible that I showed here.",
                    "label": 0
                },
                {
                    "sent": "It was possible to make the condensed representations, but it is not possible to mine the frequent itemsets at such low frequency thresholds.",
                    "label": 0
                },
                {
                    "sent": "Most of the work is done by rules of limited depth.",
                    "label": 0
                },
                {
                    "sent": "So we don't actually usually do not need the more complex rules, although you can show in theory that there can exist cases in which you might need them.",
                    "label": 0
                },
                {
                    "sent": "We are in.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is efficient so calculating the non derivable itemsets plus deducing derivable itemsets is in many cases outperforms apriori.",
                    "label": 0
                },
                {
                    "sent": "The apriori algorithm, although in 2012 this is not such a strong argument.",
                    "label": 0
                },
                {
                    "sent": "Anymore I guess.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me go to the next one.",
                    "label": 0
                },
                {
                    "sent": "I still have like 10 minutes left, so this was our work for trying to reduce the collection of frequent itemsets.",
                    "label": 0
                },
                {
                    "sent": "OK, 5 minutes left.",
                    "label": 0
                },
                {
                    "sent": "So let's now go to some more recent approaches towards non redundant patron mining because after this non drivable itemsets and after the extensions that we made also other people have been proposed.",
                    "label": 0
                },
                {
                    "sent": "Different ways to reduce this collection, or at least to access or to remove the redundancy from this collection.",
                    "label": 0
                },
                {
                    "sent": "Now the overview that I will very quickly give now is extremely biased and this is because I restrict myself mainly two proposals that in my feeling also have this flavor of trying to understand how supports interact and how learns this.",
                    "label": 0
                },
                {
                    "sent": "What does it learn us about the support of other items?",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I want to give analysts illustrative example of tiles.",
                    "label": 0
                },
                {
                    "sent": "So now I've been talking about frequent itemsets all along.",
                    "label": 0
                },
                {
                    "sent": "Let's now move to tiles, finding large tiles in a data set.",
                    "label": 0
                },
                {
                    "sent": "Is just like you can we have 01 matrix Now we're allowed to reorder rows and columns after reordering rows and columns.",
                    "label": 0
                },
                {
                    "sent": "We want to have large box of ones.",
                    "label": 0
                },
                {
                    "sent": "Actually, for this particular data set, so usually I let public search a little bit to see if they can find them.",
                    "label": 0
                },
                {
                    "sent": "Actually, this data set on the left if you rearrange rows and columns slightly and actually you see that this is that data, basically change the order of the products you change the order of the customers in your transaction database well, and that is the tile you can find in this particular example.",
                    "label": 0
                },
                {
                    "sent": "Now here it is very easy if you find this result clearly, it is well, it is significant.",
                    "label": 0
                },
                {
                    "sent": "You don't need statistical test to see that.",
                    "label": 0
                },
                {
                    "sent": "It's highly likely that this will pass your test that this is not random this structure.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in general, I suppose that we find a four by three tile and it is just an example.",
                    "label": 0
                },
                {
                    "sent": "So the area of this style is 12.",
                    "label": 0
                },
                {
                    "sent": "Is this significant?",
                    "label": 0
                },
                {
                    "sent": "If our database has 22 + 6 items and all items have a probability of 0.3, so just have random data set and then it's finding such a tile.",
                    "label": 0
                },
                {
                    "sent": "Is this actually significant?",
                    "label": 0
                },
                {
                    "sent": "The one approach that we recently seized that so we can do is we can characterize the distribution of the maximal maximum area of tiles in random data, then compute in this distribution, how likely is it?",
                    "label": 0
                },
                {
                    "sent": "To have a tile of size 12 or more and this then the so called P value and how extreme is the observed value under a null model expressing the background knowledge that we already have.",
                    "label": 0
                },
                {
                    "sent": "So now you can start seeing the relation with this frequent item sets on the 100 for an old model we have a model of things expressing what we already know, maybe some frequencies that we already know some rows or column marginals in our data set that we already know.",
                    "label": 0
                },
                {
                    "sent": "We make a distribution.",
                    "label": 0
                },
                {
                    "sent": "That satisfies our background knowledge that expresses our background knowledge and I want to see.",
                    "label": 0
                },
                {
                    "sent": "OK, I have this new item set.",
                    "label": 0
                },
                {
                    "sent": "I have this new tile, how likely or unlikely is it to see this style in my data set given the background knowledge that I already have is it's it's extremely likely to see it and then you can skip it.",
                    "label": 0
                },
                {
                    "sent": "You do not need to take it into your condensed representation at, otherwise they take it in and your updates you know model.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this particular example, we can rely rely to simulation.",
                    "label": 0
                },
                {
                    "sent": "I did not do it analytically, so I sampled over all databases that satisfy this condition.",
                    "label": 0
                },
                {
                    "sent": "Computed empirical P value and here you see that actually a style of size 12 or more given this data is a almost 11% of our random databases.",
                    "label": 0
                },
                {
                    "sent": "This occurs, so if you're willing to accept this as a significant result, then it means that in random data.",
                    "label": 0
                },
                {
                    "sent": "In 11% of the cases he will be accepting.",
                    "label": 0
                },
                {
                    "sent": "A random.",
                    "label": 0
                },
                {
                    "sent": "Random pattern you're accepting a pattern that has a likelihood of appearing in random data 11% of the time.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our new model expresses our prior belief or the prior believe the user.",
                    "label": 0
                },
                {
                    "sent": "We can compute it for all the items sets of roll the tiles.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the different works and where the difference is, it's in how to select the normal compute P values, free item sets and then rank the items that according to P value or information gain of information or information that you would gain if you would also take this.",
                    "label": 0
                },
                {
                    "sent": "Access it into account given the time constraints, will just go quickly.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over different approaches, first one is of the honest that I found like really a milestone or that was really a milestone for me.",
                    "label": 0
                },
                {
                    "sent": "The swap randomization model, where we assume that the background knowledge trees OK here, if your data set and your background knowledge is that you know how much every item was sold, how many times, and this one is sold 2 * 3 * 2 * 1 time, it's very realistic to assume that an analyst has this information and you also know how many products different the different people by.",
                    "label": 1
                },
                {
                    "sent": "You also have a distribution over how many products do people buy and then a guy honest actually sampled uniformly over all databases with the same number of rows and columns in the same row and column marginals.",
                    "label": 0
                },
                {
                    "sent": "So you can read this paper if you want.",
                    "label": 0
                },
                {
                    "sent": "I'll just go to.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another second class that or the maximal entropy based models, or just very quickly explaining before the chair stops me.",
                    "label": 0
                },
                {
                    "sent": "So there it is supposed that the background knowledge is that you know the frequency of some item sets.",
                    "label": 0
                },
                {
                    "sent": "Or you know that there are some tiles in your database, so you know that there is a tile of once for certain rows and columns.",
                    "label": 0
                },
                {
                    "sent": "Then you can make a distribution over different databases.",
                    "label": 0
                },
                {
                    "sent": "Or you can consider your databases distribution itself.",
                    "label": 0
                },
                {
                    "sent": "Now which distribution will you pick?",
                    "label": 0
                },
                {
                    "sent": "There are two different proposals.",
                    "label": 0
                },
                {
                    "sent": "One is to consider your database as a distribution.",
                    "label": 0
                },
                {
                    "sent": "And then of all distributions that satisfy your frequency constraints, you pick the one that has a maximal entropy and why the one that has maximal entropy.",
                    "label": 0
                },
                {
                    "sent": "That is because that is the one that contains the least amount of information actually.",
                    "label": 0
                },
                {
                    "sent": "Or he assume that there is a distribution over databases and you pick that distribution of databases that maximizes the entropy, and that's more like what tell the Beast proposal is.",
                    "label": 0
                },
                {
                    "sent": "The first one is more like Nikolaj Tatti.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And that he considered distribution over databases where on average your constraints need to be satisfied.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "While and then third recent work and after that I stopped by from US and is there based on minimal description linked and there if you want to assess how useful a set of items sets is, you actually consider your collection of item sets as being a model for your database so you know some things about your database.",
                    "label": 0
                },
                {
                    "sent": "And if your model is good, if the item says that you have in your collection are really relevant nonredundant item sets, and then you assume that they are also very good for compressing the database.",
                    "label": 0
                },
                {
                    "sent": "So every items that you have in your collection you just give a code number.",
                    "label": 0
                },
                {
                    "sent": "And if you see this item set up pairing your transaction, you replace this item set in this transaction you replace it just by this code.",
                    "label": 0
                },
                {
                    "sent": "And if you can use this code many times, then the size.",
                    "label": 0
                },
                {
                    "sent": "Of your model, so your collection of frequent item sets that you decided to keep as your significant collection.",
                    "label": 0
                },
                {
                    "sent": "Plus the size of your database that you have now compressed using this model and the sum of these two will actually be smaller than your original database.",
                    "label": 0
                },
                {
                    "sent": "So actually you're trying to find a model, a collection of items sets that minimizes this sum and.",
                    "label": 0
                },
                {
                    "sent": "The sides to the coding length of your model plus according length of your data given the model.",
                    "label": 0
                },
                {
                    "sent": "So here there is an explicit tradeoff that you have.",
                    "label": 0
                },
                {
                    "sent": "If you increase the size of your model, you add some extra item sets to your significant collection at this size growth that's bad, but it might also decrease the description of your data set given the model.",
                    "label": 0
                },
                {
                    "sent": "Enter.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The paper that I want to mention there is the crimp algorithm.",
                    "label": 0
                },
                {
                    "sent": "The paper proposing the crimp algorithm by Euless Matteson are know that.",
                    "label": 0
                },
                {
                    "sent": "First proposal long before this data.",
                    "label": 0
                },
                {
                    "sent": "Money Journal paper.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So general conclusion, frequent pattern mining was introduced in 1993, we had hundreds of algorithms, but the pattern explosion problem, so actually in some sense you can say mining all frequent itemsets.",
                    "label": 0
                },
                {
                    "sent": "It's somewhat wrong problem that you were studying and we were just doing it as if it wasn't all operation trying to do it as efficiently as possible, but then generating a load of patterns, many of which are actually not that interesting.",
                    "label": 0
                },
                {
                    "sent": "So in early 2000.",
                    "label": 0
                },
                {
                    "sent": "So a lot of condensed representations coming up nondurable items.",
                    "label": 0
                },
                {
                    "sent": "It was one of them at the contribution of our epic ATD paper was a sound complete and non redundant set of rules to derive support of items.",
                    "label": 0
                },
                {
                    "sent": "It's a unifying framework and a competitive condensed representation.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have more recent approaches which are more going to the statistical way which are based on the minimal description, link principle or other based on statistics with no models expressing expectation.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is the future?",
                    "label": 0
                },
                {
                    "sent": "If you want to work on this is to make these approaches more practical, because right now most of these statistically based methods and also the encoding based methods.",
                    "label": 0
                },
                {
                    "sent": "They work only on toy examples.",
                    "label": 0
                },
                {
                    "sent": "Now maybe I'm a little bit too.",
                    "label": 0
                },
                {
                    "sent": "Too harsh here.",
                    "label": 0
                },
                {
                    "sent": "But there is a performance problem.",
                    "label": 0
                },
                {
                    "sent": "You cannot apply it on industry size datasets.",
                    "label": 0
                },
                {
                    "sent": "Extending it to other pattern Amaze is also an interesting way to go to sequences.",
                    "label": 0
                },
                {
                    "sent": "There already are some proposals.",
                    "label": 0
                },
                {
                    "sent": "There's two graphs, or to dynamic graphs where we have, for example, the networks that are evolving and now you want to describe the evolution of networks have patterns for that, and then it becomes far less obvious what would be the.",
                    "label": 0
                },
                {
                    "sent": "I like the maximal entropy distribution in the last remark very personal remark from me as a reviewer.",
                    "label": 0
                },
                {
                    "sent": "And please please please stop making new algorithms for mining all frequent itemsets, because I simply think that we already have very good algorithms.",
                    "label": 0
                },
                {
                    "sent": "We already have hit the bottom there, and I don't think there is any need for more algorithms for mining.",
                    "label": 0
                },
                {
                    "sent": "Oh frequent itemsets, that's about it.",
                    "label": 0
                },
                {
                    "sent": "So suppose I want to apply this to all the data from Safeway.",
                    "label": 0
                },
                {
                    "sent": "All the products people buy.",
                    "label": 0
                },
                {
                    "sent": "There's two things that I want in addition to the the inclusion rules.",
                    "label": 0
                },
                {
                    "sent": "One is that there's an SKU for every single little stupid product.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "The Dannon yogurt, pineapple flavor, light size, 8 ounce, and the 16 ounce, etc.",
                    "label": 0
                },
                {
                    "sent": "So mining these is too many, so there's a hierarchy over these, so I can generalize it by Dan.",
                    "label": 0
                },
                {
                    "sent": "And I can generalize it by size of yogurt milk.",
                    "label": 0
                },
                {
                    "sent": "Etc.",
                    "label": 0
                },
                {
                    "sent": "So there's a whole bunch of inclusion sets there that you might be able to leverage.",
                    "label": 0
                },
                {
                    "sent": "Secondly, it's about profit, so there's different profit margins and all these things, so I want to have you know a real value associated with profit margin on these different subsets.",
                    "label": 0
                },
                {
                    "sent": "Have someone already done this?",
                    "label": 0
                },
                {
                    "sent": "Is this great future work?",
                    "label": 0
                },
                {
                    "sent": "Is it just an application well?",
                    "label": 0
                },
                {
                    "sent": "There has been quite some work on generalizing frequently set mining to this higher keys as you certain to mine Association rules between the higher keys.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So definitely there and also these different profit margins.",
                    "label": 0
                },
                {
                    "sent": "Also there some work has been done, so most of the time it's either so for the first one if it's higher key is more in the way patterns are being generated that most work is there.",
                    "label": 0
                },
                {
                    "sent": "So how can I mine efficiently at different levels in my hierarchy and probably also with different support thresholds?",
                    "label": 0
                },
                {
                    "sent": "And for the second one, there have been some proposals of utility based data mining where you would actually not compute the support of the item sets which would impose some kind of utility constraint on your item sets, not express how useful your items that is.",
                    "label": 0
                },
                {
                    "sent": "For example the the costs of the total cost of your item set.",
                    "label": 0
                },
                {
                    "sent": "And then you try to mine.",
                    "label": 0
                },
                {
                    "sent": "All those items that satisfies a minimal utility threshold.",
                    "label": 0
                },
                {
                    "sent": "So there have been some works if you talk about the combination of the two and make it condensed representations for this utility based item sets.",
                    "label": 0
                },
                {
                    "sent": "I'm not aware of any works there and I don't know if it will be easy to generalize the frameworks there, but it could actually be.",
                    "label": 0
                },
                {
                    "sent": "A nice way to future research.",
                    "label": 0
                },
                {
                    "sent": "There were, yeah, so mining frequent itemsets is somehow related to privacy considerations.",
                    "label": 0
                },
                {
                    "sent": "Say for example K anonymity requires you have at least K objects which you don't know where you are not able to separate them.",
                    "label": 0
                },
                {
                    "sent": "There are some connections, or is there some research on that topic combining itemset mining with privacy issues?",
                    "label": 0
                },
                {
                    "sent": "Well, in one of my papers I've mentioned this is a.",
                    "label": 0
                },
                {
                    "sent": "Possible application of this derivation rules and obtaining the sets in the sense of OK.",
                    "label": 0
                },
                {
                    "sent": "Suppose you do mining on an item set and you want to publish the results.",
                    "label": 0
                },
                {
                    "sent": "For example, the Federal Bureau of Statistics want to publish some of the statistics about their data, which corresponds roughly to the frequencies of item sets.",
                    "label": 0
                },
                {
                    "sent": "And now you want to know by publishing.",
                    "label": 0
                },
                {
                    "sent": "These statistics, but to actually reveal about my data set.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, if you look at outputs anonymity, how much does my output reveal from my data set itself?",
                    "label": 0
                },
                {
                    "sent": "There it can vary in that context, it can be helpful, yes.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks for the nice talk.",
                    "label": 0
                }
            ]
        }
    }
}