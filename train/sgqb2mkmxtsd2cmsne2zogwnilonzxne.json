{
    "id": "sgqb2mkmxtsd2cmsne2zogwnilonzxne",
    "title": "Penalized empirical risk minimization in the estimation of thresholds",
    "info": {
        "author": [
            "Leila Mohammadi, Eurandom"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2004",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mcslw04_mohammadi_perme/",
    "segmentation": [
        [
            "The estimation thresholds image shop.",
            "OK, thank you very much.",
            "And we already.",
            "So if you talks about penalized empirical is minimization?",
            "OK, this is another one for perhaps a bit different."
        ],
        [
            "So as usual, we have a random variable XY we take.",
            "IID random sample of size NX1Y1X NYN and we call X vector X 1X10.",
            "We call X an instance and suppose it comes from SpaceX.",
            "And why is the label?",
            "It takes values minus one one, so it is a classification problem we want.",
            "To predict the value of Y foreign UX if you take any extra this SpaceX, what is the label Y of X?",
            "A base classifier is a function edge from X 2 -- 1 and one.",
            "And because their collection of base classifiers is set edge, so now it is a general class of classifier.",
            "But later in my example I show you a very strict classical us for years.",
            "So the misclassification happens if y * H of X is negative.",
            "Because it takes values minus one.",
            "One Y also takes values minus one one.",
            "So we misclassify extend by by HF y * H of X is negative.",
            "So at this moment I consider very general loss function L from R-20 and Infinity.",
            "Suppose L is bounded.",
            "So."
        ],
        [
            "Empirical risk is.",
            "Expectation of L of minus Y times each of EXE under the empirical distribution function.",
            "So we use L of minus YH of X because we want to minimize the loss function.",
            "So we somehow it's better to use this for minus one YH of X.",
            "So it can be written in this form 1 / N Some Salvation I from one of minus yhr XI.",
            "And then that the article is ezelle of Edge, the expectation of this.",
            "So this helps.",
            "H only takes the values plus or minus one, yes, right?",
            "So of course it was founded because we already existing L of minus one another.",
            "OK, so this is first identified, but then I thought OK, maybe I need it later.",
            "OK. And this is a love edge.",
            "So."
        ],
        [
            "Oh suppose the class edge is too complex.",
            "It can be a very large killer 7 classifier, so we use a penalty P / H. And then so.",
            "Usually we minimize empirical reason wiser.",
            "But if H is very large, very complex, then we use a penalty P ^2 H we add a penalty to LNR and then we minimize over Nicholas H. And then again we use the at zero.",
            "That is, the admin of elevates the minimum of the theoretical risk.",
            "So we want to show the.",
            "And how fast it chats?",
            "Gets closer to it's not.",
            "So this is what we want to show the bound for the error of a chat in.",
            "We call Tower Square Edge conditional H3.",
            "All of it is.",
            "That is the order for this coverage minus LHT plus psquare of H. So it is just a definition and if we have a bound for this then we are already happy.",
            "That was good of each hat.",
            "In conditional it's not.",
            "So this is just a definition.",
            "And then we restrict ourselves to the killers of Edge in edge, such that our square is a smaller or equal to Delta sketch O of Delta.",
            "So first we do our method using this class and then we.",
            "We can set the class H. For some reason we have to restrict ourselves to not of Delta because then this is a small Delta square, so we have a bound for the penalty and we have a bound for the excess risk.",
            "Then we can do more.",
            "But then we also use another method to generalize to the case of the whole H. And here we need to define UI of edge.",
            "L of minus YH of XI minus expectation of this conditional X.",
            "So this expectation is over Y.",
            "This is the definition of uih.",
            "And then we consider an metrics on HD one up to BN.",
            "And we assume that the absolute value of you."
        ],
        [
            "I have 8 minus.",
            "UHT is is not an awkward too.",
            "The absolute value of a random variable WI.",
            "Times the I that we fixed here, the run up to the the eye of H of XI still doubt XI for all I.",
            "And for all, the Chan is still in edge.",
            "And this will you always suppose are uniformly sub kocian such that for an M and Sigma 0 squared, the maximum of Ms squared times expectation of Exp.",
            "Absolute value of OK, this doesn't matter.",
            "W I ^2 / X ^2 -- 1 is a smaller local two Sigma 0 square.",
            "Later I will show you why I consider this assumption, and then before dsquared of HNH deal is.",
            "Run over and some of the I squared of each honesty.",
            "Later again, I come back to this definition.",
            "Here is the definition of entropy.",
            "Probably a lot of you know this.",
            "If P is a subset of metric space endowed with a metric M that you covering number NUT&M is defined as the number of balls.",
            "OK, it should be the smallest number of balls introduced, right?",
            "You that covers T the smallest number of balls that covers.",
            "TV tributs with respect to this metric and I fixed here.",
            "And then the EU entropy is defined as the logarithm of this time.",
            "OK.",
            "The end of HI.",
            "Define as LNH minus a luggage that is empirical process.",
            "That"
        ],
        [
            "That is what I want to find a bond for.",
            "So the first lemma suppose all these assumptions hold and also the Supreme of the of HNH North over the whole killers edge.",
            "Is bounded by us, so this is a fixed number.",
            "Then for some 7 the constants, even depending on M and Sigma, not.",
            "And for all Sigma positive and Delta positive, satisfying this assumption.",
            "We have the probability of supremum of.",
            "The Vienna page, now that is defined here, the end of it, minus VN of HB?",
            "Get an article to Delta.",
            "And 1 / N summation W. I scored a smaller local to Sigma Square.",
            "This probability is bounded from above by C1.",
            "XP minus Anna Sigma Delta squared over C1 square square.",
            "So this assumption is for having a bound for the entropy of the class edge.",
            "If you have this here, I.",
            "So something because first I didn't want to prove it, but I can also show you a little bit of proof.",
            "Suppose that W is a random variable with expectation of the value is 0.",
            "I'm suppose K squared expectation of each cover W square over K square for some K. Minus one is 1.2610.",
            "Then we can show that expectation of E to the power, but the value is a small town or equal to eat the power.",
            "2.",
            "Ghesquiere plastic, Monroe Square times, but I scored.",
            "So if we have this on these.",
            "We imply this.",
            "To prove this, we use the Championships inequality 1st and then we.",
            "Do some.",
            "Preliminary calculations and then we get this.",
            "And then if we have that value.",
            "So this is for one level.",
            "You must now suppose this holds for all WI.",
            "Photo essay.",
            "And the random variable WI such that expectational WR is.",
            "Then for all.",
            "For.",
            "Oh , in our end.",
            "And for all, a positive probability of.",
            "Absolute value formation WI.",
            "Come on, I pick it out going to a. OK.",
            "So first, if you have for sun W such that expectation of WO and this happened, we have this and then if it happens for all Wii from one to N that expects MWI is 0, then for all gamma in.",
            "So gamma is.",
            ", one.",
            "So come on in RN and for all a positive we have this.",
            "And then using this.",
            ", stuff like am I OK?",
            "Yes yes.",
            "But to prove this, the proof of this is a little bit longer, but the proof of this is sure.",
            "And then we can have something like.",
            "This.",
            "But first we use instead of the end of HI minus V NIH.",
            "We use U either if you want to remember the definition is here.",
            "First we use for UI of H minus IUR, which not.",
            "And then this is very easy and very very easy.",
            "Sequence of that of that using UI.",
            "So then we have this inequality that is.",
            "The key for having the bond.",
            "So for the.",
            "Power squared, that was.",
            "That was good of it had to end conditional, it's not.",
            "So to have a bound for the error of this, we need to have this lemma.",
            "Fast.",
            "Under integral you have Du or.",
            "I don't understand this notation with us.",
            "This one so this.",
            "This is the whole integral Max maximum of the whole integral and.",
            "This is for maximum.",
            "Yeah.",
            "Yeah yeah, maximum of this integral and.",
            "And because I'm below you have two conditions, right?",
            "Yeah, yeah, yeah.",
            "And the other one.",
            "So first we have this assumption.",
            "This notation this is F right?",
            "Which.",
            "Oh yes, yes.",
            "Here it means most happened.",
            "Yeah, sorry, I thought everybody knows this.",
            "So this means both this happen and this means the maximum of this is smaller than this.",
            "OK. Um?",
            "So.",
            "Here we have an assumption."
        ],
        [
            "I call assumption a.",
            "Suppose there are, at a positive Anki, bigger than one such that L of H minus elevation or is bigger alcohol to Ester D to the power came of age and still for all edge image.",
            "This assumption has been used a lot in classification.",
            "I think Chewbacca was the first one who used this assumption, and.",
            "This is like.",
            "Sorry this is not so yeah.",
            "I have seen such assumption in classification problem such as you founding rates of convergence for minimizer of support vector machine.",
            "That's our present as last year.",
            "Here's something like this and also in other words, overachiever, Kozan, some other people.",
            "So T here is OK, the if you remember.",
            "We fix NVI.",
            "OK, and then the square is 1 / N some of the power, two of each on HT.",
            "So we have a metric D. The assumption is that this metric is bounded from above by L of H minus allow.",
            "HTH not so this metric of HNH not.",
            "And then you know that this is positive because each note was the minimizer of their theoretical risk.",
            "So this is almost positive.",
            "The.",
            "The the symmetry.",
            "Yeah, but it wasn't.",
            "It was some average weather.",
            "Its square was an average of.",
            "In other metrics.",
            "And I suppose that.",
            "I.",
            "So.",
            "OK, we.",
            "Pi.",
            "But it could be a random matrix.",
            "So.",
            "The eye doesn't depend on data, but.",
            "And we can first, can we have this?",
            "So in practice we can have this conditional on X and then we take the expectation.",
            "But so in my example, VI doesn't depend on data.",
            "That doesn't mean that I.",
            "So.",
            "The.",
            "So let's see what is the point of having it depending on.",
            "I if the paper is my ID and.",
            "D should not depend on the base.",
            "Anne.",
            "OK, we will see this in the example later.",
            "Suppose assumption a.",
            "And then suppose this same assumption we had before.",
            "So here, before we had this small down something and.",
            "And now I suppose we for Delta is a small dent, is function of Delta side of Delta, such that side of deltoid forward 2 / K over Delta Square is in an increasing function of Delta.",
            "If that happens, then.",
            "That exist, seeing constancy through such that for.",
            "A sqrt N Delta square bigger than or equal to see two PSI.",
            "Delta in power 2 / K and for all Delta, bigger on call to Delta N. We have this inequality.",
            "So you remember P squared was there.",
            "Penalty that we considered for each night.",
            "And Tower Square is L of.",
            "It had to end minus L of each North plus there.",
            "Penalty of a chat in.",
            "Here.",
            "So.",
            "Here we have a bound for the probability of these to be large, and Now I show you a little bit how to prove this.",
            "First, we have this inequality because H had to N is the minimizer of Ln of H plus."
        ],
        [
            "PS Squared of edge.",
            "So of course this is a small quantities and then we subscribe something.",
            "To have this.",
            "Then as a definition we have this.",
            "And we end up it was a Lenovo H -- 11 H. So.",
            "The probability of power square edge has been conditional.",
            "It's not real call to 2.",
            "PS Code of it's not.",
            "Plus Delta Square is a small tool some of.",
            "The property of supremum of.",
            "This.",
            "So so before we put supermarket is this.",
            "But then we put the supremum over all.",
            "But here we use this assumption that.",
            "Anne.",
            "They have this.",
            "He used this assumption.",
            "We write this as this and then.",
            "No, sorry, first we write this as the summation of this can be Infinity.",
            "So this is the whole edge.",
            "So first we just write this as this.",
            "As a consequence of the Union of probability, and then we use that assumption.",
            "We also use assumption A.",
            "Then if H is in this set, it's not up to power is times Delta.",
            "Then we know that."
        ],
        [
            "This was the killers of old age, such that Tower Square of each constellation, or small town.",
            "For the call to this forward 2 two to the Power 2 S times Delta is square.",
            "So if H is in this colors then B.",
            "Then OK first.",
            "Explanation of the line above.",
            "OK, first we have this.",
            "This is just.",
            "Very easy, OK?",
            "Because this because this is the union of all edge nuts.",
            "Of this is a subset of old age is in fact all edge.",
            "And then we use assumption A. Anne.",
            "OK."
        ],
        [
            "Do you have question here?",
            "You said that was from, so that's just one 30, but this is so sorry I made a mistake.",
            "So the first inequality is just very easy.",
            "Because.",
            "If we have H. If you have this supremo, and then H is in the class edge.",
            "Then it can be in one of these classes.",
            "It's not, so then we have this.",
            "There's a picture of 2 S&L squared on the right hand side.",
            "Yeah.",
            "OK, there is one is OK, OK?",
            "OK, between these two there is one step that we use.",
            "These and then.",
            "If we have this is bigger than this, then you see.",
            "This is also here.",
            "This is also here, so if this beer and then we use this here because the these then we get something like this.",
            "Yes, there is one step before between these two, but then after that we use assumption A.",
            "The."
        ],
        [
            "That we have a bound over this D and then if we are in edge not up to the power S times Delta.",
            "In fact this is a smaller than two power 2D, so this is a small done.",
            "This needs further K. Is a smarter something, so we have this."
        ],
        [
            "And after this we used to move on.",
            "For all are such that red sqrt N * R is bigger than this.",
            "Then if we have this, then we have.",
            "Anne.",
            "If we have this as we get on this then."
        ],
        [
            "This is also bigger than this.",
            "Then we use Lemma Von to have this."
        ],
        [
            "OK this.",
            "Was a sketch of the proof of this theorem.",
            "Then we have.",
            "Is storing."
        ],
        [
            "We have advanced for this probability.",
            "And then as a consequence of."
        ],
        [
            "Theorem one we have this theorem.",
            "Two that is a bound for over the expectation.",
            "Under the assumption of theorem 14K is 2.",
            "We arrived at things in quality.",
            "So.",
            "But where we have four P squared away, it's not that is a function of N. Here we can sorry this Delta N. That's the function of N this.",
            "Here we can have the rate of convergence of expectation of the square of a chat in Council.",
            "Now we have an application."
        ],
        [
            "So first I considered before that.",
            "Suppose we have.",
            "The probability of Y is 1 conditional leagues.",
            "This is one of the we have to dance with the support just a minute.",
            "That we have to density then.",
            "For example, if the 10 still are in this fall then there is just one point that's here, for example this.",
            "This is design.",
            "This is this.",
            "Then there is just one point that here we preferred to us to.",
            "And classify why is 1 and here we kill a spider is minus one.",
            "But now suppose we have the true density like this, with more.",
            "Coin such that one of the dens is bigger than the other.",
            "Then we have many points.",
            "I can call them A1A K. An If care is very big.",
            "Then we have a big class of classifier.",
            "And then we use the penalty over K. So my assumption is that we have two dense.",
            "OK, we don't talk about densities, but.",
            "We we consider classical aspires like this.",
            "H is the colossa edge.",
            "Such stuff is so here I have a. OK, even up to a K1 if it's of X. EXE is in, for example A2I and A2I plus one and minus one.",
            "Otherwise, class of oldies.",
            "And then I consecrate very large.",
            "It can be anything so the killers of edge.",
            "This colors of edges very.",
            "An week so here I suppose X is the interval, 01 on HX is this.",
            "Even this big K can change minus one or one, so we have both cases.",
            "Start with minus one or start with one.",
            "And then.",
            "This H is the class of all this HA.",
            "And then you is.",
            "But I just define you as the class of all parameter A.",
            "Capital case fix.",
            "That is what I want to say.",
            "K can be anything.",
            "So first for each?",
            "Yes, yes yes here here.",
            "And so let me.",
            "So first I consider one of these edge, but then I cancel that Union.",
            "So yes it should be edge of an index K, But then I consider union in my theorem I constantly union of HK OK but now it's."
        ],
        [
            "So I consider just I think I wrote somewhere.",
            "Yes, I consider this L of T. Is."
        ],
        [
            "The indicator function T is positive.",
            "And then.",
            "The theoretical error is the L of F that is the property of items above X negative.",
            "An empirical error is the percentage of the mass killers, for example.",
            "An I call F0 of X.",
            "The property over is unconditional leaks.",
            "So I. I didn't write it here, but the so I think.",
            "I define.",
            "The eye of edge and.",
            "HD something click expectation of.",
            "Anne.",
            "They indicate pull of each of these.",
            "OK, yeah.",
            "It's exciting.",
            "Yeah, I think I just I defined like this.",
            "And then.",
            "Yes.",
            "OK. And then the square of Hi still is this.",
            "So that's I defined before that 1 / N summation is, and this is this probability conditional on X.",
            "And then we take the expectation here.",
            "Probability there.",
            "This was one of our end with some of the I ^2.",
            "So then the.",
            "So I so first I used the squared of H and still something like this was OK.",
            "I define conditional on eggs.",
            "OK that is 1 / N some of it.",
            "Indicator of each of its eyes.",
            "Is that the correct thing though it's I.",
            "But then I take the expectation of this.",
            "Then I have this.",
            "This is also clear here because assumption a in my example becomes assumption be that I wrote here there is 1/3 positive such that the absolute value of two F0 of X -- 1 is bigger and we get an ETA for all X except.",
            "AO I4I is 1K at a zero is argument of L of HA.",
            "So we're friends.",
            "I fixed a zero that is arguable of HA.",
            "Overall a.",
            "Then I assume that the probability this zero is the probability of.",
            "Why is 1 conditional leaks?",
            "I suppose this probability is away from 1/2 everywhere, but maybe not at a zero and I for all I.",
            "So you see, it is true for all leagues.",
            "So then I can first conditional I use the conditional on X and then I expect I take the expectation that is.",
            "That is why I am allowed to do that, because this assumption is so before we have an assumption a.",
            "Was that L of each minus L of H2 BK then?",
            "This car but here I use something like this.",
            "The probability of edge of.",
            "So first I use this conditionally Onyx.",
            "So in my example this is true when we have this conditional on X, we have this concern X and this.",
            "We use this.",
            "And then we take the expectation.",
            "So in general, maybe I cannot use this again this definition?",
            "That discussion on expert in this example I can do because my assumption becomes this.",
            "So.",
            "We have this now.",
            "We have even conditionally on X.",
            "This is true.",
            "And then.",
            "I assume that H."
        ],
        [
            "Is the class of all H1 of K overall K?",
            "The Union of all this A.",
            "Of K&KI for each a I have a case.",
            "OK is also a function of a.",
            "An I use this penalty P squared of HA is elected square for some lab down K a / N. So.",
            "If I can have laptop square as a function of N that is love the N. Then using lemma.",
            "An using our theorem one we can find a bound for the Tau squared up at 10.",
            "That shows us the rate of conversion.",
            "So consider the.",
            "This is a lemma that I used in the proof of this story.",
            "This is just something.",
            "Outside this this paper, it can be very easy considered the L1 metric on RM.",
            "We define the~ van B is the summation of absolute value on my mistake BI.",
            "Then at Boulby till the end of our with respect to this metric can be covered by N at most.",
            "The integer part of R / U.",
            "Plus one to power M balls with reduce U.",
            "That is what I use for this entropy calculation.",
            "So this is a lemma that I use and then I use our Theorem 2.",
            "To prove this.",
            "Suppose assumption B and then for laptop bigger or call 2 account stantons square root of like N. That's the fun can see.",
            "Five can depend on M. We have this expectation of Tau squared up at an conditional edge.",
            "Not is bounded by two.",
            "The square of it not plus C 5 / N. And because PS squared is defined as here.",
            "Then if we take laughter is equal to square root of like N, it becomes like an over N * K RK.",
            "So the rate of convergence of Edge hat in is.",
            "Like an overlay.",
            "This is our conclusion, so it means that if we have the densities like what I said, but because of assumption a we have.",
            "If.",
            "The zero.",
            "Avexis probably always one conditional picks.",
            "This is away from 1/2.",
            "It means that it is of this kind.",
            "This is 1/2.",
            "Something like this?",
            "If we have such a situation, then.",
            "If we consider the class of all classifier with Kate Threshold that care is unknown.",
            "So we consider the union of all them and we penalized empirical reason minimize it.",
            "The rate of convergence is like an over N. And we know that, OK?",
            "This is another result that I got, but it is easy to see if K is fixed and we have another example.",
            "K is one, we have just one of these, then the rate of convergence is 1 / N. This is for the case care is known.",
            "Good care is unknown.",
            "So this is the final result.",
            "That's all.",
            "Questions over.",
            "This.",
            "Pizza or something?",
            "So you're.",
            "You supposed FCO.",
            "Translate most eater over these.",
            "Positions.",
            "So there the assumption was this.",
            "All sorry 2.",
            "Yes, that was 2 -- 1 is.",
            "Minus one is bigger than ether.",
            "So yeah, it should be.",
            "Something like.",
            "It.",
            "Yeah.",
            "Final position.",
            "Anne.",
            "So because it is a constant problem, it is in the constant because it.",
            "Depends on capital M, but also easy.",
            "Yes.",
            "Let me see.",
            "Because Lambda Lambda is the penalty, right?",
            "Yes.",
            "Can only use this if we know what C5.",
            "His.",
            "So this.",
            "Yes, it is in the constant.",
            "Well, what's the problem?",
            "Well, if you want to use this in practice, you have to take Lambda bigger than C5.",
            "Group of men.",
            "Yeah.",
            "Constant depending on M. Anne.",
            "Depends on more things also.",
            "OK.",
            "So you would have to specify also eaten.",
            "His I think if this sleep 5 is a is a large constant.",
            "I didn't think about improving the constant.",
            "But the question is, what does it depend on?",
            "Because if you want to use this, you have to be able to.",
            "Yeah yes, but yeah this assumption beat that I have here.",
            "Yes, we we assume that this appetizer constanten.",
            "And then C5 may depend on it I think.",
            "This jump is a minimal minimal job.",
            "Minimal yeah yeah.",
            "His.",
            "Yeah, so the regularity condition.",
            "You will.",
            "So.",
            "Again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The estimation thresholds image shop.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "And we already.",
                    "label": 0
                },
                {
                    "sent": "So if you talks about penalized empirical is minimization?",
                    "label": 0
                },
                {
                    "sent": "OK, this is another one for perhaps a bit different.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as usual, we have a random variable XY we take.",
                    "label": 0
                },
                {
                    "sent": "IID random sample of size NX1Y1X NYN and we call X vector X 1X10.",
                    "label": 0
                },
                {
                    "sent": "We call X an instance and suppose it comes from SpaceX.",
                    "label": 0
                },
                {
                    "sent": "And why is the label?",
                    "label": 0
                },
                {
                    "sent": "It takes values minus one one, so it is a classification problem we want.",
                    "label": 0
                },
                {
                    "sent": "To predict the value of Y foreign UX if you take any extra this SpaceX, what is the label Y of X?",
                    "label": 0
                },
                {
                    "sent": "A base classifier is a function edge from X 2 -- 1 and one.",
                    "label": 0
                },
                {
                    "sent": "And because their collection of base classifiers is set edge, so now it is a general class of classifier.",
                    "label": 1
                },
                {
                    "sent": "But later in my example I show you a very strict classical us for years.",
                    "label": 0
                },
                {
                    "sent": "So the misclassification happens if y * H of X is negative.",
                    "label": 0
                },
                {
                    "sent": "Because it takes values minus one.",
                    "label": 0
                },
                {
                    "sent": "One Y also takes values minus one one.",
                    "label": 0
                },
                {
                    "sent": "So we misclassify extend by by HF y * H of X is negative.",
                    "label": 1
                },
                {
                    "sent": "So at this moment I consider very general loss function L from R-20 and Infinity.",
                    "label": 0
                },
                {
                    "sent": "Suppose L is bounded.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Empirical risk is.",
                    "label": 0
                },
                {
                    "sent": "Expectation of L of minus Y times each of EXE under the empirical distribution function.",
                    "label": 0
                },
                {
                    "sent": "So we use L of minus YH of X because we want to minimize the loss function.",
                    "label": 0
                },
                {
                    "sent": "So we somehow it's better to use this for minus one YH of X.",
                    "label": 0
                },
                {
                    "sent": "So it can be written in this form 1 / N Some Salvation I from one of minus yhr XI.",
                    "label": 1
                },
                {
                    "sent": "And then that the article is ezelle of Edge, the expectation of this.",
                    "label": 0
                },
                {
                    "sent": "So this helps.",
                    "label": 0
                },
                {
                    "sent": "H only takes the values plus or minus one, yes, right?",
                    "label": 0
                },
                {
                    "sent": "So of course it was founded because we already existing L of minus one another.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is first identified, but then I thought OK, maybe I need it later.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is a love edge.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh suppose the class edge is too complex.",
                    "label": 0
                },
                {
                    "sent": "It can be a very large killer 7 classifier, so we use a penalty P / H. And then so.",
                    "label": 0
                },
                {
                    "sent": "Usually we minimize empirical reason wiser.",
                    "label": 0
                },
                {
                    "sent": "But if H is very large, very complex, then we use a penalty P ^2 H we add a penalty to LNR and then we minimize over Nicholas H. And then again we use the at zero.",
                    "label": 0
                },
                {
                    "sent": "That is, the admin of elevates the minimum of the theoretical risk.",
                    "label": 0
                },
                {
                    "sent": "So we want to show the.",
                    "label": 0
                },
                {
                    "sent": "And how fast it chats?",
                    "label": 0
                },
                {
                    "sent": "Gets closer to it's not.",
                    "label": 0
                },
                {
                    "sent": "So this is what we want to show the bound for the error of a chat in.",
                    "label": 0
                },
                {
                    "sent": "We call Tower Square Edge conditional H3.",
                    "label": 0
                },
                {
                    "sent": "All of it is.",
                    "label": 0
                },
                {
                    "sent": "That is the order for this coverage minus LHT plus psquare of H. So it is just a definition and if we have a bound for this then we are already happy.",
                    "label": 0
                },
                {
                    "sent": "That was good of each hat.",
                    "label": 0
                },
                {
                    "sent": "In conditional it's not.",
                    "label": 0
                },
                {
                    "sent": "So this is just a definition.",
                    "label": 0
                },
                {
                    "sent": "And then we restrict ourselves to the killers of Edge in edge, such that our square is a smaller or equal to Delta sketch O of Delta.",
                    "label": 0
                },
                {
                    "sent": "So first we do our method using this class and then we.",
                    "label": 0
                },
                {
                    "sent": "We can set the class H. For some reason we have to restrict ourselves to not of Delta because then this is a small Delta square, so we have a bound for the penalty and we have a bound for the excess risk.",
                    "label": 0
                },
                {
                    "sent": "Then we can do more.",
                    "label": 0
                },
                {
                    "sent": "But then we also use another method to generalize to the case of the whole H. And here we need to define UI of edge.",
                    "label": 0
                },
                {
                    "sent": "L of minus YH of XI minus expectation of this conditional X.",
                    "label": 0
                },
                {
                    "sent": "So this expectation is over Y.",
                    "label": 0
                },
                {
                    "sent": "This is the definition of uih.",
                    "label": 0
                },
                {
                    "sent": "And then we consider an metrics on HD one up to BN.",
                    "label": 0
                },
                {
                    "sent": "And we assume that the absolute value of you.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have 8 minus.",
                    "label": 0
                },
                {
                    "sent": "UHT is is not an awkward too.",
                    "label": 0
                },
                {
                    "sent": "The absolute value of a random variable WI.",
                    "label": 0
                },
                {
                    "sent": "Times the I that we fixed here, the run up to the the eye of H of XI still doubt XI for all I.",
                    "label": 0
                },
                {
                    "sent": "And for all, the Chan is still in edge.",
                    "label": 0
                },
                {
                    "sent": "And this will you always suppose are uniformly sub kocian such that for an M and Sigma 0 squared, the maximum of Ms squared times expectation of Exp.",
                    "label": 1
                },
                {
                    "sent": "Absolute value of OK, this doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "W I ^2 / X ^2 -- 1 is a smaller local two Sigma 0 square.",
                    "label": 0
                },
                {
                    "sent": "Later I will show you why I consider this assumption, and then before dsquared of HNH deal is.",
                    "label": 0
                },
                {
                    "sent": "Run over and some of the I squared of each honesty.",
                    "label": 0
                },
                {
                    "sent": "Later again, I come back to this definition.",
                    "label": 0
                },
                {
                    "sent": "Here is the definition of entropy.",
                    "label": 0
                },
                {
                    "sent": "Probably a lot of you know this.",
                    "label": 0
                },
                {
                    "sent": "If P is a subset of metric space endowed with a metric M that you covering number NUT&M is defined as the number of balls.",
                    "label": 1
                },
                {
                    "sent": "OK, it should be the smallest number of balls introduced, right?",
                    "label": 0
                },
                {
                    "sent": "You that covers T the smallest number of balls that covers.",
                    "label": 0
                },
                {
                    "sent": "TV tributs with respect to this metric and I fixed here.",
                    "label": 0
                },
                {
                    "sent": "And then the EU entropy is defined as the logarithm of this time.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The end of HI.",
                    "label": 0
                },
                {
                    "sent": "Define as LNH minus a luggage that is empirical process.",
                    "label": 0
                },
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That is what I want to find a bond for.",
                    "label": 0
                },
                {
                    "sent": "So the first lemma suppose all these assumptions hold and also the Supreme of the of HNH North over the whole killers edge.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by us, so this is a fixed number.",
                    "label": 0
                },
                {
                    "sent": "Then for some 7 the constants, even depending on M and Sigma, not.",
                    "label": 1
                },
                {
                    "sent": "And for all Sigma positive and Delta positive, satisfying this assumption.",
                    "label": 0
                },
                {
                    "sent": "We have the probability of supremum of.",
                    "label": 0
                },
                {
                    "sent": "The Vienna page, now that is defined here, the end of it, minus VN of HB?",
                    "label": 0
                },
                {
                    "sent": "Get an article to Delta.",
                    "label": 0
                },
                {
                    "sent": "And 1 / N summation W. I scored a smaller local to Sigma Square.",
                    "label": 0
                },
                {
                    "sent": "This probability is bounded from above by C1.",
                    "label": 0
                },
                {
                    "sent": "XP minus Anna Sigma Delta squared over C1 square square.",
                    "label": 0
                },
                {
                    "sent": "So this assumption is for having a bound for the entropy of the class edge.",
                    "label": 0
                },
                {
                    "sent": "If you have this here, I.",
                    "label": 0
                },
                {
                    "sent": "So something because first I didn't want to prove it, but I can also show you a little bit of proof.",
                    "label": 0
                },
                {
                    "sent": "Suppose that W is a random variable with expectation of the value is 0.",
                    "label": 0
                },
                {
                    "sent": "I'm suppose K squared expectation of each cover W square over K square for some K. Minus one is 1.2610.",
                    "label": 0
                },
                {
                    "sent": "Then we can show that expectation of E to the power, but the value is a small town or equal to eat the power.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Ghesquiere plastic, Monroe Square times, but I scored.",
                    "label": 0
                },
                {
                    "sent": "So if we have this on these.",
                    "label": 0
                },
                {
                    "sent": "We imply this.",
                    "label": 0
                },
                {
                    "sent": "To prove this, we use the Championships inequality 1st and then we.",
                    "label": 0
                },
                {
                    "sent": "Do some.",
                    "label": 0
                },
                {
                    "sent": "Preliminary calculations and then we get this.",
                    "label": 0
                },
                {
                    "sent": "And then if we have that value.",
                    "label": 0
                },
                {
                    "sent": "So this is for one level.",
                    "label": 0
                },
                {
                    "sent": "You must now suppose this holds for all WI.",
                    "label": 0
                },
                {
                    "sent": "Photo essay.",
                    "label": 0
                },
                {
                    "sent": "And the random variable WI such that expectational WR is.",
                    "label": 0
                },
                {
                    "sent": "Then for all.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Oh , in our end.",
                    "label": 0
                },
                {
                    "sent": "And for all, a positive probability of.",
                    "label": 0
                },
                {
                    "sent": "Absolute value formation WI.",
                    "label": 0
                },
                {
                    "sent": "Come on, I pick it out going to a. OK.",
                    "label": 0
                },
                {
                    "sent": "So first, if you have for sun W such that expectation of WO and this happened, we have this and then if it happens for all Wii from one to N that expects MWI is 0, then for all gamma in.",
                    "label": 0
                },
                {
                    "sent": "So gamma is.",
                    "label": 0
                },
                {
                    "sent": ", one.",
                    "label": 0
                },
                {
                    "sent": "So come on in RN and for all a positive we have this.",
                    "label": 0
                },
                {
                    "sent": "And then using this.",
                    "label": 0
                },
                {
                    "sent": ", stuff like am I OK?",
                    "label": 0
                },
                {
                    "sent": "Yes yes.",
                    "label": 0
                },
                {
                    "sent": "But to prove this, the proof of this is a little bit longer, but the proof of this is sure.",
                    "label": 0
                },
                {
                    "sent": "And then we can have something like.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "But first we use instead of the end of HI minus V NIH.",
                    "label": 0
                },
                {
                    "sent": "We use U either if you want to remember the definition is here.",
                    "label": 0
                },
                {
                    "sent": "First we use for UI of H minus IUR, which not.",
                    "label": 0
                },
                {
                    "sent": "And then this is very easy and very very easy.",
                    "label": 0
                },
                {
                    "sent": "Sequence of that of that using UI.",
                    "label": 0
                },
                {
                    "sent": "So then we have this inequality that is.",
                    "label": 0
                },
                {
                    "sent": "The key for having the bond.",
                    "label": 0
                },
                {
                    "sent": "So for the.",
                    "label": 0
                },
                {
                    "sent": "Power squared, that was.",
                    "label": 0
                },
                {
                    "sent": "That was good of it had to end conditional, it's not.",
                    "label": 0
                },
                {
                    "sent": "So to have a bound for the error of this, we need to have this lemma.",
                    "label": 0
                },
                {
                    "sent": "Fast.",
                    "label": 0
                },
                {
                    "sent": "Under integral you have Du or.",
                    "label": 0
                },
                {
                    "sent": "I don't understand this notation with us.",
                    "label": 0
                },
                {
                    "sent": "This one so this.",
                    "label": 0
                },
                {
                    "sent": "This is the whole integral Max maximum of the whole integral and.",
                    "label": 0
                },
                {
                    "sent": "This is for maximum.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, maximum of this integral and.",
                    "label": 0
                },
                {
                    "sent": "And because I'm below you have two conditions, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "And the other one.",
                    "label": 0
                },
                {
                    "sent": "So first we have this assumption.",
                    "label": 0
                },
                {
                    "sent": "This notation this is F right?",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Here it means most happened.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry, I thought everybody knows this.",
                    "label": 0
                },
                {
                    "sent": "So this means both this happen and this means the maximum of this is smaller than this.",
                    "label": 0
                },
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we have an assumption.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I call assumption a.",
                    "label": 0
                },
                {
                    "sent": "Suppose there are, at a positive Anki, bigger than one such that L of H minus elevation or is bigger alcohol to Ester D to the power came of age and still for all edge image.",
                    "label": 0
                },
                {
                    "sent": "This assumption has been used a lot in classification.",
                    "label": 0
                },
                {
                    "sent": "I think Chewbacca was the first one who used this assumption, and.",
                    "label": 0
                },
                {
                    "sent": "This is like.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is not so yeah.",
                    "label": 0
                },
                {
                    "sent": "I have seen such assumption in classification problem such as you founding rates of convergence for minimizer of support vector machine.",
                    "label": 0
                },
                {
                    "sent": "That's our present as last year.",
                    "label": 0
                },
                {
                    "sent": "Here's something like this and also in other words, overachiever, Kozan, some other people.",
                    "label": 0
                },
                {
                    "sent": "So T here is OK, the if you remember.",
                    "label": 0
                },
                {
                    "sent": "We fix NVI.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the square is 1 / N some of the power, two of each on HT.",
                    "label": 1
                },
                {
                    "sent": "So we have a metric D. The assumption is that this metric is bounded from above by L of H minus allow.",
                    "label": 0
                },
                {
                    "sent": "HTH not so this metric of HNH not.",
                    "label": 0
                },
                {
                    "sent": "And then you know that this is positive because each note was the minimizer of their theoretical risk.",
                    "label": 0
                },
                {
                    "sent": "So this is almost positive.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The the symmetry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but it wasn't.",
                    "label": 0
                },
                {
                    "sent": "It was some average weather.",
                    "label": 0
                },
                {
                    "sent": "Its square was an average of.",
                    "label": 0
                },
                {
                    "sent": "In other metrics.",
                    "label": 0
                },
                {
                    "sent": "And I suppose that.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, we.",
                    "label": 0
                },
                {
                    "sent": "Pi.",
                    "label": 0
                },
                {
                    "sent": "But it could be a random matrix.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The eye doesn't depend on data, but.",
                    "label": 0
                },
                {
                    "sent": "And we can first, can we have this?",
                    "label": 0
                },
                {
                    "sent": "So in practice we can have this conditional on X and then we take the expectation.",
                    "label": 0
                },
                {
                    "sent": "But so in my example, VI doesn't depend on data.",
                    "label": 0
                },
                {
                    "sent": "That doesn't mean that I.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "So let's see what is the point of having it depending on.",
                    "label": 0
                },
                {
                    "sent": "I if the paper is my ID and.",
                    "label": 0
                },
                {
                    "sent": "D should not depend on the base.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK, we will see this in the example later.",
                    "label": 0
                },
                {
                    "sent": "Suppose assumption a.",
                    "label": 0
                },
                {
                    "sent": "And then suppose this same assumption we had before.",
                    "label": 0
                },
                {
                    "sent": "So here, before we had this small down something and.",
                    "label": 0
                },
                {
                    "sent": "And now I suppose we for Delta is a small dent, is function of Delta side of Delta, such that side of deltoid forward 2 / K over Delta Square is in an increasing function of Delta.",
                    "label": 0
                },
                {
                    "sent": "If that happens, then.",
                    "label": 1
                },
                {
                    "sent": "That exist, seeing constancy through such that for.",
                    "label": 0
                },
                {
                    "sent": "A sqrt N Delta square bigger than or equal to see two PSI.",
                    "label": 0
                },
                {
                    "sent": "Delta in power 2 / K and for all Delta, bigger on call to Delta N. We have this inequality.",
                    "label": 0
                },
                {
                    "sent": "So you remember P squared was there.",
                    "label": 0
                },
                {
                    "sent": "Penalty that we considered for each night.",
                    "label": 0
                },
                {
                    "sent": "And Tower Square is L of.",
                    "label": 0
                },
                {
                    "sent": "It had to end minus L of each North plus there.",
                    "label": 0
                },
                {
                    "sent": "Penalty of a chat in.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we have a bound for the probability of these to be large, and Now I show you a little bit how to prove this.",
                    "label": 0
                },
                {
                    "sent": "First, we have this inequality because H had to N is the minimizer of Ln of H plus.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "PS Squared of edge.",
                    "label": 0
                },
                {
                    "sent": "So of course this is a small quantities and then we subscribe something.",
                    "label": 0
                },
                {
                    "sent": "To have this.",
                    "label": 0
                },
                {
                    "sent": "Then as a definition we have this.",
                    "label": 0
                },
                {
                    "sent": "And we end up it was a Lenovo H -- 11 H. So.",
                    "label": 0
                },
                {
                    "sent": "The probability of power square edge has been conditional.",
                    "label": 0
                },
                {
                    "sent": "It's not real call to 2.",
                    "label": 0
                },
                {
                    "sent": "PS Code of it's not.",
                    "label": 0
                },
                {
                    "sent": "Plus Delta Square is a small tool some of.",
                    "label": 0
                },
                {
                    "sent": "The property of supremum of.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "So so before we put supermarket is this.",
                    "label": 0
                },
                {
                    "sent": "But then we put the supremum over all.",
                    "label": 0
                },
                {
                    "sent": "But here we use this assumption that.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "They have this.",
                    "label": 0
                },
                {
                    "sent": "He used this assumption.",
                    "label": 0
                },
                {
                    "sent": "We write this as this and then.",
                    "label": 0
                },
                {
                    "sent": "No, sorry, first we write this as the summation of this can be Infinity.",
                    "label": 0
                },
                {
                    "sent": "So this is the whole edge.",
                    "label": 0
                },
                {
                    "sent": "So first we just write this as this.",
                    "label": 0
                },
                {
                    "sent": "As a consequence of the Union of probability, and then we use that assumption.",
                    "label": 0
                },
                {
                    "sent": "We also use assumption A.",
                    "label": 1
                },
                {
                    "sent": "Then if H is in this set, it's not up to power is times Delta.",
                    "label": 0
                },
                {
                    "sent": "Then we know that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This was the killers of old age, such that Tower Square of each constellation, or small town.",
                    "label": 0
                },
                {
                    "sent": "For the call to this forward 2 two to the Power 2 S times Delta is square.",
                    "label": 0
                },
                {
                    "sent": "So if H is in this colors then B.",
                    "label": 1
                },
                {
                    "sent": "Then OK first.",
                    "label": 0
                },
                {
                    "sent": "Explanation of the line above.",
                    "label": 0
                },
                {
                    "sent": "OK, first we have this.",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                },
                {
                    "sent": "Very easy, OK?",
                    "label": 0
                },
                {
                    "sent": "Because this because this is the union of all edge nuts.",
                    "label": 0
                },
                {
                    "sent": "Of this is a subset of old age is in fact all edge.",
                    "label": 0
                },
                {
                    "sent": "And then we use assumption A. Anne.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do you have question here?",
                    "label": 0
                },
                {
                    "sent": "You said that was from, so that's just one 30, but this is so sorry I made a mistake.",
                    "label": 0
                },
                {
                    "sent": "So the first inequality is just very easy.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "If we have H. If you have this supremo, and then H is in the class edge.",
                    "label": 0
                },
                {
                    "sent": "Then it can be in one of these classes.",
                    "label": 0
                },
                {
                    "sent": "It's not, so then we have this.",
                    "label": 0
                },
                {
                    "sent": "There's a picture of 2 S&L squared on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, there is one is OK, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, between these two there is one step that we use.",
                    "label": 0
                },
                {
                    "sent": "These and then.",
                    "label": 0
                },
                {
                    "sent": "If we have this is bigger than this, then you see.",
                    "label": 0
                },
                {
                    "sent": "This is also here.",
                    "label": 0
                },
                {
                    "sent": "This is also here, so if this beer and then we use this here because the these then we get something like this.",
                    "label": 0
                },
                {
                    "sent": "Yes, there is one step before between these two, but then after that we use assumption A.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we have a bound over this D and then if we are in edge not up to the power S times Delta.",
                    "label": 0
                },
                {
                    "sent": "In fact this is a smaller than two power 2D, so this is a small done.",
                    "label": 0
                },
                {
                    "sent": "This needs further K. Is a smarter something, so we have this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And after this we used to move on.",
                    "label": 0
                },
                {
                    "sent": "For all are such that red sqrt N * R is bigger than this.",
                    "label": 0
                },
                {
                    "sent": "Then if we have this, then we have.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If we have this as we get on this then.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is also bigger than this.",
                    "label": 0
                },
                {
                    "sent": "Then we use Lemma Von to have this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK this.",
                    "label": 0
                },
                {
                    "sent": "Was a sketch of the proof of this theorem.",
                    "label": 0
                },
                {
                    "sent": "Then we have.",
                    "label": 0
                },
                {
                    "sent": "Is storing.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have advanced for this probability.",
                    "label": 0
                },
                {
                    "sent": "And then as a consequence of.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theorem one we have this theorem.",
                    "label": 0
                },
                {
                    "sent": "Two that is a bound for over the expectation.",
                    "label": 0
                },
                {
                    "sent": "Under the assumption of theorem 14K is 2.",
                    "label": 1
                },
                {
                    "sent": "We arrived at things in quality.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But where we have four P squared away, it's not that is a function of N. Here we can sorry this Delta N. That's the function of N this.",
                    "label": 0
                },
                {
                    "sent": "Here we can have the rate of convergence of expectation of the square of a chat in Council.",
                    "label": 0
                },
                {
                    "sent": "Now we have an application.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I considered before that.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have.",
                    "label": 0
                },
                {
                    "sent": "The probability of Y is 1 conditional leagues.",
                    "label": 0
                },
                {
                    "sent": "This is one of the we have to dance with the support just a minute.",
                    "label": 0
                },
                {
                    "sent": "That we have to density then.",
                    "label": 0
                },
                {
                    "sent": "For example, if the 10 still are in this fall then there is just one point that's here, for example this.",
                    "label": 0
                },
                {
                    "sent": "This is design.",
                    "label": 0
                },
                {
                    "sent": "This is this.",
                    "label": 0
                },
                {
                    "sent": "Then there is just one point that here we preferred to us to.",
                    "label": 0
                },
                {
                    "sent": "And classify why is 1 and here we kill a spider is minus one.",
                    "label": 0
                },
                {
                    "sent": "But now suppose we have the true density like this, with more.",
                    "label": 0
                },
                {
                    "sent": "Coin such that one of the dens is bigger than the other.",
                    "label": 0
                },
                {
                    "sent": "Then we have many points.",
                    "label": 0
                },
                {
                    "sent": "I can call them A1A K. An If care is very big.",
                    "label": 0
                },
                {
                    "sent": "Then we have a big class of classifier.",
                    "label": 0
                },
                {
                    "sent": "And then we use the penalty over K. So my assumption is that we have two dense.",
                    "label": 0
                },
                {
                    "sent": "OK, we don't talk about densities, but.",
                    "label": 0
                },
                {
                    "sent": "We we consider classical aspires like this.",
                    "label": 0
                },
                {
                    "sent": "H is the colossa edge.",
                    "label": 0
                },
                {
                    "sent": "Such stuff is so here I have a. OK, even up to a K1 if it's of X. EXE is in, for example A2I and A2I plus one and minus one.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, class of oldies.",
                    "label": 0
                },
                {
                    "sent": "And then I consecrate very large.",
                    "label": 0
                },
                {
                    "sent": "It can be anything so the killers of edge.",
                    "label": 0
                },
                {
                    "sent": "This colors of edges very.",
                    "label": 0
                },
                {
                    "sent": "An week so here I suppose X is the interval, 01 on HX is this.",
                    "label": 0
                },
                {
                    "sent": "Even this big K can change minus one or one, so we have both cases.",
                    "label": 0
                },
                {
                    "sent": "Start with minus one or start with one.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "This H is the class of all this HA.",
                    "label": 0
                },
                {
                    "sent": "And then you is.",
                    "label": 0
                },
                {
                    "sent": "But I just define you as the class of all parameter A.",
                    "label": 0
                },
                {
                    "sent": "Capital case fix.",
                    "label": 0
                },
                {
                    "sent": "That is what I want to say.",
                    "label": 0
                },
                {
                    "sent": "K can be anything.",
                    "label": 0
                },
                {
                    "sent": "So first for each?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes yes here here.",
                    "label": 0
                },
                {
                    "sent": "And so let me.",
                    "label": 0
                },
                {
                    "sent": "So first I consider one of these edge, but then I cancel that Union.",
                    "label": 0
                },
                {
                    "sent": "So yes it should be edge of an index K, But then I consider union in my theorem I constantly union of HK OK but now it's.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I consider just I think I wrote somewhere.",
                    "label": 0
                },
                {
                    "sent": "Yes, I consider this L of T. Is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The indicator function T is positive.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "The theoretical error is the L of F that is the property of items above X negative.",
                    "label": 0
                },
                {
                    "sent": "An empirical error is the percentage of the mass killers, for example.",
                    "label": 0
                },
                {
                    "sent": "An I call F0 of X.",
                    "label": 0
                },
                {
                    "sent": "The property over is unconditional leaks.",
                    "label": 0
                },
                {
                    "sent": "So I. I didn't write it here, but the so I think.",
                    "label": 0
                },
                {
                    "sent": "I define.",
                    "label": 0
                },
                {
                    "sent": "The eye of edge and.",
                    "label": 0
                },
                {
                    "sent": "HD something click expectation of.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "They indicate pull of each of these.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "It's exciting.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think I just I defined like this.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK. And then the square of Hi still is this.",
                    "label": 0
                },
                {
                    "sent": "So that's I defined before that 1 / N summation is, and this is this probability conditional on X.",
                    "label": 0
                },
                {
                    "sent": "And then we take the expectation here.",
                    "label": 0
                },
                {
                    "sent": "Probability there.",
                    "label": 0
                },
                {
                    "sent": "This was one of our end with some of the I ^2.",
                    "label": 0
                },
                {
                    "sent": "So then the.",
                    "label": 0
                },
                {
                    "sent": "So I so first I used the squared of H and still something like this was OK.",
                    "label": 0
                },
                {
                    "sent": "I define conditional on eggs.",
                    "label": 0
                },
                {
                    "sent": "OK that is 1 / N some of it.",
                    "label": 0
                },
                {
                    "sent": "Indicator of each of its eyes.",
                    "label": 0
                },
                {
                    "sent": "Is that the correct thing though it's I.",
                    "label": 0
                },
                {
                    "sent": "But then I take the expectation of this.",
                    "label": 0
                },
                {
                    "sent": "Then I have this.",
                    "label": 0
                },
                {
                    "sent": "This is also clear here because assumption a in my example becomes assumption be that I wrote here there is 1/3 positive such that the absolute value of two F0 of X -- 1 is bigger and we get an ETA for all X except.",
                    "label": 0
                },
                {
                    "sent": "AO I4I is 1K at a zero is argument of L of HA.",
                    "label": 0
                },
                {
                    "sent": "So we're friends.",
                    "label": 0
                },
                {
                    "sent": "I fixed a zero that is arguable of HA.",
                    "label": 0
                },
                {
                    "sent": "Overall a.",
                    "label": 0
                },
                {
                    "sent": "Then I assume that the probability this zero is the probability of.",
                    "label": 0
                },
                {
                    "sent": "Why is 1 conditional leaks?",
                    "label": 0
                },
                {
                    "sent": "I suppose this probability is away from 1/2 everywhere, but maybe not at a zero and I for all I.",
                    "label": 0
                },
                {
                    "sent": "So you see, it is true for all leagues.",
                    "label": 0
                },
                {
                    "sent": "So then I can first conditional I use the conditional on X and then I expect I take the expectation that is.",
                    "label": 0
                },
                {
                    "sent": "That is why I am allowed to do that, because this assumption is so before we have an assumption a.",
                    "label": 0
                },
                {
                    "sent": "Was that L of each minus L of H2 BK then?",
                    "label": 0
                },
                {
                    "sent": "This car but here I use something like this.",
                    "label": 0
                },
                {
                    "sent": "The probability of edge of.",
                    "label": 0
                },
                {
                    "sent": "So first I use this conditionally Onyx.",
                    "label": 0
                },
                {
                    "sent": "So in my example this is true when we have this conditional on X, we have this concern X and this.",
                    "label": 0
                },
                {
                    "sent": "We use this.",
                    "label": 0
                },
                {
                    "sent": "And then we take the expectation.",
                    "label": 0
                },
                {
                    "sent": "So in general, maybe I cannot use this again this definition?",
                    "label": 0
                },
                {
                    "sent": "That discussion on expert in this example I can do because my assumption becomes this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have this now.",
                    "label": 0
                },
                {
                    "sent": "We have even conditionally on X.",
                    "label": 0
                },
                {
                    "sent": "This is true.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "I assume that H.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the class of all H1 of K overall K?",
                    "label": 0
                },
                {
                    "sent": "The Union of all this A.",
                    "label": 0
                },
                {
                    "sent": "Of K&KI for each a I have a case.",
                    "label": 0
                },
                {
                    "sent": "OK is also a function of a.",
                    "label": 0
                },
                {
                    "sent": "An I use this penalty P squared of HA is elected square for some lab down K a / N. So.",
                    "label": 0
                },
                {
                    "sent": "If I can have laptop square as a function of N that is love the N. Then using lemma.",
                    "label": 0
                },
                {
                    "sent": "An using our theorem one we can find a bound for the Tau squared up at 10.",
                    "label": 0
                },
                {
                    "sent": "That shows us the rate of conversion.",
                    "label": 0
                },
                {
                    "sent": "So consider the.",
                    "label": 0
                },
                {
                    "sent": "This is a lemma that I used in the proof of this story.",
                    "label": 0
                },
                {
                    "sent": "This is just something.",
                    "label": 0
                },
                {
                    "sent": "Outside this this paper, it can be very easy considered the L1 metric on RM.",
                    "label": 1
                },
                {
                    "sent": "We define the~ van B is the summation of absolute value on my mistake BI.",
                    "label": 0
                },
                {
                    "sent": "Then at Boulby till the end of our with respect to this metric can be covered by N at most.",
                    "label": 1
                },
                {
                    "sent": "The integer part of R / U.",
                    "label": 0
                },
                {
                    "sent": "Plus one to power M balls with reduce U.",
                    "label": 0
                },
                {
                    "sent": "That is what I use for this entropy calculation.",
                    "label": 0
                },
                {
                    "sent": "So this is a lemma that I use and then I use our Theorem 2.",
                    "label": 0
                },
                {
                    "sent": "To prove this.",
                    "label": 0
                },
                {
                    "sent": "Suppose assumption B and then for laptop bigger or call 2 account stantons square root of like N. That's the fun can see.",
                    "label": 0
                },
                {
                    "sent": "Five can depend on M. We have this expectation of Tau squared up at an conditional edge.",
                    "label": 0
                },
                {
                    "sent": "Not is bounded by two.",
                    "label": 0
                },
                {
                    "sent": "The square of it not plus C 5 / N. And because PS squared is defined as here.",
                    "label": 0
                },
                {
                    "sent": "Then if we take laughter is equal to square root of like N, it becomes like an over N * K RK.",
                    "label": 0
                },
                {
                    "sent": "So the rate of convergence of Edge hat in is.",
                    "label": 0
                },
                {
                    "sent": "Like an overlay.",
                    "label": 0
                },
                {
                    "sent": "This is our conclusion, so it means that if we have the densities like what I said, but because of assumption a we have.",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "The zero.",
                    "label": 0
                },
                {
                    "sent": "Avexis probably always one conditional picks.",
                    "label": 0
                },
                {
                    "sent": "This is away from 1/2.",
                    "label": 0
                },
                {
                    "sent": "It means that it is of this kind.",
                    "label": 0
                },
                {
                    "sent": "This is 1/2.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "If we have such a situation, then.",
                    "label": 0
                },
                {
                    "sent": "If we consider the class of all classifier with Kate Threshold that care is unknown.",
                    "label": 0
                },
                {
                    "sent": "So we consider the union of all them and we penalized empirical reason minimize it.",
                    "label": 0
                },
                {
                    "sent": "The rate of convergence is like an over N. And we know that, OK?",
                    "label": 0
                },
                {
                    "sent": "This is another result that I got, but it is easy to see if K is fixed and we have another example.",
                    "label": 0
                },
                {
                    "sent": "K is one, we have just one of these, then the rate of convergence is 1 / N. This is for the case care is known.",
                    "label": 0
                },
                {
                    "sent": "Good care is unknown.",
                    "label": 0
                },
                {
                    "sent": "So this is the final result.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                },
                {
                    "sent": "Questions over.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Pizza or something?",
                    "label": 0
                },
                {
                    "sent": "So you're.",
                    "label": 0
                },
                {
                    "sent": "You supposed FCO.",
                    "label": 0
                },
                {
                    "sent": "Translate most eater over these.",
                    "label": 0
                },
                {
                    "sent": "Positions.",
                    "label": 0
                },
                {
                    "sent": "So there the assumption was this.",
                    "label": 0
                },
                {
                    "sent": "All sorry 2.",
                    "label": 0
                },
                {
                    "sent": "Yes, that was 2 -- 1 is.",
                    "label": 0
                },
                {
                    "sent": "Minus one is bigger than ether.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it should be.",
                    "label": 0
                },
                {
                    "sent": "Something like.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Final position.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So because it is a constant problem, it is in the constant because it.",
                    "label": 0
                },
                {
                    "sent": "Depends on capital M, but also easy.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "Because Lambda Lambda is the penalty, right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Can only use this if we know what C5.",
                    "label": 0
                },
                {
                    "sent": "His.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "Yes, it is in the constant.",
                    "label": 0
                },
                {
                    "sent": "Well, what's the problem?",
                    "label": 0
                },
                {
                    "sent": "Well, if you want to use this in practice, you have to take Lambda bigger than C5.",
                    "label": 0
                },
                {
                    "sent": "Group of men.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Constant depending on M. Anne.",
                    "label": 0
                },
                {
                    "sent": "Depends on more things also.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you would have to specify also eaten.",
                    "label": 0
                },
                {
                    "sent": "His I think if this sleep 5 is a is a large constant.",
                    "label": 0
                },
                {
                    "sent": "I didn't think about improving the constant.",
                    "label": 0
                },
                {
                    "sent": "But the question is, what does it depend on?",
                    "label": 0
                },
                {
                    "sent": "Because if you want to use this, you have to be able to.",
                    "label": 0
                },
                {
                    "sent": "Yeah yes, but yeah this assumption beat that I have here.",
                    "label": 0
                },
                {
                    "sent": "Yes, we we assume that this appetizer constanten.",
                    "label": 0
                },
                {
                    "sent": "And then C5 may depend on it I think.",
                    "label": 0
                },
                {
                    "sent": "This jump is a minimal minimal job.",
                    "label": 0
                },
                {
                    "sent": "Minimal yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "His.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the regularity condition.",
                    "label": 0
                },
                {
                    "sent": "You will.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                }
            ]
        }
    }
}