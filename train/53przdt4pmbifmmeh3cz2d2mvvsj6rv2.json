{
    "id": "53przdt4pmbifmmeh3cz2d2mvvsj6rv2",
    "title": "GibbsNet",
    "info": {
        "author": [
            "Alex Lamb, Montreal Institute for Learning Algorithms (MILA), University of Montreal"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_lamb_gibs/",
    "segmentation": [
        [
            "Hello, today I'm going to be talking to you guys about a new generative model that we've developed called Gibbs Net and it is an iterative way of doing adversarially learned inference."
        ],
        [
            "So just to give you guys a little bit of background, generative models are models that try to learn a data distribution given samples from that distribution.",
            "It's a very general set up, including things like classification and conditional modeling as special cases, and here I just gave you some examples of samples from the generative model."
        ],
        [
            "So one thing that people are interested in doing with generative models is including latent variables.",
            "So many generative models learn a joint distribution between observed variables X and unobserved latent variables Z.",
            "There are many reasons to do this.",
            "One reason is that in the space of latent variables, it might be a lot more natural to represent the uncertainty and what we've learned in our distribution.",
            "Other reason is these types of models can be more interpretable than models that just work in the visible space, and also you can potentially have stronger and more interesting conditional independence assumptions in the space of your latent variables.",
            "For example, if you imagine that you have something like an image, it might be the case that a good generative or a good graphical model that's directly in the observed space could just be fully connected.",
            "Whereas if you have latent variables, you might be able to draw more interesting independence assumptions."
        ],
        [
            "So one powerful way of doing inference in generative models with latent variables is called adversarially learned inference, and it was developed by my colleagues and I at Montreal here about a year ago, and the basic idea is that it's in the Gan framework, but rather than having your discriminator classify between just real examples versus generated examples it instead.",
            "Tries to classify between two different processes.",
            "One is a generative process and one is an inference process.",
            "So one type of data point that your discriminator gets is a real data point paired with what the inference network thinks the latent variables are for that real data points, so the discriminator gets that XZ pair and then the other type of X dead parrot gets is when zed is drawn from your latent prior and.",
            "The axe is drawn from your generator."
        ],
        [
            "So this is just a visualization of what that adverse aerial game looks like.",
            "So basically it's almost like you have two types of networks that are generating values.",
            "One is generating the latent variables given the observed X's and the other one is generating synthetic X values given the observed latent variables drawn from the latent prior."
        ],
        [
            "So while the adversarially learned inference algorithm works quite well, one of the limitations that it has is that it's very limited to this simple kind of model where you draw your latent variables from this simple factorized prior that you know how to sample from.",
            "So one idea that we've worked on for extending Alley is called Hierarchical Alley or Halle, and that involves.",
            "Both your inference and your generative process becoming more general directed graphical models and the other idea that I'm going to talk to you about right now is called Gibbs net.",
            "After block Gibbs sampling and the idea is to extend Ali to implicitly defined undirected graphical models."
        ],
        [
            "So the idea in Gibbs net is I'll just go through what it looks like in the simplest case where we have a single latent layer just to give a little bit more motivation.",
            "I think like kind of a main idea here is that rather than making our X marginally correspond to some kind of simple prior, we're going to allow it to come from like a more complex.",
            "Iterative process, so I'll just go through the simplest case of what Gibbs net looks like, so sampling from the model is done by blocked Gibbs sampling.",
            "So if our undirected graph is just X and said here, then for the generative process you start with a Z drawn from some prior an you run iterative chains of block Gibbs sampling, where you produce an axe.",
            "And then you infer back this ad and you produce an axe and you infer back does Ed and you keep going?",
            "And because it's only one layer, the iterative process just starts from a relax, and then you infer the zed for that X."
        ],
        [
            "In two layers, both the generative process as well as the inference procedure on real data, they both become multi step iterative processes."
        ],
        [
            "Yeah, so this is one more visualization of what Gibbs net looks like, so you can see here that in one process you take a real data point and then you just infer.",
            "So this is for the one latent layer case, so you take a real X and you infer the zed for that X.",
            "And then you also have this iterative process where you draw zed from the prior, generate the X and then you infer this ad.",
            "Generate the acts and you keep going in a loop."
        ],
        [
            "Now here you have some samples of what the unclamped chain actually does.",
            "So as you can see.",
            "As you take more and more steps, you kind of move out of this burn in and I guess get onto the data distribution and on the bottom we have the like 2D latent variables for MNIST with three digits, so you can see that as you take more steps the different digits values move farther apart of the latent space."
        ],
        [
            "So there's no, there's no theoretical reason to think that this will perform particularly well, but maybe it tells you something about exactly what the latent variables have learned and what we do here on the left with the faces is we run chains where the top half of the face is clamped to a real data point an we allow the bottom Part 2 to just evolve and come out of the generative process.",
            "And you can see that even though it's not trained to do in painting, and there's no reason to think this is going to be like a particularly good in painting model, the model still kind of does reasonably good in painting and on the right side we have a model where the X, the visible space, contains both a label and the image itself, and it's the same kind of procedure.",
            "But we've clamped the label to its observed value."
        ],
        [
            "So one thing that's interesting is on the left.",
            "I have this kind of conditional generation procedure with Ali and on the right I have this conditional generative procedure with Gibbs net.",
            "And the point here isn't to claim that Gibbs net is doing like state of the art in painting or anything, but just to show that the latent variables that we've observed are much more expressive and remember a lot more of the details about X."
        ],
        [
            "Another thing that we did was we ran sampling chains from both Ali and Gibbs net and we did a T sne on the latent space.",
            "An looked at how they move and what you can see here is that with Gibbs net, the evolution in the latent space is much more gradual, and it involves smaller moves as you run block Gibbs sampling, whereas Ali tends to make more.",
            "More sudden jumps."
        ],
        [
            "And another thing that's interesting is that because Allie if it's trained well, will have P of Z correspond to a simple prior, like a factorized Gaussian.",
            "Those latent variables for zed have to kind of fill the space.",
            "So even if you have three classes that are actually very distinct from each other in that in ferde latent space they have to be.",
            "Right next to each other, so here's an example where you have Allie on the left and Gibbs not on the right, and what you can see is with Ali.",
            "Even these very simple classes are quite close to each other, whereas with Gibbs never allowed to separate more because they don't have to correspond to."
        ],
        [
            "Assemble prior."
        ],
        [
            "So one of the nice theoretical properties of Gibbs not is that if this.",
            "If this adverse aerial game sort of reaches its fixed point, then you're guaranteed to.",
            "So if you get so, if your unclamped chain matches your clamp chain perfectly after N steps, if you sample for any more number of steps.",
            "Then you're guaranteed to stay on the data manifold, which is kind of a cool result."
        ],
        [
            "So what are some things that are appealing about Gibbs net as a way of doing adversarially learned inference?",
            "And one thing I'll add here is there have been some recent results where where people try to evaluate like mode, dropping in Gans and.",
            "Just like how well the Gan learns the diversity in the data distribution and a lot of them have found that that ally helps, so we haven't looked at it, but it's possible that Gibbs net provides a further improvement.",
            "So one thing that's appealing about Gibbs net is that your inference and your generation change share parameters with each other, which could make the model more compact and more stable than a model without such parameter sharing.",
            "Um, another thing is that sometimes, for depending on what your data is like, specifying an undirected graph could be more appealing than specifying a directed graphical model, as with hierarchical."
        ],
        [
            "Ali.",
            "So in terms of future work, actually, while I was working on this one thing that I was interested in is if you could make the I guess the queue of zed given Axon, the P of X.",
            "Given Zed in Gibbs net correspond to the conditional distributions, Annan, PBM, or a DBM an.",
            "In practice I wasn't able to get it to work in practice, but that could be kind of interesting.",
            "Another area of future work might be if you have like a more complex graphical model an rather than having one global discriminator that sees all the observed variables and all the latent variables.",
            "You could have local discriminators that just see, like little neighborhoods within the graph.",
            "Another area for interesting future work might be just kind of changing the architecture of things so that what kinds of information can be stored in zed changes.",
            "So maybe instead of Z being something that's without any spatial structure, zed could kind of remember specific spatial details.",
            "And I guess I'm also interested in learning transition operators that rather than getting to an equilibrium distribution they can kind of do like iterative improvement and gradually refined samples as you take more steps.",
            "That's it.",
            "I guess any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, today I'm going to be talking to you guys about a new generative model that we've developed called Gibbs Net and it is an iterative way of doing adversarially learned inference.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you guys a little bit of background, generative models are models that try to learn a data distribution given samples from that distribution.",
                    "label": 0
                },
                {
                    "sent": "It's a very general set up, including things like classification and conditional modeling as special cases, and here I just gave you some examples of samples from the generative model.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing that people are interested in doing with generative models is including latent variables.",
                    "label": 0
                },
                {
                    "sent": "So many generative models learn a joint distribution between observed variables X and unobserved latent variables Z.",
                    "label": 1
                },
                {
                    "sent": "There are many reasons to do this.",
                    "label": 0
                },
                {
                    "sent": "One reason is that in the space of latent variables, it might be a lot more natural to represent the uncertainty and what we've learned in our distribution.",
                    "label": 0
                },
                {
                    "sent": "Other reason is these types of models can be more interpretable than models that just work in the visible space, and also you can potentially have stronger and more interesting conditional independence assumptions in the space of your latent variables.",
                    "label": 0
                },
                {
                    "sent": "For example, if you imagine that you have something like an image, it might be the case that a good generative or a good graphical model that's directly in the observed space could just be fully connected.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you have latent variables, you might be able to draw more interesting independence assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one powerful way of doing inference in generative models with latent variables is called adversarially learned inference, and it was developed by my colleagues and I at Montreal here about a year ago, and the basic idea is that it's in the Gan framework, but rather than having your discriminator classify between just real examples versus generated examples it instead.",
                    "label": 1
                },
                {
                    "sent": "Tries to classify between two different processes.",
                    "label": 1
                },
                {
                    "sent": "One is a generative process and one is an inference process.",
                    "label": 0
                },
                {
                    "sent": "So one type of data point that your discriminator gets is a real data point paired with what the inference network thinks the latent variables are for that real data points, so the discriminator gets that XZ pair and then the other type of X dead parrot gets is when zed is drawn from your latent prior and.",
                    "label": 0
                },
                {
                    "sent": "The axe is drawn from your generator.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a visualization of what that adverse aerial game looks like.",
                    "label": 0
                },
                {
                    "sent": "So basically it's almost like you have two types of networks that are generating values.",
                    "label": 0
                },
                {
                    "sent": "One is generating the latent variables given the observed X's and the other one is generating synthetic X values given the observed latent variables drawn from the latent prior.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So while the adversarially learned inference algorithm works quite well, one of the limitations that it has is that it's very limited to this simple kind of model where you draw your latent variables from this simple factorized prior that you know how to sample from.",
                    "label": 0
                },
                {
                    "sent": "So one idea that we've worked on for extending Alley is called Hierarchical Alley or Halle, and that involves.",
                    "label": 0
                },
                {
                    "sent": "Both your inference and your generative process becoming more general directed graphical models and the other idea that I'm going to talk to you about right now is called Gibbs net.",
                    "label": 0
                },
                {
                    "sent": "After block Gibbs sampling and the idea is to extend Ali to implicitly defined undirected graphical models.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea in Gibbs net is I'll just go through what it looks like in the simplest case where we have a single latent layer just to give a little bit more motivation.",
                    "label": 0
                },
                {
                    "sent": "I think like kind of a main idea here is that rather than making our X marginally correspond to some kind of simple prior, we're going to allow it to come from like a more complex.",
                    "label": 0
                },
                {
                    "sent": "Iterative process, so I'll just go through the simplest case of what Gibbs net looks like, so sampling from the model is done by blocked Gibbs sampling.",
                    "label": 1
                },
                {
                    "sent": "So if our undirected graph is just X and said here, then for the generative process you start with a Z drawn from some prior an you run iterative chains of block Gibbs sampling, where you produce an axe.",
                    "label": 0
                },
                {
                    "sent": "And then you infer back this ad and you produce an axe and you infer back does Ed and you keep going?",
                    "label": 0
                },
                {
                    "sent": "And because it's only one layer, the iterative process just starts from a relax, and then you infer the zed for that X.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In two layers, both the generative process as well as the inference procedure on real data, they both become multi step iterative processes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so this is one more visualization of what Gibbs net looks like, so you can see here that in one process you take a real data point and then you just infer.",
                    "label": 0
                },
                {
                    "sent": "So this is for the one latent layer case, so you take a real X and you infer the zed for that X.",
                    "label": 0
                },
                {
                    "sent": "And then you also have this iterative process where you draw zed from the prior, generate the X and then you infer this ad.",
                    "label": 0
                },
                {
                    "sent": "Generate the acts and you keep going in a loop.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now here you have some samples of what the unclamped chain actually does.",
                    "label": 1
                },
                {
                    "sent": "So as you can see.",
                    "label": 0
                },
                {
                    "sent": "As you take more and more steps, you kind of move out of this burn in and I guess get onto the data distribution and on the bottom we have the like 2D latent variables for MNIST with three digits, so you can see that as you take more steps the different digits values move farther apart of the latent space.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's no, there's no theoretical reason to think that this will perform particularly well, but maybe it tells you something about exactly what the latent variables have learned and what we do here on the left with the faces is we run chains where the top half of the face is clamped to a real data point an we allow the bottom Part 2 to just evolve and come out of the generative process.",
                    "label": 0
                },
                {
                    "sent": "And you can see that even though it's not trained to do in painting, and there's no reason to think this is going to be like a particularly good in painting model, the model still kind of does reasonably good in painting and on the right side we have a model where the X, the visible space, contains both a label and the image itself, and it's the same kind of procedure.",
                    "label": 0
                },
                {
                    "sent": "But we've clamped the label to its observed value.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one thing that's interesting is on the left.",
                    "label": 0
                },
                {
                    "sent": "I have this kind of conditional generation procedure with Ali and on the right I have this conditional generative procedure with Gibbs net.",
                    "label": 0
                },
                {
                    "sent": "And the point here isn't to claim that Gibbs net is doing like state of the art in painting or anything, but just to show that the latent variables that we've observed are much more expressive and remember a lot more of the details about X.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing that we did was we ran sampling chains from both Ali and Gibbs net and we did a T sne on the latent space.",
                    "label": 0
                },
                {
                    "sent": "An looked at how they move and what you can see here is that with Gibbs net, the evolution in the latent space is much more gradual, and it involves smaller moves as you run block Gibbs sampling, whereas Ali tends to make more.",
                    "label": 0
                },
                {
                    "sent": "More sudden jumps.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another thing that's interesting is that because Allie if it's trained well, will have P of Z correspond to a simple prior, like a factorized Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Those latent variables for zed have to kind of fill the space.",
                    "label": 0
                },
                {
                    "sent": "So even if you have three classes that are actually very distinct from each other in that in ferde latent space they have to be.",
                    "label": 0
                },
                {
                    "sent": "Right next to each other, so here's an example where you have Allie on the left and Gibbs not on the right, and what you can see is with Ali.",
                    "label": 0
                },
                {
                    "sent": "Even these very simple classes are quite close to each other, whereas with Gibbs never allowed to separate more because they don't have to correspond to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assemble prior.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the nice theoretical properties of Gibbs not is that if this.",
                    "label": 0
                },
                {
                    "sent": "If this adverse aerial game sort of reaches its fixed point, then you're guaranteed to.",
                    "label": 0
                },
                {
                    "sent": "So if you get so, if your unclamped chain matches your clamp chain perfectly after N steps, if you sample for any more number of steps.",
                    "label": 0
                },
                {
                    "sent": "Then you're guaranteed to stay on the data manifold, which is kind of a cool result.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what are some things that are appealing about Gibbs net as a way of doing adversarially learned inference?",
                    "label": 0
                },
                {
                    "sent": "And one thing I'll add here is there have been some recent results where where people try to evaluate like mode, dropping in Gans and.",
                    "label": 0
                },
                {
                    "sent": "Just like how well the Gan learns the diversity in the data distribution and a lot of them have found that that ally helps, so we haven't looked at it, but it's possible that Gibbs net provides a further improvement.",
                    "label": 0
                },
                {
                    "sent": "So one thing that's appealing about Gibbs net is that your inference and your generation change share parameters with each other, which could make the model more compact and more stable than a model without such parameter sharing.",
                    "label": 0
                },
                {
                    "sent": "Um, another thing is that sometimes, for depending on what your data is like, specifying an undirected graph could be more appealing than specifying a directed graphical model, as with hierarchical.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ali.",
                    "label": 0
                },
                {
                    "sent": "So in terms of future work, actually, while I was working on this one thing that I was interested in is if you could make the I guess the queue of zed given Axon, the P of X.",
                    "label": 0
                },
                {
                    "sent": "Given Zed in Gibbs net correspond to the conditional distributions, Annan, PBM, or a DBM an.",
                    "label": 0
                },
                {
                    "sent": "In practice I wasn't able to get it to work in practice, but that could be kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "Another area of future work might be if you have like a more complex graphical model an rather than having one global discriminator that sees all the observed variables and all the latent variables.",
                    "label": 0
                },
                {
                    "sent": "You could have local discriminators that just see, like little neighborhoods within the graph.",
                    "label": 0
                },
                {
                    "sent": "Another area for interesting future work might be just kind of changing the architecture of things so that what kinds of information can be stored in zed changes.",
                    "label": 0
                },
                {
                    "sent": "So maybe instead of Z being something that's without any spatial structure, zed could kind of remember specific spatial details.",
                    "label": 0
                },
                {
                    "sent": "And I guess I'm also interested in learning transition operators that rather than getting to an equilibrium distribution they can kind of do like iterative improvement and gradually refined samples as you take more steps.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "I guess any questions?",
                    "label": 0
                }
            ]
        }
    }
}