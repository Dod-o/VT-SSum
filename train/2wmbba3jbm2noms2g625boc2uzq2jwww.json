{
    "id": "2wmbba3jbm2noms2g625boc2uzq2jwww",
    "title": "Learning Feature Hierarchies",
    "info": {
        "author": [
            "Yann LeCun, Computer Science Department, New York University (NYU)"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/mlss09us_lecun_lfh/",
    "segmentation": [
        [
            "OK, so I'm really glad that we heard.",
            "The previous two or three talks to fund my dad talk in particular, and because I'm going to be talking about sparse representations as well.",
            "Yeah, about unsupervised running as Debbie McAllister was just talking about.",
            "Um?",
            "And the title of my talk is not TBD, as returning the program, but it's actually learning feature hierarchies."
        ],
        [
            "And I'd like to make a point that I think the next frontier in machine learning is learning representations or learning features.",
            "So for for many years now, machine learning has kind of being obsessed with learning classifiers.",
            "Assuming that the features were given.",
            "But we're basically getting to the point that you know, we have kind of turnkey classifiers, which we just turn the crank and they kind of work OK, and the performance of our systems is limited by the features we're getting.",
            "Of course, with this VM's we can always play with the kernel and things like this, but you know, there's only so much you can do with this.",
            "Really, what matters is what features are you feeding the system with, and so I think the next frontier in machine learning is to actually learn the features.",
            "Or an internal representations, and it's something that a lot of us in the machine learning community actually have been obsessed with for a very, very long time.",
            "Back to the old days of neural Nets.",
            "So that includes people like like like me like Jeff Hinton's.",
            "David was referring to earlier your banjo and various other people like this.",
            "One problem though, is to learn the features.",
            "If we want to learn an entire system that includes the feature extraction and classification stage, this is going to be a very big system with lots of parameters in it, and it's pretty much hopeless to trend such systems from purely label data.",
            "So we're going to have to be able to make use of unlabeled data.",
            "Unfortunately, for a lot of problems, we get tons and tons of unlabeled data.",
            "So how do we leverage unlabeled data to kind of learn good features?"
        ],
        [
            "Alright, so here's the kind of traditional approach to to pattern recognition.",
            "Have some sort of fixed preprocessing of some kinds of feature extractor, and then plug your classifier on it and the problem of course is that you have to kind of work by hand on the on the preprocessing stage.",
            "Every time you change the problem.",
            "So in computer vision and object recognition people have kind of come up with representation that seems to work pretty well for natural images, they don't work so well on other data, and if you feed, say, speech to sift features, it's not going to work at all, so you're going to have to do something else, right?"
        ],
        [
            "So part of the problem of of learning representations is because you know we'd like to be able to.",
            "Lauren new categories of objects.",
            "If you want to vision from a very small number of labeled samples, so we allowed to look at the world and look at different objects and as many as we want.",
            "But we are only allowed to be given the names of the objects for just a few samples.",
            "OK, so that's kind of the idea of exploiting unlabeled data that come in sort of fairly unlimited supply and then sort of use a very small number of labeled samples to actually learn the classifier."
        ],
        [
            "OK, so the second point is that learning representation is going to be opening new problems for machine learning, and one of them is.",
            "Is that good representations are hierarchical.",
            "So if you want to say recognize objects in vision, you have to go from pixels to say edges to text tones, two parts, 2 objects to scenes, and as you go up in this hierarchy, sort of assemble features from the lower layer to kind of.",
            "Creates higher level features, right?",
            "So it's kind of a multi stage process.",
            "Which may or may not be feet forward, doesn't matter, but there's, you know, intermediate representations that are more and more abstract as you go up the layers.",
            "Similarly for speech recognition or language, you know you go from worst to parts of speech, sentences, text, etc.",
            "So, so the question is, how are we going to build a system like this that's composed of a kind of a cascade of trainable systems, each of which extracts representations that are more and more abstract in such a way that the system can.",
            "In the end, solve an interesting problem."
        ],
        [
            "And that that's come to be called deep learning deep because the architecture of that system is deep.",
            "It has many layers in it, or many stages if you will.",
            "So there's a bit of excitement around this idea of deep learning over the last few years in the machine learning community, and that's when I'm going to be talking about."
        ],
        [
            "And one reason for for sort of being inspired by those by sort of being interested by those deep architectures, is that there is considerable evidence that the visual system in primates, at least, or even cats and things like this, is deep.",
            "And there's some evidence that this actually feed forward, at least for kind of simple tasks such as kind of everyday.",
            "Object recognition is lots of anatomical evidence for this physiological evidence, and also psychophysics experiments that show that.",
            "And so if you cannot trust, try to trace the the circuit.",
            "If I'm wrong, but if you could kind of try to trace the circuit of you know how many layers of neural processing of neurons, if you will.",
            "The visual signal goes from the eye to the info temporal cortex where objects are encoded.",
            "It's roughly about 10 and the problem is we don't have learning algorithms that can train attend your network, so we're going to have to invent some new ones."
        ],
        [
            "They're also sort of theoretical arguments for why deep architectures are more efficient than shadow one, and so things like a super vector machine, for example, is a shadow architecture in the sense that you can see that you can think of the first layer as the result of applying the kernel function to the input.",
            "OK, so you get a feature vector that is whose size is not the number of support vectors, and then the second layer is just a linear classifier, basically a linear combiner.",
            "OK, so it's going to shadow 'cause the first layer is essentially fixed and the second layer is just a linear classifier.",
            "And so there are sort of some very very strong theoretical limitations on what you can do with this.",
            "Unless someone gives you a kind of an ideal kernel, but unfortunately no one.",
            "No one does that for you.",
            "So I'm not going to."
        ],
        [
            "This, but there are sort of a number of papers that were talking about this problem recently.",
            "There's when I wrote with revenge.",
            "Oh scaling learning algorithms towards AI is kind of this idea of sort of trying to.",
            "Be more ambitious about what we tried to do with machine learning, and instead of just trying to build good classifiers, try to solve a more kind of entire problem.",
            "End to end with machine learning such as vision or speech or language.",
            "But let me not go into those details."
        ],
        [
            "OK, so let's talk about vision for just a second.",
            "So in computer vision, most of the recent there's been a big convergence over the last couple of years of what people do in computer vision for recognizing objects, and pretty much all the good systems work the same way.",
            "It's totally amazing how that happened over the last years.",
            "Basically, you take an image, you apply your filter bank to them, say oriented edges or something of that type.",
            "But sometimes it's something different.",
            "Maybe filters that you can learn, then you apply some nonlinearity to this.",
            "It could be a rectification, could be some sort of local competition.",
            "It could be all kinds of different things like this.",
            "Or if we don't take all safety actually uses window, take all some kind and then you do some spatial pooling so you take the features that come out of your feature detector over a little area and you can have them together or do a Max operation and the purpose of this is to build a little bit of shift in variance in the in the representation.",
            "OK, so so elementary stage of feature extraction is this thing filterbank, some nonlinearities on spatial pooling, and especially the nonlinearity sometimes involved.",
            "Very harsh nonlinearities like vector quantization or things of that type.",
            "And if you have only one stage of this, you just plug a classifier on top and train this classifier in supervised mode.",
            "Alright, so an example of this is Leonard's ethnics pyramid match kernel system which uses SIFT features and see features really fall into that model and a pyramid match, kernel support vector machine for the classifier on top.",
            "And she got on the order of 65% correct on a standard data set called.",
            "Caltech 101 is a database of objects with 100 and one categories of objects from images or gathered off the web and.",
            "The characteristic of this data set, which makes it bad and good in many ways, is that you only have 30 training samples per class.",
            "OK, so you have to have good representations, otherwise you're not going to be able to learn anything.",
            "Other recognition systems like the Hog system that David was talking about are so kind of falling into that model."
        ],
        [
            "And those are really very much inspired by biology, so you could have gone to computational biologist computational scientists 15 years ago or 10 years ago.",
            "And as what your idea for how the brain does vision and they would have told you this, you know bunch of filters and some nonlinearity, and then some special pulling.",
            "In fact you could have, you could have gone too.",
            "Newer scientists, almost 50 years ago and ask them that, and at least two of them who building weasel and they were told you exactly that and fight the good day, got Nobel Prize for it.",
            "So it's not like this work is unknown.",
            "1962 that's that's a long, long time ago, and so it looks like computer vision is converging back towards what neuro scientists were saying almost 50 years ago.",
            "It's very interesting.",
            "In fact, Sift was actually inspired by biology.",
            "David Lowe says says that much.",
            "So that'll just calls called the first, the first layer, the filter banks, and only article that simple cells and the pooling layer called that complex cells.",
            "OK, so we're going to use."
        ],
        [
            "That nomenclature as well.",
            "So of course several of us have had the idea for quite awhile to kind of stack multiple stages of this and see if we can get better results by stacking multiple layers like the brain does, and perhaps from that build hierarchical representations, hierarchies of features.",
            "You know where we go from sort of low level features here to kind of slightly higher level features here that we plug to a classifier, and maybe we can stack more layers of this to get better results."
        ],
        [
            "And the first guy to have this idea instead of building a system and simulating it in on the computer is Fukushima, who build this equal to New York neutron.",
            "This is back in the early 80s, actually started in the early 70s, yet is slightly simpler system and used it.",
            "Didn't have any sophisticated learning algorithms, so you something very very simple, but it demonstrated you know some good performance on very simple letter recognition applications and then several years later.",
            "Can I came up with the idea of using a similar architecture, but training it's using back propagation, so how many of you know what backpropagation is?",
            "OK, more than CRF.",
            "So so.",
            "So basically you know just view this system as a giant parameterized function where all the coefficients of all the filters in all the layers are just trainable coefficients.",
            "Compute the gradient of some loss function that measures the discrepancy between the answer you want in the answer you get with respect to all those parameters and adjust everything using stochastic gradient descent.",
            "And surprisingly enough it actually works really well.",
            "So quite a few people I've played with architectures like this.",
            "Garcia is at France Telecom, you is at NEC Labs in California, but there's quite a few people who work with those those architectures so that those are entirely supervised convolutional networks.",
            "And basically there are stacks of those simple sales filter banks and nonlinearity, very simple narratives with a sigmoid, and then each of those planes here represents the output of applying one filter to the input OK and then.",
            "Some pooling and spatial subsampling to reduce the resolution of the representation so as to build a little bit of shifting variance in the representation.",
            "And then you stack another stage of the same thing.",
            "So filter Max again each of those guys combine inputs from several Maps in the in the previous layer.",
            "How about just adding up the result of the convolutions?",
            "Also a sigmoid nonlinearity, some sampling again, and then plug that too linear classifier and the entire system is trained at once globally using gradient descent.",
            "Recently Tommy Pojo is sitting down here have used similar architectures where the pooling method here is kind of a Max operator and the Houston sort of unsupervised learning algorithm for the second layer and sort of built in some filters for the first layer, but he and Steve smell and several people in this group have been sort of working on unsupervised learning algorithms for those architecture as well and trying to sort of analyze that from the theoretical point of view.",
            "So a lot of those things are kind of.",
            "Converging David load the inventor of Sift actually worked with this architecture also a few years ago.",
            "It was kind of interesting Thomas areas in terms lab."
        ],
        [
            "OK, so those architectures are large and they typically will contain the the one the commercial Nets.",
            "They will typically contain on the order of a few 100,002 million free parameters that you have to learn from data, and so they only work well if you have lots of labeled data essentially."
        ],
        [
            "So you know you can do handwriting recognition.",
            "We've been doing this for many years.",
            "This is sort of an example of the internal state of handwriting recognition systems based on convolutional Nets, so there's the input.",
            "The first layer of feature extraction, the complex cells here and then the second stage of simple cells, complex cells, and then some sort of global feature representation that is obtained from that, and then the classifier that you don't see.",
            "This is actually used commercially."
        ],
        [
            "Rechecks, so it's been around."
        ],
        [
            "Well, you can do."
        ],
        [
            "Face detection you can do object recognition."
        ],
        [
            "Virus kinds."
        ],
        [
            "Um?"
        ],
        [
            "You can even drive robots with this.",
            "I mean there's a lot of applications of this, some of which are commercial.",
            "There are even specialized hardware to run them so."
        ],
        [
            "But here is the bad news.",
            "The bad news is those things require a lot of.",
            "Go to training samples.",
            "So if you want to recognize handwritten digits with this, it works really well 'cause you can get you know 60 thousand 100,000 training samples of label digits and there you will just nail, you know, get the best performance ever with those supervised systems you know much better than SVM.",
            "Maybe six of the error rate or fifth of the error rate of SVM.",
            "Maybe 1/4 of the error rate of an SVM on NST OK. You know, same for this nor objected and said that I was just showing you the same for face detection.",
            "But here is the problem.",
            "On Caltech 101.",
            "It just doesn't work at all or really not well.",
            "So if you take a supervised convolutional net that you train on, take one and remember you only have 30 training samples per class.",
            "You get something like 29% correct and it's really embarrassing because Asif Bayes system with pyramid match kernel SVM gets close to 65%.",
            "So that's really not fun.",
            "So what we thought was, well, you know it's only logical is because the system has millions of free parameters.",
            "We only have a few 1000 examples to train it, so there's no way you can train.",
            "It can learn the entire task.",
            "There's just too many parameters for it, so we're going to have to use unsupervised learning.",
            "OK, and for a few years we believe that we actually got pretty good results with unsupervised running.",
            "But it turns out we were slightly wrong and I'm going to go through this."
        ],
        [
            "In in the following.",
            "OK so so so OK.",
            "So first idea is that we need unsupervised learning methods to learn the features in that system to pre learn the features in that system, from unlabeled data.",
            "And then we'll just use our label data to train maybe the last layer.",
            "Or do we find the solution that we already got pretty close to using unsupervised learning?",
            "And the strategy we're going to employ is, you know, inspired by Jeff Hinton's deep Dish Network strategy, which basically comes down to train training each layer one after the other.",
            "OK, so you train a layer with some sort of unsupervised learning algorithm, then you freeze it.",
            "Then you stack of second stage on it on top of it, and then you train that stage unsupervised.",
            "Then you freeze it.",
            "Then you stuck a sticker classifier on top, and you're trying to classify your in supervised mode.",
            "OK. And there's a bunch of papers going back to you know, 2005, 2006, by Hinton and his group your banjo Macario is one of my students who just graduated and unlikely who is in Andrew innings Group at Stanford."
        ],
        [
            "OK so here is what we're going to do for the unsupervised learning part.",
            "We're going to do something that I call the encoder decoder architecture and it's basically you can fold a lot of those models in Hinton, Bengio's and all that stuff within this kind of framework.",
            "So basically you start with an input and you're going to run the input through an encoder a predictor which is going to compute the features from the input, and then you gotta feed the features back to a decoder which is going to try to reconstruct the input.",
            "OK, and you can measure the reconstruction error.",
            "Basically the difference between the original input and what you got by running through the encoder and decoder, and they were going to rain.",
            "This is that we're going to trend the encoder and decoder so as to minimize the reconstruction error.",
            "OK, said like that you can say, well, it's kind of like PCA, right?",
            "If I make this in that linear then and that is the squared error.",
            "I just get PCA.",
            "And you would be right.",
            "That's a special case of this."
        ],
        [
            "But we're going to do something more than PCA, which I'm going to get to in a minute.",
            "OK, so first I want to tell you how we do this deep learning stuff, right?",
            "So we first train the first stage doing this encoder decoder architecture.",
            "Then we throw away the Deco."
        ],
        [
            "Sure, we generate the features that we get from the encoder and then we stack a second stage and we train that OK keeping this fixed.",
            "OK, and once we're done with this, we throw away the deal."
        ],
        [
            "Order sticker classifier on top and train the classifier and we can do two things.",
            "Either we can just train the classifier keeping the two encoders fixed.",
            "Or we can train the entire thing with back Prop will go in dissent.",
            "OK, we find the entire thing so that we get better results.",
            "But that's kind of an optional step.",
            "OK, so."
        ],
        [
            "So what about this encoder decoder training module?",
            "And that's where we're going to get into kind of sparsity arguments and things of that type.",
            "OK, so we're going to define an energy function.",
            "Yeah, why?",
            "Which is the square difference between the input and the the reconstructed input?",
            "By applying the encoder decoder to it.",
            "OK, we can interpret this energy function as a log probability.",
            "If you want of some sort of density model over over Y. OK, so to train the system what we want is that we want to make you of why small in regions of high data density and want to make it large in regions of low data density OK?",
            "Because E is.",
            "You can get probabilities out of E by taking my E to the minus E. OK, so the probability of a particular Y under the model is going to be proportional to E to the minus.",
            "The energy of that Y under the model OK given by the model.",
            "OK, so if you want to give high probabilities, you want to give high probability to regions of high density and low probability to regions of low density.",
            "Therefore low energy to regions of high density and high energy to regions of low density."
        ],
        [
            "So something like this?",
            "OK, so let's say why is uni dimensional.",
            "This is some sort of probability distribution.",
            "You want to want to learn?",
            "Or maybe a log probability distribution and you know the energy function is just minus that minus the log probability."
        ],
        [
            "And the way you go from energy to probability is you do either the minus multiplied by some positive constant and normalize.",
            "Unfortunately the normalization constant is horrible to compute in high dimensional spaces, so you want to avoid doing this."
        ],
        [
            "And it's really intractable then that's going to be the main problem.",
            "We're going to have to get around with this.",
            "These unsupervised learning methods, OK?",
            "Alright, so so, let's say let's say we want to train the density model like this on natural images OK. Or maybe I'm just patches of natural images.",
            "So what we want is, you know if this is a natural image here we want to give it low energy and if this is an unnatural image or maybe a noisy image, want to give it high energy.",
            "And if we had a box that could do this, we could do all kinds of stuff with it.",
            "We could do compression, we could do denoising, we could do as I am going to show unsupervised learning, so that's really it would be really nice if you could do this in some sort of efficient or general way."
        ],
        [
            "And here is the problem.",
            "If you want to have a probabilistic approach to this, what you're going to have to do is.",
            "For every training point you have, you're going to have to change the parameters of your encoder and decoder so that this training point gets properly constructed so you get low energy OK, but at the same time you're going to have to make sure that everything else that you've never seen before, that everything that's not a training sample gets a high energy OK, and that's really the hard part.",
            "That's basically the gradient of the denominator.",
            "Here the normalization term you want to make the energy here for on average, for all the training of the non training sample for everything outside of the training.",
            "The training set high.",
            "OK.",
            "So this is kind of what is depicted here.",
            "If you have a training sample, why here you want to push down on the energy of the training sample and you want to push up on everything else.",
            "You can't push up on everything else 'cause it's not."
        ],
        [
            "Miss Dimensional space, right?",
            "And so you know, you compute the gradient of this loss function.",
            "The likelihood the average likelihood of the points under the model, and you realize you get very simple to compute, which is a derivative of the energy with respect to the parameters at the training sample, and then you get the derivative of the log partition function, which is integral over all possible points of the probability that your model gives to that point times the derivative of the energy with respect to the parameters at that point OK. You can compute this.",
            "There's no way, so Jeff Hinton invented this Q technical contrastive divergent switch David mentioned earlier."
        ],
        [
            "I'm not going to go through that.",
            "OK, maybe I shouldn't, but I'm not going to have time.",
            "But considering this is basically a way of simplifying the second term here by."
        ],
        [
            "So replacing this integral by just essentially a single sample that you choose in a smart way.",
            "OK, so instead of pushing up on the energy of every single point, you only push up on the energy of well chosen points that are around the training samples, but not too far."
        ],
        [
            "But that was kind of the 32nd high level view of contrastive divergent.",
            "OK, we're going to do something else.",
            "We're going to use sparsity as a surrogate to maximizing the log partition function.",
            "And we're going to do this by restricting the information content of the code.",
            "Basically, by, you know, applying a sparsity penalty to the code.",
            "So this is the feature vector from which we reconstruct the input OK. And the main insight is that if the code of the feature vector Z can only take a few different configurations, is only allowed to go through, you know to take a few different values essentially.",
            "Then on your correspondingly small number of wise can be perfectly reconstructed.",
            "And I'm going to."
        ],
        [
            "Show this.",
            "On an example.",
            "OK, so let's say."
        ],
        [
            "Let's say this is a or input space, OK, and.",
            "Neutral spaces here on the other side.",
            "And."
        ],
        [
            "Let's say or 20 sample or training samples or the blue points.",
            "OK, So what we want is we want lower reconstruction error for those points and we want high reconstruction error for everything else, including those pink points.",
            "So if a feature space allows is allowed to take a lot of different values, OK, so let's say all the all the red points.",
            "Then for every point here green or blue there is a point here.",
            "Which means that you know we go through the encoder.",
            "We find a point.",
            "We go through the decoder, we decode perfectly.",
            "For every point, OK, there's no issue."
        ],
        [
            "So you know noisy samples or bad or natural images will be reconstructed just as well as as as good ones.",
            "So here's what I'm going to do.",
            "We're going to make the feature space very small.",
            "We're going to allow only a few configurations."
        ],
        [
            "In the in the space of the feature vector, OK, we're going to do this by a sparsity penalty.",
            "And then we."
        ],
        [
            "Happens is.",
            "When you take a training sample, it goes through its corresponding code and is being correctly constructed, but will take any other sample."
        ],
        [
            "It has to go to one of those, right?",
            "It has to go to one of those codes that are the only ones you allow, and you know that these guys reconstruct one of the training samples.",
            "OK, or something close to it.",
            "Alright, so so here is an example where you feed a noisy example to the system.",
            "You go through the encoder decoder and what you get is a cleaner version of it.",
            "You get the closest, maybe the closest thing that looks like the training sample or that lives on the training sample manifold.",
            "That is kind of as similar as possible to the the input you gave."
        ],
        [
            "OK, so we're going to use a sparsity penalty to restrict the information content of the code, and why you sparsity.",
            "The reason is because high dimensional sparse codes so."
        ],
        [
            "We back up just just a little with one way of restricting the information content of the code is and the simplest ways to make it low dimensional, and that's essentially what PC does.",
            "The reason PCA works is because it does not allow the feature space to be as big as the input space.",
            "If you allow the feature space to be as big as the input space, you could just get the identity function and PC.",
            "It doesn't do anything good for you.",
            "Essentially OK. Um?",
            "So here we are."
        ],
        [
            "What we want is we want to be able to produce high dimensional representations, maybe higher dimensional than the input, but we want them to be sparse on the reason for this is because in a high dimensional sparse space things are more easily separable.",
            "OK, so it's good for classification to do that.",
            "There also some neuroscience argument for it, but I'm not going to do that."
        ],
        [
            "OK, so we're going to start by just focusing on the decoder and the reason we're going to focus on the decoder is because it's a very old idea, which you know I didn't invent.",
            "I guess we could go back to all sudden feel 1997 and they sort of got inspired by the you know work on business pursuit, which is a little older than that.",
            "OK, so so so I don't have an encoder here.",
            "I only have a decoder and the decoder is linear.",
            "OK, so basically you take a feature vector Z.",
            "An you run it through a linear decoder, so you just multiplied by matrix.",
            "This could be a rectangular matrix sosias higher dimension then why generally?",
            "And then you're going to compute the square distance between why the input and the reconstructed version of Z&Z also goes through a a kind of sparsity module, so something that computes the sum of the absolute value of the terms.",
            "For example, it's exactly identical towards different.",
            "I was talking about earlier.",
            "OK, but what we're going to do with this is that we're going to learn the matrix WD.",
            "OK. And we're going to do this is very simple, so we can we write an energy function, which is a function of YZ&W OK, and so it's the square error between Y and its reconstruction WDC.",
            "Plus some constant times the sum of the absolute value of the components of the OK.",
            "So if I give you an X, if I give you a Y.",
            "You find the optimal Z by minimizing this function.",
            "OK, so this is this operation here.",
            "And for learning what we're going to do is, we're just going to minimize the average of that overall or training samples wise.",
            "OK, we're just going to do this weekend in dissent.",
            "Nothing fancy.",
            "OK, just compute the gradient of this for every training sample and do an update.",
            "You can you suggest a green dissent works really well.",
            "OK, so this is really cool."
        ],
        [
            "There's only one problem with this, which is that it's slow at runtime, because if you if you have this system, if I give you a why you have to run an optimization algorithm to find the Z. OK, so it's something that's defined was referring to earlier, and there are lots of very smart methods to do this, but it's still very slow.",
            "You have to run an optimization algorithm to do that.",
            "And also, as Steven was mentioning the solution you get is very unstable.",
            "In other words, if you change the Y just a little bit, you might wait.",
            "You might get to completely different Z. OK, that's not good for recognition."
        ],
        [
            "OK, so here's how we're going to fix this.",
            "Going to fix this by adding an encoder.",
            "OK, so going back to this encoder idea and the encoder is going to predict what the solution to this optimization problem is.",
            "OK, so assume we have the decoder that we've already trained it.",
            "So we have RWD matrix and we can give a why we find the optimal Z.",
            "So we're going to do now is we're going to plug an encoder.",
            "He ran.",
            "The encoder is going to be trained to predict the solution that Z will that we get when we run the optimization algorithm on the original energy function.",
            "OK, so now we have is a feedforward predictor for this.",
            "The sparse vector that we would otherwise have to obtain by running an optimization algorithm.",
            "It's not going to be a good approximation, but maybe it's good enough to do recognition.",
            "So in fact we actually do is slightly more complicated than this.",
            "So instead of training this first, getting optimal codes and then training a a a feedforward predictor to predict those codes, we actually going to train the entire thing at the same time.",
            "OK, so we're going to replace the original energy function that we had that only had the reconstruction error term and the sparsity term.",
            "We're going to add a third term, which is a prediction term.",
            "So is the difference between Z and whatever this guy wants to predict.",
            "Z should be.",
            "OK.",
            "So it's this big energy function on top here, which has those three terms.",
            "We can decide, so F is going to have to be nonlinear, because if it's linear kind of defeats the purpose.",
            "So this is going to have to be only near and we choose the kind of simplest nonlinear thing we can imagine, which is we take Y multiplied by matrix.",
            "WE apply hyperbolic attention to all the components like, say neural net layer, and then multiply each of the components by some constant that we also learned is sort of trainable parameters.",
            "Non you just make it an even nastier nonconvex optimization, yes?",
            "We don't have.",
            "We don't care about being things being convex or non convex, right?",
            "I don't care.",
            "Unstable before this is more stable because this function is smooth.",
            "OK, so if I give if I give a Y. I get I get dizzy and if I change why a little bit I get a similar Z because this function is smooth.",
            "Whereas the function that gives you the optimal Z4, Y with just a decoder.",
            "That's very highly non smooth OK.",
            "So that's that's the point.",
            "You can.",
            "You can view this as kind of a regularizer for the.",
            "For the other thing you can view this.",
            "The second term here as a regularizer for for the first you know the energy function you get, otherwise.",
            "We basically set Lambda through cross validation.",
            "OK so you know.",
            "Does this, no, there's no real way to learn Lambda Lambda 'cause you can't really do that.",
            "It's not a.",
            "He's going to want to go to zero or something like that, 'cause it's much easier to solve.",
            "The problem is number is 0, so.",
            "OK, so this was a really nice paper rejected from NIPS last year so you can get it as a tech report."
        ],
        [
            "And but again, you know it's a good sign.",
            "Consider rejected twice as well.",
            "OK, so here is the so the procedure is is very simple.",
            "We we get it.",
            "Why we find the Z that minimizes this overall energy function.",
            "Some of those three terms.",
            "And once we have the why we tweak the parameters of the encoder and decoder so as to make the energy of it smaller.",
            "OK. And the role of the sparsity term here is to make sure that we don't.",
            "We don't learn a kind of a a trivial identity function by going through encoder and decoder.",
            "But limit the information content of the code.",
            "That's what I was talking about earlier."
        ],
        [
            "OK, so that's that's the predictive sparsity composition algorithm.",
            "So for a given training sample, initialize Z to the output of the encoder.",
            "Then you find the Z that minimizes the energy function through some sort of gradient descent algorithm, or some smart technique did the decoder basis functions to reduce the reconstruction error of the encoder parameters to reduce the prediction error and repeat with the next training sample.",
            "OK, very simple."
        ],
        [
            "If you apply this to Ernest, you get things like this, so these are the basis functions, but they also are the filters.",
            "They actually end up being the same to transpose.",
            "So what you get here?",
            "So basically what it tells you is I can reconstruct any digit as a weighted sum of a small subset of those guys, OK?",
            "'cause each of those is is a colon in the WD matrix OK. And so when you multiply it by his past vector you get a weighted sum of a small number of those guys and can basically reconstructing digit by weighted sum of small number of those guys.",
            "And So what they end up being when you train the system is that they end up being small parts.",
            "Like you know atoms.",
            "Words as.",
            "As Stephen we call them know Alphabet letters.",
            "OK is an alphabet."
        ],
        [
            "If you apply them to natural image patches here, there are 12 by 12 natural image patches from the Berkeley data set, and this is the running algorithm actually taking place.",
            "This is a movie, it's not running in real time, but it's actually quite fast, So what you get in the end are Gabor filters.",
            "OK, no real surprise here.",
            "Similar to what Rosenfield were reporting in their decoder on your system.",
            "So this is something you know.",
            "If you have a large number of those basis functions, little nice keyboard filters.",
            "You can even have a huge number of them, and they don't die.",
            "They they all do something.",
            "Alright, so how are we going to use?"
        ],
        [
            "This for object recognition.",
            "How well does that?",
            "Does that do on Catholic went OK?",
            "Can we use this to kind of pre train the filters of a convolutional net and and get better results from that?",
            "So what we're going to do is we're going to use this real encoder decoder architecture to train a filterbank on natural image patches or size 9 by 9.",
            "We're going to train 64 filters OK. We're going to get our essentially get more filters.",
            "And then we're going to pass this through a series of nonlinearities, and then do a little special pulling.",
            "So this whole thing is, like you know, it could be like a shift, or the first stage of the convolutional net, or the first stage of Tommy Pojo's HVAC system.",
            "And then we're going to take that and plug that to a linear classifier or in SVM."
        ],
        [
            "Process the images in various ways.",
            "High pass filter normalized contrast by Baba.",
            "And the encoder again is linear matrix multiplied by.",
            "Then passed through hypertension and then multiplied by scaling.",
            "OK, so these are the."
        ],
        [
            "Filters we get on 64 with 64 filters on natural images.",
            "OK, give more shelters, essentially with some tweaks."
        ],
        [
            "Um?"
        ],
        [
            "Then we're going to expel."
        ],
        [
            "It was sort of biased."
        ],
        [
            "So nonlinearity that."
        ],
        [
            "Put on top of this so with something like, say, absolute value rectification or some sort of contrast normalization where we replace every value by itself divided by the standard deviation of his neighbors.",
            "And the pulling down sampling which we can do either with them."
        ],
        [
            "Maxwell with an average."
        ],
        [
            "OK."
        ],
        [
            "And we have four different procedures for training this system.",
            "The first one, the first procedure is, we don't train it at all.",
            "We just set the filters to random values.",
            "And that's the end of it.",
            "We're just trying to classifier.",
            "OK, second procedure is we use this PSD training to train in unsupervised mode.",
            "So this is denoted by U.",
            "3rd technique is we set the filters to random values, but we train them using backdrop simultaneously with the classifier.",
            "OK, so this would be kind of a traditional convolutional net training, supervised training and the 4th one is we initialize the weights with the result of the unsupervised PSD algorithm and then we do a refinement using gradient descent over the entire system OK. Our classifier is."
        ],
        [
            "Just multinomial logistic regression on top, but we can back propagate through that.",
            "OK, so here's an eye chart.",
            "A big table of results and it's kind of sobering to some extent.",
            "Because regardless of what 20 procedure you use, if you put the right set of nonlinearities, it basically makes no difference.",
            "You can use random completely random filters.",
            "With one stage of feature extraction, so you do you know 9 by 9 filters, sigmoid gain, absolute value rectification, contrast, normalization pulling using an average, and then you stick that to a logistic regression and you get you know in the low to mid 50s on category one.",
            "You know it doesn't break any record record is 65 or so for a single feature set.",
            "But the bad news is that it makes no difference what learning procedure you use.",
            "And it doesn't make much difference.",
            "So here is another number also using the unsupervised training procedure.",
            "Here's what you get if you just built in keyboard filters.",
            "OK, basically the same result.",
            "OK, that's really not surprising because you know what the system learns in the end, our global filters.",
            "So you get the same as with Gabor filters.",
            "And if you use the unsupervised filters and you plug them through a pyramid match kernel instead of a logistic regression, you get the same result as if you sift.",
            "OK, around 65% or so.",
            "So that's really cool.",
            "What that means is that with our very simple unsupervised learning algorithm, we can basically get the same result as if you use sift on natural images.",
            "OK, but there's a big difference.",
            "Now.",
            "We can apply this to all kinds of data that are not like natural images, and we can apply them to the second.",
            "The second stage of a similar system.",
            "OK, so let's do that now."
        ],
        [
            "OK, so before that.",
            "OK, so rectification makes a huge difference as you saw here.",
            "If you don't rectify the output you go from."
        ],
        [
            "So kind of a pure convolutional net type thing with, you know, just filters sigmoid and then pulling average pooling gives you 14.5%.",
            "This is ridiculously bad.",
            "Otherwise, if you just add rectification you get 50% OK, so it's really important to rectify.",
            "It's also somewhat important to normalize it.",
            "Sort of gives you a little boost of 4%."
        ],
        [
            "And supervised pretraining.",
            "Unfortunately it doesn't make much difference in this case it did in other cases that we saw but not in this one.",
            "And the really weird thing is that random filters work really well."
        ],
        [
            "Let me skip this.",
            "OK, so now let's go to a two stage system.",
            "So we're going to stack two stages of this question.",
            "Say again.",
            "I think they are drawn with some sort of some Gaussian independent, so each value is independently drawn from a Gaussian.",
            "OK, so let's do two stages now.",
            "OK, so we trained the first stage.",
            "Then we generate all the features on the you know on some natural images.",
            "Then we trained the second stage and supervised and then we put the whole system together and again we can either just trying to classify on top or trendy and refine the entire thing in supervised mode."
        ],
        [
            "And here is the table results.",
            "OK, so here's the good news.",
            "The good news is.",
            "With the second stage and a multinomial logistic regression on top, we get 65.5%, which is essentially the same as a pyramid match kernel system on the single stage.",
            "OK, so if you have one stage and P match carnal is VM, you get essentially the same result as you have two stages.",
            "Another secret question.",
            "So the second stage basically plays the same role as the as the.",
            "The Colonel in the Pyramid match kernel, but the cool thing is that we learned it.",
            "We didn't have to build it by hand, right?",
            "So we can apply the same method to all kinds of data that for which may be problematic.",
            "Kernel wouldn't really make sense.",
            "But again, the weird thing is that it doesn't make much difference whatever learning algorithm you use.",
            "Even random filters actually work well.",
            "At least if you have normalization and rectification, that's really a big mystery.",
            "It's a huge surprise.",
            "It's kind of sobering for machine learning.",
            "Like me, you know.",
            "And basically out of business now.",
            "But now I'm kidding, but you know.",
            "OK, this is another thing, so here indicated by GT.",
            "So this is a few built-in Gabor filters at the first layer, and you sort of templates Start learning at the second layer, which is kind of similar to me, which is what Thomas errand Merchant low were doing in the 2006.",
            "And here you get about 56%, which is, you know, quite a bit quite a bit below.",
            "Maybe you get, maybe they got better results since then, I don't know.",
            "OK, and here is the other surprise is that if you take a two stage system and you substitute logistic regression for this pyramid, match kernel SVM, you actually actually get worse performance than with logistic regression.",
            "So per match kernel is good on low level features, but it's not good on high level features."
        ],
        [
            "OK, these are cute picture."
        ],
        [
            "As of the filters, we get this first and second layer before and after."
        ],
        [
            "Supervised refinement, but I'm going to skip this."
        ],
        [
            "So here is interesting about about in NIST, so if we use this, you plus you plus technique.",
            "So where we initialized unsupervised and then we refine using supervised learning.",
            "We basically beat the record on ennist.",
            "OK, and you can't get anywhere close to this if you use Sift and pyramid, match kernel with 15 pyramid, match kernel and then this gives you 1.5% error.",
            "With this we get .53% error.",
            "So what that shows you is that you receive temporary magical works really well, natural images, but on anything else it's really not not really well suited for that.",
            "Or as this can be basically trained for on just about any data, even audio or something, we haven't tried that yet, but will.",
            "Who will soon?"
        ],
        [
            "So another question is why do random filters work?",
            "It's kind of a mystery right?",
            "So what we did was a little experiment where we took our filters, run them through the first stage of feature extraction and we degrade in this hand in input space to find the input configuration that maximizes the output of a particular feature particular variable on the on the output.",
            "OK, so it's kind of like what neuroscientists do write the book, an electrode in a neuron in the brain, and then they try to sort of figure out what configuration, what stimulus will maximize the output of that neuron.",
            "That's exactly what we did, but here we have the derivative of this thing, so we can actually back propagate through it.",
            "And you know, find the optimal input.",
            "What you find is really strange, which is that you know if you have give or filters because you go through those complex sales.",
            "This pooling mechanism, the sort of pattern that maximally activates a particular unit, is like a grading as some at some orientation and some frequency not very surprising.",
            "The surprising thing is that even with random filters, you basically get the same thing, some sort of noisy version of it.",
            "That's really strange.",
            "So basically because there is just a little bit of asymmetry in the random filters, you know when you sort of average them out and you kind of contrast, normalize them, you get response.",
            "High responses for gradings and no responses for other things, so they become orientation selective even though they are random.",
            "So it's really cool because it makes life easy for evolution if evolution wants to create a visual system, it doesn't actually need to get a learning algorithm, it just needs to get the nonlinearities right and then have random random weights.",
            "And I'm sure some theoretician in sort of compressed sensing could come up with some good justification for."
        ],
        [
            "As well, but.",
            "This is going."
        ],
        [
            "New result.",
            "But here's kind of something to kind of modulate this a little bit, which is that we made some experiments on the data set of object with sort of multiple orientations where we have almost 5000 training samples per category with five categories and 5000 training sample spaghetti with lots of different orientations.",
            "And what we can take 101 is sort of in this regime.",
            "Here we have about 30 training samples per category.",
            "OK, so in this regime here.",
            "Um?",
            "This is.",
            "Supervised learning.",
            "From random filters, there's no unsupervised learning.",
            "OK, so it's just supervised learning of the entire thing.",
            "And this is a.",
            "This is random filters OK.",
            "Forget about the blue line blue line.",
            "I'll talk about later.",
            "So this is kind of, you know, a standard convolutional net with all the right nonlinearities in it.",
            "Rectification, contrast, normalization.",
            "And it only works marginally better.",
            "This is a log log plot, but you know, it only works marginally better than random filters for a small number of training samples, but as soon as you increase the number of training samples and it gets a lot better.",
            "OK, so the reason why all the numbers are kind of the same.",
            "We can take 101 is because we are in this regime.",
            "But if we had more training samples, then running actually helps you kind of make sense right?",
            "If you have more labeled more samples, learning helps.",
            "The blue line is if you.",
            "If we use a traditional convolutional net which does not have normalization and rectification, and we see the same thing if we don't have many training samples, then this really doesn't work very well.",
            "But as we increase the number of training samples and basically the learning process compensates for the fact that we don't have the right architecture, they don't have the right nonlinearities and we get pretty much the same error rate whether we have the complicated narratives on our dinner.",
            "OK, so basically if you don't have many training samples you need to put more prior knowledge about about the about the data and that's in the form of nonlinearities and things like this here.",
            "But if you have tons of training samples, then it doesn't matter.",
            "You want to learn everything and the precise detail of the architecture don't matter because you learn anyway.",
            "It all makes sense.",
            "So we made some changes on this that allows us to."
        ],
        [
            "If So what I've told you hear the learning algorithm, only learned only trains to simple cells, and then we kind of stack the pool on top of it as kind of an afterthought, and so the other thing we did that will appear in CPR in a few weeks is an idea where we learn the complex cells simultaneously with the simple cells."
        ],
        [
            "And."
        ],
        [
            "The cute thing about this is that it does not involve changing anything about the encoder and decoder, or the only thing that changes is how we construct this prosody term.",
            "So the sparsity term instead of insisting that.",
            "So you know the instead of being the sum of the absolute value of the components of the code is basically the square root of the sum of the squares of of the codes.",
            "But the sum only takes place on blocks that overlap.",
            "OK, and those blocks constitute the filters that will be summed up within the complex cells."
        ],
        [
            "So I'm going a little fast on this, I'm realizing, but.",
            "What this is going to do is that is going to drive the system, so we're going to organize the filters in some sort of topology.",
            "Doesn't have to be today, it could be anything.",
            "And we're going to take Windows over this.",
            "Within this, we're going to compute the square root of the sum of the square of the filter outputs.",
            "Here.",
            "OK, we're going to do this on overlapping.",
            "Patches within this topology.",
            "And then or sparsity term is going to be the sum of all of those terms OK, and so the usual L1 sparsity is a limit case of this, where the pools have size 1.",
            "OK, because absolute value is equal to square root of square.",
            "What this what this does to the system is that it drives the filters that are within the pool to kind of be similar.",
            "Because it wants the smallest number of pool to be on at any one time, that's a sparsity constraint.",
            "But within a pool you can have multiple filters that are on.",
            "It really doesn't care.",
            "OK, so it tries to regroup all the put all the filters that are similar within the pool and this."
        ],
        [
            "So what what happens?",
            "So this is a bunch of filters that are drawn according to this topology that we sort of dreamed up.",
            "And this is one of the pools.",
            "OK, we have pools like this that are kind of square that is set by two or three pixels everywhere, and what you get this kind of graphic map of filters that are all similar to each other and you sum up the output of all the filters within the pool.",
            "You kind of get a complex sale kind of a local invariant feature.",
            "OK, you show this to biologists and they say, well, this looks just like you know what we see in the V1 when you move an electrode from one place to another, you get smoothly varying.",
            "Orientation selectivity."
        ],
        [
            "We even get."
        ],
        [
            "Pinwheels"
        ],
        [
            "And so we can use those features and plug them into a classifier again and."
        ],
        [
            "So put either linear classifier or pyramid match kernel on top and you know we that's the wrong number, sorry.",
            "Um?",
            "OK, it's wrong number.",
            "We actually get 65% on both.",
            "I don't know why this is 59%.",
            "This would be about 65, so we get about the same performance as we're getting with the previous algorithm.",
            "That's the sad news we going any better result.",
            "It's about the same as C."
        ],
        [
            "We have some hardware."
        ],
        [
            "Oracle so, but I'm not going to skip into this order.",
            "The last thing I'm going to show you is really quick, so is some application of those techniques for robot vision and.",
            "3 minutes or something.",
            "Two OK.",
            "So basically what we have is this robot that runs around and has four cameras and is trying to.",
            "It's kind of a bit like the thing that David was talking about.",
            "What the robot is supposed to do is tell you that whether there is an obstacle in front and what distance the obstacle is, so it has to label all the pixels in the image and tell you whether the weather is it reversible thing or non traversable thing, whether it's path or piece of grass or whether it's a tree or rock or something like that, right and.",
            "We trained the system.",
            "We get labels from stereo vision.",
            "So for objects that are nearby we can get labels from a sort of heuristic stereo vision algorithm that he goes out the 3D structure of everything that sticks out with something sticks out of the ground.",
            "We label it as not reversible, and if it's on the ground will just reversible.",
            "So for things that are nearby we get labels for things that are far away.",
            "We don't get labels, but we train the system online as a robot runs to kind of distinguish between traversable non traversable from the labels that are nearby and then we apply it to the entire image.",
            "So we can label."
        ],
        [
            "Entire image as to whether it's reversible or not.",
            "There's a lot of details that I'm going to skip, obviously, so it's using."
        ],
        [
            "So one of those convolutional Nets.",
            "And and is using this unsupervised pretraining a slightly different version of it, and this is what the classifier produces on the on the right?",
            "So Green is traversable, red is non traversable and pink."
        ],
        [
            "Is kind of the foot of obstacles.",
            "And you know."
        ],
        [
            "We can run this thing through and we get really good results which I'm going to show you, but what I'm going to show you is.",
            "Uh.",
            "He video.",
            "OK, thank you very much and I'll take question when the weather we will rent.",
            "Thank you."
        ],
        [
            "Steven structures.",
            "Can you give?",
            "An argument explained that he said that there were some already pulled out.",
            "So you want me to say few things about the theoretical arguments, OK?",
            "OK, so let me give you an argument that doesn't have to do with continuous functions, but binary functions, Boolean functions.",
            "OK, so if you want to.",
            "You can implement any Boolean function of N bits as a DNF or CNF, right?",
            "So a bunch of ends and a bunch of hours with some negation.",
            "OK, essentially a two layer architecture.",
            "If you will, but almost every single function of N bits is going to require two to the end in terms on the order of an exponential number of minterms, right?",
            "There's only a logarithmic logarithmic small number of functions that require an an exponential number of in terms.",
            "So for example, if you want to take if you want to multiply 2 by 2 billion numbers or add 2 billion numbers, let's say OK to binary numbers.",
            "You can do it within two layers, but it's going to take a huge number of in terms, but if you allow yourself to have multiple layers, so for example you allow yourself to do carry propagation, then you can do it with hardware hardware complexity order N, But then you have to have N layers.",
            "OK, another example is parity.",
            "So let's say I give you N bits and I ask you, do I have a even or odd number of ones?",
            "You can do it in two layers, but it's exponential.",
            "Or you can do it in log in layers with a bunch of stores.",
            "Ann and I, you know, linear in the number of struggle with making the number of inputs.",
            "So there is this sort of exchange between depth and breath.",
            "You can gain an exponential factor on the breath by a very, very minor increase of the depth alright for Boolean functions, and there is a sense that this is also true for continuous functions.",
            "So the functions we try to learn you know for vision and stuff like that.",
            "This kind of an exchange of you know.",
            "How much computational steps of some complexity you allow yourself versus you know how many things you have to compute in parallel?",
            "Now there's some theory about this on in the circuit theory, but for Boolean functions mostly.",
            "Any other question?",
            "You guys are hypnotized by the video.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm really glad that we heard.",
                    "label": 0
                },
                {
                    "sent": "The previous two or three talks to fund my dad talk in particular, and because I'm going to be talking about sparse representations as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, about unsupervised running as Debbie McAllister was just talking about.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And the title of my talk is not TBD, as returning the program, but it's actually learning feature hierarchies.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'd like to make a point that I think the next frontier in machine learning is learning representations or learning features.",
                    "label": 0
                },
                {
                    "sent": "So for for many years now, machine learning has kind of being obsessed with learning classifiers.",
                    "label": 0
                },
                {
                    "sent": "Assuming that the features were given.",
                    "label": 0
                },
                {
                    "sent": "But we're basically getting to the point that you know, we have kind of turnkey classifiers, which we just turn the crank and they kind of work OK, and the performance of our systems is limited by the features we're getting.",
                    "label": 1
                },
                {
                    "sent": "Of course, with this VM's we can always play with the kernel and things like this, but you know, there's only so much you can do with this.",
                    "label": 1
                },
                {
                    "sent": "Really, what matters is what features are you feeding the system with, and so I think the next frontier in machine learning is to actually learn the features.",
                    "label": 0
                },
                {
                    "sent": "Or an internal representations, and it's something that a lot of us in the machine learning community actually have been obsessed with for a very, very long time.",
                    "label": 0
                },
                {
                    "sent": "Back to the old days of neural Nets.",
                    "label": 0
                },
                {
                    "sent": "So that includes people like like like me like Jeff Hinton's.",
                    "label": 0
                },
                {
                    "sent": "David was referring to earlier your banjo and various other people like this.",
                    "label": 0
                },
                {
                    "sent": "One problem though, is to learn the features.",
                    "label": 0
                },
                {
                    "sent": "If we want to learn an entire system that includes the feature extraction and classification stage, this is going to be a very big system with lots of parameters in it, and it's pretty much hopeless to trend such systems from purely label data.",
                    "label": 1
                },
                {
                    "sent": "So we're going to have to be able to make use of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, for a lot of problems, we get tons and tons of unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So how do we leverage unlabeled data to kind of learn good features?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so here's the kind of traditional approach to to pattern recognition.",
                    "label": 0
                },
                {
                    "sent": "Have some sort of fixed preprocessing of some kinds of feature extractor, and then plug your classifier on it and the problem of course is that you have to kind of work by hand on the on the preprocessing stage.",
                    "label": 0
                },
                {
                    "sent": "Every time you change the problem.",
                    "label": 0
                },
                {
                    "sent": "So in computer vision and object recognition people have kind of come up with representation that seems to work pretty well for natural images, they don't work so well on other data, and if you feed, say, speech to sift features, it's not going to work at all, so you're going to have to do something else, right?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So part of the problem of of learning representations is because you know we'd like to be able to.",
                    "label": 0
                },
                {
                    "sent": "Lauren new categories of objects.",
                    "label": 0
                },
                {
                    "sent": "If you want to vision from a very small number of labeled samples, so we allowed to look at the world and look at different objects and as many as we want.",
                    "label": 0
                },
                {
                    "sent": "But we are only allowed to be given the names of the objects for just a few samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's kind of the idea of exploiting unlabeled data that come in sort of fairly unlimited supply and then sort of use a very small number of labeled samples to actually learn the classifier.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the second point is that learning representation is going to be opening new problems for machine learning, and one of them is.",
                    "label": 0
                },
                {
                    "sent": "Is that good representations are hierarchical.",
                    "label": 0
                },
                {
                    "sent": "So if you want to say recognize objects in vision, you have to go from pixels to say edges to text tones, two parts, 2 objects to scenes, and as you go up in this hierarchy, sort of assemble features from the lower layer to kind of.",
                    "label": 0
                },
                {
                    "sent": "Creates higher level features, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a multi stage process.",
                    "label": 0
                },
                {
                    "sent": "Which may or may not be feet forward, doesn't matter, but there's, you know, intermediate representations that are more and more abstract as you go up the layers.",
                    "label": 0
                },
                {
                    "sent": "Similarly for speech recognition or language, you know you go from worst to parts of speech, sentences, text, etc.",
                    "label": 0
                },
                {
                    "sent": "So, so the question is, how are we going to build a system like this that's composed of a kind of a cascade of trainable systems, each of which extracts representations that are more and more abstract in such a way that the system can.",
                    "label": 0
                },
                {
                    "sent": "In the end, solve an interesting problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that that's come to be called deep learning deep because the architecture of that system is deep.",
                    "label": 0
                },
                {
                    "sent": "It has many layers in it, or many stages if you will.",
                    "label": 0
                },
                {
                    "sent": "So there's a bit of excitement around this idea of deep learning over the last few years in the machine learning community, and that's when I'm going to be talking about.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one reason for for sort of being inspired by those by sort of being interested by those deep architectures, is that there is considerable evidence that the visual system in primates, at least, or even cats and things like this, is deep.",
                    "label": 1
                },
                {
                    "sent": "And there's some evidence that this actually feed forward, at least for kind of simple tasks such as kind of everyday.",
                    "label": 0
                },
                {
                    "sent": "Object recognition is lots of anatomical evidence for this physiological evidence, and also psychophysics experiments that show that.",
                    "label": 0
                },
                {
                    "sent": "And so if you cannot trust, try to trace the the circuit.",
                    "label": 0
                },
                {
                    "sent": "If I'm wrong, but if you could kind of try to trace the circuit of you know how many layers of neural processing of neurons, if you will.",
                    "label": 1
                },
                {
                    "sent": "The visual signal goes from the eye to the info temporal cortex where objects are encoded.",
                    "label": 0
                },
                {
                    "sent": "It's roughly about 10 and the problem is we don't have learning algorithms that can train attend your network, so we're going to have to invent some new ones.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They're also sort of theoretical arguments for why deep architectures are more efficient than shadow one, and so things like a super vector machine, for example, is a shadow architecture in the sense that you can see that you can think of the first layer as the result of applying the kernel function to the input.",
                    "label": 0
                },
                {
                    "sent": "OK, so you get a feature vector that is whose size is not the number of support vectors, and then the second layer is just a linear classifier, basically a linear combiner.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's going to shadow 'cause the first layer is essentially fixed and the second layer is just a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "And so there are sort of some very very strong theoretical limitations on what you can do with this.",
                    "label": 0
                },
                {
                    "sent": "Unless someone gives you a kind of an ideal kernel, but unfortunately no one.",
                    "label": 0
                },
                {
                    "sent": "No one does that for you.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This, but there are sort of a number of papers that were talking about this problem recently.",
                    "label": 0
                },
                {
                    "sent": "There's when I wrote with revenge.",
                    "label": 0
                },
                {
                    "sent": "Oh scaling learning algorithms towards AI is kind of this idea of sort of trying to.",
                    "label": 0
                },
                {
                    "sent": "Be more ambitious about what we tried to do with machine learning, and instead of just trying to build good classifiers, try to solve a more kind of entire problem.",
                    "label": 0
                },
                {
                    "sent": "End to end with machine learning such as vision or speech or language.",
                    "label": 0
                },
                {
                    "sent": "But let me not go into those details.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's talk about vision for just a second.",
                    "label": 0
                },
                {
                    "sent": "So in computer vision, most of the recent there's been a big convergence over the last couple of years of what people do in computer vision for recognizing objects, and pretty much all the good systems work the same way.",
                    "label": 0
                },
                {
                    "sent": "It's totally amazing how that happened over the last years.",
                    "label": 0
                },
                {
                    "sent": "Basically, you take an image, you apply your filter bank to them, say oriented edges or something of that type.",
                    "label": 0
                },
                {
                    "sent": "But sometimes it's something different.",
                    "label": 0
                },
                {
                    "sent": "Maybe filters that you can learn, then you apply some nonlinearity to this.",
                    "label": 0
                },
                {
                    "sent": "It could be a rectification, could be some sort of local competition.",
                    "label": 0
                },
                {
                    "sent": "It could be all kinds of different things like this.",
                    "label": 0
                },
                {
                    "sent": "Or if we don't take all safety actually uses window, take all some kind and then you do some spatial pooling so you take the features that come out of your feature detector over a little area and you can have them together or do a Max operation and the purpose of this is to build a little bit of shift in variance in the in the representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so so elementary stage of feature extraction is this thing filterbank, some nonlinearities on spatial pooling, and especially the nonlinearity sometimes involved.",
                    "label": 0
                },
                {
                    "sent": "Very harsh nonlinearities like vector quantization or things of that type.",
                    "label": 0
                },
                {
                    "sent": "And if you have only one stage of this, you just plug a classifier on top and train this classifier in supervised mode.",
                    "label": 0
                },
                {
                    "sent": "Alright, so an example of this is Leonard's ethnics pyramid match kernel system which uses SIFT features and see features really fall into that model and a pyramid match, kernel support vector machine for the classifier on top.",
                    "label": 1
                },
                {
                    "sent": "And she got on the order of 65% correct on a standard data set called.",
                    "label": 0
                },
                {
                    "sent": "Caltech 101 is a database of objects with 100 and one categories of objects from images or gathered off the web and.",
                    "label": 0
                },
                {
                    "sent": "The characteristic of this data set, which makes it bad and good in many ways, is that you only have 30 training samples per class.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have to have good representations, otherwise you're not going to be able to learn anything.",
                    "label": 0
                },
                {
                    "sent": "Other recognition systems like the Hog system that David was talking about are so kind of falling into that model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And those are really very much inspired by biology, so you could have gone to computational biologist computational scientists 15 years ago or 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "And as what your idea for how the brain does vision and they would have told you this, you know bunch of filters and some nonlinearity, and then some special pulling.",
                    "label": 0
                },
                {
                    "sent": "In fact you could have, you could have gone too.",
                    "label": 0
                },
                {
                    "sent": "Newer scientists, almost 50 years ago and ask them that, and at least two of them who building weasel and they were told you exactly that and fight the good day, got Nobel Prize for it.",
                    "label": 0
                },
                {
                    "sent": "So it's not like this work is unknown.",
                    "label": 0
                },
                {
                    "sent": "1962 that's that's a long, long time ago, and so it looks like computer vision is converging back towards what neuro scientists were saying almost 50 years ago.",
                    "label": 0
                },
                {
                    "sent": "It's very interesting.",
                    "label": 0
                },
                {
                    "sent": "In fact, Sift was actually inspired by biology.",
                    "label": 0
                },
                {
                    "sent": "David Lowe says says that much.",
                    "label": 0
                },
                {
                    "sent": "So that'll just calls called the first, the first layer, the filter banks, and only article that simple cells and the pooling layer called that complex cells.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to use.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That nomenclature as well.",
                    "label": 0
                },
                {
                    "sent": "So of course several of us have had the idea for quite awhile to kind of stack multiple stages of this and see if we can get better results by stacking multiple layers like the brain does, and perhaps from that build hierarchical representations, hierarchies of features.",
                    "label": 0
                },
                {
                    "sent": "You know where we go from sort of low level features here to kind of slightly higher level features here that we plug to a classifier, and maybe we can stack more layers of this to get better results.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the first guy to have this idea instead of building a system and simulating it in on the computer is Fukushima, who build this equal to New York neutron.",
                    "label": 0
                },
                {
                    "sent": "This is back in the early 80s, actually started in the early 70s, yet is slightly simpler system and used it.",
                    "label": 0
                },
                {
                    "sent": "Didn't have any sophisticated learning algorithms, so you something very very simple, but it demonstrated you know some good performance on very simple letter recognition applications and then several years later.",
                    "label": 0
                },
                {
                    "sent": "Can I came up with the idea of using a similar architecture, but training it's using back propagation, so how many of you know what backpropagation is?",
                    "label": 0
                },
                {
                    "sent": "OK, more than CRF.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So basically you know just view this system as a giant parameterized function where all the coefficients of all the filters in all the layers are just trainable coefficients.",
                    "label": 0
                },
                {
                    "sent": "Compute the gradient of some loss function that measures the discrepancy between the answer you want in the answer you get with respect to all those parameters and adjust everything using stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And surprisingly enough it actually works really well.",
                    "label": 0
                },
                {
                    "sent": "So quite a few people I've played with architectures like this.",
                    "label": 0
                },
                {
                    "sent": "Garcia is at France Telecom, you is at NEC Labs in California, but there's quite a few people who work with those those architectures so that those are entirely supervised convolutional networks.",
                    "label": 0
                },
                {
                    "sent": "And basically there are stacks of those simple sales filter banks and nonlinearity, very simple narratives with a sigmoid, and then each of those planes here represents the output of applying one filter to the input OK and then.",
                    "label": 0
                },
                {
                    "sent": "Some pooling and spatial subsampling to reduce the resolution of the representation so as to build a little bit of shifting variance in the representation.",
                    "label": 0
                },
                {
                    "sent": "And then you stack another stage of the same thing.",
                    "label": 0
                },
                {
                    "sent": "So filter Max again each of those guys combine inputs from several Maps in the in the previous layer.",
                    "label": 0
                },
                {
                    "sent": "How about just adding up the result of the convolutions?",
                    "label": 0
                },
                {
                    "sent": "Also a sigmoid nonlinearity, some sampling again, and then plug that too linear classifier and the entire system is trained at once globally using gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Recently Tommy Pojo is sitting down here have used similar architectures where the pooling method here is kind of a Max operator and the Houston sort of unsupervised learning algorithm for the second layer and sort of built in some filters for the first layer, but he and Steve smell and several people in this group have been sort of working on unsupervised learning algorithms for those architecture as well and trying to sort of analyze that from the theoretical point of view.",
                    "label": 0
                },
                {
                    "sent": "So a lot of those things are kind of.",
                    "label": 0
                },
                {
                    "sent": "Converging David load the inventor of Sift actually worked with this architecture also a few years ago.",
                    "label": 0
                },
                {
                    "sent": "It was kind of interesting Thomas areas in terms lab.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so those architectures are large and they typically will contain the the one the commercial Nets.",
                    "label": 0
                },
                {
                    "sent": "They will typically contain on the order of a few 100,002 million free parameters that you have to learn from data, and so they only work well if you have lots of labeled data essentially.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know you can do handwriting recognition.",
                    "label": 0
                },
                {
                    "sent": "We've been doing this for many years.",
                    "label": 0
                },
                {
                    "sent": "This is sort of an example of the internal state of handwriting recognition systems based on convolutional Nets, so there's the input.",
                    "label": 0
                },
                {
                    "sent": "The first layer of feature extraction, the complex cells here and then the second stage of simple cells, complex cells, and then some sort of global feature representation that is obtained from that, and then the classifier that you don't see.",
                    "label": 0
                },
                {
                    "sent": "This is actually used commercially.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rechecks, so it's been around.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you can do.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Face detection you can do object recognition.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Virus kinds.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can even drive robots with this.",
                    "label": 0
                },
                {
                    "sent": "I mean there's a lot of applications of this, some of which are commercial.",
                    "label": 0
                },
                {
                    "sent": "There are even specialized hardware to run them so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But here is the bad news.",
                    "label": 0
                },
                {
                    "sent": "The bad news is those things require a lot of.",
                    "label": 0
                },
                {
                    "sent": "Go to training samples.",
                    "label": 0
                },
                {
                    "sent": "So if you want to recognize handwritten digits with this, it works really well 'cause you can get you know 60 thousand 100,000 training samples of label digits and there you will just nail, you know, get the best performance ever with those supervised systems you know much better than SVM.",
                    "label": 0
                },
                {
                    "sent": "Maybe six of the error rate or fifth of the error rate of SVM.",
                    "label": 0
                },
                {
                    "sent": "Maybe 1/4 of the error rate of an SVM on NST OK. You know, same for this nor objected and said that I was just showing you the same for face detection.",
                    "label": 0
                },
                {
                    "sent": "But here is the problem.",
                    "label": 0
                },
                {
                    "sent": "On Caltech 101.",
                    "label": 0
                },
                {
                    "sent": "It just doesn't work at all or really not well.",
                    "label": 0
                },
                {
                    "sent": "So if you take a supervised convolutional net that you train on, take one and remember you only have 30 training samples per class.",
                    "label": 1
                },
                {
                    "sent": "You get something like 29% correct and it's really embarrassing because Asif Bayes system with pyramid match kernel SVM gets close to 65%.",
                    "label": 1
                },
                {
                    "sent": "So that's really not fun.",
                    "label": 0
                },
                {
                    "sent": "So what we thought was, well, you know it's only logical is because the system has millions of free parameters.",
                    "label": 0
                },
                {
                    "sent": "We only have a few 1000 examples to train it, so there's no way you can train.",
                    "label": 0
                },
                {
                    "sent": "It can learn the entire task.",
                    "label": 0
                },
                {
                    "sent": "There's just too many parameters for it, so we're going to have to use unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "OK, and for a few years we believe that we actually got pretty good results with unsupervised running.",
                    "label": 0
                },
                {
                    "sent": "But it turns out we were slightly wrong and I'm going to go through this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In in the following.",
                    "label": 0
                },
                {
                    "sent": "OK so so so OK.",
                    "label": 0
                },
                {
                    "sent": "So first idea is that we need unsupervised learning methods to learn the features in that system to pre learn the features in that system, from unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And then we'll just use our label data to train maybe the last layer.",
                    "label": 0
                },
                {
                    "sent": "Or do we find the solution that we already got pretty close to using unsupervised learning?",
                    "label": 0
                },
                {
                    "sent": "And the strategy we're going to employ is, you know, inspired by Jeff Hinton's deep Dish Network strategy, which basically comes down to train training each layer one after the other.",
                    "label": 1
                },
                {
                    "sent": "OK, so you train a layer with some sort of unsupervised learning algorithm, then you freeze it.",
                    "label": 1
                },
                {
                    "sent": "Then you stack of second stage on it on top of it, and then you train that stage unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Then you freeze it.",
                    "label": 0
                },
                {
                    "sent": "Then you stuck a sticker classifier on top, and you're trying to classify your in supervised mode.",
                    "label": 0
                },
                {
                    "sent": "OK. And there's a bunch of papers going back to you know, 2005, 2006, by Hinton and his group your banjo Macario is one of my students who just graduated and unlikely who is in Andrew innings Group at Stanford.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so here is what we're going to do for the unsupervised learning part.",
                    "label": 0
                },
                {
                    "sent": "We're going to do something that I call the encoder decoder architecture and it's basically you can fold a lot of those models in Hinton, Bengio's and all that stuff within this kind of framework.",
                    "label": 0
                },
                {
                    "sent": "So basically you start with an input and you're going to run the input through an encoder a predictor which is going to compute the features from the input, and then you gotta feed the features back to a decoder which is going to try to reconstruct the input.",
                    "label": 1
                },
                {
                    "sent": "OK, and you can measure the reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "Basically the difference between the original input and what you got by running through the encoder and decoder, and they were going to rain.",
                    "label": 1
                },
                {
                    "sent": "This is that we're going to trend the encoder and decoder so as to minimize the reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "OK, said like that you can say, well, it's kind of like PCA, right?",
                    "label": 0
                },
                {
                    "sent": "If I make this in that linear then and that is the squared error.",
                    "label": 0
                },
                {
                    "sent": "I just get PCA.",
                    "label": 1
                },
                {
                    "sent": "And you would be right.",
                    "label": 0
                },
                {
                    "sent": "That's a special case of this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we're going to do something more than PCA, which I'm going to get to in a minute.",
                    "label": 0
                },
                {
                    "sent": "OK, so first I want to tell you how we do this deep learning stuff, right?",
                    "label": 0
                },
                {
                    "sent": "So we first train the first stage doing this encoder decoder architecture.",
                    "label": 0
                },
                {
                    "sent": "Then we throw away the Deco.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, we generate the features that we get from the encoder and then we stack a second stage and we train that OK keeping this fixed.",
                    "label": 0
                },
                {
                    "sent": "OK, and once we're done with this, we throw away the deal.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Order sticker classifier on top and train the classifier and we can do two things.",
                    "label": 0
                },
                {
                    "sent": "Either we can just train the classifier keeping the two encoders fixed.",
                    "label": 0
                },
                {
                    "sent": "Or we can train the entire thing with back Prop will go in dissent.",
                    "label": 0
                },
                {
                    "sent": "OK, we find the entire thing so that we get better results.",
                    "label": 0
                },
                {
                    "sent": "But that's kind of an optional step.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what about this encoder decoder training module?",
                    "label": 0
                },
                {
                    "sent": "And that's where we're going to get into kind of sparsity arguments and things of that type.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to define an energy function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, why?",
                    "label": 0
                },
                {
                    "sent": "Which is the square difference between the input and the the reconstructed input?",
                    "label": 0
                },
                {
                    "sent": "By applying the encoder decoder to it.",
                    "label": 0
                },
                {
                    "sent": "OK, we can interpret this energy function as a log probability.",
                    "label": 1
                },
                {
                    "sent": "If you want of some sort of density model over over Y. OK, so to train the system what we want is that we want to make you of why small in regions of high data density and want to make it large in regions of low data density OK?",
                    "label": 1
                },
                {
                    "sent": "Because E is.",
                    "label": 0
                },
                {
                    "sent": "You can get probabilities out of E by taking my E to the minus E. OK, so the probability of a particular Y under the model is going to be proportional to E to the minus.",
                    "label": 0
                },
                {
                    "sent": "The energy of that Y under the model OK given by the model.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to give high probabilities, you want to give high probability to regions of high density and low probability to regions of low density.",
                    "label": 0
                },
                {
                    "sent": "Therefore low energy to regions of high density and high energy to regions of low density.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So something like this?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say why is uni dimensional.",
                    "label": 0
                },
                {
                    "sent": "This is some sort of probability distribution.",
                    "label": 0
                },
                {
                    "sent": "You want to want to learn?",
                    "label": 0
                },
                {
                    "sent": "Or maybe a log probability distribution and you know the energy function is just minus that minus the log probability.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the way you go from energy to probability is you do either the minus multiplied by some positive constant and normalize.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately the normalization constant is horrible to compute in high dimensional spaces, so you want to avoid doing this.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's really intractable then that's going to be the main problem.",
                    "label": 0
                },
                {
                    "sent": "We're going to have to get around with this.",
                    "label": 0
                },
                {
                    "sent": "These unsupervised learning methods, OK?",
                    "label": 0
                },
                {
                    "sent": "Alright, so so, let's say let's say we want to train the density model like this on natural images OK. Or maybe I'm just patches of natural images.",
                    "label": 0
                },
                {
                    "sent": "So what we want is, you know if this is a natural image here we want to give it low energy and if this is an unnatural image or maybe a noisy image, want to give it high energy.",
                    "label": 0
                },
                {
                    "sent": "And if we had a box that could do this, we could do all kinds of stuff with it.",
                    "label": 0
                },
                {
                    "sent": "We could do compression, we could do denoising, we could do as I am going to show unsupervised learning, so that's really it would be really nice if you could do this in some sort of efficient or general way.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the problem.",
                    "label": 0
                },
                {
                    "sent": "If you want to have a probabilistic approach to this, what you're going to have to do is.",
                    "label": 0
                },
                {
                    "sent": "For every training point you have, you're going to have to change the parameters of your encoder and decoder so that this training point gets properly constructed so you get low energy OK, but at the same time you're going to have to make sure that everything else that you've never seen before, that everything that's not a training sample gets a high energy OK, and that's really the hard part.",
                    "label": 0
                },
                {
                    "sent": "That's basically the gradient of the denominator.",
                    "label": 0
                },
                {
                    "sent": "Here the normalization term you want to make the energy here for on average, for all the training of the non training sample for everything outside of the training.",
                    "label": 0
                },
                {
                    "sent": "The training set high.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of what is depicted here.",
                    "label": 0
                },
                {
                    "sent": "If you have a training sample, why here you want to push down on the energy of the training sample and you want to push up on everything else.",
                    "label": 0
                },
                {
                    "sent": "You can't push up on everything else 'cause it's not.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Miss Dimensional space, right?",
                    "label": 0
                },
                {
                    "sent": "And so you know, you compute the gradient of this loss function.",
                    "label": 0
                },
                {
                    "sent": "The likelihood the average likelihood of the points under the model, and you realize you get very simple to compute, which is a derivative of the energy with respect to the parameters at the training sample, and then you get the derivative of the log partition function, which is integral over all possible points of the probability that your model gives to that point times the derivative of the energy with respect to the parameters at that point OK. You can compute this.",
                    "label": 1
                },
                {
                    "sent": "There's no way, so Jeff Hinton invented this Q technical contrastive divergent switch David mentioned earlier.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not going to go through that.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I shouldn't, but I'm not going to have time.",
                    "label": 0
                },
                {
                    "sent": "But considering this is basically a way of simplifying the second term here by.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So replacing this integral by just essentially a single sample that you choose in a smart way.",
                    "label": 0
                },
                {
                    "sent": "OK, so instead of pushing up on the energy of every single point, you only push up on the energy of well chosen points that are around the training samples, but not too far.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But that was kind of the 32nd high level view of contrastive divergent.",
                    "label": 0
                },
                {
                    "sent": "OK, we're going to do something else.",
                    "label": 0
                },
                {
                    "sent": "We're going to use sparsity as a surrogate to maximizing the log partition function.",
                    "label": 0
                },
                {
                    "sent": "And we're going to do this by restricting the information content of the code.",
                    "label": 1
                },
                {
                    "sent": "Basically, by, you know, applying a sparsity penalty to the code.",
                    "label": 0
                },
                {
                    "sent": "So this is the feature vector from which we reconstruct the input OK. And the main insight is that if the code of the feature vector Z can only take a few different configurations, is only allowed to go through, you know to take a few different values essentially.",
                    "label": 1
                },
                {
                    "sent": "Then on your correspondingly small number of wise can be perfectly reconstructed.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show this.",
                    "label": 0
                },
                {
                    "sent": "On an example.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's say this is a or input space, OK, and.",
                    "label": 0
                },
                {
                    "sent": "Neutral spaces here on the other side.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's say or 20 sample or training samples or the blue points.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we want is we want lower reconstruction error for those points and we want high reconstruction error for everything else, including those pink points.",
                    "label": 0
                },
                {
                    "sent": "So if a feature space allows is allowed to take a lot of different values, OK, so let's say all the all the red points.",
                    "label": 1
                },
                {
                    "sent": "Then for every point here green or blue there is a point here.",
                    "label": 0
                },
                {
                    "sent": "Which means that you know we go through the encoder.",
                    "label": 0
                },
                {
                    "sent": "We find a point.",
                    "label": 0
                },
                {
                    "sent": "We go through the decoder, we decode perfectly.",
                    "label": 0
                },
                {
                    "sent": "For every point, OK, there's no issue.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know noisy samples or bad or natural images will be reconstructed just as well as as as good ones.",
                    "label": 0
                },
                {
                    "sent": "So here's what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "We're going to make the feature space very small.",
                    "label": 1
                },
                {
                    "sent": "We're going to allow only a few configurations.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the in the space of the feature vector, OK, we're going to do this by a sparsity penalty.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Happens is.",
                    "label": 0
                },
                {
                    "sent": "When you take a training sample, it goes through its corresponding code and is being correctly constructed, but will take any other sample.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It has to go to one of those, right?",
                    "label": 0
                },
                {
                    "sent": "It has to go to one of those codes that are the only ones you allow, and you know that these guys reconstruct one of the training samples.",
                    "label": 0
                },
                {
                    "sent": "OK, or something close to it.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so here is an example where you feed a noisy example to the system.",
                    "label": 0
                },
                {
                    "sent": "You go through the encoder decoder and what you get is a cleaner version of it.",
                    "label": 0
                },
                {
                    "sent": "You get the closest, maybe the closest thing that looks like the training sample or that lives on the training sample manifold.",
                    "label": 0
                },
                {
                    "sent": "That is kind of as similar as possible to the the input you gave.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we're going to use a sparsity penalty to restrict the information content of the code, and why you sparsity.",
                    "label": 0
                },
                {
                    "sent": "The reason is because high dimensional sparse codes so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We back up just just a little with one way of restricting the information content of the code is and the simplest ways to make it low dimensional, and that's essentially what PC does.",
                    "label": 0
                },
                {
                    "sent": "The reason PCA works is because it does not allow the feature space to be as big as the input space.",
                    "label": 1
                },
                {
                    "sent": "If you allow the feature space to be as big as the input space, you could just get the identity function and PC.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do anything good for you.",
                    "label": 0
                },
                {
                    "sent": "Essentially OK. Um?",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we want is we want to be able to produce high dimensional representations, maybe higher dimensional than the input, but we want them to be sparse on the reason for this is because in a high dimensional sparse space things are more easily separable.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's good for classification to do that.",
                    "label": 0
                },
                {
                    "sent": "There also some neuroscience argument for it, but I'm not going to do that.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we're going to start by just focusing on the decoder and the reason we're going to focus on the decoder is because it's a very old idea, which you know I didn't invent.",
                    "label": 0
                },
                {
                    "sent": "I guess we could go back to all sudden feel 1997 and they sort of got inspired by the you know work on business pursuit, which is a little older than that.",
                    "label": 0
                },
                {
                    "sent": "OK, so so so I don't have an encoder here.",
                    "label": 0
                },
                {
                    "sent": "I only have a decoder and the decoder is linear.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically you take a feature vector Z.",
                    "label": 0
                },
                {
                    "sent": "An you run it through a linear decoder, so you just multiplied by matrix.",
                    "label": 0
                },
                {
                    "sent": "This could be a rectangular matrix sosias higher dimension then why generally?",
                    "label": 0
                },
                {
                    "sent": "And then you're going to compute the square distance between why the input and the reconstructed version of Z&Z also goes through a a kind of sparsity module, so something that computes the sum of the absolute value of the terms.",
                    "label": 0
                },
                {
                    "sent": "For example, it's exactly identical towards different.",
                    "label": 0
                },
                {
                    "sent": "I was talking about earlier.",
                    "label": 0
                },
                {
                    "sent": "OK, but what we're going to do with this is that we're going to learn the matrix WD.",
                    "label": 0
                },
                {
                    "sent": "OK. And we're going to do this is very simple, so we can we write an energy function, which is a function of YZ&W OK, and so it's the square error between Y and its reconstruction WDC.",
                    "label": 0
                },
                {
                    "sent": "Plus some constant times the sum of the absolute value of the components of the OK.",
                    "label": 0
                },
                {
                    "sent": "So if I give you an X, if I give you a Y.",
                    "label": 0
                },
                {
                    "sent": "You find the optimal Z by minimizing this function.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this operation here.",
                    "label": 0
                },
                {
                    "sent": "And for learning what we're going to do is, we're just going to minimize the average of that overall or training samples wise.",
                    "label": 0
                },
                {
                    "sent": "OK, we're just going to do this weekend in dissent.",
                    "label": 0
                },
                {
                    "sent": "Nothing fancy.",
                    "label": 0
                },
                {
                    "sent": "OK, just compute the gradient of this for every training sample and do an update.",
                    "label": 0
                },
                {
                    "sent": "You can you suggest a green dissent works really well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is really cool.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's only one problem with this, which is that it's slow at runtime, because if you if you have this system, if I give you a why you have to run an optimization algorithm to find the Z. OK, so it's something that's defined was referring to earlier, and there are lots of very smart methods to do this, but it's still very slow.",
                    "label": 0
                },
                {
                    "sent": "You have to run an optimization algorithm to do that.",
                    "label": 0
                },
                {
                    "sent": "And also, as Steven was mentioning the solution you get is very unstable.",
                    "label": 0
                },
                {
                    "sent": "In other words, if you change the Y just a little bit, you might wait.",
                    "label": 0
                },
                {
                    "sent": "You might get to completely different Z. OK, that's not good for recognition.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's how we're going to fix this.",
                    "label": 0
                },
                {
                    "sent": "Going to fix this by adding an encoder.",
                    "label": 0
                },
                {
                    "sent": "OK, so going back to this encoder idea and the encoder is going to predict what the solution to this optimization problem is.",
                    "label": 0
                },
                {
                    "sent": "OK, so assume we have the decoder that we've already trained it.",
                    "label": 0
                },
                {
                    "sent": "So we have RWD matrix and we can give a why we find the optimal Z.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do now is we're going to plug an encoder.",
                    "label": 0
                },
                {
                    "sent": "He ran.",
                    "label": 0
                },
                {
                    "sent": "The encoder is going to be trained to predict the solution that Z will that we get when we run the optimization algorithm on the original energy function.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have is a feedforward predictor for this.",
                    "label": 0
                },
                {
                    "sent": "The sparse vector that we would otherwise have to obtain by running an optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be a good approximation, but maybe it's good enough to do recognition.",
                    "label": 0
                },
                {
                    "sent": "So in fact we actually do is slightly more complicated than this.",
                    "label": 0
                },
                {
                    "sent": "So instead of training this first, getting optimal codes and then training a a a feedforward predictor to predict those codes, we actually going to train the entire thing at the same time.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to replace the original energy function that we had that only had the reconstruction error term and the sparsity term.",
                    "label": 0
                },
                {
                    "sent": "We're going to add a third term, which is a prediction term.",
                    "label": 0
                },
                {
                    "sent": "So is the difference between Z and whatever this guy wants to predict.",
                    "label": 0
                },
                {
                    "sent": "Z should be.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So it's this big energy function on top here, which has those three terms.",
                    "label": 0
                },
                {
                    "sent": "We can decide, so F is going to have to be nonlinear, because if it's linear kind of defeats the purpose.",
                    "label": 0
                },
                {
                    "sent": "So this is going to have to be only near and we choose the kind of simplest nonlinear thing we can imagine, which is we take Y multiplied by matrix.",
                    "label": 0
                },
                {
                    "sent": "WE apply hyperbolic attention to all the components like, say neural net layer, and then multiply each of the components by some constant that we also learned is sort of trainable parameters.",
                    "label": 0
                },
                {
                    "sent": "Non you just make it an even nastier nonconvex optimization, yes?",
                    "label": 0
                },
                {
                    "sent": "We don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't care about being things being convex or non convex, right?",
                    "label": 0
                },
                {
                    "sent": "I don't care.",
                    "label": 0
                },
                {
                    "sent": "Unstable before this is more stable because this function is smooth.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I give if I give a Y. I get I get dizzy and if I change why a little bit I get a similar Z because this function is smooth.",
                    "label": 0
                },
                {
                    "sent": "Whereas the function that gives you the optimal Z4, Y with just a decoder.",
                    "label": 0
                },
                {
                    "sent": "That's very highly non smooth OK.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the point.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "You can view this as kind of a regularizer for the.",
                    "label": 0
                },
                {
                    "sent": "For the other thing you can view this.",
                    "label": 0
                },
                {
                    "sent": "The second term here as a regularizer for for the first you know the energy function you get, otherwise.",
                    "label": 0
                },
                {
                    "sent": "We basically set Lambda through cross validation.",
                    "label": 0
                },
                {
                    "sent": "OK so you know.",
                    "label": 0
                },
                {
                    "sent": "Does this, no, there's no real way to learn Lambda Lambda 'cause you can't really do that.",
                    "label": 0
                },
                {
                    "sent": "It's not a.",
                    "label": 0
                },
                {
                    "sent": "He's going to want to go to zero or something like that, 'cause it's much easier to solve.",
                    "label": 0
                },
                {
                    "sent": "The problem is number is 0, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was a really nice paper rejected from NIPS last year so you can get it as a tech report.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And but again, you know it's a good sign.",
                    "label": 0
                },
                {
                    "sent": "Consider rejected twice as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the so the procedure is is very simple.",
                    "label": 0
                },
                {
                    "sent": "We we get it.",
                    "label": 0
                },
                {
                    "sent": "Why we find the Z that minimizes this overall energy function.",
                    "label": 0
                },
                {
                    "sent": "Some of those three terms.",
                    "label": 0
                },
                {
                    "sent": "And once we have the why we tweak the parameters of the encoder and decoder so as to make the energy of it smaller.",
                    "label": 0
                },
                {
                    "sent": "OK. And the role of the sparsity term here is to make sure that we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't learn a kind of a a trivial identity function by going through encoder and decoder.",
                    "label": 0
                },
                {
                    "sent": "But limit the information content of the code.",
                    "label": 0
                },
                {
                    "sent": "That's what I was talking about earlier.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's that's the predictive sparsity composition algorithm.",
                    "label": 0
                },
                {
                    "sent": "So for a given training sample, initialize Z to the output of the encoder.",
                    "label": 0
                },
                {
                    "sent": "Then you find the Z that minimizes the energy function through some sort of gradient descent algorithm, or some smart technique did the decoder basis functions to reduce the reconstruction error of the encoder parameters to reduce the prediction error and repeat with the next training sample.",
                    "label": 0
                },
                {
                    "sent": "OK, very simple.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you apply this to Ernest, you get things like this, so these are the basis functions, but they also are the filters.",
                    "label": 0
                },
                {
                    "sent": "They actually end up being the same to transpose.",
                    "label": 0
                },
                {
                    "sent": "So what you get here?",
                    "label": 0
                },
                {
                    "sent": "So basically what it tells you is I can reconstruct any digit as a weighted sum of a small subset of those guys, OK?",
                    "label": 0
                },
                {
                    "sent": "'cause each of those is is a colon in the WD matrix OK. And so when you multiply it by his past vector you get a weighted sum of a small number of those guys and can basically reconstructing digit by weighted sum of small number of those guys.",
                    "label": 0
                },
                {
                    "sent": "And So what they end up being when you train the system is that they end up being small parts.",
                    "label": 0
                },
                {
                    "sent": "Like you know atoms.",
                    "label": 0
                },
                {
                    "sent": "Words as.",
                    "label": 0
                },
                {
                    "sent": "As Stephen we call them know Alphabet letters.",
                    "label": 0
                },
                {
                    "sent": "OK is an alphabet.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you apply them to natural image patches here, there are 12 by 12 natural image patches from the Berkeley data set, and this is the running algorithm actually taking place.",
                    "label": 0
                },
                {
                    "sent": "This is a movie, it's not running in real time, but it's actually quite fast, So what you get in the end are Gabor filters.",
                    "label": 0
                },
                {
                    "sent": "OK, no real surprise here.",
                    "label": 0
                },
                {
                    "sent": "Similar to what Rosenfield were reporting in their decoder on your system.",
                    "label": 0
                },
                {
                    "sent": "So this is something you know.",
                    "label": 0
                },
                {
                    "sent": "If you have a large number of those basis functions, little nice keyboard filters.",
                    "label": 0
                },
                {
                    "sent": "You can even have a huge number of them, and they don't die.",
                    "label": 0
                },
                {
                    "sent": "They they all do something.",
                    "label": 0
                },
                {
                    "sent": "Alright, so how are we going to use?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This for object recognition.",
                    "label": 0
                },
                {
                    "sent": "How well does that?",
                    "label": 0
                },
                {
                    "sent": "Does that do on Catholic went OK?",
                    "label": 0
                },
                {
                    "sent": "Can we use this to kind of pre train the filters of a convolutional net and and get better results from that?",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to use this real encoder decoder architecture to train a filterbank on natural image patches or size 9 by 9.",
                    "label": 0
                },
                {
                    "sent": "We're going to train 64 filters OK. We're going to get our essentially get more filters.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to pass this through a series of nonlinearities, and then do a little special pulling.",
                    "label": 0
                },
                {
                    "sent": "So this whole thing is, like you know, it could be like a shift, or the first stage of the convolutional net, or the first stage of Tommy Pojo's HVAC system.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to take that and plug that to a linear classifier or in SVM.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process the images in various ways.",
                    "label": 0
                },
                {
                    "sent": "High pass filter normalized contrast by Baba.",
                    "label": 0
                },
                {
                    "sent": "And the encoder again is linear matrix multiplied by.",
                    "label": 0
                },
                {
                    "sent": "Then passed through hypertension and then multiplied by scaling.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Filters we get on 64 with 64 filters on natural images.",
                    "label": 0
                },
                {
                    "sent": "OK, give more shelters, essentially with some tweaks.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we're going to expel.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was sort of biased.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So nonlinearity that.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put on top of this so with something like, say, absolute value rectification or some sort of contrast normalization where we replace every value by itself divided by the standard deviation of his neighbors.",
                    "label": 0
                },
                {
                    "sent": "And the pulling down sampling which we can do either with them.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maxwell with an average.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have four different procedures for training this system.",
                    "label": 0
                },
                {
                    "sent": "The first one, the first procedure is, we don't train it at all.",
                    "label": 0
                },
                {
                    "sent": "We just set the filters to random values.",
                    "label": 0
                },
                {
                    "sent": "And that's the end of it.",
                    "label": 0
                },
                {
                    "sent": "We're just trying to classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, second procedure is we use this PSD training to train in unsupervised mode.",
                    "label": 0
                },
                {
                    "sent": "So this is denoted by U.",
                    "label": 0
                },
                {
                    "sent": "3rd technique is we set the filters to random values, but we train them using backdrop simultaneously with the classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so this would be kind of a traditional convolutional net training, supervised training and the 4th one is we initialize the weights with the result of the unsupervised PSD algorithm and then we do a refinement using gradient descent over the entire system OK. Our classifier is.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just multinomial logistic regression on top, but we can back propagate through that.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's an eye chart.",
                    "label": 0
                },
                {
                    "sent": "A big table of results and it's kind of sobering to some extent.",
                    "label": 0
                },
                {
                    "sent": "Because regardless of what 20 procedure you use, if you put the right set of nonlinearities, it basically makes no difference.",
                    "label": 0
                },
                {
                    "sent": "You can use random completely random filters.",
                    "label": 0
                },
                {
                    "sent": "With one stage of feature extraction, so you do you know 9 by 9 filters, sigmoid gain, absolute value rectification, contrast, normalization pulling using an average, and then you stick that to a logistic regression and you get you know in the low to mid 50s on category one.",
                    "label": 0
                },
                {
                    "sent": "You know it doesn't break any record record is 65 or so for a single feature set.",
                    "label": 0
                },
                {
                    "sent": "But the bad news is that it makes no difference what learning procedure you use.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't make much difference.",
                    "label": 0
                },
                {
                    "sent": "So here is another number also using the unsupervised training procedure.",
                    "label": 0
                },
                {
                    "sent": "Here's what you get if you just built in keyboard filters.",
                    "label": 0
                },
                {
                    "sent": "OK, basically the same result.",
                    "label": 0
                },
                {
                    "sent": "OK, that's really not surprising because you know what the system learns in the end, our global filters.",
                    "label": 0
                },
                {
                    "sent": "So you get the same as with Gabor filters.",
                    "label": 0
                },
                {
                    "sent": "And if you use the unsupervised filters and you plug them through a pyramid match kernel instead of a logistic regression, you get the same result as if you sift.",
                    "label": 0
                },
                {
                    "sent": "OK, around 65% or so.",
                    "label": 0
                },
                {
                    "sent": "So that's really cool.",
                    "label": 0
                },
                {
                    "sent": "What that means is that with our very simple unsupervised learning algorithm, we can basically get the same result as if you use sift on natural images.",
                    "label": 0
                },
                {
                    "sent": "OK, but there's a big difference.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "We can apply this to all kinds of data that are not like natural images, and we can apply them to the second.",
                    "label": 0
                },
                {
                    "sent": "The second stage of a similar system.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's do that now.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so before that.",
                    "label": 0
                },
                {
                    "sent": "OK, so rectification makes a huge difference as you saw here.",
                    "label": 0
                },
                {
                    "sent": "If you don't rectify the output you go from.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So kind of a pure convolutional net type thing with, you know, just filters sigmoid and then pulling average pooling gives you 14.5%.",
                    "label": 0
                },
                {
                    "sent": "This is ridiculously bad.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if you just add rectification you get 50% OK, so it's really important to rectify.",
                    "label": 0
                },
                {
                    "sent": "It's also somewhat important to normalize it.",
                    "label": 0
                },
                {
                    "sent": "Sort of gives you a little boost of 4%.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And supervised pretraining.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately it doesn't make much difference in this case it did in other cases that we saw but not in this one.",
                    "label": 0
                },
                {
                    "sent": "And the really weird thing is that random filters work really well.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me skip this.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's go to a two stage system.",
                    "label": 0
                },
                {
                    "sent": "So we're going to stack two stages of this question.",
                    "label": 0
                },
                {
                    "sent": "Say again.",
                    "label": 0
                },
                {
                    "sent": "I think they are drawn with some sort of some Gaussian independent, so each value is independently drawn from a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's do two stages now.",
                    "label": 0
                },
                {
                    "sent": "OK, so we trained the first stage.",
                    "label": 0
                },
                {
                    "sent": "Then we generate all the features on the you know on some natural images.",
                    "label": 0
                },
                {
                    "sent": "Then we trained the second stage and supervised and then we put the whole system together and again we can either just trying to classify on top or trendy and refine the entire thing in supervised mode.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is the table results.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the good news.",
                    "label": 0
                },
                {
                    "sent": "The good news is.",
                    "label": 0
                },
                {
                    "sent": "With the second stage and a multinomial logistic regression on top, we get 65.5%, which is essentially the same as a pyramid match kernel system on the single stage.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have one stage and P match carnal is VM, you get essentially the same result as you have two stages.",
                    "label": 0
                },
                {
                    "sent": "Another secret question.",
                    "label": 0
                },
                {
                    "sent": "So the second stage basically plays the same role as the as the.",
                    "label": 0
                },
                {
                    "sent": "The Colonel in the Pyramid match kernel, but the cool thing is that we learned it.",
                    "label": 0
                },
                {
                    "sent": "We didn't have to build it by hand, right?",
                    "label": 0
                },
                {
                    "sent": "So we can apply the same method to all kinds of data that for which may be problematic.",
                    "label": 0
                },
                {
                    "sent": "Kernel wouldn't really make sense.",
                    "label": 0
                },
                {
                    "sent": "But again, the weird thing is that it doesn't make much difference whatever learning algorithm you use.",
                    "label": 0
                },
                {
                    "sent": "Even random filters actually work well.",
                    "label": 0
                },
                {
                    "sent": "At least if you have normalization and rectification, that's really a big mystery.",
                    "label": 0
                },
                {
                    "sent": "It's a huge surprise.",
                    "label": 0
                },
                {
                    "sent": "It's kind of sobering for machine learning.",
                    "label": 0
                },
                {
                    "sent": "Like me, you know.",
                    "label": 0
                },
                {
                    "sent": "And basically out of business now.",
                    "label": 0
                },
                {
                    "sent": "But now I'm kidding, but you know.",
                    "label": 0
                },
                {
                    "sent": "OK, this is another thing, so here indicated by GT.",
                    "label": 0
                },
                {
                    "sent": "So this is a few built-in Gabor filters at the first layer, and you sort of templates Start learning at the second layer, which is kind of similar to me, which is what Thomas errand Merchant low were doing in the 2006.",
                    "label": 0
                },
                {
                    "sent": "And here you get about 56%, which is, you know, quite a bit quite a bit below.",
                    "label": 0
                },
                {
                    "sent": "Maybe you get, maybe they got better results since then, I don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, and here is the other surprise is that if you take a two stage system and you substitute logistic regression for this pyramid, match kernel SVM, you actually actually get worse performance than with logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So per match kernel is good on low level features, but it's not good on high level features.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, these are cute picture.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As of the filters, we get this first and second layer before and after.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Supervised refinement, but I'm going to skip this.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is interesting about about in NIST, so if we use this, you plus you plus technique.",
                    "label": 0
                },
                {
                    "sent": "So where we initialized unsupervised and then we refine using supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We basically beat the record on ennist.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can't get anywhere close to this if you use Sift and pyramid, match kernel with 15 pyramid, match kernel and then this gives you 1.5% error.",
                    "label": 0
                },
                {
                    "sent": "With this we get .53% error.",
                    "label": 0
                },
                {
                    "sent": "So what that shows you is that you receive temporary magical works really well, natural images, but on anything else it's really not not really well suited for that.",
                    "label": 0
                },
                {
                    "sent": "Or as this can be basically trained for on just about any data, even audio or something, we haven't tried that yet, but will.",
                    "label": 0
                },
                {
                    "sent": "Who will soon?",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another question is why do random filters work?",
                    "label": 0
                },
                {
                    "sent": "It's kind of a mystery right?",
                    "label": 0
                },
                {
                    "sent": "So what we did was a little experiment where we took our filters, run them through the first stage of feature extraction and we degrade in this hand in input space to find the input configuration that maximizes the output of a particular feature particular variable on the on the output.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's kind of like what neuroscientists do write the book, an electrode in a neuron in the brain, and then they try to sort of figure out what configuration, what stimulus will maximize the output of that neuron.",
                    "label": 0
                },
                {
                    "sent": "That's exactly what we did, but here we have the derivative of this thing, so we can actually back propagate through it.",
                    "label": 0
                },
                {
                    "sent": "And you know, find the optimal input.",
                    "label": 0
                },
                {
                    "sent": "What you find is really strange, which is that you know if you have give or filters because you go through those complex sales.",
                    "label": 0
                },
                {
                    "sent": "This pooling mechanism, the sort of pattern that maximally activates a particular unit, is like a grading as some at some orientation and some frequency not very surprising.",
                    "label": 0
                },
                {
                    "sent": "The surprising thing is that even with random filters, you basically get the same thing, some sort of noisy version of it.",
                    "label": 0
                },
                {
                    "sent": "That's really strange.",
                    "label": 0
                },
                {
                    "sent": "So basically because there is just a little bit of asymmetry in the random filters, you know when you sort of average them out and you kind of contrast, normalize them, you get response.",
                    "label": 0
                },
                {
                    "sent": "High responses for gradings and no responses for other things, so they become orientation selective even though they are random.",
                    "label": 0
                },
                {
                    "sent": "So it's really cool because it makes life easy for evolution if evolution wants to create a visual system, it doesn't actually need to get a learning algorithm, it just needs to get the nonlinearities right and then have random random weights.",
                    "label": 0
                },
                {
                    "sent": "And I'm sure some theoretician in sort of compressed sensing could come up with some good justification for.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well, but.",
                    "label": 0
                },
                {
                    "sent": "This is going.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New result.",
                    "label": 0
                },
                {
                    "sent": "But here's kind of something to kind of modulate this a little bit, which is that we made some experiments on the data set of object with sort of multiple orientations where we have almost 5000 training samples per category with five categories and 5000 training sample spaghetti with lots of different orientations.",
                    "label": 0
                },
                {
                    "sent": "And what we can take 101 is sort of in this regime.",
                    "label": 0
                },
                {
                    "sent": "Here we have about 30 training samples per category.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this regime here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning.",
                    "label": 0
                },
                {
                    "sent": "From random filters, there's no unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just supervised learning of the entire thing.",
                    "label": 0
                },
                {
                    "sent": "And this is a.",
                    "label": 0
                },
                {
                    "sent": "This is random filters OK.",
                    "label": 0
                },
                {
                    "sent": "Forget about the blue line blue line.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about later.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of, you know, a standard convolutional net with all the right nonlinearities in it.",
                    "label": 0
                },
                {
                    "sent": "Rectification, contrast, normalization.",
                    "label": 0
                },
                {
                    "sent": "And it only works marginally better.",
                    "label": 0
                },
                {
                    "sent": "This is a log log plot, but you know, it only works marginally better than random filters for a small number of training samples, but as soon as you increase the number of training samples and it gets a lot better.",
                    "label": 0
                },
                {
                    "sent": "OK, so the reason why all the numbers are kind of the same.",
                    "label": 0
                },
                {
                    "sent": "We can take 101 is because we are in this regime.",
                    "label": 0
                },
                {
                    "sent": "But if we had more training samples, then running actually helps you kind of make sense right?",
                    "label": 0
                },
                {
                    "sent": "If you have more labeled more samples, learning helps.",
                    "label": 0
                },
                {
                    "sent": "The blue line is if you.",
                    "label": 0
                },
                {
                    "sent": "If we use a traditional convolutional net which does not have normalization and rectification, and we see the same thing if we don't have many training samples, then this really doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "But as we increase the number of training samples and basically the learning process compensates for the fact that we don't have the right architecture, they don't have the right nonlinearities and we get pretty much the same error rate whether we have the complicated narratives on our dinner.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically if you don't have many training samples you need to put more prior knowledge about about the about the data and that's in the form of nonlinearities and things like this here.",
                    "label": 0
                },
                {
                    "sent": "But if you have tons of training samples, then it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You want to learn everything and the precise detail of the architecture don't matter because you learn anyway.",
                    "label": 0
                },
                {
                    "sent": "It all makes sense.",
                    "label": 0
                },
                {
                    "sent": "So we made some changes on this that allows us to.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If So what I've told you hear the learning algorithm, only learned only trains to simple cells, and then we kind of stack the pool on top of it as kind of an afterthought, and so the other thing we did that will appear in CPR in a few weeks is an idea where we learn the complex cells simultaneously with the simple cells.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The cute thing about this is that it does not involve changing anything about the encoder and decoder, or the only thing that changes is how we construct this prosody term.",
                    "label": 0
                },
                {
                    "sent": "So the sparsity term instead of insisting that.",
                    "label": 0
                },
                {
                    "sent": "So you know the instead of being the sum of the absolute value of the components of the code is basically the square root of the sum of the squares of of the codes.",
                    "label": 0
                },
                {
                    "sent": "But the sum only takes place on blocks that overlap.",
                    "label": 0
                },
                {
                    "sent": "OK, and those blocks constitute the filters that will be summed up within the complex cells.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going a little fast on this, I'm realizing, but.",
                    "label": 0
                },
                {
                    "sent": "What this is going to do is that is going to drive the system, so we're going to organize the filters in some sort of topology.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have to be today, it could be anything.",
                    "label": 0
                },
                {
                    "sent": "And we're going to take Windows over this.",
                    "label": 0
                },
                {
                    "sent": "Within this, we're going to compute the square root of the sum of the square of the filter outputs.",
                    "label": 1
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "OK, we're going to do this on overlapping.",
                    "label": 0
                },
                {
                    "sent": "Patches within this topology.",
                    "label": 0
                },
                {
                    "sent": "And then or sparsity term is going to be the sum of all of those terms OK, and so the usual L1 sparsity is a limit case of this, where the pools have size 1.",
                    "label": 0
                },
                {
                    "sent": "OK, because absolute value is equal to square root of square.",
                    "label": 0
                },
                {
                    "sent": "What this what this does to the system is that it drives the filters that are within the pool to kind of be similar.",
                    "label": 0
                },
                {
                    "sent": "Because it wants the smallest number of pool to be on at any one time, that's a sparsity constraint.",
                    "label": 0
                },
                {
                    "sent": "But within a pool you can have multiple filters that are on.",
                    "label": 0
                },
                {
                    "sent": "It really doesn't care.",
                    "label": 0
                },
                {
                    "sent": "OK, so it tries to regroup all the put all the filters that are similar within the pool and this.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what what happens?",
                    "label": 0
                },
                {
                    "sent": "So this is a bunch of filters that are drawn according to this topology that we sort of dreamed up.",
                    "label": 0
                },
                {
                    "sent": "And this is one of the pools.",
                    "label": 0
                },
                {
                    "sent": "OK, we have pools like this that are kind of square that is set by two or three pixels everywhere, and what you get this kind of graphic map of filters that are all similar to each other and you sum up the output of all the filters within the pool.",
                    "label": 0
                },
                {
                    "sent": "You kind of get a complex sale kind of a local invariant feature.",
                    "label": 0
                },
                {
                    "sent": "OK, you show this to biologists and they say, well, this looks just like you know what we see in the V1 when you move an electrode from one place to another, you get smoothly varying.",
                    "label": 0
                },
                {
                    "sent": "Orientation selectivity.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We even get.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pinwheels",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we can use those features and plug them into a classifier again and.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So put either linear classifier or pyramid match kernel on top and you know we that's the wrong number, sorry.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, it's wrong number.",
                    "label": 0
                },
                {
                    "sent": "We actually get 65% on both.",
                    "label": 0
                },
                {
                    "sent": "I don't know why this is 59%.",
                    "label": 0
                },
                {
                    "sent": "This would be about 65, so we get about the same performance as we're getting with the previous algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's the sad news we going any better result.",
                    "label": 0
                },
                {
                    "sent": "It's about the same as C.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have some hardware.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oracle so, but I'm not going to skip into this order.",
                    "label": 0
                },
                {
                    "sent": "The last thing I'm going to show you is really quick, so is some application of those techniques for robot vision and.",
                    "label": 0
                },
                {
                    "sent": "3 minutes or something.",
                    "label": 0
                },
                {
                    "sent": "Two OK.",
                    "label": 0
                },
                {
                    "sent": "So basically what we have is this robot that runs around and has four cameras and is trying to.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a bit like the thing that David was talking about.",
                    "label": 0
                },
                {
                    "sent": "What the robot is supposed to do is tell you that whether there is an obstacle in front and what distance the obstacle is, so it has to label all the pixels in the image and tell you whether the weather is it reversible thing or non traversable thing, whether it's path or piece of grass or whether it's a tree or rock or something like that, right and.",
                    "label": 0
                },
                {
                    "sent": "We trained the system.",
                    "label": 0
                },
                {
                    "sent": "We get labels from stereo vision.",
                    "label": 0
                },
                {
                    "sent": "So for objects that are nearby we can get labels from a sort of heuristic stereo vision algorithm that he goes out the 3D structure of everything that sticks out with something sticks out of the ground.",
                    "label": 0
                },
                {
                    "sent": "We label it as not reversible, and if it's on the ground will just reversible.",
                    "label": 0
                },
                {
                    "sent": "So for things that are nearby we get labels for things that are far away.",
                    "label": 0
                },
                {
                    "sent": "We don't get labels, but we train the system online as a robot runs to kind of distinguish between traversable non traversable from the labels that are nearby and then we apply it to the entire image.",
                    "label": 0
                },
                {
                    "sent": "So we can label.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Entire image as to whether it's reversible or not.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of details that I'm going to skip, obviously, so it's using.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of those convolutional Nets.",
                    "label": 0
                },
                {
                    "sent": "And and is using this unsupervised pretraining a slightly different version of it, and this is what the classifier produces on the on the right?",
                    "label": 0
                },
                {
                    "sent": "So Green is traversable, red is non traversable and pink.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is kind of the foot of obstacles.",
                    "label": 0
                },
                {
                    "sent": "And you know.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can run this thing through and we get really good results which I'm going to show you, but what I'm going to show you is.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "He video.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much and I'll take question when the weather we will rent.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Steven structures.",
                    "label": 0
                },
                {
                    "sent": "Can you give?",
                    "label": 0
                },
                {
                    "sent": "An argument explained that he said that there were some already pulled out.",
                    "label": 0
                },
                {
                    "sent": "So you want me to say few things about the theoretical arguments, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so let me give you an argument that doesn't have to do with continuous functions, but binary functions, Boolean functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to.",
                    "label": 0
                },
                {
                    "sent": "You can implement any Boolean function of N bits as a DNF or CNF, right?",
                    "label": 0
                },
                {
                    "sent": "So a bunch of ends and a bunch of hours with some negation.",
                    "label": 0
                },
                {
                    "sent": "OK, essentially a two layer architecture.",
                    "label": 0
                },
                {
                    "sent": "If you will, but almost every single function of N bits is going to require two to the end in terms on the order of an exponential number of minterms, right?",
                    "label": 0
                },
                {
                    "sent": "There's only a logarithmic logarithmic small number of functions that require an an exponential number of in terms.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you want to take if you want to multiply 2 by 2 billion numbers or add 2 billion numbers, let's say OK to binary numbers.",
                    "label": 0
                },
                {
                    "sent": "You can do it within two layers, but it's going to take a huge number of in terms, but if you allow yourself to have multiple layers, so for example you allow yourself to do carry propagation, then you can do it with hardware hardware complexity order N, But then you have to have N layers.",
                    "label": 0
                },
                {
                    "sent": "OK, another example is parity.",
                    "label": 0
                },
                {
                    "sent": "So let's say I give you N bits and I ask you, do I have a even or odd number of ones?",
                    "label": 0
                },
                {
                    "sent": "You can do it in two layers, but it's exponential.",
                    "label": 0
                },
                {
                    "sent": "Or you can do it in log in layers with a bunch of stores.",
                    "label": 0
                },
                {
                    "sent": "Ann and I, you know, linear in the number of struggle with making the number of inputs.",
                    "label": 0
                },
                {
                    "sent": "So there is this sort of exchange between depth and breath.",
                    "label": 0
                },
                {
                    "sent": "You can gain an exponential factor on the breath by a very, very minor increase of the depth alright for Boolean functions, and there is a sense that this is also true for continuous functions.",
                    "label": 0
                },
                {
                    "sent": "So the functions we try to learn you know for vision and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "This kind of an exchange of you know.",
                    "label": 0
                },
                {
                    "sent": "How much computational steps of some complexity you allow yourself versus you know how many things you have to compute in parallel?",
                    "label": 0
                },
                {
                    "sent": "Now there's some theory about this on in the circuit theory, but for Boolean functions mostly.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "You guys are hypnotized by the video.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}