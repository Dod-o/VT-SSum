{
    "id": "3zunzzxxiagootajlpnpc3t277c6povq",
    "title": "Horizon-Independent Optimal Prediction with Log-Loss in Exponential Families",
    "info": {
        "author": [
            "Fares Hedayati, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_hedayati_prediction/",
    "segmentation": [
        [
            "So this is joint work with three Peters, Barcliff, Grunwald and Harmos and Voytek Kowski.",
            "So they were."
        ],
        [
            "5th in online learning on their log loss at each at time T we want to predict this valuable YT which is in red here and we give our prediction in form of a probability distribution.",
            "So given the past history we want to see what's YT, what's the, what's the probability of YT and our losses?",
            "The negative log of what we have predicted?",
            "So and we can consult with a group of expert, which in our case are IID parametric experts.",
            "And the goal is to minimize the cumulative loss.",
            "Cumulative.",
            "Loss in comp."
        ],
        [
            "Isn't it the best expert in class?",
            "So this difference you can see.",
            "So the 1st.",
            "Is our cumulative loss, and the 2nd is the cumulative loss of the best expert.",
            "In a sense we want to.",
            "You're competing with maximum likelihood, as you can see.",
            "So this is you have log of maximum likelihood of sequence Y N / R. Distribution because at each time you are giving a conditional distribution, if you multiply all these conditionals, we get a joint.",
            "So this is very important to remember, so we have a joint distribution over N random variables.",
            "So we look at that joint distribution and we look at the maximum likelihood.",
            "So the ratio we want to minimize this is the regret that."
        ],
        [
            "Yep.",
            "So any joint distribution corresponds to conditionals at each time.",
            "By marginalization an any product of conditions is a joint distribution.",
            "So for a given horizon N, we know that the."
        ],
        [
            "Best thing we can do.",
            "Yeah, the horizon is given to us and we want to come up with a sequential probability assignment that is minimax.",
            "Regret is the least for the worst possible data.",
            "And the best way to do it is.",
            "Through an algorithm called MLE, normalized maximum likelihood.",
            "So you say.",
            "Let's take a look at the entire sequence.",
            "What's the the probability that I assigned to this sequence is proportional to its maximum likelihood?",
            "And at the end you market value normalized to get their probability.",
            "But the problem with normalized maximum likelihood is.",
            "That it is defined in terms of a joint distribution and you have to normalize at each time T to get the conditions, so that's that's that's a huge problem.",
            "If let's say you're doing binary prediction at time T, you have to do 2 two N -- T sums to marginalized, right?",
            "So that's a very big problem.",
            "So instead of that, people looked at alternatives, one alternate."
        ],
        [
            "User base in a strategy you put the prior on your experts and you update your posterior and mix very easy and natural way too.",
            "Do this problem and.",
            "Yeah, so that's one way and another."
        ],
        [
            "He is one step at look up sequential normals maximum likelihood.",
            "Instead of looking at the entire sequence and marginalizing, we just look at one.",
            "One random variable ahead.",
            "You know you can't.",
            "It's defined in red, so you look at the next variable and so think what happens if I observe Whitey and you concatenated with your history, look at the maximum likelihood and then normalize.",
            "It's a very easy way, you know.",
            "Both this and the previous one are naturally defined in terms of conditionals, which is really easy to compliment to implement.",
            "So now you know previous."
        ],
        [
            "It was shown that these three algorithms, namely base."
        ],
        [
            "Asian under a very specific prior called Jeffreys prior SNL.",
            "Want to look up an normalized maximum likelihood.",
            "We have shown that they are all exactly the same or all different, meaning that if either if two of these are the same, there should be this same as the third one, so it's an equivalence class, so it's quite interesting if we have.",
            "These are three all the same, or they're all different, so we have.",
            "So the question."
        ],
        [
            "Is so when are these the same?",
            "So if there are exchangeable, if this process is extreme."
        ],
        [
            "Triple SML.",
            "Then there are all the same.",
            "But the question is when does this happen in?",
            "For example, let's say exponential families.",
            "So in this paper we"
        ],
        [
            "Shown that this phenomena can only happen in these three families."
        ],
        [
            "Namely gaussian."
        ],
        [
            ", and Tweedie distribution of order three 1.5 and any one to one transformation of these families come to our poster.",
            "We have more details, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with three Peters, Barcliff, Grunwald and Harmos and Voytek Kowski.",
                    "label": 0
                },
                {
                    "sent": "So they were.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "5th in online learning on their log loss at each at time T we want to predict this valuable YT which is in red here and we give our prediction in form of a probability distribution.",
                    "label": 1
                },
                {
                    "sent": "So given the past history we want to see what's YT, what's the, what's the probability of YT and our losses?",
                    "label": 0
                },
                {
                    "sent": "The negative log of what we have predicted?",
                    "label": 1
                },
                {
                    "sent": "So and we can consult with a group of expert, which in our case are IID parametric experts.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to minimize the cumulative loss.",
                    "label": 0
                },
                {
                    "sent": "Cumulative.",
                    "label": 0
                },
                {
                    "sent": "Loss in comp.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Isn't it the best expert in class?",
                    "label": 0
                },
                {
                    "sent": "So this difference you can see.",
                    "label": 0
                },
                {
                    "sent": "So the 1st.",
                    "label": 0
                },
                {
                    "sent": "Is our cumulative loss, and the 2nd is the cumulative loss of the best expert.",
                    "label": 1
                },
                {
                    "sent": "In a sense we want to.",
                    "label": 0
                },
                {
                    "sent": "You're competing with maximum likelihood, as you can see.",
                    "label": 0
                },
                {
                    "sent": "So this is you have log of maximum likelihood of sequence Y N / R. Distribution because at each time you are giving a conditional distribution, if you multiply all these conditionals, we get a joint.",
                    "label": 0
                },
                {
                    "sent": "So this is very important to remember, so we have a joint distribution over N random variables.",
                    "label": 0
                },
                {
                    "sent": "So we look at that joint distribution and we look at the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the ratio we want to minimize this is the regret that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So any joint distribution corresponds to conditionals at each time.",
                    "label": 1
                },
                {
                    "sent": "By marginalization an any product of conditions is a joint distribution.",
                    "label": 1
                },
                {
                    "sent": "So for a given horizon N, we know that the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Best thing we can do.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the horizon is given to us and we want to come up with a sequential probability assignment that is minimax.",
                    "label": 0
                },
                {
                    "sent": "Regret is the least for the worst possible data.",
                    "label": 0
                },
                {
                    "sent": "And the best way to do it is.",
                    "label": 0
                },
                {
                    "sent": "Through an algorithm called MLE, normalized maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "So you say.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look at the entire sequence.",
                    "label": 0
                },
                {
                    "sent": "What's the the probability that I assigned to this sequence is proportional to its maximum likelihood?",
                    "label": 0
                },
                {
                    "sent": "And at the end you market value normalized to get their probability.",
                    "label": 0
                },
                {
                    "sent": "But the problem with normalized maximum likelihood is.",
                    "label": 0
                },
                {
                    "sent": "That it is defined in terms of a joint distribution and you have to normalize at each time T to get the conditions, so that's that's that's a huge problem.",
                    "label": 0
                },
                {
                    "sent": "If let's say you're doing binary prediction at time T, you have to do 2 two N -- T sums to marginalized, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a very big problem.",
                    "label": 0
                },
                {
                    "sent": "So instead of that, people looked at alternatives, one alternate.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "User base in a strategy you put the prior on your experts and you update your posterior and mix very easy and natural way too.",
                    "label": 0
                },
                {
                    "sent": "Do this problem and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's one way and another.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He is one step at look up sequential normals maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "Instead of looking at the entire sequence and marginalizing, we just look at one.",
                    "label": 0
                },
                {
                    "sent": "One random variable ahead.",
                    "label": 0
                },
                {
                    "sent": "You know you can't.",
                    "label": 0
                },
                {
                    "sent": "It's defined in red, so you look at the next variable and so think what happens if I observe Whitey and you concatenated with your history, look at the maximum likelihood and then normalize.",
                    "label": 1
                },
                {
                    "sent": "It's a very easy way, you know.",
                    "label": 0
                },
                {
                    "sent": "Both this and the previous one are naturally defined in terms of conditionals, which is really easy to compliment to implement.",
                    "label": 1
                },
                {
                    "sent": "So now you know previous.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was shown that these three algorithms, namely base.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asian under a very specific prior called Jeffreys prior SNL.",
                    "label": 0
                },
                {
                    "sent": "Want to look up an normalized maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "We have shown that they are all exactly the same or all different, meaning that if either if two of these are the same, there should be this same as the third one, so it's an equivalence class, so it's quite interesting if we have.",
                    "label": 0
                },
                {
                    "sent": "These are three all the same, or they're all different, so we have.",
                    "label": 0
                },
                {
                    "sent": "So the question.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is so when are these the same?",
                    "label": 0
                },
                {
                    "sent": "So if there are exchangeable, if this process is extreme.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Triple SML.",
                    "label": 0
                },
                {
                    "sent": "Then there are all the same.",
                    "label": 0
                },
                {
                    "sent": "But the question is when does this happen in?",
                    "label": 0
                },
                {
                    "sent": "For example, let's say exponential families.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shown that this phenomena can only happen in these three families.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Namely gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": ", and Tweedie distribution of order three 1.5 and any one to one transformation of these families come to our poster.",
                    "label": 0
                },
                {
                    "sent": "We have more details, thanks.",
                    "label": 0
                }
            ]
        }
    }
}