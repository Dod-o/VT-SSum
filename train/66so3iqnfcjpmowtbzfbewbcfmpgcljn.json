{
    "id": "66so3iqnfcjpmowtbzfbewbcfmpgcljn",
    "title": "Label Propagation for Fine-Grained Cross-Lingual Genre Classification",
    "info": {
        "author": [
            "Philipp Petrenz, School of Informatics, University of Edinburgh"
        ],
        "published": "Jan. 11, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Computational Linguistics",
            "Top->Computer Science->Multilingual Information Access"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_petrenz_genre_classification/",
    "segmentation": [
        [
            "Hi, my name is Phillip.",
            "It runs from the University of Edinburgh and I'm presenting our paper label propagation for fine grained cross lingual stratification which is joint work with Bonnie Weber."
        ],
        [
            "OK, so it's a paper with a long title with lots of big birds, but basically we're trying to classify text into Sean Rice, and the reason why we want to do this is at 8 can help users of information retrieval systems to filter search results by genre and be it can inform NLP applications such as text summarization, since the genre of a text dictates the structure and location of important information within the text."
        ],
        [
            "Trouble is, showing classification is typically a supervised machine learning task, and as such requires labeled training data which you might not have in the language that you're trying to look at.",
            "So one way to remedy this is to use cross lingual techniques where you can exploit the knowledge you might have in one language to help classify texts in your target language, and for the particular application of genre classification we've shown that this is possible even without the use of machine translation.",
            "However, all prior work has been carried out on very broad.",
            "Sean Rick categories to four classes only, and so in order to make this a bit more practical, we're trying to find a way to classify more fine grained journalism."
        ],
        [
            "And if you talk to people who work in genre classification and you ask him what genre is you get a lot of different answers.",
            "But one thing most people would agree is that they're defined along several high level dimensions such as reading level of the target audience, the medium, the communicated purpose.",
            "But I try to convince you what I'm trying to give you factual information, and some people say the topic is part of that definition.",
            "OK, and so that is also reflected in the types of text representations people use when they do this classification, people use lexical features.",
            "People use presentation features, structural features and so on, and So what we were trying to do is to find a method that lets us have several separate different sets of features and we combine them and we also needed a method that lets us exploit unlabeled text.",
            "The target language.",
            "And so we we found that label propagation works actually quite well for this task, since it's very easily extended to several feature space."
        ],
        [
            "And before I talk about this a bit more, let me show you what kind of features we're working with.",
            "We have three different types of text representations, and the first one is a cross lingual set that lets us bridge the gap between our source and target language, and these are features which are mostly simple statistics such as the type token ratio, the average word length, and so on, and refused them before, and they were quite well, and we've added 12.",
            "Frequencies of universal part of speech tags which were proposed by petrifying colleagues last year to the set and then we have two feature sets which we only use in the target language.",
            "Once we cross that gap and they are part of speech histograms which proposed by Feldman and all for monolingual classification in English, and they capture sort of structural properties of attacks and then we have a simple backwards.",
            "Feature set, which is a lexical feature set that captures both content and function words."
        ],
        [
            "OK, so with these three feature sets, what we have?",
            "Assume we have a few data points and the Red Bulls I can see represent our target language text and the blue icons here represent two different changes in our source language.",
            "Where we have labels and we want to build a graph where each node represents one text in either source or target language."
        ],
        [
            "The first thing we want to do is we pick a target language text.",
            "This one here and we compute a ranking based on the distances in the cross lingual feature space of all the source language text to the target language text and then."
        ],
        [
            "We use these rank positions from him into a Gaussian function and we get we get weights for the edges that we then draw."
        ],
        [
            "Between the corresponding notes.",
            "So the closer source language text is to that target language text.",
            "The higher the weight of the directed edge will be, that's representative of dark versus light arrows."
        ],
        [
            "OK, and the next step would be to do the exact same thing again, except now we do this for target language text among each other.",
            "So we use the same feature set, we compute distances and then we use a Gaussian function to assign weights to the."
        ],
        [
            "And then we go ahead and do the exact same thing again, but now for the part of speech histogram set and only for the target language text between each other."
        ],
        [
            "And we do the same thing for the third feature set we have, and then what we end up with is.",
            "Three separate crafts for our three feature spaces and bonecraft with source target links.",
            "We merge them all together so we end up with a graph where.",
            "Each two target language texts have several directed edges between them.",
            "And we also at a.",
            "An edge that feeds into itself for each target language text, so you can think of this as a craft with several layers that chair notes but not edge weights."
        ],
        [
            "An after we've done this on the actual propagation algorithm is is very straightforward and very similar to what I'm sure you've seen before, so each node is assigned a vector of labels of genre or probabilities, and these probabilities are then.",
            "These are then propagated through the edges, and we do this iteratively until our algorithm converges, and then we just picked for each.",
            "For each node, we picked the label with high probability in the."
        ],
        [
            "And it's a vector.",
            "OK to evaluate how this works in practice, we've we've conducted experiments with the Brown corpus in English.",
            "The stock homeyer corpus in Swedish and the Lancaster Corpus of Mandarin Chinese.",
            "We've identified 9 genre classes in these, and as you can see here.",
            "The distribution of classes is both cute and language dependent, so it's a bit different from language language.",
            "Which is still challenging, but realistic.",
            "I guess we've implemented three baselines.",
            "The first one is a SVM wrapper.",
            "Algorithm that we've proposed earlier this year for which was designed to classify broader genres.",
            "And the second one we've translated every text in our in our data set into English using Google Translate, and then use a monolingual classifier schoener classifier too.",
            "To classify our target text and the third one is the same algorithm that we use here.",
            "The label propagation, except instead of using these separate.",
            "Sets of features.",
            "We've combined them all, except for the cross lingual set to cross the language gap just to see how well our algorithm makes use of the separate feed."
        ],
        [
            "Classes by hand.",
            "So I mean the different datasets have different shorter classes, right?",
            "Different number of classes.",
            "They have very similar genre classes.",
            "The Swedish data set is a little bit different to the other ones, but very similar, and so there's there modeled after each other really, so there's no real hand alignment is.",
            "Yeah, you can just trivial alignment.",
            "That's not that one has 20 classes and the other one has 10 and the next thing you know the the Chinese and the English datasets have 15 classes.",
            "The Swedish actually has 12 classes and we've combined all subclasses of fiction.",
            "So we end up with nine classes.",
            "We've had to just throw away one glass in the English and.",
            "Chinese data sets.",
            "Which was production, which is sort of a funny type of genre description.",
            "Anyway, OK, now we've evaluated.",
            "Our algorithms using minifon scores since the.",
            "The class distribution is skewed.",
            "We thought this would probably be the best way to do this.",
            "We've also recorded accuracy, but this is mean if one score and.",
            "For English and Swedish, is the target language.",
            "We the green bar.",
            "Here is our algorithm.",
            "We outperformed the other.",
            "The other algorithms for Chinese best performing algorithms.",
            "The machine translation based systems and we think that this is probably because.",
            "The features that we used were designed for monolingual genre classification in English and therefore do not perform as well as.",
            "Machine translation here.",
            "We do see, however, that it outperforms the combined feature sets, which makes, which means that the algorithm does make use of having these features that separated, and we also see that it outperforms our performance after the first iteration of our iterative algorithm, which means we do gain from exploiting the knowledge in the unlabeled.",
            "Target text data."
        ],
        [
            "OK, to conclude that fine grained classification of genres across languages is possible and we use a separate feature sets here, and we exploit unlabeled target language data.",
            "It performs for some languages it performs better than a machine translation based technique, but not for all and.",
            "Our one approach for future work is to.",
            "To combine features that we get from using machine translation with this algorithm, since very flexible as to what kind of features you can use with it."
        ],
        [
            "OK, thank you, take questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, my name is Phillip.",
                    "label": 0
                },
                {
                    "sent": "It runs from the University of Edinburgh and I'm presenting our paper label propagation for fine grained cross lingual stratification which is joint work with Bonnie Weber.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it's a paper with a long title with lots of big birds, but basically we're trying to classify text into Sean Rice, and the reason why we want to do this is at 8 can help users of information retrieval systems to filter search results by genre and be it can inform NLP applications such as text summarization, since the genre of a text dictates the structure and location of important information within the text.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trouble is, showing classification is typically a supervised machine learning task, and as such requires labeled training data which you might not have in the language that you're trying to look at.",
                    "label": 0
                },
                {
                    "sent": "So one way to remedy this is to use cross lingual techniques where you can exploit the knowledge you might have in one language to help classify texts in your target language, and for the particular application of genre classification we've shown that this is possible even without the use of machine translation.",
                    "label": 1
                },
                {
                    "sent": "However, all prior work has been carried out on very broad.",
                    "label": 0
                },
                {
                    "sent": "Sean Rick categories to four classes only, and so in order to make this a bit more practical, we're trying to find a way to classify more fine grained journalism.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if you talk to people who work in genre classification and you ask him what genre is you get a lot of different answers.",
                    "label": 0
                },
                {
                    "sent": "But one thing most people would agree is that they're defined along several high level dimensions such as reading level of the target audience, the medium, the communicated purpose.",
                    "label": 1
                },
                {
                    "sent": "But I try to convince you what I'm trying to give you factual information, and some people say the topic is part of that definition.",
                    "label": 0
                },
                {
                    "sent": "OK, and so that is also reflected in the types of text representations people use when they do this classification, people use lexical features.",
                    "label": 0
                },
                {
                    "sent": "People use presentation features, structural features and so on, and So what we were trying to do is to find a method that lets us have several separate different sets of features and we combine them and we also needed a method that lets us exploit unlabeled text.",
                    "label": 0
                },
                {
                    "sent": "The target language.",
                    "label": 0
                },
                {
                    "sent": "And so we we found that label propagation works actually quite well for this task, since it's very easily extended to several feature space.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And before I talk about this a bit more, let me show you what kind of features we're working with.",
                    "label": 0
                },
                {
                    "sent": "We have three different types of text representations, and the first one is a cross lingual set that lets us bridge the gap between our source and target language, and these are features which are mostly simple statistics such as the type token ratio, the average word length, and so on, and refused them before, and they were quite well, and we've added 12.",
                    "label": 1
                },
                {
                    "sent": "Frequencies of universal part of speech tags which were proposed by petrifying colleagues last year to the set and then we have two feature sets which we only use in the target language.",
                    "label": 0
                },
                {
                    "sent": "Once we cross that gap and they are part of speech histograms which proposed by Feldman and all for monolingual classification in English, and they capture sort of structural properties of attacks and then we have a simple backwards.",
                    "label": 1
                },
                {
                    "sent": "Feature set, which is a lexical feature set that captures both content and function words.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so with these three feature sets, what we have?",
                    "label": 0
                },
                {
                    "sent": "Assume we have a few data points and the Red Bulls I can see represent our target language text and the blue icons here represent two different changes in our source language.",
                    "label": 0
                },
                {
                    "sent": "Where we have labels and we want to build a graph where each node represents one text in either source or target language.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first thing we want to do is we pick a target language text.",
                    "label": 0
                },
                {
                    "sent": "This one here and we compute a ranking based on the distances in the cross lingual feature space of all the source language text to the target language text and then.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use these rank positions from him into a Gaussian function and we get we get weights for the edges that we then draw.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between the corresponding notes.",
                    "label": 0
                },
                {
                    "sent": "So the closer source language text is to that target language text.",
                    "label": 0
                },
                {
                    "sent": "The higher the weight of the directed edge will be, that's representative of dark versus light arrows.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the next step would be to do the exact same thing again, except now we do this for target language text among each other.",
                    "label": 0
                },
                {
                    "sent": "So we use the same feature set, we compute distances and then we use a Gaussian function to assign weights to the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we go ahead and do the exact same thing again, but now for the part of speech histogram set and only for the target language text between each other.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we do the same thing for the third feature set we have, and then what we end up with is.",
                    "label": 0
                },
                {
                    "sent": "Three separate crafts for our three feature spaces and bonecraft with source target links.",
                    "label": 0
                },
                {
                    "sent": "We merge them all together so we end up with a graph where.",
                    "label": 0
                },
                {
                    "sent": "Each two target language texts have several directed edges between them.",
                    "label": 0
                },
                {
                    "sent": "And we also at a.",
                    "label": 0
                },
                {
                    "sent": "An edge that feeds into itself for each target language text, so you can think of this as a craft with several layers that chair notes but not edge weights.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An after we've done this on the actual propagation algorithm is is very straightforward and very similar to what I'm sure you've seen before, so each node is assigned a vector of labels of genre or probabilities, and these probabilities are then.",
                    "label": 0
                },
                {
                    "sent": "These are then propagated through the edges, and we do this iteratively until our algorithm converges, and then we just picked for each.",
                    "label": 0
                },
                {
                    "sent": "For each node, we picked the label with high probability in the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's a vector.",
                    "label": 0
                },
                {
                    "sent": "OK to evaluate how this works in practice, we've we've conducted experiments with the Brown corpus in English.",
                    "label": 0
                },
                {
                    "sent": "The stock homeyer corpus in Swedish and the Lancaster Corpus of Mandarin Chinese.",
                    "label": 0
                },
                {
                    "sent": "We've identified 9 genre classes in these, and as you can see here.",
                    "label": 1
                },
                {
                    "sent": "The distribution of classes is both cute and language dependent, so it's a bit different from language language.",
                    "label": 0
                },
                {
                    "sent": "Which is still challenging, but realistic.",
                    "label": 0
                },
                {
                    "sent": "I guess we've implemented three baselines.",
                    "label": 1
                },
                {
                    "sent": "The first one is a SVM wrapper.",
                    "label": 0
                },
                {
                    "sent": "Algorithm that we've proposed earlier this year for which was designed to classify broader genres.",
                    "label": 0
                },
                {
                    "sent": "And the second one we've translated every text in our in our data set into English using Google Translate, and then use a monolingual classifier schoener classifier too.",
                    "label": 0
                },
                {
                    "sent": "To classify our target text and the third one is the same algorithm that we use here.",
                    "label": 0
                },
                {
                    "sent": "The label propagation, except instead of using these separate.",
                    "label": 0
                },
                {
                    "sent": "Sets of features.",
                    "label": 0
                },
                {
                    "sent": "We've combined them all, except for the cross lingual set to cross the language gap just to see how well our algorithm makes use of the separate feed.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classes by hand.",
                    "label": 0
                },
                {
                    "sent": "So I mean the different datasets have different shorter classes, right?",
                    "label": 0
                },
                {
                    "sent": "Different number of classes.",
                    "label": 0
                },
                {
                    "sent": "They have very similar genre classes.",
                    "label": 0
                },
                {
                    "sent": "The Swedish data set is a little bit different to the other ones, but very similar, and so there's there modeled after each other really, so there's no real hand alignment is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can just trivial alignment.",
                    "label": 0
                },
                {
                    "sent": "That's not that one has 20 classes and the other one has 10 and the next thing you know the the Chinese and the English datasets have 15 classes.",
                    "label": 0
                },
                {
                    "sent": "The Swedish actually has 12 classes and we've combined all subclasses of fiction.",
                    "label": 0
                },
                {
                    "sent": "So we end up with nine classes.",
                    "label": 0
                },
                {
                    "sent": "We've had to just throw away one glass in the English and.",
                    "label": 0
                },
                {
                    "sent": "Chinese data sets.",
                    "label": 0
                },
                {
                    "sent": "Which was production, which is sort of a funny type of genre description.",
                    "label": 0
                },
                {
                    "sent": "Anyway, OK, now we've evaluated.",
                    "label": 0
                },
                {
                    "sent": "Our algorithms using minifon scores since the.",
                    "label": 0
                },
                {
                    "sent": "The class distribution is skewed.",
                    "label": 0
                },
                {
                    "sent": "We thought this would probably be the best way to do this.",
                    "label": 0
                },
                {
                    "sent": "We've also recorded accuracy, but this is mean if one score and.",
                    "label": 0
                },
                {
                    "sent": "For English and Swedish, is the target language.",
                    "label": 1
                },
                {
                    "sent": "We the green bar.",
                    "label": 0
                },
                {
                    "sent": "Here is our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We outperformed the other.",
                    "label": 0
                },
                {
                    "sent": "The other algorithms for Chinese best performing algorithms.",
                    "label": 1
                },
                {
                    "sent": "The machine translation based systems and we think that this is probably because.",
                    "label": 0
                },
                {
                    "sent": "The features that we used were designed for monolingual genre classification in English and therefore do not perform as well as.",
                    "label": 0
                },
                {
                    "sent": "Machine translation here.",
                    "label": 0
                },
                {
                    "sent": "We do see, however, that it outperforms the combined feature sets, which makes, which means that the algorithm does make use of having these features that separated, and we also see that it outperforms our performance after the first iteration of our iterative algorithm, which means we do gain from exploiting the knowledge in the unlabeled.",
                    "label": 0
                },
                {
                    "sent": "Target text data.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, to conclude that fine grained classification of genres across languages is possible and we use a separate feature sets here, and we exploit unlabeled target language data.",
                    "label": 1
                },
                {
                    "sent": "It performs for some languages it performs better than a machine translation based technique, but not for all and.",
                    "label": 1
                },
                {
                    "sent": "Our one approach for future work is to.",
                    "label": 0
                },
                {
                    "sent": "To combine features that we get from using machine translation with this algorithm, since very flexible as to what kind of features you can use with it.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you, take questions.",
                    "label": 0
                }
            ]
        }
    }
}