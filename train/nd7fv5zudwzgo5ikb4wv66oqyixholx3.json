{
    "id": "nd7fv5zudwzgo5ikb4wv66oqyixholx3",
    "title": "A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation",
    "info": {
        "author": [
            "Aaron Defazio, College of Engineering and Computer Science, Australian National University"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods->Convex Optimization",
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/machine_defazio_scale_free_networks/",
    "segmentation": [
        [
            "Hi everybody."
        ],
        [
            "So the problem we looked at was structural learning for gourds, mark of random fields, letting encourages scale free structures.",
            "So for those of you unaware of what scale free means, it's just a property of networks.",
            "Where they have a heavy tailed degree distribution, this can often be visualized, such as the case on the right where you have kind of a hub and spoke structure compared to independently sampled edges, which is what you see on the left.",
            "This guys mark of random field structure learning problem is also sometimes known as sparse inverse covariance selection.",
            "There's a few papers on it at NIPS this year, but we consider the case where you have prior knowledge that the structure is scale free."
        ],
        [
            "So the way we go about this is we formulate a.",
            "Set function on the edge set of the graph that gives low weight to scale free networks.",
            "We then show that this set function is submodular and that it can be relaxed to a convex, although non differentiable regularizer.",
            "The structure of that regularizer is shown on the right.",
            "There it looks fairly straightforward with the only complexity being that the waiting of the entries of your edge matrix X depends on the ordering it's been cut off in the slides here.",
            "Unfortunately, on the edge there but the waiting there depends on the rank.",
            "Order of the elements.",
            "Rather than just their ordering, and that's where their complexity comes from.",
            "And it turns out using this regularizer you can solve the current selection problem in a similar way to people at how people have done it before using the alternating direction method of multipliers, but with a different proximal operator approximate operator for this regularizer, and it turns out that this proximal operator has quite interesting structure.",
            "It isn't a simple closed form expression for evaluating it, you've got to use the separate optimization method, but Fortunately we're able to define a very simple dual decomposition method.",
            "That shows linear convergence for avoiding this proximal operator on the right, you can see a graph comparing it to two generic ways of solving the proximal operator, and you can see how our method this is on a log scale is substantially faster to convergence.",
            "So just to kind of visualization of what this method."
        ],
        [
            "As we run it on a gene network reconstruction task on the left, you can see the output of a small subset of this network using our method on the right, just using the standard sparse inverse covariance selection within one regularizer method, and you can see more clearly a hub and spoke structure in our reconstruction, as well as a clustering at the right, which is indicative of probably some sort of hidden factor which is influencing those jeans.",
            "Our poster is tomorrow W 47.",
            "Thanks for your time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everybody.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem we looked at was structural learning for gourds, mark of random fields, letting encourages scale free structures.",
                    "label": 1
                },
                {
                    "sent": "So for those of you unaware of what scale free means, it's just a property of networks.",
                    "label": 0
                },
                {
                    "sent": "Where they have a heavy tailed degree distribution, this can often be visualized, such as the case on the right where you have kind of a hub and spoke structure compared to independently sampled edges, which is what you see on the left.",
                    "label": 1
                },
                {
                    "sent": "This guys mark of random field structure learning problem is also sometimes known as sparse inverse covariance selection.",
                    "label": 0
                },
                {
                    "sent": "There's a few papers on it at NIPS this year, but we consider the case where you have prior knowledge that the structure is scale free.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way we go about this is we formulate a.",
                    "label": 1
                },
                {
                    "sent": "Set function on the edge set of the graph that gives low weight to scale free networks.",
                    "label": 1
                },
                {
                    "sent": "We then show that this set function is submodular and that it can be relaxed to a convex, although non differentiable regularizer.",
                    "label": 1
                },
                {
                    "sent": "The structure of that regularizer is shown on the right.",
                    "label": 1
                },
                {
                    "sent": "There it looks fairly straightforward with the only complexity being that the waiting of the entries of your edge matrix X depends on the ordering it's been cut off in the slides here.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, on the edge there but the waiting there depends on the rank.",
                    "label": 0
                },
                {
                    "sent": "Order of the elements.",
                    "label": 0
                },
                {
                    "sent": "Rather than just their ordering, and that's where their complexity comes from.",
                    "label": 0
                },
                {
                    "sent": "And it turns out using this regularizer you can solve the current selection problem in a similar way to people at how people have done it before using the alternating direction method of multipliers, but with a different proximal operator approximate operator for this regularizer, and it turns out that this proximal operator has quite interesting structure.",
                    "label": 0
                },
                {
                    "sent": "It isn't a simple closed form expression for evaluating it, you've got to use the separate optimization method, but Fortunately we're able to define a very simple dual decomposition method.",
                    "label": 0
                },
                {
                    "sent": "That shows linear convergence for avoiding this proximal operator on the right, you can see a graph comparing it to two generic ways of solving the proximal operator, and you can see how our method this is on a log scale is substantially faster to convergence.",
                    "label": 0
                },
                {
                    "sent": "So just to kind of visualization of what this method.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we run it on a gene network reconstruction task on the left, you can see the output of a small subset of this network using our method on the right, just using the standard sparse inverse covariance selection within one regularizer method, and you can see more clearly a hub and spoke structure in our reconstruction, as well as a clustering at the right, which is indicative of probably some sort of hidden factor which is influencing those jeans.",
                    "label": 0
                },
                {
                    "sent": "Our poster is tomorrow W 47.",
                    "label": 1
                },
                {
                    "sent": "Thanks for your time.",
                    "label": 0
                }
            ]
        }
    }
}