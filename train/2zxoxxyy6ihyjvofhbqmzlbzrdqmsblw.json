{
    "id": "2zxoxxyy6ihyjvofhbqmzlbzrdqmsblw",
    "title": "Undirected Graphical Models",
    "info": {
        "author": [
            "Aaron Courville, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_courville_graphical_models/",
    "segmentation": [
        [
            "Hi right so I guess I haven't even put my name in my name is Aaron.",
            "Please do ask questions.",
            "I think it will be more fun for both of us.",
            "I understand that you that's a pretty good group and that you've been asking lots of questions.",
            "That's fantastic, so I'm going to talk about.",
            "Mainly I was scheduled to talk about undirected graphical models as an introduction to undirected graphical models, but I understand you haven't yet seen much about directed graphical models, so sometimes I'm going to refer back and compare to directed graphical models and give you a little bit of background on what directed graphical models look like.",
            "Just to give you a bit of context, so I'm just curious in terms of your background who has worked with undirected graphical models will start there before?",
            "OK, so some number like you're familiar with our BMS.",
            "That's probably the main thing that people have worked on show of hands for our BMS 'cause that's mainly the topic of this talk.",
            "OK good so a few people have but probably not the majority of you.",
            "That's great."
        ],
        [
            "OK, so this is my overview, right?",
            "So largely on largely.",
            "Frankly, I'll be spending most of my time talking bout restricted Boltzmann machine right here, but on the way up we're going to talk about some of the differences between undirected and directed graphical models.",
            "We're going to talk about a central idea in graphical models, which is conditional independence, and we're going to do a brief probability review of 1 slide on that and then talk about how that relates to the graphical models will be discussing.",
            "We're going to talk about energy function formalism or just.",
            "Energy based models.",
            "And then we're going to talk about maximum likelihood learning in the general case for these energy based models.",
            "And then we're going to go into restricted Boltzmann machine as an example of one of these kinds of models.",
            "And then, if time permits, an I'm not sure it well.",
            "I'm going to talk about work that I've contributed in this area, which is the spike and slab RBM.",
            "Just a few slides on that, just to give you an idea of flavor of how you can extend this formalism in interesting ways."
        ],
        [
            "Alright, so let's get started.",
            "So probabilistic graphical models.",
            "What is it?",
            "Well, really, it's it's the marriage of probability theory and graph theory.",
            "Here we have our probabilistic graphical models represented as a graph.",
            "So nodes represent random variables.",
            "That's our our probability theory here in the edges encode conditional independences, or rather to be more accurate, it's the absence of edges that encode conditional independencies.",
            "Right, so the graph structure is essentially what gives you this set of conditional independencies and the nodes themselves are the random variables, and you can then perform inference with the probabilistic model associated with the graph in the graph can actually help you decide how you're going to perform inference.",
            "It will tell you, for example, if inference in this graph is going to be something that you can do tractably or efficiently or not, and will get into a little bit of that."
        ],
        [
            "So right so the as I mentioned, there are two basic flavors of graphical models directed graphical models, an undirected graphical models.",
            "The way these are also called base directed graph models are also called Bayes Nets or belief networks, and the consists of a set of nodes and directed edges.",
            "So basically little arrows between those nodes, and the arrows encode factorized conditional probability distributions right?",
            "So these are the little or CPD.",
            "These are the little probability models that you combine into a big set of joint probabilities using this graph, undirected graphical models also can set a set of all of nodes, but these edges now are undirected, and they're a bit more general in their representation.",
            "The edges here in code conditional independence, but not necessarily as a conditional probability distribution.",
            "The big difference there was what will get into is that in the case of an undirected model, these these local potentials.",
            "Don't need to be normalized, you instead pull out the normalizer and it's not going to be global normalization, and that's going to give us some advantages, but also some disadvantages, and we're going to see how we can deal with those.",
            "And as I say today, we're going to focus almost exclusively on undirected graphical models, and I'll just refer to directed graphical models in passing.",
            "Just to give you a bit of context for the differences."
        ],
        [
            "So first a little probability review.",
            "So just basically conditional independence.",
            "So first, what is what is probabilistic independence?",
            "What we have if we have a joint probability between of variables X&Y, they are independent.",
            "If the joint probability is equal to the product of their marginal probabilities, right P of X * P of Y.",
            "So that's simple, marginal independence or just independence.",
            "Conditional independence is a little bit more complicated and it's sort of it's represented right here.",
            "So we say that.",
            "X is conditionally independent of Y given zed, so this is like hence conditional independence.",
            "If if we have the following statement, if if P of X the joint probability of X&Y given Z is just equal to P of X given zed and PFY given said so, it's a natural extension of this idea of marginal independence, right?",
            "And by extension, we also have that if X is marginally is conditioning dependent of why then we have this following statement is true that P of X given Y&Z is just equal to P of X.",
            "Given zed, this falls naturally an intuitive way to see this is you're just saying that why provides no new information if it's actually independent of of of X conditioning on it gives you no new information about X, right?",
            "Once you have said you get no new information about X.",
            "So an example for example of something like this would be.",
            "Let's say you hear Thunder.",
            "And you want to know if the grass is wet, you know, and the conditioning variable is you've looked outside and it's raining, right?",
            "So once you know it's raining, you don't.",
            "It doesn't matter if you've heard Thunder or not, right?",
            "You know that it's it's raining.",
            "Or the grass is wet anyway.",
            "Right, so or here's a maybe a better example here, so that Thunder, given rain and lightning is just equal to the probability of Thunder given lightning, right?",
            "You don't actually need to know that it's raining once you.",
            "Once you have lightning because there's a.",
            "In this case, there's a tight causal connection between lightning and Thunder."
        ],
        [
            "Right, so let's let's think about these.",
            "So what I've said is that these graphs are there to essentially represent conditional independence, and that's basically what these graphs are going to do for us at a large scale.",
            "And the way to think about that is this is if we think of this Gray box.",
            "Here is the set of all probabilistic models.",
            "Then we can represent.",
            "We can think of it as in the all possible.",
            "Set of conditional dependencies that can exist in the world.",
            "And then we can think of the set of models we can represent with graphical models as being this large group.",
            "Here the set we can we can represent with directed models.",
            "You can think of this group here and the set we can represent with undirected models there.",
            "Yes.",
            "Which cannot be represented with graphical models, yes.",
            "You mean one that's right here?",
            "Yeah, I don't think I can give you an example of that.",
            "Actually, right now I'll think about it.",
            "Maybe we'll come back.",
            "Yeah, see my worldview.",
            "Everything can be represented as a graphical model, but I know for a fact that it actually isn't true, but these are usually.",
            "These are pretty borderline cases like you usually you're playing with things like the you know you have measure 0 or little zero probabilities part of your space that you can't actually represent things like this.",
            "So usually these kinds of things that they exist, but they aren't usually things that we were too concerned with in practice."
        ],
        [
            "Right, so now we're talking talk about conditional independence, and specifically, we're talking about undirected graphical models and their relationship to conditional independence.",
            "So here we have four variables, and I've written here 2 conditional independent statements.",
            "Here we have a is independent of C given B&D, and here we have B is independent of D given A&C.",
            "Alright, and now the point of this slide is basically just to say it's hard to think about some forms of conditional in dependencies using directed graphical models.",
            "So here are two attempts to do just that to try to model this set of conditional independencies without introducing any new variables which won't actually help in this case anyway.",
            "And this so if you were just to think.",
            "Well, maybe this model represents that.",
            "In fact, actually, I mean this is something you might not know about if you haven't studied directed graphical models before, but this models there's a few things going on here that that if you've heard about D separation, this is actually going to tell you how to get from this graph into this set of conditional independent statements, but very briefly, what I can say is that given B&DA, the paths are blocked from A to C here, so we know that, given B&DA is independent of C, that's what the separation means.",
            "Essentially, you're blocking the path.",
            "But in directed graphical models, it gets a little bit more tricky than that, because in this case that's why we have, for example, this relationship.",
            "So we've got one of the two here, right here.",
            "We've got this one, but now the problem is, what about the next one?",
            "And this is where directed graphical models get tricky.",
            "It's because if you condition what you want is the statement that if I condition on A&CB&D become independent right, whereas directed graphical model, you can't actually do that.",
            "And the reason is is because you get this explaining away effect with C. This is if you've ever heard of explaining away.",
            "This is what we're talking about.",
            "What that means is that if I observe C, which is, which is the child of two parent nodes in this case, then those parent nodes become coupled.",
            "They become non independent, whereas if I just don't observe C then they remain potentially independent if all other.",
            "If they're independent through all other paths.",
            "So in this case I can render B&D independent by just conditioning on a 'cause this is kind of one of these normal relationships where if I block the path from B to D through this.",
            "Pathway, then they are independent.",
            "In this case, it's like the opposite.",
            "When you've got this this Arrowhead to Arrowhead connection through a variable or by the way, any of its children.",
            "If you condition on any of those variables, it actually opens that pathway.",
            "So here we have a context where where we can't condition on C, so using that graph structure we can't actually get the model with a set of conditional independencies.",
            "We're looking for, which is this set up here.",
            "Alright, so here's another attempt.",
            "So this is a C&BD here, so we have B&D.",
            "Here is the children of A&C.",
            "Now here we have the statement that again we now can get this relationship right because now see is in A is in this parent relationship like a was over here.",
            "So now we can basically get this statement that we were missing before that conditioned on A&CB is independent of D. We've got that now.",
            "But now the problem is now both of these two are in this relationship.",
            "This child, this explaining away effect with A&B.",
            "So if I were to condition on either one of them or both of them, these two become non conditionally independent.",
            "So what I have to do is I have to basically not condition on either one of them and that gives me this conditional independence statement right here.",
            "So these are the conditional in dependencies that this graph represent.",
            "You can see this is not the same as this, so does anyone want to try to another directed graphical model?",
            "Maybe they have something in mind that would actually represent this?",
            "OK, anybody who maybe have some have seen undirected graphical models want to try for undirected graphical models.",
            "This one without edges exactly.",
            "Yeah, this is the undirected graphical model that gives you that structure will see in a minute why, but basically it in a nutshell it's because this comp it's it's.",
            "It's a very similar sort of dynamic with the conditional independence ease.",
            "It's just this.",
            "Explaining away effect doesn't exist basically so as soon as I condition on A&C, I've blocked all paths from B to D will go over this in more detail, but that's basically the structure here.",
            "Is there any questions up to this point?",
            "OK. Yeah.",
            "No, I wouldn't necessarily say that so.",
            "And it's it's not.",
            "It's a question of OK, so more independence.",
            "Do you mean like more sets of in dependencies?",
            "So I'll give you an example of this is kind of the Canonical example of a graphical model that can be represented in an undirected model that can't be represented in a directed model.",
            "The example of the opposite.",
            "Is take this model and just remove a right this exact explaining away effect B&D are independent.",
            "And that's and then you have this relationship where you're right.",
            "So B&D are marginally independent, but C is dependent on weight.",
            "Sorry.",
            "Given see, they're not independent, right right?",
            "You can't represent that that structure with a directed with an undirected graphical model, so they're complementary, right?",
            "That's why when we go back to our circle here, this is sort of what we were really talking about.",
            "Is this space right here?",
            "The kinds of structures that exist here.",
            "And that last thing was an example of the kind of model that represents here, right?",
            "They can't be represented.",
            "This is sort of.",
            "If you like the space of conditional Independencies, and that can be represented the directed model, and one thing you could ask is what is in between here.",
            "We'll get to that in a few slides."
        ],
        [
            "Where I'm sorry where are you?",
            "Yes, absolutely you can actually.",
            "In fact, these things are our innocence common now and they were.",
            "They were really developed in some sense, or at least made more common in the deep learning literature actually.",
            "So you can think of a actually hung like who's going to be speaking after me is is going to talk about stacking RBM's and one way to interpret that.",
            "The structure you get out of that is has exactly one of these hybrid models right where because the bottom layers are going to be directed in the top layer is going to be undirected.",
            "So they're not very common outside of the deep learning literature, but they're actually quite common in the deep learning literature, yeah?",
            "Very good, very good question.",
            "So so we're going to get into some of that sort of in due course, but basically they have very different properties in almost every way you can look at it.",
            "So one example is actually this right here, right?",
            "So so I'll just explain this and then we'll come back and finish off your question, 'cause there's actually a few elements to it that will go over, but it's good to discuss right now.",
            "So.",
            "So here's a more.",
            "Maybe this is."
        ],
        [
            "So if we think of this as a bit of a toy example that we wouldn't likely see and in."
        ],
        [
            "Life may be something that we we care about.",
            "Here's an example that's a little bit more important, so here we have a 2D lattice structure, and these are pretty common when trying to model things like images, right?",
            "This is like the two D MRF that used to see quite frequently in the vision community.",
            "And this is kind of a directed model version of the same model, right?",
            "So here's an example where.",
            "The relationship between pixels.",
            "We would like that to be relatively symmetric, right?",
            "There's a few examples of models that do that explicitly.",
            "Do not have that relationship like actually made what you thought you heard about yesterday is an example of that, but for the most part, models like this tend to be preferred for modeling the interaction between things like pixels, because there the notion that one pixel is the parent of another can be awkward, right?",
            "And we'll see in a bit more detail what we can mean by that by it's awkward, but here's a real life example, so this, by the way, is just.",
            "An image I picked off where the task here is image segmentation, so you're trying to give each pixel a label.",
            "Here we have so labels for bicycle and labels for person here that are that are labeled.",
            "And the goal here is to basically, yeah, just just label as much of the image as you can, or perhaps some subset of classes.",
            "And this structure for that task, the structure seems to make much more sense from the point of view of just being able to model our intuitions of two, how pixels should relate to pixels.",
            "Now the rest of your question how when other like what are the other advantages, disadvantages of directed versus undirected?",
            "So there's two other things that you really want to worry about when you're doing anything with graphical models.",
            "It's is it easy in our context anyways, is it easy to do inference in this model?",
            "Presumably you're going to have latent variables or things you don't observe.",
            "That's most of the point of doing graphical models is that you've got some variables you observe and some variables you don't observe, and then you want to know how to do the variables you observe once you've observed them.",
            "Change your distribution about the variables you haven't seen yet, right?",
            "That's called and that question.",
            "You're asking the computation associated with that we call inference.",
            "And that is very different between the two different models.",
            "That process of inference turns out to be very different, so depending on the structure.",
            "Usually you'd go with one or the other, and often it's much more complicated in an undirected.",
            "Sorry in a directed graphical model than an undirected graphical model, for reasons we'll get into now.",
            "The flip side of that is learning learning turns out to be, and this is another thing will get into in the course of today.",
            "Learning turns out to be much more complicated in the context of undirected graphical models undirected graphical models.",
            "This is at least in the case where you've observed all the variables.",
            "In that case in the directed graphical model, you actually have.",
            "This fortunate thing happened where everything decomposes and you've just got.",
            "You've just got to estimate the parameters of your little conditional probabilities, right?",
            "So in the case of discrete variables, you're just filling in values for tables.",
            "Very simple because they normalize right?",
            "They do this local normalization that I mentioned.",
            "Undirected graphical models don't do that local normalization, and so you've got this thing that we've got this problem that we're going to talk about that comes up, so learning generally harder and undirected graphical models inference generally easier in undirected graphical models would be the first order of proximation."
        ],
        [
            "OK.",
            "Right, so so how can we formalize this idea of conditional independence that we talked about a bit in terms of undirected graphical models?",
            "So we have three sets.",
            "In this case, we're going to do three sets of nodes here, so we've just taken like a little piece of R2D lattice.",
            "Here we're going to set A and set B.",
            "We have the following property that we say that a is independent of B given C if C separates A&B in the graph.",
            "So here we have our graph structure and we've sort of alluded to this already, but here we have a bit more of a.",
            "Formal notion of that.",
            "So if I observe if I condition on B or the all the elements in C that is X13 here and X2 three, this is another way I observe those variables.",
            "And I have this graph structure that means that these variables over here are conditionally independent of these, right?",
            "And all I've done is just block the path.",
            "Through them, right?",
            "So that's basically the notion here is that if our conditioning set blocks all paths between two variables between all sets of variables here an all sets of variables here, then we can say that these two sets are conditionally independent under this graph.",
            "Other questions about that it's a very important notion.",
            "It's something that's basically the core of undirected graphical models.",
            "From at least you know in terms of inference and writing things down.",
            "OK, everybody's got that good."
        ],
        [
            "Alright, so now if we sort of take that same basic idea and we extend it to our larger model, we can develop things like an idea of a Markov blanket.",
            "So with the Markov blanket is it's basically the smallest set of units.",
            "Say we take a particular unit like X2 three.",
            "Here it's Markov blanket is the minimal set of nodes that we have to condition on set of variables.",
            "We have to condition in order to render that unit independent of all other units.",
            "So we sort of put down this blanket and now all of a sudden this guy doesn't care about what the values of all of these are.",
            "Because we've observed all its neighbors.",
            "That's how when we think about this, the way this guy interacts with the world around it is through these four variable.",
            "That's it.",
            "So once you fix those values, it doesn't.",
            "Knowing anything about any of these other variables is irrelevant to that to that variable.",
            "To this variable right here.",
            "OK, that idea is called the Markov blanket."
        ],
        [
            "So if we take that same basic structure and we ask well, what is it?",
            "The Markov blanket for the case of that 2D causal structure, though they directed graphical model that we saw, we see something a little bit different here, right?",
            "So here's our X3.",
            "If we look at what the Markov blanket is of this in the directed graphical model sense, there's a rule you have to apply, which is the things that are in in for directed graphical model.",
            "The things that are in your Markov blanket are your parents, your children and the other parents.",
            "Of your children, this is again because of this conditioning of this, this explaining away effect, right?",
            "Once you can, sorry.",
            "It's the no no.",
            "It's the other parents of your children, but in this case it happens to be both.",
            "But the one you need to condition on is in general the rule to apply is other parents of your children.",
            "It's good, though I'm glad you so.",
            "So now I guess so anyway, the point here is that.",
            "This Markov blanket seems to me if we care about things like pixels makes a whole lot more sense than this one, right?",
            "'cause we were sort of arbitrarily picking these two corners to condition on is something important to tell us about what this guy is doing right when you think about pixels, this feels very asymmetric, like if these guys are important, why not these for example.",
            "So as I'm just reinforcing this idea that sometimes undirected graphical models makes sense for whatever the context of whatever you're modeling.",
            "For example, in this case pixels another case where this all becomes important.",
            "And will motivate PBM's later.",
            "Is another example.",
            "Which I will draw now.",
            "Right, so so this is an example of images.",
            "We're not going to see a whole lot of that, but we will see models that look a bit like this.",
            "So let's imagine I have two sets of random variables.",
            "OK.",
            "I'm just drawing.",
            "Two sets of random variables and I have this kind of relationship.",
            "And then again.",
            "Yeah, that's right.",
            "All right, right?",
            "That's model number that's directed model right directed graphical model.",
            "By the way, another way we often call these kinds of models is dag's directed acyclic graphs, and the reason for that is because if you're going to start drawing arrows on these things, you can't draw a directed cycle.",
            "Your model can have cycles.",
            "This model, for example, has lots of cycles, right?",
            "You can go from here to here to here, to here, to back to here, right in an undirected cycle.",
            "If you have a directed cycle, that means you're following a loop.",
            "You in the direction of all the arrows.",
            "Then you have something that's basically nonsensical ale depend on BBL.",
            "Depend on on CNC bilderback depend on A and then in a causal cycle let's say so that you typically don't have in a case where this might make sense.",
            "Or you might feel an intuition about doing that is often in sequential modeling, but really there you've sort of broken things across time, so it's like the next day will depend on the previous a affectively, that kind of thing.",
            "So this is our.",
            "Dag directed model.",
            "And now here we have our undirected graphical model, and it's going to be basically exactly the same thing.",
            "Accept.",
            "Undirected.",
            "So.",
            "Right now, now this is a kind of model.",
            "We see all the time.",
            "In fact, this is we will see effectively in GBM this model right here is also something we see often.",
            "Machine learning, right?",
            "So sparse coding can be interpreted as this kind of graphical model.",
            "Or even there's the generative model for principle components analysis can be interpreted with this kind of graphical model.",
            "Actually, that one actually can also be interpreted as that kind of graphical model, but that's a special case.",
            "But the yeah, so we have.",
            "We have these two graphical models that appear often literature, very different properties, and it's just about their nature.",
            "The fact that this is undirected and that's directed.",
            "OK, So what are the differences?",
            "Let's imagine these are called latent variable models.",
            "Because we have like these things which you can think about is latent causes.",
            "These ads are saying things we don't observe in the world.",
            "X is are things we observe in the world, right?",
            "And so now we're going to play this game.",
            "Let's imagine we observe X in both cases and see what kind of conditional independencies we have in the Zeds.",
            "Alright, so so who can tell me?",
            "First over here, let's condition on all the X is and can somebody tell me what happens with the zeds?",
            "They are independent.",
            "There are conditionally independent why.",
            "Right, right?",
            "Because because these guys are not actually directly connected to each other, but they are connected to each other through the X.",
            "Is we condition on the X is and now all paths between these ads are blocked.",
            "So we have all pairs of of Zeds.",
            "Here are conditionally independent good.",
            "OK, that's generally a property we want.",
            "In that case, right?",
            "Because what that means is now we can actually because they don't interact.",
            "Inference is going to be easy because we don't care what the values of the rest of these are.",
            "When we're computing the value of this one, this is turned out to be very, very important to us.",
            "When we were talking about doing inference in the context of a model PBM, this model, for example, what happens I observe all the X is.",
            "They are all dependent, right?",
            "Because of that you got this massive explaining away effect going on right?",
            "All of these guys can be interpreted as causes for any one of these.",
            "So as soon as you condition on something, they're all linked, basically implicitly.",
            "So Interestingly, if you don't observe this, they're all independent.",
            "That is not true over there, right?",
            "So they have this different kind of relationship, a consequence of that, by the way, is that this model is far simpler to sample from, because all you have to do is first.",
            "Usually you'd write this down because of the you have these CPD relationships.",
            "You would write this down as P, Zedd, where the presumably independent for all these P of X given zed.",
            "Right now I can just use this way of describing the model as a way to sample from it as well.",
            "I sample independently from all those ads 1st and then conditional those values.",
            "I sampled the X.",
            "Very simple, there's no interactions.",
            "You can sample given all of these guys.",
            "These guys are all independent.",
            "You can sample from them easilly.",
            "We do not have that kind of relationship with our undirected graphical model.",
            "So these are two models that are very popular in the machine learning community.",
            "There only difference basically in terms of the apology is there is whether they are directed or undirected and you end up having very different kinds of consequences for that different behaviors.",
            "And we'll see in the context of the PBM, how we get around some of these issues that pop up, especially when we talk about doing sampling from this model, and also how we talk about learning in that model alright.",
            "Oh absolutely, I mean, yeah, I've drawn the Markov blanket for that graph.",
            "If I decide to include sort of diagonal elements as my neighbors, and if I draw that graph, then yeah, they would have to be in the Markov blanket.",
            "Yeah, absolutely.",
            "It's the oh so the Markov blanket.",
            "It's just that's a definition of term, so one of the things I should clear up right now.",
            "There's gonna be a few.",
            "There's this.",
            "This talk is a little terminology heavy, so one of the one of these terminologies we're going to use this notion of the Markov blanket, and we're not going to use it very much, so it's if you don't happen to know all the details of it.",
            "That's fine, but it is defined as the minimal set of units, so just so that it's unique 'cause otherwise.",
            "It's like all of the units, it's all subsets.",
            "Up until those four L Subs also all sets that include those four units and not X2 three would be defined as the Markov like it.",
            "So that's not very useful definition, yeah?",
            "Sure, yeah.",
            "In that kind of in a graph structure where you have so.",
            "So I don't usually think about the topology too much when I'm thinking about which kind of graph structure I want to use.",
            "It's more like I think it makes more sense to think about the nature of the problem, like it just happens that if you end up having a problem that it's kind of causal, relationships between things usually directed arcs make more sense.",
            "You'll have an easier time modeling the process because you're going to be able to follow your intuition of what causes what.",
            "Right whereas in the case, for example like pixels, we have no notion that pixel you know a pixel to the left has caused the pixel to the right right?",
            "So modeling them as just being correlated to each other and being sort of agnostic as to the causal relationship makes more sense there.",
            "So so in terms of the actual topology you describe, I can't really say, at least I mean.",
            "Yeah, I can't really say until you unless you tell me.",
            "For example, like some other elements of the problem.",
            "Like is it really kind of a difficult learning problem or is it a difficult inference problem?",
            "These things might help me decide which one I want to use.",
            "Sorry, which one would be harder.",
            "OK.",
            "Right?",
            "What what is I missed?",
            "The very first thing you said in the sense of what is the topic?",
            "That what are you trying to model?",
            "I know that apology, but what was what is the subject of what you're trying to model?",
            "Say.",
            "Rumors are OK. Ha.",
            "Yeah.",
            "Well, I mean it depends, right?",
            "If so, the way I would think about it is do you want to model rumors as being one directional or sort of associative like two people exchange rumors, or one person really sort of propagating rumors to another person, right?",
            "That would to me whichever one I want to use.",
            "There would be what determines which graph, which type of graphical makes more sense?",
            "So I mean, you can always, then the question would be like.",
            "Well, maybe one makes more sense, but it's really hard to work with, so I'll use the other.",
            "I mean that can always be an option.",
            "That's kind of you.",
            "Always make approximations when you do modeling, so so that could be something you would end up doing all right."
        ],
        [
            "Let's let's move on OK, yeah, so we've talked about a lot about conditional.",
            "Independence, we've talked about a lot about the graph structure.",
            "Now let's start populating these graphs with parameters right?",
            "Let's start connecting these conditional independence properties two probability distributions.",
            "So in the context of directed graphical models, I've already mentioned that the way we parameterized these things As for a given given basically given unit here.",
            "Oh, actually this is this is wrong.",
            "This is PAB given a is actually what this corresponds to here, But yeah, that would be the conditional density that would be associated with this graph.",
            "So that's again PFB given a.",
            "In general, what we have is we can write down any joint probability distribution is equal to the product over the individual elements condition on their parents, right?",
            "This is a way we can factorize graphs according to a directed graphical model."
        ],
        [
            "In the context of undirected graphical models, an now is a good time for me to mention undirected graphical models, Markov networks, Markov, random fields are all basically referring to the same thing.",
            "The consequences of having these different names is basically just because they've sort of emerged independent communities.",
            "Right, so the way we parameterize these models is with these things called factors, and these factors you can think of it as a generalization of these.",
            "Conditional probability distributions.",
            "So so this for example.",
            "In this case, we're basically modeling it with this with this factor here, and we can refer to this as this combination as a clique.",
            "A clique is just a set of units that are complete under the graph, so any subset of units that who form a complete graph, meaning all of these elements of the subset are connected to each other.",
            "That's what forms a clique.",
            "So right, so let's just so we can kind of formalize this and say that.",
            "Let's see be any set of ABA Capital C is a set of cliques.",
            "Then for a subscription for a given clique, this is just going to be an element for click.",
            "This is important notation 'cause we're going to carry this through a little bit.",
            "We define a factor also called a potential function or clique potential, and we're going to refer to this fee.",
            "Sub C Here as a non negative function which Maps basically this set of units.",
            "Now here X is just a set of variables that are associated with that clique.",
            "Slight abuse of notation here, but it's much simpler to think of to write things down this way.",
            "So this right here is going to be some set of factors which are associated with a given click, right?",
            "And so a unit.",
            "For example, if I had another another edge here and then a C down here a variable CB would be involved in two clicks, right?",
            "So this is not a clicks or not?",
            "Don't think of it as a partitioning of your set of variables, it's just the set of all interacting units in a way you can think of it that way, alright?"
        ],
        [
            "So we've now defined basically what is our building block for undirected graphical model.",
            "Now we can go and think about what.",
            "How do we express a joint probability distribution with an undirected graphical model.",
            "And we do that essentially in a sort of a similar way as with the directed graphical model, which is just it's a product over these clique potentials.",
            "That's it.",
            "And this said right, this set is are what we call the partition function.",
            "We've already mentioned this.",
            "This is essentially are normalizing constant, and this actually adds an awful lot of complexity to undirected graphical model.",
            "So where where this file here is very nice?",
            "Because it doesn't have to be normalized, it's very general.",
            "We sort of have to pay for that generality in specifying in basically having to define this to be a proper probability distribution that normalizes proper joint probability distribution.",
            "We have this said.",
            "We have to have this thing normalized and in general this thing is basically just some overall variables of our of.",
            "You know, the unnormalized portion.",
            "Alright, so now if we think of back to our toy example here on four variables, we can write down the probability distribution associated with that or joint probability distribution.",
            "This way, right?",
            "Because we have these four variables here.",
            "And we're going to write them down, so every edge essentially is defining in this.",
            "In this case, a clique.",
            "So because it's a clique, because I can't add any new variables, so I have four cliques in this graph, right?",
            "I have a click here.",
            "I have a click here.",
            "I have a click here and I have a click here.",
            "We know that that's the set of cliques in this graph, because I can't add any node to any one of those cliques and have it still be a clique.",
            "So if I were to do that, for example, if I were to say I've gotta click here, maybe I want to add D to that click like maybe I want a B, D. Well if I add D then there's actually it's not a complete graph anymore, right?",
            "It's missing this edge.",
            "Yeah.",
            "No maximum clique is actually something else, so I'm talking specifically about clicks, but in right here in a minute I'm really talking about what a maximal clique is.",
            "A maximal clique is a context where you can add no other unit and have it still be a clique.",
            "Oh sorry, you're right, actually yeah, yeah.",
            "Good point.",
            "Yeah, a can be a click.",
            "You can absorb it into the other cliques as well.",
            "Yeah, that's right, you could have a clique unitary clicks on all of these variables is a good point actually.",
            "Yeah, so right.",
            "So we have a joint probability distribution defined as a product over these clicks.",
            "With our normalizing constant.",
            "And that's just given by again, the product over the leaks summed over all of the variables.",
            "Alright, any other questions on that?",
            "OK."
        ],
        [
            "So what is a maximal clique, right?",
            "So that was the question came up so a click.",
            "First of all.",
            "I mean I said this in words, but it is just a subset of nodes whose induced sub graph, right?",
            "So if we just take the set, the set of nodes and then and take its graph that was associated with that set of nodes, it's complete, meaning that all units connect to all other units.",
            "Now maximal clique is 1 where you cannot add anymore nodes.",
            "OK, so the difference in this case there isn't really much difference between those two, because every clique is a maximal clique.",
            "Other than the unitary clicks here, if I had to click on ABC or D, then that would not be a maximal clique over here.",
            "For example, we have a bit of a different case here.",
            "I can imagine I have two maximal cliques.",
            "I have BABCD here and I have a BD.",
            "If I added any, if I added C for example to the clique Abd, this is no longer a clique, so that's so.",
            "I've grown those clicks as far as I can and that defines a maximal clique.",
            "In this context.",
            "Um?",
            "Right?",
            "Yeah, that's right.",
            "So yeah, so these are just two examples of cliques.",
            "In fact, maximal cliques in these cases, so these dotted lines here are such essentially encircling the cliques."
        ],
        [
            "Now an interesting fact so far what we've talked about is a parameterisation.",
            "So we talk about joint probability models with us last couple slides on that.",
            "Before that we were talking about conditional independent structures on a graph.",
            "So one question is like how do you actually relate these two out?",
            "We've sort of slipped that in that were like, really kind of these things are sort of the same thing, but there actually, you know there's work to be done to do that.",
            "In fact, this is what's the famous Hammersley Clifford theorem is essentially what relates these two.",
            "So for any positive distribution it has to be positive, meaning that.",
            "It has to give some nonzero probability mass to every combination of variables, or else actually this theorem doesn't apply whose conditional independence can be represented as an undirected graph.",
            "It can be parameterized by a product of factors that basically are consistent with that graph.",
            "Right, so this is essentially a statement of Hammersley, Clifford, and it basically allows us to connect joint probability models and these graphs together right so that's what we're going to be basically exploiting when we talk about these undirected graphical models."
        ],
        [
            "Alright, so one thing that you might be wondering is what is this part in the middle here?",
            "Like is what kinds of things are here, right?",
            "So I mentioned so again, these are the set of conditional independent statements.",
            "What set of conditional independent statements are represented can be both represented by directed model and equivalently by an undirected model well."
        ],
        [
            "Turns out the answer is chordal graphs.",
            "Anything that people can represent it as a chordal, undirected graph can also be represented without any loss of information as a directed graphical model.",
            "So what do I mean by that?",
            "A chordal graph is just one where all undirected cycles again in this is in the undirected model A4.",
            "More vertices have a cord.",
            "This is the notation heavy part.",
            "What's Accord accord is just an edge that is not part of that cycle but connects two vertices in this cycle.",
            "So basically what that saying is you can't have loops like this one.",
            "You can have a loop of four more variables in your graph.",
            "If you have a loop with with four variables like this is this still has a loop with four variables, But here it has a cord.",
            "Here's a loop with four variables, but here it has a cord.",
            "OK, so if your graph has that kind of structure.",
            "Then it can be represented as either a model or an undirected graphical model without loss of information.",
            "So I say that because you can always convert a directed graphical model into an undirected graphical model representation, but you are not guaranteed to lose information.",
            "For example, all of the conditional independencies that are represented in the directed graphical model might not be represented in the equivalent undirected graphical model.",
            "So where that is basically not where there's no loss of information.",
            "Is in these cases of the chordal graphs."
        ],
        [
            "OK, so for example tree structures are chordal because they have no loops at all.",
            "Anything without loops is chordal.",
            "Anything then?"
        ],
        [
            "Again, anything that has a loop, but where those loops are basically broken down into complete triplets has that structure as well, yes?",
            "Right, so we're just words, right?",
            "So I can.",
            "I'm not sure.",
            "I'm not specified how we write down the directed graphical model.",
            "I'm just saying that it is possible to write down a directed graphical model.",
            "It's just it's easier to specify in the context of undirected graphical models.",
            "Usually when we talk about the theory of of directed an undirected graphical models, an awful lot of time, what we actually do, for example, if we want to do inferencing.",
            "These models were not going to get into that in this lecture, but often.",
            "Often mechanically, what we do is we just take the directed graphical model, convert it to an undirected graphical model, and work in that space.",
            "It sometimes we lose information doing that.",
            "This is a context where we lose no information doing that.",
            "Yeah.",
            "So what if we have more than one coordinate graph?",
            "That's no problem, yeah, you in fact, you will need more than one court.",
            "If, for example, you have a cycle that's let's say 5.",
            "Let's say you have a big cycle of five units.",
            "And one cord is is only cuts off one part, but you you have a remaining cycle of four units, you'll need another cord to break off that cycle.",
            "So that's no problem at all.",
            "Any other questions?",
            "OK."
        ],
        [
            "So again, yeah, that's where we're at.",
            "OK, so now we're going to sort of walk to a little bit more specialization in terms of the parameterisation we had.",
            "These flies, these clique potentials are factors.",
            "Then I haven't told you how these things are actually parameterized or what these things really look like, so we're going to kind of walk towards something that's going to be a bit more, maybe recognizable.",
            "So the first thing we're going to introduce the notion of an energy based model.",
            "So I mentioned that these these clique potentials had this one limit is they actually have their output has to be non negative.",
            "Right, so there's a kind of a restriction on them.",
            "That way, sometimes you might want to have a representation of the model that, for example, of the parameters that doesn't have that restriction.",
            "One way you can do that while ensuring this non negative structure and the positive probability distribution is to enforce this kind of a representation.",
            "So you're going to take your factor or your click potential and just represent it in the log space basically.",
            "So now we're going to take our file here and just E to the some other factor of that click, right?",
            "So again, this is sort of an equivalent way to think about it, but the basic idea here is now that now if we use these kinds of factors, we've just, we can represent our probability distribution, our joint probability distribution.",
            "This way, right?",
            "So here, and this is just again, we've all we've done is we've just taken all are factors.",
            "This was a product of factors, and now we just replaced it with this exponentiation exponentiated form.",
            "So now that comes out as to the exponential of a sum, just equivalent to a product of exponentials, right?",
            "So.",
            "And a little bit of terminology.",
            "Sometimes this E. Some people refer to this E as an energy function.",
            "I actually like to refer to the entire sum here as the energy function, so I prefer to think of it as this kind of a structure where we have exponential negative and then we have our energy function.",
            "That's how we're going to be referring in the context of restricted Boltzmann machines, and here we have where Z is equal to the sum over.",
            "Again, this is just our partition function.",
            "We've not done anything, no magic here, it's still again the sum over all latent variables.",
            "Of this same term of the unnormalized distribution here.",
            "OK, so these are energy based models."
        ],
        [
            "Now we can go one step more specific and talk about what we call log linear models.",
            "Now log linear models are energy based models which are undirected graphical models.",
            "They're just a particular specialized kind of energy based models which are particularly useful for us.",
            "So in that case what we do is we take these these exponential features or factors and we basically parameterized them.",
            "Now we're actually going to add the parameters that we're going to learn.",
            "Right, So what we have is we use it basically composed of two things right parameter for each clique and a feature over the observed data like you can think of this as a sum sufficient statistic of your data or just some general feature of your data, right?",
            "So one thing about this, I'm sort of used this particular form.",
            "Of course log linear models are slightly more general than this.",
            "You can imagine sharing parameters or having more than one parameter forgiven F If you like, so that's a little bit more flexible than what I've defined here.",
            "I've just defined it this way for simplicity.",
            "But you have a bit more generalization that I'm showing you right, so the joint probability distribution in this case is of course given by this kind of of property, where we still have our partition function E. And now we've replaced essentially are exponentiated factors with this feature.",
            "Times are parameter log linear because it's exponentiated an, then we have were linear in the parameters, and we're also happened to be linear in our features.",
            "Alright."
        ],
        [
            "Any questions up to this point?",
            "So now we've basically gone as far as we're going to go in terms of the formalism of the model, the undirected graphical model we're going to talk about in general, but now what would like to do is talk about how we learn in this model.",
            "So we've done a little bit.",
            "We talked a little bit about inference when I drew that thing on the board, right?",
            "The other thing that we're going to spend a lot of time doing machine learning is learning these models.",
            "So in the context we're going to be doing that in most often is using maximum likelihood learning.",
            "So what does that mean?",
            "Basically, what we're interested in is just.",
            "Maximizing the log probability of the data.",
            "Subject to basically by over our parameters, right?",
            "So we're trying to find the parameters that maximize the log likelihood of the data.",
            "That's essentially what we're saying here.",
            "So now what we can do, and this is again in the context of everything is observed.",
            "So my observations now.",
            "So I've this is kind of the case where we almost never have in the sense of real, often never observe all of the variables.",
            "But let's say I've written down a graphical model in this setting where I actually observe everything that I'm set up.",
            "So this is kind of the easiest possible case you can have for learning, and we're going to see is in.",
            "Even in this case this super easy case, we have a problem when trying to learn.",
            "From these models, these undirected graphical models, so let's go ahead and do that.",
            "So this is again, this is our probability distribution.",
            "We were observing all of X here, so we're just going to write down what this is, the sort of general form, right?",
            "So now it's log here, so we're just sticking out our partition function.",
            "This product, by the way, is just over the data examples, right?",
            "So these are independent samples, so so we have our product when we once we push our log inside that becomes a sum over data points here.",
            "And then we're going to again.",
            "This is our log of our clique potentials here.",
            "So and again, zed here what we've gone from here to here.",
            "We're just pushed set out of this sum.",
            "So we've got this quantity here because it doesn't actually depend on the on the data points, so this is just the data size times log Z and over here what we've done is we've just plugged in our log linear parameterisation, right?",
            "Like I said, this is a choice, right?",
            "This is much more general than this, but turns out we're going to most.",
            "And find ourselves in this context.",
            "Certainly in the context of a restricted Boltzmann machine, which is really what we're going to be focusing on that really fits into this kind of model structure.",
            "So right, so now we're going to do is we're going to say that that this actually turns out to be fairly easy to work with because it totally decomposes, right?",
            "This is just a bunch of sums over independent pieces.",
            "This does not decompose, and this is a problem because this is a function of our parameters.",
            "So here all of the pieces are intermixed together and this."
        ],
        [
            "Think about decomposed what that is?",
            "This log.",
            "Here we have a sum.",
            "This is are some overall X right.",
            "This term does not depend on the data.",
            "This is a sum over all values of all of your variables and this turns out to be complicated, so we're not going to be able to find a closed form solution like we can in most cases for directed graphical model.",
            "It's fully observed.",
            "On the other hand, what we might be able to do still is actually find a gradient, 'cause then we can at least train these models using some sort of gradient method.",
            "So if we ask that question, what is the gradient of this log?",
            "Partition function here.",
            "This contribution to the likelihood we end up we can just see.",
            "So this is basically pretty simple to do right.",
            "We just the derivative of this log puts all of this down here and then.",
            "All the terms that are not actually is a partial derivative.",
            "So all the terms that are not actually associated with that just disappear because it's essentially normalized out.",
            "So you can remove all of those right away.",
            "And what we end up with is an expectation over the model distribution.",
            "Here for each click potential.",
            "So this actually does decompose over the clique potentials.",
            "But it's still in an exponent.",
            "It's still a.",
            "You need to get an expectation from the probability distribution of the marginal probability distribution for that.",
            "So in other words, it's as hard as computing the marginal probabilities, which turns out not to be very simple, because you need to, because we've defined as a joint probability distribution, right?",
            "This model is not defined by its marginals, so you actually have to if you wanted to compute the marginal, you'd have to do the work of integrating all those other variables away.",
            "So we've got it in some form.",
            "Looks simple, but this is actually not in general."
        ],
        [
            "Very simple to compute.",
            "So what can we do in this case?",
            "So we're here.",
            "This is just basically putting it all together, so this term turns out to be the the data term.",
            "Let's call it.",
            "This is essentially things that we can actually get marginal simply from the data.",
            "So this first term ends up being easy, right?",
            "This is just this part of the contribution of the partial derivative here.",
            "This part of the partial derivative is complicated, and this is what we just talked about.",
            "This is the partition function, part contribution, and what we end up with is these two terms, and this is you're going to see this over and over again.",
            "Undirected graphical models that you got this part that depends on the data.",
            "And a part that depends on the model and the trick is this is often intractable even in the case of fully observed X.",
            "So this is our simplest possible setting and this part of even trying to learn this model gets complicated because of that."
        ],
        [
            "Context, So what do you do in that context?",
            "Well, there's a few different things you could do.",
            "You could try doing something like pseudo likelihood to make an approximation of that partition function.",
            "That can work, but as soon as we introduce latent variables like, it's complicated.",
            "So what we can do is is sort of a general purpose algorithm and what we're going to exploit in the context of a restricted Boltzmann machines is we're going to use Monte Carlo samples or Monte Carlo methods.",
            "Essentially sampling from these things.",
            "So we're going to sample from this expectation to come up with an approximation of the gradient.",
            "That's going to be our what we're going to do, but there's disadvantages with doing this.",
            "Can anyone think of what might be a disadvantage with doing sampling in this context here?",
            "It's expensive, right?",
            "So sampling is kind of always expensive.",
            "You can imagine just having to generate a lot of samples, but there's a particular flavor in what and how it's expensive.",
            "Right, it's hard to find.",
            "Yeah, let's let's say samples from the accurate distribution, right?",
            "So let's so things like you know, you can try something like important sampling.",
            "That's usually a nonstarter, because if you don't know your distribution ahead of time, actually kind of.",
            "These are usually high dimensional spaces, so actually trying to find the right distribution this high dimensional thing.",
            "It's basically over before it begins now.",
            "One method that does actually work quite well in the context of high dimensional sampling is something like a Monte Carlo MCMC.",
            "So I I'm on.",
            "Markov chain Monte Carlo, right?",
            "So this is a case where your conditioning and doing dependent samples one after the other.",
            "Right now who can think of so that we have a sampler that might work in this context?",
            "Is there a problem with that?",
            "Convergence you have to wait a long time and you're what you're doing is you're doing this in the inner loop of a gradient descent algorithm, right?",
            "This is a big problem because you are basically saying every time you want to get a new gradient update, you're going to have to wait for your Monte Carlo sampler to converge in order to be off the races.",
            "So now that's essentially going to be the issue that we're going to have them, and we're going to see how we can address it in that context."
        ],
        [
            "Alright, so now we're going to move right to restricted Boltzmann machines, and I'm going to go a little bit faster 'cause I'm realizing we're kind of running a bit out of."
        ],
        [
            "Time, so this is the this is the restricted Boltzmann machine, right?",
            "I've already written it on the board, so here we can see a bit of a different formalism here.",
            "So we have our energy function.",
            "As I mentioned, we've talked about energy functions in the kinds of it, so it's an energy based model.",
            "And nice thing about energy based models is you can just write down the energy function and then now you can start to think about how the model works right?",
            "So here it is in vector notation if you write out all the variables we have this form here.",
            "So we have a weight matrix here represented here.",
            "That that that encodes the interactions between these and then we have a bunch of unitary cliques.",
            "These are unitary variables that encode biases on our on our units.",
            "Here are visible units X and are hidden units H, so that's another point that's important for the DBM.",
            "X is basically always going to be what we call visible layer.",
            "This is, these are things we're going to observe, and H is going to be our hidden layer, and these are binary.",
            "These are typically binary, but we can generalize the PBM to include nonbinary X.",
            "Right?",
            "So, so now what we're going to do is we can actually think about how we fit this RBM in the context of things."
        ],
        [
            "Been discussing right.",
            "So first of all, just think of it from a Markov network or as an undirected graphical model from the point of view of a vector node like Siri, have just one node here that's vector valued in another node.",
            "Here that's a vector valued.",
            "We have.",
            "Our factors are flies.",
            "Here are just expressed in this form, right?",
            "So you can see now that it's in this exponentiation here, so it's in this negative.",
            "So this this this energy based formalism here."
        ],
        [
            "Another way we can interpret this is break apart those vectors.",
            "ANAN build the the scalar version of this, and here we have a bunch of pairwise factors right between these scalars here.",
            "So this is essentially equivalent representation.",
            "So now we've just broken up these vectors into into scalars."
        ],
        [
            "And that's basically equivalent."
        ],
        [
            "This is basically back to our just our normal framework where we're going to look at this."
        ],
        [
            "Now the question is, we talked a little bit already about this question of how to do inference in this model, right?",
            "So that's what we're talking about here.",
            "So inference in the context of an RBM, basically in context of any model, is about computing conditional distributions.",
            "For example, we might be interested in the probability of H given X.",
            "And that's going to be just given by.",
            "That turns out, and we've already seen this.",
            "Basically that from the Markov properties of this network that if you condition on X, we can actually that these are actually independent, right?",
            "We've already covered that turns out because of this, because the PBM is perfectly symmetric in its its conditional dependencies between X&Y.",
            "That we also have this property.",
            "That condition on HX is are all independent, and we're going to exploit this property of.",
            "Of these two, being independent from one another condition on the others.",
            "So rather conditioned on HX is are independent, conditioned on the X is H as independent.",
            "And where we and it turns out that that we can actually derive a fairly simple form for these conditionals here, where this thing is just given by the sigmoid of of.",
            "The logistic sigmoid of the bias associated with that unit plus the weight matrix associated with the weight vector associated with that unit in dot product with the visible units.",
            "That's the case of H here.",
            "Now this should be familiar to you, right?",
            "So this looks an awful lot like the activation, so there's a nice relationship between these two and we actually have a very similar symmetrical relationship for P of X.",
            "Given H. It's again given by a sigmoid, where we have the bias plus the weight metrics, the weight vector.",
            "Associated with that visible unit and projecting on to H."
        ],
        [
            "Alright, so we can actually do this thing where we go through the math and analyze the conditional independence that we've already done this in the context of the just looking at the graph, but it's useful to actually derive the structure of the graph, describe the structure of these conditionals, so we'll just go through this really quickly.",
            "First thing we do is we notice that, well, we're just writing down the conditional distribution here, so that's just equal to the joint over P of X.",
            "What's P of X?",
            "Well, we haven't gotten explicit form of P of X, so we're just going to write P of X is being the joint.",
            "Where I marginalized out H. Right, that's all we're doing here."
        ],
        [
            "This is just expanding and I'm just plopping down our joint probability distribution in these two cases.",
            "What we can notice is we can actually cancel out the zeds here in both cases, and because neither one actually because we have different H on the bottom here, but the relationship to X is identical, so we can actually cancel out the terms that involve only X as well.",
            "So we can do that.",
            "And now we have this kind of a structure and where what we've done here is just written out like the explicitly all of the different dependencies.",
            "Now this is a.",
            "This is summing over all possible values of all H is on the bottom here.",
            "So what we notice is that we can.",
            "Actually this is just a sum exponential of some, so we can actually write this as a product of the exponential elements in that sum, which we've got here and now.",
            "We can do this thing that because there's no interaction here, because these these products are all independent of one another with respect to H. We can actually tease them apart, and we can write this.",
            "Flip these around.",
            "So instead of being a large sum of these of this product, we can turn it into a product of sums.",
            "Right, so now we can.",
            "Now what we've done, and this is actually the crucial step, because now you can see that what we have here is a bunch of it's a product over a product.",
            "Or in other words, it's so it's this actually now.",
            "So here it's a product over product we can do is for each one of these.",
            "Now it's just a sum over the individual.",
            "H is so we can actually go ahead and do that.",
            "We plug because they are binary.",
            "We plug zero in here and that's just basically resolves to be one because this is zero and this is 0.",
            "So E to the zero is one that gives us that term and we we plug in H is equal to 1 and that gives us this other term here.",
            "Sorry question, yeah.",
            "Alright, so.",
            "So then what we have is we end up having this product of pieces that look like this right where each individual element is only a function of is only depends on this H sub J right?",
            "Only a single H. So we now know we've derived the fact that it factors.",
            "And that's just that's that's this form.",
            "Now to show that this form actually is the sigmoid, well, it's essentially just trivial.",
            "Trivial to derive that you can just go from this to this form by just dividing out the top here and then.",
            "That's just the definition of the logistic sigmoid, right?",
            "So there we've essentially defined our conditional distributions and it's just the same thing going the other way."
        ],
        [
            "Another thing that we might want to know about is the free energy in the interest of time."
        ],
        [
            "Actually going to skip this, but you can see that we can derive something similar using the same basic tricks.",
            "The free.",
            "Oh sorry, what were?"
        ],
        [
            "Really looking at here is the marginal probability on X, right?",
            "So H is are these things that don't really exist in the world?",
            "X is what really exists, so we might want to imagine just right being able to write down a joint distribution over the X is once we've marginalized out H. That's essentially what we're doing, and this is basically just showing you can actually do that.",
            "Tractably, it's a little surprising, maybe that you can do that, but you can.",
            "And what you end up with is something that has this form.",
            "It's got this softplus function in here and that just is basically, this is what the softplus function Maps, and it just basically looks like this so.",
            "What this is essentially saying is that.",
            "That the the marginal probabilities will, or the joint probability an X will go up for cases where X is similar, the dot product between X and elements of those of the weight vector are are high."
        ],
        [
            "So right, so now, let's talk about training in this model.",
            "Right, OK, so so now we've already talked about training in the general case.",
            "There's actually one element that adds complexity to this story.",
            "That is that we have in the PBM, and that's that.",
            "What I described in the case of training general undirected graphical models, is that they were fully observed, right?",
            "That's what we were discussing.",
            "This fully observed case, and even there we had this problem with the contribution to the partition function, PBM's are not fully observed, but inference in this model is trivial.",
            "Right, at least in the case of computing P. Of X given AP of H given X, this turns out to be pretty simple.",
            "We're going to be able to exploit that, and the difference is essentially the difference between this being fully observable will not comes in right here, so we already saw that we had these two terms.",
            "If we compute the gradient right, this is again just we're trying to.",
            "In this case, we're minimizing the negative log likelihood instead of maximizing log likelihood.",
            "And if we compute what that is, what we end up with here is this term.",
            "Which we are going to positive face.",
            "This is before exactly the same thing as what we had before when we called it the the data term and then before what we had was something called the model term.",
            "Here we're going to call that the negative face.",
            "It's exactly the same thing, it's just slightly different terminology.",
            "I I chose the terminology of determine model term because they meant something positive phase negative phase in this context means something in the literature of these methods, But you can think of these things exactly the same.",
            "The only difference is that here we actually have an additional expectation.",
            "Over H right?",
            "We didn't have this before.",
            "It was basically just the marginals from the data itself, so we've added this one slight layer of complexity, but that turns out to be no problem at all.",
            "This expectation, because it's conditioned on X, and we've already seen that H factorizes when conditioned on X, so this turns out to be readily computable, readily tractable.",
            "That presents us no trouble.",
            "This still presents us.",
            "This is our partition function.",
            "This presents us with the same problem it always did, which is we need to be able to sample from the model.",
            "In this case, what that means is we need to be able to sample from the joint of PXH.",
            "And that presents us with the same problems we had before.",
            "We can do so with Monte Carlo Monte Markov chain Monte Carlo methods, but then we have this problem of having a long burn in and this is about where the technology was up until.",
            "You know mid 2000s and then."
        ],
        [
            "Jeff Hinton proposed and colleagues I should say proposed or maybe yeah proposed a an algorithm called contrastive divergent's, so the contrastive divergent basically has three ideas on top of this setting that we've talked about already, which is that we need a Markov chain Monte Carlo and in our context here that's going to be we're going to be able to Gibbs sampling for that, But here's the idea.",
            "Basically.",
            "First idea is instead of doing this big, expensive expectation, we're going to be doing stochastic gradient descent anyway, so a little bit of variance doesn't really matter.",
            "What we're going to do is we're going to actually replace this expectation by a point estimate at X.",
            "In practice, you can use a mini batch for this to reduce your variance a little bit, but this is the first idea.",
            "We're going to just use a subsampling.",
            "We're not going to collect that many samples, so our Monte Carlo method isn't going to be that expensive.",
            "The second thing we're going to do is we're going to do our sampling based on Gibbs sampling, and we're going to use that because now we can exploit this nice conditional independence properties of the of the restricted Boltzmann machine, right?",
            "So conditioned on X we can sample the H condition on that H. We can sample the X back and forth.",
            "It's a very efficient sampling scheme, right?",
            "'cause we can, it's called Block Gibbs sampler where we can sample on a large block of variables because they're all rendered independent conditioned on the other part.",
            "So we can kind of think about this process here is being sort of your sampling up and down.",
            "Now the last thing we're going to add to this process is to deal specifically with this idea of this long chain burn in process, and for that we're going to start sampling the chain at the data point itself, right?",
            "So we're going to say that.",
            "Well, we really care about is how the model diverges from the data distribution.",
            "We don't really need to have a sense of the data distribution itself, So what we're going to do is just initialize it at the data and watch how it goes away from the data distribution and count that as our approximation of the gradient.",
            "That is basically what we're saying here with four four contrast images, and there's a nice interpretation."
        ],
        [
            "This you can think of it as saying, OK, I'm going to start with my data here X of T and we've got the corresponding HFT that you infer.",
            "In this case, we're kind of considering this sample, but really what we're talking about is doing a full expectation over this, so we're not inducing any.",
            "In practice, we don't induce variance for that, but we're here in the energy formulation we want to push this down or in the probability formulation we want to push this probability up of the data, and then we're just going to let this thing sample a little bit, and we're going to.",
            "Push down at the place where it samples.",
            "So we want to push down the place where the model things are likely and push up where the data wants to be, and this combination of pushing down and pushing up, pushing, pulling up, pushing down."
        ],
        [
            "Is actually our learning algorithm and will change this so right.",
            "So this is our sample here.",
            "So we want to make these kinds of things less likely and things that look like the data order come from the data more likely, right?",
            "So this is the basic element of contrastive diver."
        ],
        [
            "Right?",
            "Good OK, so again, we're back to our this being hard to compute, but now we've got this a contrastive divergent algorithm which is going to be able to compute that.",
            "So anybody want to suggest what some issues are with contrastive divergent's?",
            "Yeah.",
            "Right, but there were relying on is the parameterisation of the model.",
            "We don't have to get them all right, we just have to get enough of them to let because the space is always going to be smoothed over by our parameters like this, I mean the whole principle of maximum likelihood depends on this anyway, 'cause you never want to just push up Delta functions where the data exists, right?",
            "So This is why, for example, mixture of Gaussians, the unregularized mixture of Gaussian model maximum likelihood really makes no sense because just the collapsing on the data points gives you infinite or arbitrarily high likelihood so.",
            "Same basic idea.",
            "Here is just you need.",
            "You're going to rely on the memorization to smooth things out, both in the data and the negative examples.",
            "Anybody else see a problem with contrast versions, yeah.",
            "Yes, that actually is a serious issue.",
            "I'd like to get back to that.",
            "Maybe in the questions at the end if there we have time, it's that's a bit of a finesse point.",
            "I was just going to mention that the big problem you have contrastive divergences Tobias sampling scheme, right?",
            "You start at the data you're running, Gibbs sampling.",
            "You don't let it run to convergence in general.",
            "We actually in practice don't let it run very long, so this actually induces bias.",
            "Turns out this bias is actually not that bad in most cases, so it's sometimes a little bit surprising that this is actually a pretty functional learning algorithm.",
            "But it actually does work fairly well.",
            "So.",
            "So right, so getting into a little bit more of the math of this, we can compute our what we had here was we just had these terms here that we need to compute just to fill this out a little bit.",
            "We don't need to go."
        ],
        [
            "On this in too much detail, but this is basically so we just we're just going to say OK are partial derivative.",
            "We're just going to fill it in for our weight matrix here.",
            "Compute the gradient with respect to weight matrix.",
            "This turns out to be fairly straightforward, right?",
            "It's just a linear function of these weights here, so we end up with just this term for partial derivative with respect to that element, and then the gradient.",
            "Now meaning with respect to the whole matrix W is just given by this formulation here.",
            "Right?",
            "So."
        ],
        [
            "So so you can go over the math yourselves if you're interested, But it turns out these kinds of calculations turn out to be fairly straightforward.",
            "We can do the same thing in the context of the conditional expectation here, so we're just plugging in now this term, and this is our now.",
            "Our expectation over over this term, and what happens in the case of the data distribution we end up with something like this, which is just given our data example.",
            "Here we compute.",
            "This is from the data and we just compute this probability.",
            "This we've already derived with these are.",
            "These are just given by this sigmoid, right?",
            "So we can actually compute these this expectation trivially.",
            "We've already mentioned this, but this just goes into a bit more detail on that point."
        ],
        [
            "So now to put it all together in terms of an update formula for contrastive divergent, we're going to say we're given some X and then this this point that we sample.",
            "We started X and we run Gibbs sampling for a little while and we get to X~ here and the learning rule for our weight matrix ends up looking like this, so our gradient we just fill in all the pieces and we do the substitutions that we've just arrived and we end up with something like this, right?",
            "So this is just basically saying that the gradient, the actual update for stochastic gradient descent.",
            "Turns out to be fairly straightforward, and it's just basically a difference of sufficient statistics, some derived from the data, some derived from the model, and the model.",
            "Sufficient statistics here are approximated by the sample that was kind of like Gibbs sampled away from the data."
        ],
        [
            "Alright, so.",
            "Now, so the sort of pseudo code for CD CDK here will get into what K is in just in just a minute is we generate a number of his first, we start off with a training example.",
            "In general, again, we're going to use a mini batch for this to reduce the variance a little bit, so we're not just going to do this with one sample, although you could do it with one sample.",
            "You can generate a negative sample that corresponds to this data by doing case steps of Gibbs sampling.",
            "That's our CDK starting at X, and you update the parameters.",
            "We derive this formula for the update.",
            "These two are similarly in fact easier to derive.",
            "You will let you do that on your own if you like, and then you just essentially iterate this process.",
            "This is the training algorithm for CD."
        ],
        [
            "So getting back to K for a minute now, I mentioned that this is a biased sample, right?",
            "'cause we're starting at the data and we're not letting it go very far now.",
            "The bigger K is mean in the mortgage steps we have, the less bias we're going to be inducing in the model, and that's going to necessarily going to help.",
            "In practice, K = 1 works.",
            "I will say that K cool one that means we'd actually literally do just one sample, so this is a highly biased sampler, right?",
            "So it works.",
            "This is something that was actually very, very common to do.",
            "Back when the you know, circa 2006 it works very well if you're doing.",
            "If you're using it for like feature extraction, unsupervised pre training.",
            "If you actually want to use CD as a as a sort of to actually you want to take this the PBM.",
            "Seriously, as a generative model of the data, you're going to want to use a larger.",
            "I found K = 25 actually works very well.",
            "In fact, that's a very hard algorithm to beat in terms of modeling the data for an art for GBM.",
            "But there are other choices."
        ],
        [
            "For example, there's something called persistent CD.",
            "This turns out to be.",
            "This name was given by Tiedeman in ICL paper in 2008.",
            "This turns out to be essentially a reinvention of a more general concept called stochastic maximum likelihood and the basic idea here is that you can think of it as sort of a variation on CD, where instead of starting at the data, you start at the last sample.",
            "From a Markov chain, so think of it like this.",
            "You've got this, you've got this Markov chain running in the background and you're just going to be as you as you do your updates in the model you're just going to be running this thing on the model itself, and you're just going to drawing samples from that Markov chain.",
            "So what the big advantage of this is is that you don't have to wait for it to reconverge after for every gradient update.",
            "You essentially are relying on the fact that your updates to the model are small.",
            "They're just small gradient updates.",
            "So you think that the model is going to be close to where it was at the last time step, so convergence is going to be fast.",
            "And so again, in practice we often use just one sample to make it converge.",
            "That works fairly well.",
            "So.",
            "One issue that this method has and this is basically probably what you might think.",
            "Well, at least what I thought was sort of the closest thing to the original maximum likelihood solution for this, right?",
            "So because you're actually drawing samples from the data and not reinitializing them, so drawing samples from the model and not real initializing them at the data as CD does, so you would think that this would work very well.",
            "It turns out that if you're modeling a data set that's highly multimodal, persistent CD actually has some pretty strong effects that where.",
            "The negative phase get stuck in these modes is going negative phase sampling right?",
            "'cause we're drawing samples for the negative phase contribution to the gradient, right?",
            "So we call this negative phase sampling.",
            "You get stuck in these modes and you can actually end up having this problem of being you kind of not being able to move around very well and CD kind of gets out of this to some extent because you're always restarting the model app data, so there's in some sense a little bit of a better.",
            "It's a little bit better in terms of being able to explore the space, So what you actually see when this it's actually a pretty interesting dynamic phenomenon, because what ends up happening is when you're when you end up being stuck in a mode.",
            "This is again the negative phase of the of the gradient, right?",
            "Which means you're busy.",
            "Unlearning that mode.",
            "So, for example, on something like MNIST, what ends up happening is, let's say you're learning on this.",
            "Actually, we can go to M list."
        ],
        [
            "Yeah, this is amnesty.",
            "I'm sure you've seen this by now, so this is.",
            "These are basically just digits.",
            "There's 10 digits there, they're clustered pretty heavily, so the probability distribution is basically multimodal, and there's roughly 10 modes for each digit.",
            "Maybe there's a few other modes, for example because you have sevens like this and sevens like this, right?",
            "So what ends up happening with persistent CD is that.",
            "It quickly builds these modes, which is good, but at some point you end up with, say, a few more of your negative phase samples, say in the seven mode.",
            "Then you're getting in your data right?",
            "So they start to be under represented overrepresented in your negative phase and under represented in your positive phase that has the effect of pushing that mode up right?",
            "So you were basically unlearning sevens for a little while, until the mode gets big enough that those samples can actually leave, and then then you will back relearn this model.",
            "So what ends up happening is that you actually see this if you monitor the samples throughout training is you end up with this process of it just constantly.",
            "If you keep at least a constant learning rate, it's just constantly remembering and forgetting individual samples.",
            "Individual digits through this learning process.",
            "City, my experience has been is a little bit more stable and you know if you care you can actually get better likelihood using CDK with a relatively high value of K compared to using persistent contrastive divergent."
        ],
        [
            "So right and these are the kinds of filters you get so they look rather pen stroke like.",
            "Yeah so and then right?",
            "So these are just the filters you get with with CD you can generally."
        ],
        [
            "Is this of course to have."
        ],
        [
            "Of other kinds of data, right?",
            "So we talked about.",
            "In that case it was binary data, yeah?",
            "Oh yeah, sure, sure.",
            "I feel like I've played a little bit with that.",
            "I can't remember what happened.",
            "It wasn't dramatically.",
            "It didn't feel like the best of both worlds is what I recall.",
            "Right, so so this is just this.",
            "I will have to leave it here 'cause I'm essentially out of time but but the this is the energy function that would go with the PBM model.",
            "If instead of modeling binary data you want to model Gaussian data right?",
            "And so it's essentially this very similar.",
            "So up till here it's basically the same.",
            "But what we've done is we've just added this term here right to keep it.",
            "Basically, this is just a way to keep the model.",
            "Able to normalize and not to like if you don't have this part of the model here then what ends up happening is if you try to compute the partition function you can't.",
            "That integral explodes.",
            "So you need this kind of term to bound the size that X can be right.",
            "So this turns out like that.",
            "We call this the Gaussian Bernoulli RBM, and if you were to."
        ],
        [
            "Use that and compute filters.",
            "They you know looks like something like this.",
            "If you put a sparsity, perhaps partially prior on your latent variables, that can you can end up with a slightly better model that performs better in the filters.",
            "Look a little bit better than this.",
            "But in general this is kind of a not very satisfying model, because we want to use real value data for doing things like modeling natural images."
        ],
        [
            "And so this is where our spike and slab came in, and I'm not going to go over this.",
            "I've run out of time, but I'll just say that what we did with this model is we basically augmented the latent variables to have not just binary H but real value.",
            "So here what we've got V for visible units here instead of X.",
            "These are real valued now, like in the Gaussian binary RBM.",
            "But now we have S as well as H. And there's just like a an element wise product of these two variables and."
        ],
        [
            "You make a careful choice of the energy function.",
            "This is a bit more of a complicated form of it, but this is the energy function we use and then you just go through the same kind of derivations we've done in the context of the normal PBM.",
            "You can do the same thing and what you end up with is you can derive like what we exploited in the case of the normal GBM.",
            "Is this blockage sampling where you sample one set of variables given the other and then you sample it back and that turned out to be a very efficient way to do inference?"
        ],
        [
            "That you can do learning.",
            "We can basically do something equivalent in the context of the spike and slab RBM, where you condition on.",
            "Basically you do these three sets of variables.",
            "So instead of being like a two way block upsampling, it's a 3 way ball game sampling.",
            "Ann, I think I'm going to just stop there."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi right so I guess I haven't even put my name in my name is Aaron.",
                    "label": 0
                },
                {
                    "sent": "Please do ask questions.",
                    "label": 0
                },
                {
                    "sent": "I think it will be more fun for both of us.",
                    "label": 0
                },
                {
                    "sent": "I understand that you that's a pretty good group and that you've been asking lots of questions.",
                    "label": 0
                },
                {
                    "sent": "That's fantastic, so I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Mainly I was scheduled to talk about undirected graphical models as an introduction to undirected graphical models, but I understand you haven't yet seen much about directed graphical models, so sometimes I'm going to refer back and compare to directed graphical models and give you a little bit of background on what directed graphical models look like.",
                    "label": 1
                },
                {
                    "sent": "Just to give you a bit of context, so I'm just curious in terms of your background who has worked with undirected graphical models will start there before?",
                    "label": 0
                },
                {
                    "sent": "OK, so some number like you're familiar with our BMS.",
                    "label": 0
                },
                {
                    "sent": "That's probably the main thing that people have worked on show of hands for our BMS 'cause that's mainly the topic of this talk.",
                    "label": 0
                },
                {
                    "sent": "OK good so a few people have but probably not the majority of you.",
                    "label": 0
                },
                {
                    "sent": "That's great.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is my overview, right?",
                    "label": 0
                },
                {
                    "sent": "So largely on largely.",
                    "label": 0
                },
                {
                    "sent": "Frankly, I'll be spending most of my time talking bout restricted Boltzmann machine right here, but on the way up we're going to talk about some of the differences between undirected and directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about a central idea in graphical models, which is conditional independence, and we're going to do a brief probability review of 1 slide on that and then talk about how that relates to the graphical models will be discussing.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about energy function formalism or just.",
                    "label": 1
                },
                {
                    "sent": "Energy based models.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to talk about maximum likelihood learning in the general case for these energy based models.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to go into restricted Boltzmann machine as an example of one of these kinds of models.",
                    "label": 0
                },
                {
                    "sent": "And then, if time permits, an I'm not sure it well.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about work that I've contributed in this area, which is the spike and slab RBM.",
                    "label": 0
                },
                {
                    "sent": "Just a few slides on that, just to give you an idea of flavor of how you can extend this formalism in interesting ways.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's get started.",
                    "label": 0
                },
                {
                    "sent": "So probabilistic graphical models.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "Well, really, it's it's the marriage of probability theory and graph theory.",
                    "label": 0
                },
                {
                    "sent": "Here we have our probabilistic graphical models represented as a graph.",
                    "label": 1
                },
                {
                    "sent": "So nodes represent random variables.",
                    "label": 1
                },
                {
                    "sent": "That's our our probability theory here in the edges encode conditional independences, or rather to be more accurate, it's the absence of edges that encode conditional independencies.",
                    "label": 1
                },
                {
                    "sent": "Right, so the graph structure is essentially what gives you this set of conditional independencies and the nodes themselves are the random variables, and you can then perform inference with the probabilistic model associated with the graph in the graph can actually help you decide how you're going to perform inference.",
                    "label": 1
                },
                {
                    "sent": "It will tell you, for example, if inference in this graph is going to be something that you can do tractably or efficiently or not, and will get into a little bit of that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So right so the as I mentioned, there are two basic flavors of graphical models directed graphical models, an undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "The way these are also called base directed graph models are also called Bayes Nets or belief networks, and the consists of a set of nodes and directed edges.",
                    "label": 1
                },
                {
                    "sent": "So basically little arrows between those nodes, and the arrows encode factorized conditional probability distributions right?",
                    "label": 0
                },
                {
                    "sent": "So these are the little or CPD.",
                    "label": 0
                },
                {
                    "sent": "These are the little probability models that you combine into a big set of joint probabilities using this graph, undirected graphical models also can set a set of all of nodes, but these edges now are undirected, and they're a bit more general in their representation.",
                    "label": 0
                },
                {
                    "sent": "The edges here in code conditional independence, but not necessarily as a conditional probability distribution.",
                    "label": 0
                },
                {
                    "sent": "The big difference there was what will get into is that in the case of an undirected model, these these local potentials.",
                    "label": 0
                },
                {
                    "sent": "Don't need to be normalized, you instead pull out the normalizer and it's not going to be global normalization, and that's going to give us some advantages, but also some disadvantages, and we're going to see how we can deal with those.",
                    "label": 0
                },
                {
                    "sent": "And as I say today, we're going to focus almost exclusively on undirected graphical models, and I'll just refer to directed graphical models in passing.",
                    "label": 1
                },
                {
                    "sent": "Just to give you a bit of context for the differences.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first a little probability review.",
                    "label": 0
                },
                {
                    "sent": "So just basically conditional independence.",
                    "label": 0
                },
                {
                    "sent": "So first, what is what is probabilistic independence?",
                    "label": 0
                },
                {
                    "sent": "What we have if we have a joint probability between of variables X&Y, they are independent.",
                    "label": 0
                },
                {
                    "sent": "If the joint probability is equal to the product of their marginal probabilities, right P of X * P of Y.",
                    "label": 0
                },
                {
                    "sent": "So that's simple, marginal independence or just independence.",
                    "label": 0
                },
                {
                    "sent": "Conditional independence is a little bit more complicated and it's sort of it's represented right here.",
                    "label": 0
                },
                {
                    "sent": "So we say that.",
                    "label": 0
                },
                {
                    "sent": "X is conditionally independent of Y given zed, so this is like hence conditional independence.",
                    "label": 1
                },
                {
                    "sent": "If if we have the following statement, if if P of X the joint probability of X&Y given Z is just equal to P of X given zed and PFY given said so, it's a natural extension of this idea of marginal independence, right?",
                    "label": 0
                },
                {
                    "sent": "And by extension, we also have that if X is marginally is conditioning dependent of why then we have this following statement is true that P of X given Y&Z is just equal to P of X.",
                    "label": 0
                },
                {
                    "sent": "Given zed, this falls naturally an intuitive way to see this is you're just saying that why provides no new information if it's actually independent of of of X conditioning on it gives you no new information about X, right?",
                    "label": 0
                },
                {
                    "sent": "Once you have said you get no new information about X.",
                    "label": 0
                },
                {
                    "sent": "So an example for example of something like this would be.",
                    "label": 0
                },
                {
                    "sent": "Let's say you hear Thunder.",
                    "label": 0
                },
                {
                    "sent": "And you want to know if the grass is wet, you know, and the conditioning variable is you've looked outside and it's raining, right?",
                    "label": 0
                },
                {
                    "sent": "So once you know it's raining, you don't.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter if you've heard Thunder or not, right?",
                    "label": 0
                },
                {
                    "sent": "You know that it's it's raining.",
                    "label": 0
                },
                {
                    "sent": "Or the grass is wet anyway.",
                    "label": 0
                },
                {
                    "sent": "Right, so or here's a maybe a better example here, so that Thunder, given rain and lightning is just equal to the probability of Thunder given lightning, right?",
                    "label": 0
                },
                {
                    "sent": "You don't actually need to know that it's raining once you.",
                    "label": 0
                },
                {
                    "sent": "Once you have lightning because there's a.",
                    "label": 0
                },
                {
                    "sent": "In this case, there's a tight causal connection between lightning and Thunder.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so let's let's think about these.",
                    "label": 0
                },
                {
                    "sent": "So what I've said is that these graphs are there to essentially represent conditional independence, and that's basically what these graphs are going to do for us at a large scale.",
                    "label": 0
                },
                {
                    "sent": "And the way to think about that is this is if we think of this Gray box.",
                    "label": 0
                },
                {
                    "sent": "Here is the set of all probabilistic models.",
                    "label": 1
                },
                {
                    "sent": "Then we can represent.",
                    "label": 0
                },
                {
                    "sent": "We can think of it as in the all possible.",
                    "label": 0
                },
                {
                    "sent": "Set of conditional dependencies that can exist in the world.",
                    "label": 0
                },
                {
                    "sent": "And then we can think of the set of models we can represent with graphical models as being this large group.",
                    "label": 0
                },
                {
                    "sent": "Here the set we can we can represent with directed models.",
                    "label": 0
                },
                {
                    "sent": "You can think of this group here and the set we can represent with undirected models there.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 1
                },
                {
                    "sent": "Which cannot be represented with graphical models, yes.",
                    "label": 0
                },
                {
                    "sent": "You mean one that's right here?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't think I can give you an example of that.",
                    "label": 0
                },
                {
                    "sent": "Actually, right now I'll think about it.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll come back.",
                    "label": 0
                },
                {
                    "sent": "Yeah, see my worldview.",
                    "label": 0
                },
                {
                    "sent": "Everything can be represented as a graphical model, but I know for a fact that it actually isn't true, but these are usually.",
                    "label": 0
                },
                {
                    "sent": "These are pretty borderline cases like you usually you're playing with things like the you know you have measure 0 or little zero probabilities part of your space that you can't actually represent things like this.",
                    "label": 0
                },
                {
                    "sent": "So usually these kinds of things that they exist, but they aren't usually things that we were too concerned with in practice.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so now we're talking talk about conditional independence, and specifically, we're talking about undirected graphical models and their relationship to conditional independence.",
                    "label": 0
                },
                {
                    "sent": "So here we have four variables, and I've written here 2 conditional independent statements.",
                    "label": 0
                },
                {
                    "sent": "Here we have a is independent of C given B&D, and here we have B is independent of D given A&C.",
                    "label": 0
                },
                {
                    "sent": "Alright, and now the point of this slide is basically just to say it's hard to think about some forms of conditional in dependencies using directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "So here are two attempts to do just that to try to model this set of conditional independencies without introducing any new variables which won't actually help in this case anyway.",
                    "label": 0
                },
                {
                    "sent": "And this so if you were just to think.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe this model represents that.",
                    "label": 0
                },
                {
                    "sent": "In fact, actually, I mean this is something you might not know about if you haven't studied directed graphical models before, but this models there's a few things going on here that that if you've heard about D separation, this is actually going to tell you how to get from this graph into this set of conditional independent statements, but very briefly, what I can say is that given B&DA, the paths are blocked from A to C here, so we know that, given B&DA is independent of C, that's what the separation means.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you're blocking the path.",
                    "label": 0
                },
                {
                    "sent": "But in directed graphical models, it gets a little bit more tricky than that, because in this case that's why we have, for example, this relationship.",
                    "label": 0
                },
                {
                    "sent": "So we've got one of the two here, right here.",
                    "label": 0
                },
                {
                    "sent": "We've got this one, but now the problem is, what about the next one?",
                    "label": 0
                },
                {
                    "sent": "And this is where directed graphical models get tricky.",
                    "label": 1
                },
                {
                    "sent": "It's because if you condition what you want is the statement that if I condition on A&CB&D become independent right, whereas directed graphical model, you can't actually do that.",
                    "label": 0
                },
                {
                    "sent": "And the reason is is because you get this explaining away effect with C. This is if you've ever heard of explaining away.",
                    "label": 0
                },
                {
                    "sent": "This is what we're talking about.",
                    "label": 0
                },
                {
                    "sent": "What that means is that if I observe C, which is, which is the child of two parent nodes in this case, then those parent nodes become coupled.",
                    "label": 0
                },
                {
                    "sent": "They become non independent, whereas if I just don't observe C then they remain potentially independent if all other.",
                    "label": 0
                },
                {
                    "sent": "If they're independent through all other paths.",
                    "label": 0
                },
                {
                    "sent": "So in this case I can render B&D independent by just conditioning on a 'cause this is kind of one of these normal relationships where if I block the path from B to D through this.",
                    "label": 0
                },
                {
                    "sent": "Pathway, then they are independent.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's like the opposite.",
                    "label": 0
                },
                {
                    "sent": "When you've got this this Arrowhead to Arrowhead connection through a variable or by the way, any of its children.",
                    "label": 0
                },
                {
                    "sent": "If you condition on any of those variables, it actually opens that pathway.",
                    "label": 0
                },
                {
                    "sent": "So here we have a context where where we can't condition on C, so using that graph structure we can't actually get the model with a set of conditional independencies.",
                    "label": 0
                },
                {
                    "sent": "We're looking for, which is this set up here.",
                    "label": 0
                },
                {
                    "sent": "Alright, so here's another attempt.",
                    "label": 0
                },
                {
                    "sent": "So this is a C&BD here, so we have B&D.",
                    "label": 0
                },
                {
                    "sent": "Here is the children of A&C.",
                    "label": 0
                },
                {
                    "sent": "Now here we have the statement that again we now can get this relationship right because now see is in A is in this parent relationship like a was over here.",
                    "label": 0
                },
                {
                    "sent": "So now we can basically get this statement that we were missing before that conditioned on A&CB is independent of D. We've got that now.",
                    "label": 0
                },
                {
                    "sent": "But now the problem is now both of these two are in this relationship.",
                    "label": 0
                },
                {
                    "sent": "This child, this explaining away effect with A&B.",
                    "label": 0
                },
                {
                    "sent": "So if I were to condition on either one of them or both of them, these two become non conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "So what I have to do is I have to basically not condition on either one of them and that gives me this conditional independence statement right here.",
                    "label": 0
                },
                {
                    "sent": "So these are the conditional in dependencies that this graph represent.",
                    "label": 0
                },
                {
                    "sent": "You can see this is not the same as this, so does anyone want to try to another directed graphical model?",
                    "label": 0
                },
                {
                    "sent": "Maybe they have something in mind that would actually represent this?",
                    "label": 0
                },
                {
                    "sent": "OK, anybody who maybe have some have seen undirected graphical models want to try for undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "This one without edges exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is the undirected graphical model that gives you that structure will see in a minute why, but basically it in a nutshell it's because this comp it's it's.",
                    "label": 1
                },
                {
                    "sent": "It's a very similar sort of dynamic with the conditional independence ease.",
                    "label": 0
                },
                {
                    "sent": "It's just this.",
                    "label": 0
                },
                {
                    "sent": "Explaining away effect doesn't exist basically so as soon as I condition on A&C, I've blocked all paths from B to D will go over this in more detail, but that's basically the structure here.",
                    "label": 0
                },
                {
                    "sent": "Is there any questions up to this point?",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, I wouldn't necessarily say that so.",
                    "label": 0
                },
                {
                    "sent": "And it's it's not.",
                    "label": 0
                },
                {
                    "sent": "It's a question of OK, so more independence.",
                    "label": 0
                },
                {
                    "sent": "Do you mean like more sets of in dependencies?",
                    "label": 0
                },
                {
                    "sent": "So I'll give you an example of this is kind of the Canonical example of a graphical model that can be represented in an undirected model that can't be represented in a directed model.",
                    "label": 0
                },
                {
                    "sent": "The example of the opposite.",
                    "label": 0
                },
                {
                    "sent": "Is take this model and just remove a right this exact explaining away effect B&D are independent.",
                    "label": 0
                },
                {
                    "sent": "And that's and then you have this relationship where you're right.",
                    "label": 0
                },
                {
                    "sent": "So B&D are marginally independent, but C is dependent on weight.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Given see, they're not independent, right right?",
                    "label": 0
                },
                {
                    "sent": "You can't represent that that structure with a directed with an undirected graphical model, so they're complementary, right?",
                    "label": 0
                },
                {
                    "sent": "That's why when we go back to our circle here, this is sort of what we were really talking about.",
                    "label": 0
                },
                {
                    "sent": "Is this space right here?",
                    "label": 0
                },
                {
                    "sent": "The kinds of structures that exist here.",
                    "label": 0
                },
                {
                    "sent": "And that last thing was an example of the kind of model that represents here, right?",
                    "label": 0
                },
                {
                    "sent": "They can't be represented.",
                    "label": 1
                },
                {
                    "sent": "This is sort of.",
                    "label": 0
                },
                {
                    "sent": "If you like the space of conditional Independencies, and that can be represented the directed model, and one thing you could ask is what is in between here.",
                    "label": 0
                },
                {
                    "sent": "We'll get to that in a few slides.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where I'm sorry where are you?",
                    "label": 0
                },
                {
                    "sent": "Yes, absolutely you can actually.",
                    "label": 0
                },
                {
                    "sent": "In fact, these things are our innocence common now and they were.",
                    "label": 0
                },
                {
                    "sent": "They were really developed in some sense, or at least made more common in the deep learning literature actually.",
                    "label": 0
                },
                {
                    "sent": "So you can think of a actually hung like who's going to be speaking after me is is going to talk about stacking RBM's and one way to interpret that.",
                    "label": 0
                },
                {
                    "sent": "The structure you get out of that is has exactly one of these hybrid models right where because the bottom layers are going to be directed in the top layer is going to be undirected.",
                    "label": 0
                },
                {
                    "sent": "So they're not very common outside of the deep learning literature, but they're actually quite common in the deep learning literature, yeah?",
                    "label": 0
                },
                {
                    "sent": "Very good, very good question.",
                    "label": 0
                },
                {
                    "sent": "So so we're going to get into some of that sort of in due course, but basically they have very different properties in almost every way you can look at it.",
                    "label": 0
                },
                {
                    "sent": "So one example is actually this right here, right?",
                    "label": 0
                },
                {
                    "sent": "So so I'll just explain this and then we'll come back and finish off your question, 'cause there's actually a few elements to it that will go over, but it's good to discuss right now.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here's a more.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we think of this as a bit of a toy example that we wouldn't likely see and in.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Life may be something that we we care about.",
                    "label": 0
                },
                {
                    "sent": "Here's an example that's a little bit more important, so here we have a 2D lattice structure, and these are pretty common when trying to model things like images, right?",
                    "label": 1
                },
                {
                    "sent": "This is like the two D MRF that used to see quite frequently in the vision community.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of a directed model version of the same model, right?",
                    "label": 0
                },
                {
                    "sent": "So here's an example where.",
                    "label": 0
                },
                {
                    "sent": "The relationship between pixels.",
                    "label": 0
                },
                {
                    "sent": "We would like that to be relatively symmetric, right?",
                    "label": 0
                },
                {
                    "sent": "There's a few examples of models that do that explicitly.",
                    "label": 0
                },
                {
                    "sent": "Do not have that relationship like actually made what you thought you heard about yesterday is an example of that, but for the most part, models like this tend to be preferred for modeling the interaction between things like pixels, because there the notion that one pixel is the parent of another can be awkward, right?",
                    "label": 1
                },
                {
                    "sent": "And we'll see in a bit more detail what we can mean by that by it's awkward, but here's a real life example, so this, by the way, is just.",
                    "label": 0
                },
                {
                    "sent": "An image I picked off where the task here is image segmentation, so you're trying to give each pixel a label.",
                    "label": 1
                },
                {
                    "sent": "Here we have so labels for bicycle and labels for person here that are that are labeled.",
                    "label": 0
                },
                {
                    "sent": "And the goal here is to basically, yeah, just just label as much of the image as you can, or perhaps some subset of classes.",
                    "label": 0
                },
                {
                    "sent": "And this structure for that task, the structure seems to make much more sense from the point of view of just being able to model our intuitions of two, how pixels should relate to pixels.",
                    "label": 0
                },
                {
                    "sent": "Now the rest of your question how when other like what are the other advantages, disadvantages of directed versus undirected?",
                    "label": 0
                },
                {
                    "sent": "So there's two other things that you really want to worry about when you're doing anything with graphical models.",
                    "label": 0
                },
                {
                    "sent": "It's is it easy in our context anyways, is it easy to do inference in this model?",
                    "label": 0
                },
                {
                    "sent": "Presumably you're going to have latent variables or things you don't observe.",
                    "label": 0
                },
                {
                    "sent": "That's most of the point of doing graphical models is that you've got some variables you observe and some variables you don't observe, and then you want to know how to do the variables you observe once you've observed them.",
                    "label": 0
                },
                {
                    "sent": "Change your distribution about the variables you haven't seen yet, right?",
                    "label": 0
                },
                {
                    "sent": "That's called and that question.",
                    "label": 0
                },
                {
                    "sent": "You're asking the computation associated with that we call inference.",
                    "label": 0
                },
                {
                    "sent": "And that is very different between the two different models.",
                    "label": 0
                },
                {
                    "sent": "That process of inference turns out to be very different, so depending on the structure.",
                    "label": 0
                },
                {
                    "sent": "Usually you'd go with one or the other, and often it's much more complicated in an undirected.",
                    "label": 0
                },
                {
                    "sent": "Sorry in a directed graphical model than an undirected graphical model, for reasons we'll get into now.",
                    "label": 0
                },
                {
                    "sent": "The flip side of that is learning learning turns out to be, and this is another thing will get into in the course of today.",
                    "label": 0
                },
                {
                    "sent": "Learning turns out to be much more complicated in the context of undirected graphical models undirected graphical models.",
                    "label": 1
                },
                {
                    "sent": "This is at least in the case where you've observed all the variables.",
                    "label": 0
                },
                {
                    "sent": "In that case in the directed graphical model, you actually have.",
                    "label": 0
                },
                {
                    "sent": "This fortunate thing happened where everything decomposes and you've just got.",
                    "label": 0
                },
                {
                    "sent": "You've just got to estimate the parameters of your little conditional probabilities, right?",
                    "label": 0
                },
                {
                    "sent": "So in the case of discrete variables, you're just filling in values for tables.",
                    "label": 0
                },
                {
                    "sent": "Very simple because they normalize right?",
                    "label": 0
                },
                {
                    "sent": "They do this local normalization that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Undirected graphical models don't do that local normalization, and so you've got this thing that we've got this problem that we're going to talk about that comes up, so learning generally harder and undirected graphical models inference generally easier in undirected graphical models would be the first order of proximation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so so how can we formalize this idea of conditional independence that we talked about a bit in terms of undirected graphical models?",
                    "label": 1
                },
                {
                    "sent": "So we have three sets.",
                    "label": 1
                },
                {
                    "sent": "In this case, we're going to do three sets of nodes here, so we've just taken like a little piece of R2D lattice.",
                    "label": 0
                },
                {
                    "sent": "Here we're going to set A and set B.",
                    "label": 0
                },
                {
                    "sent": "We have the following property that we say that a is independent of B given C if C separates A&B in the graph.",
                    "label": 1
                },
                {
                    "sent": "So here we have our graph structure and we've sort of alluded to this already, but here we have a bit more of a.",
                    "label": 0
                },
                {
                    "sent": "Formal notion of that.",
                    "label": 0
                },
                {
                    "sent": "So if I observe if I condition on B or the all the elements in C that is X13 here and X2 three, this is another way I observe those variables.",
                    "label": 0
                },
                {
                    "sent": "And I have this graph structure that means that these variables over here are conditionally independent of these, right?",
                    "label": 0
                },
                {
                    "sent": "And all I've done is just block the path.",
                    "label": 0
                },
                {
                    "sent": "Through them, right?",
                    "label": 0
                },
                {
                    "sent": "So that's basically the notion here is that if our conditioning set blocks all paths between two variables between all sets of variables here an all sets of variables here, then we can say that these two sets are conditionally independent under this graph.",
                    "label": 0
                },
                {
                    "sent": "Other questions about that it's a very important notion.",
                    "label": 0
                },
                {
                    "sent": "It's something that's basically the core of undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "From at least you know in terms of inference and writing things down.",
                    "label": 0
                },
                {
                    "sent": "OK, everybody's got that good.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so now if we sort of take that same basic idea and we extend it to our larger model, we can develop things like an idea of a Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "So with the Markov blanket is it's basically the smallest set of units.",
                    "label": 1
                },
                {
                    "sent": "Say we take a particular unit like X2 three.",
                    "label": 0
                },
                {
                    "sent": "Here it's Markov blanket is the minimal set of nodes that we have to condition on set of variables.",
                    "label": 0
                },
                {
                    "sent": "We have to condition in order to render that unit independent of all other units.",
                    "label": 0
                },
                {
                    "sent": "So we sort of put down this blanket and now all of a sudden this guy doesn't care about what the values of all of these are.",
                    "label": 0
                },
                {
                    "sent": "Because we've observed all its neighbors.",
                    "label": 0
                },
                {
                    "sent": "That's how when we think about this, the way this guy interacts with the world around it is through these four variable.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "So once you fix those values, it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Knowing anything about any of these other variables is irrelevant to that to that variable.",
                    "label": 0
                },
                {
                    "sent": "To this variable right here.",
                    "label": 0
                },
                {
                    "sent": "OK, that idea is called the Markov blanket.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we take that same basic structure and we ask well, what is it?",
                    "label": 0
                },
                {
                    "sent": "The Markov blanket for the case of that 2D causal structure, though they directed graphical model that we saw, we see something a little bit different here, right?",
                    "label": 0
                },
                {
                    "sent": "So here's our X3.",
                    "label": 0
                },
                {
                    "sent": "If we look at what the Markov blanket is of this in the directed graphical model sense, there's a rule you have to apply, which is the things that are in in for directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "The things that are in your Markov blanket are your parents, your children and the other parents.",
                    "label": 0
                },
                {
                    "sent": "Of your children, this is again because of this conditioning of this, this explaining away effect, right?",
                    "label": 0
                },
                {
                    "sent": "Once you can, sorry.",
                    "label": 0
                },
                {
                    "sent": "It's the no no.",
                    "label": 0
                },
                {
                    "sent": "It's the other parents of your children, but in this case it happens to be both.",
                    "label": 0
                },
                {
                    "sent": "But the one you need to condition on is in general the rule to apply is other parents of your children.",
                    "label": 0
                },
                {
                    "sent": "It's good, though I'm glad you so.",
                    "label": 0
                },
                {
                    "sent": "So now I guess so anyway, the point here is that.",
                    "label": 0
                },
                {
                    "sent": "This Markov blanket seems to me if we care about things like pixels makes a whole lot more sense than this one, right?",
                    "label": 0
                },
                {
                    "sent": "'cause we were sort of arbitrarily picking these two corners to condition on is something important to tell us about what this guy is doing right when you think about pixels, this feels very asymmetric, like if these guys are important, why not these for example.",
                    "label": 0
                },
                {
                    "sent": "So as I'm just reinforcing this idea that sometimes undirected graphical models makes sense for whatever the context of whatever you're modeling.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case pixels another case where this all becomes important.",
                    "label": 0
                },
                {
                    "sent": "And will motivate PBM's later.",
                    "label": 0
                },
                {
                    "sent": "Is another example.",
                    "label": 0
                },
                {
                    "sent": "Which I will draw now.",
                    "label": 0
                },
                {
                    "sent": "Right, so so this is an example of images.",
                    "label": 0
                },
                {
                    "sent": "We're not going to see a whole lot of that, but we will see models that look a bit like this.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine I have two sets of random variables.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm just drawing.",
                    "label": 0
                },
                {
                    "sent": "Two sets of random variables and I have this kind of relationship.",
                    "label": 0
                },
                {
                    "sent": "And then again.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "All right, right?",
                    "label": 0
                },
                {
                    "sent": "That's model number that's directed model right directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "By the way, another way we often call these kinds of models is dag's directed acyclic graphs, and the reason for that is because if you're going to start drawing arrows on these things, you can't draw a directed cycle.",
                    "label": 0
                },
                {
                    "sent": "Your model can have cycles.",
                    "label": 0
                },
                {
                    "sent": "This model, for example, has lots of cycles, right?",
                    "label": 0
                },
                {
                    "sent": "You can go from here to here to here, to here, to back to here, right in an undirected cycle.",
                    "label": 0
                },
                {
                    "sent": "If you have a directed cycle, that means you're following a loop.",
                    "label": 1
                },
                {
                    "sent": "You in the direction of all the arrows.",
                    "label": 0
                },
                {
                    "sent": "Then you have something that's basically nonsensical ale depend on BBL.",
                    "label": 0
                },
                {
                    "sent": "Depend on on CNC bilderback depend on A and then in a causal cycle let's say so that you typically don't have in a case where this might make sense.",
                    "label": 0
                },
                {
                    "sent": "Or you might feel an intuition about doing that is often in sequential modeling, but really there you've sort of broken things across time, so it's like the next day will depend on the previous a affectively, that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So this is our.",
                    "label": 0
                },
                {
                    "sent": "Dag directed model.",
                    "label": 0
                },
                {
                    "sent": "And now here we have our undirected graphical model, and it's going to be basically exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "Accept.",
                    "label": 0
                },
                {
                    "sent": "Undirected.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right now, now this is a kind of model.",
                    "label": 0
                },
                {
                    "sent": "We see all the time.",
                    "label": 0
                },
                {
                    "sent": "In fact, this is we will see effectively in GBM this model right here is also something we see often.",
                    "label": 0
                },
                {
                    "sent": "Machine learning, right?",
                    "label": 0
                },
                {
                    "sent": "So sparse coding can be interpreted as this kind of graphical model.",
                    "label": 1
                },
                {
                    "sent": "Or even there's the generative model for principle components analysis can be interpreted with this kind of graphical model.",
                    "label": 0
                },
                {
                    "sent": "Actually, that one actually can also be interpreted as that kind of graphical model, but that's a special case.",
                    "label": 0
                },
                {
                    "sent": "But the yeah, so we have.",
                    "label": 0
                },
                {
                    "sent": "We have these two graphical models that appear often literature, very different properties, and it's just about their nature.",
                    "label": 0
                },
                {
                    "sent": "The fact that this is undirected and that's directed.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are the differences?",
                    "label": 0
                },
                {
                    "sent": "Let's imagine these are called latent variable models.",
                    "label": 0
                },
                {
                    "sent": "Because we have like these things which you can think about is latent causes.",
                    "label": 0
                },
                {
                    "sent": "These ads are saying things we don't observe in the world.",
                    "label": 0
                },
                {
                    "sent": "X is are things we observe in the world, right?",
                    "label": 0
                },
                {
                    "sent": "And so now we're going to play this game.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine we observe X in both cases and see what kind of conditional independencies we have in the Zeds.",
                    "label": 0
                },
                {
                    "sent": "Alright, so so who can tell me?",
                    "label": 0
                },
                {
                    "sent": "First over here, let's condition on all the X is and can somebody tell me what happens with the zeds?",
                    "label": 0
                },
                {
                    "sent": "They are independent.",
                    "label": 0
                },
                {
                    "sent": "There are conditionally independent why.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "Because because these guys are not actually directly connected to each other, but they are connected to each other through the X.",
                    "label": 0
                },
                {
                    "sent": "Is we condition on the X is and now all paths between these ads are blocked.",
                    "label": 0
                },
                {
                    "sent": "So we have all pairs of of Zeds.",
                    "label": 0
                },
                {
                    "sent": "Here are conditionally independent good.",
                    "label": 0
                },
                {
                    "sent": "OK, that's generally a property we want.",
                    "label": 0
                },
                {
                    "sent": "In that case, right?",
                    "label": 0
                },
                {
                    "sent": "Because what that means is now we can actually because they don't interact.",
                    "label": 0
                },
                {
                    "sent": "Inference is going to be easy because we don't care what the values of the rest of these are.",
                    "label": 0
                },
                {
                    "sent": "When we're computing the value of this one, this is turned out to be very, very important to us.",
                    "label": 0
                },
                {
                    "sent": "When we were talking about doing inference in the context of a model PBM, this model, for example, what happens I observe all the X is.",
                    "label": 0
                },
                {
                    "sent": "They are all dependent, right?",
                    "label": 0
                },
                {
                    "sent": "Because of that you got this massive explaining away effect going on right?",
                    "label": 0
                },
                {
                    "sent": "All of these guys can be interpreted as causes for any one of these.",
                    "label": 0
                },
                {
                    "sent": "So as soon as you condition on something, they're all linked, basically implicitly.",
                    "label": 0
                },
                {
                    "sent": "So Interestingly, if you don't observe this, they're all independent.",
                    "label": 1
                },
                {
                    "sent": "That is not true over there, right?",
                    "label": 0
                },
                {
                    "sent": "So they have this different kind of relationship, a consequence of that, by the way, is that this model is far simpler to sample from, because all you have to do is first.",
                    "label": 0
                },
                {
                    "sent": "Usually you'd write this down because of the you have these CPD relationships.",
                    "label": 0
                },
                {
                    "sent": "You would write this down as P, Zedd, where the presumably independent for all these P of X given zed.",
                    "label": 0
                },
                {
                    "sent": "Right now I can just use this way of describing the model as a way to sample from it as well.",
                    "label": 0
                },
                {
                    "sent": "I sample independently from all those ads 1st and then conditional those values.",
                    "label": 0
                },
                {
                    "sent": "I sampled the X.",
                    "label": 0
                },
                {
                    "sent": "Very simple, there's no interactions.",
                    "label": 0
                },
                {
                    "sent": "You can sample given all of these guys.",
                    "label": 1
                },
                {
                    "sent": "These guys are all independent.",
                    "label": 0
                },
                {
                    "sent": "You can sample from them easilly.",
                    "label": 0
                },
                {
                    "sent": "We do not have that kind of relationship with our undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "So these are two models that are very popular in the machine learning community.",
                    "label": 0
                },
                {
                    "sent": "There only difference basically in terms of the apology is there is whether they are directed or undirected and you end up having very different kinds of consequences for that different behaviors.",
                    "label": 0
                },
                {
                    "sent": "And we'll see in the context of the PBM, how we get around some of these issues that pop up, especially when we talk about doing sampling from this model, and also how we talk about learning in that model alright.",
                    "label": 0
                },
                {
                    "sent": "Oh absolutely, I mean, yeah, I've drawn the Markov blanket for that graph.",
                    "label": 0
                },
                {
                    "sent": "If I decide to include sort of diagonal elements as my neighbors, and if I draw that graph, then yeah, they would have to be in the Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "Yeah, absolutely.",
                    "label": 0
                },
                {
                    "sent": "It's the oh so the Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "It's just that's a definition of term, so one of the things I should clear up right now.",
                    "label": 0
                },
                {
                    "sent": "There's gonna be a few.",
                    "label": 0
                },
                {
                    "sent": "There's this.",
                    "label": 1
                },
                {
                    "sent": "This talk is a little terminology heavy, so one of the one of these terminologies we're going to use this notion of the Markov blanket, and we're not going to use it very much, so it's if you don't happen to know all the details of it.",
                    "label": 0
                },
                {
                    "sent": "That's fine, but it is defined as the minimal set of units, so just so that it's unique 'cause otherwise.",
                    "label": 0
                },
                {
                    "sent": "It's like all of the units, it's all subsets.",
                    "label": 0
                },
                {
                    "sent": "Up until those four L Subs also all sets that include those four units and not X2 three would be defined as the Markov like it.",
                    "label": 0
                },
                {
                    "sent": "So that's not very useful definition, yeah?",
                    "label": 0
                },
                {
                    "sent": "Sure, yeah.",
                    "label": 0
                },
                {
                    "sent": "In that kind of in a graph structure where you have so.",
                    "label": 0
                },
                {
                    "sent": "So I don't usually think about the topology too much when I'm thinking about which kind of graph structure I want to use.",
                    "label": 0
                },
                {
                    "sent": "It's more like I think it makes more sense to think about the nature of the problem, like it just happens that if you end up having a problem that it's kind of causal, relationships between things usually directed arcs make more sense.",
                    "label": 0
                },
                {
                    "sent": "You'll have an easier time modeling the process because you're going to be able to follow your intuition of what causes what.",
                    "label": 0
                },
                {
                    "sent": "Right whereas in the case, for example like pixels, we have no notion that pixel you know a pixel to the left has caused the pixel to the right right?",
                    "label": 1
                },
                {
                    "sent": "So modeling them as just being correlated to each other and being sort of agnostic as to the causal relationship makes more sense there.",
                    "label": 0
                },
                {
                    "sent": "So so in terms of the actual topology you describe, I can't really say, at least I mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I can't really say until you unless you tell me.",
                    "label": 0
                },
                {
                    "sent": "For example, like some other elements of the problem.",
                    "label": 0
                },
                {
                    "sent": "Like is it really kind of a difficult learning problem or is it a difficult inference problem?",
                    "label": 0
                },
                {
                    "sent": "These things might help me decide which one I want to use.",
                    "label": 0
                },
                {
                    "sent": "Sorry, which one would be harder.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "What what is I missed?",
                    "label": 0
                },
                {
                    "sent": "The very first thing you said in the sense of what is the topic?",
                    "label": 0
                },
                {
                    "sent": "That what are you trying to model?",
                    "label": 0
                },
                {
                    "sent": "I know that apology, but what was what is the subject of what you're trying to model?",
                    "label": 0
                },
                {
                    "sent": "Say.",
                    "label": 0
                },
                {
                    "sent": "Rumors are OK. Ha.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean it depends, right?",
                    "label": 0
                },
                {
                    "sent": "If so, the way I would think about it is do you want to model rumors as being one directional or sort of associative like two people exchange rumors, or one person really sort of propagating rumors to another person, right?",
                    "label": 0
                },
                {
                    "sent": "That would to me whichever one I want to use.",
                    "label": 0
                },
                {
                    "sent": "There would be what determines which graph, which type of graphical makes more sense?",
                    "label": 0
                },
                {
                    "sent": "So I mean, you can always, then the question would be like.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe one makes more sense, but it's really hard to work with, so I'll use the other.",
                    "label": 0
                },
                {
                    "sent": "I mean that can always be an option.",
                    "label": 0
                },
                {
                    "sent": "That's kind of you.",
                    "label": 0
                },
                {
                    "sent": "Always make approximations when you do modeling, so so that could be something you would end up doing all right.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's let's move on OK, yeah, so we've talked about a lot about conditional.",
                    "label": 0
                },
                {
                    "sent": "Independence, we've talked about a lot about the graph structure.",
                    "label": 0
                },
                {
                    "sent": "Now let's start populating these graphs with parameters right?",
                    "label": 0
                },
                {
                    "sent": "Let's start connecting these conditional independence properties two probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So in the context of directed graphical models, I've already mentioned that the way we parameterized these things As for a given given basically given unit here.",
                    "label": 1
                },
                {
                    "sent": "Oh, actually this is this is wrong.",
                    "label": 0
                },
                {
                    "sent": "This is PAB given a is actually what this corresponds to here, But yeah, that would be the conditional density that would be associated with this graph.",
                    "label": 0
                },
                {
                    "sent": "So that's again PFB given a.",
                    "label": 0
                },
                {
                    "sent": "In general, what we have is we can write down any joint probability distribution is equal to the product over the individual elements condition on their parents, right?",
                    "label": 1
                },
                {
                    "sent": "This is a way we can factorize graphs according to a directed graphical model.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the context of undirected graphical models, an now is a good time for me to mention undirected graphical models, Markov networks, Markov, random fields are all basically referring to the same thing.",
                    "label": 0
                },
                {
                    "sent": "The consequences of having these different names is basically just because they've sort of emerged independent communities.",
                    "label": 0
                },
                {
                    "sent": "Right, so the way we parameterize these models is with these things called factors, and these factors you can think of it as a generalization of these.",
                    "label": 0
                },
                {
                    "sent": "Conditional probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So so this for example.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're basically modeling it with this with this factor here, and we can refer to this as this combination as a clique.",
                    "label": 0
                },
                {
                    "sent": "A clique is just a set of units that are complete under the graph, so any subset of units that who form a complete graph, meaning all of these elements of the subset are connected to each other.",
                    "label": 0
                },
                {
                    "sent": "That's what forms a clique.",
                    "label": 0
                },
                {
                    "sent": "So right, so let's just so we can kind of formalize this and say that.",
                    "label": 0
                },
                {
                    "sent": "Let's see be any set of ABA Capital C is a set of cliques.",
                    "label": 1
                },
                {
                    "sent": "Then for a subscription for a given clique, this is just going to be an element for click.",
                    "label": 0
                },
                {
                    "sent": "This is important notation 'cause we're going to carry this through a little bit.",
                    "label": 0
                },
                {
                    "sent": "We define a factor also called a potential function or clique potential, and we're going to refer to this fee.",
                    "label": 1
                },
                {
                    "sent": "Sub C Here as a non negative function which Maps basically this set of units.",
                    "label": 0
                },
                {
                    "sent": "Now here X is just a set of variables that are associated with that clique.",
                    "label": 0
                },
                {
                    "sent": "Slight abuse of notation here, but it's much simpler to think of to write things down this way.",
                    "label": 0
                },
                {
                    "sent": "So this right here is going to be some set of factors which are associated with a given click, right?",
                    "label": 0
                },
                {
                    "sent": "And so a unit.",
                    "label": 0
                },
                {
                    "sent": "For example, if I had another another edge here and then a C down here a variable CB would be involved in two clicks, right?",
                    "label": 0
                },
                {
                    "sent": "So this is not a clicks or not?",
                    "label": 0
                },
                {
                    "sent": "Don't think of it as a partitioning of your set of variables, it's just the set of all interacting units in a way you can think of it that way, alright?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've now defined basically what is our building block for undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "Now we can go and think about what.",
                    "label": 0
                },
                {
                    "sent": "How do we express a joint probability distribution with an undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "And we do that essentially in a sort of a similar way as with the directed graphical model, which is just it's a product over these clique potentials.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "And this said right, this set is are what we call the partition function.",
                    "label": 1
                },
                {
                    "sent": "We've already mentioned this.",
                    "label": 0
                },
                {
                    "sent": "This is essentially are normalizing constant, and this actually adds an awful lot of complexity to undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "So where where this file here is very nice?",
                    "label": 0
                },
                {
                    "sent": "Because it doesn't have to be normalized, it's very general.",
                    "label": 0
                },
                {
                    "sent": "We sort of have to pay for that generality in specifying in basically having to define this to be a proper probability distribution that normalizes proper joint probability distribution.",
                    "label": 0
                },
                {
                    "sent": "We have this said.",
                    "label": 0
                },
                {
                    "sent": "We have to have this thing normalized and in general this thing is basically just some overall variables of our of.",
                    "label": 0
                },
                {
                    "sent": "You know, the unnormalized portion.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now if we think of back to our toy example here on four variables, we can write down the probability distribution associated with that or joint probability distribution.",
                    "label": 0
                },
                {
                    "sent": "This way, right?",
                    "label": 0
                },
                {
                    "sent": "Because we have these four variables here.",
                    "label": 0
                },
                {
                    "sent": "And we're going to write them down, so every edge essentially is defining in this.",
                    "label": 0
                },
                {
                    "sent": "In this case, a clique.",
                    "label": 0
                },
                {
                    "sent": "So because it's a clique, because I can't add any new variables, so I have four cliques in this graph, right?",
                    "label": 0
                },
                {
                    "sent": "I have a click here.",
                    "label": 0
                },
                {
                    "sent": "I have a click here.",
                    "label": 0
                },
                {
                    "sent": "I have a click here and I have a click here.",
                    "label": 0
                },
                {
                    "sent": "We know that that's the set of cliques in this graph, because I can't add any node to any one of those cliques and have it still be a clique.",
                    "label": 0
                },
                {
                    "sent": "So if I were to do that, for example, if I were to say I've gotta click here, maybe I want to add D to that click like maybe I want a B, D. Well if I add D then there's actually it's not a complete graph anymore, right?",
                    "label": 0
                },
                {
                    "sent": "It's missing this edge.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No maximum clique is actually something else, so I'm talking specifically about clicks, but in right here in a minute I'm really talking about what a maximal clique is.",
                    "label": 0
                },
                {
                    "sent": "A maximal clique is a context where you can add no other unit and have it still be a clique.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, you're right, actually yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Good point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, a can be a click.",
                    "label": 0
                },
                {
                    "sent": "You can absorb it into the other cliques as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right, you could have a clique unitary clicks on all of these variables is a good point actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so right.",
                    "label": 0
                },
                {
                    "sent": "So we have a joint probability distribution defined as a product over these clicks.",
                    "label": 0
                },
                {
                    "sent": "With our normalizing constant.",
                    "label": 1
                },
                {
                    "sent": "And that's just given by again, the product over the leaks summed over all of the variables.",
                    "label": 0
                },
                {
                    "sent": "Alright, any other questions on that?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is a maximal clique, right?",
                    "label": 1
                },
                {
                    "sent": "So that was the question came up so a click.",
                    "label": 0
                },
                {
                    "sent": "First of all.",
                    "label": 1
                },
                {
                    "sent": "I mean I said this in words, but it is just a subset of nodes whose induced sub graph, right?",
                    "label": 0
                },
                {
                    "sent": "So if we just take the set, the set of nodes and then and take its graph that was associated with that set of nodes, it's complete, meaning that all units connect to all other units.",
                    "label": 0
                },
                {
                    "sent": "Now maximal clique is 1 where you cannot add anymore nodes.",
                    "label": 1
                },
                {
                    "sent": "OK, so the difference in this case there isn't really much difference between those two, because every clique is a maximal clique.",
                    "label": 0
                },
                {
                    "sent": "Other than the unitary clicks here, if I had to click on ABC or D, then that would not be a maximal clique over here.",
                    "label": 0
                },
                {
                    "sent": "For example, we have a bit of a different case here.",
                    "label": 0
                },
                {
                    "sent": "I can imagine I have two maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "I have BABCD here and I have a BD.",
                    "label": 0
                },
                {
                    "sent": "If I added any, if I added C for example to the clique Abd, this is no longer a clique, so that's so.",
                    "label": 0
                },
                {
                    "sent": "I've grown those clicks as far as I can and that defines a maximal clique.",
                    "label": 0
                },
                {
                    "sent": "In this context.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so these are just two examples of cliques.",
                    "label": 0
                },
                {
                    "sent": "In fact, maximal cliques in these cases, so these dotted lines here are such essentially encircling the cliques.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now an interesting fact so far what we've talked about is a parameterisation.",
                    "label": 0
                },
                {
                    "sent": "So we talk about joint probability models with us last couple slides on that.",
                    "label": 0
                },
                {
                    "sent": "Before that we were talking about conditional independent structures on a graph.",
                    "label": 0
                },
                {
                    "sent": "So one question is like how do you actually relate these two out?",
                    "label": 0
                },
                {
                    "sent": "We've sort of slipped that in that were like, really kind of these things are sort of the same thing, but there actually, you know there's work to be done to do that.",
                    "label": 0
                },
                {
                    "sent": "In fact, this is what's the famous Hammersley Clifford theorem is essentially what relates these two.",
                    "label": 0
                },
                {
                    "sent": "So for any positive distribution it has to be positive, meaning that.",
                    "label": 1
                },
                {
                    "sent": "It has to give some nonzero probability mass to every combination of variables, or else actually this theorem doesn't apply whose conditional independence can be represented as an undirected graph.",
                    "label": 1
                },
                {
                    "sent": "It can be parameterized by a product of factors that basically are consistent with that graph.",
                    "label": 1
                },
                {
                    "sent": "Right, so this is essentially a statement of Hammersley, Clifford, and it basically allows us to connect joint probability models and these graphs together right so that's what we're going to be basically exploiting when we talk about these undirected graphical models.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so one thing that you might be wondering is what is this part in the middle here?",
                    "label": 0
                },
                {
                    "sent": "Like is what kinds of things are here, right?",
                    "label": 0
                },
                {
                    "sent": "So I mentioned so again, these are the set of conditional independent statements.",
                    "label": 0
                },
                {
                    "sent": "What set of conditional independent statements are represented can be both represented by directed model and equivalently by an undirected model well.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Turns out the answer is chordal graphs.",
                    "label": 0
                },
                {
                    "sent": "Anything that people can represent it as a chordal, undirected graph can also be represented without any loss of information as a directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "A chordal graph is just one where all undirected cycles again in this is in the undirected model A4.",
                    "label": 1
                },
                {
                    "sent": "More vertices have a cord.",
                    "label": 1
                },
                {
                    "sent": "This is the notation heavy part.",
                    "label": 0
                },
                {
                    "sent": "What's Accord accord is just an edge that is not part of that cycle but connects two vertices in this cycle.",
                    "label": 1
                },
                {
                    "sent": "So basically what that saying is you can't have loops like this one.",
                    "label": 0
                },
                {
                    "sent": "You can have a loop of four more variables in your graph.",
                    "label": 0
                },
                {
                    "sent": "If you have a loop with with four variables like this is this still has a loop with four variables, But here it has a cord.",
                    "label": 0
                },
                {
                    "sent": "Here's a loop with four variables, but here it has a cord.",
                    "label": 1
                },
                {
                    "sent": "OK, so if your graph has that kind of structure.",
                    "label": 0
                },
                {
                    "sent": "Then it can be represented as either a model or an undirected graphical model without loss of information.",
                    "label": 0
                },
                {
                    "sent": "So I say that because you can always convert a directed graphical model into an undirected graphical model representation, but you are not guaranteed to lose information.",
                    "label": 0
                },
                {
                    "sent": "For example, all of the conditional independencies that are represented in the directed graphical model might not be represented in the equivalent undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "So where that is basically not where there's no loss of information.",
                    "label": 0
                },
                {
                    "sent": "Is in these cases of the chordal graphs.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for example tree structures are chordal because they have no loops at all.",
                    "label": 0
                },
                {
                    "sent": "Anything without loops is chordal.",
                    "label": 0
                },
                {
                    "sent": "Anything then?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, anything that has a loop, but where those loops are basically broken down into complete triplets has that structure as well, yes?",
                    "label": 0
                },
                {
                    "sent": "Right, so we're just words, right?",
                    "label": 0
                },
                {
                    "sent": "So I can.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I'm not specified how we write down the directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying that it is possible to write down a directed graphical model.",
                    "label": 1
                },
                {
                    "sent": "It's just it's easier to specify in the context of undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "Usually when we talk about the theory of of directed an undirected graphical models, an awful lot of time, what we actually do, for example, if we want to do inferencing.",
                    "label": 0
                },
                {
                    "sent": "These models were not going to get into that in this lecture, but often.",
                    "label": 0
                },
                {
                    "sent": "Often mechanically, what we do is we just take the directed graphical model, convert it to an undirected graphical model, and work in that space.",
                    "label": 1
                },
                {
                    "sent": "It sometimes we lose information doing that.",
                    "label": 0
                },
                {
                    "sent": "This is a context where we lose no information doing that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So what if we have more than one coordinate graph?",
                    "label": 0
                },
                {
                    "sent": "That's no problem, yeah, you in fact, you will need more than one court.",
                    "label": 0
                },
                {
                    "sent": "If, for example, you have a cycle that's let's say 5.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have a big cycle of five units.",
                    "label": 1
                },
                {
                    "sent": "And one cord is is only cuts off one part, but you you have a remaining cycle of four units, you'll need another cord to break off that cycle.",
                    "label": 0
                },
                {
                    "sent": "So that's no problem at all.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, yeah, that's where we're at.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're going to sort of walk to a little bit more specialization in terms of the parameterisation we had.",
                    "label": 0
                },
                {
                    "sent": "These flies, these clique potentials are factors.",
                    "label": 0
                },
                {
                    "sent": "Then I haven't told you how these things are actually parameterized or what these things really look like, so we're going to kind of walk towards something that's going to be a bit more, maybe recognizable.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we're going to introduce the notion of an energy based model.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned that these these clique potentials had this one limit is they actually have their output has to be non negative.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's a kind of a restriction on them.",
                    "label": 0
                },
                {
                    "sent": "That way, sometimes you might want to have a representation of the model that, for example, of the parameters that doesn't have that restriction.",
                    "label": 0
                },
                {
                    "sent": "One way you can do that while ensuring this non negative structure and the positive probability distribution is to enforce this kind of a representation.",
                    "label": 0
                },
                {
                    "sent": "So you're going to take your factor or your click potential and just represent it in the log space basically.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to take our file here and just E to the some other factor of that click, right?",
                    "label": 0
                },
                {
                    "sent": "So again, this is sort of an equivalent way to think about it, but the basic idea here is now that now if we use these kinds of factors, we've just, we can represent our probability distribution, our joint probability distribution.",
                    "label": 0
                },
                {
                    "sent": "This way, right?",
                    "label": 0
                },
                {
                    "sent": "So here, and this is just again, we've all we've done is we've just taken all are factors.",
                    "label": 0
                },
                {
                    "sent": "This was a product of factors, and now we just replaced it with this exponentiation exponentiated form.",
                    "label": 0
                },
                {
                    "sent": "So now that comes out as to the exponential of a sum, just equivalent to a product of exponentials, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And a little bit of terminology.",
                    "label": 0
                },
                {
                    "sent": "Sometimes this E. Some people refer to this E as an energy function.",
                    "label": 0
                },
                {
                    "sent": "I actually like to refer to the entire sum here as the energy function, so I prefer to think of it as this kind of a structure where we have exponential negative and then we have our energy function.",
                    "label": 0
                },
                {
                    "sent": "That's how we're going to be referring in the context of restricted Boltzmann machines, and here we have where Z is equal to the sum over.",
                    "label": 0
                },
                {
                    "sent": "Again, this is just our partition function.",
                    "label": 0
                },
                {
                    "sent": "We've not done anything, no magic here, it's still again the sum over all latent variables.",
                    "label": 0
                },
                {
                    "sent": "Of this same term of the unnormalized distribution here.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are energy based models.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can go one step more specific and talk about what we call log linear models.",
                    "label": 0
                },
                {
                    "sent": "Now log linear models are energy based models which are undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "They're just a particular specialized kind of energy based models which are particularly useful for us.",
                    "label": 1
                },
                {
                    "sent": "So in that case what we do is we take these these exponential features or factors and we basically parameterized them.",
                    "label": 0
                },
                {
                    "sent": "Now we're actually going to add the parameters that we're going to learn.",
                    "label": 0
                },
                {
                    "sent": "Right, So what we have is we use it basically composed of two things right parameter for each clique and a feature over the observed data like you can think of this as a sum sufficient statistic of your data or just some general feature of your data, right?",
                    "label": 1
                },
                {
                    "sent": "So one thing about this, I'm sort of used this particular form.",
                    "label": 1
                },
                {
                    "sent": "Of course log linear models are slightly more general than this.",
                    "label": 0
                },
                {
                    "sent": "You can imagine sharing parameters or having more than one parameter forgiven F If you like, so that's a little bit more flexible than what I've defined here.",
                    "label": 0
                },
                {
                    "sent": "I've just defined it this way for simplicity.",
                    "label": 0
                },
                {
                    "sent": "But you have a bit more generalization that I'm showing you right, so the joint probability distribution in this case is of course given by this kind of of property, where we still have our partition function E. And now we've replaced essentially are exponentiated factors with this feature.",
                    "label": 0
                },
                {
                    "sent": "Times are parameter log linear because it's exponentiated an, then we have were linear in the parameters, and we're also happened to be linear in our features.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any questions up to this point?",
                    "label": 0
                },
                {
                    "sent": "So now we've basically gone as far as we're going to go in terms of the formalism of the model, the undirected graphical model we're going to talk about in general, but now what would like to do is talk about how we learn in this model.",
                    "label": 0
                },
                {
                    "sent": "So we've done a little bit.",
                    "label": 0
                },
                {
                    "sent": "We talked a little bit about inference when I drew that thing on the board, right?",
                    "label": 0
                },
                {
                    "sent": "The other thing that we're going to spend a lot of time doing machine learning is learning these models.",
                    "label": 0
                },
                {
                    "sent": "So in the context we're going to be doing that in most often is using maximum likelihood learning.",
                    "label": 1
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "Basically, what we're interested in is just.",
                    "label": 0
                },
                {
                    "sent": "Maximizing the log probability of the data.",
                    "label": 0
                },
                {
                    "sent": "Subject to basically by over our parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So we're trying to find the parameters that maximize the log likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "That's essentially what we're saying here.",
                    "label": 0
                },
                {
                    "sent": "So now what we can do, and this is again in the context of everything is observed.",
                    "label": 0
                },
                {
                    "sent": "So my observations now.",
                    "label": 0
                },
                {
                    "sent": "So I've this is kind of the case where we almost never have in the sense of real, often never observe all of the variables.",
                    "label": 0
                },
                {
                    "sent": "But let's say I've written down a graphical model in this setting where I actually observe everything that I'm set up.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of the easiest possible case you can have for learning, and we're going to see is in.",
                    "label": 0
                },
                {
                    "sent": "Even in this case this super easy case, we have a problem when trying to learn.",
                    "label": 0
                },
                {
                    "sent": "From these models, these undirected graphical models, so let's go ahead and do that.",
                    "label": 0
                },
                {
                    "sent": "So this is again, this is our probability distribution.",
                    "label": 0
                },
                {
                    "sent": "We were observing all of X here, so we're just going to write down what this is, the sort of general form, right?",
                    "label": 0
                },
                {
                    "sent": "So now it's log here, so we're just sticking out our partition function.",
                    "label": 1
                },
                {
                    "sent": "This product, by the way, is just over the data examples, right?",
                    "label": 0
                },
                {
                    "sent": "So these are independent samples, so so we have our product when we once we push our log inside that becomes a sum over data points here.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to again.",
                    "label": 0
                },
                {
                    "sent": "This is our log of our clique potentials here.",
                    "label": 0
                },
                {
                    "sent": "So and again, zed here what we've gone from here to here.",
                    "label": 0
                },
                {
                    "sent": "We're just pushed set out of this sum.",
                    "label": 0
                },
                {
                    "sent": "So we've got this quantity here because it doesn't actually depend on the on the data points, so this is just the data size times log Z and over here what we've done is we've just plugged in our log linear parameterisation, right?",
                    "label": 0
                },
                {
                    "sent": "Like I said, this is a choice, right?",
                    "label": 0
                },
                {
                    "sent": "This is much more general than this, but turns out we're going to most.",
                    "label": 1
                },
                {
                    "sent": "And find ourselves in this context.",
                    "label": 0
                },
                {
                    "sent": "Certainly in the context of a restricted Boltzmann machine, which is really what we're going to be focusing on that really fits into this kind of model structure.",
                    "label": 0
                },
                {
                    "sent": "So right, so now we're going to do is we're going to say that that this actually turns out to be fairly easy to work with because it totally decomposes, right?",
                    "label": 0
                },
                {
                    "sent": "This is just a bunch of sums over independent pieces.",
                    "label": 1
                },
                {
                    "sent": "This does not decompose, and this is a problem because this is a function of our parameters.",
                    "label": 0
                },
                {
                    "sent": "So here all of the pieces are intermixed together and this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think about decomposed what that is?",
                    "label": 0
                },
                {
                    "sent": "This log.",
                    "label": 0
                },
                {
                    "sent": "Here we have a sum.",
                    "label": 0
                },
                {
                    "sent": "This is are some overall X right.",
                    "label": 0
                },
                {
                    "sent": "This term does not depend on the data.",
                    "label": 0
                },
                {
                    "sent": "This is a sum over all values of all of your variables and this turns out to be complicated, so we're not going to be able to find a closed form solution like we can in most cases for directed graphical model.",
                    "label": 1
                },
                {
                    "sent": "It's fully observed.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, what we might be able to do still is actually find a gradient, 'cause then we can at least train these models using some sort of gradient method.",
                    "label": 1
                },
                {
                    "sent": "So if we ask that question, what is the gradient of this log?",
                    "label": 0
                },
                {
                    "sent": "Partition function here.",
                    "label": 0
                },
                {
                    "sent": "This contribution to the likelihood we end up we can just see.",
                    "label": 0
                },
                {
                    "sent": "So this is basically pretty simple to do right.",
                    "label": 0
                },
                {
                    "sent": "We just the derivative of this log puts all of this down here and then.",
                    "label": 0
                },
                {
                    "sent": "All the terms that are not actually is a partial derivative.",
                    "label": 0
                },
                {
                    "sent": "So all the terms that are not actually associated with that just disappear because it's essentially normalized out.",
                    "label": 0
                },
                {
                    "sent": "So you can remove all of those right away.",
                    "label": 0
                },
                {
                    "sent": "And what we end up with is an expectation over the model distribution.",
                    "label": 0
                },
                {
                    "sent": "Here for each click potential.",
                    "label": 0
                },
                {
                    "sent": "So this actually does decompose over the clique potentials.",
                    "label": 0
                },
                {
                    "sent": "But it's still in an exponent.",
                    "label": 1
                },
                {
                    "sent": "It's still a.",
                    "label": 0
                },
                {
                    "sent": "You need to get an expectation from the probability distribution of the marginal probability distribution for that.",
                    "label": 0
                },
                {
                    "sent": "So in other words, it's as hard as computing the marginal probabilities, which turns out not to be very simple, because you need to, because we've defined as a joint probability distribution, right?",
                    "label": 0
                },
                {
                    "sent": "This model is not defined by its marginals, so you actually have to if you wanted to compute the marginal, you'd have to do the work of integrating all those other variables away.",
                    "label": 0
                },
                {
                    "sent": "So we've got it in some form.",
                    "label": 1
                },
                {
                    "sent": "Looks simple, but this is actually not in general.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very simple to compute.",
                    "label": 0
                },
                {
                    "sent": "So what can we do in this case?",
                    "label": 0
                },
                {
                    "sent": "So we're here.",
                    "label": 0
                },
                {
                    "sent": "This is just basically putting it all together, so this term turns out to be the the data term.",
                    "label": 1
                },
                {
                    "sent": "Let's call it.",
                    "label": 0
                },
                {
                    "sent": "This is essentially things that we can actually get marginal simply from the data.",
                    "label": 0
                },
                {
                    "sent": "So this first term ends up being easy, right?",
                    "label": 0
                },
                {
                    "sent": "This is just this part of the contribution of the partial derivative here.",
                    "label": 1
                },
                {
                    "sent": "This part of the partial derivative is complicated, and this is what we just talked about.",
                    "label": 0
                },
                {
                    "sent": "This is the partition function, part contribution, and what we end up with is these two terms, and this is you're going to see this over and over again.",
                    "label": 1
                },
                {
                    "sent": "Undirected graphical models that you got this part that depends on the data.",
                    "label": 0
                },
                {
                    "sent": "And a part that depends on the model and the trick is this is often intractable even in the case of fully observed X.",
                    "label": 0
                },
                {
                    "sent": "So this is our simplest possible setting and this part of even trying to learn this model gets complicated because of that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Context, So what do you do in that context?",
                    "label": 0
                },
                {
                    "sent": "Well, there's a few different things you could do.",
                    "label": 0
                },
                {
                    "sent": "You could try doing something like pseudo likelihood to make an approximation of that partition function.",
                    "label": 0
                },
                {
                    "sent": "That can work, but as soon as we introduce latent variables like, it's complicated.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is is sort of a general purpose algorithm and what we're going to exploit in the context of a restricted Boltzmann machines is we're going to use Monte Carlo samples or Monte Carlo methods.",
                    "label": 1
                },
                {
                    "sent": "Essentially sampling from these things.",
                    "label": 0
                },
                {
                    "sent": "So we're going to sample from this expectation to come up with an approximation of the gradient.",
                    "label": 0
                },
                {
                    "sent": "That's going to be our what we're going to do, but there's disadvantages with doing this.",
                    "label": 0
                },
                {
                    "sent": "Can anyone think of what might be a disadvantage with doing sampling in this context here?",
                    "label": 0
                },
                {
                    "sent": "It's expensive, right?",
                    "label": 0
                },
                {
                    "sent": "So sampling is kind of always expensive.",
                    "label": 0
                },
                {
                    "sent": "You can imagine just having to generate a lot of samples, but there's a particular flavor in what and how it's expensive.",
                    "label": 0
                },
                {
                    "sent": "Right, it's hard to find.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let's let's say samples from the accurate distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So let's so things like you know, you can try something like important sampling.",
                    "label": 0
                },
                {
                    "sent": "That's usually a nonstarter, because if you don't know your distribution ahead of time, actually kind of.",
                    "label": 0
                },
                {
                    "sent": "These are usually high dimensional spaces, so actually trying to find the right distribution this high dimensional thing.",
                    "label": 0
                },
                {
                    "sent": "It's basically over before it begins now.",
                    "label": 0
                },
                {
                    "sent": "One method that does actually work quite well in the context of high dimensional sampling is something like a Monte Carlo MCMC.",
                    "label": 0
                },
                {
                    "sent": "So I I'm on.",
                    "label": 0
                },
                {
                    "sent": "Markov chain Monte Carlo, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a case where your conditioning and doing dependent samples one after the other.",
                    "label": 0
                },
                {
                    "sent": "Right now who can think of so that we have a sampler that might work in this context?",
                    "label": 0
                },
                {
                    "sent": "Is there a problem with that?",
                    "label": 0
                },
                {
                    "sent": "Convergence you have to wait a long time and you're what you're doing is you're doing this in the inner loop of a gradient descent algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "This is a big problem because you are basically saying every time you want to get a new gradient update, you're going to have to wait for your Monte Carlo sampler to converge in order to be off the races.",
                    "label": 0
                },
                {
                    "sent": "So now that's essentially going to be the issue that we're going to have them, and we're going to see how we can address it in that context.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now we're going to move right to restricted Boltzmann machines, and I'm going to go a little bit faster 'cause I'm realizing we're kind of running a bit out of.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time, so this is the this is the restricted Boltzmann machine, right?",
                    "label": 0
                },
                {
                    "sent": "I've already written it on the board, so here we can see a bit of a different formalism here.",
                    "label": 0
                },
                {
                    "sent": "So we have our energy function.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, we've talked about energy functions in the kinds of it, so it's an energy based model.",
                    "label": 0
                },
                {
                    "sent": "And nice thing about energy based models is you can just write down the energy function and then now you can start to think about how the model works right?",
                    "label": 0
                },
                {
                    "sent": "So here it is in vector notation if you write out all the variables we have this form here.",
                    "label": 0
                },
                {
                    "sent": "So we have a weight matrix here represented here.",
                    "label": 0
                },
                {
                    "sent": "That that that encodes the interactions between these and then we have a bunch of unitary cliques.",
                    "label": 0
                },
                {
                    "sent": "These are unitary variables that encode biases on our on our units.",
                    "label": 0
                },
                {
                    "sent": "Here are visible units X and are hidden units H, so that's another point that's important for the DBM.",
                    "label": 0
                },
                {
                    "sent": "X is basically always going to be what we call visible layer.",
                    "label": 0
                },
                {
                    "sent": "This is, these are things we're going to observe, and H is going to be our hidden layer, and these are binary.",
                    "label": 0
                },
                {
                    "sent": "These are typically binary, but we can generalize the PBM to include nonbinary X.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So, so now what we're going to do is we can actually think about how we fit this RBM in the context of things.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Been discussing right.",
                    "label": 0
                },
                {
                    "sent": "So first of all, just think of it from a Markov network or as an undirected graphical model from the point of view of a vector node like Siri, have just one node here that's vector valued in another node.",
                    "label": 0
                },
                {
                    "sent": "Here that's a vector valued.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "Our factors are flies.",
                    "label": 0
                },
                {
                    "sent": "Here are just expressed in this form, right?",
                    "label": 0
                },
                {
                    "sent": "So you can see now that it's in this exponentiation here, so it's in this negative.",
                    "label": 0
                },
                {
                    "sent": "So this this this energy based formalism here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way we can interpret this is break apart those vectors.",
                    "label": 0
                },
                {
                    "sent": "ANAN build the the scalar version of this, and here we have a bunch of pairwise factors right between these scalars here.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially equivalent representation.",
                    "label": 0
                },
                {
                    "sent": "So now we've just broken up these vectors into into scalars.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's basically equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is basically back to our just our normal framework where we're going to look at this.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the question is, we talked a little bit already about this question of how to do inference in this model, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what we're talking about here.",
                    "label": 0
                },
                {
                    "sent": "So inference in the context of an RBM, basically in context of any model, is about computing conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "For example, we might be interested in the probability of H given X.",
                    "label": 0
                },
                {
                    "sent": "And that's going to be just given by.",
                    "label": 0
                },
                {
                    "sent": "That turns out, and we've already seen this.",
                    "label": 0
                },
                {
                    "sent": "Basically that from the Markov properties of this network that if you condition on X, we can actually that these are actually independent, right?",
                    "label": 0
                },
                {
                    "sent": "We've already covered that turns out because of this, because the PBM is perfectly symmetric in its its conditional dependencies between X&Y.",
                    "label": 0
                },
                {
                    "sent": "That we also have this property.",
                    "label": 0
                },
                {
                    "sent": "That condition on HX is are all independent, and we're going to exploit this property of.",
                    "label": 0
                },
                {
                    "sent": "Of these two, being independent from one another condition on the others.",
                    "label": 0
                },
                {
                    "sent": "So rather conditioned on HX is are independent, conditioned on the X is H as independent.",
                    "label": 0
                },
                {
                    "sent": "And where we and it turns out that that we can actually derive a fairly simple form for these conditionals here, where this thing is just given by the sigmoid of of.",
                    "label": 0
                },
                {
                    "sent": "The logistic sigmoid of the bias associated with that unit plus the weight matrix associated with the weight vector associated with that unit in dot product with the visible units.",
                    "label": 0
                },
                {
                    "sent": "That's the case of H here.",
                    "label": 0
                },
                {
                    "sent": "Now this should be familiar to you, right?",
                    "label": 0
                },
                {
                    "sent": "So this looks an awful lot like the activation, so there's a nice relationship between these two and we actually have a very similar symmetrical relationship for P of X.",
                    "label": 0
                },
                {
                    "sent": "Given H. It's again given by a sigmoid, where we have the bias plus the weight metrics, the weight vector.",
                    "label": 0
                },
                {
                    "sent": "Associated with that visible unit and projecting on to H.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we can actually do this thing where we go through the math and analyze the conditional independence that we've already done this in the context of the just looking at the graph, but it's useful to actually derive the structure of the graph, describe the structure of these conditionals, so we'll just go through this really quickly.",
                    "label": 0
                },
                {
                    "sent": "First thing we do is we notice that, well, we're just writing down the conditional distribution here, so that's just equal to the joint over P of X.",
                    "label": 0
                },
                {
                    "sent": "What's P of X?",
                    "label": 0
                },
                {
                    "sent": "Well, we haven't gotten explicit form of P of X, so we're just going to write P of X is being the joint.",
                    "label": 0
                },
                {
                    "sent": "Where I marginalized out H. Right, that's all we're doing here.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just expanding and I'm just plopping down our joint probability distribution in these two cases.",
                    "label": 0
                },
                {
                    "sent": "What we can notice is we can actually cancel out the zeds here in both cases, and because neither one actually because we have different H on the bottom here, but the relationship to X is identical, so we can actually cancel out the terms that involve only X as well.",
                    "label": 0
                },
                {
                    "sent": "So we can do that.",
                    "label": 0
                },
                {
                    "sent": "And now we have this kind of a structure and where what we've done here is just written out like the explicitly all of the different dependencies.",
                    "label": 0
                },
                {
                    "sent": "Now this is a.",
                    "label": 0
                },
                {
                    "sent": "This is summing over all possible values of all H is on the bottom here.",
                    "label": 0
                },
                {
                    "sent": "So what we notice is that we can.",
                    "label": 0
                },
                {
                    "sent": "Actually this is just a sum exponential of some, so we can actually write this as a product of the exponential elements in that sum, which we've got here and now.",
                    "label": 0
                },
                {
                    "sent": "We can do this thing that because there's no interaction here, because these these products are all independent of one another with respect to H. We can actually tease them apart, and we can write this.",
                    "label": 0
                },
                {
                    "sent": "Flip these around.",
                    "label": 0
                },
                {
                    "sent": "So instead of being a large sum of these of this product, we can turn it into a product of sums.",
                    "label": 0
                },
                {
                    "sent": "Right, so now we can.",
                    "label": 0
                },
                {
                    "sent": "Now what we've done, and this is actually the crucial step, because now you can see that what we have here is a bunch of it's a product over a product.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, it's so it's this actually now.",
                    "label": 0
                },
                {
                    "sent": "So here it's a product over product we can do is for each one of these.",
                    "label": 0
                },
                {
                    "sent": "Now it's just a sum over the individual.",
                    "label": 0
                },
                {
                    "sent": "H is so we can actually go ahead and do that.",
                    "label": 0
                },
                {
                    "sent": "We plug because they are binary.",
                    "label": 0
                },
                {
                    "sent": "We plug zero in here and that's just basically resolves to be one because this is zero and this is 0.",
                    "label": 0
                },
                {
                    "sent": "So E to the zero is one that gives us that term and we we plug in H is equal to 1 and that gives us this other term here.",
                    "label": 0
                },
                {
                    "sent": "Sorry question, yeah.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So then what we have is we end up having this product of pieces that look like this right where each individual element is only a function of is only depends on this H sub J right?",
                    "label": 0
                },
                {
                    "sent": "Only a single H. So we now know we've derived the fact that it factors.",
                    "label": 0
                },
                {
                    "sent": "And that's just that's that's this form.",
                    "label": 0
                },
                {
                    "sent": "Now to show that this form actually is the sigmoid, well, it's essentially just trivial.",
                    "label": 0
                },
                {
                    "sent": "Trivial to derive that you can just go from this to this form by just dividing out the top here and then.",
                    "label": 0
                },
                {
                    "sent": "That's just the definition of the logistic sigmoid, right?",
                    "label": 0
                },
                {
                    "sent": "So there we've essentially defined our conditional distributions and it's just the same thing going the other way.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing that we might want to know about is the free energy in the interest of time.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually going to skip this, but you can see that we can derive something similar using the same basic tricks.",
                    "label": 0
                },
                {
                    "sent": "The free.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, what were?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really looking at here is the marginal probability on X, right?",
                    "label": 0
                },
                {
                    "sent": "So H is are these things that don't really exist in the world?",
                    "label": 0
                },
                {
                    "sent": "X is what really exists, so we might want to imagine just right being able to write down a joint distribution over the X is once we've marginalized out H. That's essentially what we're doing, and this is basically just showing you can actually do that.",
                    "label": 0
                },
                {
                    "sent": "Tractably, it's a little surprising, maybe that you can do that, but you can.",
                    "label": 0
                },
                {
                    "sent": "And what you end up with is something that has this form.",
                    "label": 0
                },
                {
                    "sent": "It's got this softplus function in here and that just is basically, this is what the softplus function Maps, and it just basically looks like this so.",
                    "label": 0
                },
                {
                    "sent": "What this is essentially saying is that.",
                    "label": 0
                },
                {
                    "sent": "That the the marginal probabilities will, or the joint probability an X will go up for cases where X is similar, the dot product between X and elements of those of the weight vector are are high.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So right, so now, let's talk about training in this model.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so so now we've already talked about training in the general case.",
                    "label": 0
                },
                {
                    "sent": "There's actually one element that adds complexity to this story.",
                    "label": 0
                },
                {
                    "sent": "That is that we have in the PBM, and that's that.",
                    "label": 0
                },
                {
                    "sent": "What I described in the case of training general undirected graphical models, is that they were fully observed, right?",
                    "label": 0
                },
                {
                    "sent": "That's what we were discussing.",
                    "label": 0
                },
                {
                    "sent": "This fully observed case, and even there we had this problem with the contribution to the partition function, PBM's are not fully observed, but inference in this model is trivial.",
                    "label": 0
                },
                {
                    "sent": "Right, at least in the case of computing P. Of X given AP of H given X, this turns out to be pretty simple.",
                    "label": 0
                },
                {
                    "sent": "We're going to be able to exploit that, and the difference is essentially the difference between this being fully observable will not comes in right here, so we already saw that we had these two terms.",
                    "label": 0
                },
                {
                    "sent": "If we compute the gradient right, this is again just we're trying to.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're minimizing the negative log likelihood instead of maximizing log likelihood.",
                    "label": 0
                },
                {
                    "sent": "And if we compute what that is, what we end up with here is this term.",
                    "label": 0
                },
                {
                    "sent": "Which we are going to positive face.",
                    "label": 0
                },
                {
                    "sent": "This is before exactly the same thing as what we had before when we called it the the data term and then before what we had was something called the model term.",
                    "label": 0
                },
                {
                    "sent": "Here we're going to call that the negative face.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same thing, it's just slightly different terminology.",
                    "label": 0
                },
                {
                    "sent": "I I chose the terminology of determine model term because they meant something positive phase negative phase in this context means something in the literature of these methods, But you can think of these things exactly the same.",
                    "label": 1
                },
                {
                    "sent": "The only difference is that here we actually have an additional expectation.",
                    "label": 0
                },
                {
                    "sent": "Over H right?",
                    "label": 0
                },
                {
                    "sent": "We didn't have this before.",
                    "label": 0
                },
                {
                    "sent": "It was basically just the marginals from the data itself, so we've added this one slight layer of complexity, but that turns out to be no problem at all.",
                    "label": 0
                },
                {
                    "sent": "This expectation, because it's conditioned on X, and we've already seen that H factorizes when conditioned on X, so this turns out to be readily computable, readily tractable.",
                    "label": 0
                },
                {
                    "sent": "That presents us no trouble.",
                    "label": 0
                },
                {
                    "sent": "This still presents us.",
                    "label": 0
                },
                {
                    "sent": "This is our partition function.",
                    "label": 0
                },
                {
                    "sent": "This presents us with the same problem it always did, which is we need to be able to sample from the model.",
                    "label": 0
                },
                {
                    "sent": "In this case, what that means is we need to be able to sample from the joint of PXH.",
                    "label": 0
                },
                {
                    "sent": "And that presents us with the same problems we had before.",
                    "label": 0
                },
                {
                    "sent": "We can do so with Monte Carlo Monte Markov chain Monte Carlo methods, but then we have this problem of having a long burn in and this is about where the technology was up until.",
                    "label": 0
                },
                {
                    "sent": "You know mid 2000s and then.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jeff Hinton proposed and colleagues I should say proposed or maybe yeah proposed a an algorithm called contrastive divergent's, so the contrastive divergent basically has three ideas on top of this setting that we've talked about already, which is that we need a Markov chain Monte Carlo and in our context here that's going to be we're going to be able to Gibbs sampling for that, But here's the idea.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "First idea is instead of doing this big, expensive expectation, we're going to be doing stochastic gradient descent anyway, so a little bit of variance doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're going to actually replace this expectation by a point estimate at X.",
                    "label": 1
                },
                {
                    "sent": "In practice, you can use a mini batch for this to reduce your variance a little bit, but this is the first idea.",
                    "label": 0
                },
                {
                    "sent": "We're going to just use a subsampling.",
                    "label": 0
                },
                {
                    "sent": "We're not going to collect that many samples, so our Monte Carlo method isn't going to be that expensive.",
                    "label": 0
                },
                {
                    "sent": "The second thing we're going to do is we're going to do our sampling based on Gibbs sampling, and we're going to use that because now we can exploit this nice conditional independence properties of the of the restricted Boltzmann machine, right?",
                    "label": 0
                },
                {
                    "sent": "So conditioned on X we can sample the H condition on that H. We can sample the X back and forth.",
                    "label": 0
                },
                {
                    "sent": "It's a very efficient sampling scheme, right?",
                    "label": 0
                },
                {
                    "sent": "'cause we can, it's called Block Gibbs sampler where we can sample on a large block of variables because they're all rendered independent conditioned on the other part.",
                    "label": 0
                },
                {
                    "sent": "So we can kind of think about this process here is being sort of your sampling up and down.",
                    "label": 0
                },
                {
                    "sent": "Now the last thing we're going to add to this process is to deal specifically with this idea of this long chain burn in process, and for that we're going to start sampling the chain at the data point itself, right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to say that.",
                    "label": 0
                },
                {
                    "sent": "Well, we really care about is how the model diverges from the data distribution.",
                    "label": 0
                },
                {
                    "sent": "We don't really need to have a sense of the data distribution itself, So what we're going to do is just initialize it at the data and watch how it goes away from the data distribution and count that as our approximation of the gradient.",
                    "label": 0
                },
                {
                    "sent": "That is basically what we're saying here with four four contrast images, and there's a nice interpretation.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This you can think of it as saying, OK, I'm going to start with my data here X of T and we've got the corresponding HFT that you infer.",
                    "label": 0
                },
                {
                    "sent": "In this case, we're kind of considering this sample, but really what we're talking about is doing a full expectation over this, so we're not inducing any.",
                    "label": 0
                },
                {
                    "sent": "In practice, we don't induce variance for that, but we're here in the energy formulation we want to push this down or in the probability formulation we want to push this probability up of the data, and then we're just going to let this thing sample a little bit, and we're going to.",
                    "label": 0
                },
                {
                    "sent": "Push down at the place where it samples.",
                    "label": 0
                },
                {
                    "sent": "So we want to push down the place where the model things are likely and push up where the data wants to be, and this combination of pushing down and pushing up, pushing, pulling up, pushing down.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is actually our learning algorithm and will change this so right.",
                    "label": 0
                },
                {
                    "sent": "So this is our sample here.",
                    "label": 0
                },
                {
                    "sent": "So we want to make these kinds of things less likely and things that look like the data order come from the data more likely, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the basic element of contrastive diver.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Good OK, so again, we're back to our this being hard to compute, but now we've got this a contrastive divergent algorithm which is going to be able to compute that.",
                    "label": 1
                },
                {
                    "sent": "So anybody want to suggest what some issues are with contrastive divergent's?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, but there were relying on is the parameterisation of the model.",
                    "label": 0
                },
                {
                    "sent": "We don't have to get them all right, we just have to get enough of them to let because the space is always going to be smoothed over by our parameters like this, I mean the whole principle of maximum likelihood depends on this anyway, 'cause you never want to just push up Delta functions where the data exists, right?",
                    "label": 0
                },
                {
                    "sent": "So This is why, for example, mixture of Gaussians, the unregularized mixture of Gaussian model maximum likelihood really makes no sense because just the collapsing on the data points gives you infinite or arbitrarily high likelihood so.",
                    "label": 0
                },
                {
                    "sent": "Same basic idea.",
                    "label": 0
                },
                {
                    "sent": "Here is just you need.",
                    "label": 0
                },
                {
                    "sent": "You're going to rely on the memorization to smooth things out, both in the data and the negative examples.",
                    "label": 0
                },
                {
                    "sent": "Anybody else see a problem with contrast versions, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, that actually is a serious issue.",
                    "label": 1
                },
                {
                    "sent": "I'd like to get back to that.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the questions at the end if there we have time, it's that's a bit of a finesse point.",
                    "label": 0
                },
                {
                    "sent": "I was just going to mention that the big problem you have contrastive divergences Tobias sampling scheme, right?",
                    "label": 0
                },
                {
                    "sent": "You start at the data you're running, Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "You don't let it run to convergence in general.",
                    "label": 0
                },
                {
                    "sent": "We actually in practice don't let it run very long, so this actually induces bias.",
                    "label": 0
                },
                {
                    "sent": "Turns out this bias is actually not that bad in most cases, so it's sometimes a little bit surprising that this is actually a pretty functional learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "But it actually does work fairly well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So right, so getting into a little bit more of the math of this, we can compute our what we had here was we just had these terms here that we need to compute just to fill this out a little bit.",
                    "label": 0
                },
                {
                    "sent": "We don't need to go.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this in too much detail, but this is basically so we just we're just going to say OK are partial derivative.",
                    "label": 0
                },
                {
                    "sent": "We're just going to fill it in for our weight matrix here.",
                    "label": 0
                },
                {
                    "sent": "Compute the gradient with respect to weight matrix.",
                    "label": 0
                },
                {
                    "sent": "This turns out to be fairly straightforward, right?",
                    "label": 0
                },
                {
                    "sent": "It's just a linear function of these weights here, so we end up with just this term for partial derivative with respect to that element, and then the gradient.",
                    "label": 0
                },
                {
                    "sent": "Now meaning with respect to the whole matrix W is just given by this formulation here.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so you can go over the math yourselves if you're interested, But it turns out these kinds of calculations turn out to be fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "We can do the same thing in the context of the conditional expectation here, so we're just plugging in now this term, and this is our now.",
                    "label": 0
                },
                {
                    "sent": "Our expectation over over this term, and what happens in the case of the data distribution we end up with something like this, which is just given our data example.",
                    "label": 0
                },
                {
                    "sent": "Here we compute.",
                    "label": 0
                },
                {
                    "sent": "This is from the data and we just compute this probability.",
                    "label": 0
                },
                {
                    "sent": "This we've already derived with these are.",
                    "label": 0
                },
                {
                    "sent": "These are just given by this sigmoid, right?",
                    "label": 0
                },
                {
                    "sent": "So we can actually compute these this expectation trivially.",
                    "label": 0
                },
                {
                    "sent": "We've already mentioned this, but this just goes into a bit more detail on that point.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now to put it all together in terms of an update formula for contrastive divergent, we're going to say we're given some X and then this this point that we sample.",
                    "label": 0
                },
                {
                    "sent": "We started X and we run Gibbs sampling for a little while and we get to X~ here and the learning rule for our weight matrix ends up looking like this, so our gradient we just fill in all the pieces and we do the substitutions that we've just arrived and we end up with something like this, right?",
                    "label": 1
                },
                {
                    "sent": "So this is just basically saying that the gradient, the actual update for stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Turns out to be fairly straightforward, and it's just basically a difference of sufficient statistics, some derived from the data, some derived from the model, and the model.",
                    "label": 0
                },
                {
                    "sent": "Sufficient statistics here are approximated by the sample that was kind of like Gibbs sampled away from the data.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Now, so the sort of pseudo code for CD CDK here will get into what K is in just in just a minute is we generate a number of his first, we start off with a training example.",
                    "label": 0
                },
                {
                    "sent": "In general, again, we're going to use a mini batch for this to reduce the variance a little bit, so we're not just going to do this with one sample, although you could do it with one sample.",
                    "label": 0
                },
                {
                    "sent": "You can generate a negative sample that corresponds to this data by doing case steps of Gibbs sampling.",
                    "label": 1
                },
                {
                    "sent": "That's our CDK starting at X, and you update the parameters.",
                    "label": 0
                },
                {
                    "sent": "We derive this formula for the update.",
                    "label": 0
                },
                {
                    "sent": "These two are similarly in fact easier to derive.",
                    "label": 0
                },
                {
                    "sent": "You will let you do that on your own if you like, and then you just essentially iterate this process.",
                    "label": 0
                },
                {
                    "sent": "This is the training algorithm for CD.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So getting back to K for a minute now, I mentioned that this is a biased sample, right?",
                    "label": 0
                },
                {
                    "sent": "'cause we're starting at the data and we're not letting it go very far now.",
                    "label": 0
                },
                {
                    "sent": "The bigger K is mean in the mortgage steps we have, the less bias we're going to be inducing in the model, and that's going to necessarily going to help.",
                    "label": 1
                },
                {
                    "sent": "In practice, K = 1 works.",
                    "label": 0
                },
                {
                    "sent": "I will say that K cool one that means we'd actually literally do just one sample, so this is a highly biased sampler, right?",
                    "label": 0
                },
                {
                    "sent": "So it works.",
                    "label": 0
                },
                {
                    "sent": "This is something that was actually very, very common to do.",
                    "label": 0
                },
                {
                    "sent": "Back when the you know, circa 2006 it works very well if you're doing.",
                    "label": 0
                },
                {
                    "sent": "If you're using it for like feature extraction, unsupervised pre training.",
                    "label": 0
                },
                {
                    "sent": "If you actually want to use CD as a as a sort of to actually you want to take this the PBM.",
                    "label": 0
                },
                {
                    "sent": "Seriously, as a generative model of the data, you're going to want to use a larger.",
                    "label": 0
                },
                {
                    "sent": "I found K = 25 actually works very well.",
                    "label": 0
                },
                {
                    "sent": "In fact, that's a very hard algorithm to beat in terms of modeling the data for an art for GBM.",
                    "label": 0
                },
                {
                    "sent": "But there are other choices.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, there's something called persistent CD.",
                    "label": 1
                },
                {
                    "sent": "This turns out to be.",
                    "label": 0
                },
                {
                    "sent": "This name was given by Tiedeman in ICL paper in 2008.",
                    "label": 0
                },
                {
                    "sent": "This turns out to be essentially a reinvention of a more general concept called stochastic maximum likelihood and the basic idea here is that you can think of it as sort of a variation on CD, where instead of starting at the data, you start at the last sample.",
                    "label": 0
                },
                {
                    "sent": "From a Markov chain, so think of it like this.",
                    "label": 0
                },
                {
                    "sent": "You've got this, you've got this Markov chain running in the background and you're just going to be as you as you do your updates in the model you're just going to be running this thing on the model itself, and you're just going to drawing samples from that Markov chain.",
                    "label": 0
                },
                {
                    "sent": "So what the big advantage of this is is that you don't have to wait for it to reconverge after for every gradient update.",
                    "label": 0
                },
                {
                    "sent": "You essentially are relying on the fact that your updates to the model are small.",
                    "label": 0
                },
                {
                    "sent": "They're just small gradient updates.",
                    "label": 0
                },
                {
                    "sent": "So you think that the model is going to be close to where it was at the last time step, so convergence is going to be fast.",
                    "label": 0
                },
                {
                    "sent": "And so again, in practice we often use just one sample to make it converge.",
                    "label": 0
                },
                {
                    "sent": "That works fairly well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One issue that this method has and this is basically probably what you might think.",
                    "label": 0
                },
                {
                    "sent": "Well, at least what I thought was sort of the closest thing to the original maximum likelihood solution for this, right?",
                    "label": 0
                },
                {
                    "sent": "So because you're actually drawing samples from the data and not reinitializing them, so drawing samples from the model and not real initializing them at the data as CD does, so you would think that this would work very well.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you're modeling a data set that's highly multimodal, persistent CD actually has some pretty strong effects that where.",
                    "label": 0
                },
                {
                    "sent": "The negative phase get stuck in these modes is going negative phase sampling right?",
                    "label": 0
                },
                {
                    "sent": "'cause we're drawing samples for the negative phase contribution to the gradient, right?",
                    "label": 0
                },
                {
                    "sent": "So we call this negative phase sampling.",
                    "label": 0
                },
                {
                    "sent": "You get stuck in these modes and you can actually end up having this problem of being you kind of not being able to move around very well and CD kind of gets out of this to some extent because you're always restarting the model app data, so there's in some sense a little bit of a better.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit better in terms of being able to explore the space, So what you actually see when this it's actually a pretty interesting dynamic phenomenon, because what ends up happening is when you're when you end up being stuck in a mode.",
                    "label": 0
                },
                {
                    "sent": "This is again the negative phase of the of the gradient, right?",
                    "label": 1
                },
                {
                    "sent": "Which means you're busy.",
                    "label": 0
                },
                {
                    "sent": "Unlearning that mode.",
                    "label": 0
                },
                {
                    "sent": "So, for example, on something like MNIST, what ends up happening is, let's say you're learning on this.",
                    "label": 0
                },
                {
                    "sent": "Actually, we can go to M list.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, this is amnesty.",
                    "label": 0
                },
                {
                    "sent": "I'm sure you've seen this by now, so this is.",
                    "label": 0
                },
                {
                    "sent": "These are basically just digits.",
                    "label": 0
                },
                {
                    "sent": "There's 10 digits there, they're clustered pretty heavily, so the probability distribution is basically multimodal, and there's roughly 10 modes for each digit.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a few other modes, for example because you have sevens like this and sevens like this, right?",
                    "label": 0
                },
                {
                    "sent": "So what ends up happening with persistent CD is that.",
                    "label": 0
                },
                {
                    "sent": "It quickly builds these modes, which is good, but at some point you end up with, say, a few more of your negative phase samples, say in the seven mode.",
                    "label": 0
                },
                {
                    "sent": "Then you're getting in your data right?",
                    "label": 0
                },
                {
                    "sent": "So they start to be under represented overrepresented in your negative phase and under represented in your positive phase that has the effect of pushing that mode up right?",
                    "label": 0
                },
                {
                    "sent": "So you were basically unlearning sevens for a little while, until the mode gets big enough that those samples can actually leave, and then then you will back relearn this model.",
                    "label": 0
                },
                {
                    "sent": "So what ends up happening is that you actually see this if you monitor the samples throughout training is you end up with this process of it just constantly.",
                    "label": 0
                },
                {
                    "sent": "If you keep at least a constant learning rate, it's just constantly remembering and forgetting individual samples.",
                    "label": 0
                },
                {
                    "sent": "Individual digits through this learning process.",
                    "label": 0
                },
                {
                    "sent": "City, my experience has been is a little bit more stable and you know if you care you can actually get better likelihood using CDK with a relatively high value of K compared to using persistent contrastive divergent.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So right and these are the kinds of filters you get so they look rather pen stroke like.",
                    "label": 0
                },
                {
                    "sent": "Yeah so and then right?",
                    "label": 0
                },
                {
                    "sent": "So these are just the filters you get with with CD you can generally.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this of course to have.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of other kinds of data, right?",
                    "label": 0
                },
                {
                    "sent": "So we talked about.",
                    "label": 0
                },
                {
                    "sent": "In that case it was binary data, yeah?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, sure, sure.",
                    "label": 0
                },
                {
                    "sent": "I feel like I've played a little bit with that.",
                    "label": 0
                },
                {
                    "sent": "I can't remember what happened.",
                    "label": 0
                },
                {
                    "sent": "It wasn't dramatically.",
                    "label": 0
                },
                {
                    "sent": "It didn't feel like the best of both worlds is what I recall.",
                    "label": 0
                },
                {
                    "sent": "Right, so so this is just this.",
                    "label": 0
                },
                {
                    "sent": "I will have to leave it here 'cause I'm essentially out of time but but the this is the energy function that would go with the PBM model.",
                    "label": 0
                },
                {
                    "sent": "If instead of modeling binary data you want to model Gaussian data right?",
                    "label": 0
                },
                {
                    "sent": "And so it's essentially this very similar.",
                    "label": 0
                },
                {
                    "sent": "So up till here it's basically the same.",
                    "label": 0
                },
                {
                    "sent": "But what we've done is we've just added this term here right to keep it.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is just a way to keep the model.",
                    "label": 0
                },
                {
                    "sent": "Able to normalize and not to like if you don't have this part of the model here then what ends up happening is if you try to compute the partition function you can't.",
                    "label": 0
                },
                {
                    "sent": "That integral explodes.",
                    "label": 0
                },
                {
                    "sent": "So you need this kind of term to bound the size that X can be right.",
                    "label": 0
                },
                {
                    "sent": "So this turns out like that.",
                    "label": 0
                },
                {
                    "sent": "We call this the Gaussian Bernoulli RBM, and if you were to.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use that and compute filters.",
                    "label": 0
                },
                {
                    "sent": "They you know looks like something like this.",
                    "label": 0
                },
                {
                    "sent": "If you put a sparsity, perhaps partially prior on your latent variables, that can you can end up with a slightly better model that performs better in the filters.",
                    "label": 0
                },
                {
                    "sent": "Look a little bit better than this.",
                    "label": 0
                },
                {
                    "sent": "But in general this is kind of a not very satisfying model, because we want to use real value data for doing things like modeling natural images.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is where our spike and slab came in, and I'm not going to go over this.",
                    "label": 0
                },
                {
                    "sent": "I've run out of time, but I'll just say that what we did with this model is we basically augmented the latent variables to have not just binary H but real value.",
                    "label": 0
                },
                {
                    "sent": "So here what we've got V for visible units here instead of X.",
                    "label": 0
                },
                {
                    "sent": "These are real valued now, like in the Gaussian binary RBM.",
                    "label": 0
                },
                {
                    "sent": "But now we have S as well as H. And there's just like a an element wise product of these two variables and.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You make a careful choice of the energy function.",
                    "label": 0
                },
                {
                    "sent": "This is a bit more of a complicated form of it, but this is the energy function we use and then you just go through the same kind of derivations we've done in the context of the normal PBM.",
                    "label": 0
                },
                {
                    "sent": "You can do the same thing and what you end up with is you can derive like what we exploited in the case of the normal GBM.",
                    "label": 0
                },
                {
                    "sent": "Is this blockage sampling where you sample one set of variables given the other and then you sample it back and that turned out to be a very efficient way to do inference?",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you can do learning.",
                    "label": 0
                },
                {
                    "sent": "We can basically do something equivalent in the context of the spike and slab RBM, where you condition on.",
                    "label": 0
                },
                {
                    "sent": "Basically you do these three sets of variables.",
                    "label": 0
                },
                {
                    "sent": "So instead of being like a two way block upsampling, it's a 3 way ball game sampling.",
                    "label": 0
                },
                {
                    "sent": "Ann, I think I'm going to just stop there.",
                    "label": 0
                }
            ]
        }
    }
}