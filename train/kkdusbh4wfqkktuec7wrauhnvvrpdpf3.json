{
    "id": "kkdusbh4wfqkktuec7wrauhnvvrpdpf3",
    "title": "On Sequence Kernels for SVM classification of sets of vector",
    "info": {
        "author": [
            "Khalid Daoudi, INRIA - The French National Institute for Research in Computer Science and Control"
        ],
        "published": "Feb. 15, 2008",
        "recorded": "February 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/mcvc08_daoudi_sksvm/",
    "segmentation": [
        [
            "We will talk on sequence kernels for SVM classification.",
            "OK, thank you.",
            "So I'm going to present you summary of the major major work that has been performed within the E team 11 on dynamic kernels.",
            "So this work was mainly done by my former refresh the student Yamla.",
            "I know I do, right was in close collaboration with Frances back.",
            "From from equality.",
            "Mean and this work was performed as I said before, within each England and this work concern.",
            "Concerns the the framework of developing kernels to classify dynamic sequences of observations.",
            "So, as you know, SVM's are one of the most popular.",
            "Algorithm for performing classification for static data.",
            "However, when you are dealing with dynamic data, classical SVM failed to provide satisfactory solution.",
            "And recently this there has been an interest great interest in building kernels that can.",
            "Classify.",
            "The dynamic sequence of observation.",
            "So for the purpose of this work we concentrated on variable length data, so sets of vectors.",
            "So our goal is to classify set variable lengths of vectors and we will use speaker verification as vehicle to demonstrate the effectiveness of the approach we have developed."
        ],
        [
            "OK, so speaker verification is binary task so we have to.",
            "We have natural so we we need to decide which we were."
        ],
        [
            "Weather is it the true speaker or an imp?"
        ],
        [
            "Master, so why?",
            "And and SVM.",
            "Our error has been built 244 binary classifica."
        ],
        [
            "However, in in speech, the speech verification, the.",
            "They don't provide satisfactory results in speaker in speaker location is said and.",
            "And and we need to extend this DSP algorithm to dynamic data, so we can first deal with the true problem which is classifying sequences and also to be able to handle large corpuses of data.",
            "Because in speech applications the mountain data is very huge and SVM's as they are.",
            "N squared, they fail to process such amount of data."
        ],
        [
            "OK."
        ],
        [
            "So.",
            "I will suppose that everybody is at least is a little bit familiar with SVM, so.",
            "Kernels are basically similar to measure and 11 property which is very useful which is necessary to for Colonel Air to be to satisfy the Mercer property, which means it has to be symmetric and positive.",
            "Define it, which means by the medicine term that there exists.",
            "And mapping Phi that that that can be highly dimensional or even infinite such that the kernel is Dutch product of this."
        ],
        [
            "Of this mapping.",
            "So in sequence kernels there is mainly three families, so the first one is mutual information based kernel which which assumes the knowledge of in a priority distribution on the data.",
            "The second class is Colonel between densities, so it assumes that we can estimate density on on server data, and the third class of.",
            "Of sequence kernels are the combination of vector kernels and which means that you build the functional between Inter and intra sequence elements.",
            "And this is it's this last class that we are going through."
        ],
        [
            "To talk about."
        ],
        [
            "So probably the most known information which information be basic and is the Fisher kernel and in the literature it is the first real sequence current that has been developed.",
            "And the.",
            "And and in the form of this current, given two sequences X&Y is given by.",
            "The formula that you see where where Phi is a mapping which is based, which is computed as the gradient.",
            "As the gradient of the log likelihood of the data.",
            "And S is the 2nd second moment.",
            "It's second moment May 2nd moment matrix or flag which is called the information matrix.",
            "The Fisher information made."
        ],
        [
            "So.",
            "Here is an example on what the Fisher, what is the notion of."
        ],
        [
            "Of Of distance means in deficient kernel.",
            "So you see.",
            "I don't, I don't know.",
            "But anyway, if you have questions, we can come back to this."
        ],
        [
            "This stuff here.",
            "Then so occurs between probability densities.",
            "So as I said before, it assumes that you have a generative model on the data and the large class.",
            "Of of probability densities, kernels is the probability product kernel switch.",
            "Which are given.",
            "As the L as a form of LQ form of the product between between the between the probability densities.",
            "Unfortunately.",
            "We cannot have.",
            "Analytical expression for these kind of kernel except when Q = 1 then we can have an electric analytical formula for the girls in the mixture Gaussian density.",
            "And and one trick that that has to be used when dealing with these kernels is to use spherical normalization to achieve a better representation.",
            "Robustness in the stability in the value of the kernels.",
            "And another another class of probability for the colonels is the exponent exponential embedding of divergences, which mean once we have densities, then we can compute divergences between these densities.",
            "They have to be symmetric, of course.",
            "So for example, the symmetrical versions and then perform an exponential embedding and then we end up with with the curve."
        ],
        [
            "OK, now we come to the class that interest us.",
            "So the combination of vector kernel, so the easiest.",
            "The simplest example of.",
            "Of such kernels is to take just to take the average of the enter.",
            "Enter sequence element.",
            "However, you can see if you if the sequences are too large then then the complexity of Carol becomes intractable because it's chi squared."
        ],
        [
            "And the.",
            "So recently there has been.",
            "In the field of speaker verification, Campbell and his collaborator developed to a very nice kernel based on polynomial expansions, which is caused called generalized linear discriminants sequence kernel, and this kernel performs explicitly a mapping of each.",
            "Element of the sequence.",
            "Using polymer polynomial expansions and then it is normalized by the 2nd 2.",
            "The matrix of 2nd moments of the polymer polynomial expansion.",
            "And the.",
            "And it has an advantage that it has high efficiency during training becausw.",
            "Because the SVM is linear at the end, because you need just to perform the square root of the of the normalization matrix and then it becomes linear during test.",
            "So this this this kernel has worked his work, wedding field speaker verification however.",
            "Because of the dimensionality of of the mapping, we cannot use degrees higher than three because otherwise it becomes intractable.",
            "So, given the good result that has been obtained by digital disk errors, we we we thought that it could be.",
            "It could be.",
            "It could be nice if.",
            "We take.",
            "This this form of the kernel's basis to develop a richer family which can exploit actually the kernel trick, as is supposed to be done when we are dealing with kernels, because I remind you that this Carol perform explicitly in mapping, so you work directly in the future space, so you do the mapping of each element than your normal.",
            "Then you work, so you work directly in the feature space, so."
        ],
        [
            "We said, we said."
        ],
        [
            "OK, let's let's consider such class of cameras.",
            "But now we make no assumption on the mapping file.",
            "The only assumption we make we make is that it satisfies the Mercer property.",
            "We consider the second moment matrix or five.",
            "Then we need to normalize, of course, or to regularize by because the mapping can be infinite.",
            "So so to ensure to ensure that we can invert the matrix.",
            "And our goal is so our first goal is to try to compute this quantity, but without of course, using 5 becausw.",
            "In principle, we don't even know what Phi is."
        ],
        [
            "And use the kernel trick to achieve this.",
            "So we were able to.",
            "If this goal.",
            "So this is not just another form another form.",
            "If we instead of considering the the second moment, the metric was second model.",
            "Consider the covariance matrix.",
            "Then you can see that actually what we are doing is that a minimalist distance we are computing Mal Mal Anubis distance.",
            "But in the future space.",
            "So this way we can see this this this class of kernels also as careless between probability distribution but.",
            "In the future space.",
            "And this way it it so by.",
            "This way we can relate this class of care to Gaussian processes for people who know about Gaussian pro."
        ],
        [
            "This is.",
            "So.",
            "I'm not going to.",
            "To explains all the formula, just give the results so we could achieve the results and you and you see that for.",
            "That the kernel can be computed without using Fi, so this is the 1st result without computing expertise expertise Phi Phi, but by use."
        ],
        [
            "Even.",
            "The the mapping see.",
            "This map in.",
            "The upside be, which is called the empirical empirical mark map and which is shown over there.",
            "So what what?",
            "What just the summary here is that we could we could criminalize the original formula as you."
        ],
        [
            "Yeah, so when there are two cases when epsilon is equal."
        ],
        [
            "Zero, and when it is not, but the formulas looks the same."
        ],
        [
            "Also, we when we are considering covariance matrix we have we have an A."
        ],
        [
            "Formula for this.",
            "And and here.",
            "The advantage of such.",
            "Canalization so in term of conditional complexity.",
            "So you see that we all the time girl from.",
            "We replace the dimension D, which is the dimension of the feature space.",
            "By the number of.",
            "Offer of training data and.",
            "So this force small, small, small problems can be useful, particularly if D is infinite.",
            "However, for large class large scale problems as the ones we are dealing with in speech then.",
            "We cannot.",
            "We cannot really use this generalized form."
        ],
        [
            "So.",
            "We so excited to find an approximate, inappropriate and approximate form to do this kernels but up.",
            "Attractable"
        ],
        [
            "And do we achieve this by using a low rank approach, low rank?",
            "Approach matrix approximation.",
            "So and this was done by using the incomplete risk decomposition."
        ],
        [
            "And this is an example of what what the ICD makes.",
            "So the the training data is in is in blue and the result of the incomplete list quizzes are red.",
            "So actually makes a kind of clustering and you see that the clustering of the ICD is.",
            "Keeps keep keeps the measure behavior of the training data."
        ],
        [
            "OK, so here is the approximate form.",
            "Using the ICD to see that it's almost the same, the same formula as before, except that now the mapping C is not on the empirical Maps is not on the full data for training data, but on the codebook given given by the ICD and the code book there is.",
            "Has a parameter M that that we can tune as we want, depending on the tractability we want and the approximation we want to achieve, and you see again that now with the approximate form any replaced by M. And if M is small enough, which is the case actually in our application, then we have a tractable Argo."
        ],
        [
            "Waiting for the killer."
        ],
        [
            "So.",
            "Let me show you some experiments now using this kernels so.",
            "Do the experiments are made on NIST evaluation?",
            "2005"
        ],
        [
            "So I I."
        ],
        [
            "So, so this is just the parameters and.",
            "So here is an experiment to see if what is better.",
            "For the choice of the organization parameter and what is better is to use the covariance matrix or the 2nd order matrix.",
            "And you can see that actually the use of epsilon, the regularization parameter is not necessary, doesn't improve the user.",
            "And actually this is this is normal becausw when we make the ICD we are doing a kind of organization, so the epsilon is not necessary anymore and also the centering of the data using the covariance matrix is not necessary because in our application.",
            "In the preprocessing we did.",
            "The data are already some.",
            "Centered"
        ],
        [
            "OK.",
            "So now.",
            "Well, this is for the choice.",
            "For example, if you are using if vector.",
            "The vector kernel is is Gaussian then then you know it's known that we have to choose the parameter or oh, so this is a new particle.",
            "Estimation of all on our data and we find that the optimal value of all is exactly what is suggested by the theory which is shown below there."
        ],
        [
            "Now for the choice of the parameter M. Let me remind you that we have hundreds of thousands of data, so we see at a certain point, so the bigger M is, the better the result is until a certain point where the result becomes stable and this is also very very nice because it means that at certain level we are achieved.",
            "We have achieved a good approximation, so we don't need to choose bigger and bigger M."
        ],
        [
            "So here here comparison between.",
            "Our kennel the feature space monopoly sequence count.",
            "This is how we called it.",
            "Anne.",
            "In comparison with Jel, this and the classical approach, the generative approach, which is which is used in speaking verification, which is based on Gaussian mixture models.",
            "So you can see that as expected, the our Carol outperforms GL days and this is was our initial goal.",
            "Solution lies the GL desk so and actually I don't.",
            "I don't have it here, but we checked also that when we are using a polynomial kernel and with the degree tree we have exactly the same result as jeles.",
            "On experimental data.",
            "And you see that the results are."
        ],
        [
            "Are competitive with the classical approach, but the good result here is that when we fuse our kennel and GMM one, we improve considerably the result, which means that the two approaches are complementary because one is purely discriminative and your one is generative and in in East evaluation always the best systems are the ones that are that.",
            "Choose different classifiers."
        ],
        [
            "OK, so.",
            "Here is an experiment where we compared.",
            "We compare the all.",
            "All the sequence kernels that can be used in speaker verification so.",
            "Fisher Kernel has been used in in speaker verification.",
            "There is nice thesis on it and nice PhD thesis on it.",
            "Do the UBM GMM so this is the reference the supervector GMM so this is a very recent approach that is based on.",
            "Under well, anyway, if somebody is interested, but the Super vector James is so far the best, best method in speaker verification term of NIST evaluation results probability product kernels.",
            "We've never seen anything on it in speaker vacation, so they are implemented it.",
            "And and here is comparison.",
            "So between all.",
            "The three classes of kernels in the field of speaker picture and you can see that.",
            "That that our kernel is is is the best except in comparison with the Super factor GMM.",
            "Which outperforms it."
        ],
        [
            "But when we fuse again, vote.",
            "We are improving the results so because so again.",
            "Diffusion between our discriminative technique based on our new kernel and the one based on the generative models."
        ],
        [
            "Improves the result.",
            "So and actually so as a conclusion.",
            "So in this work performed within each team 11, we develop theoretical and experimental acute expression of kernels between variable lab lunch sets.",
            "So as you said, there is lot lots of possible kernels.",
            "And we and we have we have now and you are each class of careless that that can be.",
            "Be used in different application not only in in speech.",
            "And the experimental comparison showed that we are performing very nice.",
            "We have very nice results with this with this new kernels and that diffusion with Fusion with the state of the art kernels improve the results.",
            "So there are many.",
            "Open problem that remains is one of the.",
            "The most important one wants, but also the most difficult one is how to adapt to adapt these models too.",
            "New to new test conditions.",
            "This is the advantage of generative approaches when we are is that they are easy to adapt, but so far nobody has succeeded to to do the same adaptation for for for discriminative.",
            "Classifiers and the under well, the other perspective are more related to speech, so I'm not going.",
            "And so I want to mention that there is a. John, before he left he.",
            "We developed a software which implements all these kernels that I talked about.",
            "And so, and it's ready for you so.",
            "If somebody is interested in it will be happy to.",
            "To provide it OK, I think I'm done.",
            "Thank you.",
            "OK, are there any questions?",
            "Evaluated.",
            "How how the accuracy varies with the length of the sequence?",
            "Yeah yeah, because.",
            "Begin speech, I mean what?",
            "We could have many ways to define the sequence, so, and this was actually.",
            "An interrogation for us is when we have a natural.",
            "Should we cut it in 2, three or keep it low?",
            "So we made expect experiments.",
            "By by taking the sequences and putting them in different parts or taking them randomly or so, and we found that the best result was when we consider.",
            "But the sequence isn't matter as it doesn't change the fact that you reduce the size or augmented doesn't change.",
            "So and and this is nice because.",
            "Because it means that.",
            "It captures the because for in the field of speaker verification, what you want to capture is the identity of the speaker.",
            "So when you consider natural of this speaker.",
            "With the Colonel capture it, and actually if you cut it in pieces, you are just doing.",
            "Redundancy and it doesn't improve anything.",
            "So to answer your question, we made experiments on the side of Sequent, and we found that in the field of speaker verification, naturans is enough, however.",
            "In other applications I don't know.",
            "I think it I think that it will depend really on the nature of the application, what kind of information are you trying to capture?",
            "I don't know if this answers your question, so how long is an utterance?",
            "Is it 2 seconds or 10 seconds?",
            "Or yeah, it's it's 2 seconds.",
            "OK, are there any other questions?",
            "OK, let's thank the speaker again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We will talk on sequence kernels for SVM classification.",
                    "label": 1
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to present you summary of the major major work that has been performed within the E team 11 on dynamic kernels.",
                    "label": 0
                },
                {
                    "sent": "So this work was mainly done by my former refresh the student Yamla.",
                    "label": 0
                },
                {
                    "sent": "I know I do, right was in close collaboration with Frances back.",
                    "label": 0
                },
                {
                    "sent": "From from equality.",
                    "label": 0
                },
                {
                    "sent": "Mean and this work was performed as I said before, within each England and this work concern.",
                    "label": 0
                },
                {
                    "sent": "Concerns the the framework of developing kernels to classify dynamic sequences of observations.",
                    "label": 0
                },
                {
                    "sent": "So, as you know, SVM's are one of the most popular.",
                    "label": 0
                },
                {
                    "sent": "Algorithm for performing classification for static data.",
                    "label": 0
                },
                {
                    "sent": "However, when you are dealing with dynamic data, classical SVM failed to provide satisfactory solution.",
                    "label": 0
                },
                {
                    "sent": "And recently this there has been an interest great interest in building kernels that can.",
                    "label": 0
                },
                {
                    "sent": "Classify.",
                    "label": 0
                },
                {
                    "sent": "The dynamic sequence of observation.",
                    "label": 0
                },
                {
                    "sent": "So for the purpose of this work we concentrated on variable length data, so sets of vectors.",
                    "label": 0
                },
                {
                    "sent": "So our goal is to classify set variable lengths of vectors and we will use speaker verification as vehicle to demonstrate the effectiveness of the approach we have developed.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so speaker verification is binary task so we have to.",
                    "label": 0
                },
                {
                    "sent": "We have natural so we we need to decide which we were.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weather is it the true speaker or an imp?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Master, so why?",
                    "label": 0
                },
                {
                    "sent": "And and SVM.",
                    "label": 0
                },
                {
                    "sent": "Our error has been built 244 binary classifica.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, in in speech, the speech verification, the.",
                    "label": 0
                },
                {
                    "sent": "They don't provide satisfactory results in speaker in speaker location is said and.",
                    "label": 1
                },
                {
                    "sent": "And and we need to extend this DSP algorithm to dynamic data, so we can first deal with the true problem which is classifying sequences and also to be able to handle large corpuses of data.",
                    "label": 1
                },
                {
                    "sent": "Because in speech applications the mountain data is very huge and SVM's as they are.",
                    "label": 1
                },
                {
                    "sent": "N squared, they fail to process such amount of data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I will suppose that everybody is at least is a little bit familiar with SVM, so.",
                    "label": 0
                },
                {
                    "sent": "Kernels are basically similar to measure and 11 property which is very useful which is necessary to for Colonel Air to be to satisfy the Mercer property, which means it has to be symmetric and positive.",
                    "label": 0
                },
                {
                    "sent": "Define it, which means by the medicine term that there exists.",
                    "label": 0
                },
                {
                    "sent": "And mapping Phi that that that can be highly dimensional or even infinite such that the kernel is Dutch product of this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of this mapping.",
                    "label": 0
                },
                {
                    "sent": "So in sequence kernels there is mainly three families, so the first one is mutual information based kernel which which assumes the knowledge of in a priority distribution on the data.",
                    "label": 1
                },
                {
                    "sent": "The second class is Colonel between densities, so it assumes that we can estimate density on on server data, and the third class of.",
                    "label": 0
                },
                {
                    "sent": "Of sequence kernels are the combination of vector kernels and which means that you build the functional between Inter and intra sequence elements.",
                    "label": 1
                },
                {
                    "sent": "And this is it's this last class that we are going through.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To talk about.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So probably the most known information which information be basic and is the Fisher kernel and in the literature it is the first real sequence current that has been developed.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "And and in the form of this current, given two sequences X&Y is given by.",
                    "label": 0
                },
                {
                    "sent": "The formula that you see where where Phi is a mapping which is based, which is computed as the gradient.",
                    "label": 0
                },
                {
                    "sent": "As the gradient of the log likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "And S is the 2nd second moment.",
                    "label": 0
                },
                {
                    "sent": "It's second moment May 2nd moment matrix or flag which is called the information matrix.",
                    "label": 0
                },
                {
                    "sent": "The Fisher information made.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here is an example on what the Fisher, what is the notion of.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of Of distance means in deficient kernel.",
                    "label": 0
                },
                {
                    "sent": "So you see.",
                    "label": 0
                },
                {
                    "sent": "I don't, I don't know.",
                    "label": 0
                },
                {
                    "sent": "But anyway, if you have questions, we can come back to this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This stuff here.",
                    "label": 0
                },
                {
                    "sent": "Then so occurs between probability densities.",
                    "label": 1
                },
                {
                    "sent": "So as I said before, it assumes that you have a generative model on the data and the large class.",
                    "label": 1
                },
                {
                    "sent": "Of of probability densities, kernels is the probability product kernel switch.",
                    "label": 0
                },
                {
                    "sent": "Which are given.",
                    "label": 0
                },
                {
                    "sent": "As the L as a form of LQ form of the product between between the between the probability densities.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately.",
                    "label": 0
                },
                {
                    "sent": "We cannot have.",
                    "label": 0
                },
                {
                    "sent": "Analytical expression for these kind of kernel except when Q = 1 then we can have an electric analytical formula for the girls in the mixture Gaussian density.",
                    "label": 0
                },
                {
                    "sent": "And and one trick that that has to be used when dealing with these kernels is to use spherical normalization to achieve a better representation.",
                    "label": 1
                },
                {
                    "sent": "Robustness in the stability in the value of the kernels.",
                    "label": 0
                },
                {
                    "sent": "And another another class of probability for the colonels is the exponent exponential embedding of divergences, which mean once we have densities, then we can compute divergences between these densities.",
                    "label": 1
                },
                {
                    "sent": "They have to be symmetric, of course.",
                    "label": 0
                },
                {
                    "sent": "So for example, the symmetrical versions and then perform an exponential embedding and then we end up with with the curve.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we come to the class that interest us.",
                    "label": 0
                },
                {
                    "sent": "So the combination of vector kernel, so the easiest.",
                    "label": 0
                },
                {
                    "sent": "The simplest example of.",
                    "label": 0
                },
                {
                    "sent": "Of such kernels is to take just to take the average of the enter.",
                    "label": 0
                },
                {
                    "sent": "Enter sequence element.",
                    "label": 0
                },
                {
                    "sent": "However, you can see if you if the sequences are too large then then the complexity of Carol becomes intractable because it's chi squared.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "So recently there has been.",
                    "label": 0
                },
                {
                    "sent": "In the field of speaker verification, Campbell and his collaborator developed to a very nice kernel based on polynomial expansions, which is caused called generalized linear discriminants sequence kernel, and this kernel performs explicitly a mapping of each.",
                    "label": 0
                },
                {
                    "sent": "Element of the sequence.",
                    "label": 0
                },
                {
                    "sent": "Using polymer polynomial expansions and then it is normalized by the 2nd 2.",
                    "label": 0
                },
                {
                    "sent": "The matrix of 2nd moments of the polymer polynomial expansion.",
                    "label": 1
                },
                {
                    "sent": "And the.",
                    "label": 1
                },
                {
                    "sent": "And it has an advantage that it has high efficiency during training becausw.",
                    "label": 1
                },
                {
                    "sent": "Because the SVM is linear at the end, because you need just to perform the square root of the of the normalization matrix and then it becomes linear during test.",
                    "label": 0
                },
                {
                    "sent": "So this this this kernel has worked his work, wedding field speaker verification however.",
                    "label": 0
                },
                {
                    "sent": "Because of the dimensionality of of the mapping, we cannot use degrees higher than three because otherwise it becomes intractable.",
                    "label": 0
                },
                {
                    "sent": "So, given the good result that has been obtained by digital disk errors, we we we thought that it could be.",
                    "label": 0
                },
                {
                    "sent": "It could be.",
                    "label": 0
                },
                {
                    "sent": "It could be nice if.",
                    "label": 0
                },
                {
                    "sent": "We take.",
                    "label": 0
                },
                {
                    "sent": "This this form of the kernel's basis to develop a richer family which can exploit actually the kernel trick, as is supposed to be done when we are dealing with kernels, because I remind you that this Carol perform explicitly in mapping, so you work directly in the future space, so you do the mapping of each element than your normal.",
                    "label": 1
                },
                {
                    "sent": "Then you work, so you work directly in the feature space, so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We said, we said.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's let's consider such class of cameras.",
                    "label": 0
                },
                {
                    "sent": "But now we make no assumption on the mapping file.",
                    "label": 0
                },
                {
                    "sent": "The only assumption we make we make is that it satisfies the Mercer property.",
                    "label": 0
                },
                {
                    "sent": "We consider the second moment matrix or five.",
                    "label": 0
                },
                {
                    "sent": "Then we need to normalize, of course, or to regularize by because the mapping can be infinite.",
                    "label": 0
                },
                {
                    "sent": "So so to ensure to ensure that we can invert the matrix.",
                    "label": 0
                },
                {
                    "sent": "And our goal is so our first goal is to try to compute this quantity, but without of course, using 5 becausw.",
                    "label": 0
                },
                {
                    "sent": "In principle, we don't even know what Phi is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And use the kernel trick to achieve this.",
                    "label": 0
                },
                {
                    "sent": "So we were able to.",
                    "label": 0
                },
                {
                    "sent": "If this goal.",
                    "label": 0
                },
                {
                    "sent": "So this is not just another form another form.",
                    "label": 0
                },
                {
                    "sent": "If we instead of considering the the second moment, the metric was second model.",
                    "label": 0
                },
                {
                    "sent": "Consider the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Then you can see that actually what we are doing is that a minimalist distance we are computing Mal Mal Anubis distance.",
                    "label": 0
                },
                {
                    "sent": "But in the future space.",
                    "label": 0
                },
                {
                    "sent": "So this way we can see this this this class of kernels also as careless between probability distribution but.",
                    "label": 0
                },
                {
                    "sent": "In the future space.",
                    "label": 0
                },
                {
                    "sent": "And this way it it so by.",
                    "label": 0
                },
                {
                    "sent": "This way we can relate this class of care to Gaussian processes for people who know about Gaussian pro.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to.",
                    "label": 0
                },
                {
                    "sent": "To explains all the formula, just give the results so we could achieve the results and you and you see that for.",
                    "label": 0
                },
                {
                    "sent": "That the kernel can be computed without using Fi, so this is the 1st result without computing expertise expertise Phi Phi, but by use.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even.",
                    "label": 0
                },
                {
                    "sent": "The the mapping see.",
                    "label": 0
                },
                {
                    "sent": "This map in.",
                    "label": 0
                },
                {
                    "sent": "The upside be, which is called the empirical empirical mark map and which is shown over there.",
                    "label": 0
                },
                {
                    "sent": "So what what?",
                    "label": 0
                },
                {
                    "sent": "What just the summary here is that we could we could criminalize the original formula as you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so when there are two cases when epsilon is equal.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zero, and when it is not, but the formulas looks the same.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, we when we are considering covariance matrix we have we have an A.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formula for this.",
                    "label": 0
                },
                {
                    "sent": "And and here.",
                    "label": 0
                },
                {
                    "sent": "The advantage of such.",
                    "label": 0
                },
                {
                    "sent": "Canalization so in term of conditional complexity.",
                    "label": 0
                },
                {
                    "sent": "So you see that we all the time girl from.",
                    "label": 0
                },
                {
                    "sent": "We replace the dimension D, which is the dimension of the feature space.",
                    "label": 1
                },
                {
                    "sent": "By the number of.",
                    "label": 0
                },
                {
                    "sent": "Offer of training data and.",
                    "label": 0
                },
                {
                    "sent": "So this force small, small, small problems can be useful, particularly if D is infinite.",
                    "label": 1
                },
                {
                    "sent": "However, for large class large scale problems as the ones we are dealing with in speech then.",
                    "label": 0
                },
                {
                    "sent": "We cannot.",
                    "label": 0
                },
                {
                    "sent": "We cannot really use this generalized form.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We so excited to find an approximate, inappropriate and approximate form to do this kernels but up.",
                    "label": 0
                },
                {
                    "sent": "Attractable",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And do we achieve this by using a low rank approach, low rank?",
                    "label": 0
                },
                {
                    "sent": "Approach matrix approximation.",
                    "label": 0
                },
                {
                    "sent": "So and this was done by using the incomplete risk decomposition.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is an example of what what the ICD makes.",
                    "label": 0
                },
                {
                    "sent": "So the the training data is in is in blue and the result of the incomplete list quizzes are red.",
                    "label": 0
                },
                {
                    "sent": "So actually makes a kind of clustering and you see that the clustering of the ICD is.",
                    "label": 0
                },
                {
                    "sent": "Keeps keep keeps the measure behavior of the training data.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here is the approximate form.",
                    "label": 0
                },
                {
                    "sent": "Using the ICD to see that it's almost the same, the same formula as before, except that now the mapping C is not on the empirical Maps is not on the full data for training data, but on the codebook given given by the ICD and the code book there is.",
                    "label": 0
                },
                {
                    "sent": "Has a parameter M that that we can tune as we want, depending on the tractability we want and the approximation we want to achieve, and you see again that now with the approximate form any replaced by M. And if M is small enough, which is the case actually in our application, then we have a tractable Argo.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Waiting for the killer.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me show you some experiments now using this kernels so.",
                    "label": 0
                },
                {
                    "sent": "Do the experiments are made on NIST evaluation?",
                    "label": 0
                },
                {
                    "sent": "2005",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I I.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so this is just the parameters and.",
                    "label": 0
                },
                {
                    "sent": "So here is an experiment to see if what is better.",
                    "label": 0
                },
                {
                    "sent": "For the choice of the organization parameter and what is better is to use the covariance matrix or the 2nd order matrix.",
                    "label": 0
                },
                {
                    "sent": "And you can see that actually the use of epsilon, the regularization parameter is not necessary, doesn't improve the user.",
                    "label": 0
                },
                {
                    "sent": "And actually this is this is normal becausw when we make the ICD we are doing a kind of organization, so the epsilon is not necessary anymore and also the centering of the data using the covariance matrix is not necessary because in our application.",
                    "label": 0
                },
                {
                    "sent": "In the preprocessing we did.",
                    "label": 0
                },
                {
                    "sent": "The data are already some.",
                    "label": 0
                },
                {
                    "sent": "Centered",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Well, this is for the choice.",
                    "label": 0
                },
                {
                    "sent": "For example, if you are using if vector.",
                    "label": 0
                },
                {
                    "sent": "The vector kernel is is Gaussian then then you know it's known that we have to choose the parameter or oh, so this is a new particle.",
                    "label": 0
                },
                {
                    "sent": "Estimation of all on our data and we find that the optimal value of all is exactly what is suggested by the theory which is shown below there.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for the choice of the parameter M. Let me remind you that we have hundreds of thousands of data, so we see at a certain point, so the bigger M is, the better the result is until a certain point where the result becomes stable and this is also very very nice because it means that at certain level we are achieved.",
                    "label": 0
                },
                {
                    "sent": "We have achieved a good approximation, so we don't need to choose bigger and bigger M.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here here comparison between.",
                    "label": 0
                },
                {
                    "sent": "Our kennel the feature space monopoly sequence count.",
                    "label": 0
                },
                {
                    "sent": "This is how we called it.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In comparison with Jel, this and the classical approach, the generative approach, which is which is used in speaking verification, which is based on Gaussian mixture models.",
                    "label": 0
                },
                {
                    "sent": "So you can see that as expected, the our Carol outperforms GL days and this is was our initial goal.",
                    "label": 0
                },
                {
                    "sent": "Solution lies the GL desk so and actually I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't have it here, but we checked also that when we are using a polynomial kernel and with the degree tree we have exactly the same result as jeles.",
                    "label": 0
                },
                {
                    "sent": "On experimental data.",
                    "label": 0
                },
                {
                    "sent": "And you see that the results are.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are competitive with the classical approach, but the good result here is that when we fuse our kennel and GMM one, we improve considerably the result, which means that the two approaches are complementary because one is purely discriminative and your one is generative and in in East evaluation always the best systems are the ones that are that.",
                    "label": 0
                },
                {
                    "sent": "Choose different classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here is an experiment where we compared.",
                    "label": 0
                },
                {
                    "sent": "We compare the all.",
                    "label": 0
                },
                {
                    "sent": "All the sequence kernels that can be used in speaker verification so.",
                    "label": 0
                },
                {
                    "sent": "Fisher Kernel has been used in in speaker verification.",
                    "label": 0
                },
                {
                    "sent": "There is nice thesis on it and nice PhD thesis on it.",
                    "label": 0
                },
                {
                    "sent": "Do the UBM GMM so this is the reference the supervector GMM so this is a very recent approach that is based on.",
                    "label": 0
                },
                {
                    "sent": "Under well, anyway, if somebody is interested, but the Super vector James is so far the best, best method in speaker verification term of NIST evaluation results probability product kernels.",
                    "label": 0
                },
                {
                    "sent": "We've never seen anything on it in speaker vacation, so they are implemented it.",
                    "label": 0
                },
                {
                    "sent": "And and here is comparison.",
                    "label": 0
                },
                {
                    "sent": "So between all.",
                    "label": 0
                },
                {
                    "sent": "The three classes of kernels in the field of speaker picture and you can see that.",
                    "label": 0
                },
                {
                    "sent": "That that our kernel is is is the best except in comparison with the Super factor GMM.",
                    "label": 0
                },
                {
                    "sent": "Which outperforms it.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But when we fuse again, vote.",
                    "label": 0
                },
                {
                    "sent": "We are improving the results so because so again.",
                    "label": 0
                },
                {
                    "sent": "Diffusion between our discriminative technique based on our new kernel and the one based on the generative models.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Improves the result.",
                    "label": 0
                },
                {
                    "sent": "So and actually so as a conclusion.",
                    "label": 0
                },
                {
                    "sent": "So in this work performed within each team 11, we develop theoretical and experimental acute expression of kernels between variable lab lunch sets.",
                    "label": 1
                },
                {
                    "sent": "So as you said, there is lot lots of possible kernels.",
                    "label": 0
                },
                {
                    "sent": "And we and we have we have now and you are each class of careless that that can be.",
                    "label": 1
                },
                {
                    "sent": "Be used in different application not only in in speech.",
                    "label": 0
                },
                {
                    "sent": "And the experimental comparison showed that we are performing very nice.",
                    "label": 0
                },
                {
                    "sent": "We have very nice results with this with this new kernels and that diffusion with Fusion with the state of the art kernels improve the results.",
                    "label": 0
                },
                {
                    "sent": "So there are many.",
                    "label": 0
                },
                {
                    "sent": "Open problem that remains is one of the.",
                    "label": 0
                },
                {
                    "sent": "The most important one wants, but also the most difficult one is how to adapt to adapt these models too.",
                    "label": 0
                },
                {
                    "sent": "New to new test conditions.",
                    "label": 0
                },
                {
                    "sent": "This is the advantage of generative approaches when we are is that they are easy to adapt, but so far nobody has succeeded to to do the same adaptation for for for discriminative.",
                    "label": 0
                },
                {
                    "sent": "Classifiers and the under well, the other perspective are more related to speech, so I'm not going.",
                    "label": 0
                },
                {
                    "sent": "And so I want to mention that there is a. John, before he left he.",
                    "label": 0
                },
                {
                    "sent": "We developed a software which implements all these kernels that I talked about.",
                    "label": 0
                },
                {
                    "sent": "And so, and it's ready for you so.",
                    "label": 0
                },
                {
                    "sent": "If somebody is interested in it will be happy to.",
                    "label": 0
                },
                {
                    "sent": "To provide it OK, I think I'm done.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Evaluated.",
                    "label": 0
                },
                {
                    "sent": "How how the accuracy varies with the length of the sequence?",
                    "label": 1
                },
                {
                    "sent": "Yeah yeah, because.",
                    "label": 0
                },
                {
                    "sent": "Begin speech, I mean what?",
                    "label": 0
                },
                {
                    "sent": "We could have many ways to define the sequence, so, and this was actually.",
                    "label": 0
                },
                {
                    "sent": "An interrogation for us is when we have a natural.",
                    "label": 0
                },
                {
                    "sent": "Should we cut it in 2, three or keep it low?",
                    "label": 0
                },
                {
                    "sent": "So we made expect experiments.",
                    "label": 0
                },
                {
                    "sent": "By by taking the sequences and putting them in different parts or taking them randomly or so, and we found that the best result was when we consider.",
                    "label": 0
                },
                {
                    "sent": "But the sequence isn't matter as it doesn't change the fact that you reduce the size or augmented doesn't change.",
                    "label": 0
                },
                {
                    "sent": "So and and this is nice because.",
                    "label": 0
                },
                {
                    "sent": "Because it means that.",
                    "label": 0
                },
                {
                    "sent": "It captures the because for in the field of speaker verification, what you want to capture is the identity of the speaker.",
                    "label": 0
                },
                {
                    "sent": "So when you consider natural of this speaker.",
                    "label": 0
                },
                {
                    "sent": "With the Colonel capture it, and actually if you cut it in pieces, you are just doing.",
                    "label": 0
                },
                {
                    "sent": "Redundancy and it doesn't improve anything.",
                    "label": 0
                },
                {
                    "sent": "So to answer your question, we made experiments on the side of Sequent, and we found that in the field of speaker verification, naturans is enough, however.",
                    "label": 0
                },
                {
                    "sent": "In other applications I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think it I think that it will depend really on the nature of the application, what kind of information are you trying to capture?",
                    "label": 0
                },
                {
                    "sent": "I don't know if this answers your question, so how long is an utterance?",
                    "label": 0
                },
                {
                    "sent": "Is it 2 seconds or 10 seconds?",
                    "label": 0
                },
                {
                    "sent": "Or yeah, it's it's 2 seconds.",
                    "label": 0
                },
                {
                    "sent": "OK, are there any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}