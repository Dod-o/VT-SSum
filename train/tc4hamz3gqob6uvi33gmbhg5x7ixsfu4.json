{
    "id": "tc4hamz3gqob6uvi33gmbhg5x7ixsfu4",
    "title": "We need a BIT more GUTS (Grand Unified Theory of Statistics)",
    "info": {
        "author": [
            "Peter Gr\u00fcnwald, Centrum Wiskunde & Informatica (CWI)"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_grunwald_guts/",
    "segmentation": [
        [
            "So, well, first of all, thank you thanks to the organizers for inviting me here.",
            "This morning I don't know if you have been this morning, but this morning we heard several people talk about the need for.",
            "In front of the.",
            "OK, yeah, there's a maybe a distance problem, but I will try to overcome it.",
            "Yes, yes, push the button next.",
            "Yes, where was I?",
            "Um, yes.",
            "So there was talk that there should be more unity machine learning, maybe more relations reductions between different machine learning problems.",
            "On the other hand, there was at the same time or warning to which I think everyone agreed that was even said.",
            "Beware of the man who is 1 method for everything.",
            "Now to some extent.",
            "I agree with that, but to another extent, I think I want to take up the challenge and show some guts here."
        ],
        [
            "And I will argue that a lot of machine learning problems can be expressed in terms of bits, although the bit has to be taken with a grain of salt, as we will see.",
            "But the main thing remains that I think there's a lot of unification possible.",
            "Not everything but a lot under one particular.",
            "Let's say group of ideas and to explain it, let me start with the end of my book, of course.",
            "And if you know I've written a book about minimum description length and in the final chapter of the book I Give an overview of how it stands in relation to other machine learning approaches and I come to the conclusion that although its advocates say that you know it's, it's a hammer that can will solve every problem for you.",
            "That's not true.",
            "There are problems, definitely even lots of problems which you cannot very readily solve with MDM.",
            "However.",
            "All of these problems, still, you can somehow make a link to compression in the sense that if you are able to really learn something to predict better than my random guessing, then implicitly you can always say that you have also compressed your data.",
            "And I think this is often a useful realization which leads to new insights.",
            "And that is what this talk is about."
        ],
        [
            "So it's not about MDL, it's also not about base, although we will do very similar things.",
            "So the main thesis is a wide variety of problems in machine learning, and statistics can be rephrased as compression or what is equivalent sequential gambolling and in the Kelly setting which means that you can put your money on different outcomes and and that is the essential thing which makes it innocence equivalent to coding data.",
            "You are free to divide your money in any way you like so you don't have to put everything.",
            "On one outcome you can divide things.",
            "And log loss prediction as studied in the theoretical machine learning communities.",
            "Also the same thing and then this is useful to do so.",
            "Many things we will do will have a base in flavor, but still, as we will see, this is really not based.",
            "There is an important twist."
        ],
        [
            "So in other words, that I really like, which was mentioned this morning about these people who like to explain everything in terms of 1 theory that is called in a science fiction novel.",
            "But I've seen that other people are using it as well.",
            "Now, mono couso tech Sophia.",
            "So I agree that in general this is a bad idea to try to explain everything with the same tool.",
            "The same hammer, but I will again also for the sake of argument, go a little bit in its defense, and saying that at least in some cases.",
            "I'd be very good idea to look at existing approaches and try to find out what really works about them.",
            "What doesn't work and try to find the Common Core.",
            "And here is 1 example artificial intelligence in the 1980s.",
            "If you would ask somebody at the HK conference in the 1980s how to represent uncertainty, they would say depends on the problem.",
            "You could use Dempster Shafer certainty factors like in my singing expert system, fuzzy logic, possibility theory, nonmonotonic logic.",
            "I've even worked on that and in some cases where you have repeatable events, you might want to use probability.",
            "Yes."
        ],
        [
            "So.",
            "Actually, the UI conference was started by Judea Pearl Ann.",
            "Raise alarm on off and a few other peoples because they thought this was nonsense.",
            "If you look at UAA now and even at each car you see that most paper most papers have the same way of dealing with uncertainty because it's just the most general, an successful it's probability or in some cases sets of probabilities.",
            "But even if you work with sets of probabilities, the basic notion is still probability.",
            "So I think here it's this was very good develop."
        ],
        [
            "So again, we're going to use probability for predictions, but with A twist.",
            "Anne."
        ],
        [
            "Yes.",
            "So when I talk about a code of data in this talk, I really mean a sequential prediction strategy relative to log loss, and I'll explain what that is in a moment.",
            "So the idea is that even if you don't know about data compression, you can follow this talk, because sequential prediction with loss is relatively easy to explain.",
            "Of course, if we wanted something general, we will often have to deal with other loss functions, like in classification, the 01 loss or asymmetric loss.",
            "So what we will do here is we will transform those two log loss for that has been done a lot in the past by basins.",
            "Anne, but things can go wrong if you do this, and that is what a large part of this talk will be about.",
            "How can you do this in the right way?",
            "And of course, the idea of this transformation is the simplest case, something you're all familiar with.",
            "It was already realized by Gauss that if you do regression, you can do least squares.",
            "You can also assume that Y is equal to a linear function of X + 0, mean Gaussian noise and maximum likelihood.",
            "Least squares is the same as maximizing log likelihood, which is really the same as minimizing love loss.",
            "That's the trans type of transformation we're going to use."
        ],
        [
            "So first I'm going to explain the sequential prediction, and for those of you not familiar with it, this will also be a crash course about what the people who go to the cold computational Learning Theory conference are usually working on.",
            "And we will come up with something with our NIPS posted this year was about the mixability gap and we will show how that unifies several different things.",
            "Working with different loss functions.",
            "I think a thing called that ebook of exponent.",
            "If you do assume a true distribution and based learning models are wrong, this will be the end of the hard part and then in the last 10 minutes I will switch to something where I don't won't give any details, just an overview so it will be a lot more relaxed where I will see where we will see that the same issues about sequential data compression can also be used to address some issues in statistics testing of hypothesis."
        ],
        [
            "So online prediction.",
            "So here assume we have some sequence of data can be discrete or continuous.",
            "They will take space, take values in some space, curly why?",
            "So the goal is to sequentially predict the next outcome given the past.",
            "And we will assume for now that we do our predictions using distribution later.",
            "We will also look at, let's say point predictions, things like that.",
            "So a prediction strategy S is in a function which Maps the past to prediction for the next outcome, right for each individual for each initial sequence of outcomes.",
            "Or strategy tells you given that initial sequence, how should you predict the next outcome, right?",
            "You can think of a Markov chain, for example is doing this."
        ],
        [
            "So you can measure the qualities of such predictions in many ways, and it sets here the main focus will be on the log loss.",
            "So then for a single outcome, if you predicted by distribution P, you define the log lost to the minus logarithm of P alright, logarithm base two here later.",
            "In this talk I switch to natural logarithm, but it doesn't make any difference for the results.",
            "And then I always use this notation, so the loss on a sequence if you use a strategy for predicting it is the sum of the individual losses.",
            "So you predict the first outcome.",
            "You predict the second, given the first, the third given first 2, and so on, and you Add all the losses.",
            "There are two practical settings which correspond to this.",
            "The first is data compression, so you can think of predicting with lock loss as essentially the same thing as coding data.",
            "There are some cave caveats because quotes always have integer lengths, but by and large these that's not a problem.",
            "You can match these two things, and another thing is what I also said before Kelly Gambolling, where you gamble on the different outcomes in why some bookie tells you if that is the outcome you get so much money that is.",
            "You get so much money and you're allowed to put your capital in any way you like on the different outcomes.",
            "And then if you exponentiate everything here, it turns out that the smaller the lock loss, the larger the factor by which your capital grows in time.",
            "It turns out to be equivalent."
        ],
        [
            "So now if we for a moment we make probabilistic assumptions and we think that our data comes from some distribution completely arbitrary, any distribution, then because lock losses a certain property which is that it's proper, the optimal predictions we can make.",
            "If we think that our to predict by makes a lot of sense, the conditional distributions corresponding to this distribution, right?",
            "If you think the data comes from this distribution and you want to use the conditional distribution given the past to predict.",
            "So what happens in this logarithmic loss prediction setting is that we turn the reasoning around and we say, well, if we have a prediction strategy S that tells you for each past probability distribution over the next outcome, right?",
            "It's a distribution, so now you can.",
            "Create a new distribution by multiplying these probabilities right?",
            "You can think of these as conditional probabilities for the next outcome given the past, and you define a joint probability by the multiplication of these predicted probabilities.",
            "Which is just this, and it turns out that if you sum this over all values of why you get one.",
            "So now you've created a new distribution.",
            "Which is just a distribution over data which corresponds to your prediction strategy.",
            "So from now on we will identify probabilistic prediction strategies with distributions over the whole sequence.",
            "There are some issues here.",
            "If you assign probability zero to some things, I've just gloss over that."
        ],
        [
            "So.",
            "Now what we want to do, and this will be the key goal in the first part of the talk is suppose we have a large set of predictors.",
            "And we think that some of them might be good.",
            "We don't know which one.",
            "And we want some algorithm which takes us input to predictors the predictors of black box.",
            "We don't need to know how they work and it outputs.",
            "Some predictions such that we want that we perform about as well as the best one in this set.",
            "And I'll let this set be countable, because it makes the mathematics easy, but otherwise it can be very large, so it could be all Markov chains of each order.",
            "And to make it accountable and make the parameters rational values for example.",
            "And later we will also do this.",
            "With polynomials and things like that.",
            "So this think of this as a potentially large set, even though it's countable.",
            "And now I somehow want to get some algorithms such that if one of these is a good sequential low class predictor, I can also make good luck loss predictions."
        ],
        [
            "So one possibility which turns out to be.",
            "In a way, perhaps even the best one.",
            "Is to do this as a basin would, so you put some prior distribution mass function on the set of distributions rather than parameters.",
            "You can define a marginal distribution based on marginal distribution like this.",
            "And now you predict, like a bashan would, just with the conditional distribution, right?",
            "Given the past you have.",
            "This is just the definition of conditional probability.",
            "But it's a trivial rewrite, shows this is identical to the prediction of the next outcome given the past, according to theater weighted overall theater by the posterior rather than the prior right?",
            "So you weight each theater by how good it predicted in the past.",
            "If it predicted well if its probability was high, then its log loss was small.",
            "It get your large wait for the next prediction."
        ],
        [
            "Yes.",
            "So this is just base posterior where we used Bayes rule."
        ],
        [
            "Um?",
            "So how well does this work?",
            "So here there's some interesting mathematics which many of you will know.",
            "If you don't know what what's important is to remember the conclusion, not so much the mathematics.",
            "So.",
            "If we have an arbitrary strategy, we can think of that as a probability distribution, right?",
            "We can always predict the next outcome given the past and then for total loss will be the sum of these losses.",
            "So by definition it's the sum of these minus logarithms.",
            "So that's as we've seen before.",
            "If you write down the definition of this, you get this ratio here of two probabilities, the minus the sum of minus lockers minus log of a product.",
            "So if you write out the product in the ratio, everything cancels diagonally and you end up with this.",
            "Right, so this total loss is just a minus log likelihood you assigned to the data.",
            "That's what makes log loss so nice mathematically.",
            "This holds for every P and for the base strategy it must also hold.",
            "And for this we can say more.",
            "We know that the total loss we make if we predict like a base in route right with a prior.",
            "Overall these theater.",
            "It's the minus log of this probability.",
            "So that's the minus log of this song.",
            "Now some is always larger than each of its terms, so minus log off so much smaller.",
            "Then minus log of each of its terms so.",
            "This minus log of this so much smaller than minus look this times this because the logarithm of a product is the sum of the logarithms.",
            "We get that the total loss we make with base sequentially.",
            "Is bounded by the loss we make with any theater.",
            "Plus an additional term which is kind of the penalty for Theta.",
            "And the interesting thing is that this usually increases linearly nanan the penalties constant constant and end.",
            "So as we get more and more data, will will be about as good as theater, or perhaps even better, because this holds for all data and for all Theta, right?",
            "So if we have a bad theater here, it will also hold for a good data and we will be at least as good as a good theater.",
            "Plus disturb.",
            "This term here is has a different interpretation from a prior but similar but still different from the usual interpretation of a prior.",
            "In Bayesian statistics it measures something that has been called luckiness, for example by Bob and others.",
            "Basically, it means that if you have many predictors, you put a prior on them.",
            "Now, if you hope that some predictor will perform well, you might want to put a large prior on it.",
            "If that predicted, and indeed works well, then if you are basing the total loss, you may compare to death predictor.",
            "We have a small extra overhead.",
            "Yeah, this is 1/2.",
            "You will never have an overhead of just one.",
            "Right, if you were unlucky and the best predictor was something you gave a small prior to, your overhead will be much larger, right?",
            "Because you have maybe 1000 minus log 1000 thousands much larger.",
            "Right, but still as you get more and more data you will catch up with that predictor as well."
        ],
        [
            "So in the literature list, this is called basis Universal relative to the set of distributions.",
            "Theater, universal coat or universal predictor.",
            "The total low class you make if you predict sequentially with base is bound by the law class you make with any of the theater plus an additional term.",
            "And the additional term, and this is amazing, doesn't depend on N. So no matter how much you get, doesn't get larger."
        ],
        [
            "Now you might say, well, this is log loss, but in practice we often want to do classification like problems.",
            "Other other problems are the loss functions.",
            "So can we do the same thing for other loss functions?",
            "And the answer is no.",
            "Standard base if you try to do it standard naive Bayesian way with 01 loss, then in the worst case, overall data.",
            "Note, we're not assuming the data are probabilistically general generated right to be looking at all data in the worst case overall database will be very bad and then you can design an optimal algorithm.",
            "The worst case and it will get regret where there's suddenly a sqrt N before this term.",
            "Which is pretty much worse than constant right square root of N is like.",
            "This will increase the previous hearing an, but you learn a lot slower.",
            "Basically you will learn to imitate a good theater a lot slower without a loss functions.",
            "And a lot of the work in the cold community has been about.",
            "Why is this the case?",
            "And maybe in some cases even with 01 loss, can we do better?"
        ],
        [
            "So what we will do is we'll try to replicate the whole story for other loss functions by transforming them to log loss, and then we will see what will go wrong."
        ],
        [
            "And to do that, I already announced that we use this idea, which in a way is already due to Gauss.",
            "I called it entropy fication myself at some point, but name hasn't caught on.",
            "It either explicitly or implicitly occurs in a lot of papers over the last 20 years.",
            "So suppose you have some loss function, for example 01 loss, then this is zero or one, and this would also be 01.",
            "We allow that it's infinite.",
            "Now.",
            "For every action with this loss function, we can transform it to a probability distribution probabilistic prediction.",
            "By taking the probability of Y to be equal to E to the minus eater times the loss.",
            "Right, so this is probability now.",
            "Depends on a an eater.",
            "You can extend this to an outcomes by independence.",
            "Yep."
        ],
        [
            "So.",
            "It turns out that you can do this for just about every loss function, although superficially there seems to be a big problem because this so this is at pizza is to normalize.",
            "This is just the sum in the discrete case, or otherwise the integral overall Y of this, so that this thing if you sum it over by becomes one becomes a real probability.",
            "So you see that in general this should depend on A and interesting before the 01 loss and the squared loss.",
            "It doesn't depend on a, so there we can remove a from here and this.",
            "This set doesn't depend on a for other loss functions, you can use a trick to get the same thing.",
            "It's too complicated to deal with here, but you can do that with asymmetric loss functions as well.",
            "But it's more complicated, but so let's just ignore the set for this talk.",
            "Let's just act as if it doesn't depend on the action.",
            "So now we have a mapping from every action to loss functions."
        ],
        [
            "And with this mapping we have, we have that the log loss we make on a sequence of data.",
            "If we use a distribution corresponding to an action, so that's minus log this probability.",
            "So from now on I use natural logarithms.",
            "So then the log in the cancels.",
            "That's just eater times to some of the losses plus.",
            "A term which does not depend on the data only on the sample size.",
            "So it's an affair."
        ],
        [
            "Function of the data.",
            "So.",
            "This means that we've transformed the origonal.",
            "Loss function and corresponding set of actions to a new.",
            "Setting where the loss function is not a log arhythmic loss and the corresponding the set of actions has been transformed to a set of probability distributions such that for every action.",
            "If an action veteran action is.",
            "The better its corresponding probability distribution is in the lock sense, and that's what we need for a good mapping so that what is good loss in the original setting is now good luck loss."
        ],
        [
            "So.",
            "If you look at the difference between.",
            "Two different prediction strategies.",
            "You see that?",
            "Now this this sets cancel from everything and you see that the difference.",
            "Enriching lock loss of the transformed strategies is equal to the original difference times ether.",
            "This means that if a difference between two loss functions in the original game gets 2 times as large, the differences of the transformed actions in the locals game also gets twice as large, which is exactly what we would like.",
            "And note also that I hear I also do this for strategies, right?",
            "This doesn't just hold for fixed actions which you don't change overtime, but you can also do this.",
            "You can map each strategy in the original loss function to a strategy for the log loss."
        ],
        [
            "So prime example already said his least squares, so then you get this.",
            "Ellis loss.",
            "Here becomes the squared loss.",
            "So now if we have an action which can depend on additional X value, this becomes Wyman HX squared and then you can actually calculate this integral and you find that if you do a variable transformation eat as 1 / 2 Sigma squared, then that either becomes.",
            "So that's an integral over the Sigma squared because square root, 2\u03c0 Sigma squared and this just becomes a familiar story.",
            "For maximum likelihood Gaussian noise right?"
        ],
        [
            "So now we're going to do this transformation whether or not it's a good idea.",
            "We don't know yet.",
            "I would just like to notice that it's done a lot in the literature.",
            "So for every.",
            "Action or strategy theater.",
            "We map it.",
            "Two distribution and now we do base with that distribution.",
            "So we fix a meet and we do base and then now the basin prediction of the next outcome given the previous outcomes will be, well, the probability according to see to weighted by the posterior that's just base right?",
            "So but it now looks like this one.",
            "I was at Easter time Times E to the minus eat a loss but waited by the posterior.",
            "And if you calculate the posterior doceti to cancel again from here and here.",
            "So just eat reminds eat at times some of wealthy to prime.",
            "Of this thing."
        ],
        [
            "So.",
            "Now, um.",
            "Before we go on there very important note to make, which is that if you do Bayesian prediction with the log loss then you basically you predict by mixing all the theater according to the posterior.",
            "So you predict with a mixture of all the theater.",
            "And a mixture of all the seats are may not be an element of your model unless your set of fetus is convex.",
            "If it's not convex then you may step outside.",
            "Now.",
            "If we want a prediction strategy.",
            "Sequential strategy that we can also apply in the original law setting.",
            "And of course we can transform the original setting.",
            "To the log loss, we get new prediction strategies PSE to the minus eater loss, but if we do base in this lock loss space and we want to map it back, we have a problem because based sometimes mixes these each of the minus either things and there may be no action who's lost somehow corresponds to the lock loss of the mixture, right?",
            "We're mixing things in any original space, we may not be allowed to do that.",
            "Right, so we need some if we do base.",
            "That's what we're going to try to do.",
            "We want to keep the good feature features of base, which is that it learns very fast with a log loss.",
            "So we map everything to the log loss we want to base their.",
            "But now we get Bashan predictions, but these predictions are mixture, so they may not map back to the original space, so they may not be there, maybe become meaningless if we want to make prediction for the original loss function.",
            "So we need some algorithm to transform base back to the original space.",
            "So one thing you could do is not predict by the patient predictive distribution, which is a mixture, but take the maximum a posterior distribution based on the posterior.",
            "Alright, so I call it use red to denote an algorithm for an algorithm to transform the base predictive distribution, or rather the posterior to something which Maps back to the original space.",
            "So can you make some Maple story and then the low class will be minus log of the probability of the maximum posteriori.",
            "This can be met back to the original space.",
            "This is again a linear function of what some action of the loss some action makes in the original space.",
            "You can also this is probably where the real basin would do take the P which minimizes the posterior expected loss.",
            "So you look at what is your expected loss according to your posterior.",
            "You take the P which minimizes it.",
            "You predict with that."
        ],
        [
            "Another example, which you're going to use a lot this, rather than mixing according to the posterior, you randomize.",
            "According to it.",
            "We will allow this, so we'll say, OK, I really want to mix, right?",
            "I have probability distributions and mix them.",
            "I get a new distribution that doesn't work.",
            "What I can do instead is I take the posterior to choose one of the distributions and then I play that distribution.",
            "And that does also map back to an action in the original space.",
            "And S notation.",
            "I again so I have different.",
            "Transformations from bashan.",
            "Predictions to general algorithms, and I will use for an arbitrary transformation.",
            "The low class on the whole sequence I will forget about this notation, just as to some of these losses."
        ],
        [
            "So now we get to the clue the first part.",
            "Of the talk.",
            "So.",
            "We notice this thing will behave very well, right?",
            "Anne.",
            "This thing may behave worse because we somehow have to transform base so that it doesn't step outside the model essentially.",
            "And we will call the difference, which depends on the algorithm we will use for the transformation.",
            "The mixability gap.",
            "I call it with a star because at our NIPS paper it was defined slightly differently.",
            "So this is in terms of bits.",
            "If you think of coding the extra number of bits.",
            "You lose if you do coding by being forced to use something which Maps also to the original loss function.",
            "Yeah."
        ],
        [
            "So now we know that for every data basis it's very nice property, right?",
            "This means that if we transform base to some other algorithm which stays inside the model, we still have this nice property.",
            "But we have an additional loss, which is this gap, right difference?",
            "It's just by definition it's the difference between what bass does and what the transform thing does.",
            "But this thing can be interpreted back.",
            "To the original space of loss functions were interested in like for example 01 loss.",
            "This looks like minus log E to the minus eater, so it's eat at times loss plus something you have the same plus something here that cancels so you get eater times last smaller equal.",
            "Anita Times loss you divide by Eater then this thing becomes equal to this thing, so the original loss we started with the transformation of base we can use that in the original loss for the original loss function algorithm.",
            "It makes the same noise every theater minus lock the prior.",
            "With an extra factor, if we take either small, so we hope that we can take it a large.",
            "Plus an extra thing which comes because we cannot use the predictive based distrib."
        ],
        [
            "No, it's not.",
            "So here later I will put in.",
            "When it gets really unclear, I'll put it in here, but for here I decided to leave it out.",
            "But it definitely depends on them, yes, so in a way this is an old story.",
            "It's not due to me, although in its original form it's not.",
            "It's phrased in a slightly different way, which doesn't mention bits, but it's essentially the same and it goes back to fork.",
            "And so he says he solves this issue that we have this extra term here.",
            "We want to be as good as base, but we can be so folks solves it by saying, well, some loss functions have a property which equals mixability.",
            "And that means that if you take an ether small enough but larger than zero, then all these predictions, these based predictive distributions actually can be met back.",
            "So there exists some action in the original space.",
            "That always performs on all possible data, at least as good as the base predicted distribution.",
            "So for essentially these are loss functions which are bounded and strictly convex.",
            "It's not exactly the same as mixable, but then it will usually go right.",
            "If you have convexity, you're usually OK.",
            "So then you can solve this, but you have to usually take ether source smaller than one to get this.",
            "So for the squared loss, if the outcomes are bounded between minus one and one think you need to take it as 1/2 so you get an extra factor 2 here and then this you can throw this away."
        ],
        [
            "So this same thing we've already seen next one."
        ],
        [
            "So there's another group of algorithms which have been cited a lot in the literature called Hatch and its variants, and the details are a bit different.",
            "But to put it in nicely in this story, you can think of hatch essentially as doing this thing, but randomizing, according to the posterior rather than mixing by the posterior, if you randomize according to the posterior rather than mix, then you get here something which well depends on the difference between the two.",
            "And in the hedge algorithms, they take a finite Theatre, a uniform prior.",
            "And then they show that this thing actually can be pretty large, but it's easy to get smaller.",
            "This gap will get smaller as well, so it's easy to get smaller even with the with the absolute loss, which is not mixable.",
            "You can get this thing smaller and smaller, so then they optimize overeater they have about on this.",
            "And then they optimize this whole thing so that the bound is minimized and then in the end they end up picking an ether which gets smaller and smaller in time.",
            "And it can get the smallest 1 / sqrt N. So basically you are learning.",
            "You do something like base, but your learning rate decreases.",
            "You learn slower and slower.",
            "And then you get this regret."
        ],
        [
            "You can do that.",
            "So you see here this minus log prior has been transformed to lock K have a uniform price.",
            "It's not 1 / K. So notice that if the best experts, the best Theta, has lost zero, then this is very good.",
            "You're like in the log lost case, but if the best expert has linear loss, this is not so good.",
            "You have a square root of end times a constant here, so it's much worse."
        ],
        [
            "So now the first thing which is really new.",
            "This thing if we do the randomized posterior will also be small if the posterior concentrates.",
            "And that is actually what the patient will believe will happen if your base, and you think that as you get more and more data, you will learn you will put all your posterior mass on things which are close to each other.",
            "Extreme case, if the posterior puts all its mass on one distribution, then clearly there is no difference between randomizing according to the posterior and mixing according to the posterior.",
            "By mixing you put probability on one or something predicted.",
            "Otherwise you choose something with probability one and predict with it if all the things are close to each other.",
            "Small Public Library versions to each other.",
            "You can also show that the difference between randomizing and predicting goes to zero and so first mixing and then predicting is always better.",
            "And the difference gets smaller and smaller.",
            "If the posterior is more and more concentrated, so in our NIPS 2011 paper we.",
            "Use this.",
            "Annina hedge, like context with the finer theater in a uniform prior and then we see that if the posterior need concentrates.",
            "We get a lot better balance because this thing becomes much smaller."
        ],
        [
            "But I want to talk with you about something which we're working on now.",
            "And now we're going to assume that the data are IID, so we got.",
            "Do something different.",
            "This holds for all possible data, so in particular it holds if data are IID.",
            "And then we're going to connect it to some ideas in classification which seem entirely unconnected to basing inference."
        ],
        [
            "So now we have also X data and said we have a sample which is ID X&Y data, typical learning setting.",
            "We now have a countable set of predictors will use as running example to 01 loss which is not mixable, so you cannot use Firefox tricks.",
            "Um?",
            "Each is a classifier.",
            "Something happened with his aether.",
            "And so this setting, so it's known to be kind of a bad setting.",
            "Um?",
            "Because the 01 loss doesn't work very well.",
            "Anne.",
            "And so it's known that if you look at not the sequential loss, but the risks are the expected difference between your learning algorithm if you use it to predict the next why value and the Bayes classifier, it can only go to zero at 1 / sqrt N if you go to sequential predicting have to essentially multiplied it by N, so then it becomes square root of and again so it's bad.",
            "But she broke off in papers which in theory community have become very famous outside the theory community waiting up, but inside theory for a few years, lots and lots of paper about.",
            "See backups conditions.",
            "He shows that if the true distribution here, right?",
            "So we assume that this is generated going through some piece star.",
            "If that is in some sense well behaved, then you can do better than this and well behaved means that the probability that you get an X such that the true probability that Y is one is very close to 1/2 is small, so with very high probability you get an X such that the probability that Y is one is close to one or close to 0.",
            "So you're far away from the decision boundary that makes the problem easier."
        ],
        [
            "So, um.",
            "How does this relate to what just been doing?",
            "So.",
            "Now in this setting where there's a distribution, we can define an optimal hypothesis.",
            "Let's just assume it exists.",
            "So maybe in classification is just the optimal classifier in R model.",
            "Minimizes expected loss overall hypothesis.",
            "And so we know that this holds for all individual data, so in particular it also holds if we take expectations according to some distributions and also the X values won't change anything.",
            "So this is just the same thing as we had before.",
            "Anne.",
            "If you use urandom, you use the base algorithm with some given eater.",
            "You have a posterior and you make an actual prediction classification of the next X by randomizing a quarter according to the posterior randomized prediction than you expected loss.",
            "Is bounded by the expected loss of base plus this difference, but you lose because you cannot use base, so that's just the total loss of the best 1 minus locked prior of the best one again divided by ether.",
            "So this we already knew."
        ],
        [
            "Same thing when I write it differently.",
            "This is an abbreviation for.",
            "How will this random predicting with the randomized for serior behaves summed over an outcomes in 01 less compared to the best?",
            "Data, right?",
            "That's the total extra loss you make compared to the best one in expectation."
        ],
        [
            "Yeah.",
            "So now we can prove that.",
            "Suppose that P star has see book of Exponent Q.",
            "So this Q.",
            "Tells you how difficult classification with P star is.",
            "0 means that there's no constraints at all on Q. Infinity means that the probability that there is with probability one you're never closer than some epsilon to 1/2.",
            "So then you're really always far away from this decision boundary.",
            "That's the easiest case.",
            "So if P star has a certain typical exponent Q.",
            "You set the learning rate in this way.",
            "And then you look at what is the expected thing I lose because I cannot use what I really want to use, which is mixing according to base you see is bounded by constant times minus log prior, so it becomes of the same order magnitude as this.",
            "And essentially you're still performing well if you set it to like this.",
            "So now that we have this is some constant we have to let Y to decrease with M in the worst case, without any assumptions on distribution and Q is 0 and then we get into the minus one half 1 / sqrt N. So then the learning rate goes just like in H is 1 / sqrt N right in the best case.",
            "I I get here I need it but I get back to that in the best case.",
            "So Q is Infinity then this becomes independent event.",
            "It becomes one.",
            "So then heater doesn't decrease.",
            "The learning rate doesn't decrease overtime and we already have that.",
            "This thing behaves nicely."
        ],
        [
            "1.",
            "So if you plug in this learning rate into this formula.",
            "You see that now we get.",
            "That's the risk we make is.",
            "Minus locked prior times something.",
            "Which, if in the best case.",
            "And it appears it disappears and it's just like in the Lockless case where in happy case like last loss and in the worst case it becomes square root of M. And.",
            "These are very close to the minimax optimal rates.",
            "The best rates you can get to this for these problems.",
            "The only problem is that here if Q gets becomes zero then this should be square root.",
            "Around here I'm working on that one such."
        ],
        [
            "It using base with that pizza and randomizing gives you the smallest tier one loss on the sample yet so far, and you use that Eater to predict the next why right?",
            "So it turns out so basically, it's like maximum likelihood plug-in estimation.",
            "You pick the ether which was best in the past, the learning rate and use it to predict the next outcome.",
            "It turns out that this works, so you get something which works at least as well up to constant factors as if you would not know that E book of exponent of Q in advance.",
            "And there's some idea from Poland and hotter here in a very different context, which is needed to prove this.",
            "Exponentially in Edmond.",
            "Sorry.",
            "In it gets yeah, I mean well it's for all there is some fixed constants that for all N. And your.",
            "Your total risk so far is within a constant factor of what you would get.",
            "If you would have used the right type of exponent.",
            "Right, so so the constant doesn't change overtime.",
            "That's clear.",
            "I mean it, what happens is essentially you can guess the constant just by looking at the data.",
            "That's that's what happens here."
        ],
        [
            "So.",
            "If Eater decreases slow enough so that the book of condition holds, we have that this thing behaves like the penalty we have for base itself already.",
            "The minus lock prior, and we're in a very nice case so."
        ],
        [
            "Now we step back from these other loss functions and we just do probabilistic prediction again with a countable set of distributions.",
            "And we will make maybe naive assumption, namely at one of the theater which gets positive.",
            "Prior mass is true.",
            "It's a true conditional distribution of Y given X.",
            "Now he said Peter is 1 and then with this these definitions.",
            "This is what we're doing is just ordinary base.",
            "We just randomize rather than predict according to predict if mixing distribution.",
            "And then it turns out that if we do that.",
            "Then we get.",
            "We also get that the expected thing we look for randomization is bounded by constant times minus log prior.",
            "So basically, if we do ordinary base and the model is true and we would randomize rather than mix, we would only lose a constant extra term compared to base."
        ],
        [
            "But if the true distribution is not in there, this may not hold.",
            "Right, so if the best distribution in terms of KLF versions, which is what we would like base to learn to converge to, is at non zero pale diversions from the true distribution, and we may not have this."
        ],
        [
            "And then it can even happen that.",
            "Even as you get more and more data, this difference remains large forever.",
            "Much, much larger than this.",
            "So this means that.",
            "Predicting by randomizing remains worse, much worse than predicting by mixing.",
            "And this is the key to what goes wrong.",
            "If you do can go wrong if you base when the model is wrong."
        ],
        [
            "Because when I first got the idea that maybe base doesn't work very well.",
            "Sometimes when the model is wrong, I thought it can be because base is so good, right?",
            "The difference between your log loss and the one of the best approximation to the truth is always bounded by minus lock the prior independent events, so that's very good.",
            "You predict very well, right?",
            "But if this is large, then it still can happen that.",
            "So this thing must be small.",
            "But this thing, if you randomize, can be much larger than this thing, right?",
            "So what does it mean?",
            "That means that if you do Bashan sequential prediction, you do predict about as well or even better than the best element in your model, right?",
            "Which is closest to the truth, but you do so not by having a posterior which concentrates on that element, but by having a posterior which puts its mass on many very bad distributions and mixing them, they become good.",
            "And this actually happens so."
        ],
        [
            "So John and I in our 2007 paper the examples we have there.",
            "What can happen if you do Bayesian inference when the model is wrong?",
            "Give an example where if you work it out that you can show that exactly this happens.",
            "So base in local sense gives good predictions.",
            "But the predictions are done.",
            "Actually the predictions become better than the predictions of the best element of your model, because base keeps mixing bad things.",
            "But the mix is very good, and that's also expressed by this thing."
        ],
        [
            "So now if you're real Bashan.",
            "Then you would think that you would look at how large will this gap be, right?",
            "This difference between predicting, mixing and posterior randomizing and then on the reasonable assumption in your prior.",
            "It turns out that.",
            "You will think the expectation you have.",
            "Of how large this will be smaller than some constant independent of N. So this means that if you're a real subjective base and you think that you will learn fast that the posterior will concentrate.",
            "So you can think of looking at this Delta star.",
            "And now it's completely detached from this loss function store.",
            "You can also think of this as a test if you just do probabilistic inference with base, whether your model is correct.",
            "If this Delta is large, then something happened which you didn't expect, right?",
            "I realize this is fake, it's just because it's not clear how large this constant is and so on.",
            "But this is an indication that something happened which you didn't expect to happen as a bashan, and this may be an indication that either your model your prior is something wrong about it, so you can think of measuring this Delta.",
            "The difference between mixing and.",
            "Predicting.",
            "My randomization as some kind of test of whether your model model is correct."
        ],
        [
            "And so this brings us to the second part of the talk.",
            "OK, no.",
            "So what's interesting here is that people mostly know the use of data compression statistics or machine learning.",
            "They they know some Merle minimum message length of MDL or dissolution of prior and this is all about sequential prediction or batch learning or estimation.",
            "But historically at least as important was the use of codes compression for testing hypothesis and this has been somehow completely forgotten.",
            "But this is in a way how it started in the beginning there was a lot more.",
            "Work on this, so I'm going to say a bit more about that so you can skip the next one."
        ],
        [
            "So.",
            "What we said here is if we have other loss functions, we can map it to the lock loss and then we can still do something like MDL base, but we have to use constraint codes which remain in the model.",
            "Um?",
            "Here if we use quotes to test, there's no reason to stay in the model.",
            "We can use any code we like to and how will we use quotes to try to show that some hypothesis about the data is wrong by compressing it more than we would expect.",
            "So in this case of Delta, which you just saw, a basin thinks the Delta will be small if Delta is large.",
            "This is an indication that something is wrong, so now we will look at this from a general point of view."
        ],
        [
            "So suppose I have a hypothesis which I'm going to test that the data come from distribution P0.",
            "No, for any distribution Q it turns out this is just Markov's inequality that the probability that you can compress the data with Q more than with P0 by K bits is small and two to the minus K. So to compress that, the probability that Q uses a code compress it more than 20 bits small and two to the minus 20 incredibly small."
        ],
        [
            "So now let's exponentiate.",
            "Let's drop the losses.",
            "This is equivalent.",
            "Two saying the probability on the P0 that the likelihood ratio between POQ is much smaller than Alpha is much smaller than Alpha for any Alpha next."
        ],
        [
            "You can prove both of these very easy by markets inequality.",
            "So Markov says the probability that Q / P is larger than B smaller than this expectation divided by B.",
            "This expectation is 1 because the P zeros cancel and some over X end of Q is 1, so it's 1 / B.",
            "So if you take Elf is 1 / B you get this one, and if you exponentiate you get this one."
        ],
        [
            "So likelihood ratio with difference is a P value, because this is the different definition of a P value.",
            "Right, so the probability that P0 gives much smaller probability than some alternative Q under P0.",
            "Is smaller than Alpha.",
            "P values are usually defined with having this equal to Alpha.",
            "So this is a non strict P."
        ],
        [
            "And the fact that it is non strict is actually great.",
            "And we will see why now."
        ],
        [
            "So just to tell you that significance testing is something which is used a lot in almost all medical experiments are based on significance testing.",
            "And it's usually based on P values, so you have P0 like medication is no effect and you test it against one or more alternatives.",
            "Q, Like medication at least as this in this effect.",
            "And you reject P0 and say the medication has an effect.",
            "If you observe AP value smaller than 0.05.",
            "So."
        ],
        [
            "Next one.",
            "So the problem is that the standard P values that people use are incredibly problematic, which may be one reason for why there are big problems in medicine that you know many things when you try to replicate them, turn out not to work anymore.",
            "Think about 30% of the most cited results when people try to replicate them.",
            "Are not replicable."
        ],
        [
            "Give"
        ],
        [
            "But I'll skip to cart."
        ],
        [
            "You can see later it's a lot of fun with it."
        ],
        [
            "It explains how P values work.",
            "So.",
            "There are many problems with P values with.",
            "Unfortunately, people don't realize they're hard to interpret this 0.05 bound.",
            "It's all about fails if you have more than two actions not go into that.",
            "An important thing is that they are not robust.",
            "If you change the way you sample data.",
            "So suppose.",
            "You want to check whether medication works.",
            "You've already tested 60 patients.",
            "You have P value 0.06 almost significant.",
            "What do you do?",
            "You think?",
            "Well, maybe it just works.",
            "I just don't have enough data to see it.",
            "So let's test three more people.",
            "You're not allowed to do that.",
            "Now you could say that's cheating.",
            "Of course you shouldn't be allowed to do that.",
            "Perhaps as a psychologist told me he once had a case like this.",
            "You sent it to a Journal.",
            "You have 0.06.",
            "The reply you get is why didn't you test it on a few more people.",
            "People do this all the time, right?",
            "Psychologists and Medical Sciences.",
            "They have to be more careful, but it happens all the time.",
            "What's worse is that.",
            "The P values also depends on what you would have done in this situation that did not occur in this.",
            "I think you cannot see that as cheating.",
            "This is really a problem.",
            "People hardly any partition noses I think."
        ],
        [
            "So suppose I test new medication on hundreds.",
            "I got P 0.04, so I'm happy so.",
            "But just to be sure I ask a statistician whether I've done everything well.",
            "An Association is not what would you have done?",
            "If you'd reached an almost but not quite significant result.",
            "Atenas 100.",
            "That's what if you had had 0.06 instead of 0.04 as well, I don't know.",
            "I haven't thought about that.",
            "Maybe of course I would have asked my boss.",
            "Maybe you would have given me more money to test more people and then the statistician must say, well, your experiment is dead because the P value calculated doesn't say anything about the experiment you did, so you don't have a significant result.",
            "Many people don't even believe this, but you can try it out for yourself.",
            "It's true."
        ],
        [
            "So now if we use this likelihood ratio prefers this problem goes away.",
            "I won't explain it here, but it does.",
            "And the reason is because the P value is not strict.",
            "There's a slight wiggle room, and this wiggle room is just good enough.",
            "So we let this problem go away.",
            "It doesn't depend on what you do in situations which wouldn't occur.",
            "OK, so then let's go on."
        ],
        [
            "So this is the the end.",
            "So in sequential prediction batch learning we transformed hypothesis to the log loss.",
            "And then we use some constraint based code.",
            "In testing you can do a similar thing every arbitrary test you have a test statistic, you can transform it to a likelihood ratio test, which is basically data compression using a similar method is here and the compression test will then be a lot more robust than the original test."
        ],
        [
            "So, um.",
            "Well, given the time, I think I also skipped."
        ],
        [
            "This.",
            "And so, why isn't this patient, right?",
            "We transform everything to a probability distribution and then we predict according to that and go to the next one."
        ],
        [
            "So this is the final point, and this is the part which is missing, so I think I have a rather general theory here, which in a way extends base.",
            "But we want to do sequential prediction or testing with distributions, but we don't want to use these to assign probabilities to arbitrary events.",
            "If we do prediction with log loss, we create distribution which only work for for log loss.",
            "If we do 01 loss we create something which works for 01 N unlock loss.",
            "If somebody asks us to make a prediction about a very different function of the data, we would say no, we don't know about that right?",
            "And this sense this is different from probability as it's usually conceived."
        ],
        [
            "61 so.",
            "In ordinary probability, you have a measurable set, right?",
            "You can define measurable sets and you can say that some sets are not measurable and you don't want to say anything about the probability you say it's still defined.",
            "What I really want to do is to generalize this and saying that, OK, you have some functional prediction problem.",
            "Then you want to use probabilities.",
            "Only for that prediction problem, right?",
            "And because use probabilities you can combine them with priors and all kinds of nice things.",
            "But you keep restricting their use because you're never going to assume that these probabilities are true in the classical sense of the word.",
            "So that's the story.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, well, first of all, thank you thanks to the organizers for inviting me here.",
                    "label": 0
                },
                {
                    "sent": "This morning I don't know if you have been this morning, but this morning we heard several people talk about the need for.",
                    "label": 0
                },
                {
                    "sent": "In front of the.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, there's a maybe a distance problem, but I will try to overcome it.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, push the button next.",
                    "label": 0
                },
                {
                    "sent": "Yes, where was I?",
                    "label": 0
                },
                {
                    "sent": "Um, yes.",
                    "label": 0
                },
                {
                    "sent": "So there was talk that there should be more unity machine learning, maybe more relations reductions between different machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, there was at the same time or warning to which I think everyone agreed that was even said.",
                    "label": 0
                },
                {
                    "sent": "Beware of the man who is 1 method for everything.",
                    "label": 0
                },
                {
                    "sent": "Now to some extent.",
                    "label": 0
                },
                {
                    "sent": "I agree with that, but to another extent, I think I want to take up the challenge and show some guts here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I will argue that a lot of machine learning problems can be expressed in terms of bits, although the bit has to be taken with a grain of salt, as we will see.",
                    "label": 0
                },
                {
                    "sent": "But the main thing remains that I think there's a lot of unification possible.",
                    "label": 0
                },
                {
                    "sent": "Not everything but a lot under one particular.",
                    "label": 0
                },
                {
                    "sent": "Let's say group of ideas and to explain it, let me start with the end of my book, of course.",
                    "label": 1
                },
                {
                    "sent": "And if you know I've written a book about minimum description length and in the final chapter of the book I Give an overview of how it stands in relation to other machine learning approaches and I come to the conclusion that although its advocates say that you know it's, it's a hammer that can will solve every problem for you.",
                    "label": 0
                },
                {
                    "sent": "That's not true.",
                    "label": 0
                },
                {
                    "sent": "There are problems, definitely even lots of problems which you cannot very readily solve with MDM.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "All of these problems, still, you can somehow make a link to compression in the sense that if you are able to really learn something to predict better than my random guessing, then implicitly you can always say that you have also compressed your data.",
                    "label": 0
                },
                {
                    "sent": "And I think this is often a useful realization which leads to new insights.",
                    "label": 0
                },
                {
                    "sent": "And that is what this talk is about.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's not about MDL, it's also not about base, although we will do very similar things.",
                    "label": 0
                },
                {
                    "sent": "So the main thesis is a wide variety of problems in machine learning, and statistics can be rephrased as compression or what is equivalent sequential gambolling and in the Kelly setting which means that you can put your money on different outcomes and and that is the essential thing which makes it innocence equivalent to coding data.",
                    "label": 1
                },
                {
                    "sent": "You are free to divide your money in any way you like so you don't have to put everything.",
                    "label": 0
                },
                {
                    "sent": "On one outcome you can divide things.",
                    "label": 0
                },
                {
                    "sent": "And log loss prediction as studied in the theoretical machine learning communities.",
                    "label": 1
                },
                {
                    "sent": "Also the same thing and then this is useful to do so.",
                    "label": 0
                },
                {
                    "sent": "Many things we will do will have a base in flavor, but still, as we will see, this is really not based.",
                    "label": 0
                },
                {
                    "sent": "There is an important twist.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in other words, that I really like, which was mentioned this morning about these people who like to explain everything in terms of 1 theory that is called in a science fiction novel.",
                    "label": 0
                },
                {
                    "sent": "But I've seen that other people are using it as well.",
                    "label": 0
                },
                {
                    "sent": "Now, mono couso tech Sophia.",
                    "label": 0
                },
                {
                    "sent": "So I agree that in general this is a bad idea to try to explain everything with the same tool.",
                    "label": 0
                },
                {
                    "sent": "The same hammer, but I will again also for the sake of argument, go a little bit in its defense, and saying that at least in some cases.",
                    "label": 0
                },
                {
                    "sent": "I'd be very good idea to look at existing approaches and try to find out what really works about them.",
                    "label": 0
                },
                {
                    "sent": "What doesn't work and try to find the Common Core.",
                    "label": 0
                },
                {
                    "sent": "And here is 1 example artificial intelligence in the 1980s.",
                    "label": 0
                },
                {
                    "sent": "If you would ask somebody at the HK conference in the 1980s how to represent uncertainty, they would say depends on the problem.",
                    "label": 1
                },
                {
                    "sent": "You could use Dempster Shafer certainty factors like in my singing expert system, fuzzy logic, possibility theory, nonmonotonic logic.",
                    "label": 0
                },
                {
                    "sent": "I've even worked on that and in some cases where you have repeatable events, you might want to use probability.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Actually, the UI conference was started by Judea Pearl Ann.",
                    "label": 0
                },
                {
                    "sent": "Raise alarm on off and a few other peoples because they thought this was nonsense.",
                    "label": 0
                },
                {
                    "sent": "If you look at UAA now and even at each car you see that most paper most papers have the same way of dealing with uncertainty because it's just the most general, an successful it's probability or in some cases sets of probabilities.",
                    "label": 0
                },
                {
                    "sent": "But even if you work with sets of probabilities, the basic notion is still probability.",
                    "label": 0
                },
                {
                    "sent": "So I think here it's this was very good develop.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, we're going to use probability for predictions, but with A twist.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So when I talk about a code of data in this talk, I really mean a sequential prediction strategy relative to log loss, and I'll explain what that is in a moment.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that even if you don't know about data compression, you can follow this talk, because sequential prediction with loss is relatively easy to explain.",
                    "label": 0
                },
                {
                    "sent": "Of course, if we wanted something general, we will often have to deal with other loss functions, like in classification, the 01 loss or asymmetric loss.",
                    "label": 0
                },
                {
                    "sent": "So what we will do here is we will transform those two log loss for that has been done a lot in the past by basins.",
                    "label": 0
                },
                {
                    "sent": "Anne, but things can go wrong if you do this, and that is what a large part of this talk will be about.",
                    "label": 0
                },
                {
                    "sent": "How can you do this in the right way?",
                    "label": 0
                },
                {
                    "sent": "And of course, the idea of this transformation is the simplest case, something you're all familiar with.",
                    "label": 0
                },
                {
                    "sent": "It was already realized by Gauss that if you do regression, you can do least squares.",
                    "label": 0
                },
                {
                    "sent": "You can also assume that Y is equal to a linear function of X + 0, mean Gaussian noise and maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Least squares is the same as maximizing log likelihood, which is really the same as minimizing love loss.",
                    "label": 0
                },
                {
                    "sent": "That's the trans type of transformation we're going to use.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I'm going to explain the sequential prediction, and for those of you not familiar with it, this will also be a crash course about what the people who go to the cold computational Learning Theory conference are usually working on.",
                    "label": 0
                },
                {
                    "sent": "And we will come up with something with our NIPS posted this year was about the mixability gap and we will show how that unifies several different things.",
                    "label": 0
                },
                {
                    "sent": "Working with different loss functions.",
                    "label": 0
                },
                {
                    "sent": "I think a thing called that ebook of exponent.",
                    "label": 0
                },
                {
                    "sent": "If you do assume a true distribution and based learning models are wrong, this will be the end of the hard part and then in the last 10 minutes I will switch to something where I don't won't give any details, just an overview so it will be a lot more relaxed where I will see where we will see that the same issues about sequential data compression can also be used to address some issues in statistics testing of hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So online prediction.",
                    "label": 0
                },
                {
                    "sent": "So here assume we have some sequence of data can be discrete or continuous.",
                    "label": 0
                },
                {
                    "sent": "They will take space, take values in some space, curly why?",
                    "label": 0
                },
                {
                    "sent": "So the goal is to sequentially predict the next outcome given the past.",
                    "label": 1
                },
                {
                    "sent": "And we will assume for now that we do our predictions using distribution later.",
                    "label": 0
                },
                {
                    "sent": "We will also look at, let's say point predictions, things like that.",
                    "label": 1
                },
                {
                    "sent": "So a prediction strategy S is in a function which Maps the past to prediction for the next outcome, right for each individual for each initial sequence of outcomes.",
                    "label": 0
                },
                {
                    "sent": "Or strategy tells you given that initial sequence, how should you predict the next outcome, right?",
                    "label": 0
                },
                {
                    "sent": "You can think of a Markov chain, for example is doing this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can measure the qualities of such predictions in many ways, and it sets here the main focus will be on the log loss.",
                    "label": 1
                },
                {
                    "sent": "So then for a single outcome, if you predicted by distribution P, you define the log lost to the minus logarithm of P alright, logarithm base two here later.",
                    "label": 0
                },
                {
                    "sent": "In this talk I switch to natural logarithm, but it doesn't make any difference for the results.",
                    "label": 0
                },
                {
                    "sent": "And then I always use this notation, so the loss on a sequence if you use a strategy for predicting it is the sum of the individual losses.",
                    "label": 0
                },
                {
                    "sent": "So you predict the first outcome.",
                    "label": 0
                },
                {
                    "sent": "You predict the second, given the first, the third given first 2, and so on, and you Add all the losses.",
                    "label": 0
                },
                {
                    "sent": "There are two practical settings which correspond to this.",
                    "label": 1
                },
                {
                    "sent": "The first is data compression, so you can think of predicting with lock loss as essentially the same thing as coding data.",
                    "label": 0
                },
                {
                    "sent": "There are some cave caveats because quotes always have integer lengths, but by and large these that's not a problem.",
                    "label": 0
                },
                {
                    "sent": "You can match these two things, and another thing is what I also said before Kelly Gambolling, where you gamble on the different outcomes in why some bookie tells you if that is the outcome you get so much money that is.",
                    "label": 0
                },
                {
                    "sent": "You get so much money and you're allowed to put your capital in any way you like on the different outcomes.",
                    "label": 0
                },
                {
                    "sent": "And then if you exponentiate everything here, it turns out that the smaller the lock loss, the larger the factor by which your capital grows in time.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now if we for a moment we make probabilistic assumptions and we think that our data comes from some distribution completely arbitrary, any distribution, then because lock losses a certain property which is that it's proper, the optimal predictions we can make.",
                    "label": 0
                },
                {
                    "sent": "If we think that our to predict by makes a lot of sense, the conditional distributions corresponding to this distribution, right?",
                    "label": 1
                },
                {
                    "sent": "If you think the data comes from this distribution and you want to use the conditional distribution given the past to predict.",
                    "label": 0
                },
                {
                    "sent": "So what happens in this logarithmic loss prediction setting is that we turn the reasoning around and we say, well, if we have a prediction strategy S that tells you for each past probability distribution over the next outcome, right?",
                    "label": 0
                },
                {
                    "sent": "It's a distribution, so now you can.",
                    "label": 0
                },
                {
                    "sent": "Create a new distribution by multiplying these probabilities right?",
                    "label": 0
                },
                {
                    "sent": "You can think of these as conditional probabilities for the next outcome given the past, and you define a joint probability by the multiplication of these predicted probabilities.",
                    "label": 0
                },
                {
                    "sent": "Which is just this, and it turns out that if you sum this over all values of why you get one.",
                    "label": 0
                },
                {
                    "sent": "So now you've created a new distribution.",
                    "label": 0
                },
                {
                    "sent": "Which is just a distribution over data which corresponds to your prediction strategy.",
                    "label": 0
                },
                {
                    "sent": "So from now on we will identify probabilistic prediction strategies with distributions over the whole sequence.",
                    "label": 0
                },
                {
                    "sent": "There are some issues here.",
                    "label": 0
                },
                {
                    "sent": "If you assign probability zero to some things, I've just gloss over that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now what we want to do, and this will be the key goal in the first part of the talk is suppose we have a large set of predictors.",
                    "label": 0
                },
                {
                    "sent": "And we think that some of them might be good.",
                    "label": 0
                },
                {
                    "sent": "We don't know which one.",
                    "label": 0
                },
                {
                    "sent": "And we want some algorithm which takes us input to predictors the predictors of black box.",
                    "label": 0
                },
                {
                    "sent": "We don't need to know how they work and it outputs.",
                    "label": 0
                },
                {
                    "sent": "Some predictions such that we want that we perform about as well as the best one in this set.",
                    "label": 0
                },
                {
                    "sent": "And I'll let this set be countable, because it makes the mathematics easy, but otherwise it can be very large, so it could be all Markov chains of each order.",
                    "label": 1
                },
                {
                    "sent": "And to make it accountable and make the parameters rational values for example.",
                    "label": 0
                },
                {
                    "sent": "And later we will also do this.",
                    "label": 0
                },
                {
                    "sent": "With polynomials and things like that.",
                    "label": 0
                },
                {
                    "sent": "So this think of this as a potentially large set, even though it's countable.",
                    "label": 0
                },
                {
                    "sent": "And now I somehow want to get some algorithms such that if one of these is a good sequential low class predictor, I can also make good luck loss predictions.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one possibility which turns out to be.",
                    "label": 0
                },
                {
                    "sent": "In a way, perhaps even the best one.",
                    "label": 0
                },
                {
                    "sent": "Is to do this as a basin would, so you put some prior distribution mass function on the set of distributions rather than parameters.",
                    "label": 1
                },
                {
                    "sent": "You can define a marginal distribution based on marginal distribution like this.",
                    "label": 0
                },
                {
                    "sent": "And now you predict, like a bashan would, just with the conditional distribution, right?",
                    "label": 0
                },
                {
                    "sent": "Given the past you have.",
                    "label": 0
                },
                {
                    "sent": "This is just the definition of conditional probability.",
                    "label": 0
                },
                {
                    "sent": "But it's a trivial rewrite, shows this is identical to the prediction of the next outcome given the past, according to theater weighted overall theater by the posterior rather than the prior right?",
                    "label": 0
                },
                {
                    "sent": "So you weight each theater by how good it predicted in the past.",
                    "label": 0
                },
                {
                    "sent": "If it predicted well if its probability was high, then its log loss was small.",
                    "label": 0
                },
                {
                    "sent": "It get your large wait for the next prediction.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So this is just base posterior where we used Bayes rule.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So how well does this work?",
                    "label": 0
                },
                {
                    "sent": "So here there's some interesting mathematics which many of you will know.",
                    "label": 0
                },
                {
                    "sent": "If you don't know what what's important is to remember the conclusion, not so much the mathematics.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we have an arbitrary strategy, we can think of that as a probability distribution, right?",
                    "label": 0
                },
                {
                    "sent": "We can always predict the next outcome given the past and then for total loss will be the sum of these losses.",
                    "label": 0
                },
                {
                    "sent": "So by definition it's the sum of these minus logarithms.",
                    "label": 0
                },
                {
                    "sent": "So that's as we've seen before.",
                    "label": 0
                },
                {
                    "sent": "If you write down the definition of this, you get this ratio here of two probabilities, the minus the sum of minus lockers minus log of a product.",
                    "label": 0
                },
                {
                    "sent": "So if you write out the product in the ratio, everything cancels diagonally and you end up with this.",
                    "label": 0
                },
                {
                    "sent": "Right, so this total loss is just a minus log likelihood you assigned to the data.",
                    "label": 0
                },
                {
                    "sent": "That's what makes log loss so nice mathematically.",
                    "label": 0
                },
                {
                    "sent": "This holds for every P and for the base strategy it must also hold.",
                    "label": 0
                },
                {
                    "sent": "And for this we can say more.",
                    "label": 0
                },
                {
                    "sent": "We know that the total loss we make if we predict like a base in route right with a prior.",
                    "label": 0
                },
                {
                    "sent": "Overall these theater.",
                    "label": 0
                },
                {
                    "sent": "It's the minus log of this probability.",
                    "label": 0
                },
                {
                    "sent": "So that's the minus log of this song.",
                    "label": 0
                },
                {
                    "sent": "Now some is always larger than each of its terms, so minus log off so much smaller.",
                    "label": 0
                },
                {
                    "sent": "Then minus log of each of its terms so.",
                    "label": 0
                },
                {
                    "sent": "This minus log of this so much smaller than minus look this times this because the logarithm of a product is the sum of the logarithms.",
                    "label": 0
                },
                {
                    "sent": "We get that the total loss we make with base sequentially.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by the loss we make with any theater.",
                    "label": 0
                },
                {
                    "sent": "Plus an additional term which is kind of the penalty for Theta.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that this usually increases linearly nanan the penalties constant constant and end.",
                    "label": 0
                },
                {
                    "sent": "So as we get more and more data, will will be about as good as theater, or perhaps even better, because this holds for all data and for all Theta, right?",
                    "label": 0
                },
                {
                    "sent": "So if we have a bad theater here, it will also hold for a good data and we will be at least as good as a good theater.",
                    "label": 0
                },
                {
                    "sent": "Plus disturb.",
                    "label": 0
                },
                {
                    "sent": "This term here is has a different interpretation from a prior but similar but still different from the usual interpretation of a prior.",
                    "label": 0
                },
                {
                    "sent": "In Bayesian statistics it measures something that has been called luckiness, for example by Bob and others.",
                    "label": 0
                },
                {
                    "sent": "Basically, it means that if you have many predictors, you put a prior on them.",
                    "label": 0
                },
                {
                    "sent": "Now, if you hope that some predictor will perform well, you might want to put a large prior on it.",
                    "label": 0
                },
                {
                    "sent": "If that predicted, and indeed works well, then if you are basing the total loss, you may compare to death predictor.",
                    "label": 0
                },
                {
                    "sent": "We have a small extra overhead.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is 1/2.",
                    "label": 0
                },
                {
                    "sent": "You will never have an overhead of just one.",
                    "label": 0
                },
                {
                    "sent": "Right, if you were unlucky and the best predictor was something you gave a small prior to, your overhead will be much larger, right?",
                    "label": 0
                },
                {
                    "sent": "Because you have maybe 1000 minus log 1000 thousands much larger.",
                    "label": 0
                },
                {
                    "sent": "Right, but still as you get more and more data you will catch up with that predictor as well.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the literature list, this is called basis Universal relative to the set of distributions.",
                    "label": 0
                },
                {
                    "sent": "Theater, universal coat or universal predictor.",
                    "label": 0
                },
                {
                    "sent": "The total low class you make if you predict sequentially with base is bound by the law class you make with any of the theater plus an additional term.",
                    "label": 0
                },
                {
                    "sent": "And the additional term, and this is amazing, doesn't depend on N. So no matter how much you get, doesn't get larger.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now you might say, well, this is log loss, but in practice we often want to do classification like problems.",
                    "label": 0
                },
                {
                    "sent": "Other other problems are the loss functions.",
                    "label": 1
                },
                {
                    "sent": "So can we do the same thing for other loss functions?",
                    "label": 0
                },
                {
                    "sent": "And the answer is no.",
                    "label": 0
                },
                {
                    "sent": "Standard base if you try to do it standard naive Bayesian way with 01 loss, then in the worst case, overall data.",
                    "label": 0
                },
                {
                    "sent": "Note, we're not assuming the data are probabilistically general generated right to be looking at all data in the worst case overall database will be very bad and then you can design an optimal algorithm.",
                    "label": 1
                },
                {
                    "sent": "The worst case and it will get regret where there's suddenly a sqrt N before this term.",
                    "label": 0
                },
                {
                    "sent": "Which is pretty much worse than constant right square root of N is like.",
                    "label": 0
                },
                {
                    "sent": "This will increase the previous hearing an, but you learn a lot slower.",
                    "label": 0
                },
                {
                    "sent": "Basically you will learn to imitate a good theater a lot slower without a loss functions.",
                    "label": 0
                },
                {
                    "sent": "And a lot of the work in the cold community has been about.",
                    "label": 0
                },
                {
                    "sent": "Why is this the case?",
                    "label": 0
                },
                {
                    "sent": "And maybe in some cases even with 01 loss, can we do better?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we will do is we'll try to replicate the whole story for other loss functions by transforming them to log loss, and then we will see what will go wrong.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to do that, I already announced that we use this idea, which in a way is already due to Gauss.",
                    "label": 0
                },
                {
                    "sent": "I called it entropy fication myself at some point, but name hasn't caught on.",
                    "label": 0
                },
                {
                    "sent": "It either explicitly or implicitly occurs in a lot of papers over the last 20 years.",
                    "label": 0
                },
                {
                    "sent": "So suppose you have some loss function, for example 01 loss, then this is zero or one, and this would also be 01.",
                    "label": 0
                },
                {
                    "sent": "We allow that it's infinite.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "For every action with this loss function, we can transform it to a probability distribution probabilistic prediction.",
                    "label": 1
                },
                {
                    "sent": "By taking the probability of Y to be equal to E to the minus eater times the loss.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is probability now.",
                    "label": 0
                },
                {
                    "sent": "Depends on a an eater.",
                    "label": 0
                },
                {
                    "sent": "You can extend this to an outcomes by independence.",
                    "label": 1
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can do this for just about every loss function, although superficially there seems to be a big problem because this so this is at pizza is to normalize.",
                    "label": 1
                },
                {
                    "sent": "This is just the sum in the discrete case, or otherwise the integral overall Y of this, so that this thing if you sum it over by becomes one becomes a real probability.",
                    "label": 0
                },
                {
                    "sent": "So you see that in general this should depend on A and interesting before the 01 loss and the squared loss.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on a, so there we can remove a from here and this.",
                    "label": 0
                },
                {
                    "sent": "This set doesn't depend on a for other loss functions, you can use a trick to get the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's too complicated to deal with here, but you can do that with asymmetric loss functions as well.",
                    "label": 0
                },
                {
                    "sent": "But it's more complicated, but so let's just ignore the set for this talk.",
                    "label": 0
                },
                {
                    "sent": "Let's just act as if it doesn't depend on the action.",
                    "label": 1
                },
                {
                    "sent": "So now we have a mapping from every action to loss functions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with this mapping we have, we have that the log loss we make on a sequence of data.",
                    "label": 0
                },
                {
                    "sent": "If we use a distribution corresponding to an action, so that's minus log this probability.",
                    "label": 0
                },
                {
                    "sent": "So from now on I use natural logarithms.",
                    "label": 0
                },
                {
                    "sent": "So then the log in the cancels.",
                    "label": 0
                },
                {
                    "sent": "That's just eater times to some of the losses plus.",
                    "label": 0
                },
                {
                    "sent": "A term which does not depend on the data only on the sample size.",
                    "label": 0
                },
                {
                    "sent": "So it's an affair.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function of the data.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This means that we've transformed the origonal.",
                    "label": 0
                },
                {
                    "sent": "Loss function and corresponding set of actions to a new.",
                    "label": 0
                },
                {
                    "sent": "Setting where the loss function is not a log arhythmic loss and the corresponding the set of actions has been transformed to a set of probability distributions such that for every action.",
                    "label": 0
                },
                {
                    "sent": "If an action veteran action is.",
                    "label": 0
                },
                {
                    "sent": "The better its corresponding probability distribution is in the lock sense, and that's what we need for a good mapping so that what is good loss in the original setting is now good luck loss.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you look at the difference between.",
                    "label": 0
                },
                {
                    "sent": "Two different prediction strategies.",
                    "label": 0
                },
                {
                    "sent": "You see that?",
                    "label": 0
                },
                {
                    "sent": "Now this this sets cancel from everything and you see that the difference.",
                    "label": 0
                },
                {
                    "sent": "Enriching lock loss of the transformed strategies is equal to the original difference times ether.",
                    "label": 1
                },
                {
                    "sent": "This means that if a difference between two loss functions in the original game gets 2 times as large, the differences of the transformed actions in the locals game also gets twice as large, which is exactly what we would like.",
                    "label": 0
                },
                {
                    "sent": "And note also that I hear I also do this for strategies, right?",
                    "label": 0
                },
                {
                    "sent": "This doesn't just hold for fixed actions which you don't change overtime, but you can also do this.",
                    "label": 0
                },
                {
                    "sent": "You can map each strategy in the original loss function to a strategy for the log loss.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So prime example already said his least squares, so then you get this.",
                    "label": 0
                },
                {
                    "sent": "Ellis loss.",
                    "label": 0
                },
                {
                    "sent": "Here becomes the squared loss.",
                    "label": 0
                },
                {
                    "sent": "So now if we have an action which can depend on additional X value, this becomes Wyman HX squared and then you can actually calculate this integral and you find that if you do a variable transformation eat as 1 / 2 Sigma squared, then that either becomes.",
                    "label": 0
                },
                {
                    "sent": "So that's an integral over the Sigma squared because square root, 2\u03c0 Sigma squared and this just becomes a familiar story.",
                    "label": 0
                },
                {
                    "sent": "For maximum likelihood Gaussian noise right?",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're going to do this transformation whether or not it's a good idea.",
                    "label": 0
                },
                {
                    "sent": "We don't know yet.",
                    "label": 0
                },
                {
                    "sent": "I would just like to notice that it's done a lot in the literature.",
                    "label": 0
                },
                {
                    "sent": "So for every.",
                    "label": 0
                },
                {
                    "sent": "Action or strategy theater.",
                    "label": 0
                },
                {
                    "sent": "We map it.",
                    "label": 0
                },
                {
                    "sent": "Two distribution and now we do base with that distribution.",
                    "label": 0
                },
                {
                    "sent": "So we fix a meet and we do base and then now the basin prediction of the next outcome given the previous outcomes will be, well, the probability according to see to weighted by the posterior that's just base right?",
                    "label": 0
                },
                {
                    "sent": "So but it now looks like this one.",
                    "label": 0
                },
                {
                    "sent": "I was at Easter time Times E to the minus eat a loss but waited by the posterior.",
                    "label": 0
                },
                {
                    "sent": "And if you calculate the posterior doceti to cancel again from here and here.",
                    "label": 0
                },
                {
                    "sent": "So just eat reminds eat at times some of wealthy to prime.",
                    "label": 0
                },
                {
                    "sent": "Of this thing.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now, um.",
                    "label": 0
                },
                {
                    "sent": "Before we go on there very important note to make, which is that if you do Bayesian prediction with the log loss then you basically you predict by mixing all the theater according to the posterior.",
                    "label": 0
                },
                {
                    "sent": "So you predict with a mixture of all the theater.",
                    "label": 1
                },
                {
                    "sent": "And a mixture of all the seats are may not be an element of your model unless your set of fetus is convex.",
                    "label": 0
                },
                {
                    "sent": "If it's not convex then you may step outside.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "If we want a prediction strategy.",
                    "label": 1
                },
                {
                    "sent": "Sequential strategy that we can also apply in the original law setting.",
                    "label": 0
                },
                {
                    "sent": "And of course we can transform the original setting.",
                    "label": 0
                },
                {
                    "sent": "To the log loss, we get new prediction strategies PSE to the minus eater loss, but if we do base in this lock loss space and we want to map it back, we have a problem because based sometimes mixes these each of the minus either things and there may be no action who's lost somehow corresponds to the lock loss of the mixture, right?",
                    "label": 0
                },
                {
                    "sent": "We're mixing things in any original space, we may not be allowed to do that.",
                    "label": 1
                },
                {
                    "sent": "Right, so we need some if we do base.",
                    "label": 0
                },
                {
                    "sent": "That's what we're going to try to do.",
                    "label": 0
                },
                {
                    "sent": "We want to keep the good feature features of base, which is that it learns very fast with a log loss.",
                    "label": 0
                },
                {
                    "sent": "So we map everything to the log loss we want to base their.",
                    "label": 0
                },
                {
                    "sent": "But now we get Bashan predictions, but these predictions are mixture, so they may not map back to the original space, so they may not be there, maybe become meaningless if we want to make prediction for the original loss function.",
                    "label": 0
                },
                {
                    "sent": "So we need some algorithm to transform base back to the original space.",
                    "label": 0
                },
                {
                    "sent": "So one thing you could do is not predict by the patient predictive distribution, which is a mixture, but take the maximum a posterior distribution based on the posterior.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I call it use red to denote an algorithm for an algorithm to transform the base predictive distribution, or rather the posterior to something which Maps back to the original space.",
                    "label": 0
                },
                {
                    "sent": "So can you make some Maple story and then the low class will be minus log of the probability of the maximum posteriori.",
                    "label": 0
                },
                {
                    "sent": "This can be met back to the original space.",
                    "label": 1
                },
                {
                    "sent": "This is again a linear function of what some action of the loss some action makes in the original space.",
                    "label": 0
                },
                {
                    "sent": "You can also this is probably where the real basin would do take the P which minimizes the posterior expected loss.",
                    "label": 0
                },
                {
                    "sent": "So you look at what is your expected loss according to your posterior.",
                    "label": 0
                },
                {
                    "sent": "You take the P which minimizes it.",
                    "label": 0
                },
                {
                    "sent": "You predict with that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example, which you're going to use a lot this, rather than mixing according to the posterior, you randomize.",
                    "label": 0
                },
                {
                    "sent": "According to it.",
                    "label": 0
                },
                {
                    "sent": "We will allow this, so we'll say, OK, I really want to mix, right?",
                    "label": 0
                },
                {
                    "sent": "I have probability distributions and mix them.",
                    "label": 0
                },
                {
                    "sent": "I get a new distribution that doesn't work.",
                    "label": 0
                },
                {
                    "sent": "What I can do instead is I take the posterior to choose one of the distributions and then I play that distribution.",
                    "label": 0
                },
                {
                    "sent": "And that does also map back to an action in the original space.",
                    "label": 0
                },
                {
                    "sent": "And S notation.",
                    "label": 0
                },
                {
                    "sent": "I again so I have different.",
                    "label": 0
                },
                {
                    "sent": "Transformations from bashan.",
                    "label": 0
                },
                {
                    "sent": "Predictions to general algorithms, and I will use for an arbitrary transformation.",
                    "label": 0
                },
                {
                    "sent": "The low class on the whole sequence I will forget about this notation, just as to some of these losses.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we get to the clue the first part.",
                    "label": 0
                },
                {
                    "sent": "Of the talk.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We notice this thing will behave very well, right?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This thing may behave worse because we somehow have to transform base so that it doesn't step outside the model essentially.",
                    "label": 0
                },
                {
                    "sent": "And we will call the difference, which depends on the algorithm we will use for the transformation.",
                    "label": 0
                },
                {
                    "sent": "The mixability gap.",
                    "label": 0
                },
                {
                    "sent": "I call it with a star because at our NIPS paper it was defined slightly differently.",
                    "label": 0
                },
                {
                    "sent": "So this is in terms of bits.",
                    "label": 1
                },
                {
                    "sent": "If you think of coding the extra number of bits.",
                    "label": 0
                },
                {
                    "sent": "You lose if you do coding by being forced to use something which Maps also to the original loss function.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we know that for every data basis it's very nice property, right?",
                    "label": 0
                },
                {
                    "sent": "This means that if we transform base to some other algorithm which stays inside the model, we still have this nice property.",
                    "label": 0
                },
                {
                    "sent": "But we have an additional loss, which is this gap, right difference?",
                    "label": 0
                },
                {
                    "sent": "It's just by definition it's the difference between what bass does and what the transform thing does.",
                    "label": 0
                },
                {
                    "sent": "But this thing can be interpreted back.",
                    "label": 0
                },
                {
                    "sent": "To the original space of loss functions were interested in like for example 01 loss.",
                    "label": 0
                },
                {
                    "sent": "This looks like minus log E to the minus eater, so it's eat at times loss plus something you have the same plus something here that cancels so you get eater times last smaller equal.",
                    "label": 0
                },
                {
                    "sent": "Anita Times loss you divide by Eater then this thing becomes equal to this thing, so the original loss we started with the transformation of base we can use that in the original loss for the original loss function algorithm.",
                    "label": 0
                },
                {
                    "sent": "It makes the same noise every theater minus lock the prior.",
                    "label": 0
                },
                {
                    "sent": "With an extra factor, if we take either small, so we hope that we can take it a large.",
                    "label": 0
                },
                {
                    "sent": "Plus an extra thing which comes because we cannot use the predictive based distrib.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, it's not.",
                    "label": 0
                },
                {
                    "sent": "So here later I will put in.",
                    "label": 0
                },
                {
                    "sent": "When it gets really unclear, I'll put it in here, but for here I decided to leave it out.",
                    "label": 0
                },
                {
                    "sent": "But it definitely depends on them, yes, so in a way this is an old story.",
                    "label": 0
                },
                {
                    "sent": "It's not due to me, although in its original form it's not.",
                    "label": 0
                },
                {
                    "sent": "It's phrased in a slightly different way, which doesn't mention bits, but it's essentially the same and it goes back to fork.",
                    "label": 0
                },
                {
                    "sent": "And so he says he solves this issue that we have this extra term here.",
                    "label": 0
                },
                {
                    "sent": "We want to be as good as base, but we can be so folks solves it by saying, well, some loss functions have a property which equals mixability.",
                    "label": 0
                },
                {
                    "sent": "And that means that if you take an ether small enough but larger than zero, then all these predictions, these based predictive distributions actually can be met back.",
                    "label": 0
                },
                {
                    "sent": "So there exists some action in the original space.",
                    "label": 0
                },
                {
                    "sent": "That always performs on all possible data, at least as good as the base predicted distribution.",
                    "label": 0
                },
                {
                    "sent": "So for essentially these are loss functions which are bounded and strictly convex.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly the same as mixable, but then it will usually go right.",
                    "label": 0
                },
                {
                    "sent": "If you have convexity, you're usually OK.",
                    "label": 0
                },
                {
                    "sent": "So then you can solve this, but you have to usually take ether source smaller than one to get this.",
                    "label": 0
                },
                {
                    "sent": "So for the squared loss, if the outcomes are bounded between minus one and one think you need to take it as 1/2 so you get an extra factor 2 here and then this you can throw this away.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this same thing we've already seen next one.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's another group of algorithms which have been cited a lot in the literature called Hatch and its variants, and the details are a bit different.",
                    "label": 0
                },
                {
                    "sent": "But to put it in nicely in this story, you can think of hatch essentially as doing this thing, but randomizing, according to the posterior rather than mixing by the posterior, if you randomize according to the posterior rather than mix, then you get here something which well depends on the difference between the two.",
                    "label": 0
                },
                {
                    "sent": "And in the hedge algorithms, they take a finite Theatre, a uniform prior.",
                    "label": 0
                },
                {
                    "sent": "And then they show that this thing actually can be pretty large, but it's easy to get smaller.",
                    "label": 0
                },
                {
                    "sent": "This gap will get smaller as well, so it's easy to get smaller even with the with the absolute loss, which is not mixable.",
                    "label": 0
                },
                {
                    "sent": "You can get this thing smaller and smaller, so then they optimize overeater they have about on this.",
                    "label": 0
                },
                {
                    "sent": "And then they optimize this whole thing so that the bound is minimized and then in the end they end up picking an ether which gets smaller and smaller in time.",
                    "label": 0
                },
                {
                    "sent": "And it can get the smallest 1 / sqrt N. So basically you are learning.",
                    "label": 0
                },
                {
                    "sent": "You do something like base, but your learning rate decreases.",
                    "label": 1
                },
                {
                    "sent": "You learn slower and slower.",
                    "label": 0
                },
                {
                    "sent": "And then you get this regret.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do that.",
                    "label": 0
                },
                {
                    "sent": "So you see here this minus log prior has been transformed to lock K have a uniform price.",
                    "label": 0
                },
                {
                    "sent": "It's not 1 / K. So notice that if the best experts, the best Theta, has lost zero, then this is very good.",
                    "label": 0
                },
                {
                    "sent": "You're like in the log lost case, but if the best expert has linear loss, this is not so good.",
                    "label": 0
                },
                {
                    "sent": "You have a square root of end times a constant here, so it's much worse.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the first thing which is really new.",
                    "label": 0
                },
                {
                    "sent": "This thing if we do the randomized posterior will also be small if the posterior concentrates.",
                    "label": 0
                },
                {
                    "sent": "And that is actually what the patient will believe will happen if your base, and you think that as you get more and more data, you will learn you will put all your posterior mass on things which are close to each other.",
                    "label": 0
                },
                {
                    "sent": "Extreme case, if the posterior puts all its mass on one distribution, then clearly there is no difference between randomizing according to the posterior and mixing according to the posterior.",
                    "label": 0
                },
                {
                    "sent": "By mixing you put probability on one or something predicted.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you choose something with probability one and predict with it if all the things are close to each other.",
                    "label": 0
                },
                {
                    "sent": "Small Public Library versions to each other.",
                    "label": 0
                },
                {
                    "sent": "You can also show that the difference between randomizing and predicting goes to zero and so first mixing and then predicting is always better.",
                    "label": 0
                },
                {
                    "sent": "And the difference gets smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "If the posterior is more and more concentrated, so in our NIPS 2011 paper we.",
                    "label": 1
                },
                {
                    "sent": "Use this.",
                    "label": 0
                },
                {
                    "sent": "Annina hedge, like context with the finer theater in a uniform prior and then we see that if the posterior need concentrates.",
                    "label": 0
                },
                {
                    "sent": "We get a lot better balance because this thing becomes much smaller.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I want to talk with you about something which we're working on now.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to assume that the data are IID, so we got.",
                    "label": 0
                },
                {
                    "sent": "Do something different.",
                    "label": 0
                },
                {
                    "sent": "This holds for all possible data, so in particular it holds if data are IID.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to connect it to some ideas in classification which seem entirely unconnected to basing inference.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have also X data and said we have a sample which is ID X&Y data, typical learning setting.",
                    "label": 0
                },
                {
                    "sent": "We now have a countable set of predictors will use as running example to 01 loss which is not mixable, so you cannot use Firefox tricks.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Each is a classifier.",
                    "label": 0
                },
                {
                    "sent": "Something happened with his aether.",
                    "label": 1
                },
                {
                    "sent": "And so this setting, so it's known to be kind of a bad setting.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Because the 01 loss doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And so it's known that if you look at not the sequential loss, but the risks are the expected difference between your learning algorithm if you use it to predict the next why value and the Bayes classifier, it can only go to zero at 1 / sqrt N if you go to sequential predicting have to essentially multiplied it by N, so then it becomes square root of and again so it's bad.",
                    "label": 0
                },
                {
                    "sent": "But she broke off in papers which in theory community have become very famous outside the theory community waiting up, but inside theory for a few years, lots and lots of paper about.",
                    "label": 0
                },
                {
                    "sent": "See backups conditions.",
                    "label": 1
                },
                {
                    "sent": "He shows that if the true distribution here, right?",
                    "label": 0
                },
                {
                    "sent": "So we assume that this is generated going through some piece star.",
                    "label": 1
                },
                {
                    "sent": "If that is in some sense well behaved, then you can do better than this and well behaved means that the probability that you get an X such that the true probability that Y is one is very close to 1/2 is small, so with very high probability you get an X such that the probability that Y is one is close to one or close to 0.",
                    "label": 0
                },
                {
                    "sent": "So you're far away from the decision boundary that makes the problem easier.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "How does this relate to what just been doing?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now in this setting where there's a distribution, we can define an optimal hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Let's just assume it exists.",
                    "label": 0
                },
                {
                    "sent": "So maybe in classification is just the optimal classifier in R model.",
                    "label": 0
                },
                {
                    "sent": "Minimizes expected loss overall hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And so we know that this holds for all individual data, so in particular it also holds if we take expectations according to some distributions and also the X values won't change anything.",
                    "label": 1
                },
                {
                    "sent": "So this is just the same thing as we had before.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If you use urandom, you use the base algorithm with some given eater.",
                    "label": 0
                },
                {
                    "sent": "You have a posterior and you make an actual prediction classification of the next X by randomizing a quarter according to the posterior randomized prediction than you expected loss.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by the expected loss of base plus this difference, but you lose because you cannot use base, so that's just the total loss of the best 1 minus locked prior of the best one again divided by ether.",
                    "label": 0
                },
                {
                    "sent": "So this we already knew.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same thing when I write it differently.",
                    "label": 0
                },
                {
                    "sent": "This is an abbreviation for.",
                    "label": 0
                },
                {
                    "sent": "How will this random predicting with the randomized for serior behaves summed over an outcomes in 01 less compared to the best?",
                    "label": 0
                },
                {
                    "sent": "Data, right?",
                    "label": 0
                },
                {
                    "sent": "That's the total extra loss you make compared to the best one in expectation.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So now we can prove that.",
                    "label": 0
                },
                {
                    "sent": "Suppose that P star has see book of Exponent Q.",
                    "label": 0
                },
                {
                    "sent": "So this Q.",
                    "label": 0
                },
                {
                    "sent": "Tells you how difficult classification with P star is.",
                    "label": 0
                },
                {
                    "sent": "0 means that there's no constraints at all on Q. Infinity means that the probability that there is with probability one you're never closer than some epsilon to 1/2.",
                    "label": 0
                },
                {
                    "sent": "So then you're really always far away from this decision boundary.",
                    "label": 0
                },
                {
                    "sent": "That's the easiest case.",
                    "label": 0
                },
                {
                    "sent": "So if P star has a certain typical exponent Q.",
                    "label": 0
                },
                {
                    "sent": "You set the learning rate in this way.",
                    "label": 0
                },
                {
                    "sent": "And then you look at what is the expected thing I lose because I cannot use what I really want to use, which is mixing according to base you see is bounded by constant times minus log prior, so it becomes of the same order magnitude as this.",
                    "label": 0
                },
                {
                    "sent": "And essentially you're still performing well if you set it to like this.",
                    "label": 0
                },
                {
                    "sent": "So now that we have this is some constant we have to let Y to decrease with M in the worst case, without any assumptions on distribution and Q is 0 and then we get into the minus one half 1 / sqrt N. So then the learning rate goes just like in H is 1 / sqrt N right in the best case.",
                    "label": 0
                },
                {
                    "sent": "I I get here I need it but I get back to that in the best case.",
                    "label": 0
                },
                {
                    "sent": "So Q is Infinity then this becomes independent event.",
                    "label": 0
                },
                {
                    "sent": "It becomes one.",
                    "label": 0
                },
                {
                    "sent": "So then heater doesn't decrease.",
                    "label": 0
                },
                {
                    "sent": "The learning rate doesn't decrease overtime and we already have that.",
                    "label": 0
                },
                {
                    "sent": "This thing behaves nicely.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "So if you plug in this learning rate into this formula.",
                    "label": 0
                },
                {
                    "sent": "You see that now we get.",
                    "label": 0
                },
                {
                    "sent": "That's the risk we make is.",
                    "label": 0
                },
                {
                    "sent": "Minus locked prior times something.",
                    "label": 0
                },
                {
                    "sent": "Which, if in the best case.",
                    "label": 0
                },
                {
                    "sent": "And it appears it disappears and it's just like in the Lockless case where in happy case like last loss and in the worst case it becomes square root of M. And.",
                    "label": 0
                },
                {
                    "sent": "These are very close to the minimax optimal rates.",
                    "label": 0
                },
                {
                    "sent": "The best rates you can get to this for these problems.",
                    "label": 0
                },
                {
                    "sent": "The only problem is that here if Q gets becomes zero then this should be square root.",
                    "label": 0
                },
                {
                    "sent": "Around here I'm working on that one such.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It using base with that pizza and randomizing gives you the smallest tier one loss on the sample yet so far, and you use that Eater to predict the next why right?",
                    "label": 0
                },
                {
                    "sent": "So it turns out so basically, it's like maximum likelihood plug-in estimation.",
                    "label": 0
                },
                {
                    "sent": "You pick the ether which was best in the past, the learning rate and use it to predict the next outcome.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this works, so you get something which works at least as well up to constant factors as if you would not know that E book of exponent of Q in advance.",
                    "label": 1
                },
                {
                    "sent": "And there's some idea from Poland and hotter here in a very different context, which is needed to prove this.",
                    "label": 0
                },
                {
                    "sent": "Exponentially in Edmond.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "In it gets yeah, I mean well it's for all there is some fixed constants that for all N. And your.",
                    "label": 0
                },
                {
                    "sent": "Your total risk so far is within a constant factor of what you would get.",
                    "label": 0
                },
                {
                    "sent": "If you would have used the right type of exponent.",
                    "label": 0
                },
                {
                    "sent": "Right, so so the constant doesn't change overtime.",
                    "label": 0
                },
                {
                    "sent": "That's clear.",
                    "label": 0
                },
                {
                    "sent": "I mean it, what happens is essentially you can guess the constant just by looking at the data.",
                    "label": 0
                },
                {
                    "sent": "That's that's what happens here.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If Eater decreases slow enough so that the book of condition holds, we have that this thing behaves like the penalty we have for base itself already.",
                    "label": 0
                },
                {
                    "sent": "The minus lock prior, and we're in a very nice case so.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we step back from these other loss functions and we just do probabilistic prediction again with a countable set of distributions.",
                    "label": 1
                },
                {
                    "sent": "And we will make maybe naive assumption, namely at one of the theater which gets positive.",
                    "label": 0
                },
                {
                    "sent": "Prior mass is true.",
                    "label": 0
                },
                {
                    "sent": "It's a true conditional distribution of Y given X.",
                    "label": 0
                },
                {
                    "sent": "Now he said Peter is 1 and then with this these definitions.",
                    "label": 0
                },
                {
                    "sent": "This is what we're doing is just ordinary base.",
                    "label": 0
                },
                {
                    "sent": "We just randomize rather than predict according to predict if mixing distribution.",
                    "label": 1
                },
                {
                    "sent": "And then it turns out that if we do that.",
                    "label": 0
                },
                {
                    "sent": "Then we get.",
                    "label": 0
                },
                {
                    "sent": "We also get that the expected thing we look for randomization is bounded by constant times minus log prior.",
                    "label": 0
                },
                {
                    "sent": "So basically, if we do ordinary base and the model is true and we would randomize rather than mix, we would only lose a constant extra term compared to base.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if the true distribution is not in there, this may not hold.",
                    "label": 0
                },
                {
                    "sent": "Right, so if the best distribution in terms of KLF versions, which is what we would like base to learn to converge to, is at non zero pale diversions from the true distribution, and we may not have this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then it can even happen that.",
                    "label": 0
                },
                {
                    "sent": "Even as you get more and more data, this difference remains large forever.",
                    "label": 0
                },
                {
                    "sent": "Much, much larger than this.",
                    "label": 0
                },
                {
                    "sent": "So this means that.",
                    "label": 0
                },
                {
                    "sent": "Predicting by randomizing remains worse, much worse than predicting by mixing.",
                    "label": 0
                },
                {
                    "sent": "And this is the key to what goes wrong.",
                    "label": 1
                },
                {
                    "sent": "If you do can go wrong if you base when the model is wrong.",
                    "label": 1
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because when I first got the idea that maybe base doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "Sometimes when the model is wrong, I thought it can be because base is so good, right?",
                    "label": 1
                },
                {
                    "sent": "The difference between your log loss and the one of the best approximation to the truth is always bounded by minus lock the prior independent events, so that's very good.",
                    "label": 0
                },
                {
                    "sent": "You predict very well, right?",
                    "label": 1
                },
                {
                    "sent": "But if this is large, then it still can happen that.",
                    "label": 0
                },
                {
                    "sent": "So this thing must be small.",
                    "label": 0
                },
                {
                    "sent": "But this thing, if you randomize, can be much larger than this thing, right?",
                    "label": 0
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "That means that if you do Bashan sequential prediction, you do predict about as well or even better than the best element in your model, right?",
                    "label": 0
                },
                {
                    "sent": "Which is closest to the truth, but you do so not by having a posterior which concentrates on that element, but by having a posterior which puts its mass on many very bad distributions and mixing them, they become good.",
                    "label": 0
                },
                {
                    "sent": "And this actually happens so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So John and I in our 2007 paper the examples we have there.",
                    "label": 0
                },
                {
                    "sent": "What can happen if you do Bayesian inference when the model is wrong?",
                    "label": 1
                },
                {
                    "sent": "Give an example where if you work it out that you can show that exactly this happens.",
                    "label": 0
                },
                {
                    "sent": "So base in local sense gives good predictions.",
                    "label": 0
                },
                {
                    "sent": "But the predictions are done.",
                    "label": 0
                },
                {
                    "sent": "Actually the predictions become better than the predictions of the best element of your model, because base keeps mixing bad things.",
                    "label": 0
                },
                {
                    "sent": "But the mix is very good, and that's also expressed by this thing.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now if you're real Bashan.",
                    "label": 0
                },
                {
                    "sent": "Then you would think that you would look at how large will this gap be, right?",
                    "label": 0
                },
                {
                    "sent": "This difference between predicting, mixing and posterior randomizing and then on the reasonable assumption in your prior.",
                    "label": 0
                },
                {
                    "sent": "It turns out that.",
                    "label": 0
                },
                {
                    "sent": "You will think the expectation you have.",
                    "label": 0
                },
                {
                    "sent": "Of how large this will be smaller than some constant independent of N. So this means that if you're a real subjective base and you think that you will learn fast that the posterior will concentrate.",
                    "label": 1
                },
                {
                    "sent": "So you can think of looking at this Delta star.",
                    "label": 0
                },
                {
                    "sent": "And now it's completely detached from this loss function store.",
                    "label": 0
                },
                {
                    "sent": "You can also think of this as a test if you just do probabilistic inference with base, whether your model is correct.",
                    "label": 0
                },
                {
                    "sent": "If this Delta is large, then something happened which you didn't expect, right?",
                    "label": 0
                },
                {
                    "sent": "I realize this is fake, it's just because it's not clear how large this constant is and so on.",
                    "label": 1
                },
                {
                    "sent": "But this is an indication that something happened which you didn't expect to happen as a bashan, and this may be an indication that either your model your prior is something wrong about it, so you can think of measuring this Delta.",
                    "label": 0
                },
                {
                    "sent": "The difference between mixing and.",
                    "label": 0
                },
                {
                    "sent": "Predicting.",
                    "label": 0
                },
                {
                    "sent": "My randomization as some kind of test of whether your model model is correct.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this brings us to the second part of the talk.",
                    "label": 0
                },
                {
                    "sent": "OK, no.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting here is that people mostly know the use of data compression statistics or machine learning.",
                    "label": 0
                },
                {
                    "sent": "They they know some Merle minimum message length of MDL or dissolution of prior and this is all about sequential prediction or batch learning or estimation.",
                    "label": 0
                },
                {
                    "sent": "But historically at least as important was the use of codes compression for testing hypothesis and this has been somehow completely forgotten.",
                    "label": 0
                },
                {
                    "sent": "But this is in a way how it started in the beginning there was a lot more.",
                    "label": 0
                },
                {
                    "sent": "Work on this, so I'm going to say a bit more about that so you can skip the next one.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we said here is if we have other loss functions, we can map it to the lock loss and then we can still do something like MDL base, but we have to use constraint codes which remain in the model.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "Here if we use quotes to test, there's no reason to stay in the model.",
                    "label": 0
                },
                {
                    "sent": "We can use any code we like to and how will we use quotes to try to show that some hypothesis about the data is wrong by compressing it more than we would expect.",
                    "label": 0
                },
                {
                    "sent": "So in this case of Delta, which you just saw, a basin thinks the Delta will be small if Delta is large.",
                    "label": 0
                },
                {
                    "sent": "This is an indication that something is wrong, so now we will look at this from a general point of view.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So suppose I have a hypothesis which I'm going to test that the data come from distribution P0.",
                    "label": 0
                },
                {
                    "sent": "No, for any distribution Q it turns out this is just Markov's inequality that the probability that you can compress the data with Q more than with P0 by K bits is small and two to the minus K. So to compress that, the probability that Q uses a code compress it more than 20 bits small and two to the minus 20 incredibly small.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's exponentiate.",
                    "label": 0
                },
                {
                    "sent": "Let's drop the losses.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent.",
                    "label": 0
                },
                {
                    "sent": "Two saying the probability on the P0 that the likelihood ratio between POQ is much smaller than Alpha is much smaller than Alpha for any Alpha next.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can prove both of these very easy by markets inequality.",
                    "label": 0
                },
                {
                    "sent": "So Markov says the probability that Q / P is larger than B smaller than this expectation divided by B.",
                    "label": 0
                },
                {
                    "sent": "This expectation is 1 because the P zeros cancel and some over X end of Q is 1, so it's 1 / B.",
                    "label": 0
                },
                {
                    "sent": "So if you take Elf is 1 / B you get this one, and if you exponentiate you get this one.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So likelihood ratio with difference is a P value, because this is the different definition of a P value.",
                    "label": 0
                },
                {
                    "sent": "Right, so the probability that P0 gives much smaller probability than some alternative Q under P0.",
                    "label": 0
                },
                {
                    "sent": "Is smaller than Alpha.",
                    "label": 0
                },
                {
                    "sent": "P values are usually defined with having this equal to Alpha.",
                    "label": 0
                },
                {
                    "sent": "So this is a non strict P.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the fact that it is non strict is actually great.",
                    "label": 0
                },
                {
                    "sent": "And we will see why now.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to tell you that significance testing is something which is used a lot in almost all medical experiments are based on significance testing.",
                    "label": 0
                },
                {
                    "sent": "And it's usually based on P values, so you have P0 like medication is no effect and you test it against one or more alternatives.",
                    "label": 0
                },
                {
                    "sent": "Q, Like medication at least as this in this effect.",
                    "label": 0
                },
                {
                    "sent": "And you reject P0 and say the medication has an effect.",
                    "label": 0
                },
                {
                    "sent": "If you observe AP value smaller than 0.05.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next one.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that the standard P values that people use are incredibly problematic, which may be one reason for why there are big problems in medicine that you know many things when you try to replicate them, turn out not to work anymore.",
                    "label": 1
                },
                {
                    "sent": "Think about 30% of the most cited results when people try to replicate them.",
                    "label": 0
                },
                {
                    "sent": "Are not replicable.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'll skip to cart.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see later it's a lot of fun with it.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It explains how P values work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are many problems with P values with.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately, people don't realize they're hard to interpret this 0.05 bound.",
                    "label": 1
                },
                {
                    "sent": "It's all about fails if you have more than two actions not go into that.",
                    "label": 1
                },
                {
                    "sent": "An important thing is that they are not robust.",
                    "label": 0
                },
                {
                    "sent": "If you change the way you sample data.",
                    "label": 0
                },
                {
                    "sent": "So suppose.",
                    "label": 0
                },
                {
                    "sent": "You want to check whether medication works.",
                    "label": 0
                },
                {
                    "sent": "You've already tested 60 patients.",
                    "label": 0
                },
                {
                    "sent": "You have P value 0.06 almost significant.",
                    "label": 0
                },
                {
                    "sent": "What do you do?",
                    "label": 0
                },
                {
                    "sent": "You think?",
                    "label": 0
                },
                {
                    "sent": "Well, maybe it just works.",
                    "label": 0
                },
                {
                    "sent": "I just don't have enough data to see it.",
                    "label": 0
                },
                {
                    "sent": "So let's test three more people.",
                    "label": 0
                },
                {
                    "sent": "You're not allowed to do that.",
                    "label": 0
                },
                {
                    "sent": "Now you could say that's cheating.",
                    "label": 0
                },
                {
                    "sent": "Of course you shouldn't be allowed to do that.",
                    "label": 0
                },
                {
                    "sent": "Perhaps as a psychologist told me he once had a case like this.",
                    "label": 0
                },
                {
                    "sent": "You sent it to a Journal.",
                    "label": 0
                },
                {
                    "sent": "You have 0.06.",
                    "label": 0
                },
                {
                    "sent": "The reply you get is why didn't you test it on a few more people.",
                    "label": 0
                },
                {
                    "sent": "People do this all the time, right?",
                    "label": 0
                },
                {
                    "sent": "Psychologists and Medical Sciences.",
                    "label": 0
                },
                {
                    "sent": "They have to be more careful, but it happens all the time.",
                    "label": 0
                },
                {
                    "sent": "What's worse is that.",
                    "label": 0
                },
                {
                    "sent": "The P values also depends on what you would have done in this situation that did not occur in this.",
                    "label": 1
                },
                {
                    "sent": "I think you cannot see that as cheating.",
                    "label": 0
                },
                {
                    "sent": "This is really a problem.",
                    "label": 0
                },
                {
                    "sent": "People hardly any partition noses I think.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So suppose I test new medication on hundreds.",
                    "label": 0
                },
                {
                    "sent": "I got P 0.04, so I'm happy so.",
                    "label": 0
                },
                {
                    "sent": "But just to be sure I ask a statistician whether I've done everything well.",
                    "label": 1
                },
                {
                    "sent": "An Association is not what would you have done?",
                    "label": 1
                },
                {
                    "sent": "If you'd reached an almost but not quite significant result.",
                    "label": 0
                },
                {
                    "sent": "Atenas 100.",
                    "label": 0
                },
                {
                    "sent": "That's what if you had had 0.06 instead of 0.04 as well, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I haven't thought about that.",
                    "label": 0
                },
                {
                    "sent": "Maybe of course I would have asked my boss.",
                    "label": 0
                },
                {
                    "sent": "Maybe you would have given me more money to test more people and then the statistician must say, well, your experiment is dead because the P value calculated doesn't say anything about the experiment you did, so you don't have a significant result.",
                    "label": 0
                },
                {
                    "sent": "Many people don't even believe this, but you can try it out for yourself.",
                    "label": 0
                },
                {
                    "sent": "It's true.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now if we use this likelihood ratio prefers this problem goes away.",
                    "label": 1
                },
                {
                    "sent": "I won't explain it here, but it does.",
                    "label": 0
                },
                {
                    "sent": "And the reason is because the P value is not strict.",
                    "label": 0
                },
                {
                    "sent": "There's a slight wiggle room, and this wiggle room is just good enough.",
                    "label": 0
                },
                {
                    "sent": "So we let this problem go away.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on what you do in situations which wouldn't occur.",
                    "label": 1
                },
                {
                    "sent": "OK, so then let's go on.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the the end.",
                    "label": 0
                },
                {
                    "sent": "So in sequential prediction batch learning we transformed hypothesis to the log loss.",
                    "label": 0
                },
                {
                    "sent": "And then we use some constraint based code.",
                    "label": 0
                },
                {
                    "sent": "In testing you can do a similar thing every arbitrary test you have a test statistic, you can transform it to a likelihood ratio test, which is basically data compression using a similar method is here and the compression test will then be a lot more robust than the original test.",
                    "label": 1
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Well, given the time, I think I also skipped.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "And so, why isn't this patient, right?",
                    "label": 0
                },
                {
                    "sent": "We transform everything to a probability distribution and then we predict according to that and go to the next one.",
                    "label": 1
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the final point, and this is the part which is missing, so I think I have a rather general theory here, which in a way extends base.",
                    "label": 0
                },
                {
                    "sent": "But we want to do sequential prediction or testing with distributions, but we don't want to use these to assign probabilities to arbitrary events.",
                    "label": 0
                },
                {
                    "sent": "If we do prediction with log loss, we create distribution which only work for for log loss.",
                    "label": 0
                },
                {
                    "sent": "If we do 01 loss we create something which works for 01 N unlock loss.",
                    "label": 0
                },
                {
                    "sent": "If somebody asks us to make a prediction about a very different function of the data, we would say no, we don't know about that right?",
                    "label": 1
                },
                {
                    "sent": "And this sense this is different from probability as it's usually conceived.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "61 so.",
                    "label": 0
                },
                {
                    "sent": "In ordinary probability, you have a measurable set, right?",
                    "label": 0
                },
                {
                    "sent": "You can define measurable sets and you can say that some sets are not measurable and you don't want to say anything about the probability you say it's still defined.",
                    "label": 0
                },
                {
                    "sent": "What I really want to do is to generalize this and saying that, OK, you have some functional prediction problem.",
                    "label": 0
                },
                {
                    "sent": "Then you want to use probabilities.",
                    "label": 0
                },
                {
                    "sent": "Only for that prediction problem, right?",
                    "label": 0
                },
                {
                    "sent": "And because use probabilities you can combine them with priors and all kinds of nice things.",
                    "label": 0
                },
                {
                    "sent": "But you keep restricting their use because you're never going to assume that these probabilities are true in the classical sense of the word.",
                    "label": 0
                },
                {
                    "sent": "So that's the story.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}