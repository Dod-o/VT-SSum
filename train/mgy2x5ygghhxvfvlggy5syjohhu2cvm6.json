{
    "id": "mgy2x5ygghhxvfvlggy5syjohhu2cvm6",
    "title": "Encoding Category Correlations into Bilingual Topic Modeling for Cross-Lingual Taxonomy Alignment",
    "info": {
        "author": [
            "Tianxing Wu, Southeast University"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_wu_category_correlations/",
    "segmentation": [
        [
            "Good morning everyone.",
            "Thanks for coming to my talk.",
            "My name is teaching you from Southeast University, China.",
            "In this talk I will introduce our work encoding categorical relations into bilingual topic modeling for cross lingual taxonomy alignment.",
            "It is the joint work of tension grilling shrimp and come from South East University.",
            "Ann lay from Karlsruhe Institute of Technology.",
            "OK, let's begin."
        ],
        [
            "Here is the outline.",
            "I will first introduce the motivation, then models experiments.",
            "Finally conclusions and future work.",
            "OK, now motor."
        ],
        [
            "Nation.",
            "While the most crucial kinds of knowledge on the web is taxonomy, which is used to organizing classified web data, for example, we can see different product catalogs such as the eBay product taxonomy and different website directories such as Yahoo Direct."
        ],
        [
            "Free on the web.",
            "Also, as the advent of more and more multilingual resources, the web has become a global information space.",
            "So to facilitate schema knowledge sharing across languages.",
            "We need to deal with the problem with cross lingual taxonomy alignment, which is actually to find the relation called the most relevant to between categories from the taxonomies of different languages."
        ],
        [
            "However, there are two problems of the previous work on cross lingual taxonomy alignment.",
            "First, this strongly rely on domain specific information, which is not always available, such as book instances in book domain or calculation items in the finance domain.",
            "Second, they strongly rely on string similarities to capture different kinds of features.",
            "For example, given a Chinese Calgary called Hawaiian down full, we can get its corresponding translated category outdoor sportswear using Google Translate.",
            "But we find that most relevant category is atlanticare power in eBay product taxonomy an these two English categories are totally dissimilar in string similarity."
        ],
        [
            "So in our work published in AAA 2016, we have proposed a new approach to use the vector similarities of categories for cross lingual taxonomy alignment.",
            "Here we first use a cross lingual string similarity to fighter candidate metric algorithm pairs from 2 taxonomies of different languages at the same time.",
            "We extract the texture context of each category in detail.",
            "We first extract the snippets returned by Google.",
            "After querying each category and then construct paired bilinga.",
            "Documents using our translation tool.",
            "After that we will propose bilingual topic model to learn vector representations of each category and compute the relevance between categories using cosine similarity."
        ],
        [
            "However, we found a new problem here.",
            "That is, it only models the textual context of Calgary without considering explicit categorical relations.",
            "First, cooccurrence correlations which exists between the categories and their Co occurring words in text.",
            "With such correlation, maybe we can learn better vector representations of each category.",
            "The second is structural correlations, which means us.",
            "Associations are among the categories of ancestor descendant relationships in a taxonomy.",
            "As we all know that the structural correlations is always useful in the mapping problem.",
            "To see."
        ],
        [
            "This new problem with some rise to challenges here.",
            "First, how to capture both corcorans correlations an structure correlations.",
            "Second, how to integrate such explicit category correlations in bilingual topic modeling?"
        ],
        [
            "Before giving our solution here, we first introduce an important concept in bilingual topic modeling, which is modeling object.",
            "There are two existing bilingual topic models which are by LDA ANB TM.",
            "So modern object in Bio idea is a pair of bilingual documents and model it.",
            "The modeling object in Babe TM is a button."
        ],
        [
            "Abide him, use the imbibed in ozone order War Pair Co occurring a pair of bilingual documents.",
            "Here is an example to show how to generate vitamins from a pair of bilingual documents.",
            "Supposing we have two languages as NT, we will have three kinds of items, which are the bottom with the words of language as the bottom with the words of language T and the items with the words of different languages.",
            "Now overview our solution.",
            "We first transform the cooccurrence correlations."
        ],
        [
            "And the structural correlations into a prior Calgary distribution of each modeling object within integrate all prior Calgary.",
            "Distributions into bilingual topic modeling by design.",
            "General steps of generating awarding each modeling object.",
            "Here the first step is joining category from the prior Calgary distribution of our modeling object.",
            "The second is drawing a topic from the category topic distribution.",
            "The last one is drawing a word from the topic world distribution of some language.",
            "OK, the next is the details of our new models."
        ],
        [
            "After applying our proposed solution to be I LDA, we get a new model called CBI LDA.",
            "The left figure is the graphical representation in the general process of CBI odho.",
            "We first set the number of topics and values for directly priors.",
            "And then for each topic we sample to topic word distributions of different languages from directly distributions.",
            "Then for each category we also sample a category topic distribution from directly distribution.",
            "Finally, for each word in each pair of bilingual documents, we sample a category from the prior category distribution, then a topic from the Calgary topic distribution.",
            "An award from the topic world distribution.",
            "All these steps are based on multinomial distributions."
        ],
        [
            "Also, after applying our solution to, BTN will get the new bilingual topic model CBI BTM.",
            "It has a similar generated process to that of CBI LDA."
        ],
        [
            "So only difference here is their corresponding modeling objects here for each vitamin C. By BTM, we sample a category from prior Calgary topic distribution, a topic from a.",
            "From a category top distribution and two words from a topic word distribution, these steps are all based on multinomial distributions."
        ],
        [
            "And to estimate the parameters we use drip sampling.",
            "Here we are meant lots of formulas of derivation.",
            "Finally we can get the formula here to compute the category topic distribution for each category, which is actually the topic vector representation of each category."
        ],
        [
            "Another important part in our models is how to compute the prior category distribution with different category correlations for Co occurrence correlations.",
            "Each modeling object is denoted as a mix mixture of categories.",
            "When the words in the modeling object Co occur with these categories in pair, an impaired bilingual documents session mixture serves as prior category distribution of each modeling object.",
            "This formula is to compute the prior category probability Pi of each category C based on the Co occurrence correlation and this probability satisfied the uniform distribution."
        ],
        [
            "And here, besides the core course correlation, we also have to another two structure correlations.",
            "So first one is information content based structure, correlation and the second one is possibly in space structure correlations.",
            "Full structural correlations based on information content.",
            "The central idea is that Co occurring categories of modeling objects should have different importances, since they may convey different amounts of information in a taxonomic structure, and the more abstract category that means more closer to the root of our taxonomy, the lower is information content, an all there will there will be no need to further differentiate with descenders.",
            "Queries so we can update.",
            "So.",
            "We can update the Calgary prior Calgary distribution of each model knobs."
        ],
        [
            "Checked with these formulas where considering this kind of structure correlations.",
            "For the structure correlations based on path length will give an example here.",
            "If pair if a pair of bilingual documents has category, computer vision is ancestor categories such as artificial intelligence are also relevant to this pair of bilingual documents.",
            "Here we we argue that when given their number, J modeling objects are the greater distance in the taxonomy between our Co occurring Calgarian its ancestor category.",
            "So lower probability of this ancestor category being are relevant category of our.",
            "So we compute the propagation probability with this formula."
        ],
        [
            "Sings Ancestor card we can get different propagation probabilities from different descendant categories.",
            "We also design an algorithm to update the prior category distribution of each category when considering structure correlations based on path lengths.",
            "OK, the third part experiments."
        ],
        [
            "We evaluated our purpose approach on two different kinds of real world data sets, which are publicly available.",
            "Allowing these datasets is actually two decks, two tasks.",
            "The first one is cross lingual product catalog alignment.",
            "We try to align the product taxonomy of jd.com, which is one of the largest Chinese Beatles, say online retailers to thatofebay.com.",
            "The second is cost linger, website directory alignments.",
            "We align the website directory of Chinese demos.",
            "They talk to that of Yahoo Directory."
        ],
        [
            "We have different baselines which are ontology matching systems, variants of our models.",
            "An existing bilingual topic models, so evaluation metrics, RMR, NPR, one here.",
            "A.",
            "Means our Bollinger models only consider the cooccurrence correlation and be means we use cooccurrence correlations and information content based structure correlations and it is easy for us to find that our models with all the three Calgary corporations have the best performance."
        ],
        [
            "We also have another experiment on the efficiency of our models.",
            "We can find that Sibm has much higher time complexity than that of CBI LDA for cross lingual taxonomy alignment.",
            "If users have a high demand on accuracy and do not care about the efficiency, we suggest to use CC bcme.",
            "If users care more about the efficiency an can accept a little lower accuracy.",
            "We recommend CBI LDA.",
            "OK, finally conclusion."
        ],
        [
            "In future work.",
            "In this work, we propose a unified solution to including categorical relations into bilingual topic modeling for cross lingual taxonomy alignment, which leverages the benefits from both vector similarities.",
            "An explicit Calgary correlations.",
            "Then we have two new bilingual topic models which are CC by LDA NCBI BTM which outperformed the state of baselines on on cross lingual taxonomy alignment.",
            "Future work we will first."
        ],
        [
            "Validate our approach on some domain specific taxonomies such as the data sets in.",
            "Oh yeah, multiform track an also plan to utilize the structure information in large basis to enhance our approach for cross lingual taxonomy alignment and the last one is we will apply our models to cross lingual taxonomy alignment in large graphs to benefit across linger, knowledge Fusion and cross lingual semantic search.",
            "Here are the references.",
            "Also, thank you.",
            "I had a question coming from my own domain.",
            "I need to work with eleven different language European languages to to match categories of fashion products and one problem we have is that different cultures see the products differently.",
            "So for example in English and German there's a word for a code word for a check it, but induction finish you have only one word for both.",
            "Um, how do you work with this kind of cultural differences in multi lingual matching?",
            "OK, so very good question.",
            "Actually we also find that cultural differences do exist in between different languages and here in this work we only map the Chinese taxonomies to English taxonomies and we have the solution that we aim to encode different categories in different languages into the vector representations and so that we find that the.",
            "Performance is much better than only using the string similarities based on translation to us.",
            "As my observation, so I'm curious about your experiment over mapping ebay.com versusjd.com.",
            "So could you share the data size of your of the experiment, say how many product catalogs are you mapping between the two sites?",
            "Actually, in these two taxonomies we have labeled, we have labeled more about more than 100.",
            "Match the categories.",
            "So here we only we test our approach on this label data set so.",
            "Here we do not try to align all the categories in these two taxonomies.",
            "So in our test in our labeled datasets so we can find that we can.",
            "Call we can get a much better performance than the state of our approach.",
            "Yeah, that's it means when you talk about product catalogs, you actually means the categories.",
            "Yeah, I mean, the categories?",
            "Yeah, only categories I see.",
            "Yeah, thanks.",
            "So at the beginning, before you apply your solution, you get the you acquire the contact the textual information for the categories using search engines and you also use a translation tool to tackle ambiguity that there may be on the keywords, yes.",
            "It's a very good question.",
            "I do not sure this the details in the slides.",
            "Actually we have some strategies to find the correct texture context for each category.",
            "For example, if Calgary we joined submit the Calgarians ancestor category to the Google and to find that extra content.",
            "For example, maybe in Yahoo Directory sports have two meanings for first, the first meaning is physical activities or means.",
            "Spalled scoots an so, but when you take the categories sports with its ancestor category, for example, some shopping services or.",
            "Physical activities and they can.",
            "You can avoid this ambiguities."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "Thanks for coming to my talk.",
                    "label": 0
                },
                {
                    "sent": "My name is teaching you from Southeast University, China.",
                    "label": 0
                },
                {
                    "sent": "In this talk I will introduce our work encoding categorical relations into bilingual topic modeling for cross lingual taxonomy alignment.",
                    "label": 1
                },
                {
                    "sent": "It is the joint work of tension grilling shrimp and come from South East University.",
                    "label": 1
                },
                {
                    "sent": "Ann lay from Karlsruhe Institute of Technology.",
                    "label": 0
                },
                {
                    "sent": "OK, let's begin.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the outline.",
                    "label": 0
                },
                {
                    "sent": "I will first introduce the motivation, then models experiments.",
                    "label": 0
                },
                {
                    "sent": "Finally conclusions and future work.",
                    "label": 1
                },
                {
                    "sent": "OK, now motor.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "While the most crucial kinds of knowledge on the web is taxonomy, which is used to organizing classified web data, for example, we can see different product catalogs such as the eBay product taxonomy and different website directories such as Yahoo Direct.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Free on the web.",
                    "label": 0
                },
                {
                    "sent": "Also, as the advent of more and more multilingual resources, the web has become a global information space.",
                    "label": 1
                },
                {
                    "sent": "So to facilitate schema knowledge sharing across languages.",
                    "label": 0
                },
                {
                    "sent": "We need to deal with the problem with cross lingual taxonomy alignment, which is actually to find the relation called the most relevant to between categories from the taxonomies of different languages.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, there are two problems of the previous work on cross lingual taxonomy alignment.",
                    "label": 0
                },
                {
                    "sent": "First, this strongly rely on domain specific information, which is not always available, such as book instances in book domain or calculation items in the finance domain.",
                    "label": 1
                },
                {
                    "sent": "Second, they strongly rely on string similarities to capture different kinds of features.",
                    "label": 0
                },
                {
                    "sent": "For example, given a Chinese Calgary called Hawaiian down full, we can get its corresponding translated category outdoor sportswear using Google Translate.",
                    "label": 0
                },
                {
                    "sent": "But we find that most relevant category is atlanticare power in eBay product taxonomy an these two English categories are totally dissimilar in string similarity.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our work published in AAA 2016, we have proposed a new approach to use the vector similarities of categories for cross lingual taxonomy alignment.",
                    "label": 0
                },
                {
                    "sent": "Here we first use a cross lingual string similarity to fighter candidate metric algorithm pairs from 2 taxonomies of different languages at the same time.",
                    "label": 1
                },
                {
                    "sent": "We extract the texture context of each category in detail.",
                    "label": 1
                },
                {
                    "sent": "We first extract the snippets returned by Google.",
                    "label": 1
                },
                {
                    "sent": "After querying each category and then construct paired bilinga.",
                    "label": 0
                },
                {
                    "sent": "Documents using our translation tool.",
                    "label": 0
                },
                {
                    "sent": "After that we will propose bilingual topic model to learn vector representations of each category and compute the relevance between categories using cosine similarity.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, we found a new problem here.",
                    "label": 1
                },
                {
                    "sent": "That is, it only models the textual context of Calgary without considering explicit categorical relations.",
                    "label": 1
                },
                {
                    "sent": "First, cooccurrence correlations which exists between the categories and their Co occurring words in text.",
                    "label": 1
                },
                {
                    "sent": "With such correlation, maybe we can learn better vector representations of each category.",
                    "label": 1
                },
                {
                    "sent": "The second is structural correlations, which means us.",
                    "label": 0
                },
                {
                    "sent": "Associations are among the categories of ancestor descendant relationships in a taxonomy.",
                    "label": 0
                },
                {
                    "sent": "As we all know that the structural correlations is always useful in the mapping problem.",
                    "label": 0
                },
                {
                    "sent": "To see.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This new problem with some rise to challenges here.",
                    "label": 0
                },
                {
                    "sent": "First, how to capture both corcorans correlations an structure correlations.",
                    "label": 1
                },
                {
                    "sent": "Second, how to integrate such explicit category correlations in bilingual topic modeling?",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before giving our solution here, we first introduce an important concept in bilingual topic modeling, which is modeling object.",
                    "label": 1
                },
                {
                    "sent": "There are two existing bilingual topic models which are by LDA ANB TM.",
                    "label": 1
                },
                {
                    "sent": "So modern object in Bio idea is a pair of bilingual documents and model it.",
                    "label": 0
                },
                {
                    "sent": "The modeling object in Babe TM is a button.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Abide him, use the imbibed in ozone order War Pair Co occurring a pair of bilingual documents.",
                    "label": 0
                },
                {
                    "sent": "Here is an example to show how to generate vitamins from a pair of bilingual documents.",
                    "label": 1
                },
                {
                    "sent": "Supposing we have two languages as NT, we will have three kinds of items, which are the bottom with the words of language as the bottom with the words of language T and the items with the words of different languages.",
                    "label": 0
                },
                {
                    "sent": "Now overview our solution.",
                    "label": 0
                },
                {
                    "sent": "We first transform the cooccurrence correlations.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the structural correlations into a prior Calgary distribution of each modeling object within integrate all prior Calgary.",
                    "label": 1
                },
                {
                    "sent": "Distributions into bilingual topic modeling by design.",
                    "label": 1
                },
                {
                    "sent": "General steps of generating awarding each modeling object.",
                    "label": 0
                },
                {
                    "sent": "Here the first step is joining category from the prior Calgary distribution of our modeling object.",
                    "label": 0
                },
                {
                    "sent": "The second is drawing a topic from the category topic distribution.",
                    "label": 1
                },
                {
                    "sent": "The last one is drawing a word from the topic world distribution of some language.",
                    "label": 0
                },
                {
                    "sent": "OK, the next is the details of our new models.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After applying our proposed solution to be I LDA, we get a new model called CBI LDA.",
                    "label": 0
                },
                {
                    "sent": "The left figure is the graphical representation in the general process of CBI odho.",
                    "label": 0
                },
                {
                    "sent": "We first set the number of topics and values for directly priors.",
                    "label": 0
                },
                {
                    "sent": "And then for each topic we sample to topic word distributions of different languages from directly distributions.",
                    "label": 0
                },
                {
                    "sent": "Then for each category we also sample a category topic distribution from directly distribution.",
                    "label": 0
                },
                {
                    "sent": "Finally, for each word in each pair of bilingual documents, we sample a category from the prior category distribution, then a topic from the Calgary topic distribution.",
                    "label": 0
                },
                {
                    "sent": "An award from the topic world distribution.",
                    "label": 0
                },
                {
                    "sent": "All these steps are based on multinomial distributions.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, after applying our solution to, BTN will get the new bilingual topic model CBI BTM.",
                    "label": 0
                },
                {
                    "sent": "It has a similar generated process to that of CBI LDA.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So only difference here is their corresponding modeling objects here for each vitamin C. By BTM, we sample a category from prior Calgary topic distribution, a topic from a.",
                    "label": 0
                },
                {
                    "sent": "From a category top distribution and two words from a topic word distribution, these steps are all based on multinomial distributions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to estimate the parameters we use drip sampling.",
                    "label": 0
                },
                {
                    "sent": "Here we are meant lots of formulas of derivation.",
                    "label": 1
                },
                {
                    "sent": "Finally we can get the formula here to compute the category topic distribution for each category, which is actually the topic vector representation of each category.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another important part in our models is how to compute the prior category distribution with different category correlations for Co occurrence correlations.",
                    "label": 0
                },
                {
                    "sent": "Each modeling object is denoted as a mix mixture of categories.",
                    "label": 1
                },
                {
                    "sent": "When the words in the modeling object Co occur with these categories in pair, an impaired bilingual documents session mixture serves as prior category distribution of each modeling object.",
                    "label": 1
                },
                {
                    "sent": "This formula is to compute the prior category probability Pi of each category C based on the Co occurrence correlation and this probability satisfied the uniform distribution.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here, besides the core course correlation, we also have to another two structure correlations.",
                    "label": 0
                },
                {
                    "sent": "So first one is information content based structure, correlation and the second one is possibly in space structure correlations.",
                    "label": 0
                },
                {
                    "sent": "Full structural correlations based on information content.",
                    "label": 1
                },
                {
                    "sent": "The central idea is that Co occurring categories of modeling objects should have different importances, since they may convey different amounts of information in a taxonomic structure, and the more abstract category that means more closer to the root of our taxonomy, the lower is information content, an all there will there will be no need to further differentiate with descenders.",
                    "label": 1
                },
                {
                    "sent": "Queries so we can update.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can update the Calgary prior Calgary distribution of each model knobs.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Checked with these formulas where considering this kind of structure correlations.",
                    "label": 0
                },
                {
                    "sent": "For the structure correlations based on path length will give an example here.",
                    "label": 1
                },
                {
                    "sent": "If pair if a pair of bilingual documents has category, computer vision is ancestor categories such as artificial intelligence are also relevant to this pair of bilingual documents.",
                    "label": 1
                },
                {
                    "sent": "Here we we argue that when given their number, J modeling objects are the greater distance in the taxonomy between our Co occurring Calgarian its ancestor category.",
                    "label": 0
                },
                {
                    "sent": "So lower probability of this ancestor category being are relevant category of our.",
                    "label": 0
                },
                {
                    "sent": "So we compute the propagation probability with this formula.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sings Ancestor card we can get different propagation probabilities from different descendant categories.",
                    "label": 0
                },
                {
                    "sent": "We also design an algorithm to update the prior category distribution of each category when considering structure correlations based on path lengths.",
                    "label": 1
                },
                {
                    "sent": "OK, the third part experiments.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We evaluated our purpose approach on two different kinds of real world data sets, which are publicly available.",
                    "label": 1
                },
                {
                    "sent": "Allowing these datasets is actually two decks, two tasks.",
                    "label": 0
                },
                {
                    "sent": "The first one is cross lingual product catalog alignment.",
                    "label": 0
                },
                {
                    "sent": "We try to align the product taxonomy of jd.com, which is one of the largest Chinese Beatles, say online retailers to thatofebay.com.",
                    "label": 0
                },
                {
                    "sent": "The second is cost linger, website directory alignments.",
                    "label": 0
                },
                {
                    "sent": "We align the website directory of Chinese demos.",
                    "label": 0
                },
                {
                    "sent": "They talk to that of Yahoo Directory.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have different baselines which are ontology matching systems, variants of our models.",
                    "label": 1
                },
                {
                    "sent": "An existing bilingual topic models, so evaluation metrics, RMR, NPR, one here.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Means our Bollinger models only consider the cooccurrence correlation and be means we use cooccurrence correlations and information content based structure correlations and it is easy for us to find that our models with all the three Calgary corporations have the best performance.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also have another experiment on the efficiency of our models.",
                    "label": 0
                },
                {
                    "sent": "We can find that Sibm has much higher time complexity than that of CBI LDA for cross lingual taxonomy alignment.",
                    "label": 0
                },
                {
                    "sent": "If users have a high demand on accuracy and do not care about the efficiency, we suggest to use CC bcme.",
                    "label": 0
                },
                {
                    "sent": "If users care more about the efficiency an can accept a little lower accuracy.",
                    "label": 1
                },
                {
                    "sent": "We recommend CBI LDA.",
                    "label": 0
                },
                {
                    "sent": "OK, finally conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In future work.",
                    "label": 0
                },
                {
                    "sent": "In this work, we propose a unified solution to including categorical relations into bilingual topic modeling for cross lingual taxonomy alignment, which leverages the benefits from both vector similarities.",
                    "label": 1
                },
                {
                    "sent": "An explicit Calgary correlations.",
                    "label": 0
                },
                {
                    "sent": "Then we have two new bilingual topic models which are CC by LDA NCBI BTM which outperformed the state of baselines on on cross lingual taxonomy alignment.",
                    "label": 0
                },
                {
                    "sent": "Future work we will first.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Validate our approach on some domain specific taxonomies such as the data sets in.",
                    "label": 1
                },
                {
                    "sent": "Oh yeah, multiform track an also plan to utilize the structure information in large basis to enhance our approach for cross lingual taxonomy alignment and the last one is we will apply our models to cross lingual taxonomy alignment in large graphs to benefit across linger, knowledge Fusion and cross lingual semantic search.",
                    "label": 1
                },
                {
                    "sent": "Here are the references.",
                    "label": 0
                },
                {
                    "sent": "Also, thank you.",
                    "label": 0
                },
                {
                    "sent": "I had a question coming from my own domain.",
                    "label": 0
                },
                {
                    "sent": "I need to work with eleven different language European languages to to match categories of fashion products and one problem we have is that different cultures see the products differently.",
                    "label": 0
                },
                {
                    "sent": "So for example in English and German there's a word for a code word for a check it, but induction finish you have only one word for both.",
                    "label": 0
                },
                {
                    "sent": "Um, how do you work with this kind of cultural differences in multi lingual matching?",
                    "label": 0
                },
                {
                    "sent": "OK, so very good question.",
                    "label": 0
                },
                {
                    "sent": "Actually we also find that cultural differences do exist in between different languages and here in this work we only map the Chinese taxonomies to English taxonomies and we have the solution that we aim to encode different categories in different languages into the vector representations and so that we find that the.",
                    "label": 0
                },
                {
                    "sent": "Performance is much better than only using the string similarities based on translation to us.",
                    "label": 0
                },
                {
                    "sent": "As my observation, so I'm curious about your experiment over mapping ebay.com versusjd.com.",
                    "label": 0
                },
                {
                    "sent": "So could you share the data size of your of the experiment, say how many product catalogs are you mapping between the two sites?",
                    "label": 0
                },
                {
                    "sent": "Actually, in these two taxonomies we have labeled, we have labeled more about more than 100.",
                    "label": 0
                },
                {
                    "sent": "Match the categories.",
                    "label": 0
                },
                {
                    "sent": "So here we only we test our approach on this label data set so.",
                    "label": 0
                },
                {
                    "sent": "Here we do not try to align all the categories in these two taxonomies.",
                    "label": 0
                },
                {
                    "sent": "So in our test in our labeled datasets so we can find that we can.",
                    "label": 0
                },
                {
                    "sent": "Call we can get a much better performance than the state of our approach.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's it means when you talk about product catalogs, you actually means the categories.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, the categories?",
                    "label": 0
                },
                {
                    "sent": "Yeah, only categories I see.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks.",
                    "label": 0
                },
                {
                    "sent": "So at the beginning, before you apply your solution, you get the you acquire the contact the textual information for the categories using search engines and you also use a translation tool to tackle ambiguity that there may be on the keywords, yes.",
                    "label": 0
                },
                {
                    "sent": "It's a very good question.",
                    "label": 0
                },
                {
                    "sent": "I do not sure this the details in the slides.",
                    "label": 0
                },
                {
                    "sent": "Actually we have some strategies to find the correct texture context for each category.",
                    "label": 0
                },
                {
                    "sent": "For example, if Calgary we joined submit the Calgarians ancestor category to the Google and to find that extra content.",
                    "label": 0
                },
                {
                    "sent": "For example, maybe in Yahoo Directory sports have two meanings for first, the first meaning is physical activities or means.",
                    "label": 0
                },
                {
                    "sent": "Spalled scoots an so, but when you take the categories sports with its ancestor category, for example, some shopping services or.",
                    "label": 0
                },
                {
                    "sent": "Physical activities and they can.",
                    "label": 0
                },
                {
                    "sent": "You can avoid this ambiguities.",
                    "label": 0
                }
            ]
        }
    }
}