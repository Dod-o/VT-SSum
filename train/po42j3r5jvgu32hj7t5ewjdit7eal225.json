{
    "id": "po42j3r5jvgu32hj7t5ewjdit7eal225",
    "title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials",
    "info": {
        "author": [
            "Philipp Kr\u00e4henb\u00fchl, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/nips2011_kraehenbuehl_potentials/",
    "segmentation": [
        [
            "So the problem we're looking at in this?"
        ],
        [
            "Paper is multi class image segmentation which is assigning a class label to every pixel in an image where class labels can, for example be chair, table or background."
        ],
        [
            "Now this multi class image segmentation is usually approach as a map inference problem in a condition."
        ],
        [
            "Random field, where the unary term describes on how likely a certain pixel is to take a specific label and those likelihoods are normally found using first extracting some features from an image.",
            "Intent training a classifier on those features.",
            "In our paper, we use texts on boost.",
            "Proposed by Shelton at all and this classifier classifieds.",
            "Every pixel independently of one another."
        ],
        [
            "Such that the final classification can be fairly noisy or inconsistent over the whole image."
        ],
        [
            "So This is why the CRF also contains a pairwise term which ensures or which?",
            "Encourages a consistent labeling over an image.",
            "In the most simple CRF model.",
            "A pairwise term is only formulated over directly neighboring pixels.",
            "And most commonly used is the color sensitive Potts model, which you can see in the bottom of the slide, and it expresses our belief that close by pixels or neighboring pixels with the same color over the similar color should be labeled the same."
        ],
        [
            "Now, inference in the simple great structure is chasing CRF models is fairly efficient.",
            "For example, graph cuts takes roughly a second to do inference in the image here, and decide which contains 50,000 variables."
        ],
        [
            "However, the expressive power of those grid structure models is not.",
            "There is not.",
            "It's very limited."
        ],
        [
            "This is mainly due to the fact that interactions can only be modeled over the over directly adjacent variables."
        ],
        [
            "And the great structure itself further leads to an excessive smoothing of object boundaries.",
            "As you can see on the image on the side here, and this accepted, successive smoothing is also known as the shrinking bias."
        ],
        [
            "No, what we do in our paper.",
            "We use a slightly different CRF model, in which every node has a pairwise connection to every other node.",
            "At."
        ],
        [
            "Then we simply vary the strength of this connection."
        ],
        [
            "And this is what the model gives us."
        ],
        [
            "It's clear since every note is a connection to every note every other note, we can model both local interactions as well as very long range interactions between different various."
        ],
        [
            "Listen to image.",
            "And just by looking at the image here in the side, you can see that the shrinking bias is no longer a problem."
        ],
        [
            "Now we're not the first ones to explore fully connected CRF models.",
            "There's been a large body of work in computer vision on how to use fully connected CRF models on region based CRF's.",
            "However, they're only tractable up to a few 100 variables and what we're dealing."
        ],
        [
            "With a 10s of thousands of variables and billions of edges between those variables, which is computationally much more expensive."
        ],
        [
            "So what I'm going to show you in this talk is how to do efficient inference at those fully connected models and with the deficient if it efficient inference.",
            "I mean inference in a fraction of a second in a model of the size of the image here in the site which contains 50,000 variables and just as a reference, traditional inference methods such as MCMC inference take over a day to do inference in such a model, and graph cuts doesn't even converge within three days."
        ],
        [
            "The only restriction we have to our model is that the pairwise potential needs to be a linear combination of Gaussian kernels and what I mean with that is that we take our fully connected model."
        ],
        [
            "And.",
            "The pairwise potential in this fully connected."
        ],
        [
            "Model is a product of a label compatibility function, mu."
        ],
        [
            "And some are weighted sum of Gaussian kernels where the Gaussian kernels can be and can have an arbitrary."
        ],
        [
            "Shape.",
            "And they can be formulated over an arbitrary feature space."
        ],
        [
            "To be a little more concrete, the model we used in all our experiments is a two kernel model."
        ],
        [
            "Where at label compatibility function is either a simple path."
        ],
        [
            "This model or is learned as a semi metric function from data."
        ],
        [
            "The first kernel in our model is a simple appearance kernel which just expressed this our belief that close by objects with a similar color should be.",
            "You should have the same label and it's a direct extension of the color sensitive Potts model I showed you earlier in those slides."
        ],
        [
            "And the second kernel is just a local smoothness kernel which in which discourages single pixel noise."
        ],
        [
            "Now that we've defined our model, what we want to do is we want to find the most likely assignment or the most likely labeling under that model."
        ],
        [
            "And we do."
        ],
        [
            "Just using a mean field approximation."
        ],
        [
            "Where the meaningful approximation finds a simple probability distribution Q, which is a product of independent marginals and it finds Q, such as this that is as close as possible to pee in terms of the KL diversions.",
            "And once we found Q, we can approximate the maximum posteriori simply by taking the maximum of each of the independent marginals."
        ],
        [
            "Now the means for the approximation is found using the formula here on top of the slide and for detailed derivation I would like to refer you to our paper."
        ],
        [
            "What is formula says is that we first need to initialize our guests for Q and we do this to just, we just initialize it to the unary potentials and then we."
        ],
        [
            "Iterate over."
        ],
        [
            "A message passing step which propagates all approximations Q from any variable to any other variable in the CRF."
        ],
        [
            "Then we apply label compatibility transformation for every variable independent."
        ],
        [
            "Add in the unary."
        ],
        [
            "Term and normalize."
        ],
        [
            "Now it's not hard to see that the computationally expensive part of this mean field approximation is a mess."
        ],
        [
            "Message passing step since it requires us to pass a message from every pixel to every other pixel into CRF."
        ],
        [
            "However, by choosing Gaussian edge potentials, this message passing step is nothing else than a high dimensional Gaussian filter or bilateral filter, and there's been a large body of work in both computer vision and computer graphics on how to evaluate those high dimensional Gaussian filters efficiently, and the method we use is the call to permit it or let us proposed by Adams at all, and I'll now give you some intuition why those Gaussian filters can be evaluated more efficiently."
        ],
        [
            "Let's start by looking at some work done by Paris and Toronto.",
            "Person Toronto observed that given it has an arbitrary Hayden."
        ],
        [
            "Additional input signal as soon as it's convolved with the Gaussian kernel, the signal is smooth and band limited.",
            "As you can see here in green."
        ],
        [
            "And any such smooth and band limited function can be represented by sparse set of samples, which is a direct consequence of the Nyquist theorem."
        ],
        [
            "Now that we know that we can represent the result of the convolution with this poor set of samples pericenter on proposed the following algorithm."
        ],
        [
            "First, downsample the input signal onto this poor set of samples."
        ],
        [
            "Then simply blur the sampled signal in the discrete domain."
        ],
        [
            "And upsample the signal again into the continuous domain.",
            "No sampling in an arbitrary high dimensional space is not trivial, and it actually like the naive sampling requires an exponential number of samples so exponentially in the number of dimensions, and this is known as the curse of dimensionality.",
            "This is why Adams at all proposed."
        ],
        [
            "For material, let us which can take an arbitrary high dimensional input signal shown in green here.",
            "And then test lights the space around that input signal into regular simplices."
        ],
        [
            "And then for every point on the high dimensional input signal, it looks at the enclosing simplex, distributes its value onto this enclosing."
        ],
        [
            "Simplex."
        ],
        [
            "Then blurs the values on the simplices along each dimension of the discretization."
        ],
        [
            "And then OP samples the signal again by looking at the enclosing simplex and using Linea."
        ],
        [
            "Interpolation."
        ],
        [
            "So this parameter let us allow us to take our message passing step in the mean field approximation."
        ],
        [
            "And replace it with a simple high dimensional filter."
        ],
        [
            "Leading to an algorithm that is linear in the number of variables and independent of the number of pairwise connections in the CRF."
        ],
        [
            "Now we can also do learning using high dimensional Gaussian filtering and we can learn both the label compatibility function mu and the weights of the kernels using a filter."
        ],
        [
            "However, learning the shape of the kernel leads to non Gaussian convolution which cannot be computed with a filter and for more details and that I would like to refer you to our paper."
        ],
        [
            "Now let's look at some results.",
            "We evaluated our algorithm on 2 standard datasets.",
            "The first one is the MSR.",
            "See data set which contains almost 600 images and 21 object classes.",
            "And then then."
        ],
        [
            "Emesor See data set the unary classifier, namely, text on boost, already performs fairly well with an 84% accuracy."
        ],
        [
            "And the great CRF only improves slightly upon this accuracy, mainly by reducing some noise in the segmentation."
        ],
        [
            "Now if you look at the fully connected CRF model.",
            "It yields an three times higher improvement than the grid CRF and.",
            "The time required to do inference is roughly a factor of 5 lower.",
            "Now if you look at the segmentations produced by the fully connected CRF, they look much cleaner than the ones produced by either the unary term or the great CRF.",
            "And to measure this cleaner segment."
        ],
        [
            "Mission accuracy we took 94 images out of the MSR, see data set.",
            "We hand annotated them pick select curtly and then use something called the Tri map evaluation, proposed by Kohli at all and the Tri map evaluation measures to percentage of the misclassified pixels along object bound."
        ],
        [
            "Trees.",
            "And what I mean with that is.",
            "Given."
        ],
        [
            "Arbitrary image we look at it's Pixel, Eckert segmentation."
        ],
        [
            "Look at the object boundaries and define the end pixel region around that object boundary as the Tri map of size N. And here you see a pix try map of size."
        ],
        [
            "Four, this is a triumph of size 8."
        ],
        [
            "Now this year is the trauma palliation on the bottom.",
            "You can see the fully connected model which has a much lower.",
            "Error, then both the grid CRF4 unique classifier and.",
            "Here the numbers show the Tri map evaluation for size Infinity, which is for the complete image.",
            "Now again, you can see that the fully connected model has roughly a three times improvement over the simple grid structured CRF."
        ],
        [
            "So the second data set we evaluated our algorithm on is the VOC 2010 data set, which contains almost 2000 images, 20 classes plus a separate background class.",
            "And then here again, the fully connected model improves roughly by a factor of three over the simple grid structure model."
        ],
        [
            "Now this far I've only talked about fully connected CRF's in terms of images, but there is nothing restricting neither the mean field approximation nor the filtering to simple images.",
            "The only restriction, the mean further filtering implies on our algorithm is that we need to use Euclidean feature space.",
            "And we can for example, use the fully connected CRF to segment point clouds, where we use the XYZ position, normal and color of every point as a feature.",
            "Or we can imply we can.",
            "Music to segment meshes where the XYZ position normal if every vertex is used."
        ],
        [
            "So in summary.",
            "I've shown you a fully connected model in which pairwise potentials a linear combination of Gaussian kernels and have presented you with an efficient inference algorithm and is fully connected models where the inference algorithm is linear in the number of variables and independent of the number of pairwise connections."
        ],
        [
            "Now let's look at some future work.",
            "One area of future work we are currently investigating is going beyond the simple mean field approximation and finding an approximation algorithm that is more accurate and I personally believe the key to that is lifting some of the restrictions to filtering imply the filtering imposed on our algorithm, which is the filtering needs to update all variables at once and finding a filtering algorithm that is.",
            "That allows us to update single variables at once is what I think is the key to finding a better inference algorithm in those fully connected models."
        ],
        [
            "Then the second area we are extending the fully connected model to its continuous variables and we are currently investigating using.",
            "Using it for depth reconstruction or optical flow estimation."
        ],
        [
            "Then another area.",
            "It's very interesting is extending it to non Euclidean spaces.",
            "For example in measures or general graphs.",
            "Euclidean space, mitre doesn't exist or is not very meaningful such that spaces such as to distichs or diffusion distance or way more meaningful in those spaces."
        ],
        [
            "And the last point is going beyond the simple label compatibility function.",
            "Right now the label compatibility function is strictly separated from any features in the image, so finding a label compatibility function that can use some of the underlying features in the image would greatly increase the overall accuracy of the order.",
            "The overall power expressive power of the model."
        ],
        [
            "All right, I'd like to mention that all the code for the fully connected CRF models it is available online under the following URL here.",
            "Thank you for your attention and feel free to stop by my poster tonight W 14 and I'm happy to take any questions.",
            "So the question was, what was the computational platform for the timings?",
            "So it was a single core machine CPU and think it was an I-7930."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem we're looking at in this?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper is multi class image segmentation which is assigning a class label to every pixel in an image where class labels can, for example be chair, table or background.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this multi class image segmentation is usually approach as a map inference problem in a condition.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Random field, where the unary term describes on how likely a certain pixel is to take a specific label and those likelihoods are normally found using first extracting some features from an image.",
                    "label": 0
                },
                {
                    "sent": "Intent training a classifier on those features.",
                    "label": 0
                },
                {
                    "sent": "In our paper, we use texts on boost.",
                    "label": 0
                },
                {
                    "sent": "Proposed by Shelton at all and this classifier classifieds.",
                    "label": 0
                },
                {
                    "sent": "Every pixel independently of one another.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such that the final classification can be fairly noisy or inconsistent over the whole image.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So This is why the CRF also contains a pairwise term which ensures or which?",
                    "label": 1
                },
                {
                    "sent": "Encourages a consistent labeling over an image.",
                    "label": 0
                },
                {
                    "sent": "In the most simple CRF model.",
                    "label": 0
                },
                {
                    "sent": "A pairwise term is only formulated over directly neighboring pixels.",
                    "label": 1
                },
                {
                    "sent": "And most commonly used is the color sensitive Potts model, which you can see in the bottom of the slide, and it expresses our belief that close by pixels or neighboring pixels with the same color over the similar color should be labeled the same.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, inference in the simple great structure is chasing CRF models is fairly efficient.",
                    "label": 0
                },
                {
                    "sent": "For example, graph cuts takes roughly a second to do inference in the image here, and decide which contains 50,000 variables.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, the expressive power of those grid structure models is not.",
                    "label": 0
                },
                {
                    "sent": "There is not.",
                    "label": 0
                },
                {
                    "sent": "It's very limited.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is mainly due to the fact that interactions can only be modeled over the over directly adjacent variables.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the great structure itself further leads to an excessive smoothing of object boundaries.",
                    "label": 0
                },
                {
                    "sent": "As you can see on the image on the side here, and this accepted, successive smoothing is also known as the shrinking bias.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, what we do in our paper.",
                    "label": 0
                },
                {
                    "sent": "We use a slightly different CRF model, in which every node has a pairwise connection to every other node.",
                    "label": 1
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we simply vary the strength of this connection.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what the model gives us.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's clear since every note is a connection to every note every other note, we can model both local interactions as well as very long range interactions between different various.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Listen to image.",
                    "label": 0
                },
                {
                    "sent": "And just by looking at the image here in the side, you can see that the shrinking bias is no longer a problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we're not the first ones to explore fully connected CRF models.",
                    "label": 1
                },
                {
                    "sent": "There's been a large body of work in computer vision on how to use fully connected CRF models on region based CRF's.",
                    "label": 1
                },
                {
                    "sent": "However, they're only tractable up to a few 100 variables and what we're dealing.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With a 10s of thousands of variables and billions of edges between those variables, which is computationally much more expensive.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm going to show you in this talk is how to do efficient inference at those fully connected models and with the deficient if it efficient inference.",
                    "label": 0
                },
                {
                    "sent": "I mean inference in a fraction of a second in a model of the size of the image here in the site which contains 50,000 variables and just as a reference, traditional inference methods such as MCMC inference take over a day to do inference in such a model, and graph cuts doesn't even converge within three days.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The only restriction we have to our model is that the pairwise potential needs to be a linear combination of Gaussian kernels and what I mean with that is that we take our fully connected model.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The pairwise potential in this fully connected.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model is a product of a label compatibility function, mu.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And some are weighted sum of Gaussian kernels where the Gaussian kernels can be and can have an arbitrary.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shape.",
                    "label": 0
                },
                {
                    "sent": "And they can be formulated over an arbitrary feature space.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be a little more concrete, the model we used in all our experiments is a two kernel model.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where at label compatibility function is either a simple path.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This model or is learned as a semi metric function from data.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first kernel in our model is a simple appearance kernel which just expressed this our belief that close by objects with a similar color should be.",
                    "label": 0
                },
                {
                    "sent": "You should have the same label and it's a direct extension of the color sensitive Potts model I showed you earlier in those slides.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second kernel is just a local smoothness kernel which in which discourages single pixel noise.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that we've defined our model, what we want to do is we want to find the most likely assignment or the most likely labeling under that model.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we do.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just using a mean field approximation.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the meaningful approximation finds a simple probability distribution Q, which is a product of independent marginals and it finds Q, such as this that is as close as possible to pee in terms of the KL diversions.",
                    "label": 0
                },
                {
                    "sent": "And once we found Q, we can approximate the maximum posteriori simply by taking the maximum of each of the independent marginals.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the means for the approximation is found using the formula here on top of the slide and for detailed derivation I would like to refer you to our paper.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is formula says is that we first need to initialize our guests for Q and we do this to just, we just initialize it to the unary potentials and then we.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Iterate over.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A message passing step which propagates all approximations Q from any variable to any other variable in the CRF.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we apply label compatibility transformation for every variable independent.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add in the unary.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Term and normalize.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now it's not hard to see that the computationally expensive part of this mean field approximation is a mess.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Message passing step since it requires us to pass a message from every pixel to every other pixel into CRF.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, by choosing Gaussian edge potentials, this message passing step is nothing else than a high dimensional Gaussian filter or bilateral filter, and there's been a large body of work in both computer vision and computer graphics on how to evaluate those high dimensional Gaussian filters efficiently, and the method we use is the call to permit it or let us proposed by Adams at all, and I'll now give you some intuition why those Gaussian filters can be evaluated more efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's start by looking at some work done by Paris and Toronto.",
                    "label": 0
                },
                {
                    "sent": "Person Toronto observed that given it has an arbitrary Hayden.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Additional input signal as soon as it's convolved with the Gaussian kernel, the signal is smooth and band limited.",
                    "label": 0
                },
                {
                    "sent": "As you can see here in green.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And any such smooth and band limited function can be represented by sparse set of samples, which is a direct consequence of the Nyquist theorem.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that we know that we can represent the result of the convolution with this poor set of samples pericenter on proposed the following algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, downsample the input signal onto this poor set of samples.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then simply blur the sampled signal in the discrete domain.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And upsample the signal again into the continuous domain.",
                    "label": 0
                },
                {
                    "sent": "No sampling in an arbitrary high dimensional space is not trivial, and it actually like the naive sampling requires an exponential number of samples so exponentially in the number of dimensions, and this is known as the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "This is why Adams at all proposed.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For material, let us which can take an arbitrary high dimensional input signal shown in green here.",
                    "label": 0
                },
                {
                    "sent": "And then test lights the space around that input signal into regular simplices.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then for every point on the high dimensional input signal, it looks at the enclosing simplex, distributes its value onto this enclosing.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simplex.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then blurs the values on the simplices along each dimension of the discretization.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then OP samples the signal again by looking at the enclosing simplex and using Linea.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interpolation.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this parameter let us allow us to take our message passing step in the mean field approximation.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And replace it with a simple high dimensional filter.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leading to an algorithm that is linear in the number of variables and independent of the number of pairwise connections in the CRF.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we can also do learning using high dimensional Gaussian filtering and we can learn both the label compatibility function mu and the weights of the kernels using a filter.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, learning the shape of the kernel leads to non Gaussian convolution which cannot be computed with a filter and for more details and that I would like to refer you to our paper.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at some results.",
                    "label": 0
                },
                {
                    "sent": "We evaluated our algorithm on 2 standard datasets.",
                    "label": 0
                },
                {
                    "sent": "The first one is the MSR.",
                    "label": 0
                },
                {
                    "sent": "See data set which contains almost 600 images and 21 object classes.",
                    "label": 0
                },
                {
                    "sent": "And then then.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Emesor See data set the unary classifier, namely, text on boost, already performs fairly well with an 84% accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the great CRF only improves slightly upon this accuracy, mainly by reducing some noise in the segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if you look at the fully connected CRF model.",
                    "label": 0
                },
                {
                    "sent": "It yields an three times higher improvement than the grid CRF and.",
                    "label": 0
                },
                {
                    "sent": "The time required to do inference is roughly a factor of 5 lower.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at the segmentations produced by the fully connected CRF, they look much cleaner than the ones produced by either the unary term or the great CRF.",
                    "label": 0
                },
                {
                    "sent": "And to measure this cleaner segment.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission accuracy we took 94 images out of the MSR, see data set.",
                    "label": 0
                },
                {
                    "sent": "We hand annotated them pick select curtly and then use something called the Tri map evaluation, proposed by Kohli at all and the Tri map evaluation measures to percentage of the misclassified pixels along object bound.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trees.",
                    "label": 0
                },
                {
                    "sent": "And what I mean with that is.",
                    "label": 0
                },
                {
                    "sent": "Given.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arbitrary image we look at it's Pixel, Eckert segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the object boundaries and define the end pixel region around that object boundary as the Tri map of size N. And here you see a pix try map of size.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Four, this is a triumph of size 8.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this year is the trauma palliation on the bottom.",
                    "label": 0
                },
                {
                    "sent": "You can see the fully connected model which has a much lower.",
                    "label": 1
                },
                {
                    "sent": "Error, then both the grid CRF4 unique classifier and.",
                    "label": 0
                },
                {
                    "sent": "Here the numbers show the Tri map evaluation for size Infinity, which is for the complete image.",
                    "label": 0
                },
                {
                    "sent": "Now again, you can see that the fully connected model has roughly a three times improvement over the simple grid structured CRF.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second data set we evaluated our algorithm on is the VOC 2010 data set, which contains almost 2000 images, 20 classes plus a separate background class.",
                    "label": 0
                },
                {
                    "sent": "And then here again, the fully connected model improves roughly by a factor of three over the simple grid structure model.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this far I've only talked about fully connected CRF's in terms of images, but there is nothing restricting neither the mean field approximation nor the filtering to simple images.",
                    "label": 1
                },
                {
                    "sent": "The only restriction, the mean further filtering implies on our algorithm is that we need to use Euclidean feature space.",
                    "label": 0
                },
                {
                    "sent": "And we can for example, use the fully connected CRF to segment point clouds, where we use the XYZ position, normal and color of every point as a feature.",
                    "label": 0
                },
                {
                    "sent": "Or we can imply we can.",
                    "label": 0
                },
                {
                    "sent": "Music to segment meshes where the XYZ position normal if every vertex is used.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in summary.",
                    "label": 0
                },
                {
                    "sent": "I've shown you a fully connected model in which pairwise potentials a linear combination of Gaussian kernels and have presented you with an efficient inference algorithm and is fully connected models where the inference algorithm is linear in the number of variables and independent of the number of pairwise connections.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at some future work.",
                    "label": 0
                },
                {
                    "sent": "One area of future work we are currently investigating is going beyond the simple mean field approximation and finding an approximation algorithm that is more accurate and I personally believe the key to that is lifting some of the restrictions to filtering imply the filtering imposed on our algorithm, which is the filtering needs to update all variables at once and finding a filtering algorithm that is.",
                    "label": 0
                },
                {
                    "sent": "That allows us to update single variables at once is what I think is the key to finding a better inference algorithm in those fully connected models.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the second area we are extending the fully connected model to its continuous variables and we are currently investigating using.",
                    "label": 0
                },
                {
                    "sent": "Using it for depth reconstruction or optical flow estimation.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then another area.",
                    "label": 0
                },
                {
                    "sent": "It's very interesting is extending it to non Euclidean spaces.",
                    "label": 0
                },
                {
                    "sent": "For example in measures or general graphs.",
                    "label": 0
                },
                {
                    "sent": "Euclidean space, mitre doesn't exist or is not very meaningful such that spaces such as to distichs or diffusion distance or way more meaningful in those spaces.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the last point is going beyond the simple label compatibility function.",
                    "label": 1
                },
                {
                    "sent": "Right now the label compatibility function is strictly separated from any features in the image, so finding a label compatibility function that can use some of the underlying features in the image would greatly increase the overall accuracy of the order.",
                    "label": 0
                },
                {
                    "sent": "The overall power expressive power of the model.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All right, I'd like to mention that all the code for the fully connected CRF models it is available online under the following URL here.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention and feel free to stop by my poster tonight W 14 and I'm happy to take any questions.",
                    "label": 0
                },
                {
                    "sent": "So the question was, what was the computational platform for the timings?",
                    "label": 0
                },
                {
                    "sent": "So it was a single core machine CPU and think it was an I-7930.",
                    "label": 0
                }
            ]
        }
    }
}