{
    "id": "sjohtaxldaiajdps52b3cskthcou7g62",
    "title": "Regularization and Feature Selection in Least Squares Temporal-Difference Learning",
    "info": {
        "author": [
            "J. Zico Kolter, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Sept. 17, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_kolter_rfsl/",
    "segmentation": [
        [
            "Yes, thank you for the introduction.",
            "This is work on regularization and feature selection and least squares, temporal difference learning and this is work with my advisor, Andrew, or both.",
            "The Computer science Department at."
        ],
        [
            "So to give a brief outline, what we're interested in is applying R L2 continuous or large state spaces.",
            "Sorry, Satan accesses possibly and do this.",
            "We're going to do our L with linear function approximation.",
            "We can't represent Alfonso, please.",
            "We need linear function approximation, and in particular we're going to focus on the least squares, temporal difference or LST family of algorithms for doing this.",
            "They've proven very effective in practice and I'll talk about this briefly, but we're going to focus on these.",
            "Class of algorithms.",
            "However, the problem with these approaches are that when the number of features is very large, the algorithms can be very computationally expensive and can also overfit to the data.",
            "And So what this talk is about?",
            "Is we're going to present a method?"
        ],
        [
            "For regularization and feature selection and LST Deann particular, we're going to focus on L1 regularization.",
            "We're going to introduce a notion of an L1 regularize fixed point and develop an efficient algorithm for finding these regularised fixed point."
        ],
        [
            "But let's take a little step back about the bigger picture here.",
            "Why would we want to do feature selection?",
            "Well, in supervised learning, at least feature selection has proven very powerful at being able to take a huge number of features and just throw it at an algorithm and have the algorithm figure out what to do with this huge number of features so we don't have to ourselves as the designer specify what features to use and supervised learning.",
            "There are really two.",
            "There are some more, but sort of two general themes to how feature selection is done.",
            "The first is slightly older approach, which is what I'm calling.",
            "Grieving methods are things like forward selection or backward selection etc.",
            "And these add features one at a time in a greedy manner, and an approach that's gotten a lot more attention recently is methods based on convex optimization.",
            "In particular methods based on L1 norm optimization and minimizing L1 norms, which is sum of absolute values of a weight vector.",
            "This tends to lead this sparse solutions.",
            "There's a whole session later today on just on L1 methods that have clearly taken off in supervised learning."
        ],
        [
            "Archer so we want to sort of think about the same for reinforcement learning, right and."
        ],
        [
            "In terms of greedy methods, this isn't a perfect analogy, but there's been a lot of work in this and the one I'm giving an example of some of Ron Pars work two years ago.",
            "I smell and the idea here.",
            "It's like I said, it's not perfect analogy, but the idea here is you're adding features one at a time to sort of greedily improve your estimate of the value function and doing feature."
        ],
        [
            "Selection in that manner.",
            "And this work then, is about extending these convex L1 methods to the setting.",
            "In particular of these TD algorithms for reinforcement learning."
        ],
        [
            "OK, so first of all I want to talk a little bit.",
            "Just give some background on least squares temporal difference learning algorithm."
        ],
        [
            "So our problem set up.",
            "We have a Markov chain.",
            "We're actually not going to worry about actions for now.",
            "Just think of a Markov chain.",
            "Have a set of states.",
            "We have a reward.",
            "We have transition probabilities and we have a discount factor.",
            "And the goal is just to compute the value function for this Markov chain forgiven state we want to its value or its expected sum of rewards, discounted rewards."
        ],
        [
            "And this is a hard problem for two reasons.",
            "This is sort of the standard RLRL setting.",
            "Here is a hard problem for two reasons.",
            "First of all, we don't know the true state transitions or the reward.",
            "We only have access to samples of these.",
            "And Secondly.",
            "The state space is actually too large to represent the value function anyway, we can't enumerate all the states.",
            "We can't just list a value for every state, so we have to rely on some form of approximation.",
            "And the temporal."
        ],
        [
            "Difference family of algorithms is does address this setting so these have a long history.",
            "Obviously in reinforcement learning this session is about.",
            "And in particular, we're going to focus on as I referred to before the least squares temporal difference family of algorithms.",
            "I'll describe what these mean in a second, but these are all STD algorithms, and by this I mean the whole sort of family of algorithms actually that have arisen from this.",
            "The original paper was the Brackey embarked on paper 96, then boy in 99 extended this to a setting of eligibility, eligibility traces and then log back is in par.",
            "Extending this to a policy iteration setting, so this is the whole family algorithms that we're going to look at in this in this work.",
            "And these algorithms work very well in practice, and one of the reasons is that they make very efficient use of the data.",
            "They take all the data treated as a whole and learn a very good estimate of the value function from all this data.",
            "So how do these work?"
        ],
        [
            "Um again, sort of a quick overview here.",
            "This is going to take awhile.",
            "Not this one, but the next one is going to be somewhat technical, so hopefully this is this.",
            "People have seen some of this stuff before.",
            "This position is about, but anyway, what we're going to do is we're going to say that we can't represent the function explicitly, so we're going to use linear function approximation.",
            "We're going to the value of a state.",
            "S is equal to some."
        ],
        [
            "Amateurs.",
            "Say K parameters times a feat."
        ],
        [
            "Vector.",
            "State features scrapping that state.",
            "Very common, and I'm not talking about quickly what these are going to be yet.",
            "I'll get to that briefly, but we need that approximation."
        ],
        [
            "Now someone who will Harry equation here.",
            "But TD algorithms in general what they're trying to do.",
            "In one interpretation is fine parameters W. Remember these parameters define what the value function is there trying to find parameters W that satisfy a fixed point equation, and this is going to be the hairiest slide in the talk.",
            "So and don't worry too much, it doesn't quite make sense, but should I go through it sort of sort of briefly so here."
        ],
        [
            "You so expensive W. We have some parameters W an we're going to end essentially represents our value function.",
            "What we're going to do is we're going to buy what's called a Bellman Bellman operator to it.",
            "Going to do some minimization here, we want to get out is the same as our original weight vector.",
            "Again, don't worry too much.",
            "I'll try to explain it a little bit more, but you here is the optimization variable."
        ],
        [
            "Fee here is a matrix of all the state features."
        ],
        [
            "R is a vector of all the rewards."
        ],
        [
            "I empty the matrix of transition probabilities."
        ],
        [
            "And actually, sometimes this is easier understood in the following sense.",
            "It's saying that under these definitions and no one is too much.",
            "But on the definitions this term here VW is a list of all our values and what TD methods try to do is try to find a set of parameters W such that if you take this value function, apply the bellman operator, and then project back into the space, the span of these of these bases, you'll get the original value function back.",
            "OK, so again like I said, this is not the details.",
            "Here are not that important to the remainder of the talk.",
            "This is just what TD methods do under some interpretation and I'm using this first formalism because I want to emphasize the optimization problem inherent in this fixed point.",
            "But you can think of it just as we have some.",
            "Waits W we apply some funky operation to them and we get back.",
            "We want to get back our original weights W that's the point that we tried to find.",
            "Anna."
        ],
        [
            "In fact, the LSD algorithm.",
            "Find W bet approximately satisfies this equation using only samples from the MVP.",
            "That's sort of what the LCD algorithm does.",
            "It does a very good job of doing that.",
            "OK, so we had this Al Gore."
        ],
        [
            "Why?",
            "What's the problem?",
            "Why would why would we want to think about doing feature selection and the reason here is that LSD and there have been some extensions that make a little more efficient, but essentially LSD.",
            "Listen, it's in its most basic form, requires storing an inverting K by K matrix case a number of features here, right?",
            "And this can be extremely slow for large K. Their finest, and even if you don't convert the whole thing with some incremental methods of stuff to store the whole thing, this can be extremely slow, extremely memory intensive.",
            "And what this means in practice is that oftentimes practitioners have to put a lot of effort into sort of picking the right features to use for function approximation in RL.",
            "And the other problem really though, is that for men, if you have a lot of features, so right, so to get this sort of pick this good features, because if you have a lot of features and only a few samples L STD algorithms can actually overfit to the training set, right?",
            "They can there's just too many features, not enough samples, they can overfit.",
            "So this motivates our look into feature selection and these L1 approximations to L L1, regularization's to LSD."
        ],
        [
            "OK, so now we'll talk about regularization and feature selection in LSD.",
            "To the main."
        ],
        [
            "Contribution or paper and the basic idea is very simple.",
            "Right, we're going to take that fixed point equation that I mentioned before.",
            "The basis for this these TD approaches.",
            "I was going to regularization term.",
            "Right simple enough."
        ],
        [
            "And in fact, in this paper, like I said, we're going to focus on L1 regularization.",
            "Hopefully most of 'em, but most people have heard of this before, but L1 regularization.",
            "Like I mentioned, it's the sum of the absolute values of the weight vector, and when you minimize this, it encourages a lot of the weights to be actually identically zero.",
            "What this means is they don't play any role in the actual solution, so you're essentially selecting the best features to use for the problem.",
            "Um, other benefits of the approach is going over.",
            "Avoid overfitting.",
            "To some degree.",
            "We can now start to use much fewer samples with more features.",
            "And computationally, this approach avoids storing an inverting the full K by K matrix.",
            "We can work with a much smaller subset of features and without having to invert or even store this full matrix of all the features."
        ],
        [
            "OK, so that's sort of the basic motivation about why we want to do this now.",
            "What's the problem here?",
            "Well, the bad news is that, unfortunately, for L1 regularize LSD, there's no long.",
            "First of all, there's no longer a closed form solution for that W. The nice thing about LSD is very simple.",
            "You have a closed form solution for what the optimal W is.",
            "Only actually wrote in the slides, but it's fairly straightforward.",
            "And people that know conversation so that that's OK 'cause we can just do conversation.",
            "You know these L1 norm things are never closed form.",
            "We can just optimize it with convex optimization.",
            "That should be fine, but in fact the optimal W can't even be expressed as a convex optimization problem, so.",
            "Seems like bad news right?",
            "But the good news of the fortunate."
        ],
        [
            "We we can efficiently find these L1 regularize fixed points and we do this using an algorithm similar to what's called the least angle regression algorithm or the Lars algorithm.",
            "This algorithm statistics and this is an algorithm for L1 regularize least squares.",
            "But in this work we take it and we adapt it to the setting of finding these L1 regularize fixed points for LSD.",
            "OK, so.",
            "I don't want to describe the whole."
        ],
        [
            "Algorithm here, even describing Lars itself as a whole lecture and the update rules are actually somewhat complex.",
            "The first time you see him, so I'm just going to give some basic intuition about how this algorithm works.",
            "The basic idea is that we take our L1 regularize fixed point equations, and we express this in terms of the optimality conditions for a convex problem.",
            "This is a bunch of conditions that we need to satisfy for a point to be an L1 regularize fixed point.",
            "And then beginning with a solution of all zeros.",
            "This is a fully regularised regularised away all the weights beginning from this fully regularize solution, we proceed incrementally down the regularization path of this L1 regularize fixed point.",
            "And it turns out this is sort of very nice property about L1 norms in general is that we can make adjustments in WR weights that are piecewise linear, and we can find a closed form for these adjustments.",
            "So despite the sorry not closed form, but we can find an analytical form for these adjustments.",
            "So despite the fact that we can't find a closed form for the W, the solution W satisfies equation.",
            "We can compute it in some sense antithetically by this by an iterative process.",
            "And then we stopped when we reached the desired amount of regularization.",
            "So again, just hoping to give a very broad overview here, the details are in the paper, obviously, but just the basic idea is that we're adapting an algorithm like such as well without the large algorithm to find these fixed points."
        ],
        [
            "And what can we say about what's this album is going to do?",
            "What we can show is that under certain conditions these are similar to the conditions required.",
            "They're not identical, but they are similar to the conditions required for the convergence of ordinary TD algorithms.",
            "But in these circumstances, the L1 regularizer fixpoint is guaranteed to exist.",
            "It's guaranteed to be unique.",
            "And the large TD will efficiently find this fixed point."
        ],
        [
            "A little bit about the computational capacity of the algorithm.",
            "Large CD finds it so the computational complexity sort of remember the nice thing is we don't have to store the invert the whole K by K matrix, and in fact the combination complexity is still cubic in the number of non 0 features, but only linear in the number of total features.",
            "That's very nice 'cause usually the number of total features we wanted to just throw a whole bunch of features at an algorithm.",
            "Let it pick the right ones.",
            "And so we can use the disk complexity is usually much, much lower than the complexity of LSD, which all."
        ],
        [
            "Demonstrating that.",
            "OK, so I just want to briefly some experimental results.",
            "These are not large scale simulations here.",
            "These are just sort of small scale toy examples to demonstrate that these effects do occur to occur in practice, or at least in small scale examples.",
            "OK, so the first one we're going to."
        ],
        [
            "Uses a 20 state chain domain.",
            "Is this from lageson parse LSP?",
            "I paper and there's 20 states as such in a chain actions at the at the extremal States and I'm sorry rewards at the extreme states two actions and go left or right and we're going to use large TD wrapped around with an LSP style policy iteration defines policies for this domain.",
            "They're going to be 5 relevant features which are carbs on the state space and in the paper with an Enron and Michaels paper that showed this was actually good enough to actually.",
            "Do well with these with these features with these five RBF so and then in addition these RBS we're going to generate a very number of relevant Gaussian noise features to add onto this that are just sort of ultimately going to be irrelevant.",
            "Features, but another algorithm like LCD might actually select these to use.",
            "So here's sort of the."
        ],
        [
            "A graph of the performance this is.",
            "This is the reward we achieve, and this is the number of samples for 100,000 relevant features.",
            "Alright, so um.",
            "Both albums eventually can do pretty well, given enough enough samples, but using much fewer samples.",
            "The LG TV algorithm can can outperform LSD.",
            "These LCD approaches that try to use all the features that there are."
        ],
        [
            "The same thing, and this is sort of what we would expect.",
            "The same thing holds if we sort of plot that discount reward versus a number of relevant relevant features, so very quickly.",
            "LST methods will performing much worse, but for a while the L1 regularization will perform much better.",
            "And finally, probably in some in some sense you know most importantly right is that."
        ],
        [
            "This graph shows the computational time of these different methods, so the computation of LCD grows very quickly with a number of relevant features, whereas as we expected by our computational complexity, the running time of large DD for L1 regularization doesn't increase that much because basically linearly in the number of features."
        ],
        [
            "OK, we also ran an amount car demand again.",
            "Very simple domain.",
            "We had 500 examples collected from 50 episodes and we used so just with 520 samples.",
            "We used about more than 1300 basis functions and these were automatically generated RBF with different bandwidths and in this setting is just sort of a point results I don't make too much of it but just shows that there can in fact be a case where essentially large."
        ],
        [
            "Large city will succeed at getting the car the car up the Hill, whereas LSD will just always fail.",
            "It's just there's too many features here for it to really learn anything useful from this from this domain."
        ],
        [
            "OK, so I think I'm running out of time, but just wanted to mention there is a lot of related work.",
            "There's been a lot of work collection also some work by people here on regularization in RL, but this is actually focuses mainly on L2 regularization for proving convergence of policy races like that.",
            "There's also some work on kernel selection which is related, but again slightly different, so I don't want to spend too much time on this.",
            "I'm happy to talk afterwards about related."
        ],
        [
            "So, to summarize, LSD, the algorithm, it's great 'cause it can learn value function approximations using only samples from the MVP, but for a lot of features it can be computationally expensive an overfits the data.",
            "So in this paper what we did is we presented a regularization framework for LSD using L1 regularization is it encourages sparse solutions, it prevents overfitting and it's computationally efficient.",
            "So all the advantages of this approach."
        ],
        [
            "So thank you, I'm happy to take any questions."
        ],
        [
            "Yes.",
            "For larger scale problem 'cause this is this, this the targets larger scale.",
            "For example, for a problem like computer glitch, number of the features, that is like a win 1,000,000.",
            "So expect like 100 features are active there times right?",
            "So you still.",
            "Have the same computation complexity of order N squared because the million times a million because P is like 100 RPM power, three is a million, so you got a million times a million conditional please.",
            "OK, so so yes.",
            "So in some sense I mean yes, looking at the different factors in the terms, you can always sort of say at certain times are going to be more or less so.",
            "The LSD by itself, just that the standard one would actually be K cubed.",
            "So it would be so you have to install it in invert the matrix so you can do it incrementally, but those are actually going to complicate the constant factor.",
            "Then it's going to be very high and.",
            "STD's case yes if you sort of do it do it sort of the correct way in terms of the matrix updates, but it's still going to high constant term on that on that on sort of how you do it by those by the way you up to the updates and I guess equally important is that you still have to store the entire K by K matrix, which even that a million billion matrix that's hard to store.",
            "You can't run out of memory, so the memory computations are going to be there.",
            "You can't even do that.",
            "Where is this?",
            "Even if the running times large you still it will still be tractable in terms of space.",
            "Memory, I think that's actually will be the bottleneck are heading up for like a million features.",
            "I think it would be the memory requirements yearly him against.",
            "What is the fixed point of?",
            "I didn't get quite is it?",
            "It doesn't have the same feedback at the end as the elastic, so if you if you have no regularization, right?",
            "If then then of course will be the same fixed point.",
            "But no, this fixed point is essentially saying we're going to add a little bit of relation to the fixed point, and we're going to have you know that once we applies operation an regularize we want to achieve our same are same thing.",
            "So this is.",
            "I can try to discuss more, maybe maybe afterwards, but basically what we're saying is that.",
            "And this is sort of a detail that again is not too crucial to the whole paper, but essentially what we're doing is we're taking this idea of fixed points that either so foundational to TD and saying what would happen if we put a regulation term in that actual fixed point.",
            "That's good question.",
            "Three so that theory.",
            "Can you show that these models actually leads to whatever causes 10 target OKC, right?",
            "So I haven't done sort of consistency.",
            "I haven't sort of the theory we've done so far is just showing that the algorithm that works we haven't thought about consistency or proving these things.",
            "These results do exist out there for supervised learning certainly, so I suspect they're doable, but we haven't.",
            "We haven't done that yet.",
            "Regularization coefficients.",
            "Yeah, so so in this we essentially check a bunch of them up until the point where there's too many features to use.",
            "I mean because largest incremental you can stop at any point, so you can essentially keep adding more and more features until maybe you can't store them anymore or you get a good enough estimate, etc.",
            "But I really think that both for supervised learning and for this method the meta meta hyperparameters is certain type of parameters is certainly a topic of active research still.",
            "That's all the time we have.",
            "Let's thank our speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, thank you for the introduction.",
                    "label": 0
                },
                {
                    "sent": "This is work on regularization and feature selection and least squares, temporal difference learning and this is work with my advisor, Andrew, or both.",
                    "label": 1
                },
                {
                    "sent": "The Computer science Department at.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to give a brief outline, what we're interested in is applying R L2 continuous or large state spaces.",
                    "label": 0
                },
                {
                    "sent": "Sorry, Satan accesses possibly and do this.",
                    "label": 0
                },
                {
                    "sent": "We're going to do our L with linear function approximation.",
                    "label": 1
                },
                {
                    "sent": "We can't represent Alfonso, please.",
                    "label": 0
                },
                {
                    "sent": "We need linear function approximation, and in particular we're going to focus on the least squares, temporal difference or LST family of algorithms for doing this.",
                    "label": 1
                },
                {
                    "sent": "They've proven very effective in practice and I'll talk about this briefly, but we're going to focus on these.",
                    "label": 0
                },
                {
                    "sent": "Class of algorithms.",
                    "label": 0
                },
                {
                    "sent": "However, the problem with these approaches are that when the number of features is very large, the algorithms can be very computationally expensive and can also overfit to the data.",
                    "label": 1
                },
                {
                    "sent": "And So what this talk is about?",
                    "label": 0
                },
                {
                    "sent": "Is we're going to present a method?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For regularization and feature selection and LST Deann particular, we're going to focus on L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "We're going to introduce a notion of an L1 regularize fixed point and develop an efficient algorithm for finding these regularised fixed point.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But let's take a little step back about the bigger picture here.",
                    "label": 0
                },
                {
                    "sent": "Why would we want to do feature selection?",
                    "label": 0
                },
                {
                    "sent": "Well, in supervised learning, at least feature selection has proven very powerful at being able to take a huge number of features and just throw it at an algorithm and have the algorithm figure out what to do with this huge number of features so we don't have to ourselves as the designer specify what features to use and supervised learning.",
                    "label": 0
                },
                {
                    "sent": "There are really two.",
                    "label": 0
                },
                {
                    "sent": "There are some more, but sort of two general themes to how feature selection is done.",
                    "label": 0
                },
                {
                    "sent": "The first is slightly older approach, which is what I'm calling.",
                    "label": 0
                },
                {
                    "sent": "Grieving methods are things like forward selection or backward selection etc.",
                    "label": 0
                },
                {
                    "sent": "And these add features one at a time in a greedy manner, and an approach that's gotten a lot more attention recently is methods based on convex optimization.",
                    "label": 0
                },
                {
                    "sent": "In particular methods based on L1 norm optimization and minimizing L1 norms, which is sum of absolute values of a weight vector.",
                    "label": 0
                },
                {
                    "sent": "This tends to lead this sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "There's a whole session later today on just on L1 methods that have clearly taken off in supervised learning.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Archer so we want to sort of think about the same for reinforcement learning, right and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of greedy methods, this isn't a perfect analogy, but there's been a lot of work in this and the one I'm giving an example of some of Ron Pars work two years ago.",
                    "label": 0
                },
                {
                    "sent": "I smell and the idea here.",
                    "label": 0
                },
                {
                    "sent": "It's like I said, it's not perfect analogy, but the idea here is you're adding features one at a time to sort of greedily improve your estimate of the value function and doing feature.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Selection in that manner.",
                    "label": 0
                },
                {
                    "sent": "And this work then, is about extending these convex L1 methods to the setting.",
                    "label": 1
                },
                {
                    "sent": "In particular of these TD algorithms for reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first of all I want to talk a little bit.",
                    "label": 0
                },
                {
                    "sent": "Just give some background on least squares temporal difference learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our problem set up.",
                    "label": 0
                },
                {
                    "sent": "We have a Markov chain.",
                    "label": 1
                },
                {
                    "sent": "We're actually not going to worry about actions for now.",
                    "label": 0
                },
                {
                    "sent": "Just think of a Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Have a set of states.",
                    "label": 1
                },
                {
                    "sent": "We have a reward.",
                    "label": 1
                },
                {
                    "sent": "We have transition probabilities and we have a discount factor.",
                    "label": 0
                },
                {
                    "sent": "And the goal is just to compute the value function for this Markov chain forgiven state we want to its value or its expected sum of rewards, discounted rewards.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is a hard problem for two reasons.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the standard RLRL setting.",
                    "label": 0
                },
                {
                    "sent": "Here is a hard problem for two reasons.",
                    "label": 0
                },
                {
                    "sent": "First of all, we don't know the true state transitions or the reward.",
                    "label": 1
                },
                {
                    "sent": "We only have access to samples of these.",
                    "label": 0
                },
                {
                    "sent": "And Secondly.",
                    "label": 0
                },
                {
                    "sent": "The state space is actually too large to represent the value function anyway, we can't enumerate all the states.",
                    "label": 1
                },
                {
                    "sent": "We can't just list a value for every state, so we have to rely on some form of approximation.",
                    "label": 0
                },
                {
                    "sent": "And the temporal.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Difference family of algorithms is does address this setting so these have a long history.",
                    "label": 0
                },
                {
                    "sent": "Obviously in reinforcement learning this session is about.",
                    "label": 0
                },
                {
                    "sent": "And in particular, we're going to focus on as I referred to before the least squares temporal difference family of algorithms.",
                    "label": 1
                },
                {
                    "sent": "I'll describe what these mean in a second, but these are all STD algorithms, and by this I mean the whole sort of family of algorithms actually that have arisen from this.",
                    "label": 0
                },
                {
                    "sent": "The original paper was the Brackey embarked on paper 96, then boy in 99 extended this to a setting of eligibility, eligibility traces and then log back is in par.",
                    "label": 0
                },
                {
                    "sent": "Extending this to a policy iteration setting, so this is the whole family algorithms that we're going to look at in this in this work.",
                    "label": 1
                },
                {
                    "sent": "And these algorithms work very well in practice, and one of the reasons is that they make very efficient use of the data.",
                    "label": 0
                },
                {
                    "sent": "They take all the data treated as a whole and learn a very good estimate of the value function from all this data.",
                    "label": 0
                },
                {
                    "sent": "So how do these work?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um again, sort of a quick overview here.",
                    "label": 0
                },
                {
                    "sent": "This is going to take awhile.",
                    "label": 0
                },
                {
                    "sent": "Not this one, but the next one is going to be somewhat technical, so hopefully this is this.",
                    "label": 0
                },
                {
                    "sent": "People have seen some of this stuff before.",
                    "label": 0
                },
                {
                    "sent": "This position is about, but anyway, what we're going to do is we're going to say that we can't represent the function explicitly, so we're going to use linear function approximation.",
                    "label": 0
                },
                {
                    "sent": "We're going to the value of a state.",
                    "label": 0
                },
                {
                    "sent": "S is equal to some.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amateurs.",
                    "label": 0
                },
                {
                    "sent": "Say K parameters times a feat.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vector.",
                    "label": 0
                },
                {
                    "sent": "State features scrapping that state.",
                    "label": 1
                },
                {
                    "sent": "Very common, and I'm not talking about quickly what these are going to be yet.",
                    "label": 0
                },
                {
                    "sent": "I'll get to that briefly, but we need that approximation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now someone who will Harry equation here.",
                    "label": 0
                },
                {
                    "sent": "But TD algorithms in general what they're trying to do.",
                    "label": 0
                },
                {
                    "sent": "In one interpretation is fine parameters W. Remember these parameters define what the value function is there trying to find parameters W that satisfy a fixed point equation, and this is going to be the hairiest slide in the talk.",
                    "label": 1
                },
                {
                    "sent": "So and don't worry too much, it doesn't quite make sense, but should I go through it sort of sort of briefly so here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You so expensive W. We have some parameters W an we're going to end essentially represents our value function.",
                    "label": 1
                },
                {
                    "sent": "What we're going to do is we're going to buy what's called a Bellman Bellman operator to it.",
                    "label": 0
                },
                {
                    "sent": "Going to do some minimization here, we want to get out is the same as our original weight vector.",
                    "label": 0
                },
                {
                    "sent": "Again, don't worry too much.",
                    "label": 0
                },
                {
                    "sent": "I'll try to explain it a little bit more, but you here is the optimization variable.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fee here is a matrix of all the state features.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "R is a vector of all the rewards.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I empty the matrix of transition probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And actually, sometimes this is easier understood in the following sense.",
                    "label": 1
                },
                {
                    "sent": "It's saying that under these definitions and no one is too much.",
                    "label": 0
                },
                {
                    "sent": "But on the definitions this term here VW is a list of all our values and what TD methods try to do is try to find a set of parameters W such that if you take this value function, apply the bellman operator, and then project back into the space, the span of these of these bases, you'll get the original value function back.",
                    "label": 0
                },
                {
                    "sent": "OK, so again like I said, this is not the details.",
                    "label": 0
                },
                {
                    "sent": "Here are not that important to the remainder of the talk.",
                    "label": 1
                },
                {
                    "sent": "This is just what TD methods do under some interpretation and I'm using this first formalism because I want to emphasize the optimization problem inherent in this fixed point.",
                    "label": 0
                },
                {
                    "sent": "But you can think of it just as we have some.",
                    "label": 0
                },
                {
                    "sent": "Waits W we apply some funky operation to them and we get back.",
                    "label": 0
                },
                {
                    "sent": "We want to get back our original weights W that's the point that we tried to find.",
                    "label": 0
                },
                {
                    "sent": "Anna.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, the LSD algorithm.",
                    "label": 0
                },
                {
                    "sent": "Find W bet approximately satisfies this equation using only samples from the MVP.",
                    "label": 0
                },
                {
                    "sent": "That's sort of what the LCD algorithm does.",
                    "label": 0
                },
                {
                    "sent": "It does a very good job of doing that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we had this Al Gore.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "What's the problem?",
                    "label": 0
                },
                {
                    "sent": "Why would why would we want to think about doing feature selection and the reason here is that LSD and there have been some extensions that make a little more efficient, but essentially LSD.",
                    "label": 0
                },
                {
                    "sent": "Listen, it's in its most basic form, requires storing an inverting K by K matrix case a number of features here, right?",
                    "label": 0
                },
                {
                    "sent": "And this can be extremely slow for large K. Their finest, and even if you don't convert the whole thing with some incremental methods of stuff to store the whole thing, this can be extremely slow, extremely memory intensive.",
                    "label": 0
                },
                {
                    "sent": "And what this means in practice is that oftentimes practitioners have to put a lot of effort into sort of picking the right features to use for function approximation in RL.",
                    "label": 0
                },
                {
                    "sent": "And the other problem really though, is that for men, if you have a lot of features, so right, so to get this sort of pick this good features, because if you have a lot of features and only a few samples L STD algorithms can actually overfit to the training set, right?",
                    "label": 0
                },
                {
                    "sent": "They can there's just too many features, not enough samples, they can overfit.",
                    "label": 0
                },
                {
                    "sent": "So this motivates our look into feature selection and these L1 approximations to L L1, regularization's to LSD.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we'll talk about regularization and feature selection in LSD.",
                    "label": 0
                },
                {
                    "sent": "To the main.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Contribution or paper and the basic idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "Right, we're going to take that fixed point equation that I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "The basis for this these TD approaches.",
                    "label": 0
                },
                {
                    "sent": "I was going to regularization term.",
                    "label": 0
                },
                {
                    "sent": "Right simple enough.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, in this paper, like I said, we're going to focus on L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "Hopefully most of 'em, but most people have heard of this before, but L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "Like I mentioned, it's the sum of the absolute values of the weight vector, and when you minimize this, it encourages a lot of the weights to be actually identically zero.",
                    "label": 0
                },
                {
                    "sent": "What this means is they don't play any role in the actual solution, so you're essentially selecting the best features to use for the problem.",
                    "label": 0
                },
                {
                    "sent": "Um, other benefits of the approach is going over.",
                    "label": 0
                },
                {
                    "sent": "Avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "To some degree.",
                    "label": 0
                },
                {
                    "sent": "We can now start to use much fewer samples with more features.",
                    "label": 0
                },
                {
                    "sent": "And computationally, this approach avoids storing an inverting the full K by K matrix.",
                    "label": 0
                },
                {
                    "sent": "We can work with a much smaller subset of features and without having to invert or even store this full matrix of all the features.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's sort of the basic motivation about why we want to do this now.",
                    "label": 0
                },
                {
                    "sent": "What's the problem here?",
                    "label": 0
                },
                {
                    "sent": "Well, the bad news is that, unfortunately, for L1 regularize LSD, there's no long.",
                    "label": 0
                },
                {
                    "sent": "First of all, there's no longer a closed form solution for that W. The nice thing about LSD is very simple.",
                    "label": 0
                },
                {
                    "sent": "You have a closed form solution for what the optimal W is.",
                    "label": 0
                },
                {
                    "sent": "Only actually wrote in the slides, but it's fairly straightforward.",
                    "label": 0
                },
                {
                    "sent": "And people that know conversation so that that's OK 'cause we can just do conversation.",
                    "label": 0
                },
                {
                    "sent": "You know these L1 norm things are never closed form.",
                    "label": 0
                },
                {
                    "sent": "We can just optimize it with convex optimization.",
                    "label": 0
                },
                {
                    "sent": "That should be fine, but in fact the optimal W can't even be expressed as a convex optimization problem, so.",
                    "label": 0
                },
                {
                    "sent": "Seems like bad news right?",
                    "label": 0
                },
                {
                    "sent": "But the good news of the fortunate.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We we can efficiently find these L1 regularize fixed points and we do this using an algorithm similar to what's called the least angle regression algorithm or the Lars algorithm.",
                    "label": 0
                },
                {
                    "sent": "This algorithm statistics and this is an algorithm for L1 regularize least squares.",
                    "label": 0
                },
                {
                    "sent": "But in this work we take it and we adapt it to the setting of finding these L1 regularize fixed points for LSD.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I don't want to describe the whole.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm here, even describing Lars itself as a whole lecture and the update rules are actually somewhat complex.",
                    "label": 0
                },
                {
                    "sent": "The first time you see him, so I'm just going to give some basic intuition about how this algorithm works.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is that we take our L1 regularize fixed point equations, and we express this in terms of the optimality conditions for a convex problem.",
                    "label": 0
                },
                {
                    "sent": "This is a bunch of conditions that we need to satisfy for a point to be an L1 regularize fixed point.",
                    "label": 0
                },
                {
                    "sent": "And then beginning with a solution of all zeros.",
                    "label": 0
                },
                {
                    "sent": "This is a fully regularised regularised away all the weights beginning from this fully regularize solution, we proceed incrementally down the regularization path of this L1 regularize fixed point.",
                    "label": 0
                },
                {
                    "sent": "And it turns out this is sort of very nice property about L1 norms in general is that we can make adjustments in WR weights that are piecewise linear, and we can find a closed form for these adjustments.",
                    "label": 0
                },
                {
                    "sent": "So despite the sorry not closed form, but we can find an analytical form for these adjustments.",
                    "label": 0
                },
                {
                    "sent": "So despite the fact that we can't find a closed form for the W, the solution W satisfies equation.",
                    "label": 0
                },
                {
                    "sent": "We can compute it in some sense antithetically by this by an iterative process.",
                    "label": 0
                },
                {
                    "sent": "And then we stopped when we reached the desired amount of regularization.",
                    "label": 0
                },
                {
                    "sent": "So again, just hoping to give a very broad overview here, the details are in the paper, obviously, but just the basic idea is that we're adapting an algorithm like such as well without the large algorithm to find these fixed points.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what can we say about what's this album is going to do?",
                    "label": 0
                },
                {
                    "sent": "What we can show is that under certain conditions these are similar to the conditions required.",
                    "label": 0
                },
                {
                    "sent": "They're not identical, but they are similar to the conditions required for the convergence of ordinary TD algorithms.",
                    "label": 0
                },
                {
                    "sent": "But in these circumstances, the L1 regularizer fixpoint is guaranteed to exist.",
                    "label": 0
                },
                {
                    "sent": "It's guaranteed to be unique.",
                    "label": 0
                },
                {
                    "sent": "And the large TD will efficiently find this fixed point.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little bit about the computational capacity of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Large CD finds it so the computational complexity sort of remember the nice thing is we don't have to store the invert the whole K by K matrix, and in fact the combination complexity is still cubic in the number of non 0 features, but only linear in the number of total features.",
                    "label": 0
                },
                {
                    "sent": "That's very nice 'cause usually the number of total features we wanted to just throw a whole bunch of features at an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Let it pick the right ones.",
                    "label": 0
                },
                {
                    "sent": "And so we can use the disk complexity is usually much, much lower than the complexity of LSD, which all.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Demonstrating that.",
                    "label": 0
                },
                {
                    "sent": "OK, so I just want to briefly some experimental results.",
                    "label": 0
                },
                {
                    "sent": "These are not large scale simulations here.",
                    "label": 0
                },
                {
                    "sent": "These are just sort of small scale toy examples to demonstrate that these effects do occur to occur in practice, or at least in small scale examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first one we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uses a 20 state chain domain.",
                    "label": 0
                },
                {
                    "sent": "Is this from lageson parse LSP?",
                    "label": 0
                },
                {
                    "sent": "I paper and there's 20 states as such in a chain actions at the at the extremal States and I'm sorry rewards at the extreme states two actions and go left or right and we're going to use large TD wrapped around with an LSP style policy iteration defines policies for this domain.",
                    "label": 0
                },
                {
                    "sent": "They're going to be 5 relevant features which are carbs on the state space and in the paper with an Enron and Michaels paper that showed this was actually good enough to actually.",
                    "label": 0
                },
                {
                    "sent": "Do well with these with these features with these five RBF so and then in addition these RBS we're going to generate a very number of relevant Gaussian noise features to add onto this that are just sort of ultimately going to be irrelevant.",
                    "label": 0
                },
                {
                    "sent": "Features, but another algorithm like LCD might actually select these to use.",
                    "label": 0
                },
                {
                    "sent": "So here's sort of the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A graph of the performance this is.",
                    "label": 0
                },
                {
                    "sent": "This is the reward we achieve, and this is the number of samples for 100,000 relevant features.",
                    "label": 1
                },
                {
                    "sent": "Alright, so um.",
                    "label": 0
                },
                {
                    "sent": "Both albums eventually can do pretty well, given enough enough samples, but using much fewer samples.",
                    "label": 0
                },
                {
                    "sent": "The LG TV algorithm can can outperform LSD.",
                    "label": 0
                },
                {
                    "sent": "These LCD approaches that try to use all the features that there are.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same thing, and this is sort of what we would expect.",
                    "label": 0
                },
                {
                    "sent": "The same thing holds if we sort of plot that discount reward versus a number of relevant relevant features, so very quickly.",
                    "label": 0
                },
                {
                    "sent": "LST methods will performing much worse, but for a while the L1 regularization will perform much better.",
                    "label": 0
                },
                {
                    "sent": "And finally, probably in some in some sense you know most importantly right is that.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This graph shows the computational time of these different methods, so the computation of LCD grows very quickly with a number of relevant features, whereas as we expected by our computational complexity, the running time of large DD for L1 regularization doesn't increase that much because basically linearly in the number of features.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we also ran an amount car demand again.",
                    "label": 0
                },
                {
                    "sent": "Very simple domain.",
                    "label": 0
                },
                {
                    "sent": "We had 500 examples collected from 50 episodes and we used so just with 520 samples.",
                    "label": 0
                },
                {
                    "sent": "We used about more than 1300 basis functions and these were automatically generated RBF with different bandwidths and in this setting is just sort of a point results I don't make too much of it but just shows that there can in fact be a case where essentially large.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large city will succeed at getting the car the car up the Hill, whereas LSD will just always fail.",
                    "label": 0
                },
                {
                    "sent": "It's just there's too many features here for it to really learn anything useful from this from this domain.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I think I'm running out of time, but just wanted to mention there is a lot of related work.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work collection also some work by people here on regularization in RL, but this is actually focuses mainly on L2 regularization for proving convergence of policy races like that.",
                    "label": 0
                },
                {
                    "sent": "There's also some work on kernel selection which is related, but again slightly different, so I don't want to spend too much time on this.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to talk afterwards about related.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, to summarize, LSD, the algorithm, it's great 'cause it can learn value function approximations using only samples from the MVP, but for a lot of features it can be computationally expensive an overfits the data.",
                    "label": 0
                },
                {
                    "sent": "So in this paper what we did is we presented a regularization framework for LSD using L1 regularization is it encourages sparse solutions, it prevents overfitting and it's computationally efficient.",
                    "label": 0
                },
                {
                    "sent": "So all the advantages of this approach.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you, I'm happy to take any questions.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "For larger scale problem 'cause this is this, this the targets larger scale.",
                    "label": 0
                },
                {
                    "sent": "For example, for a problem like computer glitch, number of the features, that is like a win 1,000,000.",
                    "label": 0
                },
                {
                    "sent": "So expect like 100 features are active there times right?",
                    "label": 0
                },
                {
                    "sent": "So you still.",
                    "label": 0
                },
                {
                    "sent": "Have the same computation complexity of order N squared because the million times a million because P is like 100 RPM power, three is a million, so you got a million times a million conditional please.",
                    "label": 0
                },
                {
                    "sent": "OK, so so yes.",
                    "label": 0
                },
                {
                    "sent": "So in some sense I mean yes, looking at the different factors in the terms, you can always sort of say at certain times are going to be more or less so.",
                    "label": 0
                },
                {
                    "sent": "The LSD by itself, just that the standard one would actually be K cubed.",
                    "label": 0
                },
                {
                    "sent": "So it would be so you have to install it in invert the matrix so you can do it incrementally, but those are actually going to complicate the constant factor.",
                    "label": 0
                },
                {
                    "sent": "Then it's going to be very high and.",
                    "label": 0
                },
                {
                    "sent": "STD's case yes if you sort of do it do it sort of the correct way in terms of the matrix updates, but it's still going to high constant term on that on that on sort of how you do it by those by the way you up to the updates and I guess equally important is that you still have to store the entire K by K matrix, which even that a million billion matrix that's hard to store.",
                    "label": 0
                },
                {
                    "sent": "You can't run out of memory, so the memory computations are going to be there.",
                    "label": 0
                },
                {
                    "sent": "You can't even do that.",
                    "label": 0
                },
                {
                    "sent": "Where is this?",
                    "label": 0
                },
                {
                    "sent": "Even if the running times large you still it will still be tractable in terms of space.",
                    "label": 0
                },
                {
                    "sent": "Memory, I think that's actually will be the bottleneck are heading up for like a million features.",
                    "label": 0
                },
                {
                    "sent": "I think it would be the memory requirements yearly him against.",
                    "label": 0
                },
                {
                    "sent": "What is the fixed point of?",
                    "label": 0
                },
                {
                    "sent": "I didn't get quite is it?",
                    "label": 0
                },
                {
                    "sent": "It doesn't have the same feedback at the end as the elastic, so if you if you have no regularization, right?",
                    "label": 0
                },
                {
                    "sent": "If then then of course will be the same fixed point.",
                    "label": 0
                },
                {
                    "sent": "But no, this fixed point is essentially saying we're going to add a little bit of relation to the fixed point, and we're going to have you know that once we applies operation an regularize we want to achieve our same are same thing.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "I can try to discuss more, maybe maybe afterwards, but basically what we're saying is that.",
                    "label": 0
                },
                {
                    "sent": "And this is sort of a detail that again is not too crucial to the whole paper, but essentially what we're doing is we're taking this idea of fixed points that either so foundational to TD and saying what would happen if we put a regulation term in that actual fixed point.",
                    "label": 0
                },
                {
                    "sent": "That's good question.",
                    "label": 0
                },
                {
                    "sent": "Three so that theory.",
                    "label": 0
                },
                {
                    "sent": "Can you show that these models actually leads to whatever causes 10 target OKC, right?",
                    "label": 0
                },
                {
                    "sent": "So I haven't done sort of consistency.",
                    "label": 0
                },
                {
                    "sent": "I haven't sort of the theory we've done so far is just showing that the algorithm that works we haven't thought about consistency or proving these things.",
                    "label": 0
                },
                {
                    "sent": "These results do exist out there for supervised learning certainly, so I suspect they're doable, but we haven't.",
                    "label": 0
                },
                {
                    "sent": "We haven't done that yet.",
                    "label": 0
                },
                {
                    "sent": "Regularization coefficients.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so in this we essentially check a bunch of them up until the point where there's too many features to use.",
                    "label": 0
                },
                {
                    "sent": "I mean because largest incremental you can stop at any point, so you can essentially keep adding more and more features until maybe you can't store them anymore or you get a good enough estimate, etc.",
                    "label": 0
                },
                {
                    "sent": "But I really think that both for supervised learning and for this method the meta meta hyperparameters is certain type of parameters is certainly a topic of active research still.",
                    "label": 0
                },
                {
                    "sent": "That's all the time we have.",
                    "label": 0
                },
                {
                    "sent": "Let's thank our speaker.",
                    "label": 0
                }
            ]
        }
    }
}