{
    "id": "tn6uuba3f6muavxg6yjopb4rif77b4pi",
    "title": "Kernel Methods",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Jan. 25, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlss2012_scholkopf_kernel/",
    "segmentation": [
        [
            "I was going to maybe talk 510 minutes just in general because this is the first of the summer school about machine learning because it is a school and machine learning we should sort of assume that people don't yet know machine learning.",
            "But of course I think almost everybody does.",
            "But usually we have a few people at the summer school who really starting from zero, just with a background in computer science or whatever.",
            "Talking about background.",
            "It would be interesting to know how many.",
            "How many of you are computer scientists?",
            "That's about half maybe how many are engineers?",
            "Uh, huh mathematicians?",
            "Physicists psychologists, neuro scientists.",
            "And linguists.",
            "Have I forgotten something?",
            "That's it, huh?",
            "Yeah, OK, so that's two people.",
            "Mix in a machine learning class.",
            "And."
        ],
        [
            "And so I briefly want to tell you something about machine learning.",
            "From my point of view or empirical inference, which is the process of drawing conclusions from empirical observations.",
            "So, for instance, we might have scientific inference, or we might get observations of this shape here.",
            "I didn't bring a laser pointer, we don't.",
            "We don't have one here.",
            "I have one in my hotel room.",
            "I can bring it next time.",
            "Well, maybe here's one.",
            "But I could use that mouse pointer Meanwhile, so we might have observations of these two OBS observers.",
            "Joint observations of X&Y.",
            "Maybe that looked like this, in which case we might be willing to speculate that there's a thank you very much.",
            "There's a linear relationship between the between these two observations, so between these two quantities, observables X&Y.",
            "Which may be looks like this, but on the other hand, who tells us it's not in reality and much more complicated relationship that explains a set of data points, something for instance like this, which also might be describable in mathematical terms.",
            "And my perfectly explain the data and this is of course an old question which was started by a number of people, including labor, and it's down here.",
            "Who had this thought experiment?",
            "The Julia I have this from the Gregory Chitine.",
            "Thought experiment is you take a an ink quill pen and you dip it into your ink and then splash it over the paper.",
            "You get some random points like we said, well, you could always find some mathematical explanation for these random points.",
            "So so in which case or leggings was asking a question, in which case would we call such an expression a law of nature?",
            "So certainly if it's just something random and if we can always find something section explanation, we wouldn't call it a lot of nature.",
            "So to call it a lot later, it has to satisfy additional criteria and maybe machine learning is a lot of machine learning is about what kind of criteria it should satisfy and what we can guarantee if it satisfies these criteria."
        ],
        [
            "And from there, so here's a quote for physicist for physicists.",
            "For this.",
            "With this rather fault, this was just sort of a stupid question.",
            "He said that basically it would be evident.",
            "So if you're experimenting statistics, if you have to do any nontrivial analysis of the data, you have done a better experiment.",
            "So this is the enemy of all machine learning, but in those days machine learning didn't exist and big computers didn't exist, of course.",
            "So this was maybe a reasonable point of view.",
            "It's certainly so this is a certain type of inductive bias, and maybe it's reasonable for him."
        ],
        [
            "Background, but maybe not for us.",
            "At least we're in the business of doing inference or doing non trivial statistics.",
            "And so let's look at the second example of empirical inference from the world of perception.",
            "So if you look at these handwritten digits who have seen these digits before.",
            "The French the percentage is going down.",
            "We were all raised with these digits in the old days as PhD students.",
            "We all had to work on this data set.",
            "This is the USPS data set of zip codes collected in the post Office of Buffalo or something like that.",
            "And you can easily recognize these zip codes and we have worked on this for a long time.",
            "We can recognize them even better, but still you see these digits.",
            "But if I permute the pixels.",
            "By fixed permutation.",
            "So this is always the same permutation.",
            "You have a much harder time seeing the digits.",
            "Maybe you can still see that there's something different about the zeros compared to the threes, but it's much harder.",
            "So the question is, is it is it now a hard problem or is it an easy problem and we aren't in this case?",
            "Or one answer would be that in reality it's a hard problem, but this one looks relatively easy to us, so that's a partial answer that you can.",
            "You can debate about that.",
            "This one looks easy to ask because we've been trained on this kind of data for a long, long time.",
            "During our life.",
            "This kind of data looks different.",
            "Looks looks harder for us.",
            "We're not used to this data, which maybe makes it more interesting, or at least makes it apparent that it's a hard problem.",
            "And of course a lot of the problems that we're dealing with.",
            "For instance, in bioinformatics, use of this type that we haven't seen that kind of data before.",
            "So this is a brain has learned to solve this kind of problems and we can see that this one looks easy to ask and there's a famous neuro scientists Horace Barlow.",
            "But once said that the brain is nothing but a statistical decision organ.",
            "So we have different organs in our body, some of them pump blood, so them clean the blood and there's one that's built for statistical decisions.",
            "Let's the brain.",
            "So if you are studying empirical inference and learning in a sense we are also doing theory."
        ],
        [
            "Tickle brain research.",
            "The level of looking at neurons by the level of tasks that the brain is to solve.",
            "So now it showed you two inference problems.",
            "When looked easy is 2 dimensional problem where you should directly see what's going on or where.",
            "We probably see, in which case we can generalize which case we can't.",
            "The other one that looked hard.",
            "And.",
            "Let's look at another hard inference problem on.",
            "Let's look at some maybe what are general properties of hard inference problems.",
            "So this is a problem.",
            "Bioinformatics and it's a classifier.",
            "So this is just a component of a larger system.",
            "To find jeans, this is a classifier that classifieds DNA sequence locations.",
            "I forgot to ask there are there computational biologists in the audience also.",
            "OK so I missed a significant group before in my question so.",
            "Classifying human a human DNA sequence locations into three classes doesn't matter if you don't know about these topics, it's just a three class classification problem, which is highly unbalanced, so the classes don't have the same size and the input data is pieces of DNA sequence of a certain length, so it's a window around the place that you want to classify.",
            "This is 141.",
            "We have 15 million training points.",
            "A certain type of machine learning methods, and this is a certain type of performance criterion called the precision recall curve.",
            "It doesn't matter what the details are.",
            "The point that I want to make here only is.",
            "But here as you increase the number of training examples, performance gets better.",
            "So initially for small training sets you are more or less at chance level, so the data set looks random to us.",
            "But as we increase the number of training points quite a bit here we are at 10 million.",
            "We can do a very good job at solving this classification problem and maybe this looks like a trivial point, but I think it's important to keep this in mind when we think about machine learning.",
            "If we were a human looking at this data set, we would always be in a regime somewhere down here and we would look at small datasets.",
            "We would maybe think that this is completely random.",
            "There's no structure, but if we have advanced machine learning methods that can handle large datasets, find nonlinear relationships.",
            "We can be over here and we can certainly see a lot of structure, so a lot of stuff that maybe it might appear human random to us.",
            "Humans may not be so random and we might be able to find structure with machine learning, and I think that's only gradually being acknowledged that this can be a real paradigm shift in how we interpret data in the world.",
            "Nothing machine learning is going to play an important role in that so.",
            "Setting the right topic and so what's characteristic of such problems.",
            "They are typically high dimensional, so you have to look at many variables simultaneously.",
            "They are complex in various ways.",
            "We have little prior knowledge, by which I mean that, for instance, we don't have a full mechanistic model for the data.",
            "If we had that, or if the scientists had that, then probably they wouldn't bother with asking us for help or they wouldn't bother with applying machine learning.",
            "And of course, as a consequence of these points, typically we need large datasets.",
            "In these kind of course only be processed if we have computers, which was of course an enabling technology for machine learning.",
            "The increase in computing power over the last decades and if."
        ],
        [
            "We have advanced inference methods.",
            "So yeah, so I already made this point first one.",
            "We can solve problems that humans can't solve.",
            "And.",
            "Even though this is probably just because of the data sets, or even if it's just the data set size dimensionality, it's a major step."
        ],
        [
            "OK, so the main what is the main issue of machine learning and maybe the main issue is generalization.",
            "And just to give you an idea what generalization about is about or what is the problem, suppose we see this number sequence 1247.",
            "Ask the question what is the next number in this sequence?",
            "Any any guesses?",
            "1116 any others?",
            "12 OK so 11 is its first good solution.",
            "It's a simple sequence.",
            "It's called the lazy caterer sequence because it's the maximum number of pieces into which you can cut a piece of cake with N cuts.",
            "You just have to make sure that every additional cuts, intersects or previous cuts.",
            "So in that case the end cut gives you an additional pieces and they are not the same size, but there are pieces, so this is a simple sequence.",
            "12 was also good.",
            "Is this 113?",
            "Is the so called Triple Nachi sequence where each number is the sum of the previous 316?",
            "We're going to ask Neil for an explanation later on.",
            "You said 60 in India.",
            "Or someone else?",
            "And well, this one is also a nice one.",
            "This is a sequence that ends at 28, is just a set of devices of 28.",
            "This is the one that delivers, came up with his decimal expansions of \u03c0 and E interleaved, which is also a nice sequence for a mathematician.",
            "And of course, you can ask the question now which one?",
            "Which one is right and.",
            "Oh, so there's this website where you can enter these digits and you get a lot of hits of meaningful sequence with sequences that start like that."
        ],
        [
            "So you can ask the question which one is correct?",
            "Which one generalizes?",
            "And of course you will say this is sort of a stupid questions, even though a lot of intelligence tests are built on this kind of thing.",
            "And there's no.",
            "There's not really a way to tell, so that's the problem of induction in philosophy.",
            "Now statistical learning theory, which is.",
            "Maybe the most complete theory of generalization that we have.",
            "Asked a slightly different question.",
            "It doesn't ask which one is correct.",
            "But it asks more metal question.",
            "It asks what are how?",
            "What kind of principles one should use to come up with laws that are probably correct and there should be a bit more accurate here to come up with laws that are probably as correct on the test data as they are on the training data.",
            "So it's hard to come up with laws where the training error does not mislead you about the test error that you should expect in the future.",
            "So that's a slightly simpler problem, and also this problem has been studied in philosophy.",
            "It has been called the demarcation problem.",
            "So that both these kind of questions have been discussed extensively by Papa and Papa calls this well, maybe probably not only provide proper cause, this is action problem or Humes problem.",
            "The problem of David Hume and he calls this demarcation problem or counts problem.",
            "So this was this is what country was interested with.",
            "One of the things can't Immanuel Kant was interested in.",
            "So basically the question what are the methodological principles that distinguish physics from metaphysics?",
            "So what should we be doing to produce valid natural scientific knowledge?"
        ],
        [
            "OK, and I think I'm going not to not talk about this."
        ],
        [
            "No, because I will talk."
        ],
        [
            "It is in."
        ],
        [
            "More detail later on, so I'm moving now to my latex slides."
        ],
        [
            "OK.",
            "The course about kernel methods, but I would actually start start with a bit of learning theory because I always think it's nice to have a bit of learning theory at the beginning."
        ],
        [
            "Of a summer school.",
            "So statistical learning theory was started by public challenges in the 60s and we have a model.",
            "Which is that we observed data generated by an unknown stochastic regularity is unknown but fixed.",
            "I'll just say more about this in a minute, and learning consists of extracting this regularity.",
            "And the interesting thing about this area, maybe?",
            "Maybe I have to close this a bit more, so this area was basically started by public in 10 minutes during their PhD thesis.",
            "Maybe they were interested in this problem of induction.",
            "And it turned out that they looked at it in a relatively simple scenario, but it turns out that a lot of structure is already there.",
            "There are a lot of interesting stuff came out and they found these notions of capacity or complexity of function classes, and I think working in machine learning.",
            "You should all know about these, even even if maybe for your work you're not going to need things like VC dimension.",
            "Sorry for taking the Seaview here.",
            "And OK so."
        ],
        [
            "Let's see.",
            "So let's start with the problem of pattern recognition.",
            "So in pattern recognition, and that's also the problem.",
            "But then when you started with, you want to learn a function simple function that just takes 2 possible output values.",
            "So you have some input domain, which could be any set and you have output values plus minus one and you make the assumption that they were generated ID from an unknown underlying probability distribution joint distribution.",
            "So that's sort of a random experiment.",
            "Oh this is work.",
            "No.",
            "So you have a random experiment.",
            "This includes a special case.",
            "Of course.",
            "The case where you have a deterministic relationship between X&Y but usually we don't have that, so this is a general relationship between X&Y and.",
            "Our goal is now to find a function which takes as input X and produces as output away, which is somehow similar to those.",
            "Or which somehow typical under this distribution?",
            "And by this we mean that it will minimize F of X.",
            "There's a loss function between F of X&Y.",
            "This is a particularly simple loss function such that the expectation over this unknown distribution of that loss will be minimized.",
            "This is called the risk, and this loss function here is called the 01 classification loss function.",
            "Weight is 01, so if you plug in, YY is plus minus one.",
            "X or F of X also takes values plus minus one, so this is plus minus one.",
            "This plus minus one.",
            "We take a modulus and divided by a factor of two, so this quantity is zero or one depending on whether F of X is equal to Y or different from Y.",
            "So we want to minimize the expectation of this.",
            "And but we cannot even compute this quantity because P is unknown.",
            "So we need an induction principle.",
            "We need a principle to get us to a function which is close to the minimizer function, which has a risk close to the smallest possible one.",
            "We want to find that function based on the empirical data set.",
            "And the simplest induction principle is called empirical risk minimization.",
            "And this work public generally starts to study in Russia in the 60s.",
            "And this principle is basically just says we replace the average over the unknown distribution by an empirical average.",
            "Over the training training sample, instead of training examples.",
            "So typically this whole set is called a training sample, and one of these is an example.",
            "You always see people calling this a sample and.",
            "So you want to.",
            "Minimize the training error.",
            "This quantity.",
            "That's the empirical approximation of this.",
            "And find a function that minimizes this quantity and the question is when would that function be somehow close to this one?",
            "Or when would the risk of that function converge to the minimal risk and?",
            "By the first, you might think that this is trivial.",
            "There's this thing called the law of large numbers, which says something about convergence of means to expectations.",
            "So remember that this.",
            "This thing here is just an empirical mean of a certain random variable.",
            "In this the expectational the same random variable, and so we would think that she."
        ],
        [
            "Be easy.",
            "The law of large numbers tells us that such a quantity will converge to his expectation as the number of observations goes to Infinity.",
            "Whenever we have some fixed function F here.",
            "Unfortunately, that's not enough for us, so this does not imply that empirical risk mitigation will give us the optimal result in the limit.",
            "So that's called consistency in statistics.",
            "And it turns out that for consistency to be true, we need a uniform version of this law of large numbers and it's uniform overall functions.",
            "So here I said that conversion is true for any fixed function in our last bit.",
            "More specific about this later we need it uniformly over all functions that the learning machine can implement.",
            "So I sort of glossed over this before, but.",
            "Well, I'll get to this now here."
        ],
        [
            "So uniform convergence.",
            "So typically a learning machine.",
            "You have a set, a whole set of functions that you choose from.",
            "So we set of functions, empirical observations.",
            "Now you want to choose a function based on your observations.",
            "Each of these functions will have a certain risk, so certain expected error.",
            "Each of the functions will have a certain empirical risk assessment training error for a given set of observations and the law of large numbers classical of national that.",
            "And.",
            "For every fixed function, if the training error will converge to the true error in probabilities, so the probability of a large deviation will go to zero, actually exponentially fast.",
            "But it doesn't.",
            "All this does not imply that as we keep increasing the number of observations.",
            "So recall is we get more observations, empirical error changes.",
            "So this I'm running out of battery here.",
            "This curve here will change.",
            "And this code will gradually, in all points will get closer to this curve, but this will not imply that the minimize the minimum of this curve will converge towards the minimum of this curve, surprisingly, and the reason is basically the difference between uniform convergence and pointwise convergence, and I think the mathematicians among you will probably immediately see why this is the case.",
            "But I'm not going to explain it now.",
            "Will go through more mathematics during the course of this lecture budget.",
            "Just this is just to get a little bit of intuition in the 1st place, so I guess you can imagine if we have uniform convergence.",
            "So if this curve converges to this one at the same speed everywhere, basically then also the minimum will converge.",
            "Which is."
        ],
        [
            "Basically what we want.",
            "OK, so.",
            "So sometimes people ask how to choose that set of functions that we should do our inference with.",
            "So what kind of functions should our learning machine be able to implement?",
            "And since we maybe don't know what is the problem, we want to learn, we might be tempted to say, well, how about just taking all functions that take our input for the amazing paint domain and maybe to plus minus one and that actually turns out to be impossible.",
            "And it's relatively easy to see once you think about your different conversions as followers, so we have a suppose we have a training set like before, and suppose in addition we already have the we have the test inputs X bar and let's assume for simplicity that is joined from the training inputs.",
            "So in a continuous problem with the problem has a density, then this will be almost always true.",
            "This probability that is not true will be 0 basically, and if this is the case, so there's no overlap, then of course whenever you give me some function F. So suppose you give me a function F and tell me I reckon this is a good solution for the problem.",
            "So I've looked at the training set and I came up with this function FI think this function will be good.",
            "I can give you another function F star which is defined as follows, or which is constructed as follows.",
            "F star agrees with F on all the training points, but it disagrees with F on all the other points on all the test points because they are disjoint, so these are disjoined.",
            "I can always construct such a function if I have no restriction on the class of functions.",
            "So now, based on the training set, there is no means of choosing which one of these two functions is better.",
            "They say exactly the same, but on the test that they give opposite results.",
            "Therefore, if you are.",
            "If you go back to."
        ],
        [
            "This picture these two functions will have the same empirical risk.",
            "They have the same training error, but on the test point there could be arbitrarily different so that they can have very different actual risks.",
            "Therefore, if one of these two functions had an actual risk that was close to the empirical risk, the other one might have an actual risk that is far away from the empirical risks, so it will never be the case that these curves are close together everywhere.",
            "So in this case uniform convergence will be impossible."
        ],
        [
            "Now this kind of observation has sometimes been called the no free lunch theorem, but it was basically so.",
            "This was in machine learning, popularized under this name, but it was basically of course known in statistics for a long time.",
            "That this is a basic problem.",
            "So the conclusion is we must have a restriction on the class of functions that we allow."
        ],
        [
            "There's several ways of doing this, and maybe the two best known ones are one from learning theory.",
            "We have to take into account the capacity or complexity of the class of functions, or the Bayesian way, which is to place prior distributions on the class of functions."
        ],
        [
            "OK, so in my lecture I will at least this first lecture.",
            "I will take the first point of view with the goal of introducing these capacity concepts to you.",
            "And for this we need a slightly more detailed analysis of what I explained so far.",
            "And to do this, let's define some quantities here.",
            "Let's define these random variables.",
            "Cy I.",
            "These are defined as follows.",
            "They're basically the loss of the example XIYI, so they can be zero or one, so these are called sometimes called Bernoulli trials, is like flipping a coin.",
            "So statistically this is nothing but flipping a coin.",
            "And they are independent, spending their trials.",
            "Provided that this is a fixed function that doesn't know anything about the whole training set, and this is a function, and we plug in X&Y, and since we have made an assumption which I hope I've said, but if not, I repeat it now it's probably written on the slides, so all the XI.",
            "Why are why are independently drawn from the same joint probability distribution over X&Y so Zion?",
            "Here's that are independent from each other, so X1Y one is independent from X2Y2 and so on.",
            "Of course, the X is not independent of the way, otherwise it would be pointless to do learning.",
            "But each XI why appear is drawn independently.",
            "And independent penalty trials.",
            "We can write down the empirical mean.",
            "Is this quantity which is in the same as the empirical risk?",
            "And we can consider the expected value, which then will simply formally be the same as the actual risk.",
            "And then we can ask the question when does the empirical mean converge through the expectation and so on."
        ],
        [
            "And this is something called channel sound.",
            "Which basically tells us about the speed of the law of large numbers.",
            "This was.",
            "Developed a phone by statistician by the name of Herman Channel, American Statistician.",
            "And.",
            "The bond basically says that the probability that.",
            "This empirical mean.",
            "And the actual the expectation deviates by at least epsilon is upper bounded by this quantity that goes to zero exponentially fast, no matter how small epsilon is.",
            "So for any fixed, potentially very small epsilon.",
            "The probability of this deviation goes to zero exponentially fast, so as long as we just take enough observations, these two quantities will be close to each other.",
            "Basically, as close as we want them to be.",
            "And this probability here.",
            "So just so you don't get confused, this is the probability of obtaining a so.",
            "So this is a product measure.",
            "So-called product measure is the probability of obtaining a sample XI one through Siam.",
            "With this property here.",
            "OK, so this is not over single draw a training example, but this is overdrawing a whole training set.",
            "And there's a corollary that one can prove based on this relatively easy in the corollary now looks not at the deviation between mean and expectation, but the deviation between 2 means.",
            "So certainly you can imagine if every mean is close to its expectation, then also every two independent means should be close to each other.",
            "Maybe a little bit further apart and a little bit further apart is here reflected by this factor of two, which we are losing.",
            "Here and we also losing here factor of two.",
            "So basically if you hear plug in epsilon over two, you will end up with this thing here 'cause it's squared.",
            "OK, so let's remember that we can do that as well."
        ],
        [
            "And I've already told you, but just to remind you, if we translate this.",
            "So there's this correspondence between the expectation of science and the Pyramid of Science and the risk actual and empirical risk.",
            "And if we plug this back into this channel bond, then basically this tells us that the probability of a large deviation between test error or actual risk and training error goes to zero exponentially fast for every function F that we could plug into here.",
            "But every individual function is not a uniform statement yet.",
            "This is not truly uniformly over all functions, so it's true for one fixed function F. But unfortunately we are not allowed to look at the data.",
            "Before choosing that function, if otherwise we could just say, well, we'll just choose the training error minimizer as our function F. And then this will give us bound how far the actual risk of the training error minimizer will be from the training error.",
            "So why are we not allowed to do that?",
            "Let's see if I have that on here.",
            "I don't have the money, so any any guesses why this would be illegal?",
            "Speculations.",
            "So I'll tell you why this is illegal.",
            "If we go back here.",
            "I said before."
        ],
        [
            "That these things here are independent penalty trials.",
            "And that's important because."
        ],
        [
            "If they were not, then we wouldn't have the channel button, so the general found uses this independence.",
            "However, if we know use the training data, the whole set of training."
        ],
        [
            "Points to choose a function.",
            "So for instance to choose the function that has the minimal risk on the training set.",
            "So the function that minimizes the training error.",
            "Of course, this function looks at all the training points, so now these training points were independent.",
            "But somehow this function has looked at all of them, so it has absorbed some knowledge from each of the training points.",
            "And therefore, if we then plug that function in here.",
            "Maybe I'll put it here."
        ],
        [
            "If not, we plug in a function here which has seen all training points X one through XN&Y, one Thruway, and then of course the size that we have constructed from that function are no longer independent.",
            "Each of these size knows a little bit about the other size, they are no longer independent, so that's that's not wouldn't be a valid."
        ],
        [
            "So it's it's more complicated.",
            "We have to do the more complete."
        ],
        [
            "Dated a route and of course that's interesting because it will lead us to a lot of interesting structure.",
            "And basically what it leads to.",
            "Is.",
            "BBC, yeah.",
            "Yeah, so and.",
            "What if applicant ever knickers have proven is the following, but they have proven a number of things.",
            "But this is one of the central theorems of machine learning, and we're not going to prove this, but.",
            "I'll show you some other things, but let me go through this briefly first so there are necessary and sufficient conditions for a certain type of consistency, so I mentioned before consistency, so just roughly consistency means in the limit of infinite data points.",
            "Empirical empirical risk minimization.",
            "Will give us the the right result, by which I mean it will give us a risk which will be as close as.",
            "Arbitrarily close to the lowest possible risk, it doesn't necessarily tell us.",
            "We will find a function which is our truly close to the best possible function, because there might be multiple functions that have the minimal risk, but it tells us we find we will gradually get functions whose risks will get arbitrarily close to the best possible.",
            "So that's the question of consistency, and it turns out one has to use a notion of nontrivial consistency, which I can't go into detail on.",
            "But if you're into learning theory, then certainly you should study that.",
            "And because.",
            "This notion of non triviality is constructed when there is constructed in such a way that these conditions have become necessary and sufficient.",
            "So example might argue one doesn't have to do it.",
            "This way.",
            "We could do it differently, which case this would not become necessary and sufficient, but that's just a side note.",
            "So now for general purpose machine learning audience, see there's a notion of consistency such that the following is necessary and sufficient for uniform for uniform convergence.",
            "Sorry for consistency.",
            "And the necessary and sufficient conditions for uniform for consistency, sorry, say again, the necessary and sufficient conditions for consistency of empirical risk, minimizations of training and neural network or whatever.",
            "By minimizing the training error, is to have a certain type of uniform convergence and a certain type is the following.",
            "It's a young 1 sided uniform convergence uniformly over all functions that can be implemented by a learning machine.",
            "Which means.",
            "This quantity here.",
            "So the supremum over all functions that we can implement with the learning machine of this deviation.",
            "Now without a modulus.",
            "So that's why it's 1 sided.",
            "And.",
            "The probability that this deviation in the One Direction is larger than epsilon will go to zero no matter how we choose epsilon.",
            "So this now relates to the picture that I showed you before this graphical picture and the reason.",
            "So this is basically uniform convergence, but it's slightly weaker.",
            "It's only from one side.",
            "The reason for this is that we are only interested in consistency of empirical risk minimization if we were also interested in empirical maximization, then we would have.",
            "2 sided uniform convergence.",
            "But the point here is that this is a property that.",
            "Principle you could.",
            "It's a property of a learning machine.",
            "In principle you could check whether you learning machine has this property.",
            "It takes into account the whole set of functions that the learning machine can implement, because here we have a supremum.",
            "And of course it's it's hard to check.",
            "This is sort of an unwieldy condition and therefore it's natural to ask the question.",
            "Are there other properties of learning machines that maybe imply this or maybe even that are equivalent to this?",
            "And it turns out that there are such properties imply this or that with additional.",
            "Requirements so for instance, if we say that if we want this to be true for all underlying probability distributions, because here this risk, of course there's an expectation over an unknown probability distribution, so this with this is true depends, should be more specific about this.",
            "It depends not only in the learning machine, but also on the problem on the unknown probability distribute distribution.",
            "But if we wanted this to be true for all probability distributions, then maybe we would find quantities that only properties of the learning machines.",
            "And that's in."
        ],
        [
            "Not true.",
            "So let's take a closer look at this quantity that.",
            "The last theorem was talking about.",
            "So just to remind you."
        ],
        [
            "This thing here, so we want to see how does this thing behave windows this probability go to 0, how faster?"
        ],
        [
            "Go to program to zero and so on."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was going to maybe talk 510 minutes just in general because this is the first of the summer school about machine learning because it is a school and machine learning we should sort of assume that people don't yet know machine learning.",
                    "label": 0
                },
                {
                    "sent": "But of course I think almost everybody does.",
                    "label": 0
                },
                {
                    "sent": "But usually we have a few people at the summer school who really starting from zero, just with a background in computer science or whatever.",
                    "label": 0
                },
                {
                    "sent": "Talking about background.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting to know how many.",
                    "label": 0
                },
                {
                    "sent": "How many of you are computer scientists?",
                    "label": 0
                },
                {
                    "sent": "That's about half maybe how many are engineers?",
                    "label": 0
                },
                {
                    "sent": "Uh, huh mathematicians?",
                    "label": 0
                },
                {
                    "sent": "Physicists psychologists, neuro scientists.",
                    "label": 0
                },
                {
                    "sent": "And linguists.",
                    "label": 0
                },
                {
                    "sent": "Have I forgotten something?",
                    "label": 0
                },
                {
                    "sent": "That's it, huh?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so that's two people.",
                    "label": 0
                },
                {
                    "sent": "Mix in a machine learning class.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I briefly want to tell you something about machine learning.",
                    "label": 0
                },
                {
                    "sent": "From my point of view or empirical inference, which is the process of drawing conclusions from empirical observations.",
                    "label": 1
                },
                {
                    "sent": "So, for instance, we might have scientific inference, or we might get observations of this shape here.",
                    "label": 0
                },
                {
                    "sent": "I didn't bring a laser pointer, we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't have one here.",
                    "label": 0
                },
                {
                    "sent": "I have one in my hotel room.",
                    "label": 0
                },
                {
                    "sent": "I can bring it next time.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe here's one.",
                    "label": 0
                },
                {
                    "sent": "But I could use that mouse pointer Meanwhile, so we might have observations of these two OBS observers.",
                    "label": 0
                },
                {
                    "sent": "Joint observations of X&Y.",
                    "label": 0
                },
                {
                    "sent": "Maybe that looked like this, in which case we might be willing to speculate that there's a thank you very much.",
                    "label": 0
                },
                {
                    "sent": "There's a linear relationship between the between these two observations, so between these two quantities, observables X&Y.",
                    "label": 0
                },
                {
                    "sent": "Which may be looks like this, but on the other hand, who tells us it's not in reality and much more complicated relationship that explains a set of data points, something for instance like this, which also might be describable in mathematical terms.",
                    "label": 0
                },
                {
                    "sent": "And my perfectly explain the data and this is of course an old question which was started by a number of people, including labor, and it's down here.",
                    "label": 0
                },
                {
                    "sent": "Who had this thought experiment?",
                    "label": 0
                },
                {
                    "sent": "The Julia I have this from the Gregory Chitine.",
                    "label": 0
                },
                {
                    "sent": "Thought experiment is you take a an ink quill pen and you dip it into your ink and then splash it over the paper.",
                    "label": 0
                },
                {
                    "sent": "You get some random points like we said, well, you could always find some mathematical explanation for these random points.",
                    "label": 0
                },
                {
                    "sent": "So so in which case or leggings was asking a question, in which case would we call such an expression a law of nature?",
                    "label": 0
                },
                {
                    "sent": "So certainly if it's just something random and if we can always find something section explanation, we wouldn't call it a lot of nature.",
                    "label": 0
                },
                {
                    "sent": "So to call it a lot later, it has to satisfy additional criteria and maybe machine learning is a lot of machine learning is about what kind of criteria it should satisfy and what we can guarantee if it satisfies these criteria.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And from there, so here's a quote for physicist for physicists.",
                    "label": 0
                },
                {
                    "sent": "For this.",
                    "label": 0
                },
                {
                    "sent": "With this rather fault, this was just sort of a stupid question.",
                    "label": 0
                },
                {
                    "sent": "He said that basically it would be evident.",
                    "label": 0
                },
                {
                    "sent": "So if you're experimenting statistics, if you have to do any nontrivial analysis of the data, you have done a better experiment.",
                    "label": 1
                },
                {
                    "sent": "So this is the enemy of all machine learning, but in those days machine learning didn't exist and big computers didn't exist, of course.",
                    "label": 0
                },
                {
                    "sent": "So this was maybe a reasonable point of view.",
                    "label": 0
                },
                {
                    "sent": "It's certainly so this is a certain type of inductive bias, and maybe it's reasonable for him.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Background, but maybe not for us.",
                    "label": 0
                },
                {
                    "sent": "At least we're in the business of doing inference or doing non trivial statistics.",
                    "label": 0
                },
                {
                    "sent": "And so let's look at the second example of empirical inference from the world of perception.",
                    "label": 0
                },
                {
                    "sent": "So if you look at these handwritten digits who have seen these digits before.",
                    "label": 0
                },
                {
                    "sent": "The French the percentage is going down.",
                    "label": 0
                },
                {
                    "sent": "We were all raised with these digits in the old days as PhD students.",
                    "label": 0
                },
                {
                    "sent": "We all had to work on this data set.",
                    "label": 0
                },
                {
                    "sent": "This is the USPS data set of zip codes collected in the post Office of Buffalo or something like that.",
                    "label": 0
                },
                {
                    "sent": "And you can easily recognize these zip codes and we have worked on this for a long time.",
                    "label": 0
                },
                {
                    "sent": "We can recognize them even better, but still you see these digits.",
                    "label": 0
                },
                {
                    "sent": "But if I permute the pixels.",
                    "label": 0
                },
                {
                    "sent": "By fixed permutation.",
                    "label": 0
                },
                {
                    "sent": "So this is always the same permutation.",
                    "label": 0
                },
                {
                    "sent": "You have a much harder time seeing the digits.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can still see that there's something different about the zeros compared to the threes, but it's much harder.",
                    "label": 0
                },
                {
                    "sent": "So the question is, is it is it now a hard problem or is it an easy problem and we aren't in this case?",
                    "label": 0
                },
                {
                    "sent": "Or one answer would be that in reality it's a hard problem, but this one looks relatively easy to us, so that's a partial answer that you can.",
                    "label": 0
                },
                {
                    "sent": "You can debate about that.",
                    "label": 0
                },
                {
                    "sent": "This one looks easy to ask because we've been trained on this kind of data for a long, long time.",
                    "label": 0
                },
                {
                    "sent": "During our life.",
                    "label": 0
                },
                {
                    "sent": "This kind of data looks different.",
                    "label": 0
                },
                {
                    "sent": "Looks looks harder for us.",
                    "label": 0
                },
                {
                    "sent": "We're not used to this data, which maybe makes it more interesting, or at least makes it apparent that it's a hard problem.",
                    "label": 0
                },
                {
                    "sent": "And of course a lot of the problems that we're dealing with.",
                    "label": 0
                },
                {
                    "sent": "For instance, in bioinformatics, use of this type that we haven't seen that kind of data before.",
                    "label": 0
                },
                {
                    "sent": "So this is a brain has learned to solve this kind of problems and we can see that this one looks easy to ask and there's a famous neuro scientists Horace Barlow.",
                    "label": 0
                },
                {
                    "sent": "But once said that the brain is nothing but a statistical decision organ.",
                    "label": 1
                },
                {
                    "sent": "So we have different organs in our body, some of them pump blood, so them clean the blood and there's one that's built for statistical decisions.",
                    "label": 0
                },
                {
                    "sent": "Let's the brain.",
                    "label": 0
                },
                {
                    "sent": "So if you are studying empirical inference and learning in a sense we are also doing theory.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tickle brain research.",
                    "label": 0
                },
                {
                    "sent": "The level of looking at neurons by the level of tasks that the brain is to solve.",
                    "label": 0
                },
                {
                    "sent": "So now it showed you two inference problems.",
                    "label": 0
                },
                {
                    "sent": "When looked easy is 2 dimensional problem where you should directly see what's going on or where.",
                    "label": 0
                },
                {
                    "sent": "We probably see, in which case we can generalize which case we can't.",
                    "label": 0
                },
                {
                    "sent": "The other one that looked hard.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Let's look at another hard inference problem on.",
                    "label": 0
                },
                {
                    "sent": "Let's look at some maybe what are general properties of hard inference problems.",
                    "label": 0
                },
                {
                    "sent": "So this is a problem.",
                    "label": 0
                },
                {
                    "sent": "Bioinformatics and it's a classifier.",
                    "label": 0
                },
                {
                    "sent": "So this is just a component of a larger system.",
                    "label": 0
                },
                {
                    "sent": "To find jeans, this is a classifier that classifieds DNA sequence locations.",
                    "label": 1
                },
                {
                    "sent": "I forgot to ask there are there computational biologists in the audience also.",
                    "label": 0
                },
                {
                    "sent": "OK so I missed a significant group before in my question so.",
                    "label": 0
                },
                {
                    "sent": "Classifying human a human DNA sequence locations into three classes doesn't matter if you don't know about these topics, it's just a three class classification problem, which is highly unbalanced, so the classes don't have the same size and the input data is pieces of DNA sequence of a certain length, so it's a window around the place that you want to classify.",
                    "label": 0
                },
                {
                    "sent": "This is 141.",
                    "label": 0
                },
                {
                    "sent": "We have 15 million training points.",
                    "label": 0
                },
                {
                    "sent": "A certain type of machine learning methods, and this is a certain type of performance criterion called the precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter what the details are.",
                    "label": 0
                },
                {
                    "sent": "The point that I want to make here only is.",
                    "label": 0
                },
                {
                    "sent": "But here as you increase the number of training examples, performance gets better.",
                    "label": 0
                },
                {
                    "sent": "So initially for small training sets you are more or less at chance level, so the data set looks random to us.",
                    "label": 0
                },
                {
                    "sent": "But as we increase the number of training points quite a bit here we are at 10 million.",
                    "label": 0
                },
                {
                    "sent": "We can do a very good job at solving this classification problem and maybe this looks like a trivial point, but I think it's important to keep this in mind when we think about machine learning.",
                    "label": 0
                },
                {
                    "sent": "If we were a human looking at this data set, we would always be in a regime somewhere down here and we would look at small datasets.",
                    "label": 0
                },
                {
                    "sent": "We would maybe think that this is completely random.",
                    "label": 0
                },
                {
                    "sent": "There's no structure, but if we have advanced machine learning methods that can handle large datasets, find nonlinear relationships.",
                    "label": 0
                },
                {
                    "sent": "We can be over here and we can certainly see a lot of structure, so a lot of stuff that maybe it might appear human random to us.",
                    "label": 0
                },
                {
                    "sent": "Humans may not be so random and we might be able to find structure with machine learning, and I think that's only gradually being acknowledged that this can be a real paradigm shift in how we interpret data in the world.",
                    "label": 0
                },
                {
                    "sent": "Nothing machine learning is going to play an important role in that so.",
                    "label": 0
                },
                {
                    "sent": "Setting the right topic and so what's characteristic of such problems.",
                    "label": 0
                },
                {
                    "sent": "They are typically high dimensional, so you have to look at many variables simultaneously.",
                    "label": 0
                },
                {
                    "sent": "They are complex in various ways.",
                    "label": 0
                },
                {
                    "sent": "We have little prior knowledge, by which I mean that, for instance, we don't have a full mechanistic model for the data.",
                    "label": 1
                },
                {
                    "sent": "If we had that, or if the scientists had that, then probably they wouldn't bother with asking us for help or they wouldn't bother with applying machine learning.",
                    "label": 0
                },
                {
                    "sent": "And of course, as a consequence of these points, typically we need large datasets.",
                    "label": 0
                },
                {
                    "sent": "In these kind of course only be processed if we have computers, which was of course an enabling technology for machine learning.",
                    "label": 0
                },
                {
                    "sent": "The increase in computing power over the last decades and if.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have advanced inference methods.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so I already made this point first one.",
                    "label": 0
                },
                {
                    "sent": "We can solve problems that humans can't solve.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Even though this is probably just because of the data sets, or even if it's just the data set size dimensionality, it's a major step.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the main what is the main issue of machine learning and maybe the main issue is generalization.",
                    "label": 0
                },
                {
                    "sent": "And just to give you an idea what generalization about is about or what is the problem, suppose we see this number sequence 1247.",
                    "label": 0
                },
                {
                    "sent": "Ask the question what is the next number in this sequence?",
                    "label": 0
                },
                {
                    "sent": "Any any guesses?",
                    "label": 0
                },
                {
                    "sent": "1116 any others?",
                    "label": 0
                },
                {
                    "sent": "12 OK so 11 is its first good solution.",
                    "label": 0
                },
                {
                    "sent": "It's a simple sequence.",
                    "label": 0
                },
                {
                    "sent": "It's called the lazy caterer sequence because it's the maximum number of pieces into which you can cut a piece of cake with N cuts.",
                    "label": 0
                },
                {
                    "sent": "You just have to make sure that every additional cuts, intersects or previous cuts.",
                    "label": 0
                },
                {
                    "sent": "So in that case the end cut gives you an additional pieces and they are not the same size, but there are pieces, so this is a simple sequence.",
                    "label": 0
                },
                {
                    "sent": "12 was also good.",
                    "label": 0
                },
                {
                    "sent": "Is this 113?",
                    "label": 0
                },
                {
                    "sent": "Is the so called Triple Nachi sequence where each number is the sum of the previous 316?",
                    "label": 0
                },
                {
                    "sent": "We're going to ask Neil for an explanation later on.",
                    "label": 0
                },
                {
                    "sent": "You said 60 in India.",
                    "label": 0
                },
                {
                    "sent": "Or someone else?",
                    "label": 0
                },
                {
                    "sent": "And well, this one is also a nice one.",
                    "label": 0
                },
                {
                    "sent": "This is a sequence that ends at 28, is just a set of devices of 28.",
                    "label": 1
                },
                {
                    "sent": "This is the one that delivers, came up with his decimal expansions of \u03c0 and E interleaved, which is also a nice sequence for a mathematician.",
                    "label": 0
                },
                {
                    "sent": "And of course, you can ask the question now which one?",
                    "label": 0
                },
                {
                    "sent": "Which one is right and.",
                    "label": 0
                },
                {
                    "sent": "Oh, so there's this website where you can enter these digits and you get a lot of hits of meaningful sequence with sequences that start like that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can ask the question which one is correct?",
                    "label": 1
                },
                {
                    "sent": "Which one generalizes?",
                    "label": 0
                },
                {
                    "sent": "And of course you will say this is sort of a stupid questions, even though a lot of intelligence tests are built on this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "And there's no.",
                    "label": 0
                },
                {
                    "sent": "There's not really a way to tell, so that's the problem of induction in philosophy.",
                    "label": 0
                },
                {
                    "sent": "Now statistical learning theory, which is.",
                    "label": 1
                },
                {
                    "sent": "Maybe the most complete theory of generalization that we have.",
                    "label": 0
                },
                {
                    "sent": "Asked a slightly different question.",
                    "label": 0
                },
                {
                    "sent": "It doesn't ask which one is correct.",
                    "label": 0
                },
                {
                    "sent": "But it asks more metal question.",
                    "label": 0
                },
                {
                    "sent": "It asks what are how?",
                    "label": 0
                },
                {
                    "sent": "What kind of principles one should use to come up with laws that are probably correct and there should be a bit more accurate here to come up with laws that are probably as correct on the test data as they are on the training data.",
                    "label": 1
                },
                {
                    "sent": "So it's hard to come up with laws where the training error does not mislead you about the test error that you should expect in the future.",
                    "label": 0
                },
                {
                    "sent": "So that's a slightly simpler problem, and also this problem has been studied in philosophy.",
                    "label": 0
                },
                {
                    "sent": "It has been called the demarcation problem.",
                    "label": 0
                },
                {
                    "sent": "So that both these kind of questions have been discussed extensively by Papa and Papa calls this well, maybe probably not only provide proper cause, this is action problem or Humes problem.",
                    "label": 0
                },
                {
                    "sent": "The problem of David Hume and he calls this demarcation problem or counts problem.",
                    "label": 0
                },
                {
                    "sent": "So this was this is what country was interested with.",
                    "label": 0
                },
                {
                    "sent": "One of the things can't Immanuel Kant was interested in.",
                    "label": 0
                },
                {
                    "sent": "So basically the question what are the methodological principles that distinguish physics from metaphysics?",
                    "label": 0
                },
                {
                    "sent": "So what should we be doing to produce valid natural scientific knowledge?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and I think I'm going not to not talk about this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, because I will talk.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is in.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More detail later on, so I'm moving now to my latex slides.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The course about kernel methods, but I would actually start start with a bit of learning theory because I always think it's nice to have a bit of learning theory at the beginning.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of a summer school.",
                    "label": 0
                },
                {
                    "sent": "So statistical learning theory was started by public challenges in the 60s and we have a model.",
                    "label": 1
                },
                {
                    "sent": "Which is that we observed data generated by an unknown stochastic regularity is unknown but fixed.",
                    "label": 1
                },
                {
                    "sent": "I'll just say more about this in a minute, and learning consists of extracting this regularity.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing about this area, maybe?",
                    "label": 0
                },
                {
                    "sent": "Maybe I have to close this a bit more, so this area was basically started by public in 10 minutes during their PhD thesis.",
                    "label": 0
                },
                {
                    "sent": "Maybe they were interested in this problem of induction.",
                    "label": 1
                },
                {
                    "sent": "And it turned out that they looked at it in a relatively simple scenario, but it turns out that a lot of structure is already there.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of interesting stuff came out and they found these notions of capacity or complexity of function classes, and I think working in machine learning.",
                    "label": 0
                },
                {
                    "sent": "You should all know about these, even even if maybe for your work you're not going to need things like VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Sorry for taking the Seaview here.",
                    "label": 0
                },
                {
                    "sent": "And OK so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "So let's start with the problem of pattern recognition.",
                    "label": 0
                },
                {
                    "sent": "So in pattern recognition, and that's also the problem.",
                    "label": 0
                },
                {
                    "sent": "But then when you started with, you want to learn a function simple function that just takes 2 possible output values.",
                    "label": 0
                },
                {
                    "sent": "So you have some input domain, which could be any set and you have output values plus minus one and you make the assumption that they were generated ID from an unknown underlying probability distribution joint distribution.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of a random experiment.",
                    "label": 0
                },
                {
                    "sent": "Oh this is work.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So you have a random experiment.",
                    "label": 0
                },
                {
                    "sent": "This includes a special case.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "The case where you have a deterministic relationship between X&Y but usually we don't have that, so this is a general relationship between X&Y and.",
                    "label": 0
                },
                {
                    "sent": "Our goal is now to find a function which takes as input X and produces as output away, which is somehow similar to those.",
                    "label": 0
                },
                {
                    "sent": "Or which somehow typical under this distribution?",
                    "label": 0
                },
                {
                    "sent": "And by this we mean that it will minimize F of X.",
                    "label": 0
                },
                {
                    "sent": "There's a loss function between F of X&Y.",
                    "label": 0
                },
                {
                    "sent": "This is a particularly simple loss function such that the expectation over this unknown distribution of that loss will be minimized.",
                    "label": 0
                },
                {
                    "sent": "This is called the risk, and this loss function here is called the 01 classification loss function.",
                    "label": 0
                },
                {
                    "sent": "Weight is 01, so if you plug in, YY is plus minus one.",
                    "label": 0
                },
                {
                    "sent": "X or F of X also takes values plus minus one, so this is plus minus one.",
                    "label": 0
                },
                {
                    "sent": "This plus minus one.",
                    "label": 0
                },
                {
                    "sent": "We take a modulus and divided by a factor of two, so this quantity is zero or one depending on whether F of X is equal to Y or different from Y.",
                    "label": 0
                },
                {
                    "sent": "So we want to minimize the expectation of this.",
                    "label": 0
                },
                {
                    "sent": "And but we cannot even compute this quantity because P is unknown.",
                    "label": 1
                },
                {
                    "sent": "So we need an induction principle.",
                    "label": 1
                },
                {
                    "sent": "We need a principle to get us to a function which is close to the minimizer function, which has a risk close to the smallest possible one.",
                    "label": 0
                },
                {
                    "sent": "We want to find that function based on the empirical data set.",
                    "label": 1
                },
                {
                    "sent": "And the simplest induction principle is called empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "And this work public generally starts to study in Russia in the 60s.",
                    "label": 1
                },
                {
                    "sent": "And this principle is basically just says we replace the average over the unknown distribution by an empirical average.",
                    "label": 1
                },
                {
                    "sent": "Over the training training sample, instead of training examples.",
                    "label": 0
                },
                {
                    "sent": "So typically this whole set is called a training sample, and one of these is an example.",
                    "label": 0
                },
                {
                    "sent": "You always see people calling this a sample and.",
                    "label": 0
                },
                {
                    "sent": "So you want to.",
                    "label": 0
                },
                {
                    "sent": "Minimize the training error.",
                    "label": 0
                },
                {
                    "sent": "This quantity.",
                    "label": 0
                },
                {
                    "sent": "That's the empirical approximation of this.",
                    "label": 0
                },
                {
                    "sent": "And find a function that minimizes this quantity and the question is when would that function be somehow close to this one?",
                    "label": 0
                },
                {
                    "sent": "Or when would the risk of that function converge to the minimal risk and?",
                    "label": 0
                },
                {
                    "sent": "By the first, you might think that this is trivial.",
                    "label": 0
                },
                {
                    "sent": "There's this thing called the law of large numbers, which says something about convergence of means to expectations.",
                    "label": 0
                },
                {
                    "sent": "So remember that this.",
                    "label": 0
                },
                {
                    "sent": "This thing here is just an empirical mean of a certain random variable.",
                    "label": 0
                },
                {
                    "sent": "In this the expectational the same random variable, and so we would think that she.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be easy.",
                    "label": 0
                },
                {
                    "sent": "The law of large numbers tells us that such a quantity will converge to his expectation as the number of observations goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Whenever we have some fixed function F here.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, that's not enough for us, so this does not imply that empirical risk mitigation will give us the optimal result in the limit.",
                    "label": 1
                },
                {
                    "sent": "So that's called consistency in statistics.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that for consistency to be true, we need a uniform version of this law of large numbers and it's uniform overall functions.",
                    "label": 0
                },
                {
                    "sent": "So here I said that conversion is true for any fixed function in our last bit.",
                    "label": 1
                },
                {
                    "sent": "More specific about this later we need it uniformly over all functions that the learning machine can implement.",
                    "label": 0
                },
                {
                    "sent": "So I sort of glossed over this before, but.",
                    "label": 0
                },
                {
                    "sent": "Well, I'll get to this now here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "So typically a learning machine.",
                    "label": 0
                },
                {
                    "sent": "You have a set, a whole set of functions that you choose from.",
                    "label": 0
                },
                {
                    "sent": "So we set of functions, empirical observations.",
                    "label": 0
                },
                {
                    "sent": "Now you want to choose a function based on your observations.",
                    "label": 0
                },
                {
                    "sent": "Each of these functions will have a certain risk, so certain expected error.",
                    "label": 0
                },
                {
                    "sent": "Each of the functions will have a certain empirical risk assessment training error for a given set of observations and the law of large numbers classical of national that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "For every fixed function, if the training error will converge to the true error in probabilities, so the probability of a large deviation will go to zero, actually exponentially fast.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't.",
                    "label": 0
                },
                {
                    "sent": "All this does not imply that as we keep increasing the number of observations.",
                    "label": 0
                },
                {
                    "sent": "So recall is we get more observations, empirical error changes.",
                    "label": 0
                },
                {
                    "sent": "So this I'm running out of battery here.",
                    "label": 0
                },
                {
                    "sent": "This curve here will change.",
                    "label": 0
                },
                {
                    "sent": "And this code will gradually, in all points will get closer to this curve, but this will not imply that the minimize the minimum of this curve will converge towards the minimum of this curve, surprisingly, and the reason is basically the difference between uniform convergence and pointwise convergence, and I think the mathematicians among you will probably immediately see why this is the case.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to explain it now.",
                    "label": 0
                },
                {
                    "sent": "Will go through more mathematics during the course of this lecture budget.",
                    "label": 0
                },
                {
                    "sent": "Just this is just to get a little bit of intuition in the 1st place, so I guess you can imagine if we have uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "So if this curve converges to this one at the same speed everywhere, basically then also the minimum will converge.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically what we want.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So sometimes people ask how to choose that set of functions that we should do our inference with.",
                    "label": 1
                },
                {
                    "sent": "So what kind of functions should our learning machine be able to implement?",
                    "label": 0
                },
                {
                    "sent": "And since we maybe don't know what is the problem, we want to learn, we might be tempted to say, well, how about just taking all functions that take our input for the amazing paint domain and maybe to plus minus one and that actually turns out to be impossible.",
                    "label": 0
                },
                {
                    "sent": "And it's relatively easy to see once you think about your different conversions as followers, so we have a suppose we have a training set like before, and suppose in addition we already have the we have the test inputs X bar and let's assume for simplicity that is joined from the training inputs.",
                    "label": 0
                },
                {
                    "sent": "So in a continuous problem with the problem has a density, then this will be almost always true.",
                    "label": 0
                },
                {
                    "sent": "This probability that is not true will be 0 basically, and if this is the case, so there's no overlap, then of course whenever you give me some function F. So suppose you give me a function F and tell me I reckon this is a good solution for the problem.",
                    "label": 0
                },
                {
                    "sent": "So I've looked at the training set and I came up with this function FI think this function will be good.",
                    "label": 0
                },
                {
                    "sent": "I can give you another function F star which is defined as follows, or which is constructed as follows.",
                    "label": 0
                },
                {
                    "sent": "F star agrees with F on all the training points, but it disagrees with F on all the other points on all the test points because they are disjoint, so these are disjoined.",
                    "label": 0
                },
                {
                    "sent": "I can always construct such a function if I have no restriction on the class of functions.",
                    "label": 0
                },
                {
                    "sent": "So now, based on the training set, there is no means of choosing which one of these two functions is better.",
                    "label": 1
                },
                {
                    "sent": "They say exactly the same, but on the test that they give opposite results.",
                    "label": 0
                },
                {
                    "sent": "Therefore, if you are.",
                    "label": 0
                },
                {
                    "sent": "If you go back to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This picture these two functions will have the same empirical risk.",
                    "label": 0
                },
                {
                    "sent": "They have the same training error, but on the test point there could be arbitrarily different so that they can have very different actual risks.",
                    "label": 0
                },
                {
                    "sent": "Therefore, if one of these two functions had an actual risk that was close to the empirical risk, the other one might have an actual risk that is far away from the empirical risks, so it will never be the case that these curves are close together everywhere.",
                    "label": 0
                },
                {
                    "sent": "So in this case uniform convergence will be impossible.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this kind of observation has sometimes been called the no free lunch theorem, but it was basically so.",
                    "label": 1
                },
                {
                    "sent": "This was in machine learning, popularized under this name, but it was basically of course known in statistics for a long time.",
                    "label": 0
                },
                {
                    "sent": "That this is a basic problem.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion is we must have a restriction on the class of functions that we allow.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's several ways of doing this, and maybe the two best known ones are one from learning theory.",
                    "label": 0
                },
                {
                    "sent": "We have to take into account the capacity or complexity of the class of functions, or the Bayesian way, which is to place prior distributions on the class of functions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in my lecture I will at least this first lecture.",
                    "label": 0
                },
                {
                    "sent": "I will take the first point of view with the goal of introducing these capacity concepts to you.",
                    "label": 0
                },
                {
                    "sent": "And for this we need a slightly more detailed analysis of what I explained so far.",
                    "label": 1
                },
                {
                    "sent": "And to do this, let's define some quantities here.",
                    "label": 0
                },
                {
                    "sent": "Let's define these random variables.",
                    "label": 0
                },
                {
                    "sent": "Cy I.",
                    "label": 0
                },
                {
                    "sent": "These are defined as follows.",
                    "label": 0
                },
                {
                    "sent": "They're basically the loss of the example XIYI, so they can be zero or one, so these are called sometimes called Bernoulli trials, is like flipping a coin.",
                    "label": 0
                },
                {
                    "sent": "So statistically this is nothing but flipping a coin.",
                    "label": 0
                },
                {
                    "sent": "And they are independent, spending their trials.",
                    "label": 1
                },
                {
                    "sent": "Provided that this is a fixed function that doesn't know anything about the whole training set, and this is a function, and we plug in X&Y, and since we have made an assumption which I hope I've said, but if not, I repeat it now it's probably written on the slides, so all the XI.",
                    "label": 0
                },
                {
                    "sent": "Why are why are independently drawn from the same joint probability distribution over X&Y so Zion?",
                    "label": 0
                },
                {
                    "sent": "Here's that are independent from each other, so X1Y one is independent from X2Y2 and so on.",
                    "label": 0
                },
                {
                    "sent": "Of course, the X is not independent of the way, otherwise it would be pointless to do learning.",
                    "label": 0
                },
                {
                    "sent": "But each XI why appear is drawn independently.",
                    "label": 0
                },
                {
                    "sent": "And independent penalty trials.",
                    "label": 1
                },
                {
                    "sent": "We can write down the empirical mean.",
                    "label": 0
                },
                {
                    "sent": "Is this quantity which is in the same as the empirical risk?",
                    "label": 0
                },
                {
                    "sent": "And we can consider the expected value, which then will simply formally be the same as the actual risk.",
                    "label": 0
                },
                {
                    "sent": "And then we can ask the question when does the empirical mean converge through the expectation and so on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is something called channel sound.",
                    "label": 0
                },
                {
                    "sent": "Which basically tells us about the speed of the law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "This was.",
                    "label": 0
                },
                {
                    "sent": "Developed a phone by statistician by the name of Herman Channel, American Statistician.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The bond basically says that the probability that.",
                    "label": 0
                },
                {
                    "sent": "This empirical mean.",
                    "label": 0
                },
                {
                    "sent": "And the actual the expectation deviates by at least epsilon is upper bounded by this quantity that goes to zero exponentially fast, no matter how small epsilon is.",
                    "label": 0
                },
                {
                    "sent": "So for any fixed, potentially very small epsilon.",
                    "label": 0
                },
                {
                    "sent": "The probability of this deviation goes to zero exponentially fast, so as long as we just take enough observations, these two quantities will be close to each other.",
                    "label": 0
                },
                {
                    "sent": "Basically, as close as we want them to be.",
                    "label": 0
                },
                {
                    "sent": "And this probability here.",
                    "label": 0
                },
                {
                    "sent": "So just so you don't get confused, this is the probability of obtaining a so.",
                    "label": 0
                },
                {
                    "sent": "So this is a product measure.",
                    "label": 1
                },
                {
                    "sent": "So-called product measure is the probability of obtaining a sample XI one through Siam.",
                    "label": 1
                },
                {
                    "sent": "With this property here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is not over single draw a training example, but this is overdrawing a whole training set.",
                    "label": 0
                },
                {
                    "sent": "And there's a corollary that one can prove based on this relatively easy in the corollary now looks not at the deviation between mean and expectation, but the deviation between 2 means.",
                    "label": 0
                },
                {
                    "sent": "So certainly you can imagine if every mean is close to its expectation, then also every two independent means should be close to each other.",
                    "label": 0
                },
                {
                    "sent": "Maybe a little bit further apart and a little bit further apart is here reflected by this factor of two, which we are losing.",
                    "label": 0
                },
                {
                    "sent": "Here and we also losing here factor of two.",
                    "label": 0
                },
                {
                    "sent": "So basically if you hear plug in epsilon over two, you will end up with this thing here 'cause it's squared.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's remember that we can do that as well.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I've already told you, but just to remind you, if we translate this.",
                    "label": 0
                },
                {
                    "sent": "So there's this correspondence between the expectation of science and the Pyramid of Science and the risk actual and empirical risk.",
                    "label": 0
                },
                {
                    "sent": "And if we plug this back into this channel bond, then basically this tells us that the probability of a large deviation between test error or actual risk and training error goes to zero exponentially fast for every function F that we could plug into here.",
                    "label": 1
                },
                {
                    "sent": "But every individual function is not a uniform statement yet.",
                    "label": 0
                },
                {
                    "sent": "This is not truly uniformly over all functions, so it's true for one fixed function F. But unfortunately we are not allowed to look at the data.",
                    "label": 1
                },
                {
                    "sent": "Before choosing that function, if otherwise we could just say, well, we'll just choose the training error minimizer as our function F. And then this will give us bound how far the actual risk of the training error minimizer will be from the training error.",
                    "label": 0
                },
                {
                    "sent": "So why are we not allowed to do that?",
                    "label": 0
                },
                {
                    "sent": "Let's see if I have that on here.",
                    "label": 0
                },
                {
                    "sent": "I don't have the money, so any any guesses why this would be illegal?",
                    "label": 0
                },
                {
                    "sent": "Speculations.",
                    "label": 0
                },
                {
                    "sent": "So I'll tell you why this is illegal.",
                    "label": 0
                },
                {
                    "sent": "If we go back here.",
                    "label": 0
                },
                {
                    "sent": "I said before.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That these things here are independent penalty trials.",
                    "label": 0
                },
                {
                    "sent": "And that's important because.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If they were not, then we wouldn't have the channel button, so the general found uses this independence.",
                    "label": 0
                },
                {
                    "sent": "However, if we know use the training data, the whole set of training.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Points to choose a function.",
                    "label": 0
                },
                {
                    "sent": "So for instance to choose the function that has the minimal risk on the training set.",
                    "label": 0
                },
                {
                    "sent": "So the function that minimizes the training error.",
                    "label": 1
                },
                {
                    "sent": "Of course, this function looks at all the training points, so now these training points were independent.",
                    "label": 0
                },
                {
                    "sent": "But somehow this function has looked at all of them, so it has absorbed some knowledge from each of the training points.",
                    "label": 1
                },
                {
                    "sent": "And therefore, if we then plug that function in here.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll put it here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If not, we plug in a function here which has seen all training points X one through XN&Y, one Thruway, and then of course the size that we have constructed from that function are no longer independent.",
                    "label": 0
                },
                {
                    "sent": "Each of these size knows a little bit about the other size, they are no longer independent, so that's that's not wouldn't be a valid.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's it's more complicated.",
                    "label": 0
                },
                {
                    "sent": "We have to do the more complete.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dated a route and of course that's interesting because it will lead us to a lot of interesting structure.",
                    "label": 0
                },
                {
                    "sent": "And basically what it leads to.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "BBC, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so and.",
                    "label": 0
                },
                {
                    "sent": "What if applicant ever knickers have proven is the following, but they have proven a number of things.",
                    "label": 0
                },
                {
                    "sent": "But this is one of the central theorems of machine learning, and we're not going to prove this, but.",
                    "label": 0
                },
                {
                    "sent": "I'll show you some other things, but let me go through this briefly first so there are necessary and sufficient conditions for a certain type of consistency, so I mentioned before consistency, so just roughly consistency means in the limit of infinite data points.",
                    "label": 0
                },
                {
                    "sent": "Empirical empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "Will give us the the right result, by which I mean it will give us a risk which will be as close as.",
                    "label": 0
                },
                {
                    "sent": "Arbitrarily close to the lowest possible risk, it doesn't necessarily tell us.",
                    "label": 0
                },
                {
                    "sent": "We will find a function which is our truly close to the best possible function, because there might be multiple functions that have the minimal risk, but it tells us we find we will gradually get functions whose risks will get arbitrarily close to the best possible.",
                    "label": 0
                },
                {
                    "sent": "So that's the question of consistency, and it turns out one has to use a notion of nontrivial consistency, which I can't go into detail on.",
                    "label": 0
                },
                {
                    "sent": "But if you're into learning theory, then certainly you should study that.",
                    "label": 0
                },
                {
                    "sent": "And because.",
                    "label": 0
                },
                {
                    "sent": "This notion of non triviality is constructed when there is constructed in such a way that these conditions have become necessary and sufficient.",
                    "label": 0
                },
                {
                    "sent": "So example might argue one doesn't have to do it.",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                },
                {
                    "sent": "We could do it differently, which case this would not become necessary and sufficient, but that's just a side note.",
                    "label": 0
                },
                {
                    "sent": "So now for general purpose machine learning audience, see there's a notion of consistency such that the following is necessary and sufficient for uniform for uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "Sorry for consistency.",
                    "label": 0
                },
                {
                    "sent": "And the necessary and sufficient conditions for uniform for consistency, sorry, say again, the necessary and sufficient conditions for consistency of empirical risk, minimizations of training and neural network or whatever.",
                    "label": 1
                },
                {
                    "sent": "By minimizing the training error, is to have a certain type of uniform convergence and a certain type is the following.",
                    "label": 0
                },
                {
                    "sent": "It's a young 1 sided uniform convergence uniformly over all functions that can be implemented by a learning machine.",
                    "label": 1
                },
                {
                    "sent": "Which means.",
                    "label": 0
                },
                {
                    "sent": "This quantity here.",
                    "label": 0
                },
                {
                    "sent": "So the supremum over all functions that we can implement with the learning machine of this deviation.",
                    "label": 0
                },
                {
                    "sent": "Now without a modulus.",
                    "label": 0
                },
                {
                    "sent": "So that's why it's 1 sided.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The probability that this deviation in the One Direction is larger than epsilon will go to zero no matter how we choose epsilon.",
                    "label": 0
                },
                {
                    "sent": "So this now relates to the picture that I showed you before this graphical picture and the reason.",
                    "label": 0
                },
                {
                    "sent": "So this is basically uniform convergence, but it's slightly weaker.",
                    "label": 0
                },
                {
                    "sent": "It's only from one side.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is that we are only interested in consistency of empirical risk minimization if we were also interested in empirical maximization, then we would have.",
                    "label": 0
                },
                {
                    "sent": "2 sided uniform convergence.",
                    "label": 0
                },
                {
                    "sent": "But the point here is that this is a property that.",
                    "label": 0
                },
                {
                    "sent": "Principle you could.",
                    "label": 0
                },
                {
                    "sent": "It's a property of a learning machine.",
                    "label": 1
                },
                {
                    "sent": "In principle you could check whether you learning machine has this property.",
                    "label": 1
                },
                {
                    "sent": "It takes into account the whole set of functions that the learning machine can implement, because here we have a supremum.",
                    "label": 0
                },
                {
                    "sent": "And of course it's it's hard to check.",
                    "label": 0
                },
                {
                    "sent": "This is sort of an unwieldy condition and therefore it's natural to ask the question.",
                    "label": 0
                },
                {
                    "sent": "Are there other properties of learning machines that maybe imply this or maybe even that are equivalent to this?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that there are such properties imply this or that with additional.",
                    "label": 0
                },
                {
                    "sent": "Requirements so for instance, if we say that if we want this to be true for all underlying probability distributions, because here this risk, of course there's an expectation over an unknown probability distribution, so this with this is true depends, should be more specific about this.",
                    "label": 0
                },
                {
                    "sent": "It depends not only in the learning machine, but also on the problem on the unknown probability distribute distribution.",
                    "label": 0
                },
                {
                    "sent": "But if we wanted this to be true for all probability distributions, then maybe we would find quantities that only properties of the learning machines.",
                    "label": 0
                },
                {
                    "sent": "And that's in.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not true.",
                    "label": 0
                },
                {
                    "sent": "So let's take a closer look at this quantity that.",
                    "label": 1
                },
                {
                    "sent": "The last theorem was talking about.",
                    "label": 0
                },
                {
                    "sent": "So just to remind you.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This thing here, so we want to see how does this thing behave windows this probability go to 0, how faster?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go to program to zero and so on.",
                    "label": 0
                }
            ]
        }
    }
}