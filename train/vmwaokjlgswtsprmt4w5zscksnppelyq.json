{
    "id": "vmwaokjlgswtsprmt4w5zscksnppelyq",
    "title": "A Generative Dyadic Aspect Model for Evidence Accumulation Clustering",
    "info": {
        "author": [
            "M\u00e1rio A. T. Figueiredo, Instituto de Telecomunicacoes Lisboa"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/simbad2011_figueiredo_clustering/",
    "segmentation": [
        [
            "So hello everybody, thanks for being here on such a nice day in such a nice place.",
            "Probably much better than these.",
            "So this is this is a joint work I've been doing with unrelated, essentially Andreasen and Anna's work, and I've been collaborating recently.",
            "Actually, it was Anna who was supposed to give the talk.",
            "She's here in Venice, but she's not feeling very well, so I'm giving the talk.",
            "I bet she'll be here with us tomorrow."
        ],
        [
            "So just just a brief outline of the talk.",
            "So this is this is one of the recent efforts that have been made to give a probabilistic framework to this approach called clustering ensembles or evidence accumulation clustering, which is unafraid, and Daniel Jane developed a few years ago.",
            "In this case, this is based on a generative mixture model based on these dyadic aspect models and the resulting algorithm or the resulting approach goes by the name of Pinker.",
            "This is the acronym for these, which sounds funny because in Portuguese means a big nose.",
            "So we were very happy with the acronym if nothing else.",
            "Then I'll talk about some experimental results and conclusions."
        ],
        [
            "In future work so.",
            "For those of you who are not aware of these, so the notation is very simple, we have a set of objects which will indexes as fun to N and somehow we build and assemble of clustering.",
            "So each of these it's not one clustering, but and assemble of Sam.",
            "This should be a big am.",
            "Its resolution is not very good, so we have M clusterings where each of these clustering is a partition of the data, meaning that each clustering is a collection of subsets of the data which jointly contain all the data and but they are.",
            "This joint, so it's a partition, and the idea is that somehow there must be a way in which we can combine these clusterings so that we can obtain a better partitioning than any of these clusterings alone.",
            "OK, we at least with few assumptions, so in that sense, a more robust."
        ],
        [
            "Partitioning.",
            "So one of the methods to do this clustering ensemble techniques, is something that unafraid and Daniel Jane have been proposing since 2001.",
            "So about 10 years ago, and the idea is to.",
            "Using a symbol of clusterings to provide pairwise information.",
            "So essentially what they do is they.",
            "Run a set of clustering algorithms.",
            "It could be like this.",
            "Same algorithm with different parameterizations that could be even different algorithms.",
            "And then you have a set of set of clusterings and then maybe with different numbers of partitions in each one.",
            "And then from these you accumulate the evidence provided by all these clusterings in the form of a matrix C which is called the Co occurrence matrix, which essentially counts for each pair of objects, how many times they have a Kurd together in a cluster.",
            "So that's the information that's given by.",
            "These these collection of clusterings and then use these in some global using some final clustering technique.",
            "Using these we to extract some sort of what they called the consensus partition.",
            "Which is the consensus among all the individual partitions.",
            "In the ensemble."
        ],
        [
            "So the idea in this in this talk, which by we could be also sort of summarized in one slide, but then submits.",
            "Maybe some people are not familiar with dyadic models, so the idea is very simple.",
            "It's just to look at the results of the of the clustering Ensemble as a collection of dyadic data, and these years idea of dyadic data was introduced well.",
            "It's sort of a precursor of latent of probabilistic latent semantic analysis.",
            "And it's essentially saying that each datum is a tired.",
            "It's a pair of objects, and then so we can look at the Co occurrence matrix as a sample from some model that generates dyadic data."
        ],
        [
            "I'll be more specific in what follows, so.",
            "I'll do notice as soon as big, as a collection of pairs of objects while I am in ZMR objects, so they belong to the Cartesian product of the collection of objects with itself and a given pair belongs to the set of observed sequences.",
            "If they do not contain the same object.",
            "Of course, an object always curves with itself in any clustering, so there's no information in that fact, and so they have to be different objects, and they have to be.",
            "There has to be some clustering in the assemble.",
            "Where these two objects belong to the same cluster, and so this you, specially if this happens four times, then you have four of these pairs, so it's not the number of, but the fact that they haven't collection of pairs that describe the results of all your clustering ensembles.",
            "So you can look at the occurrence matrix, the one that counts how many times each pair of objects, Coker and sort of a collector is a summary of the information contained in S. OK, so just simply counts.",
            "The indicator function.",
            "The number of times that the indicator of that given pair was equal to YZ, so that counts how many times a pair while the occurs in the clustering ensemble.",
            "So we're not going to use occurrence matrix directly.",
            "You're going to start one step back and started these and use this collection of all pairs of objects that Co occur in the same and then the end will come back to these Co occurrence matrix but slightly different version of it."
        ],
        [
            "So.",
            "So the idea here is that you have the apophysis that we do, is that the underlying clusters are revealed by the observations in this sequence of pairs and so for that we adopt generative model, which is that kind of dyadic aspect model of Hoffman at all.",
            "And essentially it's very simple.",
            "It just says that these these pairs that is seen srid samples of a pair of random variables.",
            "OK, there's a latent variable with very similar to what I didn't talk about it a little while ago.",
            "Just the difference is that it's not documents an words, but.",
            "Collection of objects and collection of objects.",
            "So it's cooccurrence from the same set and then the pairs of objects are iid themselves.",
            "OK, they are independent and follow the same distribution conditioned on this latent variable, meaning that the problem joint probability of having a pair of objects Co occurring given the latent variable R is just a product of these two variables.",
            "So they are independent and they have the same distribution.",
            "So the probability that you observe a given object.",
            "In the first or in the second position of the pair, given the latent variable is the same.",
            "So of course from this weekend, since this is, there's a latent variable are we know the conditional distribution.",
            "So we simply to obtain the joint probability of a given pair of objects and simply have to sum over our marginalizing over familiar form of a finite mixture."
        ],
        [
            "With all terms.",
            "And of course, this is parameterized by 2, two or a collection of distributions.",
            "Not too, but are plus one.",
            "So we have the probability that the random the latent variable takes each of its possible values one 2L, and then you have.",
            "The distribution of objects given each.",
            "It's possible value of the latent variable R."
        ],
        [
            "OK, and so we summarized we write these compactly as a vector of distribution for the for the latent variable, which we call P. And the matrix be indexed by the latent variable and by the object index which simply collects.",
            "What is the probability that you observe of the J given that latent variable text value R and is collected in a matrix B, which is of course a stochastic matrix?",
            "Because when you sum across rows along Detroit tester something so in this notation everything becomes shorter.",
            "For example, the joint probability using Bayes law is simply the product of two of these elements of these matrix times, PR joint probability can marginalized.",
            "Over R so you can do now.",
            "You can compute everything, anything."
        ],
        [
            "Wanted these parameters OK so.",
            "If we now assume that our collection asks of pairs contains IID samples of these random variable, then we simply write the joint distribution conditional on the parameters of the product over all the elements in the collection.",
            "Of the probability of each pair, which is just a mixture model that we just introduced.",
            "Of course, if we knew.",
            "If we knew the value that the latent variable had for each pair, then we could write the so-called complete look like complete likelihood.",
            "And it's easy to see that if you take the log, these become.",
            "These all become sums.",
            "And you have this familiar form.",
            "This is a typical treatment in the mixtures, which is to write everything in terms of binary indicator variables, which shows that the log likelihood is linear with respect to what you don't know which is.",
            "The indicator variable of each of the latent variables.",
            "OK, so of course this is begging for EM because there's missing data and everything is linear with respect to the missing data and all the models have."
        ],
        [
            "Simply everything is multinomial, so this is a typical.",
            "This is, well, this is not new courses in Hoffman's paper in this stuff.",
            "So the algorithm, as you know, simply finds the maximum marginal likelihood given all the observed data.",
            "There's two well known steps you just have to compute expectation with respect to these latent variable to get this Q function and then update."
        ],
        [
            "Parameter estimates by maximizing this, so what's interesting now is that everything is closed form.",
            "This is all well known, so the result of these step, which is computing the Q function, is essentially contained in a matrix R at Mr, which has which expresses what is the conditional probability that a given pair was generated by a given cluster, and these all has closed form solution.",
            "This is all these are simply all of them.",
            "Weighted versions of parameter estimation for multinomial."
        ],
        [
            "But finally, so we can update the distribution of all the all the clusters of all the all the latent variable values and we can update the matrix B which tells you what is the probability that you observe each document.",
            "Each object inside each cluster.",
            "Now these estimate of B of these matrix that is essential in generative model.",
            "Interestingly, depends on a on a matrix on a set of matrices C at.",
            "Which have this form.",
            "So if you recall the the Association matrix was exactly at this form.",
            "Without these are term here, so it was just counting how many times each pair of objects had occurred.",
            "Now what we have here is a weighted version of the Association Matrix, where each pair is not weighted equally, but they are.",
            "Each pair is weighted according to how likely it is that it belongs to a given cluster, let's say.",
            "And so instead of 1 Co sociation matrix, you have L consociation matrices, one for each.",
            "Very possible value of the latent variable.",
            "So it was important to start not from the consociation matrix, but from the full collection of pairs, because now we see that if we want to implement these EM algorithm, what we need is not the consociation matrix.",
            "But the set of weighted versions of the consociation matrix, so this is a new kind, of course."
        ],
        [
            "Ciation matrix.",
            "Of course, all the once we the algorithm ends, we have these estimates of these parameters which are very obvious meanings.",
            "So of course P1 2PL head are simply the cluster probabilities and the bee hats are sort of the degrees of ownership of a given object by given cluster.",
            "Notice that this is this is some sums to one over variable, why not over variable R?",
            "So this is not the probability that.",
            "A given object belongs to a given cluster.",
            "It's probability that a given object was produced by a given class, which is different, so but if you want it the other way around, if you want the probability that a given object was produced by a given cluster, you can still have it by simply by invoking both law.",
            "Disjoint is trivial, you simply normalize it, and you have the probability that a given object.",
            "The probability that given a given object it was produced by cluster R."
        ],
        [
            "So everything is easy.",
            "Oh, so no.",
            "Andre has evaluated these on several benchmark datasets, not only two dimensional, but these for some reason decided to show the dimensional ones here.",
            "These are familiar, and in this case all the clustering assembles are run by running K means with different numbers of clusters and different numbers of initializations.",
            "So it's could be done in another way but that."
        ],
        [
            "Authorities so just to give an idea of what kind of results we have we have.",
            "This is for the well known Iris data set where we know that there's one very well separated cluster and two not so well separated clusters.",
            "So what the output for this is?",
            "Each of these plots, which would be 1 red, 1 blue and green, is one of the lines each of the three lines of matrix B which shares what is the probability that each of the.",
            "Points in the data set belongs to each of the clusters.",
            "So, for example, points 1 to 50 clearly belong to cluster to the red cluster, which I think is number 3 and the other is not so clear."
        ],
        [
            "OK. No, the baseline which we were trying to beat was another type of mixture.",
            "Model for clustering assembles which is based on a different kind of assumption.",
            "And accepting 2 cases, all the accuracies.",
            "This is of course for all unknown.",
            "Without using class labels.",
            "It's just clustering.",
            "So in all the cases.",
            "These these model beat the other one which is called mixture model.",
            "We just call.",
            "It doesn't have a name, so just calling it mixture model.",
            "It's not a mixture model, it's a different type of mixture model for representing clustering ensembles.",
            "And so these are of course, preliminary results in this still."
        ],
        [
            "Work going on and so let me conclude by summarizing and saying that what I've talked about was a probabilistic generative model for consensus clustering.",
            "Which is based on a dyadic aspect model of the evidence accumulation clustering.",
            "Framework, it's sort of.",
            "The consensus partition is distracted by solving the maximum likelihood estimation problem.",
            "If I am an importantly show that you cannot only rely on the on the Co sociation matrix, you need weighted versions of the consociation matrix when you're doing.",
            "When you're running EM, the method yields probabilistic assignments for each sample to each cluster, which was not true of previous methods that used assemble clustering, and these initial experiments are promising.",
            "And of course, this is very important since this is a probabilistic generative model.",
            "This opens the door to dealing with model selection problems with model selection question, which is how many clusters in this case, all these experiments were run.",
            "Assuming that you know how many clusters are in the data, which which is equal to the number of classes, so of course we can use that.",
            "We can open the door to much more.",
            "Well, we can start with crude stuff, or we can go into nonparametric approaches to."
        ],
        [
            "All these models, and of course this has to be.",
            "Said we have to acknowledge this kind of support.",
            "Thank you.",
            "So if I understand correctly, you basically have one data set and then you have a lot of class tonight Mrs which gives you a lot of platforms and then you classify classmates, cluster clustering you cluster based on the results of the clustering.",
            "OK, so you close to the object.",
            "So basis for for a Courier from different idols operating on the same data set to learn something about this I didn't understand.",
            "I didn't understand.",
            "Can you repeat?",
            "Places where you have one data set and you let this different island, or just in terms of the data set, so that you can solve a variety of different classrooms.",
            "That then the assignment of objects to particular classrooms based on these different assignments is more trustfull than than looking at the original data.",
            "It sounds like psychology or viruses.",
            "So it's crowdsourcing.",
            "It's crowdsourcing for clustering.",
            "I would say it's sort of crowdsourcing for clustering.",
            "I'm not that it was not me who proposed summer clustering.",
            "OK, so I don't feel.",
            "Like I'm the best person to defend the philosophy behind it, but I think it's a reasonable.",
            "It's a reasonable idea if you have.",
            "If you have a multitude of clusterings.",
            "Then you're looking, and if you believe there's an underlying cluster structure, then essentially you're looking at different projections of that underlying structure.",
            "CS seen by different clustering algorithms, and if you can leverage that multitude of use, maybe you can extract more reliable clustering.",
            "So is there a guarantee that find all these items over fit that in the Enfield consensus will be better than the overfitting is usually not a problem because it respects Co occurrences.",
            "If you have if you if you believe that there's a real underlying set of clusters, and if you use intermediate algorithm which all use more than that, still the objects that in the real clustering belong to the same cluster will Co occur much more frequently than those that do not.",
            "So of course there's an issue of choosing the number of clusters for the final clustering.",
            "But the problem of choosing the number of clusters for the intermediate clusterings it's much alleviated.",
            "Actually not an issue as long as it's not.",
            "Very very far away.",
            "You can do it with 23456789 for this case, for example you.",
            "For say, if I remember.",
            "If your number of clusters are but use around, say, 10.",
            "The clustering Cindy assemble have numbers from 3 or 4 up to 20 or 30 and use everything to build the final consensus."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hello everybody, thanks for being here on such a nice day in such a nice place.",
                    "label": 0
                },
                {
                    "sent": "Probably much better than these.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a joint work I've been doing with unrelated, essentially Andreasen and Anna's work, and I've been collaborating recently.",
                    "label": 0
                },
                {
                    "sent": "Actually, it was Anna who was supposed to give the talk.",
                    "label": 0
                },
                {
                    "sent": "She's here in Venice, but she's not feeling very well, so I'm giving the talk.",
                    "label": 0
                },
                {
                    "sent": "I bet she'll be here with us tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just just a brief outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "So this is this is one of the recent efforts that have been made to give a probabilistic framework to this approach called clustering ensembles or evidence accumulation clustering, which is unafraid, and Daniel Jane developed a few years ago.",
                    "label": 1
                },
                {
                    "sent": "In this case, this is based on a generative mixture model based on these dyadic aspect models and the resulting algorithm or the resulting approach goes by the name of Pinker.",
                    "label": 1
                },
                {
                    "sent": "This is the acronym for these, which sounds funny because in Portuguese means a big nose.",
                    "label": 0
                },
                {
                    "sent": "So we were very happy with the acronym if nothing else.",
                    "label": 1
                },
                {
                    "sent": "Then I'll talk about some experimental results and conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In future work so.",
                    "label": 0
                },
                {
                    "sent": "For those of you who are not aware of these, so the notation is very simple, we have a set of objects which will indexes as fun to N and somehow we build and assemble of clustering.",
                    "label": 0
                },
                {
                    "sent": "So each of these it's not one clustering, but and assemble of Sam.",
                    "label": 0
                },
                {
                    "sent": "This should be a big am.",
                    "label": 0
                },
                {
                    "sent": "Its resolution is not very good, so we have M clusterings where each of these clustering is a partition of the data, meaning that each clustering is a collection of subsets of the data which jointly contain all the data and but they are.",
                    "label": 0
                },
                {
                    "sent": "This joint, so it's a partition, and the idea is that somehow there must be a way in which we can combine these clusterings so that we can obtain a better partitioning than any of these clusterings alone.",
                    "label": 0
                },
                {
                    "sent": "OK, we at least with few assumptions, so in that sense, a more robust.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Partitioning.",
                    "label": 0
                },
                {
                    "sent": "So one of the methods to do this clustering ensemble techniques, is something that unafraid and Daniel Jane have been proposing since 2001.",
                    "label": 1
                },
                {
                    "sent": "So about 10 years ago, and the idea is to.",
                    "label": 0
                },
                {
                    "sent": "Using a symbol of clusterings to provide pairwise information.",
                    "label": 0
                },
                {
                    "sent": "So essentially what they do is they.",
                    "label": 0
                },
                {
                    "sent": "Run a set of clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "It could be like this.",
                    "label": 0
                },
                {
                    "sent": "Same algorithm with different parameterizations that could be even different algorithms.",
                    "label": 0
                },
                {
                    "sent": "And then you have a set of set of clusterings and then maybe with different numbers of partitions in each one.",
                    "label": 0
                },
                {
                    "sent": "And then from these you accumulate the evidence provided by all these clusterings in the form of a matrix C which is called the Co occurrence matrix, which essentially counts for each pair of objects, how many times they have a Kurd together in a cluster.",
                    "label": 0
                },
                {
                    "sent": "So that's the information that's given by.",
                    "label": 0
                },
                {
                    "sent": "These these collection of clusterings and then use these in some global using some final clustering technique.",
                    "label": 1
                },
                {
                    "sent": "Using these we to extract some sort of what they called the consensus partition.",
                    "label": 0
                },
                {
                    "sent": "Which is the consensus among all the individual partitions.",
                    "label": 0
                },
                {
                    "sent": "In the ensemble.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea in this in this talk, which by we could be also sort of summarized in one slide, but then submits.",
                    "label": 0
                },
                {
                    "sent": "Maybe some people are not familiar with dyadic models, so the idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "It's just to look at the results of the of the clustering Ensemble as a collection of dyadic data, and these years idea of dyadic data was introduced well.",
                    "label": 1
                },
                {
                    "sent": "It's sort of a precursor of latent of probabilistic latent semantic analysis.",
                    "label": 0
                },
                {
                    "sent": "And it's essentially saying that each datum is a tired.",
                    "label": 1
                },
                {
                    "sent": "It's a pair of objects, and then so we can look at the Co occurrence matrix as a sample from some model that generates dyadic data.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll be more specific in what follows, so.",
                    "label": 0
                },
                {
                    "sent": "I'll do notice as soon as big, as a collection of pairs of objects while I am in ZMR objects, so they belong to the Cartesian product of the collection of objects with itself and a given pair belongs to the set of observed sequences.",
                    "label": 0
                },
                {
                    "sent": "If they do not contain the same object.",
                    "label": 0
                },
                {
                    "sent": "Of course, an object always curves with itself in any clustering, so there's no information in that fact, and so they have to be different objects, and they have to be.",
                    "label": 0
                },
                {
                    "sent": "There has to be some clustering in the assemble.",
                    "label": 0
                },
                {
                    "sent": "Where these two objects belong to the same cluster, and so this you, specially if this happens four times, then you have four of these pairs, so it's not the number of, but the fact that they haven't collection of pairs that describe the results of all your clustering ensembles.",
                    "label": 0
                },
                {
                    "sent": "So you can look at the occurrence matrix, the one that counts how many times each pair of objects, Coker and sort of a collector is a summary of the information contained in S. OK, so just simply counts.",
                    "label": 1
                },
                {
                    "sent": "The indicator function.",
                    "label": 0
                },
                {
                    "sent": "The number of times that the indicator of that given pair was equal to YZ, so that counts how many times a pair while the occurs in the clustering ensemble.",
                    "label": 1
                },
                {
                    "sent": "So we're not going to use occurrence matrix directly.",
                    "label": 0
                },
                {
                    "sent": "You're going to start one step back and started these and use this collection of all pairs of objects that Co occur in the same and then the end will come back to these Co occurrence matrix but slightly different version of it.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that you have the apophysis that we do, is that the underlying clusters are revealed by the observations in this sequence of pairs and so for that we adopt generative model, which is that kind of dyadic aspect model of Hoffman at all.",
                    "label": 1
                },
                {
                    "sent": "And essentially it's very simple.",
                    "label": 0
                },
                {
                    "sent": "It just says that these these pairs that is seen srid samples of a pair of random variables.",
                    "label": 1
                },
                {
                    "sent": "OK, there's a latent variable with very similar to what I didn't talk about it a little while ago.",
                    "label": 0
                },
                {
                    "sent": "Just the difference is that it's not documents an words, but.",
                    "label": 0
                },
                {
                    "sent": "Collection of objects and collection of objects.",
                    "label": 0
                },
                {
                    "sent": "So it's cooccurrence from the same set and then the pairs of objects are iid themselves.",
                    "label": 0
                },
                {
                    "sent": "OK, they are independent and follow the same distribution conditioned on this latent variable, meaning that the problem joint probability of having a pair of objects Co occurring given the latent variable R is just a product of these two variables.",
                    "label": 0
                },
                {
                    "sent": "So they are independent and they have the same distribution.",
                    "label": 0
                },
                {
                    "sent": "So the probability that you observe a given object.",
                    "label": 0
                },
                {
                    "sent": "In the first or in the second position of the pair, given the latent variable is the same.",
                    "label": 0
                },
                {
                    "sent": "So of course from this weekend, since this is, there's a latent variable are we know the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "So we simply to obtain the joint probability of a given pair of objects and simply have to sum over our marginalizing over familiar form of a finite mixture.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With all terms.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is parameterized by 2, two or a collection of distributions.",
                    "label": 1
                },
                {
                    "sent": "Not too, but are plus one.",
                    "label": 0
                },
                {
                    "sent": "So we have the probability that the random the latent variable takes each of its possible values one 2L, and then you have.",
                    "label": 0
                },
                {
                    "sent": "The distribution of objects given each.",
                    "label": 1
                },
                {
                    "sent": "It's possible value of the latent variable R.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so we summarized we write these compactly as a vector of distribution for the for the latent variable, which we call P. And the matrix be indexed by the latent variable and by the object index which simply collects.",
                    "label": 1
                },
                {
                    "sent": "What is the probability that you observe of the J given that latent variable text value R and is collected in a matrix B, which is of course a stochastic matrix?",
                    "label": 1
                },
                {
                    "sent": "Because when you sum across rows along Detroit tester something so in this notation everything becomes shorter.",
                    "label": 0
                },
                {
                    "sent": "For example, the joint probability using Bayes law is simply the product of two of these elements of these matrix times, PR joint probability can marginalized.",
                    "label": 0
                },
                {
                    "sent": "Over R so you can do now.",
                    "label": 0
                },
                {
                    "sent": "You can compute everything, anything.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wanted these parameters OK so.",
                    "label": 0
                },
                {
                    "sent": "If we now assume that our collection asks of pairs contains IID samples of these random variable, then we simply write the joint distribution conditional on the parameters of the product over all the elements in the collection.",
                    "label": 0
                },
                {
                    "sent": "Of the probability of each pair, which is just a mixture model that we just introduced.",
                    "label": 0
                },
                {
                    "sent": "Of course, if we knew.",
                    "label": 0
                },
                {
                    "sent": "If we knew the value that the latent variable had for each pair, then we could write the so-called complete look like complete likelihood.",
                    "label": 0
                },
                {
                    "sent": "And it's easy to see that if you take the log, these become.",
                    "label": 0
                },
                {
                    "sent": "These all become sums.",
                    "label": 0
                },
                {
                    "sent": "And you have this familiar form.",
                    "label": 0
                },
                {
                    "sent": "This is a typical treatment in the mixtures, which is to write everything in terms of binary indicator variables, which shows that the log likelihood is linear with respect to what you don't know which is.",
                    "label": 0
                },
                {
                    "sent": "The indicator variable of each of the latent variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course this is begging for EM because there's missing data and everything is linear with respect to the missing data and all the models have.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simply everything is multinomial, so this is a typical.",
                    "label": 0
                },
                {
                    "sent": "This is, well, this is not new courses in Hoffman's paper in this stuff.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm, as you know, simply finds the maximum marginal likelihood given all the observed data.",
                    "label": 1
                },
                {
                    "sent": "There's two well known steps you just have to compute expectation with respect to these latent variable to get this Q function and then update.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parameter estimates by maximizing this, so what's interesting now is that everything is closed form.",
                    "label": 0
                },
                {
                    "sent": "This is all well known, so the result of these step, which is computing the Q function, is essentially contained in a matrix R at Mr, which has which expresses what is the conditional probability that a given pair was generated by a given cluster, and these all has closed form solution.",
                    "label": 1
                },
                {
                    "sent": "This is all these are simply all of them.",
                    "label": 0
                },
                {
                    "sent": "Weighted versions of parameter estimation for multinomial.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But finally, so we can update the distribution of all the all the clusters of all the all the latent variable values and we can update the matrix B which tells you what is the probability that you observe each document.",
                    "label": 0
                },
                {
                    "sent": "Each object inside each cluster.",
                    "label": 0
                },
                {
                    "sent": "Now these estimate of B of these matrix that is essential in generative model.",
                    "label": 1
                },
                {
                    "sent": "Interestingly, depends on a on a matrix on a set of matrices C at.",
                    "label": 0
                },
                {
                    "sent": "Which have this form.",
                    "label": 0
                },
                {
                    "sent": "So if you recall the the Association matrix was exactly at this form.",
                    "label": 0
                },
                {
                    "sent": "Without these are term here, so it was just counting how many times each pair of objects had occurred.",
                    "label": 0
                },
                {
                    "sent": "Now what we have here is a weighted version of the Association Matrix, where each pair is not weighted equally, but they are.",
                    "label": 1
                },
                {
                    "sent": "Each pair is weighted according to how likely it is that it belongs to a given cluster, let's say.",
                    "label": 0
                },
                {
                    "sent": "And so instead of 1 Co sociation matrix, you have L consociation matrices, one for each.",
                    "label": 0
                },
                {
                    "sent": "Very possible value of the latent variable.",
                    "label": 0
                },
                {
                    "sent": "So it was important to start not from the consociation matrix, but from the full collection of pairs, because now we see that if we want to implement these EM algorithm, what we need is not the consociation matrix.",
                    "label": 0
                },
                {
                    "sent": "But the set of weighted versions of the consociation matrix, so this is a new kind, of course.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ciation matrix.",
                    "label": 0
                },
                {
                    "sent": "Of course, all the once we the algorithm ends, we have these estimates of these parameters which are very obvious meanings.",
                    "label": 0
                },
                {
                    "sent": "So of course P1 2PL head are simply the cluster probabilities and the bee hats are sort of the degrees of ownership of a given object by given cluster.",
                    "label": 1
                },
                {
                    "sent": "Notice that this is this is some sums to one over variable, why not over variable R?",
                    "label": 0
                },
                {
                    "sent": "So this is not the probability that.",
                    "label": 0
                },
                {
                    "sent": "A given object belongs to a given cluster.",
                    "label": 0
                },
                {
                    "sent": "It's probability that a given object was produced by a given class, which is different, so but if you want it the other way around, if you want the probability that a given object was produced by a given cluster, you can still have it by simply by invoking both law.",
                    "label": 0
                },
                {
                    "sent": "Disjoint is trivial, you simply normalize it, and you have the probability that a given object.",
                    "label": 1
                },
                {
                    "sent": "The probability that given a given object it was produced by cluster R.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So everything is easy.",
                    "label": 0
                },
                {
                    "sent": "Oh, so no.",
                    "label": 0
                },
                {
                    "sent": "Andre has evaluated these on several benchmark datasets, not only two dimensional, but these for some reason decided to show the dimensional ones here.",
                    "label": 1
                },
                {
                    "sent": "These are familiar, and in this case all the clustering assembles are run by running K means with different numbers of clusters and different numbers of initializations.",
                    "label": 1
                },
                {
                    "sent": "So it's could be done in another way but that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Authorities so just to give an idea of what kind of results we have we have.",
                    "label": 0
                },
                {
                    "sent": "This is for the well known Iris data set where we know that there's one very well separated cluster and two not so well separated clusters.",
                    "label": 0
                },
                {
                    "sent": "So what the output for this is?",
                    "label": 0
                },
                {
                    "sent": "Each of these plots, which would be 1 red, 1 blue and green, is one of the lines each of the three lines of matrix B which shares what is the probability that each of the.",
                    "label": 0
                },
                {
                    "sent": "Points in the data set belongs to each of the clusters.",
                    "label": 0
                },
                {
                    "sent": "So, for example, points 1 to 50 clearly belong to cluster to the red cluster, which I think is number 3 and the other is not so clear.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. No, the baseline which we were trying to beat was another type of mixture.",
                    "label": 0
                },
                {
                    "sent": "Model for clustering assembles which is based on a different kind of assumption.",
                    "label": 0
                },
                {
                    "sent": "And accepting 2 cases, all the accuracies.",
                    "label": 0
                },
                {
                    "sent": "This is of course for all unknown.",
                    "label": 0
                },
                {
                    "sent": "Without using class labels.",
                    "label": 0
                },
                {
                    "sent": "It's just clustering.",
                    "label": 0
                },
                {
                    "sent": "So in all the cases.",
                    "label": 0
                },
                {
                    "sent": "These these model beat the other one which is called mixture model.",
                    "label": 0
                },
                {
                    "sent": "We just call.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have a name, so just calling it mixture model.",
                    "label": 0
                },
                {
                    "sent": "It's not a mixture model, it's a different type of mixture model for representing clustering ensembles.",
                    "label": 0
                },
                {
                    "sent": "And so these are of course, preliminary results in this still.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work going on and so let me conclude by summarizing and saying that what I've talked about was a probabilistic generative model for consensus clustering.",
                    "label": 1
                },
                {
                    "sent": "Which is based on a dyadic aspect model of the evidence accumulation clustering.",
                    "label": 1
                },
                {
                    "sent": "Framework, it's sort of.",
                    "label": 1
                },
                {
                    "sent": "The consensus partition is distracted by solving the maximum likelihood estimation problem.",
                    "label": 1
                },
                {
                    "sent": "If I am an importantly show that you cannot only rely on the on the Co sociation matrix, you need weighted versions of the consociation matrix when you're doing.",
                    "label": 0
                },
                {
                    "sent": "When you're running EM, the method yields probabilistic assignments for each sample to each cluster, which was not true of previous methods that used assemble clustering, and these initial experiments are promising.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is very important since this is a probabilistic generative model.",
                    "label": 0
                },
                {
                    "sent": "This opens the door to dealing with model selection problems with model selection question, which is how many clusters in this case, all these experiments were run.",
                    "label": 0
                },
                {
                    "sent": "Assuming that you know how many clusters are in the data, which which is equal to the number of classes, so of course we can use that.",
                    "label": 0
                },
                {
                    "sent": "We can open the door to much more.",
                    "label": 0
                },
                {
                    "sent": "Well, we can start with crude stuff, or we can go into nonparametric approaches to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these models, and of course this has to be.",
                    "label": 0
                },
                {
                    "sent": "Said we have to acknowledge this kind of support.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So if I understand correctly, you basically have one data set and then you have a lot of class tonight Mrs which gives you a lot of platforms and then you classify classmates, cluster clustering you cluster based on the results of the clustering.",
                    "label": 0
                },
                {
                    "sent": "OK, so you close to the object.",
                    "label": 0
                },
                {
                    "sent": "So basis for for a Courier from different idols operating on the same data set to learn something about this I didn't understand.",
                    "label": 0
                },
                {
                    "sent": "I didn't understand.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat?",
                    "label": 0
                },
                {
                    "sent": "Places where you have one data set and you let this different island, or just in terms of the data set, so that you can solve a variety of different classrooms.",
                    "label": 0
                },
                {
                    "sent": "That then the assignment of objects to particular classrooms based on these different assignments is more trustfull than than looking at the original data.",
                    "label": 0
                },
                {
                    "sent": "It sounds like psychology or viruses.",
                    "label": 0
                },
                {
                    "sent": "So it's crowdsourcing.",
                    "label": 0
                },
                {
                    "sent": "It's crowdsourcing for clustering.",
                    "label": 0
                },
                {
                    "sent": "I would say it's sort of crowdsourcing for clustering.",
                    "label": 0
                },
                {
                    "sent": "I'm not that it was not me who proposed summer clustering.",
                    "label": 0
                },
                {
                    "sent": "OK, so I don't feel.",
                    "label": 0
                },
                {
                    "sent": "Like I'm the best person to defend the philosophy behind it, but I think it's a reasonable.",
                    "label": 0
                },
                {
                    "sent": "It's a reasonable idea if you have.",
                    "label": 0
                },
                {
                    "sent": "If you have a multitude of clusterings.",
                    "label": 0
                },
                {
                    "sent": "Then you're looking, and if you believe there's an underlying cluster structure, then essentially you're looking at different projections of that underlying structure.",
                    "label": 0
                },
                {
                    "sent": "CS seen by different clustering algorithms, and if you can leverage that multitude of use, maybe you can extract more reliable clustering.",
                    "label": 0
                },
                {
                    "sent": "So is there a guarantee that find all these items over fit that in the Enfield consensus will be better than the overfitting is usually not a problem because it respects Co occurrences.",
                    "label": 0
                },
                {
                    "sent": "If you have if you if you believe that there's a real underlying set of clusters, and if you use intermediate algorithm which all use more than that, still the objects that in the real clustering belong to the same cluster will Co occur much more frequently than those that do not.",
                    "label": 0
                },
                {
                    "sent": "So of course there's an issue of choosing the number of clusters for the final clustering.",
                    "label": 0
                },
                {
                    "sent": "But the problem of choosing the number of clusters for the intermediate clusterings it's much alleviated.",
                    "label": 0
                },
                {
                    "sent": "Actually not an issue as long as it's not.",
                    "label": 0
                },
                {
                    "sent": "Very very far away.",
                    "label": 0
                },
                {
                    "sent": "You can do it with 23456789 for this case, for example you.",
                    "label": 0
                },
                {
                    "sent": "For say, if I remember.",
                    "label": 0
                },
                {
                    "sent": "If your number of clusters are but use around, say, 10.",
                    "label": 0
                },
                {
                    "sent": "The clustering Cindy assemble have numbers from 3 or 4 up to 20 or 30 and use everything to build the final consensus.",
                    "label": 0
                }
            ]
        }
    }
}