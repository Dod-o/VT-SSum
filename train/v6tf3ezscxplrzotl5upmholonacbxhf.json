{
    "id": "v6tf3ezscxplrzotl5upmholonacbxhf",
    "title": "The First-Order View of Boosting Methods: Computational Complexity and Connections to Regularization",
    "info": {
        "author": [
            "Paul Grigas, Industrial Engineering and Operations Research Department, UC Berkeley"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_grigas_lasso/",
    "segmentation": [
        [
            "Also follow the trend of changing my title a little bit from the original title.",
            "And the reason for that is an increase in scope of it.",
            "Hopefully I won't regret that, but yeah, so this is joint work with my advisor Rob Freund an also Rahul Mazumder postdoc at MIT, and so the main theme of this work is sort of a new perspective on boosting methods as mirror descent methods in convex optimization, and the implications of this perspective, which will hopefully be useful to at least some people here.",
            "So first sort of some motivation.",
            "Review of it for."
        ],
        [
            "Probably everyone.",
            "What are boosting methods right?",
            "So boosting methods are iterative methods that combine weak models or weak learners where we can model is just something that one can say you know, perform slightly better than random guessing.",
            "In these things typically work by adding one new week model per iteration.",
            "And the weight on each week model is typically pretty small.",
            "And so here I'll consider two different settings, right?",
            "The first setting is binary classification and the second setting is regularised or sparse linear regression.",
            "So these are things we've seen already several times during this workshop.",
            "And then I just want to remark that boosting methods are often tuned in terms of the number of iterations and the stepsize so as to perform some sort of implicit regularization, right?",
            "But in addition to implicit regularizer."
        ],
        [
            "And there is also the notion of an explicitly defined regularised problem, right?",
            "Minimize some loss function, plus a regularizer or subject to some regularization constraints.",
            "So given that boosting methods are performing implicit regularization, we ask here are boosting methods, solving any optimization problems right?",
            "And if So what are the computational guarantees we can derive through such an interpretation and then also, can we somehow adapt boosting methods?",
            "To actually solve explicitly defined regularised problems.",
            "So to start."
        ],
        [
            "Just outline all of our results here.",
            "So first thing is, we show that Adaboost well known algorithm is actually an instance of the mirror descent method to minimize the edge and also through the duality of mirror descent, maximize the margin.",
            "And we also showed that the incremental forward stagewise regression algorithm, an algorithm for solving lasso, is an instance of the classical subgradient descent method.",
            "To minimize the maximum correlation between the residuals in the predictors.",
            "And then through both of these interpretations, we derive some hopefully useful computational complexity guarantees.",
            "And then we also consider the Frank Wolfe method applied to the two regularised problems, right?",
            "Minimizing the log exponential, Lars and Lasso, and we show that these algorithms are very similar to the known boosting algorithms, Adaboost and forward stagewise an we also get additional complexity guarantees for these methods.",
            "So, um."
        ],
        [
            "Just first a quick review of the Mirror descent method.",
            "Very quick review, right?",
            "So this is a method for solving a convex optimization problem, right?",
            "Want to minimize F of X, where F is a potentially non smooth but Lipschitz continuous function and P is a convex and close that.",
            "And throughout will assume that F arises from a min Max structure, right?",
            "So this means that we have some some function fee which is convex in the first argument in cave, in the second argument, and actually our function F is defined as the solution to an optimization problem.",
            "So we saw this in the last lecture under this setting, right?",
            "There's a theorem that says that computing a subgradient of F is."
        ],
        [
            "Actually just the same thing as solving that optimization problem and then computing a gradient at the optimal solution.",
            "So all we need to do to pick out a subgradient is find the optimal solution for a fixed X and then evaluated gradient.",
            "And then actually, when PR our domain is bounded, we can also consider a dual problem, right?",
            "Which is to maximize the function P of."
        ],
        [
            "Lambda, which is defined by switching the Max and min right.",
            "And so in this primal dual setting, right?",
            "Or in general, Mirror descent uses A1 strongly convex prox function.",
            "So Prox function is just one strongly convex function which needs to be chosen such that we can solve this type of projection problem at every iteration.",
            "And I associated with this process function is a Bregman distance which is sort of a replacement for the distance measured in whichever norm.",
            "We're strongly convex with."
        ],
        [
            "So how does mirror descent work right?",
            "We so we start at an initial point X, not Lambda, not which is zero at each iteration, right?",
            "We solve our subproblem which computes the optimal solution for fixed XC.",
            "We find the gradient at the optimal solution.",
            "We pick a step size right and update our primal point according to this projection and then we also update the dual point by taking a convex combination of all these previous solutions weighted by these alphas, right?",
            "So these alphas are the step lengths and so I just sort of note here that actually you know the assignment of Lambda K plus one is just kind of this after the fact thing that really plays no role in the dynamics of the method right?",
            "Because the method is just defined in terms of these primal variables X."
        ],
        [
            "OK, so 2 examples that I'll use.",
            "The only examples I use during this talk right are the classical subgradient descent method, right?",
            "So P is RN and D is now 1/2 Euclidean norm of X squared, right?",
            "So then this proximal projection update just becomes the subgradient descent step.",
            "The other example used in the case of Adaboost is the multiplicative weight updates.",
            "So in this setting right P our domain is the simplex unit simplex are prox function.",
            "Is the entropy function which is known to be one strongly convex with respect to the one norm and now the step to the proxamol update is just this multiplicative weighting update right where we multiply by this exponential?",
            "So what are the computational guarantees?"
        ],
        [
            "The complexity of mirror descent, so you know this.",
            "These results have been known for a while due to many different people.",
            "And so in the general setting, right P can potentially be unbounded.",
            "Here we know that at least one of our iterates, minus the optimal solution must be less than this quantity, right?",
            "Which can be conveniently bounded for specific choices of the Alpha sequence, right?",
            "But in addition, if P is compact, right?",
            "And then we have some sort of strong duality, and we let the Barbie some upper bound on this Bregman distance, then actually we get this even stronger statement, which says that this.",
            "The duality gap right for some X -- P at the last data rate Lambda K plus one is bounded by this quantity.",
            "So OK."
        ],
        [
            "That was a review of the Mirror descent method.",
            "Now I want to talk about the general boosting setup and for classification, which I'm sure most people here are very familiar with, right?",
            "So here we have some data or training examples right where each X is living in some observations base each.",
            "Why is a label between negative one and plus one right?",
            "We have some base classifiers, H1 through HN and could potentially be really huge, right?",
            "We assume that this set is closed under negation, so we just consider non negative combinations of the classifiers and we want to find one that's performed significantly better than any individual classifier in our set H, right?",
            "So."
        ],
        [
            "You know, typically what we want in this for this setup, right?",
            "This problem?",
            "Well, good performance on the training data, right?",
            "Good predictive performance.",
            "That's probably the most important thing we wanted.",
            "Some kind of shrinkage in sparsity in the coefficients, right, so?",
            "So what's up?"
        ],
        [
            "Order some loss functions that sort of measure the goodness of our ability to predict, right?",
            "Well, two loss functions that are typically considered here right to have many are the margin.",
            "Which is defined as, you know, sort of the worst case.",
            "Of this inner product, right?",
            "Because we want this inner product to be positive for all examples.",
            "That means that we classified correctly, right?",
            "There's also the exponential loss which is.",
            "This quantity here and then this slog of the exponential loss right log sum X.",
            "And so one thing about the margin is it was known for awhile that that high margin implies some good generalization bounds, right?",
            "But on the other hand, the exponential loss also has good properties.",
            "So it's not clear which is the right objective, but actually what will show?"
        ],
        [
            "Is that out of boost?",
            "Is mirror descent to minimize the jewel of the margin problem right?",
            "So here the margin maximization problem.",
            "I write it as the dual is to maximize overall normalized classifiers Lambda in a unit simplex.",
            "The margin right and its dual problem by flipping the Max and min is just to minimize the edge overall weight vectors in the M dimensional simplex.",
            "So I'll make a standard assumption here, right that we can actually for any distribution over the examples we can compute.",
            "The base classifier that performs best according to that distribution, right?",
            "This is a weak learning assumption."
        ],
        [
            "So here's the description of the Adaboost algorithm, right?",
            "So at every iteration it computes JK from this weak learner.",
            "And it updates the classifier by adding this JHJK to the current classifier and then it updates the weights according to this exponential weight update, right and I?",
            "As we saw, that update is similar to the Mirror Descent update with an entry box function, so it's well known right, and it's obvious that Adaboost has these sort of sparsity and regularization properties, right?",
            "The number of base classifiers in your overall classifier at iteration K is at most K and the norm of this here Lambda is the coefficient vector of my classifier, right?",
            "So in the one norm of this is less than the sum of the alphas.",
            "So."
        ],
        [
            "There's been lots of work on understanding Adaboost in the context of optimization.",
            "And interpret it as a coordinate descent method to minimize the exponential loss for a long time.",
            "So the hedge algorithm has been interpreted as dual averaging.",
            "And so actually here we sort of show result.",
            "That's kind of contradictory to this other result from this paper.",
            "And about 10 years ago, I think, which shows the Adaboost can in fact fail to maximize the margin.",
            "But that's under this particular step size roll, which is coming from line search on the exponential loss.",
            "And there's been lots of other work.",
            "So what do we show?"
        ],
        [
            "An we show that actually you know the our interpretation is that the sequence of weight vectors in Adaboost arise as a sequence of primal variables in mirror descent.",
            "To solve this primal problem of minimizing the edge.",
            "An actually the dual variables right are exactly these convex combination of classifiers.",
            "These are exactly the classifiers that Adaboost produces at iteration K. So immediately from this."
        ],
        [
            "I.",
            "An equivalence result.",
            "You know the first question to ask is what are the implications in terms of guarantees, complexity, right?",
            "So to understand that first I just know it right that the margin of the classifier produced by Adaboost at iteration K, right?",
            "The normalized classifier is exactly this quantity P, Lambda, K. And then I also also relate the edge at iteration K to the the L Infinity norm of the logx potential loss at the unnormalized classifier.",
            "Right so.",
            "Given this these interpretations.",
            "This result, right?",
            "We get the following complexity result following guarantees.",
            "Which say that the.",
            "Right at any iteration of Adaboost."
        ],
        [
            "There is a this minimum of all the gradients.",
            "The norms of the gradients, right minus the margin.",
            "Of the classifier at iteration K. Is less than this this quantity here an in addition if we use this?",
            "In somewhat intelligent stepsize, right of square root, 2 logam over K, then we get you know, a primal dual convergence in terms of orders like order one over square root K, right?",
            "Actually, I want to interpret these results in two specific cases, right?",
            "One where the data is separable and where the data is not separable so?"
        ],
        [
            "The data is separable means that this maximum margin is positive, right?",
            "So if Rosar is greater than zero, then actually the margin is an informative quantity.",
            "So in that case I just use.",
            "We just use weak duality, right?",
            "And we say that for any iteration K of Adaboost, right the classifier?",
            "The normalized classifier produced by the algorithm is within this much of the optimal margin, right?",
            "And if we use this smart stepsize here, this constant step size we get order one over square root K convergence to the optimal margin.",
            "But there's also right the case, which is often true in practice."
        ],
        [
            "Where Rho star is equal to 0 and now the margin is no longer informative right?",
            "Because we have some trivial solutions that can solve the maximum margin problem.",
            "So in this case.",
            "Because Rockstar is 0.",
            "Actually, the bound just reduces to saying that for some iteration I along the trajectory of the algorithm right, the Infinity norm of the log exponential losses less than this right?",
            "Or in the case of the smart step size is less than something that's order one over square root K. So in a sense this is sort of saying that the unnormalized classifier produced by Adaboost right converges to.",
            "And sort of optimal solution to minimize the log exponential loss globally right?",
            "Because this norm of the gradient is going to 0, so it's eventually satisfying the 1st order optimality conditions.",
            "Anne."
        ],
        [
            "So in this non separable case right we derive guarantees for the norm of the gradient, right?",
            "But the natural question is like we'd rather have guarantees for actually the function itself.",
            "So as far as we know, we can't really get these guarantees through this mirror descent interpretation for Adaboost directly, but we actually can consider a slight modification of the algorithm where we get nice guarantees in addition to the guarantees we saw before, so the modification comes from applying the conditional gradient method, also called the Frank Wolfe method, to this problem of minimizing the log exponential loss subject to some regularization constraint on the one norm, an non negativity."
        ],
        [
            "So how does the conditional gradient method work right at any iteration we compute the gradient, we solve this subproblem and we update our new point.",
            "So actually it turns out that finding the gradient due to the structure of this log exponential loss function is just comes down to solving this optimization problem, which is just a weight vector that is given by this right so?",
            "So in a sense, the conditional gradient method also needs to store a weight vector an A."
        ],
        [
            "Updated according to the gradient rule.",
            "And solving this subproblem is actually just easy to see that that's equivalent to calling the week winner right, because we're just sort of finding the one that is best for this distribution.",
            "It's easy to see that these are equivalent, and so given these up."
        ],
        [
            "Rules for for Lambda and the weight vector right.",
            "We get this algorithm we call conditional gradient boost right?",
            "Which is just like Adaboost except net.",
            "Now we update the coefficient vector a little bit different, right for every.",
            "For JCR, new coefficient of our new model right, we add this Alpha bar Delta.",
            "But we also scale back the old.",
            "The old coefficient by this one minus Alpha K and also for everything else we scale it back by 1 minus Alpha bahrke.",
            "And then we just update these weights in a similar way.",
            "And, um."
        ],
        [
            "So what's the complexity of this conditional gradient boosting algorithm, right?",
            "So just from standard conditional gradient results, right?",
            "We can say that at iteration K well within 8 Delta squared over K + 3 of the optimal value to this problem of minimizing the log exponential loss, but also through some smoothing arguments related to the structure of this log exponential loss function, we can get some results for the margin which say that.",
            "Depending on your choice of Delta, you may be within a good solution of the maximum margin problem.",
            "And we can also get bounds for a constant stepsize."
        ],
        [
            "In addition to this 2 / K + 2 stepsize here.",
            "So OK, that was our sort of results on Adaboost, which were originally in the abstract, but are sort of a more deeper story, a little bit then what I'll tell here so.",
            "In a sense, the results of our incremental forward stagewise are a little bit simpler, but also very interesting and have potential potentially useful complexity results.",
            "So in this setting, right, we consider you know the standard linear regression model and why is our response data.",
            "X is our model matrix beta are the coefficients.",
            "And we have some noise, right?",
            "And So what do we want similar things as in the classification setting?",
            "Well, good performance on the training data, good predictive performance and shrinkage in the coefficients, and sparsity in the coefficients.",
            "So.",
            "So in this setting, right?"
        ],
        [
            "Problem which is well studied and then a lot of talk about and will be a lot more talk about.",
            "Here is the lasso problem.",
            "Which is to minimize the least squares loss, right subject to an L1 constraint.",
            "But there's also another function that we consider here, which is measuring sort of the one way to interpret it.",
            "As you know, the maximum correlation between the residuals and the predictors overall residuals.",
            "But this is also adjust the L Infinity norm of the gradient of this function.",
            "Here the least squares loss right?",
            "And so actually.",
            "Anne.",
            "Right, that will actually consider that problem in the setting of this algorithm forward stagewise."
        ],
        [
            "I.",
            "Where so this forward stagewise epsilon algorithm adds at each iteration, write a predictor which is most correlated with the current residuals and it gives it a weight of epsilon.",
            "So this is the forward stage."
        ],
        [
            "This algorithm right?",
            "We just find the predictor that's most correlated an we update the residuals and we update the coefficient vector.",
            "We just add that one new predictor at each iteration."
        ],
        [
            "And so, in addition to this, you know this sort of implicit regularization and sparsity properties, which are nice.",
            "Easy to see here that actually this forward Stagewise algorithm is just an instance of the classical subgradient descent method.",
            "To solve this problem, minimize this correlation function subject to our being a residual.",
            "And.",
            "Given this equivalence, this really simple equivalence theorem.",
            "We immediately get this nice computational guarantees for."
        ],
        [
            "Where this correlation function right for any choice of epsilon in fact?",
            "And you know, if we set epsilon in this way, we get something.",
            "Again that's order one over square root K."
        ],
        [
            "And so I want to ask a similar question that I asked in the case of Adaboost, right?",
            "Which is got some guarantees for this correlation thing or the L Infinity norm of the gradient?",
            "What about guarantees for the least squares loss?",
            "And as you can guess, right, the guarantees for the loose?"
        ],
        [
            "Where's loss come from?",
            "Another simple modification of the boosting algorithm, which corresponds to Frank Wolf on the lasso.",
            "Right, so I Frank Wolf on the lasso works in a similar way as it did with without a boost.",
            "And, um."
        ],
        [
            "Again, right due to the structure of this L1 ball, the solving the sub problem is just the same thing as is finding the predictor which is most correlated with the current residuals.",
            "And so through the algorithm, it looks like a very slight modification of forward stagewise right where at each iteration we just."
        ],
        [
            "Put the current residuals, find the predictor that's most correlated an we update the coefficient vector like this.",
            "And so, really, structurally, it's the same thing as forward stagewise just changing slightly how you update the coefficient vector.",
            "And it also has, you know, the similar sparsity.",
            "And obviously the regularization constraints are satisfied.",
            "And in fact, even for a."
        ],
        [
            "Step size Alpha bar equals epsilon over Delta plus epsilon, right?",
            "The update is even simpler.",
            "It's really just do the forward stagewise update and then renormalize.",
            "Anne.",
            "So."
        ],
        [
            "What are the complexity results for this algorithm?",
            "Right?",
            "Again, we get due to the standard Frank Wolf results.",
            "And I can actually sort of using a bit of a more advanced result, but I won't get into that here.",
            "We can say that you know at iteration K, There's some.",
            "There's some I that satisfies that within this amount of the optimal least squares loss, right?",
            "Subject to the L1 constraint.",
            "But there's also some.",
            "It also satisfies this inequality.",
            "For the.",
            "The residual the correlation with the residuals.",
            "And we can also get bounds for the constant stepsize."
        ],
        [
            "So to conclude, right?",
            "We've shown a 2 main equivalence results, right?",
            "We show that Adaboost is in fact an instance of mirror descent with an entropy prox function.",
            "And through this result we get guarantees for the margin in the case of separable data and for the L Infinity norm of the gradient of the log exponential loss otherwise.",
            "And we also shown that this forward Stagewise algorithm is equivalent to the classical subgradient descent method, which gives complexity guarantees for this residual function and.",
            "Actually, both of these results can be extended to a more general functional boosting setting, as long as the loss function right.",
            "Either the least squares in the case of regression or the log exponential loss is convex globally smooth function.",
            "And then we've also shown that you know the conditional gradient slash Frank Wolfe method applied to these problems is really just a slight modification of the classic boosting methods, which gives more guarantees in addition to the guarantees that we had before.",
            "So that's it for the talk, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also follow the trend of changing my title a little bit from the original title.",
                    "label": 0
                },
                {
                    "sent": "And the reason for that is an increase in scope of it.",
                    "label": 0
                },
                {
                    "sent": "Hopefully I won't regret that, but yeah, so this is joint work with my advisor Rob Freund an also Rahul Mazumder postdoc at MIT, and so the main theme of this work is sort of a new perspective on boosting methods as mirror descent methods in convex optimization, and the implications of this perspective, which will hopefully be useful to at least some people here.",
                    "label": 0
                },
                {
                    "sent": "So first sort of some motivation.",
                    "label": 0
                },
                {
                    "sent": "Review of it for.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probably everyone.",
                    "label": 0
                },
                {
                    "sent": "What are boosting methods right?",
                    "label": 1
                },
                {
                    "sent": "So boosting methods are iterative methods that combine weak models or weak learners where we can model is just something that one can say you know, perform slightly better than random guessing.",
                    "label": 0
                },
                {
                    "sent": "In these things typically work by adding one new week model per iteration.",
                    "label": 1
                },
                {
                    "sent": "And the weight on each week model is typically pretty small.",
                    "label": 1
                },
                {
                    "sent": "And so here I'll consider two different settings, right?",
                    "label": 1
                },
                {
                    "sent": "The first setting is binary classification and the second setting is regularised or sparse linear regression.",
                    "label": 0
                },
                {
                    "sent": "So these are things we've seen already several times during this workshop.",
                    "label": 0
                },
                {
                    "sent": "And then I just want to remark that boosting methods are often tuned in terms of the number of iterations and the stepsize so as to perform some sort of implicit regularization, right?",
                    "label": 0
                },
                {
                    "sent": "But in addition to implicit regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there is also the notion of an explicitly defined regularised problem, right?",
                    "label": 0
                },
                {
                    "sent": "Minimize some loss function, plus a regularizer or subject to some regularization constraints.",
                    "label": 0
                },
                {
                    "sent": "So given that boosting methods are performing implicit regularization, we ask here are boosting methods, solving any optimization problems right?",
                    "label": 1
                },
                {
                    "sent": "And if So what are the computational guarantees we can derive through such an interpretation and then also, can we somehow adapt boosting methods?",
                    "label": 0
                },
                {
                    "sent": "To actually solve explicitly defined regularised problems.",
                    "label": 0
                },
                {
                    "sent": "So to start.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just outline all of our results here.",
                    "label": 1
                },
                {
                    "sent": "So first thing is, we show that Adaboost well known algorithm is actually an instance of the mirror descent method to minimize the edge and also through the duality of mirror descent, maximize the margin.",
                    "label": 1
                },
                {
                    "sent": "And we also showed that the incremental forward stagewise regression algorithm, an algorithm for solving lasso, is an instance of the classical subgradient descent method.",
                    "label": 0
                },
                {
                    "sent": "To minimize the maximum correlation between the residuals in the predictors.",
                    "label": 1
                },
                {
                    "sent": "And then through both of these interpretations, we derive some hopefully useful computational complexity guarantees.",
                    "label": 0
                },
                {
                    "sent": "And then we also consider the Frank Wolfe method applied to the two regularised problems, right?",
                    "label": 0
                },
                {
                    "sent": "Minimizing the log exponential, Lars and Lasso, and we show that these algorithms are very similar to the known boosting algorithms, Adaboost and forward stagewise an we also get additional complexity guarantees for these methods.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just first a quick review of the Mirror descent method.",
                    "label": 0
                },
                {
                    "sent": "Very quick review, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a method for solving a convex optimization problem, right?",
                    "label": 0
                },
                {
                    "sent": "Want to minimize F of X, where F is a potentially non smooth but Lipschitz continuous function and P is a convex and close that.",
                    "label": 1
                },
                {
                    "sent": "And throughout will assume that F arises from a min Max structure, right?",
                    "label": 0
                },
                {
                    "sent": "So this means that we have some some function fee which is convex in the first argument in cave, in the second argument, and actually our function F is defined as the solution to an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So we saw this in the last lecture under this setting, right?",
                    "label": 0
                },
                {
                    "sent": "There's a theorem that says that computing a subgradient of F is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually just the same thing as solving that optimization problem and then computing a gradient at the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "So all we need to do to pick out a subgradient is find the optimal solution for a fixed X and then evaluated gradient.",
                    "label": 0
                },
                {
                    "sent": "And then actually, when PR our domain is bounded, we can also consider a dual problem, right?",
                    "label": 0
                },
                {
                    "sent": "Which is to maximize the function P of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lambda, which is defined by switching the Max and min right.",
                    "label": 0
                },
                {
                    "sent": "And so in this primal dual setting, right?",
                    "label": 0
                },
                {
                    "sent": "Or in general, Mirror descent uses A1 strongly convex prox function.",
                    "label": 1
                },
                {
                    "sent": "So Prox function is just one strongly convex function which needs to be chosen such that we can solve this type of projection problem at every iteration.",
                    "label": 1
                },
                {
                    "sent": "And I associated with this process function is a Bregman distance which is sort of a replacement for the distance measured in whichever norm.",
                    "label": 0
                },
                {
                    "sent": "We're strongly convex with.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does mirror descent work right?",
                    "label": 1
                },
                {
                    "sent": "We so we start at an initial point X, not Lambda, not which is zero at each iteration, right?",
                    "label": 0
                },
                {
                    "sent": "We solve our subproblem which computes the optimal solution for fixed XC.",
                    "label": 0
                },
                {
                    "sent": "We find the gradient at the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "We pick a step size right and update our primal point according to this projection and then we also update the dual point by taking a convex combination of all these previous solutions weighted by these alphas, right?",
                    "label": 0
                },
                {
                    "sent": "So these alphas are the step lengths and so I just sort of note here that actually you know the assignment of Lambda K plus one is just kind of this after the fact thing that really plays no role in the dynamics of the method right?",
                    "label": 1
                },
                {
                    "sent": "Because the method is just defined in terms of these primal variables X.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so 2 examples that I'll use.",
                    "label": 0
                },
                {
                    "sent": "The only examples I use during this talk right are the classical subgradient descent method, right?",
                    "label": 0
                },
                {
                    "sent": "So P is RN and D is now 1/2 Euclidean norm of X squared, right?",
                    "label": 1
                },
                {
                    "sent": "So then this proximal projection update just becomes the subgradient descent step.",
                    "label": 1
                },
                {
                    "sent": "The other example used in the case of Adaboost is the multiplicative weight updates.",
                    "label": 1
                },
                {
                    "sent": "So in this setting right P our domain is the simplex unit simplex are prox function.",
                    "label": 0
                },
                {
                    "sent": "Is the entropy function which is known to be one strongly convex with respect to the one norm and now the step to the proxamol update is just this multiplicative weighting update right where we multiply by this exponential?",
                    "label": 0
                },
                {
                    "sent": "So what are the computational guarantees?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The complexity of mirror descent, so you know this.",
                    "label": 0
                },
                {
                    "sent": "These results have been known for a while due to many different people.",
                    "label": 0
                },
                {
                    "sent": "And so in the general setting, right P can potentially be unbounded.",
                    "label": 0
                },
                {
                    "sent": "Here we know that at least one of our iterates, minus the optimal solution must be less than this quantity, right?",
                    "label": 0
                },
                {
                    "sent": "Which can be conveniently bounded for specific choices of the Alpha sequence, right?",
                    "label": 0
                },
                {
                    "sent": "But in addition, if P is compact, right?",
                    "label": 1
                },
                {
                    "sent": "And then we have some sort of strong duality, and we let the Barbie some upper bound on this Bregman distance, then actually we get this even stronger statement, which says that this.",
                    "label": 0
                },
                {
                    "sent": "The duality gap right for some X -- P at the last data rate Lambda K plus one is bounded by this quantity.",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That was a review of the Mirror descent method.",
                    "label": 0
                },
                {
                    "sent": "Now I want to talk about the general boosting setup and for classification, which I'm sure most people here are very familiar with, right?",
                    "label": 0
                },
                {
                    "sent": "So here we have some data or training examples right where each X is living in some observations base each.",
                    "label": 0
                },
                {
                    "sent": "Why is a label between negative one and plus one right?",
                    "label": 0
                },
                {
                    "sent": "We have some base classifiers, H1 through HN and could potentially be really huge, right?",
                    "label": 0
                },
                {
                    "sent": "We assume that this set is closed under negation, so we just consider non negative combinations of the classifiers and we want to find one that's performed significantly better than any individual classifier in our set H, right?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, typically what we want in this for this setup, right?",
                    "label": 0
                },
                {
                    "sent": "This problem?",
                    "label": 0
                },
                {
                    "sent": "Well, good performance on the training data, right?",
                    "label": 1
                },
                {
                    "sent": "Good predictive performance.",
                    "label": 0
                },
                {
                    "sent": "That's probably the most important thing we wanted.",
                    "label": 1
                },
                {
                    "sent": "Some kind of shrinkage in sparsity in the coefficients, right, so?",
                    "label": 0
                },
                {
                    "sent": "So what's up?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Order some loss functions that sort of measure the goodness of our ability to predict, right?",
                    "label": 1
                },
                {
                    "sent": "Well, two loss functions that are typically considered here right to have many are the margin.",
                    "label": 0
                },
                {
                    "sent": "Which is defined as, you know, sort of the worst case.",
                    "label": 0
                },
                {
                    "sent": "Of this inner product, right?",
                    "label": 0
                },
                {
                    "sent": "Because we want this inner product to be positive for all examples.",
                    "label": 0
                },
                {
                    "sent": "That means that we classified correctly, right?",
                    "label": 0
                },
                {
                    "sent": "There's also the exponential loss which is.",
                    "label": 1
                },
                {
                    "sent": "This quantity here and then this slog of the exponential loss right log sum X.",
                    "label": 0
                },
                {
                    "sent": "And so one thing about the margin is it was known for awhile that that high margin implies some good generalization bounds, right?",
                    "label": 1
                },
                {
                    "sent": "But on the other hand, the exponential loss also has good properties.",
                    "label": 1
                },
                {
                    "sent": "So it's not clear which is the right objective, but actually what will show?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that out of boost?",
                    "label": 0
                },
                {
                    "sent": "Is mirror descent to minimize the jewel of the margin problem right?",
                    "label": 0
                },
                {
                    "sent": "So here the margin maximization problem.",
                    "label": 1
                },
                {
                    "sent": "I write it as the dual is to maximize overall normalized classifiers Lambda in a unit simplex.",
                    "label": 1
                },
                {
                    "sent": "The margin right and its dual problem by flipping the Max and min is just to minimize the edge overall weight vectors in the M dimensional simplex.",
                    "label": 0
                },
                {
                    "sent": "So I'll make a standard assumption here, right that we can actually for any distribution over the examples we can compute.",
                    "label": 1
                },
                {
                    "sent": "The base classifier that performs best according to that distribution, right?",
                    "label": 0
                },
                {
                    "sent": "This is a weak learning assumption.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the description of the Adaboost algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "So at every iteration it computes JK from this weak learner.",
                    "label": 0
                },
                {
                    "sent": "And it updates the classifier by adding this JHJK to the current classifier and then it updates the weights according to this exponential weight update, right and I?",
                    "label": 0
                },
                {
                    "sent": "As we saw, that update is similar to the Mirror Descent update with an entry box function, so it's well known right, and it's obvious that Adaboost has these sort of sparsity and regularization properties, right?",
                    "label": 0
                },
                {
                    "sent": "The number of base classifiers in your overall classifier at iteration K is at most K and the norm of this here Lambda is the coefficient vector of my classifier, right?",
                    "label": 0
                },
                {
                    "sent": "So in the one norm of this is less than the sum of the alphas.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's been lots of work on understanding Adaboost in the context of optimization.",
                    "label": 1
                },
                {
                    "sent": "And interpret it as a coordinate descent method to minimize the exponential loss for a long time.",
                    "label": 1
                },
                {
                    "sent": "So the hedge algorithm has been interpreted as dual averaging.",
                    "label": 1
                },
                {
                    "sent": "And so actually here we sort of show result.",
                    "label": 0
                },
                {
                    "sent": "That's kind of contradictory to this other result from this paper.",
                    "label": 0
                },
                {
                    "sent": "And about 10 years ago, I think, which shows the Adaboost can in fact fail to maximize the margin.",
                    "label": 0
                },
                {
                    "sent": "But that's under this particular step size roll, which is coming from line search on the exponential loss.",
                    "label": 0
                },
                {
                    "sent": "And there's been lots of other work.",
                    "label": 0
                },
                {
                    "sent": "So what do we show?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An we show that actually you know the our interpretation is that the sequence of weight vectors in Adaboost arise as a sequence of primal variables in mirror descent.",
                    "label": 1
                },
                {
                    "sent": "To solve this primal problem of minimizing the edge.",
                    "label": 0
                },
                {
                    "sent": "An actually the dual variables right are exactly these convex combination of classifiers.",
                    "label": 0
                },
                {
                    "sent": "These are exactly the classifiers that Adaboost produces at iteration K. So immediately from this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "An equivalence result.",
                    "label": 0
                },
                {
                    "sent": "You know the first question to ask is what are the implications in terms of guarantees, complexity, right?",
                    "label": 0
                },
                {
                    "sent": "So to understand that first I just know it right that the margin of the classifier produced by Adaboost at iteration K, right?",
                    "label": 1
                },
                {
                    "sent": "The normalized classifier is exactly this quantity P, Lambda, K. And then I also also relate the edge at iteration K to the the L Infinity norm of the logx potential loss at the unnormalized classifier.",
                    "label": 1
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Given this these interpretations.",
                    "label": 0
                },
                {
                    "sent": "This result, right?",
                    "label": 0
                },
                {
                    "sent": "We get the following complexity result following guarantees.",
                    "label": 0
                },
                {
                    "sent": "Which say that the.",
                    "label": 0
                },
                {
                    "sent": "Right at any iteration of Adaboost.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a this minimum of all the gradients.",
                    "label": 0
                },
                {
                    "sent": "The norms of the gradients, right minus the margin.",
                    "label": 0
                },
                {
                    "sent": "Of the classifier at iteration K. Is less than this this quantity here an in addition if we use this?",
                    "label": 0
                },
                {
                    "sent": "In somewhat intelligent stepsize, right of square root, 2 logam over K, then we get you know, a primal dual convergence in terms of orders like order one over square root K, right?",
                    "label": 0
                },
                {
                    "sent": "Actually, I want to interpret these results in two specific cases, right?",
                    "label": 0
                },
                {
                    "sent": "One where the data is separable and where the data is not separable so?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The data is separable means that this maximum margin is positive, right?",
                    "label": 1
                },
                {
                    "sent": "So if Rosar is greater than zero, then actually the margin is an informative quantity.",
                    "label": 0
                },
                {
                    "sent": "So in that case I just use.",
                    "label": 0
                },
                {
                    "sent": "We just use weak duality, right?",
                    "label": 0
                },
                {
                    "sent": "And we say that for any iteration K of Adaboost, right the classifier?",
                    "label": 0
                },
                {
                    "sent": "The normalized classifier produced by the algorithm is within this much of the optimal margin, right?",
                    "label": 0
                },
                {
                    "sent": "And if we use this smart stepsize here, this constant step size we get order one over square root K convergence to the optimal margin.",
                    "label": 0
                },
                {
                    "sent": "But there's also right the case, which is often true in practice.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where Rho star is equal to 0 and now the margin is no longer informative right?",
                    "label": 1
                },
                {
                    "sent": "Because we have some trivial solutions that can solve the maximum margin problem.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "Because Rockstar is 0.",
                    "label": 1
                },
                {
                    "sent": "Actually, the bound just reduces to saying that for some iteration I along the trajectory of the algorithm right, the Infinity norm of the log exponential losses less than this right?",
                    "label": 0
                },
                {
                    "sent": "Or in the case of the smart step size is less than something that's order one over square root K. So in a sense this is sort of saying that the unnormalized classifier produced by Adaboost right converges to.",
                    "label": 0
                },
                {
                    "sent": "And sort of optimal solution to minimize the log exponential loss globally right?",
                    "label": 0
                },
                {
                    "sent": "Because this norm of the gradient is going to 0, so it's eventually satisfying the 1st order optimality conditions.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this non separable case right we derive guarantees for the norm of the gradient, right?",
                    "label": 1
                },
                {
                    "sent": "But the natural question is like we'd rather have guarantees for actually the function itself.",
                    "label": 1
                },
                {
                    "sent": "So as far as we know, we can't really get these guarantees through this mirror descent interpretation for Adaboost directly, but we actually can consider a slight modification of the algorithm where we get nice guarantees in addition to the guarantees we saw before, so the modification comes from applying the conditional gradient method, also called the Frank Wolfe method, to this problem of minimizing the log exponential loss subject to some regularization constraint on the one norm, an non negativity.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does the conditional gradient method work right at any iteration we compute the gradient, we solve this subproblem and we update our new point.",
                    "label": 1
                },
                {
                    "sent": "So actually it turns out that finding the gradient due to the structure of this log exponential loss function is just comes down to solving this optimization problem, which is just a weight vector that is given by this right so?",
                    "label": 0
                },
                {
                    "sent": "So in a sense, the conditional gradient method also needs to store a weight vector an A.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Updated according to the gradient rule.",
                    "label": 0
                },
                {
                    "sent": "And solving this subproblem is actually just easy to see that that's equivalent to calling the week winner right, because we're just sort of finding the one that is best for this distribution.",
                    "label": 1
                },
                {
                    "sent": "It's easy to see that these are equivalent, and so given these up.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rules for for Lambda and the weight vector right.",
                    "label": 0
                },
                {
                    "sent": "We get this algorithm we call conditional gradient boost right?",
                    "label": 0
                },
                {
                    "sent": "Which is just like Adaboost except net.",
                    "label": 0
                },
                {
                    "sent": "Now we update the coefficient vector a little bit different, right for every.",
                    "label": 0
                },
                {
                    "sent": "For JCR, new coefficient of our new model right, we add this Alpha bar Delta.",
                    "label": 0
                },
                {
                    "sent": "But we also scale back the old.",
                    "label": 0
                },
                {
                    "sent": "The old coefficient by this one minus Alpha K and also for everything else we scale it back by 1 minus Alpha bahrke.",
                    "label": 0
                },
                {
                    "sent": "And then we just update these weights in a similar way.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the complexity of this conditional gradient boosting algorithm, right?",
                    "label": 1
                },
                {
                    "sent": "So just from standard conditional gradient results, right?",
                    "label": 1
                },
                {
                    "sent": "We can say that at iteration K well within 8 Delta squared over K + 3 of the optimal value to this problem of minimizing the log exponential loss, but also through some smoothing arguments related to the structure of this log exponential loss function, we can get some results for the margin which say that.",
                    "label": 0
                },
                {
                    "sent": "Depending on your choice of Delta, you may be within a good solution of the maximum margin problem.",
                    "label": 0
                },
                {
                    "sent": "And we can also get bounds for a constant stepsize.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In addition to this 2 / K + 2 stepsize here.",
                    "label": 0
                },
                {
                    "sent": "So OK, that was our sort of results on Adaboost, which were originally in the abstract, but are sort of a more deeper story, a little bit then what I'll tell here so.",
                    "label": 0
                },
                {
                    "sent": "In a sense, the results of our incremental forward stagewise are a little bit simpler, but also very interesting and have potential potentially useful complexity results.",
                    "label": 0
                },
                {
                    "sent": "So in this setting, right, we consider you know the standard linear regression model and why is our response data.",
                    "label": 1
                },
                {
                    "sent": "X is our model matrix beta are the coefficients.",
                    "label": 1
                },
                {
                    "sent": "And we have some noise, right?",
                    "label": 0
                },
                {
                    "sent": "And So what do we want similar things as in the classification setting?",
                    "label": 0
                },
                {
                    "sent": "Well, good performance on the training data, good predictive performance and shrinkage in the coefficients, and sparsity in the coefficients.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So in this setting, right?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem which is well studied and then a lot of talk about and will be a lot more talk about.",
                    "label": 0
                },
                {
                    "sent": "Here is the lasso problem.",
                    "label": 1
                },
                {
                    "sent": "Which is to minimize the least squares loss, right subject to an L1 constraint.",
                    "label": 0
                },
                {
                    "sent": "But there's also another function that we consider here, which is measuring sort of the one way to interpret it.",
                    "label": 0
                },
                {
                    "sent": "As you know, the maximum correlation between the residuals and the predictors overall residuals.",
                    "label": 1
                },
                {
                    "sent": "But this is also adjust the L Infinity norm of the gradient of this function.",
                    "label": 0
                },
                {
                    "sent": "Here the least squares loss right?",
                    "label": 0
                },
                {
                    "sent": "And so actually.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Right, that will actually consider that problem in the setting of this algorithm forward stagewise.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Where so this forward stagewise epsilon algorithm adds at each iteration, write a predictor which is most correlated with the current residuals and it gives it a weight of epsilon.",
                    "label": 1
                },
                {
                    "sent": "So this is the forward stage.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This algorithm right?",
                    "label": 0
                },
                {
                    "sent": "We just find the predictor that's most correlated an we update the residuals and we update the coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "We just add that one new predictor at each iteration.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, in addition to this, you know this sort of implicit regularization and sparsity properties, which are nice.",
                    "label": 0
                },
                {
                    "sent": "Easy to see here that actually this forward Stagewise algorithm is just an instance of the classical subgradient descent method.",
                    "label": 1
                },
                {
                    "sent": "To solve this problem, minimize this correlation function subject to our being a residual.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Given this equivalence, this really simple equivalence theorem.",
                    "label": 0
                },
                {
                    "sent": "We immediately get this nice computational guarantees for.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where this correlation function right for any choice of epsilon in fact?",
                    "label": 1
                },
                {
                    "sent": "And you know, if we set epsilon in this way, we get something.",
                    "label": 1
                },
                {
                    "sent": "Again that's order one over square root K.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I want to ask a similar question that I asked in the case of Adaboost, right?",
                    "label": 0
                },
                {
                    "sent": "Which is got some guarantees for this correlation thing or the L Infinity norm of the gradient?",
                    "label": 0
                },
                {
                    "sent": "What about guarantees for the least squares loss?",
                    "label": 1
                },
                {
                    "sent": "And as you can guess, right, the guarantees for the loose?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where's loss come from?",
                    "label": 0
                },
                {
                    "sent": "Another simple modification of the boosting algorithm, which corresponds to Frank Wolf on the lasso.",
                    "label": 1
                },
                {
                    "sent": "Right, so I Frank Wolf on the lasso works in a similar way as it did with without a boost.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, right due to the structure of this L1 ball, the solving the sub problem is just the same thing as is finding the predictor which is most correlated with the current residuals.",
                    "label": 0
                },
                {
                    "sent": "And so through the algorithm, it looks like a very slight modification of forward stagewise right where at each iteration we just.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put the current residuals, find the predictor that's most correlated an we update the coefficient vector like this.",
                    "label": 0
                },
                {
                    "sent": "And so, really, structurally, it's the same thing as forward stagewise just changing slightly how you update the coefficient vector.",
                    "label": 0
                },
                {
                    "sent": "And it also has, you know, the similar sparsity.",
                    "label": 0
                },
                {
                    "sent": "And obviously the regularization constraints are satisfied.",
                    "label": 0
                },
                {
                    "sent": "And in fact, even for a.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step size Alpha bar equals epsilon over Delta plus epsilon, right?",
                    "label": 0
                },
                {
                    "sent": "The update is even simpler.",
                    "label": 0
                },
                {
                    "sent": "It's really just do the forward stagewise update and then renormalize.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are the complexity results for this algorithm?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Again, we get due to the standard Frank Wolf results.",
                    "label": 0
                },
                {
                    "sent": "And I can actually sort of using a bit of a more advanced result, but I won't get into that here.",
                    "label": 0
                },
                {
                    "sent": "We can say that you know at iteration K, There's some.",
                    "label": 0
                },
                {
                    "sent": "There's some I that satisfies that within this amount of the optimal least squares loss, right?",
                    "label": 0
                },
                {
                    "sent": "Subject to the L1 constraint.",
                    "label": 0
                },
                {
                    "sent": "But there's also some.",
                    "label": 0
                },
                {
                    "sent": "It also satisfies this inequality.",
                    "label": 0
                },
                {
                    "sent": "For the.",
                    "label": 0
                },
                {
                    "sent": "The residual the correlation with the residuals.",
                    "label": 0
                },
                {
                    "sent": "And we can also get bounds for the constant stepsize.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, right?",
                    "label": 0
                },
                {
                    "sent": "We've shown a 2 main equivalence results, right?",
                    "label": 0
                },
                {
                    "sent": "We show that Adaboost is in fact an instance of mirror descent with an entropy prox function.",
                    "label": 1
                },
                {
                    "sent": "And through this result we get guarantees for the margin in the case of separable data and for the L Infinity norm of the gradient of the log exponential loss otherwise.",
                    "label": 1
                },
                {
                    "sent": "And we also shown that this forward Stagewise algorithm is equivalent to the classical subgradient descent method, which gives complexity guarantees for this residual function and.",
                    "label": 0
                },
                {
                    "sent": "Actually, both of these results can be extended to a more general functional boosting setting, as long as the loss function right.",
                    "label": 0
                },
                {
                    "sent": "Either the least squares in the case of regression or the log exponential loss is convex globally smooth function.",
                    "label": 0
                },
                {
                    "sent": "And then we've also shown that you know the conditional gradient slash Frank Wolfe method applied to these problems is really just a slight modification of the classic boosting methods, which gives more guarantees in addition to the guarantees that we had before.",
                    "label": 0
                },
                {
                    "sent": "So that's it for the talk, thank you.",
                    "label": 0
                }
            ]
        }
    }
}