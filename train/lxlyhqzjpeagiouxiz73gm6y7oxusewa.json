{
    "id": "lxlyhqzjpeagiouxiz73gm6y7oxusewa",
    "title": "Spectral learning of linear dynamics from generalisedlinear observations with application to neural population data",
    "info": {
        "author": [
            "Lars Buesing, Gatsby Computational Neuroscience Unit, University College London"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/nips2012_buesing_spectral_learning/",
    "segmentation": [
        [
            "My talk is going to be based on possibly the simplest latent variable time series model you can come up with named linear dynamical systems, and there is the assumption that the latent variables are Markovian process.",
            "Everything the latent variables are normally distributed and the transition latent transition dynamics is also Gaussian linear mapping as well as the observation app into that variable is also assumed to be linear Gaussian because in this model."
        ],
        [
            "Everything is jointly normal.",
            "Parameter estimation is pretty straightforward, you could use.",
            "Very variety of different methods to do parameter estimation.",
            "For example, maximum likelihood techniques or spectral.",
            "You could also use fast spectral methods that have been developed in the 80s for special math models."
        ],
        [
            "So these these models are nice because."
        ],
        [
            "A simple but however they might not be appropriate for all the datasets of your mind account."
        ],
        [
            "For example, from Multielectrode recordings, where the data is given by small integers.",
            "So there the assumption of normality of the data is clearly violated."
        ],
        [
            "And here we fixed that by a very simple fix.",
            "We just assume that there's another additional observation process sitting on top of the dead variables and.",
            "So we assume here that this observation process is element wise, so the elements of that one at a time passed with another observation process, which we assume here is a comes from a GLM with inputs zeti and we further assume that this observation model doesn't have any free or unknown parameters."
        ],
        [
            "So and this type of model has been used by a couple of groups to model multielectrode recordings where they kind of compare."
        ],
        [
            "Favorably to competing methods and mostly in this type of data, one assumes that the observations are."
        ],
        [
            "Conditioning parts are distributed with a log rate that's given by the set variables themselves.",
            "So another in these kind of slightly generalized LDS models parameter estimation."
        ],
        [
            "Slightly more challenging in comparison to the standard case, and because of the breaking of the Gaussian normal linear Gaussian assumption and one couldn't Thierry use Senate maximum likelihood methods such as expectation maximization, but in general these generations for these type of models would would have to be numerically approximated, and which might be computationally expensive and also prone to approximation errors.",
            "And on top of these downsides of VM one still.",
            "Faces"
        ],
        [
            "Standard Hill climbing algorithm type issues.",
            "For example, local Optima as illustrated here in this little miracle example, where we identified LDS parameters with GLM observations from a given data set, and he applauded the performance of the identified LDS parameters on the Y axis.",
            "I'm going to go into details of the performance measure later on as a number of the M iterations on the X axis for multiple restarts, clearly indicating local Optima."
        ],
        [
            "And in other instances one also faces slow convergence of iterations.",
            "For example, if there are ridges in the likelihood surface, or if they just flat regions of likelihood.",
            "And these kind of issues let us to consider alternative methods for identifying the LDS parameters and one type of approach that has been shown to be very fruitful for other latent variable models recently are spectral method."
        ],
        [
            "And so we try to see whether we could use spectral methods to identify the parameters of such slightly generalized LDS systems, and this is how we go about it.",
            "So assume for a second, that is that variables themselves were directly observed, so forget about the additional observation process for a moment.",
            "So in this case our model of course reduces just a standard LDS, so we could use standard spectral methods for LDS learning, such as."
        ],
        [
            "The whole, method, which obviously the simplest method following such parameters from these models and this method kind of proceeds in the following way.",
            "So we did."
        ],
        [
            "In a past, ZT minus for each time step by just stacking the last chaos."
        ],
        [
            "Nations into a single big vector where K is a given window size.",
            "And we can do the same."
        ],
        [
            "Going to define the future vectors that plus and the key quantity of this method for different parameters is the."
        ],
        [
            "Cross covariance between these two vectors.",
            "Let's define.",
            "Let's call that Future Past cross Currents Sigma plus minus."
        ],
        [
            "Not difficult to show that this matrix has low rank and this is kind of easy to see directly from the graphical model, because for example if you do inference in such a graphical model, all the information from the past to the future has to flow through the single node XD up here and this basically is the same fact that gives rise to the slow ring structure of this matrix.",
            "And."
        ],
        [
            "So this spectrum method leverage this lowering structure of this matrix in the following way by first computing an empirical estimate of the Future Past cross covariance and then doing a low rank approximation to it by an SVD.",
            "And if the data really came from such a model, then one finds that almost all of the singular values are vanishing except for a couple of those and this."
        ],
        [
            "Response to the latent dimensionality of the problem an from the corresponding blocks of these orthogonal transformations.",
            "One could, with very easy computations on."
        ],
        [
            "Read off the LDS parameters so this is a nice method and would ideally would like to use it, but however now we don't in our model assumption, we don't observe that variables directly, but only through the wise.",
            "So one straight for."
        ],
        [
            "Thing when I try immediately is to compute this Future Past cross covariance matrix directly from the data and this.",
            "But however, if one does that one finds that doesn't have in general a low rank structure."
        ],
        [
            "As shown here in this little miracle example where plotted the singular value spectrum of this Sigma plus minus directly computed from the data, and this spectrum decays rather gradually toward 0 without showing a clear low in structure.",
            "But here's the thing, we."
        ],
        [
            "And you know, we can compute the Future Past cross covariance matrix of the data under our model.",
            "And this is of course just given by the conditional Future Past events matrix conditioned on set and then averaged over that according to our models.",
            "That is a multivariate Gaussian random variable."
        ],
        [
            "This expression here is a function of the mean and the covariance matrix of the Gaussian, and in particular it's a function of the unknown future paths cross covariance matrix of the of these at variables, which is more or less simply a sub block of the covariance matrix set.",
            "So, and we assume that our model definition that we don't have any additional parameters, unknown parameters in our observation process.",
            "So leveraging this we can basically invert this relation between the Future Past events of the data with with that one of the."
        ],
        [
            "By doing a moment matching step, basically want to numerically invert this this relation here to estimate the Future Past cross friends of the set variables from that of the data."
        ],
        [
            "And if we do that, we can uncover the lowering structure that was hidden in the.",
            "And not directly observable from the data, so that computation might look to be very costly at the first glance, but however, due to our model assumptions of the wise are conditionally independent given the Zetes.",
            "It turns out that this moment matching problem decomposes into one dimensional matching problems."
        ],
        [
            "So we can compute one element of the elements of Zets Sigma plus, minus that one at a time.",
            "So we only have to solve a number of 1 dimensional moment matching problems.",
            "So let me just."
        ],
        [
            "Dot on a concrete example.",
            "So if we look at the linear dynamical system with conditioning concerns, surveys of observations.",
            "Where the Y given with a person process that has a lock has a rate that's given by the exponentiated set variables than this."
        ],
        [
            "Matching step can basically be done in closed form and we can get an estimate of the Future Past crosscurrents of that by taking the logarithm elementwise of the future crosscurrents directly computed from the data plus an outer product of its mean term, and then subtract that logarithm of the outer product again.",
            "So."
        ],
        [
            "This constitutes our proposed algorithm for spectral learning algorithm for learning LDS parameters from generous linear observations and the algorithm proceeds in the following way.",
            "So, given some data, why some latent dimensionality end there?",
            "We want to fit to the data and some window size.",
            "It first does a moment estimation step, so we estimate the Future Past crosscurrents of the data and then in the moment matching step we converted and we solve for this Future Past crosscurrent rosette and this has approximately low rank structure which we can leverage by doing a subsequent spectral decomposition step and to finally get our LDS parameters.",
            "So we in."
        ],
        [
            "Estimated this algorithm in a couple of experiments, and all these experience we did with the LDS that I already mentioned me that one with across our distributed observations where the rate is given by the exponentiated set variables.",
            "And."
        ],
        [
            "In that case, the algorithm yields consistent estimates of the LDS parameters, so for large datasets, we expect that our algorithm, the Spectra other than, gives good results for the parameters datasets.",
            "But we were also interested in investigating whether the spectral method also works well on smaller datasets, namely in conjunction with maximum likelihood techniques.",
            "Basically, by using the spectral parameter estimates as initializers for EM iterations and hopefully avoiding too shallow local minima and slow convergence by already initializing them with some sane parameter estimates.",
            "So."
        ],
        [
            "In our experiments, we quantified the goodness of fit of the identified parameters by kind of cross prediction measure where we held out single dimensions of the data and try to predict it given the other dimensions of the data and report the results in terms of variance explained relative to a baseline model where hires of course better here."
        ],
        [
            "And basically users performance measure instead of the likelihood because we can't compute exactly the likelihood, and this is also reason why we use this goodness of fit measure on the training data as a termination criterion for them.",
            "So."
        ],
        [
            "The first set of experiments we we, we tried to identify parameters from artificial datasets which came from the same model.",
            "Basically datasets were sampled from growth ground truth.",
            "Personally, dynamical systems and we gave our algorithms already the right latent dimensions, namely 10 latent dimensions and 25 observation dimensions."
        ],
        [
            "These are the results for our first data set, which was designed to have strong temporal correlations and again the goodness of fit of the identified parameters on the Y axis here and.",
            "And here we compare our spectral the identified parameters which sit up here with the M iterations with different initializers, namely one with factor analysis, initialized initializer and with a couple of random initializations.",
            "And we see that the spectral methods identified for the spectral parameters do a lot better than I am with a different initializers, and especially in this case the spectral parameters couldn't be further refined.",
            "By YM steps."
        ],
        [
            "And we find kind of similar results for datasets which had less temporal correlations with stronger instantaneous correlations.",
            "In this case, the spectral parameters could be further improved by a couple of EM iterations, but compared to the other initializers for them, our method works yields better parameter estimates with a smaller number of iterations."
        ],
        [
            "But we also found that there are datasets where the spectrum method doesn't work as well, and apparently we found that these are datasets with very weak temporal correlations, and in that case factor analysis turns out to be the better initializer, probably because factors already assumed that there is no temporal structure in the data."
        ],
        [
            "So and in the next set of experiments we identified LDS parameters from multi electric recordings and these recordings came from motor cortex of macaque monkeys.",
            "They consisted of 86 dimensions, putative single neurons, and we built the data at 10 millisecond timings which resulted in rather sparse datasets were only 10% of the bins were non empty, so the rest was just zero."
        ],
        [
            "And these are the results for rather small data set consisting just of hundreds seconds of recordings with 86 dimensions.",
            "And here we also see that spectrum methods already give a bit better parameter estimates, but converge rather fast compared to EM iterations.",
            "Interact with factor analysis with random initializations are with.",
            "With Gaussian spectral parameter estimates.",
            "And this margin to competing methods grows.",
            "The more data you give to the algorithm, for example."
        ],
        [
            "These are the results for a data set of consisting of 500 seconds with only the 40 most active neurons, and here spectral analysis as spectral initialization does better.",
            "You might wonder why these traces kind of terminate them early, but this is due to our termination criterion with respect to this goodness of fit measure.",
            "And."
        ],
        [
            "This month, so the most extreme results are the best results for the spectral methods we find when we use all the available data and Amy consisting of 800 seconds and all the neurons.",
            "And here spectral initialization Collea outperforms the other initializers by quite a margin, and also compared for example to random initializations, which gets quite good parameter estimates.",
            "In the end we save alot of em iterations and."
        ],
        [
            "In this application, this translates directly to.",
            "Quite substantial computational savings.",
            "Basically because Yum iterations tend to be very costly and the spectral spectral initialization step is basically cheaper than a single EM iteration.",
            "Yeah, yeah."
        ],
        [
            "So there are a couple of extensions that we played around with and one that we could also do the spectral estimation trick for is.",
            "Then we can account also for external driving input.",
            "These are might be external covariates that you want to model with with whose influence that you want to model until your time series."
        ],
        [
            "And."
        ],
        [
            "So here this is."
        ],
        [
            "A simple model where they feed linearly or couple linearly to the latent States and into the zed variables.",
            "And if we assume that these these external driving inputs also jointly normally distributed, then we can do basically the spectral identification step because we can do the joint moment matching trick that we did before now jointly on U&Y to get the joint moments of UN set, from which we can use standard spectral methods to identify this externally driven.",
            "LDS system."
        ],
        [
            "So let me quickly wrap up here, so I presented a simple spectral method for identifying LDS parameters from generalized linear observations, and this method was basically a simple Stew, two stage method where we in the first step we did a moment matching step together, estimate of the Future Past crosscurrents matrix of the set variables, followed by a standard spectral system identification step for LDS methods LDS models.",
            "So and we."
        ],
        [
            "So that given substantial amount, given enough training data, sufficiently structured datasets, we found that this method gave good parameter results."
        ],
        [
            "And it also worked.",
            "It also turned out to be useful for other datasets, for example of Multielectrode recordings where it provided good initializers for M iterations.",
            "Basically, reducing the number of iterations that we had to do in until convergence yielding computational savings and also yielding to better parameter estimates."
        ],
        [
            "Let me finish by thanking Christianise left from Stanford for generously sharing the data without us and also by acknowledging our funding sources.",
            "Thank you very much for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My talk is going to be based on possibly the simplest latent variable time series model you can come up with named linear dynamical systems, and there is the assumption that the latent variables are Markovian process.",
                    "label": 0
                },
                {
                    "sent": "Everything the latent variables are normally distributed and the transition latent transition dynamics is also Gaussian linear mapping as well as the observation app into that variable is also assumed to be linear Gaussian because in this model.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything is jointly normal.",
                    "label": 0
                },
                {
                    "sent": "Parameter estimation is pretty straightforward, you could use.",
                    "label": 0
                },
                {
                    "sent": "Very variety of different methods to do parameter estimation.",
                    "label": 0
                },
                {
                    "sent": "For example, maximum likelihood techniques or spectral.",
                    "label": 0
                },
                {
                    "sent": "You could also use fast spectral methods that have been developed in the 80s for special math models.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these these models are nice because.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A simple but however they might not be appropriate for all the datasets of your mind account.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, from Multielectrode recordings, where the data is given by small integers.",
                    "label": 0
                },
                {
                    "sent": "So there the assumption of normality of the data is clearly violated.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we fixed that by a very simple fix.",
                    "label": 0
                },
                {
                    "sent": "We just assume that there's another additional observation process sitting on top of the dead variables and.",
                    "label": 0
                },
                {
                    "sent": "So we assume here that this observation process is element wise, so the elements of that one at a time passed with another observation process, which we assume here is a comes from a GLM with inputs zeti and we further assume that this observation model doesn't have any free or unknown parameters.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and this type of model has been used by a couple of groups to model multielectrode recordings where they kind of compare.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Favorably to competing methods and mostly in this type of data, one assumes that the observations are.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conditioning parts are distributed with a log rate that's given by the set variables themselves.",
                    "label": 0
                },
                {
                    "sent": "So another in these kind of slightly generalized LDS models parameter estimation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slightly more challenging in comparison to the standard case, and because of the breaking of the Gaussian normal linear Gaussian assumption and one couldn't Thierry use Senate maximum likelihood methods such as expectation maximization, but in general these generations for these type of models would would have to be numerically approximated, and which might be computationally expensive and also prone to approximation errors.",
                    "label": 1
                },
                {
                    "sent": "And on top of these downsides of VM one still.",
                    "label": 0
                },
                {
                    "sent": "Faces",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Standard Hill climbing algorithm type issues.",
                    "label": 0
                },
                {
                    "sent": "For example, local Optima as illustrated here in this little miracle example, where we identified LDS parameters with GLM observations from a given data set, and he applauded the performance of the identified LDS parameters on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go into details of the performance measure later on as a number of the M iterations on the X axis for multiple restarts, clearly indicating local Optima.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in other instances one also faces slow convergence of iterations.",
                    "label": 0
                },
                {
                    "sent": "For example, if there are ridges in the likelihood surface, or if they just flat regions of likelihood.",
                    "label": 0
                },
                {
                    "sent": "And these kind of issues let us to consider alternative methods for identifying the LDS parameters and one type of approach that has been shown to be very fruitful for other latent variable models recently are spectral method.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we try to see whether we could use spectral methods to identify the parameters of such slightly generalized LDS systems, and this is how we go about it.",
                    "label": 0
                },
                {
                    "sent": "So assume for a second, that is that variables themselves were directly observed, so forget about the additional observation process for a moment.",
                    "label": 0
                },
                {
                    "sent": "So in this case our model of course reduces just a standard LDS, so we could use standard spectral methods for LDS learning, such as.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The whole, method, which obviously the simplest method following such parameters from these models and this method kind of proceeds in the following way.",
                    "label": 0
                },
                {
                    "sent": "So we did.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a past, ZT minus for each time step by just stacking the last chaos.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nations into a single big vector where K is a given window size.",
                    "label": 0
                },
                {
                    "sent": "And we can do the same.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to define the future vectors that plus and the key quantity of this method for different parameters is the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cross covariance between these two vectors.",
                    "label": 0
                },
                {
                    "sent": "Let's define.",
                    "label": 0
                },
                {
                    "sent": "Let's call that Future Past cross Currents Sigma plus minus.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not difficult to show that this matrix has low rank and this is kind of easy to see directly from the graphical model, because for example if you do inference in such a graphical model, all the information from the past to the future has to flow through the single node XD up here and this basically is the same fact that gives rise to the slow ring structure of this matrix.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this spectrum method leverage this lowering structure of this matrix in the following way by first computing an empirical estimate of the Future Past cross covariance and then doing a low rank approximation to it by an SVD.",
                    "label": 0
                },
                {
                    "sent": "And if the data really came from such a model, then one finds that almost all of the singular values are vanishing except for a couple of those and this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Response to the latent dimensionality of the problem an from the corresponding blocks of these orthogonal transformations.",
                    "label": 0
                },
                {
                    "sent": "One could, with very easy computations on.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Read off the LDS parameters so this is a nice method and would ideally would like to use it, but however now we don't in our model assumption, we don't observe that variables directly, but only through the wise.",
                    "label": 0
                },
                {
                    "sent": "So one straight for.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing when I try immediately is to compute this Future Past cross covariance matrix directly from the data and this.",
                    "label": 0
                },
                {
                    "sent": "But however, if one does that one finds that doesn't have in general a low rank structure.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As shown here in this little miracle example where plotted the singular value spectrum of this Sigma plus minus directly computed from the data, and this spectrum decays rather gradually toward 0 without showing a clear low in structure.",
                    "label": 0
                },
                {
                    "sent": "But here's the thing, we.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know, we can compute the Future Past cross covariance matrix of the data under our model.",
                    "label": 0
                },
                {
                    "sent": "And this is of course just given by the conditional Future Past events matrix conditioned on set and then averaged over that according to our models.",
                    "label": 0
                },
                {
                    "sent": "That is a multivariate Gaussian random variable.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This expression here is a function of the mean and the covariance matrix of the Gaussian, and in particular it's a function of the unknown future paths cross covariance matrix of the of these at variables, which is more or less simply a sub block of the covariance matrix set.",
                    "label": 0
                },
                {
                    "sent": "So, and we assume that our model definition that we don't have any additional parameters, unknown parameters in our observation process.",
                    "label": 0
                },
                {
                    "sent": "So leveraging this we can basically invert this relation between the Future Past events of the data with with that one of the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By doing a moment matching step, basically want to numerically invert this this relation here to estimate the Future Past cross friends of the set variables from that of the data.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we do that, we can uncover the lowering structure that was hidden in the.",
                    "label": 0
                },
                {
                    "sent": "And not directly observable from the data, so that computation might look to be very costly at the first glance, but however, due to our model assumptions of the wise are conditionally independent given the Zetes.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this moment matching problem decomposes into one dimensional matching problems.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can compute one element of the elements of Zets Sigma plus, minus that one at a time.",
                    "label": 0
                },
                {
                    "sent": "So we only have to solve a number of 1 dimensional moment matching problems.",
                    "label": 0
                },
                {
                    "sent": "So let me just.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dot on a concrete example.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the linear dynamical system with conditioning concerns, surveys of observations.",
                    "label": 0
                },
                {
                    "sent": "Where the Y given with a person process that has a lock has a rate that's given by the exponentiated set variables than this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matching step can basically be done in closed form and we can get an estimate of the Future Past crosscurrents of that by taking the logarithm elementwise of the future crosscurrents directly computed from the data plus an outer product of its mean term, and then subtract that logarithm of the outer product again.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This constitutes our proposed algorithm for spectral learning algorithm for learning LDS parameters from generous linear observations and the algorithm proceeds in the following way.",
                    "label": 1
                },
                {
                    "sent": "So, given some data, why some latent dimensionality end there?",
                    "label": 0
                },
                {
                    "sent": "We want to fit to the data and some window size.",
                    "label": 0
                },
                {
                    "sent": "It first does a moment estimation step, so we estimate the Future Past crosscurrents of the data and then in the moment matching step we converted and we solve for this Future Past crosscurrent rosette and this has approximately low rank structure which we can leverage by doing a subsequent spectral decomposition step and to finally get our LDS parameters.",
                    "label": 1
                },
                {
                    "sent": "So we in.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimated this algorithm in a couple of experiments, and all these experience we did with the LDS that I already mentioned me that one with across our distributed observations where the rate is given by the exponentiated set variables.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In that case, the algorithm yields consistent estimates of the LDS parameters, so for large datasets, we expect that our algorithm, the Spectra other than, gives good results for the parameters datasets.",
                    "label": 1
                },
                {
                    "sent": "But we were also interested in investigating whether the spectral method also works well on smaller datasets, namely in conjunction with maximum likelihood techniques.",
                    "label": 1
                },
                {
                    "sent": "Basically, by using the spectral parameter estimates as initializers for EM iterations and hopefully avoiding too shallow local minima and slow convergence by already initializing them with some sane parameter estimates.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our experiments, we quantified the goodness of fit of the identified parameters by kind of cross prediction measure where we held out single dimensions of the data and try to predict it given the other dimensions of the data and report the results in terms of variance explained relative to a baseline model where hires of course better here.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically users performance measure instead of the likelihood because we can't compute exactly the likelihood, and this is also reason why we use this goodness of fit measure on the training data as a termination criterion for them.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first set of experiments we we, we tried to identify parameters from artificial datasets which came from the same model.",
                    "label": 0
                },
                {
                    "sent": "Basically datasets were sampled from growth ground truth.",
                    "label": 0
                },
                {
                    "sent": "Personally, dynamical systems and we gave our algorithms already the right latent dimensions, namely 10 latent dimensions and 25 observation dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are the results for our first data set, which was designed to have strong temporal correlations and again the goodness of fit of the identified parameters on the Y axis here and.",
                    "label": 1
                },
                {
                    "sent": "And here we compare our spectral the identified parameters which sit up here with the M iterations with different initializers, namely one with factor analysis, initialized initializer and with a couple of random initializations.",
                    "label": 0
                },
                {
                    "sent": "And we see that the spectral methods identified for the spectral parameters do a lot better than I am with a different initializers, and especially in this case the spectral parameters couldn't be further refined.",
                    "label": 0
                },
                {
                    "sent": "By YM steps.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we find kind of similar results for datasets which had less temporal correlations with stronger instantaneous correlations.",
                    "label": 0
                },
                {
                    "sent": "In this case, the spectral parameters could be further improved by a couple of EM iterations, but compared to the other initializers for them, our method works yields better parameter estimates with a smaller number of iterations.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we also found that there are datasets where the spectrum method doesn't work as well, and apparently we found that these are datasets with very weak temporal correlations, and in that case factor analysis turns out to be the better initializer, probably because factors already assumed that there is no temporal structure in the data.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and in the next set of experiments we identified LDS parameters from multi electric recordings and these recordings came from motor cortex of macaque monkeys.",
                    "label": 0
                },
                {
                    "sent": "They consisted of 86 dimensions, putative single neurons, and we built the data at 10 millisecond timings which resulted in rather sparse datasets were only 10% of the bins were non empty, so the rest was just zero.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are the results for rather small data set consisting just of hundreds seconds of recordings with 86 dimensions.",
                    "label": 1
                },
                {
                    "sent": "And here we also see that spectrum methods already give a bit better parameter estimates, but converge rather fast compared to EM iterations.",
                    "label": 1
                },
                {
                    "sent": "Interact with factor analysis with random initializations are with.",
                    "label": 0
                },
                {
                    "sent": "With Gaussian spectral parameter estimates.",
                    "label": 0
                },
                {
                    "sent": "And this margin to competing methods grows.",
                    "label": 0
                },
                {
                    "sent": "The more data you give to the algorithm, for example.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the results for a data set of consisting of 500 seconds with only the 40 most active neurons, and here spectral analysis as spectral initialization does better.",
                    "label": 0
                },
                {
                    "sent": "You might wonder why these traces kind of terminate them early, but this is due to our termination criterion with respect to this goodness of fit measure.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This month, so the most extreme results are the best results for the spectral methods we find when we use all the available data and Amy consisting of 800 seconds and all the neurons.",
                    "label": 0
                },
                {
                    "sent": "And here spectral initialization Collea outperforms the other initializers by quite a margin, and also compared for example to random initializations, which gets quite good parameter estimates.",
                    "label": 0
                },
                {
                    "sent": "In the end we save alot of em iterations and.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this application, this translates directly to.",
                    "label": 0
                },
                {
                    "sent": "Quite substantial computational savings.",
                    "label": 0
                },
                {
                    "sent": "Basically because Yum iterations tend to be very costly and the spectral spectral initialization step is basically cheaper than a single EM iteration.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are a couple of extensions that we played around with and one that we could also do the spectral estimation trick for is.",
                    "label": 0
                },
                {
                    "sent": "Then we can account also for external driving input.",
                    "label": 1
                },
                {
                    "sent": "These are might be external covariates that you want to model with with whose influence that you want to model until your time series.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here this is.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A simple model where they feed linearly or couple linearly to the latent States and into the zed variables.",
                    "label": 0
                },
                {
                    "sent": "And if we assume that these these external driving inputs also jointly normally distributed, then we can do basically the spectral identification step because we can do the joint moment matching trick that we did before now jointly on U&Y to get the joint moments of UN set, from which we can use standard spectral methods to identify this externally driven.",
                    "label": 1
                },
                {
                    "sent": "LDS system.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me quickly wrap up here, so I presented a simple spectral method for identifying LDS parameters from generalized linear observations, and this method was basically a simple Stew, two stage method where we in the first step we did a moment matching step together, estimate of the Future Past crosscurrents matrix of the set variables, followed by a standard spectral system identification step for LDS methods LDS models.",
                    "label": 0
                },
                {
                    "sent": "So and we.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that given substantial amount, given enough training data, sufficiently structured datasets, we found that this method gave good parameter results.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it also worked.",
                    "label": 0
                },
                {
                    "sent": "It also turned out to be useful for other datasets, for example of Multielectrode recordings where it provided good initializers for M iterations.",
                    "label": 0
                },
                {
                    "sent": "Basically, reducing the number of iterations that we had to do in until convergence yielding computational savings and also yielding to better parameter estimates.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me finish by thanking Christianise left from Stanford for generously sharing the data without us and also by acknowledging our funding sources.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for attention.",
                    "label": 0
                }
            ]
        }
    }
}