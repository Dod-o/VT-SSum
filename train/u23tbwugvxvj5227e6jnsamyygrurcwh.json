{
    "id": "u23tbwugvxvj5227e6jnsamyygrurcwh",
    "title": "Advances in Cross-Lingual Syntactic Transfer",
    "info": {
        "author": [
            "Ryan McDonald, Google, Inc."
        ],
        "published": "Jan. 11, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Computational Linguistics->Machine Translation"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_mcdonald_syntactic_transfer/",
    "segmentation": [
        [
            "I'm going to talk about cross lingual syntactic transfer, so this is the idea of taking resource rich languages like English, English and their resources and trying to use those to parse resource poor languages, and there's going to be kind of an underlying theme."
        ],
        [
            "Kind of an empirical argument against unsupervised parsing methods.",
            "There's a large body of literature on unsupervised parsing, and I'm going to try to make a point that, at least from an empirical perspective, we can."
        ],
        [
            "Do a lot better.",
            "So just to kind of get you up to speed what is parsing?",
            "Basically we take a sentence we want to predict the labeled or unlabeled head and modifier relations.",
            "So these are things like John is the subject of the verb hit.",
            "The preposition modifies the verb and not the noun.",
            "Here we're only going to do with unlabeled trees, so we can compare to previous work.",
            "But typically we want to predict labeled trees.",
            "We also."
        ],
        [
            "I want to predict part of."
        ],
        [
            "Each tag, but I'm going to assume that part of speech tag, part of speech tags are given as input to the system, which I think is actually an OK assumption.",
            "If you have a problem with that, you can ask me about it in the question.",
            "It's a common assumption that's made so you know, for English we can parse simple sentences.",
            "We can parse more complex sentences like the one down below, and we can parse them with fairly good quality.",
            "And because of this, you know we can make contributions to translation, information extraction, voice actions, and."
        ],
        [
            "Tanala sis.",
            "OK, so throughout the talk the parsing model that I'm really going to be dealing with are these well Colin MST parser for historic reasons, but they're basically CRF dependency parsing models, so we have a log linear model, and like any structured prediction task, we have to factorize the parameters and the features.",
            "Here we're just going to factorize them by arcs in the tree, so that just means that a feature can only look at the head and modifier of an arc.",
            "But any context of the input sentence.",
            "That it wants in the kind of features we're going to look at.",
            "Don't worry about that on the right here, but it's basically rich properties of the Ark and the contextual inputs.",
            "We're just going to rain.",
            "This will be F, so from a modeling perspective, kind of a standard straightforward vanilla model."
        ],
        [
            "So our goal is to build parsers for all the world's languages, and so we want a system where basically you can enter a sentence in any language and what you're going to get out is a syntactic representation of that sentence, and ideally it will be using the same kind of syntactic formalism.",
            "So in this case this is dependencies and Stanford style representation."
        ],
        [
            "So towards this end, we've built not ask.",
            "The community has built true banks in roughly 20 languages, maybe a little bit more so, at least for these 20 languages, we can build supervised systems.",
            "A few problems here.",
            "The quality varies from extremely good to basically useless.",
            "The annotation schemes differ across language, so we can't actually have this uniform syntactic output that we want, and probably most importantly, there's a lot of high profile languages that just aren't covered by this.",
            "You know these 20 languages like Southeast Asian languages, all the African languages, and others."
        ],
        [
            "So one solution that people have proposed to this is to look at unsupervised methods, and though people have studied this.",
            "For a couple of decades at least, probably the starting point of the modern methods is Dan Klein's thesis.",
            "About 10 years ago on the DMV model, and since then, there's been a flurry of work trying to improve that model, typically focused on English.",
            "But even after that, the unsupervised accuracies, the accuracies of these models are still significantly lower than the supervised accuracy, sometimes half so.",
            "For some languages, we can parse a supervised with 80% accuracy, but the unsupervised model has only 30 or 40%.",
            "Accuracy, so these kinds of methods really aren't practical."
        ],
        [
            "For most applications, So what I'm going to talk about is this idea of syntactic transfer, right?",
            "And it's quite simple.",
            "We have an English treebank, let's say or any language, and then we're going to do some kind of either data or modeling magic, and we're going to build a transfer for another language.",
            "Sorry, build a parser for another language, and in this kind of idea has gone back, at least to Rebecca has work in 2005 and earlier.",
            "David Jerowski for part of speech."
        ],
        [
            "And the typical setup that people have is I have some English resources and I want to create a parser for some other language, right?",
            "And people typically do it by relying on parallel data.",
            "So here we have.",
            "You know you're a pile sentence.",
            "Align word aligned data and what people typically do will be departs.",
            "the English side of that data and project the annotation across and try to learn some target language information and they might do this directly or by using that information as some kind of prior on unsupervised model or something of that nature.",
            "What I'm going to look at is this idea of direct transfer, which is basically to take an English parser and then run it on a foreign language directly, which might sound a bit ridiculous, but I'm going to.",
            "Kind of give you some arguments for why it isn't, but both of these ideas are orthogonal, and actually there's a lot of work that shows that if you have objective functions, you know maximizing direct transfer coupled with some regularization on parallel data information that you get optimal performance.",
            "So I'm just going to talk about the below aspect just to keep things simple."
        ],
        [
            "OK, so so why is this idea of direct transfer not completely ridiculous?",
            "So basically the idea is that what we want to do is take an English treebank and get rid of all information that is just English specific.",
            "So there's a lot of features in the data that you can imagine that would transfer across languages right?",
            "So the language specific information is really the words itself."
        ],
        [
            "So you know originally when I have this tree bank, I'm going to train some parser, but now what I can do is simply just."
        ],
        [
            "Nor the words in the tree bank.",
            "And then I basically have these part of speech tag sequences and I can retrain the exact same parser.",
            "It just has no word features and I'm going to get we're going to call the Lexicalized parser out of it apart, so that doesn't look at any word."
        ],
        [
            "Identities"
        ],
        [
            "And then I'm basically going to run that parser on a foreign language.",
            "So even if you don't speak Greek, you should be able to parse this sentence simply by looking at the part of speech tags, because now instead to go to verbs and determiners tend to go to nouns as modifiers, right?",
            "So if I"
        ],
        [
            "Of the sentence, I can just ignore the war."
        ],
        [
            "Have it run the parser on this sentence and get a representation for the sentence."
        ],
        [
            "Right, so the question here is what kind of cross lingual are universal information can be conditioned these parsers on and there's been a bunch of work here using part of speech tags, cross lingual clusters, dictionaries, things of this nature that have all sort of additively."
        ],
        [
            "True performance.",
            "So here is the performance of an English parser on a number of different languages."
        ],
        [
            "Key thing to note here is that.",
            "Not surprisingly, at the right hand of this plot is Indo European languages.",
            "English is an Indo European language, so the word order of English is closer to these languages than the set."
        ],
        [
            "On the left.",
            "So over here we have things like Vasque Arabic, Japanese, Turkish and Hungarian.",
            "These non indo European languages and actually the accuracy is much lower, so we're not capturing things like word or English.",
            "Yeah, so the lexicalized it's about 8384% yeah.",
            "Right and so on average we're about 52% across languages, which may not seem like.",
            "You know a major advancement."
        ],
        [
            "But when you compare this to the state of the art in unsupervised systems, so we have a weekly unsupervised system here where the model is given some information about basic linguistic information from a human and over here are sort of purely unsupervised methods.",
            "So this is that's good question.",
            "This is just accuracy.",
            "So this is the percentage of words that modify the right head, right?",
            "So every word in the tree has to have at most one head or has exactly 1 head in the tree, and so we're just measuring that accuracy.",
            "Across all languages.",
            "Same.",
            "They all agree that the main verb should know, so this is a problem, right?",
            "So there are, you know certain dependencies, like every tree bank.",
            "Basically adjectives modify nouns.",
            "But some treebanks in coordination phrase the conjunct is actually the head and others the conjunction and things like that, right?",
            "And so we're basically ignoring those sorts of distinctions.",
            "We have built some data that's much more uniform across languages and basically what you can see, at least on these systems here.",
            "All of these go up by about 5 to 10% as you can expect.",
            "These are just publicly released datasets, which makes it easier to compare Brian on the right.",
            "Here we have some purely well purely unsupervised system in the fact that they rely on part of speech tags, but they have no other human input except for input into the model itself and we can see it."
        ],
        [
            "Like this really naive thing of just training a parser on English and running it on these languages were already doing significantly better than a lot of these methods."
        ],
        [
            "OK, so that sort of begs the question like can we do much better than this?",
            "And the first thing people tried was to not just rely in English.",
            "So as I mentioned before, we have these 20 treebanks so why not just use all of them and there was a few papers that looked at this multi source transfer problem a couple of years ago.",
            "Some more."
        ],
        [
            "Flex and others.",
            "I'll just point you to the simplest one, which is one of the things we did and that's just to concatenate all the treebanks together and then train a model, right?",
            "It's quite simple.",
            "You get an improvement in average accuracy, so that's about a 3% improvement, which is statistically significant.",
            "This generally works across all languages, but you can see here.",
            "Not surprisingly, things like Arabic Basque, Japanese get large improvements because now they're not only relying on English, there actually relying on a more diverse set of languages whose.",
            "Word order is maybe not closer to Arabic, but may have more in common with Arabic.",
            "At Lee."
        ],
        [
            "In aggregate.",
            "Just yeah.",
            "Things like conjunctions you take care of that now we just ignore it.",
            "Yeah, this is like being doing.",
            "This is like you should be able to implement this.",
            "Download MST parser multiple parts in about 30 minutes.",
            "You'll have this system, yeah?",
            "We do actually, there are."
        ],
        [
            "Cases where we lose.",
            "So Chinese we do lose Turkish, we do lose.",
            "But you know, I think a lot of these cases are this sort of annotation differences across treebanks where these are annotated more like English and by mixing in other annotation schemes were kind of pulling the models away from the English annotations.",
            "I suspect.",
            "I don't know if that's actually the case.",
            "I understand the problem is the naming, so going up or down so the structure of behaviors.",
            "The structure well.",
            "No, I guess the answer there is no, I mean.",
            "It's pretty clear why these are going up right?",
            "So for Arabic.",
            "So it's a verb, subject object language, right?",
            "It just sees nothing like that in English, but it turns out that even though Greek is primarily subject verb object, you do see verb subject objects right.",
            "And so if you look at the accuracy of subject, object, verb, attachments for Arabic, it has gone up dramatically WHI.",
            "These have dropped.",
            "Actually, I don't have a good answer for that."
        ],
        [
            "OK, so with this observation.",
            "Nasim at all and her colleagues at MIT.",
            "Realize that you could do something much more powerful healing, so actually some of the previous work had tried to wait the different treebanks depending on what the target language was, but it turned out that didn't really help much more than just a uniform weighting across treebanks.",
            "So what here and the team at all did is they decided to propose a model where they selectively share from true banks.",
            "So if I'm trying to parse a language, it's a little bit simplistic for me to say that while I'm closer to this source language or this source language in aggregate.",
            "What I want to say is that.",
            "You know for certain constructions, I'm closer to one language, and for other constructions I'm closer to another language, so I think a good example would be Arabic, which is prepositional, right?",
            "So it would be closer to English if you're only looking at preposition attachments.",
            "But it's noun adjective, so it would be closer to romance languages if you were interested in how it adjectives attached announce.",
            "So what you want to do is take advantage of that information at a much more fine grained level.",
            "Then the tree bank level."
        ],
        [
            "So they actually proposed this generative model that has two components.",
            "The first component is simply selectional attachments, right?",
            "So this part is very simple and it's basically modeling you know, adjectives modify nouns, right?",
            "This is universal information.",
            "Every language that has adjectives and nouns.",
            "This is true.",
            "It's the definition of adjectives.",
            "And so it's basically going to learn that from all."
        ],
        [
            "Language is the selective sharing part has to do with ordering.",
            "So the second part of the model determines two things.",
            "The order of the Modifyers relative to the head and then the order on each side of the head.",
            "The order of the Modifyers relative to themselves.",
            "And they learn this model not by treating all data.",
            "Equivalently, they actually I parameters in the model that can are conditioned on properties from the World Atlas of Language.",
            "So this is a database that linguists have populated that basically tell us that for French adjectives modify nouns to the right.",
            "Arabic is a verb, subject object, language.",
            "Japanese is a subject, object, verb, language.",
            "Things like this.",
            "So this part where we actually determining the order.",
            "Of the Modifyers relative to the head, we're going to condition on or Naseem at all conditions on the World Atlas of Language to tie the right target language parameters to the right source language parameters.",
            "So here you really sharing from different subcomponents of each of the source languages."
        ],
        [
            "Right, and so this is the green line is the same at all, and the blue line is just this naive concatenate all the data.",
            "And there's."
        ],
        [
            "Few things we can see here.",
            "First is an average.",
            "It's better, so there's a 4% improve."
        ],
        [
            "ENT I think the remarkable thing here is when you look at the non Indo European languages, right?",
            "So you get this huge jump for a lot of languages.",
            "You know almost 23% for Japanese absolute.",
            "So clearly this model is capturing something important here.",
            "But there isn't."
        ],
        [
            "Outside, not to the idea of selective sharing, but to the model itself.",
            "If you look at the end of the European languages, the discriminative feature feature rich parser still outperforms the generative model.",
            "And this is partly because the source languages are very Indo European heavy, so having all those rich features from which we can learn is a good thing.",
            "For these languages, it can learn really fine grained feature correspondences in that model and it will still outperform the generative."
        ],
        [
            "So, so that's one issue, and in general the generative model that seem at all proposed was nice because it allowed them to factor the selectional versus ordering components of the model and share things in different ways.",
            "But it's really an impoverished model, and if you look at just the supervised accuracy of that generative model, so it's a model, you can train it with supervised data.",
            "It gets about 67% on average.",
            "But if you look at, you know a simple discriminative.",
            "MST parser style model.",
            "It gets about 84% on accurate on average, so there's a big gap in supervised accuracies here."
        ],
        [
            "So this kind of begs the question you know, can we do similar things for discriminative models where we try to adapt these models to target languages?",
            "And so I'm going to look at two different things to different ways of adapting these models.",
            "One is trying to take some insights from the Naseem.",
            "It all work in the selective sharing sort of idea.",
            "And then I'm also going to look at this notion of re lexical Ising these models.",
            "These models are purely D lexicalized.",
            "We only have features on part of speech tags.",
            "We don't have any features that can capture things like lexical preferences or something like that, so I'm going to try to relax guys these models through something we call ambiguity preserving training and I'll get to that in a.",
            "In a bit.",
            "Alright."
        ],
        [
            "OK."
        ],
        [
            "So this is discriminative selective sharing very briefly, so the baseline here is just the regular concatenate all the treebanks, use a discriminative model with a bunch of features that gets about 55% active."
        ],
        [
            "See.",
            "What we can do is, you know, the main problem here is that all of these features are conjoined with the notion of direction.",
            "So what side of the head the modifier occurs with?",
            "So we can get rid of all directional features.",
            "And this not only includes the explicit direction, but also these part of speech ngram context features, because they weekly do encode the ordering of words in the sentence.",
            "So this model here basically has three features.",
            "What's the head part of speech tag?",
            "What's the modifier part of speech tag?",
            "And what's the direction of attachment, right?",
            "And you can see we don't do so bad.",
            "It's about 52% accuracy, but we do.",
            "We do see a drop.",
            "We"
        ],
        [
            "Then start to share some parameters based on these World Atlas of Language properties.",
            "So the first thing we do is for particular head modifier pairs, right?",
            "So let's say preposition now, right?",
            "So where the head part of speech Tag is a preposition.",
            "The modifier is a noun, so these are cases.",
            "That for the adposition attachment.",
            "So when that feature fires, will actually can join that feature with both the direction but also a walls property.",
            "So then I'll give you an example on the next slide, but the idea is for particular features where we know something from the World Atlas of Language we can actually can join that feature with that property, and that way sort of fractured the feature space in two languages that have one particular value for that property.",
            "In languages that may have another value.",
            "And so then we're going to actually going to share those parameters between related."
        ],
        [
            "Images.",
            "In doing that, we get a nice big improvement and then the last thing we can do is throw in all the full feature set again.",
            "But here we want to fracture the feature space again, but we're going to do it by language family, so if you're in the same language family as one of the particular source languages, you're going to share all this entire rich feature set with them and all the languages in that family will share those particular parameters."
        ],
        [
            "Right?"
        ],
        [
            "So here is just an example of the first case right.",
            "We have direction when the head tag fires preposition and the modifier tag fires noun, we're going to join that with property 85 A of the World Atlas of Language, which just turns out to be the order of ad position.",
            "So this will fire the same feature will fire for languages that have the same property here and similar for language family we just say for every single rich feature just can join that with the language family and we use a really simple.",
            "Language family.",
            "Basically 2."
        ],
        [
            "Here.",
            "OK, so here are the results again.",
            "The two on the right are taking this notion of selective sharing.",
            "There's a couple of things we can see here.",
            "From the the sort of base.",
            "Just concatenate everything model we get.",
            "Oh no, we get 7% improvement absolute with just being more careful about how we construct these features.",
            "Not being so naive about it and we are now outperforming the model of naseam at all the generative model."
        ],
        [
            "And if we look for language."
        ],
        [
            "On average, we're doing better we."
        ],
        [
            "Caught up on all the non Indo European languages and even surpassed on a few."
        ],
        [
            "And on the Indo European languages we maintained the lead and we even for you know for Catalan.",
            "For instance, we get a nice big improvement here.",
            "So even the Indo European languages can get a benefit from this."
        ],
        [
            "OK, so I want to talk a little bit about re lexicalization so this is the average of this new discriminative shared model.",
            "This is a supervised two supervised versions of that Model 1 where we have no lexical features.",
            "Another where we have lexical features and we can see clearly we can cut about 25% of the error off by just adding lexical features in, so we've thrown out a pretty solid source of information.",
            "Since we've gone to the lexical as models, so we want to get that back."
        ],
        [
            "Can one work along these lines?",
            "Zeeman and Resnick looked at the simple idea of basically taking the transfer model, running it on a large amount of unlabeled data and then re lexical Ising it by just training a lexicalized model on that data, and the idea, like self training in general, is that if the data is large enough, hopefully you can get some averaging effect and things will in."
        ],
        [
            "Move we got slight improvements doing that and zoom in and Resnik basically observed the same thing.",
            "For some languages.",
            "This help for other languages it heard it was fairly inconsistent."
        ],
        [
            "But you can observe that this model here the base model, the transfer model.",
            "If you look at the K best output of this model, let's say K = 100, there's a lot of really good ambiguity at the top, as you would expect.",
            "This case is 100 and it's about 10% absolute better.",
            "So we want to take advantage of that ambiguity."
        ],
        [
            "And to give you an idea of how we might be able to take advantage of it if we want to transfer from English to Japanese, let's say right.",
            "So Japanese is a subject, object, verb, language.",
            "Maybe we have a bunch of examples like this, so maybe this word a in Japanese.",
            "The morphology of that word might tell us that it's always a subject or something like that.",
            "It should always attach to a verb.",
            "But if I'm only looking at part of speech tags and transferred from an Indo European language.",
            "For this sentence, here I'm likely going to predict that a modifies the noun next to it in some kind of compound noun.",
            "Representation right?",
            "Because for English, even though it's possible that you can have two nouns modifying a verb to the left of the verb in this particular case, it's much more likely that this is just a single now with two words like John Smith ran kind of construction, but since it is."
        ],
        [
            "Possible for the."
        ],
        [
            "We see in English or in into European language, two nouns modify a verb to the left.",
            "I will have some marginal probability on the arc that this now and is attached to this verb right, and so hopefully if I have a bunch of examples where a attaches to the verb because it's less of an ambiguous situation, I should be able to back off and learn.",
            "In this situation that they should really attached to the verb.",
            "So what I want to do is inject."
        ],
        [
            "That kind of ambiguity from the base model into the learning process."
        ],
        [
            "So we call this ambiguity, preserving self training.",
            "I don't know if that's really a great title, maybe ambiguity, aware self training, 'cause it actually doesn't preserve the ambiguity at the end of the day."
        ],
        [
            "It's this general idea that we want to encode the base model ambiguity while training, and so we're going to do that by defining the set of trees, called it Y~ and this is going to represent the set of trees that the model thought might be possible.",
            "Maybe it wasn't that herbs tree for a particular target language sentence, but it had some reasonable belief in this."
        ],
        [
            "Free."
        ],
        [
            "We want this set too.",
            "Have a number of different properties.",
            "First, it should be big enough that we encode enough good trees from the base model.",
            "It's got to be small enough that we can guide the learner because we can just make it the set of all trees.",
            "But then it's just unsupervised learning and we're not going to really learn much there.",
            "But we also want to give the model flexibility to push mass towards any of these possible trees.",
            "One thing we could do is just take the marginal probabilities from the base model, set up some optimization to try to maximize that, But then you're just going to mimic.",
            "Those particular probabilities right?",
            "You want to give the model a little bit more freedom, so we're not going to associate any scores with."
        ],
        [
            "Sharks."
        ],
        [
            "So just to make this more concrete.",
            "For every modifier M. For every possible head, so that constitutes an arc, we're just going to calculate the marginal probability of that arc from the model.",
            "We're going to sort the arcs by."
        ],
        [
            "That probability we're going to define this set a for a modifier, which is a set of possible arcs for that modifier, and we do that by simply going through this.",
            "Set of sorted arcs until we pass some maths threshold so you know maybe until we get 90% of the marginal probability mass."
        ],
        [
            "That's a hyperparameter from these sets of RX.",
            "We just let Y told to be the set of arcs are set of trees that are dry."
        ],
        [
            "Available from this arc set and then we just run a simple optimization where it's the regular CRF optimization, but we just marginalized out over this set of trees.",
            "It's quite so."
        ],
        [
            "So in standard self training, why Tilda would just be a single person, so you wouldn't have to do any."
        ],
        [
            "Here it be convex.",
            "It's not convex here.",
            "We still use LB FGS to modify it because you can, and this objective has been used in the past and parsing for Stefan Riezler used it in this case where he had a a LFG grammar that gave something bigger.",
            "A set of parses for sentence, and then he used this objective to push mass towards those parses.",
            "So this is not really a new objective.",
            "Right so."
        ],
        [
            "So if this model here is the one on the right, so we took a bunch of target language data.",
            "We ran this procedure on that data and what you can see is that you know now over the base model.",
            "We're getting almost 10% improvement.",
            "We get about four to 5% improvement over the best Naseem models.",
            "These are the generative models and there is even one where they use target language, unlabeled data with an M procedure.",
            "And since it's a generative model, that's fairly straightforward and we do get an improvement over the base MST selective sharing model.",
            "It's 2%, but it's consistent, so every model, every language we do better at some languages less than others, and some languages like Turkish, we end up getting about a 5% absolute improvement."
        ],
        [
            "OK, so the last thing is that.",
            "Right, this idea of training a model with some ambiguous set of trees doesn't necessarily have to restrict itself to a single base model, right?",
            "We could have multiple different based models that we could use to construct the set of arcs in some kind of ensemble training algorithm, and so we."
        ],
        [
            "Basically do the exact same thing."
        ],
        [
            "Before we.",
            "Instead of having a single base model, we just have K different based parsers."
        ],
        [
            "Then we define the final arc set for each modifier to simply be the union of all the base parsers sets, right?",
            "Very simple, we tried different things like intersection or things like that, but union just seemed to work the best.",
            "And really, this can be any source of information, right?",
            "It doesn't have to be a base parser.",
            "It could also be human generated constraints or anything you can imagine.",
            "And these could also be incorporated inside a prior as opposed to explicitly in the objective."
        ],
        [
            "Alright, so the base parsers we used was the Naseem model and the other MST parser, and not surprisingly like with most ensemble systems you get an improvement.",
            "The nice thing about this ensemble system is it's at training so you still learn a single model at testing time.",
            "You're not actually running all these models, you're just running the one model you learned on the unlabeled data for that."
        ],
        [
            "Target language.",
            "And so to put this in perspective, the best unsupervised accuracy over the same set of languages is about 20% lower than this model.",
            "And really we haven't done anything super complicated.",
            "We've taken a CRF model, concatenated a bunch of data, define some features in.",
            "A semi clever way and then ran a simple self training training algorithm on some unlabeled data and we're able to get 65.4% and this is an all sentence length is not restricted to like 10 or."
        ],
        [
            "Nothing like that.",
            "Alright, so some final thoughts.",
            "You know there's been just a lot of work.",
            "In syntactic transfer over the past few years that has made significant progress.",
            "So we're now about 20% absolute better than the best models in unsupervised parsing.",
            "This is an underestimate in terms of treebank divergences on the test set."
        ],
        [
            "No, for a lot of languages we're probably about 5 to 10% below what the true accuracy is, but that's probably also true for unsupervised parsing as well."
        ],
        [
            "And as I said, these are simple models with just feature engineering, so you know I've ragged on unsupervised parsing a little bit here.",
            "You know, So what?",
            "What is the situation there?",
            "I would say from an engineering perspective, if you want to build a model for some language amotivation that you need to, you can't assume anything and you have to learn a model only from the data.",
            "Seems a little misguided to me.",
            "We have a lot of information and that's either from treebanks from other languages or from databases that linguists have populated, and that's really what we should be doing.",
            "From a psycholinguistic perspective, I don't know, you know humans are able to learn language without English treebanks.",
            "But you know whether the models we study in unsupervised parsing tell us anything about that.",
            "I really don't know."
        ],
        [
            "OK, so let's take that."
        ],
        [
            "Thank you guys."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about cross lingual syntactic transfer, so this is the idea of taking resource rich languages like English, English and their resources and trying to use those to parse resource poor languages, and there's going to be kind of an underlying theme.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of an empirical argument against unsupervised parsing methods.",
                    "label": 0
                },
                {
                    "sent": "There's a large body of literature on unsupervised parsing, and I'm going to try to make a point that, at least from an empirical perspective, we can.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do a lot better.",
                    "label": 0
                },
                {
                    "sent": "So just to kind of get you up to speed what is parsing?",
                    "label": 0
                },
                {
                    "sent": "Basically we take a sentence we want to predict the labeled or unlabeled head and modifier relations.",
                    "label": 0
                },
                {
                    "sent": "So these are things like John is the subject of the verb hit.",
                    "label": 0
                },
                {
                    "sent": "The preposition modifies the verb and not the noun.",
                    "label": 0
                },
                {
                    "sent": "Here we're only going to do with unlabeled trees, so we can compare to previous work.",
                    "label": 0
                },
                {
                    "sent": "But typically we want to predict labeled trees.",
                    "label": 0
                },
                {
                    "sent": "We also.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to predict part of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Each tag, but I'm going to assume that part of speech tag, part of speech tags are given as input to the system, which I think is actually an OK assumption.",
                    "label": 0
                },
                {
                    "sent": "If you have a problem with that, you can ask me about it in the question.",
                    "label": 0
                },
                {
                    "sent": "It's a common assumption that's made so you know, for English we can parse simple sentences.",
                    "label": 0
                },
                {
                    "sent": "We can parse more complex sentences like the one down below, and we can parse them with fairly good quality.",
                    "label": 0
                },
                {
                    "sent": "And because of this, you know we can make contributions to translation, information extraction, voice actions, and.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tanala sis.",
                    "label": 0
                },
                {
                    "sent": "OK, so throughout the talk the parsing model that I'm really going to be dealing with are these well Colin MST parser for historic reasons, but they're basically CRF dependency parsing models, so we have a log linear model, and like any structured prediction task, we have to factorize the parameters and the features.",
                    "label": 0
                },
                {
                    "sent": "Here we're just going to factorize them by arcs in the tree, so that just means that a feature can only look at the head and modifier of an arc.",
                    "label": 0
                },
                {
                    "sent": "But any context of the input sentence.",
                    "label": 0
                },
                {
                    "sent": "That it wants in the kind of features we're going to look at.",
                    "label": 0
                },
                {
                    "sent": "Don't worry about that on the right here, but it's basically rich properties of the Ark and the contextual inputs.",
                    "label": 0
                },
                {
                    "sent": "We're just going to rain.",
                    "label": 0
                },
                {
                    "sent": "This will be F, so from a modeling perspective, kind of a standard straightforward vanilla model.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our goal is to build parsers for all the world's languages, and so we want a system where basically you can enter a sentence in any language and what you're going to get out is a syntactic representation of that sentence, and ideally it will be using the same kind of syntactic formalism.",
                    "label": 0
                },
                {
                    "sent": "So in this case this is dependencies and Stanford style representation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So towards this end, we've built not ask.",
                    "label": 0
                },
                {
                    "sent": "The community has built true banks in roughly 20 languages, maybe a little bit more so, at least for these 20 languages, we can build supervised systems.",
                    "label": 0
                },
                {
                    "sent": "A few problems here.",
                    "label": 0
                },
                {
                    "sent": "The quality varies from extremely good to basically useless.",
                    "label": 1
                },
                {
                    "sent": "The annotation schemes differ across language, so we can't actually have this uniform syntactic output that we want, and probably most importantly, there's a lot of high profile languages that just aren't covered by this.",
                    "label": 1
                },
                {
                    "sent": "You know these 20 languages like Southeast Asian languages, all the African languages, and others.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one solution that people have proposed to this is to look at unsupervised methods, and though people have studied this.",
                    "label": 0
                },
                {
                    "sent": "For a couple of decades at least, probably the starting point of the modern methods is Dan Klein's thesis.",
                    "label": 0
                },
                {
                    "sent": "About 10 years ago on the DMV model, and since then, there's been a flurry of work trying to improve that model, typically focused on English.",
                    "label": 0
                },
                {
                    "sent": "But even after that, the unsupervised accuracies, the accuracies of these models are still significantly lower than the supervised accuracy, sometimes half so.",
                    "label": 0
                },
                {
                    "sent": "For some languages, we can parse a supervised with 80% accuracy, but the unsupervised model has only 30 or 40%.",
                    "label": 0
                },
                {
                    "sent": "Accuracy, so these kinds of methods really aren't practical.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For most applications, So what I'm going to talk about is this idea of syntactic transfer, right?",
                    "label": 1
                },
                {
                    "sent": "And it's quite simple.",
                    "label": 0
                },
                {
                    "sent": "We have an English treebank, let's say or any language, and then we're going to do some kind of either data or modeling magic, and we're going to build a transfer for another language.",
                    "label": 0
                },
                {
                    "sent": "Sorry, build a parser for another language, and in this kind of idea has gone back, at least to Rebecca has work in 2005 and earlier.",
                    "label": 1
                },
                {
                    "sent": "David Jerowski for part of speech.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the typical setup that people have is I have some English resources and I want to create a parser for some other language, right?",
                    "label": 0
                },
                {
                    "sent": "And people typically do it by relying on parallel data.",
                    "label": 1
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "You know you're a pile sentence.",
                    "label": 0
                },
                {
                    "sent": "Align word aligned data and what people typically do will be departs.",
                    "label": 0
                },
                {
                    "sent": "the English side of that data and project the annotation across and try to learn some target language information and they might do this directly or by using that information as some kind of prior on unsupervised model or something of that nature.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to look at is this idea of direct transfer, which is basically to take an English parser and then run it on a foreign language directly, which might sound a bit ridiculous, but I'm going to.",
                    "label": 1
                },
                {
                    "sent": "Kind of give you some arguments for why it isn't, but both of these ideas are orthogonal, and actually there's a lot of work that shows that if you have objective functions, you know maximizing direct transfer coupled with some regularization on parallel data information that you get optimal performance.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to talk about the below aspect just to keep things simple.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so why is this idea of direct transfer not completely ridiculous?",
                    "label": 0
                },
                {
                    "sent": "So basically the idea is that what we want to do is take an English treebank and get rid of all information that is just English specific.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of features in the data that you can imagine that would transfer across languages right?",
                    "label": 0
                },
                {
                    "sent": "So the language specific information is really the words itself.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know originally when I have this tree bank, I'm going to train some parser, but now what I can do is simply just.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nor the words in the tree bank.",
                    "label": 0
                },
                {
                    "sent": "And then I basically have these part of speech tag sequences and I can retrain the exact same parser.",
                    "label": 0
                },
                {
                    "sent": "It just has no word features and I'm going to get we're going to call the Lexicalized parser out of it apart, so that doesn't look at any word.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Identities",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I'm basically going to run that parser on a foreign language.",
                    "label": 0
                },
                {
                    "sent": "So even if you don't speak Greek, you should be able to parse this sentence simply by looking at the part of speech tags, because now instead to go to verbs and determiners tend to go to nouns as modifiers, right?",
                    "label": 0
                },
                {
                    "sent": "So if I",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the sentence, I can just ignore the war.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have it run the parser on this sentence and get a representation for the sentence.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so the question here is what kind of cross lingual are universal information can be conditioned these parsers on and there's been a bunch of work here using part of speech tags, cross lingual clusters, dictionaries, things of this nature that have all sort of additively.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "True performance.",
                    "label": 0
                },
                {
                    "sent": "So here is the performance of an English parser on a number of different languages.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Key thing to note here is that.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, at the right hand of this plot is Indo European languages.",
                    "label": 0
                },
                {
                    "sent": "English is an Indo European language, so the word order of English is closer to these languages than the set.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the left.",
                    "label": 0
                },
                {
                    "sent": "So over here we have things like Vasque Arabic, Japanese, Turkish and Hungarian.",
                    "label": 0
                },
                {
                    "sent": "These non indo European languages and actually the accuracy is much lower, so we're not capturing things like word or English.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the lexicalized it's about 8384% yeah.",
                    "label": 0
                },
                {
                    "sent": "Right and so on average we're about 52% across languages, which may not seem like.",
                    "label": 0
                },
                {
                    "sent": "You know a major advancement.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But when you compare this to the state of the art in unsupervised systems, so we have a weekly unsupervised system here where the model is given some information about basic linguistic information from a human and over here are sort of purely unsupervised methods.",
                    "label": 0
                },
                {
                    "sent": "So this is that's good question.",
                    "label": 0
                },
                {
                    "sent": "This is just accuracy.",
                    "label": 0
                },
                {
                    "sent": "So this is the percentage of words that modify the right head, right?",
                    "label": 0
                },
                {
                    "sent": "So every word in the tree has to have at most one head or has exactly 1 head in the tree, and so we're just measuring that accuracy.",
                    "label": 0
                },
                {
                    "sent": "Across all languages.",
                    "label": 0
                },
                {
                    "sent": "Same.",
                    "label": 0
                },
                {
                    "sent": "They all agree that the main verb should know, so this is a problem, right?",
                    "label": 0
                },
                {
                    "sent": "So there are, you know certain dependencies, like every tree bank.",
                    "label": 0
                },
                {
                    "sent": "Basically adjectives modify nouns.",
                    "label": 0
                },
                {
                    "sent": "But some treebanks in coordination phrase the conjunct is actually the head and others the conjunction and things like that, right?",
                    "label": 0
                },
                {
                    "sent": "And so we're basically ignoring those sorts of distinctions.",
                    "label": 0
                },
                {
                    "sent": "We have built some data that's much more uniform across languages and basically what you can see, at least on these systems here.",
                    "label": 0
                },
                {
                    "sent": "All of these go up by about 5 to 10% as you can expect.",
                    "label": 0
                },
                {
                    "sent": "These are just publicly released datasets, which makes it easier to compare Brian on the right.",
                    "label": 0
                },
                {
                    "sent": "Here we have some purely well purely unsupervised system in the fact that they rely on part of speech tags, but they have no other human input except for input into the model itself and we can see it.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this really naive thing of just training a parser on English and running it on these languages were already doing significantly better than a lot of these methods.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that sort of begs the question like can we do much better than this?",
                    "label": 0
                },
                {
                    "sent": "And the first thing people tried was to not just rely in English.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned before, we have these 20 treebanks so why not just use all of them and there was a few papers that looked at this multi source transfer problem a couple of years ago.",
                    "label": 0
                },
                {
                    "sent": "Some more.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Flex and others.",
                    "label": 0
                },
                {
                    "sent": "I'll just point you to the simplest one, which is one of the things we did and that's just to concatenate all the treebanks together and then train a model, right?",
                    "label": 0
                },
                {
                    "sent": "It's quite simple.",
                    "label": 0
                },
                {
                    "sent": "You get an improvement in average accuracy, so that's about a 3% improvement, which is statistically significant.",
                    "label": 0
                },
                {
                    "sent": "This generally works across all languages, but you can see here.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, things like Arabic Basque, Japanese get large improvements because now they're not only relying on English, there actually relying on a more diverse set of languages whose.",
                    "label": 0
                },
                {
                    "sent": "Word order is maybe not closer to Arabic, but may have more in common with Arabic.",
                    "label": 0
                },
                {
                    "sent": "At Lee.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In aggregate.",
                    "label": 0
                },
                {
                    "sent": "Just yeah.",
                    "label": 0
                },
                {
                    "sent": "Things like conjunctions you take care of that now we just ignore it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is like being doing.",
                    "label": 0
                },
                {
                    "sent": "This is like you should be able to implement this.",
                    "label": 0
                },
                {
                    "sent": "Download MST parser multiple parts in about 30 minutes.",
                    "label": 0
                },
                {
                    "sent": "You'll have this system, yeah?",
                    "label": 0
                },
                {
                    "sent": "We do actually, there are.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cases where we lose.",
                    "label": 0
                },
                {
                    "sent": "So Chinese we do lose Turkish, we do lose.",
                    "label": 0
                },
                {
                    "sent": "But you know, I think a lot of these cases are this sort of annotation differences across treebanks where these are annotated more like English and by mixing in other annotation schemes were kind of pulling the models away from the English annotations.",
                    "label": 0
                },
                {
                    "sent": "I suspect.",
                    "label": 0
                },
                {
                    "sent": "I don't know if that's actually the case.",
                    "label": 0
                },
                {
                    "sent": "I understand the problem is the naming, so going up or down so the structure of behaviors.",
                    "label": 0
                },
                {
                    "sent": "The structure well.",
                    "label": 0
                },
                {
                    "sent": "No, I guess the answer there is no, I mean.",
                    "label": 0
                },
                {
                    "sent": "It's pretty clear why these are going up right?",
                    "label": 0
                },
                {
                    "sent": "So for Arabic.",
                    "label": 0
                },
                {
                    "sent": "So it's a verb, subject object language, right?",
                    "label": 0
                },
                {
                    "sent": "It just sees nothing like that in English, but it turns out that even though Greek is primarily subject verb object, you do see verb subject objects right.",
                    "label": 0
                },
                {
                    "sent": "And so if you look at the accuracy of subject, object, verb, attachments for Arabic, it has gone up dramatically WHI.",
                    "label": 0
                },
                {
                    "sent": "These have dropped.",
                    "label": 0
                },
                {
                    "sent": "Actually, I don't have a good answer for that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so with this observation.",
                    "label": 0
                },
                {
                    "sent": "Nasim at all and her colleagues at MIT.",
                    "label": 0
                },
                {
                    "sent": "Realize that you could do something much more powerful healing, so actually some of the previous work had tried to wait the different treebanks depending on what the target language was, but it turned out that didn't really help much more than just a uniform weighting across treebanks.",
                    "label": 0
                },
                {
                    "sent": "So what here and the team at all did is they decided to propose a model where they selectively share from true banks.",
                    "label": 0
                },
                {
                    "sent": "So if I'm trying to parse a language, it's a little bit simplistic for me to say that while I'm closer to this source language or this source language in aggregate.",
                    "label": 0
                },
                {
                    "sent": "What I want to say is that.",
                    "label": 0
                },
                {
                    "sent": "You know for certain constructions, I'm closer to one language, and for other constructions I'm closer to another language, so I think a good example would be Arabic, which is prepositional, right?",
                    "label": 0
                },
                {
                    "sent": "So it would be closer to English if you're only looking at preposition attachments.",
                    "label": 0
                },
                {
                    "sent": "But it's noun adjective, so it would be closer to romance languages if you were interested in how it adjectives attached announce.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is take advantage of that information at a much more fine grained level.",
                    "label": 0
                },
                {
                    "sent": "Then the tree bank level.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they actually proposed this generative model that has two components.",
                    "label": 1
                },
                {
                    "sent": "The first component is simply selectional attachments, right?",
                    "label": 1
                },
                {
                    "sent": "So this part is very simple and it's basically modeling you know, adjectives modify nouns, right?",
                    "label": 0
                },
                {
                    "sent": "This is universal information.",
                    "label": 0
                },
                {
                    "sent": "Every language that has adjectives and nouns.",
                    "label": 0
                },
                {
                    "sent": "This is true.",
                    "label": 1
                },
                {
                    "sent": "It's the definition of adjectives.",
                    "label": 0
                },
                {
                    "sent": "And so it's basically going to learn that from all.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Language is the selective sharing part has to do with ordering.",
                    "label": 0
                },
                {
                    "sent": "So the second part of the model determines two things.",
                    "label": 0
                },
                {
                    "sent": "The order of the Modifyers relative to the head and then the order on each side of the head.",
                    "label": 0
                },
                {
                    "sent": "The order of the Modifyers relative to themselves.",
                    "label": 0
                },
                {
                    "sent": "And they learn this model not by treating all data.",
                    "label": 0
                },
                {
                    "sent": "Equivalently, they actually I parameters in the model that can are conditioned on properties from the World Atlas of Language.",
                    "label": 1
                },
                {
                    "sent": "So this is a database that linguists have populated that basically tell us that for French adjectives modify nouns to the right.",
                    "label": 0
                },
                {
                    "sent": "Arabic is a verb, subject object, language.",
                    "label": 0
                },
                {
                    "sent": "Japanese is a subject, object, verb, language.",
                    "label": 0
                },
                {
                    "sent": "Things like this.",
                    "label": 0
                },
                {
                    "sent": "So this part where we actually determining the order.",
                    "label": 0
                },
                {
                    "sent": "Of the Modifyers relative to the head, we're going to condition on or Naseem at all conditions on the World Atlas of Language to tie the right target language parameters to the right source language parameters.",
                    "label": 0
                },
                {
                    "sent": "So here you really sharing from different subcomponents of each of the source languages.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and so this is the green line is the same at all, and the blue line is just this naive concatenate all the data.",
                    "label": 0
                },
                {
                    "sent": "And there's.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Few things we can see here.",
                    "label": 0
                },
                {
                    "sent": "First is an average.",
                    "label": 0
                },
                {
                    "sent": "It's better, so there's a 4% improve.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "ENT I think the remarkable thing here is when you look at the non Indo European languages, right?",
                    "label": 0
                },
                {
                    "sent": "So you get this huge jump for a lot of languages.",
                    "label": 0
                },
                {
                    "sent": "You know almost 23% for Japanese absolute.",
                    "label": 0
                },
                {
                    "sent": "So clearly this model is capturing something important here.",
                    "label": 0
                },
                {
                    "sent": "But there isn't.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Outside, not to the idea of selective sharing, but to the model itself.",
                    "label": 0
                },
                {
                    "sent": "If you look at the end of the European languages, the discriminative feature feature rich parser still outperforms the generative model.",
                    "label": 0
                },
                {
                    "sent": "And this is partly because the source languages are very Indo European heavy, so having all those rich features from which we can learn is a good thing.",
                    "label": 0
                },
                {
                    "sent": "For these languages, it can learn really fine grained feature correspondences in that model and it will still outperform the generative.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so that's one issue, and in general the generative model that seem at all proposed was nice because it allowed them to factor the selectional versus ordering components of the model and share things in different ways.",
                    "label": 0
                },
                {
                    "sent": "But it's really an impoverished model, and if you look at just the supervised accuracy of that generative model, so it's a model, you can train it with supervised data.",
                    "label": 1
                },
                {
                    "sent": "It gets about 67% on average.",
                    "label": 0
                },
                {
                    "sent": "But if you look at, you know a simple discriminative.",
                    "label": 0
                },
                {
                    "sent": "MST parser style model.",
                    "label": 0
                },
                {
                    "sent": "It gets about 84% on accurate on average, so there's a big gap in supervised accuracies here.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this kind of begs the question you know, can we do similar things for discriminative models where we try to adapt these models to target languages?",
                    "label": 1
                },
                {
                    "sent": "And so I'm going to look at two different things to different ways of adapting these models.",
                    "label": 0
                },
                {
                    "sent": "One is trying to take some insights from the Naseem.",
                    "label": 1
                },
                {
                    "sent": "It all work in the selective sharing sort of idea.",
                    "label": 0
                },
                {
                    "sent": "And then I'm also going to look at this notion of re lexical Ising these models.",
                    "label": 0
                },
                {
                    "sent": "These models are purely D lexicalized.",
                    "label": 0
                },
                {
                    "sent": "We only have features on part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "We don't have any features that can capture things like lexical preferences or something like that, so I'm going to try to relax guys these models through something we call ambiguity preserving training and I'll get to that in a.",
                    "label": 0
                },
                {
                    "sent": "In a bit.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is discriminative selective sharing very briefly, so the baseline here is just the regular concatenate all the treebanks, use a discriminative model with a bunch of features that gets about 55% active.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "What we can do is, you know, the main problem here is that all of these features are conjoined with the notion of direction.",
                    "label": 0
                },
                {
                    "sent": "So what side of the head the modifier occurs with?",
                    "label": 0
                },
                {
                    "sent": "So we can get rid of all directional features.",
                    "label": 0
                },
                {
                    "sent": "And this not only includes the explicit direction, but also these part of speech ngram context features, because they weekly do encode the ordering of words in the sentence.",
                    "label": 0
                },
                {
                    "sent": "So this model here basically has three features.",
                    "label": 0
                },
                {
                    "sent": "What's the head part of speech tag?",
                    "label": 0
                },
                {
                    "sent": "What's the modifier part of speech tag?",
                    "label": 0
                },
                {
                    "sent": "And what's the direction of attachment, right?",
                    "label": 0
                },
                {
                    "sent": "And you can see we don't do so bad.",
                    "label": 0
                },
                {
                    "sent": "It's about 52% accuracy, but we do.",
                    "label": 0
                },
                {
                    "sent": "We do see a drop.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then start to share some parameters based on these World Atlas of Language properties.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we do is for particular head modifier pairs, right?",
                    "label": 0
                },
                {
                    "sent": "So let's say preposition now, right?",
                    "label": 0
                },
                {
                    "sent": "So where the head part of speech Tag is a preposition.",
                    "label": 0
                },
                {
                    "sent": "The modifier is a noun, so these are cases.",
                    "label": 0
                },
                {
                    "sent": "That for the adposition attachment.",
                    "label": 0
                },
                {
                    "sent": "So when that feature fires, will actually can join that feature with both the direction but also a walls property.",
                    "label": 0
                },
                {
                    "sent": "So then I'll give you an example on the next slide, but the idea is for particular features where we know something from the World Atlas of Language we can actually can join that feature with that property, and that way sort of fractured the feature space in two languages that have one particular value for that property.",
                    "label": 0
                },
                {
                    "sent": "In languages that may have another value.",
                    "label": 0
                },
                {
                    "sent": "And so then we're going to actually going to share those parameters between related.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Images.",
                    "label": 0
                },
                {
                    "sent": "In doing that, we get a nice big improvement and then the last thing we can do is throw in all the full feature set again.",
                    "label": 0
                },
                {
                    "sent": "But here we want to fracture the feature space again, but we're going to do it by language family, so if you're in the same language family as one of the particular source languages, you're going to share all this entire rich feature set with them and all the languages in that family will share those particular parameters.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is just an example of the first case right.",
                    "label": 0
                },
                {
                    "sent": "We have direction when the head tag fires preposition and the modifier tag fires noun, we're going to join that with property 85 A of the World Atlas of Language, which just turns out to be the order of ad position.",
                    "label": 0
                },
                {
                    "sent": "So this will fire the same feature will fire for languages that have the same property here and similar for language family we just say for every single rich feature just can join that with the language family and we use a really simple.",
                    "label": 0
                },
                {
                    "sent": "Language family.",
                    "label": 0
                },
                {
                    "sent": "Basically 2.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are the results again.",
                    "label": 0
                },
                {
                    "sent": "The two on the right are taking this notion of selective sharing.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of things we can see here.",
                    "label": 0
                },
                {
                    "sent": "From the the sort of base.",
                    "label": 0
                },
                {
                    "sent": "Just concatenate everything model we get.",
                    "label": 0
                },
                {
                    "sent": "Oh no, we get 7% improvement absolute with just being more careful about how we construct these features.",
                    "label": 0
                },
                {
                    "sent": "Not being so naive about it and we are now outperforming the model of naseam at all the generative model.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we look for language.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On average, we're doing better we.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Caught up on all the non Indo European languages and even surpassed on a few.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And on the Indo European languages we maintained the lead and we even for you know for Catalan.",
                    "label": 0
                },
                {
                    "sent": "For instance, we get a nice big improvement here.",
                    "label": 0
                },
                {
                    "sent": "So even the Indo European languages can get a benefit from this.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I want to talk a little bit about re lexicalization so this is the average of this new discriminative shared model.",
                    "label": 0
                },
                {
                    "sent": "This is a supervised two supervised versions of that Model 1 where we have no lexical features.",
                    "label": 1
                },
                {
                    "sent": "Another where we have lexical features and we can see clearly we can cut about 25% of the error off by just adding lexical features in, so we've thrown out a pretty solid source of information.",
                    "label": 0
                },
                {
                    "sent": "Since we've gone to the lexical as models, so we want to get that back.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can one work along these lines?",
                    "label": 0
                },
                {
                    "sent": "Zeeman and Resnick looked at the simple idea of basically taking the transfer model, running it on a large amount of unlabeled data and then re lexical Ising it by just training a lexicalized model on that data, and the idea, like self training in general, is that if the data is large enough, hopefully you can get some averaging effect and things will in.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move we got slight improvements doing that and zoom in and Resnik basically observed the same thing.",
                    "label": 0
                },
                {
                    "sent": "For some languages.",
                    "label": 0
                },
                {
                    "sent": "This help for other languages it heard it was fairly inconsistent.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But you can observe that this model here the base model, the transfer model.",
                    "label": 0
                },
                {
                    "sent": "If you look at the K best output of this model, let's say K = 100, there's a lot of really good ambiguity at the top, as you would expect.",
                    "label": 1
                },
                {
                    "sent": "This case is 100 and it's about 10% absolute better.",
                    "label": 0
                },
                {
                    "sent": "So we want to take advantage of that ambiguity.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to give you an idea of how we might be able to take advantage of it if we want to transfer from English to Japanese, let's say right.",
                    "label": 0
                },
                {
                    "sent": "So Japanese is a subject, object, verb, language.",
                    "label": 0
                },
                {
                    "sent": "Maybe we have a bunch of examples like this, so maybe this word a in Japanese.",
                    "label": 0
                },
                {
                    "sent": "The morphology of that word might tell us that it's always a subject or something like that.",
                    "label": 0
                },
                {
                    "sent": "It should always attach to a verb.",
                    "label": 0
                },
                {
                    "sent": "But if I'm only looking at part of speech tags and transferred from an Indo European language.",
                    "label": 0
                },
                {
                    "sent": "For this sentence, here I'm likely going to predict that a modifies the noun next to it in some kind of compound noun.",
                    "label": 0
                },
                {
                    "sent": "Representation right?",
                    "label": 0
                },
                {
                    "sent": "Because for English, even though it's possible that you can have two nouns modifying a verb to the left of the verb in this particular case, it's much more likely that this is just a single now with two words like John Smith ran kind of construction, but since it is.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possible for the.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see in English or in into European language, two nouns modify a verb to the left.",
                    "label": 0
                },
                {
                    "sent": "I will have some marginal probability on the arc that this now and is attached to this verb right, and so hopefully if I have a bunch of examples where a attaches to the verb because it's less of an ambiguous situation, I should be able to back off and learn.",
                    "label": 0
                },
                {
                    "sent": "In this situation that they should really attached to the verb.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is inject.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That kind of ambiguity from the base model into the learning process.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we call this ambiguity, preserving self training.",
                    "label": 0
                },
                {
                    "sent": "I don't know if that's really a great title, maybe ambiguity, aware self training, 'cause it actually doesn't preserve the ambiguity at the end of the day.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's this general idea that we want to encode the base model ambiguity while training, and so we're going to do that by defining the set of trees, called it Y~ and this is going to represent the set of trees that the model thought might be possible.",
                    "label": 0
                },
                {
                    "sent": "Maybe it wasn't that herbs tree for a particular target language sentence, but it had some reasonable belief in this.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want this set too.",
                    "label": 0
                },
                {
                    "sent": "Have a number of different properties.",
                    "label": 0
                },
                {
                    "sent": "First, it should be big enough that we encode enough good trees from the base model.",
                    "label": 0
                },
                {
                    "sent": "It's got to be small enough that we can guide the learner because we can just make it the set of all trees.",
                    "label": 0
                },
                {
                    "sent": "But then it's just unsupervised learning and we're not going to really learn much there.",
                    "label": 0
                },
                {
                    "sent": "But we also want to give the model flexibility to push mass towards any of these possible trees.",
                    "label": 0
                },
                {
                    "sent": "One thing we could do is just take the marginal probabilities from the base model, set up some optimization to try to maximize that, But then you're just going to mimic.",
                    "label": 0
                },
                {
                    "sent": "Those particular probabilities right?",
                    "label": 0
                },
                {
                    "sent": "You want to give the model a little bit more freedom, so we're not going to associate any scores with.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sharks.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to make this more concrete.",
                    "label": 0
                },
                {
                    "sent": "For every modifier M. For every possible head, so that constitutes an arc, we're just going to calculate the marginal probability of that arc from the model.",
                    "label": 0
                },
                {
                    "sent": "We're going to sort the arcs by.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That probability we're going to define this set a for a modifier, which is a set of possible arcs for that modifier, and we do that by simply going through this.",
                    "label": 0
                },
                {
                    "sent": "Set of sorted arcs until we pass some maths threshold so you know maybe until we get 90% of the marginal probability mass.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a hyperparameter from these sets of RX.",
                    "label": 0
                },
                {
                    "sent": "We just let Y told to be the set of arcs are set of trees that are dry.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Available from this arc set and then we just run a simple optimization where it's the regular CRF optimization, but we just marginalized out over this set of trees.",
                    "label": 0
                },
                {
                    "sent": "It's quite so.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in standard self training, why Tilda would just be a single person, so you wouldn't have to do any.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here it be convex.",
                    "label": 0
                },
                {
                    "sent": "It's not convex here.",
                    "label": 0
                },
                {
                    "sent": "We still use LB FGS to modify it because you can, and this objective has been used in the past and parsing for Stefan Riezler used it in this case where he had a a LFG grammar that gave something bigger.",
                    "label": 0
                },
                {
                    "sent": "A set of parses for sentence, and then he used this objective to push mass towards those parses.",
                    "label": 0
                },
                {
                    "sent": "So this is not really a new objective.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if this model here is the one on the right, so we took a bunch of target language data.",
                    "label": 0
                },
                {
                    "sent": "We ran this procedure on that data and what you can see is that you know now over the base model.",
                    "label": 0
                },
                {
                    "sent": "We're getting almost 10% improvement.",
                    "label": 0
                },
                {
                    "sent": "We get about four to 5% improvement over the best Naseem models.",
                    "label": 0
                },
                {
                    "sent": "These are the generative models and there is even one where they use target language, unlabeled data with an M procedure.",
                    "label": 0
                },
                {
                    "sent": "And since it's a generative model, that's fairly straightforward and we do get an improvement over the base MST selective sharing model.",
                    "label": 1
                },
                {
                    "sent": "It's 2%, but it's consistent, so every model, every language we do better at some languages less than others, and some languages like Turkish, we end up getting about a 5% absolute improvement.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the last thing is that.",
                    "label": 0
                },
                {
                    "sent": "Right, this idea of training a model with some ambiguous set of trees doesn't necessarily have to restrict itself to a single base model, right?",
                    "label": 1
                },
                {
                    "sent": "We could have multiple different based models that we could use to construct the set of arcs in some kind of ensemble training algorithm, and so we.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically do the exact same thing.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before we.",
                    "label": 0
                },
                {
                    "sent": "Instead of having a single base model, we just have K different based parsers.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we define the final arc set for each modifier to simply be the union of all the base parsers sets, right?",
                    "label": 0
                },
                {
                    "sent": "Very simple, we tried different things like intersection or things like that, but union just seemed to work the best.",
                    "label": 0
                },
                {
                    "sent": "And really, this can be any source of information, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't have to be a base parser.",
                    "label": 0
                },
                {
                    "sent": "It could also be human generated constraints or anything you can imagine.",
                    "label": 0
                },
                {
                    "sent": "And these could also be incorporated inside a prior as opposed to explicitly in the objective.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the base parsers we used was the Naseem model and the other MST parser, and not surprisingly like with most ensemble systems you get an improvement.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about this ensemble system is it's at training so you still learn a single model at testing time.",
                    "label": 0
                },
                {
                    "sent": "You're not actually running all these models, you're just running the one model you learned on the unlabeled data for that.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Target language.",
                    "label": 0
                },
                {
                    "sent": "And so to put this in perspective, the best unsupervised accuracy over the same set of languages is about 20% lower than this model.",
                    "label": 0
                },
                {
                    "sent": "And really we haven't done anything super complicated.",
                    "label": 0
                },
                {
                    "sent": "We've taken a CRF model, concatenated a bunch of data, define some features in.",
                    "label": 0
                },
                {
                    "sent": "A semi clever way and then ran a simple self training training algorithm on some unlabeled data and we're able to get 65.4% and this is an all sentence length is not restricted to like 10 or.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nothing like that.",
                    "label": 0
                },
                {
                    "sent": "Alright, so some final thoughts.",
                    "label": 1
                },
                {
                    "sent": "You know there's been just a lot of work.",
                    "label": 0
                },
                {
                    "sent": "In syntactic transfer over the past few years that has made significant progress.",
                    "label": 1
                },
                {
                    "sent": "So we're now about 20% absolute better than the best models in unsupervised parsing.",
                    "label": 1
                },
                {
                    "sent": "This is an underestimate in terms of treebank divergences on the test set.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, for a lot of languages we're probably about 5 to 10% below what the true accuracy is, but that's probably also true for unsupervised parsing as well.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as I said, these are simple models with just feature engineering, so you know I've ragged on unsupervised parsing a little bit here.",
                    "label": 1
                },
                {
                    "sent": "You know, So what?",
                    "label": 0
                },
                {
                    "sent": "What is the situation there?",
                    "label": 0
                },
                {
                    "sent": "I would say from an engineering perspective, if you want to build a model for some language amotivation that you need to, you can't assume anything and you have to learn a model only from the data.",
                    "label": 0
                },
                {
                    "sent": "Seems a little misguided to me.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of information and that's either from treebanks from other languages or from databases that linguists have populated, and that's really what we should be doing.",
                    "label": 1
                },
                {
                    "sent": "From a psycholinguistic perspective, I don't know, you know humans are able to learn language without English treebanks.",
                    "label": 0
                },
                {
                    "sent": "But you know whether the models we study in unsupervised parsing tell us anything about that.",
                    "label": 0
                },
                {
                    "sent": "I really don't know.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's take that.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you guys.",
                    "label": 0
                }
            ]
        }
    }
}