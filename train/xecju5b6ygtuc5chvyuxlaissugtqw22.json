{
    "id": "xecju5b6ygtuc5chvyuxlaissugtqw22",
    "title": "Approximate system identification: Misfit versus latency",
    "info": {
        "author": [
            "Ivan Markovsky, School of Electronics and Computer Science, University of Southampton"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "May 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/aispds08_markovsky_asi/",
    "segmentation": [
        [
            "Our next speaker is Ivan Markovsky from the University of Southampton.",
            "Who's going to talk about approximate system identification misfit versus latency?",
            "Thank you.",
            "Good morning.",
            "After preparing the slides, I realized that a better title would be on user choices in system notification.",
            "To emphasize that it's more pragmatically oriented, talk about application of the theory rather than development of new theory.",
            "In any case, I'm coming from.",
            "Systems and control background and this means I'm using the keyword system notification and in this Community people prefer probably inference or warning, but I think you understand when I say system at ification, I mean estimation of parameters."
        ],
        [
            "So I start with a specific example, which is a trajectory of.",
            "Different stochastic differential equation which is on the top.",
            "So we have nonlinear function F which gives the state dynamics.",
            "X is the state which is a vector.",
            "We have initial state at 0.",
            "And we have output equation which links the observed output which is displayed here to the state and also the latent variable E which is the noise.",
            "And the first part zooms on the 1st 350 sample up 2 * 315 of the whole trajectory, which is 1000 time steps.",
            "Even if you say noise, do you mean in a statistical sense, noise or I mean latent variable?",
            "So I really say this trajectory is artificially generated by a system in this model class.",
            "But I want to make this example to emphasize that if you are given only this very general information which says nothing more than stochastic linear time invariant differential equation.",
            "Then you have to make some choices.",
            "And despite the remarkable development of theory, you really in the end when you have to do the practical work of deriving the model.",
            "You have to do some heuristics.",
            "An typical example is you look at the data and you will decide how complicated is this data and you choose somehow.",
            "Initial guess of your model costs apply.",
            "Certain estimation methods get a model check the fit and to iterate this till you get satisfactory results.",
            "So as far as I know there are no."
        ],
        [
            "Please, the automatic method that will work universally and the typical user choices or things that you have to decide initially are at least the important ones are quite binary decisions.",
            "There is a world of linear models and everything else.",
            "And the very first choice is whether you want to keep it really simple or you want to go to something more complicated.",
            "If you don't have prior knowledge that your system is complicated, you probably prefer to first try simple models, linear models.",
            "The second thing which is.",
            "Actually, the topic of this talk is the terministic versus stochastic.",
            "And the point of this talk will be that's not really a big issue.",
            "Whether you want to keep it dramatic or stochastic, it's just a different motivation of your fitting criterion.",
            "But the important will be really where you want to stay on the linear or the nominal site, and increasing order of complexity.",
            "You may want to try first linear deterministic.",
            "Linear stochastic is more advanced, nonlinear deterministic.",
            "Maybe these two can swap depending on what you assume about stochastic part and the really complete in complexity model would be non non stochastic.",
            "Now what are the pros and cons against this model classes?",
            "The complexity increases, obviously.",
            "So if you stay on the linear deterministic level and you aim for exact fit, suppose you have no noise and you know in advance my trajectory generated by modeling this model 'cause then it's really simple.",
            "You basically have to solve a system of linear equations which will give you the parameters.",
            "So it's computationally extremely efficient as well.",
            "If you go to the stochastic case, you have to do approximation, and although the model class is still linear, the optimization problem is in general, nonconvex for most interesting fitting criterion, it's nonconvex.",
            "So it's as hard as the problem can be.",
            "In fact, doesn't matter that it's a linear model costs.",
            "It is a difficult problem, but the nice thing is that in cost functional evaluation, which is evaluating the likelihood, you can do it efficiently, because this will be a linear problem.",
            "At least Norm problem, so at least efficient you can gain in efficiency by doing cost function and gradient evaluations, which in the completely general nonlinear stochastic case would be.",
            "Also difficult non convex optimization problem."
        ],
        [
            "So in this talk, I'll assume that most of the time that the the model causes a linear systems, but the concepts about misfit and latency they generalized nonlinear as well.",
            "Initially I assume the terministic systems, but also not aim for exact fitting and now have approximation as a major issue, and eventually it will go to stochastic systems who are again I have to do approximation because I have.",
            "A finite.",
            "Amount of data and I'm not able to derive exactly the models only aseptically I can converge to a model provided the model is my model class and the big point of this talk is really that deterministic approximation versus stochastic estimation.",
            "This is two sides of the same coin.",
            "The stochastic setting can be viewed as an hour by for your fitting criterion.",
            "In a deterministic setting."
        ],
        [
            "So I'll first make a point with something which is familiar to everybody, and it's rather simple we want to.",
            "Derive linear static model.",
            "So this is solving an overdetermined system of equations X approximately equal Bab.",
            "Are the measurements and X is the model parameter we minimize over X, and in the list course sense, who introduce auxiliary variable or latent variable E which we want to minimize and this E will account for the model data mismatch.",
            "So if we add this E to the right hand side.",
            "You can think of equation error.",
            "We satisfy the equations.",
            "And we can first minimize over E, which is trivial in this case because there is nothing to minimize over.",
            "We just substitute and this will give us give us criterion that we minimize over X, which is the least correspond which is convex quadratic and has analytic solution, while I want to split this becausw our code is.",
            "A measure of mismatch between the data and the model parameterized by this vector X is latency because it relates to this latent variable.",
            "E and."
        ],
        [
            "Alternative criterion is what is also called totally squares where the reasoning is in the least squares problem.",
            "I can think of this E as correction on P on the right hand side, but I keep a as it is and if both A&BS given data noisy then it makes no sense to give preference to correctional B and not correct a as well.",
            "So the total least squares criterion would be isometrically correct.",
            "A&B, so that I get consistent equations.",
            "Consistent meaning that for this corrected data I will have trajectory of my system.",
            "And I want the correction to be as small as possible, which implicitly assumes something about.",
            "Noise on the data in a stochastic setting, but for the moment it's completely terministic approximation.",
            "Now if I do again as before minimization over the corrections DD B first.",
            "Now we have an optimization problem.",
            "It's non trivial, but it's easy becausw it's a linear quadratic linear constraints and quadratic cost function.",
            "This solution or problem and its analytic solution is this formula.",
            "And now we have to minimize this expression, which I call miss fit between the data and the model over the parameters of the model.",
            "And this is turns out to be a hard problem in the sense that it's a nonconvex problem, but not all non convex problems are hard, and this is a lucky coincidence that we can solve this problem exactly by singular value or eigenvalue decomposition, and this is very much linked to PCA.",
            "So actually this solution, the total square solution is normalization of the PCA solution."
        ],
        [
            "So what is the geometric meaning?",
            "I need not go over all this for the least squares criterion.",
            "We're minimizing the sum of squares of vertical distances from the data points to the fitting coin and the fitting point is described as the subspace I'm talking about wine in a 1 dimensional in general subspace, which is perpendicular to the vector X -- 1.",
            "So."
        ],
        [
            "The picture is in in the space.",
            "N + 1 which is number of columns of the extended data matrix AB.",
            "Together we have data point ABI which is a row of our data matrix and we.",
            "Take the vertical projection.",
            "On the on, the wine defined by the model, the subspace defined by the model.",
            "And here is the error E. So if we do this for all data points, then we have the latency and we start to shift to turn the wine in order to minimize this sum of squares.",
            "This is the discourse."
        ],
        [
            "Picture for the total squares will be.",
            "In this case, we are minimizing the sum of squares of orthogonal distances.",
            "I'm not going over this because it's quite simple, but it will."
        ],
        [
            "Time the picture will be instead of vertically.",
            "Now we orthogonally project on the subspace and again we are looking for the subspace which will give us the smallest distance which is now given the interpretation of 'em."
        ],
        [
            "If it.",
            "And.",
            "But if we step back from the concrete example of this static models.",
            "What we did in the philosophically we say we're taking very simple model this.",
            "Linear models with even not dynamic and we try to fit data which can be noisy and coming from an honor system.",
            "So we have to take into account this discrepancy and the first approach is we add auxiliary variables, which means we modify the definition of our model.",
            "We allow for this external noise.",
            "If we think of the of the latent variable as external variable.",
            "In the other approach, we don't modify the model, but we modify the data.",
            "And it's quite nice that if we're looking for exact model, it doesn't matter which approach we take.",
            "The exact model will correspond.",
            "Only to the case when Miss fit and latency are equal to 0 and they will give you the same answer.",
            "But if you have approximate model then miss it and latency will give you different answers which are optimal in different sense.",
            "And the question is which one is better?"
        ],
        [
            "And this depends on prior knowledge.",
            "Better will be latency, provided we know that the data is generated according to the regression model.",
            "So indeed B is equal to a * X where axr through through accessor should be with bar, which means a true value and we add some noise which noises zero mean with unit converts multiple of the unit covariance matrix and then the maximum effort estimator, which means statistically the best thing we can do.",
            "Is latency minimization least squares?"
        ],
        [
            "Misfit will be optimal in statistical sense in a different situation when again very symmetric to what we did in the as a constraint in our optimization, where we correct for A&B we hear perturb A&B.",
            "With this perturbations DD B, which taken together are random matrix which is 0 mean with unit covariance matrix up to a scaling factor.",
            "And again, it's not difficult to see that maximal correspond to misfit minimization to the total scores."
        ],
        [
            "That's the static case.",
            "It was just a link to something that people certainly know when we go to the dynamic case.",
            "Things are more complicated, but conceptually the same.",
            "Now we have our data not in the form of a matrix, but in a form of a time series.",
            "So we explicitly account for the time dependent ordering of the data points.",
            "So this W is taken together input and output.",
            "B is stands for the model which is in the static example the subspace, so it's the set of four trajectories which are consistent with the equations.",
            "And I'm thinking about the linear time invariant model class.",
            "So this is the calligraphy M. It can be more general.",
            "And the identification problem is too.",
            "To define a mapping from the data to the model.",
            "This is how we define our approximation or stochastic estimation problem.",
            "And then we derive algorithms which will implement this mapping and finally develop listening to efficient software that should work on large datasets for climate or."
        ],
        [
            "Biology data, so the terministic point of view would be we have chosen our model costs.",
            "We are looking for a model in the model class which will best fit the data without any assumptions about the data.",
            "It can be coming from anything, so it's the least squares.",
            "POV When we don't involve the regression model we just fit.",
            "The stochastic point of view would be we assume we have a true system which generates the data according to model, either the regression or the errors in variables model, which is the same metric one with perturbation on both inputs and outputs.",
            "And we assume something about the noise zero mean Gaussian covariance matrix known after scaling factor.",
            "Then we can define the maximum likelihood and then we have estimation problem asymptotically will have nice properties where consistency and efficiency.",
            "But we have to assume more in order to be able to say more.",
            "So which one of the two we prefer?",
            "It.",
            "As I showed in the static case.",
            "Least squares correspond to regression and total squares, person variables regression, so these are the two coins of the two sides of the same coin.",
            "There exactly the same thing motivated in a different way, depends what you think is."
        ],
        [
            "More reasonable.",
            "Computationally, we have to still evaluate the criterion.",
            "The likelihood.",
            "This is the latency in the latency case or misfitting the Misfit case, and these are very much like in the static case, the smallest latent variable which will make the data augment it with this latent variable consistent with augmented model which has latency in it and in the everything variables case we work for the smallest perturbation on the data or estimate of the data.",
            "Closest to the given data, the given trajectory WD, which is exact for the model that we.",
            "We estimate.",
            "When you say extended in the mean time or no, I just mean I add another input.",
            "Because here W contains input and output.",
            "These are the observed input and you observe the output.",
            "But I say my mother will contain another input which I don't observe.",
            "This is the latent input which I'm going to estimate, and then I'll have to.",
            "Include this model into the model, like adding in X = B + E. This will be the extra input.",
            "Over, I mean you'll be is defined over this larger so very yes yeah extended be yes, right?",
            "So how we do the latency or misfit estimation?",
            "That's the main thing here, because the rest will be optimization over the model parameters, which will be.",
            "As we saw already in the static case.",
            "A difficult non convex optimization problem when the model is linear.",
            "This is still doable problem.",
            "It is in the stochastic setting correspond to common filtering in atomistic setting it's exactly the common filter again but with different interpretation.",
            "Not common filter in the sense of stochastic estimator.",
            "Common filter in the sense of how we do this projection on the model.",
            "In early square sense, so it's not different.",
            "It's the same algorithm.",
            "And you can Alternatively do it by certain structured factorization routines, but that's more about implementation.",
            "So the point is we can do this."
        ],
        [
            "Rust.",
            "And the picture in the dynamic case would be in the Earth.",
            "In variables model we have UI as the observed.",
            "Sorry you.",
            "Why is the exact input and output which are linked with a model in our model class?",
            "But we observe you dyd differ data which are perturbed with certain measurement noises and in the out regressive moving average exogenous model we have the latent variable E as another input for the model.",
            "So this should be extended.",
            "And we put the extra assumptions in the stochastic setting about E being 0 mean stationary, agatic white, and so on.",
            "Everything nice that we need in order to derive the theory.",
            "In the errors in variable setting, again we need to assume zero mean for the measurement.",
            "Noise isn't known covariance structure, so quite a lot of assumptions, but they're really needed as long as we want to make statements about confidence, ellipsoids, and if we're happy with another, which is a point estimator, we can still do fine by doing approximate approximation atomistic sense."
        ],
        [
            "So notification problem is once we have the latency and the misfit we minimize simultaneously, we minimize over the model as well.",
            "So that will be the nonconvex part of the problem, which is the difficult one.",
            "But hopefully the model will be simple meaning class parameter, fewer parameters and then this will be carried out efficiently.",
            "But if you had a stochastic different inflation, you would really be an extended model where your noise would be part of this.",
            "Is this extra right?",
            "Yes.",
            "Yes, I will have more parameters in this case because I'll have to parameterized, deterministic and stochastic part.",
            "But once I choose the parameters of the stochastic model.",
            "Evaluating the likelihood or the latency with whatever we call it, this will be the common filter, so the structure of the algorithm will be again the same.",
            "You have two levels of minimization.",
            "Dinner minimization is the one that we can do efficiently fixing the model.",
            "This is the computation of the misfit or the latency common filter, or.",
            "Structured for authorization, these are the algorithms and the outer minimization will be the nonconvex one which will be just without guarantee to finding the global solution in general work optimization methods.",
            "How mutational squared.",
            "Metric basically is not convex.",
            "Non convex is when we have, which I didn't do here.",
            "We don't show when we introduce parameters of the model and these parameters will enter here.",
            "And then sorry, then this condition.",
            "Will become nonlinear in the parameters.",
            "Given the parameters, it's linear in the data in W hat.",
            "And if we.",
            "Put the parameters in the extra variable, then it becomes nonlinear and this makes the problem nonconvex.",
            "Think of a transfer function.",
            "Parameters are the coefficients of the numerator denominator.",
            "And then with respect to this coefficient, it will be a non convex problem.",
            "Yes, you have to include them in denominator.",
            "So."
        ],
        [
            "The point in conclusion of this talk is on the first one.",
            "We have the classical regression.",
            "In the static case, least squares.",
            "In the dynamic case, prediction error methods, which are the Air Max POV which correspond into as a concept deterministically to latency minimization in the errors.",
            "In variables case we have totally squares or something which is extension of the total scores for the dynamic case, which is global total squares code.",
            "Or misfit minimization.",
            "And these are user choices you have to decide where you minimize this or this, independent of whether you do it in a stochastic order terministic setting."
        ],
        [
            "So just to come back to the example, this seemingly complicated data was generated by the terministic autonomous linear system with 16 States and these are the most of those."
        ],
        [
            "System.",
            "So the trajectory that is."
        ],
        [
            "Shown here is."
        ],
        [
            "Linear combination of this eight trajectories would probably shift in time.",
            "This is how it's generated, so it's quite simple after all."
        ],
        [
            "Thank you very much.",
            "Quest.",
            "Do you really want to be as neutral as you say?",
            "If you say, well, it's a user choice.",
            "Do you have a slight difference?",
            "I have the preference with domestic setting because it's after all we decided we're doing the same thing, but a Mystic setting is barely putting its weaknesses in front of you.",
            "It says I approximate.",
            "I have approximation error, that's it, and the stochastic setting is putting a quote of assumptions which make you think that it does something very fancy, but in fact it does the same.",
            "So that's my preference.",
            "As long as you don't have any prior knowledge like I know my uncertainties because I have latent variable which I don't observe, which is very reasonable then to do latency minimization.",
            "And as long as you know, this latent variable is with certain stochastic properties, you have to take into account the stochastic properties prior knowledge that nobody can reject.",
            "But if you don't have this knowledge and you say I think I have to do minimization of something, which is why COGS are variable, while you should call it stochastic, you don't know what it is, it is it is this additional variable that should be small, that's all.",
            "That's my point of view."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our next speaker is Ivan Markovsky from the University of Southampton.",
                    "label": 1
                },
                {
                    "sent": "Who's going to talk about approximate system identification misfit versus latency?",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Good morning.",
                    "label": 0
                },
                {
                    "sent": "After preparing the slides, I realized that a better title would be on user choices in system notification.",
                    "label": 0
                },
                {
                    "sent": "To emphasize that it's more pragmatically oriented, talk about application of the theory rather than development of new theory.",
                    "label": 0
                },
                {
                    "sent": "In any case, I'm coming from.",
                    "label": 0
                },
                {
                    "sent": "Systems and control background and this means I'm using the keyword system notification and in this Community people prefer probably inference or warning, but I think you understand when I say system at ification, I mean estimation of parameters.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I start with a specific example, which is a trajectory of.",
                    "label": 0
                },
                {
                    "sent": "Different stochastic differential equation which is on the top.",
                    "label": 0
                },
                {
                    "sent": "So we have nonlinear function F which gives the state dynamics.",
                    "label": 0
                },
                {
                    "sent": "X is the state which is a vector.",
                    "label": 0
                },
                {
                    "sent": "We have initial state at 0.",
                    "label": 0
                },
                {
                    "sent": "And we have output equation which links the observed output which is displayed here to the state and also the latent variable E which is the noise.",
                    "label": 0
                },
                {
                    "sent": "And the first part zooms on the 1st 350 sample up 2 * 315 of the whole trajectory, which is 1000 time steps.",
                    "label": 0
                },
                {
                    "sent": "Even if you say noise, do you mean in a statistical sense, noise or I mean latent variable?",
                    "label": 0
                },
                {
                    "sent": "So I really say this trajectory is artificially generated by a system in this model class.",
                    "label": 0
                },
                {
                    "sent": "But I want to make this example to emphasize that if you are given only this very general information which says nothing more than stochastic linear time invariant differential equation.",
                    "label": 0
                },
                {
                    "sent": "Then you have to make some choices.",
                    "label": 0
                },
                {
                    "sent": "And despite the remarkable development of theory, you really in the end when you have to do the practical work of deriving the model.",
                    "label": 0
                },
                {
                    "sent": "You have to do some heuristics.",
                    "label": 0
                },
                {
                    "sent": "An typical example is you look at the data and you will decide how complicated is this data and you choose somehow.",
                    "label": 0
                },
                {
                    "sent": "Initial guess of your model costs apply.",
                    "label": 0
                },
                {
                    "sent": "Certain estimation methods get a model check the fit and to iterate this till you get satisfactory results.",
                    "label": 0
                },
                {
                    "sent": "So as far as I know there are no.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Please, the automatic method that will work universally and the typical user choices or things that you have to decide initially are at least the important ones are quite binary decisions.",
                    "label": 0
                },
                {
                    "sent": "There is a world of linear models and everything else.",
                    "label": 0
                },
                {
                    "sent": "And the very first choice is whether you want to keep it really simple or you want to go to something more complicated.",
                    "label": 0
                },
                {
                    "sent": "If you don't have prior knowledge that your system is complicated, you probably prefer to first try simple models, linear models.",
                    "label": 0
                },
                {
                    "sent": "The second thing which is.",
                    "label": 0
                },
                {
                    "sent": "Actually, the topic of this talk is the terministic versus stochastic.",
                    "label": 0
                },
                {
                    "sent": "And the point of this talk will be that's not really a big issue.",
                    "label": 0
                },
                {
                    "sent": "Whether you want to keep it dramatic or stochastic, it's just a different motivation of your fitting criterion.",
                    "label": 0
                },
                {
                    "sent": "But the important will be really where you want to stay on the linear or the nominal site, and increasing order of complexity.",
                    "label": 0
                },
                {
                    "sent": "You may want to try first linear deterministic.",
                    "label": 0
                },
                {
                    "sent": "Linear stochastic is more advanced, nonlinear deterministic.",
                    "label": 1
                },
                {
                    "sent": "Maybe these two can swap depending on what you assume about stochastic part and the really complete in complexity model would be non non stochastic.",
                    "label": 0
                },
                {
                    "sent": "Now what are the pros and cons against this model classes?",
                    "label": 0
                },
                {
                    "sent": "The complexity increases, obviously.",
                    "label": 0
                },
                {
                    "sent": "So if you stay on the linear deterministic level and you aim for exact fit, suppose you have no noise and you know in advance my trajectory generated by modeling this model 'cause then it's really simple.",
                    "label": 0
                },
                {
                    "sent": "You basically have to solve a system of linear equations which will give you the parameters.",
                    "label": 0
                },
                {
                    "sent": "So it's computationally extremely efficient as well.",
                    "label": 0
                },
                {
                    "sent": "If you go to the stochastic case, you have to do approximation, and although the model class is still linear, the optimization problem is in general, nonconvex for most interesting fitting criterion, it's nonconvex.",
                    "label": 0
                },
                {
                    "sent": "So it's as hard as the problem can be.",
                    "label": 0
                },
                {
                    "sent": "In fact, doesn't matter that it's a linear model costs.",
                    "label": 1
                },
                {
                    "sent": "It is a difficult problem, but the nice thing is that in cost functional evaluation, which is evaluating the likelihood, you can do it efficiently, because this will be a linear problem.",
                    "label": 1
                },
                {
                    "sent": "At least Norm problem, so at least efficient you can gain in efficiency by doing cost function and gradient evaluations, which in the completely general nonlinear stochastic case would be.",
                    "label": 0
                },
                {
                    "sent": "Also difficult non convex optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk, I'll assume that most of the time that the the model causes a linear systems, but the concepts about misfit and latency they generalized nonlinear as well.",
                    "label": 0
                },
                {
                    "sent": "Initially I assume the terministic systems, but also not aim for exact fitting and now have approximation as a major issue, and eventually it will go to stochastic systems who are again I have to do approximation because I have.",
                    "label": 0
                },
                {
                    "sent": "A finite.",
                    "label": 0
                },
                {
                    "sent": "Amount of data and I'm not able to derive exactly the models only aseptically I can converge to a model provided the model is my model class and the big point of this talk is really that deterministic approximation versus stochastic estimation.",
                    "label": 0
                },
                {
                    "sent": "This is two sides of the same coin.",
                    "label": 1
                },
                {
                    "sent": "The stochastic setting can be viewed as an hour by for your fitting criterion.",
                    "label": 0
                },
                {
                    "sent": "In a deterministic setting.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll first make a point with something which is familiar to everybody, and it's rather simple we want to.",
                    "label": 0
                },
                {
                    "sent": "Derive linear static model.",
                    "label": 0
                },
                {
                    "sent": "So this is solving an overdetermined system of equations X approximately equal Bab.",
                    "label": 0
                },
                {
                    "sent": "Are the measurements and X is the model parameter we minimize over X, and in the list course sense, who introduce auxiliary variable or latent variable E which we want to minimize and this E will account for the model data mismatch.",
                    "label": 1
                },
                {
                    "sent": "So if we add this E to the right hand side.",
                    "label": 0
                },
                {
                    "sent": "You can think of equation error.",
                    "label": 0
                },
                {
                    "sent": "We satisfy the equations.",
                    "label": 0
                },
                {
                    "sent": "And we can first minimize over E, which is trivial in this case because there is nothing to minimize over.",
                    "label": 0
                },
                {
                    "sent": "We just substitute and this will give us give us criterion that we minimize over X, which is the least correspond which is convex quadratic and has analytic solution, while I want to split this becausw our code is.",
                    "label": 0
                },
                {
                    "sent": "A measure of mismatch between the data and the model parameterized by this vector X is latency because it relates to this latent variable.",
                    "label": 0
                },
                {
                    "sent": "E and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alternative criterion is what is also called totally squares where the reasoning is in the least squares problem.",
                    "label": 1
                },
                {
                    "sent": "I can think of this E as correction on P on the right hand side, but I keep a as it is and if both A&BS given data noisy then it makes no sense to give preference to correctional B and not correct a as well.",
                    "label": 0
                },
                {
                    "sent": "So the total least squares criterion would be isometrically correct.",
                    "label": 1
                },
                {
                    "sent": "A&B, so that I get consistent equations.",
                    "label": 0
                },
                {
                    "sent": "Consistent meaning that for this corrected data I will have trajectory of my system.",
                    "label": 0
                },
                {
                    "sent": "And I want the correction to be as small as possible, which implicitly assumes something about.",
                    "label": 0
                },
                {
                    "sent": "Noise on the data in a stochastic setting, but for the moment it's completely terministic approximation.",
                    "label": 0
                },
                {
                    "sent": "Now if I do again as before minimization over the corrections DD B first.",
                    "label": 0
                },
                {
                    "sent": "Now we have an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's non trivial, but it's easy becausw it's a linear quadratic linear constraints and quadratic cost function.",
                    "label": 0
                },
                {
                    "sent": "This solution or problem and its analytic solution is this formula.",
                    "label": 0
                },
                {
                    "sent": "And now we have to minimize this expression, which I call miss fit between the data and the model over the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "And this is turns out to be a hard problem in the sense that it's a nonconvex problem, but not all non convex problems are hard, and this is a lucky coincidence that we can solve this problem exactly by singular value or eigenvalue decomposition, and this is very much linked to PCA.",
                    "label": 0
                },
                {
                    "sent": "So actually this solution, the total square solution is normalization of the PCA solution.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the geometric meaning?",
                    "label": 0
                },
                {
                    "sent": "I need not go over all this for the least squares criterion.",
                    "label": 0
                },
                {
                    "sent": "We're minimizing the sum of squares of vertical distances from the data points to the fitting coin and the fitting point is described as the subspace I'm talking about wine in a 1 dimensional in general subspace, which is perpendicular to the vector X -- 1.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The picture is in in the space.",
                    "label": 0
                },
                {
                    "sent": "N + 1 which is number of columns of the extended data matrix AB.",
                    "label": 0
                },
                {
                    "sent": "Together we have data point ABI which is a row of our data matrix and we.",
                    "label": 0
                },
                {
                    "sent": "Take the vertical projection.",
                    "label": 0
                },
                {
                    "sent": "On the on, the wine defined by the model, the subspace defined by the model.",
                    "label": 0
                },
                {
                    "sent": "And here is the error E. So if we do this for all data points, then we have the latency and we start to shift to turn the wine in order to minimize this sum of squares.",
                    "label": 0
                },
                {
                    "sent": "This is the discourse.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Picture for the total squares will be.",
                    "label": 0
                },
                {
                    "sent": "In this case, we are minimizing the sum of squares of orthogonal distances.",
                    "label": 0
                },
                {
                    "sent": "I'm not going over this because it's quite simple, but it will.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time the picture will be instead of vertically.",
                    "label": 0
                },
                {
                    "sent": "Now we orthogonally project on the subspace and again we are looking for the subspace which will give us the smallest distance which is now given the interpretation of 'em.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If it.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "But if we step back from the concrete example of this static models.",
                    "label": 0
                },
                {
                    "sent": "What we did in the philosophically we say we're taking very simple model this.",
                    "label": 0
                },
                {
                    "sent": "Linear models with even not dynamic and we try to fit data which can be noisy and coming from an honor system.",
                    "label": 0
                },
                {
                    "sent": "So we have to take into account this discrepancy and the first approach is we add auxiliary variables, which means we modify the definition of our model.",
                    "label": 0
                },
                {
                    "sent": "We allow for this external noise.",
                    "label": 0
                },
                {
                    "sent": "If we think of the of the latent variable as external variable.",
                    "label": 0
                },
                {
                    "sent": "In the other approach, we don't modify the model, but we modify the data.",
                    "label": 1
                },
                {
                    "sent": "And it's quite nice that if we're looking for exact model, it doesn't matter which approach we take.",
                    "label": 0
                },
                {
                    "sent": "The exact model will correspond.",
                    "label": 0
                },
                {
                    "sent": "Only to the case when Miss fit and latency are equal to 0 and they will give you the same answer.",
                    "label": 0
                },
                {
                    "sent": "But if you have approximate model then miss it and latency will give you different answers which are optimal in different sense.",
                    "label": 0
                },
                {
                    "sent": "And the question is which one is better?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this depends on prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "Better will be latency, provided we know that the data is generated according to the regression model.",
                    "label": 1
                },
                {
                    "sent": "So indeed B is equal to a * X where axr through through accessor should be with bar, which means a true value and we add some noise which noises zero mean with unit converts multiple of the unit covariance matrix and then the maximum effort estimator, which means statistically the best thing we can do.",
                    "label": 0
                },
                {
                    "sent": "Is latency minimization least squares?",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Misfit will be optimal in statistical sense in a different situation when again very symmetric to what we did in the as a constraint in our optimization, where we correct for A&B we hear perturb A&B.",
                    "label": 0
                },
                {
                    "sent": "With this perturbations DD B, which taken together are random matrix which is 0 mean with unit covariance matrix up to a scaling factor.",
                    "label": 0
                },
                {
                    "sent": "And again, it's not difficult to see that maximal correspond to misfit minimization to the total scores.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the static case.",
                    "label": 0
                },
                {
                    "sent": "It was just a link to something that people certainly know when we go to the dynamic case.",
                    "label": 0
                },
                {
                    "sent": "Things are more complicated, but conceptually the same.",
                    "label": 0
                },
                {
                    "sent": "Now we have our data not in the form of a matrix, but in a form of a time series.",
                    "label": 0
                },
                {
                    "sent": "So we explicitly account for the time dependent ordering of the data points.",
                    "label": 0
                },
                {
                    "sent": "So this W is taken together input and output.",
                    "label": 0
                },
                {
                    "sent": "B is stands for the model which is in the static example the subspace, so it's the set of four trajectories which are consistent with the equations.",
                    "label": 0
                },
                {
                    "sent": "And I'm thinking about the linear time invariant model class.",
                    "label": 1
                },
                {
                    "sent": "So this is the calligraphy M. It can be more general.",
                    "label": 0
                },
                {
                    "sent": "And the identification problem is too.",
                    "label": 1
                },
                {
                    "sent": "To define a mapping from the data to the model.",
                    "label": 0
                },
                {
                    "sent": "This is how we define our approximation or stochastic estimation problem.",
                    "label": 0
                },
                {
                    "sent": "And then we derive algorithms which will implement this mapping and finally develop listening to efficient software that should work on large datasets for climate or.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Biology data, so the terministic point of view would be we have chosen our model costs.",
                    "label": 1
                },
                {
                    "sent": "We are looking for a model in the model class which will best fit the data without any assumptions about the data.",
                    "label": 1
                },
                {
                    "sent": "It can be coming from anything, so it's the least squares.",
                    "label": 0
                },
                {
                    "sent": "POV When we don't involve the regression model we just fit.",
                    "label": 0
                },
                {
                    "sent": "The stochastic point of view would be we assume we have a true system which generates the data according to model, either the regression or the errors in variables model, which is the same metric one with perturbation on both inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "And we assume something about the noise zero mean Gaussian covariance matrix known after scaling factor.",
                    "label": 0
                },
                {
                    "sent": "Then we can define the maximum likelihood and then we have estimation problem asymptotically will have nice properties where consistency and efficiency.",
                    "label": 0
                },
                {
                    "sent": "But we have to assume more in order to be able to say more.",
                    "label": 0
                },
                {
                    "sent": "So which one of the two we prefer?",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "As I showed in the static case.",
                    "label": 0
                },
                {
                    "sent": "Least squares correspond to regression and total squares, person variables regression, so these are the two coins of the two sides of the same coin.",
                    "label": 0
                },
                {
                    "sent": "There exactly the same thing motivated in a different way, depends what you think is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More reasonable.",
                    "label": 0
                },
                {
                    "sent": "Computationally, we have to still evaluate the criterion.",
                    "label": 0
                },
                {
                    "sent": "The likelihood.",
                    "label": 0
                },
                {
                    "sent": "This is the latency in the latency case or misfitting the Misfit case, and these are very much like in the static case, the smallest latent variable which will make the data augment it with this latent variable consistent with augmented model which has latency in it and in the everything variables case we work for the smallest perturbation on the data or estimate of the data.",
                    "label": 0
                },
                {
                    "sent": "Closest to the given data, the given trajectory WD, which is exact for the model that we.",
                    "label": 0
                },
                {
                    "sent": "We estimate.",
                    "label": 0
                },
                {
                    "sent": "When you say extended in the mean time or no, I just mean I add another input.",
                    "label": 0
                },
                {
                    "sent": "Because here W contains input and output.",
                    "label": 0
                },
                {
                    "sent": "These are the observed input and you observe the output.",
                    "label": 0
                },
                {
                    "sent": "But I say my mother will contain another input which I don't observe.",
                    "label": 0
                },
                {
                    "sent": "This is the latent input which I'm going to estimate, and then I'll have to.",
                    "label": 0
                },
                {
                    "sent": "Include this model into the model, like adding in X = B + E. This will be the extra input.",
                    "label": 0
                },
                {
                    "sent": "Over, I mean you'll be is defined over this larger so very yes yeah extended be yes, right?",
                    "label": 0
                },
                {
                    "sent": "So how we do the latency or misfit estimation?",
                    "label": 0
                },
                {
                    "sent": "That's the main thing here, because the rest will be optimization over the model parameters, which will be.",
                    "label": 0
                },
                {
                    "sent": "As we saw already in the static case.",
                    "label": 0
                },
                {
                    "sent": "A difficult non convex optimization problem when the model is linear.",
                    "label": 0
                },
                {
                    "sent": "This is still doable problem.",
                    "label": 0
                },
                {
                    "sent": "It is in the stochastic setting correspond to common filtering in atomistic setting it's exactly the common filter again but with different interpretation.",
                    "label": 0
                },
                {
                    "sent": "Not common filter in the sense of stochastic estimator.",
                    "label": 0
                },
                {
                    "sent": "Common filter in the sense of how we do this projection on the model.",
                    "label": 0
                },
                {
                    "sent": "In early square sense, so it's not different.",
                    "label": 0
                },
                {
                    "sent": "It's the same algorithm.",
                    "label": 0
                },
                {
                    "sent": "And you can Alternatively do it by certain structured factorization routines, but that's more about implementation.",
                    "label": 0
                },
                {
                    "sent": "So the point is we can do this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rust.",
                    "label": 0
                },
                {
                    "sent": "And the picture in the dynamic case would be in the Earth.",
                    "label": 0
                },
                {
                    "sent": "In variables model we have UI as the observed.",
                    "label": 0
                },
                {
                    "sent": "Sorry you.",
                    "label": 0
                },
                {
                    "sent": "Why is the exact input and output which are linked with a model in our model class?",
                    "label": 0
                },
                {
                    "sent": "But we observe you dyd differ data which are perturbed with certain measurement noises and in the out regressive moving average exogenous model we have the latent variable E as another input for the model.",
                    "label": 0
                },
                {
                    "sent": "So this should be extended.",
                    "label": 0
                },
                {
                    "sent": "And we put the extra assumptions in the stochastic setting about E being 0 mean stationary, agatic white, and so on.",
                    "label": 0
                },
                {
                    "sent": "Everything nice that we need in order to derive the theory.",
                    "label": 0
                },
                {
                    "sent": "In the errors in variable setting, again we need to assume zero mean for the measurement.",
                    "label": 0
                },
                {
                    "sent": "Noise isn't known covariance structure, so quite a lot of assumptions, but they're really needed as long as we want to make statements about confidence, ellipsoids, and if we're happy with another, which is a point estimator, we can still do fine by doing approximate approximation atomistic sense.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So notification problem is once we have the latency and the misfit we minimize simultaneously, we minimize over the model as well.",
                    "label": 0
                },
                {
                    "sent": "So that will be the nonconvex part of the problem, which is the difficult one.",
                    "label": 0
                },
                {
                    "sent": "But hopefully the model will be simple meaning class parameter, fewer parameters and then this will be carried out efficiently.",
                    "label": 0
                },
                {
                    "sent": "But if you had a stochastic different inflation, you would really be an extended model where your noise would be part of this.",
                    "label": 0
                },
                {
                    "sent": "Is this extra right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, I will have more parameters in this case because I'll have to parameterized, deterministic and stochastic part.",
                    "label": 0
                },
                {
                    "sent": "But once I choose the parameters of the stochastic model.",
                    "label": 0
                },
                {
                    "sent": "Evaluating the likelihood or the latency with whatever we call it, this will be the common filter, so the structure of the algorithm will be again the same.",
                    "label": 0
                },
                {
                    "sent": "You have two levels of minimization.",
                    "label": 0
                },
                {
                    "sent": "Dinner minimization is the one that we can do efficiently fixing the model.",
                    "label": 0
                },
                {
                    "sent": "This is the computation of the misfit or the latency common filter, or.",
                    "label": 0
                },
                {
                    "sent": "Structured for authorization, these are the algorithms and the outer minimization will be the nonconvex one which will be just without guarantee to finding the global solution in general work optimization methods.",
                    "label": 0
                },
                {
                    "sent": "How mutational squared.",
                    "label": 0
                },
                {
                    "sent": "Metric basically is not convex.",
                    "label": 0
                },
                {
                    "sent": "Non convex is when we have, which I didn't do here.",
                    "label": 0
                },
                {
                    "sent": "We don't show when we introduce parameters of the model and these parameters will enter here.",
                    "label": 0
                },
                {
                    "sent": "And then sorry, then this condition.",
                    "label": 0
                },
                {
                    "sent": "Will become nonlinear in the parameters.",
                    "label": 0
                },
                {
                    "sent": "Given the parameters, it's linear in the data in W hat.",
                    "label": 0
                },
                {
                    "sent": "And if we.",
                    "label": 0
                },
                {
                    "sent": "Put the parameters in the extra variable, then it becomes nonlinear and this makes the problem nonconvex.",
                    "label": 0
                },
                {
                    "sent": "Think of a transfer function.",
                    "label": 0
                },
                {
                    "sent": "Parameters are the coefficients of the numerator denominator.",
                    "label": 0
                },
                {
                    "sent": "And then with respect to this coefficient, it will be a non convex problem.",
                    "label": 0
                },
                {
                    "sent": "Yes, you have to include them in denominator.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The point in conclusion of this talk is on the first one.",
                    "label": 0
                },
                {
                    "sent": "We have the classical regression.",
                    "label": 0
                },
                {
                    "sent": "In the static case, least squares.",
                    "label": 0
                },
                {
                    "sent": "In the dynamic case, prediction error methods, which are the Air Max POV which correspond into as a concept deterministically to latency minimization in the errors.",
                    "label": 0
                },
                {
                    "sent": "In variables case we have totally squares or something which is extension of the total scores for the dynamic case, which is global total squares code.",
                    "label": 0
                },
                {
                    "sent": "Or misfit minimization.",
                    "label": 0
                },
                {
                    "sent": "And these are user choices you have to decide where you minimize this or this, independent of whether you do it in a stochastic order terministic setting.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to come back to the example, this seemingly complicated data was generated by the terministic autonomous linear system with 16 States and these are the most of those.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "System.",
                    "label": 0
                },
                {
                    "sent": "So the trajectory that is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shown here is.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Linear combination of this eight trajectories would probably shift in time.",
                    "label": 0
                },
                {
                    "sent": "This is how it's generated, so it's quite simple after all.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Quest.",
                    "label": 0
                },
                {
                    "sent": "Do you really want to be as neutral as you say?",
                    "label": 0
                },
                {
                    "sent": "If you say, well, it's a user choice.",
                    "label": 0
                },
                {
                    "sent": "Do you have a slight difference?",
                    "label": 0
                },
                {
                    "sent": "I have the preference with domestic setting because it's after all we decided we're doing the same thing, but a Mystic setting is barely putting its weaknesses in front of you.",
                    "label": 0
                },
                {
                    "sent": "It says I approximate.",
                    "label": 0
                },
                {
                    "sent": "I have approximation error, that's it, and the stochastic setting is putting a quote of assumptions which make you think that it does something very fancy, but in fact it does the same.",
                    "label": 0
                },
                {
                    "sent": "So that's my preference.",
                    "label": 0
                },
                {
                    "sent": "As long as you don't have any prior knowledge like I know my uncertainties because I have latent variable which I don't observe, which is very reasonable then to do latency minimization.",
                    "label": 0
                },
                {
                    "sent": "And as long as you know, this latent variable is with certain stochastic properties, you have to take into account the stochastic properties prior knowledge that nobody can reject.",
                    "label": 0
                },
                {
                    "sent": "But if you don't have this knowledge and you say I think I have to do minimization of something, which is why COGS are variable, while you should call it stochastic, you don't know what it is, it is it is this additional variable that should be small, that's all.",
                    "label": 0
                },
                {
                    "sent": "That's my point of view.",
                    "label": 0
                }
            ]
        }
    }
}