{
    "id": "c3sflolyx76obt2si5ov3pq2a3nx2ooh",
    "title": "What cannot be learned with Bethe Approximations",
    "info": {
        "author": [
            "Amir Globerson, School of Computer Science and Engineering, The Hebrew University of Jerusalem"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_globerson_bethe_approximations/",
    "segmentation": [
        [
            "Yeah, so I changed the title slightly to what cannot be learned with belief propagation.",
            "And this is joint work with my."
        ],
        [
            "Student three hangman.",
            "So there are multivariate signals everywhere and one of our key goals in machine learning is to have models for these.",
            "Then in particular probabilistic models.",
            "And only distributions over high dimensional signals.",
            "Anwan"
        ],
        [
            "Simple way of doing that if we want to model distribution over N variables.",
            "So if it is large that requires an exponential number of parameters, so we typically do in the context of graphical models is they assume that the probability over of these variables is given as a product of functions that are defined over a small subset of variables.",
            "So say over three or two a variables here.",
            "And I'll focus on pairwise."
        ],
        [
            "The graphical models where we have a graph G with vertices and edges and some functions on the edges and then single variables.",
            "And our distribution is given as an exponent of a sum over the potentials of the edges or product of edge potentials.",
            "So this is the standard pairwise MRF, so that's going to this is going to be the model that we're interested in."
        ],
        [
            "And the learning problem.",
            "So that's that's our focus.",
            "There is given some real distribution, it doesn't.",
            "I mean, we can assume that it comes from this model, but doesn't.",
            "It's not really important.",
            "We have some data seen from that distribution.",
            "This is not my baby, it's worries and then what we want to do is to have a learning algorithm that takes this data, is input and produces a model distribution and then actually most often were not interested in the distribution itself, but we want to ask questions or queries from this distribution.",
            "For example, what's the marginal over particular variable XI, and then there's what we call an inference procedure that returns this.",
            "Answer and what we'd like to do, what we like to happen is that if we ask the same question of the."
        ],
        [
            "Real distribution, we get similar answers obviously.",
            "So in the case where we can do exactly running an inference, this is fairly well under."
        ],
        [
            "Do it, but the problem is that in many cases, for many problems of."
        ],
        [
            "Just both the learning and the inference problems are hard computationally, in fact intractable."
        ],
        [
            "So a natural approach in this case is to approximate a either or.",
            "Generally, both of these OK, but that raises a lot of interesting theoretical question as to what we can guarantee about a procedure that's inherently approximate in the sense of what we can say about the learn their model.",
            "OK, so there's been quite a lot of work on that.",
            "Some of it we heard today."
        ],
        [
            "So our goal in particular here is to OK, Thanks.",
            "Is to understand how well we can learn with approximate inference and learning.",
            "OK, so we use these approximations and we want to be able to say something about it, and that's generally quite hard because even most approximations are not controlled and it's not easy to say how well an approximation does, so it seems like a fairly difficult problem, and there's been quite a lot of progress on it in recent years, with works from different groups, but our focus in particular here is on a problem that we thought.",
            "Was very hard to begin with and now we think we understand it better is what happens if uses your approximation scheme.",
            "A loopy belief propagation.",
            "So look, we lift up again.",
            "On one hand is a fairly good approximation.",
            "Algorithm works pretty well in practice.",
            "In some cases it's understood theoretically, but mostly it's.",
            "It's a fairly hard to analyze, and so we were interested in it because it's empirically fairly satisfying in many cases.",
            "But learning with.",
            "BP is rather poorly understood and that was our motivation.",
            "Actually.",
            "Our motivation was that we tried learning with it.",
            "In some cases they fantastically well, and in some it failed miserably and we set out to understand why that's the case.",
            "So our results in it just say that one."
        ],
        [
            "BP has what we call spectacular failure failure modes for learning.",
            "So cases where it just fails miserably.",
            "Explaining what sense we characterize those and we show that it's well correlated with empirical behavior.",
            "So when the algorithm is in this spectacular failure mode, it does fair, but when it's not, it actually performs a pretty well.",
            "So I'll show you that an it suggests our results suggest which model if you're going to learn with BP, which models should you an use in terms of structure and also a parameter constraints?",
            "On the way, we've found some insights that we think are pretty interesting on what BP point fixed points can and cannot be, so it's some general results on BP regardless of learning."
        ],
        [
            "OK, so let's start.",
            "So start with maximum likelihood, which we can think of.",
            "It's an exact learning algorithm.",
            "OK, so let's say you have M training instances where each one of those is has all the variables.",
            "A right so each one is there has any variables in it and you'd like to find the parameter Theta that maximizes the likelihood written near here.",
            "OK, so this is a standard maximum likelihood estimator.",
            "It has nice properties, consistency, efficiency, etc."
        ],
        [
            "OK, so let's just rewrite that bit.",
            "So first we take the empirical data so these merge points and calculate empirical marginals, Singleton and pairwise marginals OK. And we denote those by mu bar and they're going to be very important in what follows, and then it turns."
        ],
        [
            "So that you can write the likelihood as the dot product of new bar.",
            "This sort of empirical marginals and the parameter Theta minus the log partition function at zed affair Theta."
        ],
        [
            "Technically, they are computationally.",
            "This is going to be hard generally because of the partition function that we see in there.",
            "But let's forget about that for a moment and say we can maximize it efficiently.",
            "So assuming you could maximize efficiently, there is a nice and simple characterization of what the optimum should satisfy.",
            "The optimum where Theta and it's often referred to as the moment matching."
        ],
        [
            "OK moment matching.",
            "What does that mean?",
            "It means that in this setting, so we said we calculate marginals from the data.",
            "These are the empirical marginals an if you do maximum likelihood learning and you get this model distribution and you do exact inference on it.",
            "To get the same marginal."
        ],
        [
            "Then these would be equal.",
            "OK, this is what moving matching means.",
            "It means that the empirical marginals and those of your maximum likelihood model are the same, so that's a nice characterization of the.",
            "Maximum likelihood."
        ],
        [
            "OK, so just a bit more formally, if we denote by mu superscript data, the marginals corresponding to Model PX Theta."
        ],
        [
            "Then moment matching means that knew of data.",
            "ML is just new bar.",
            "OK, so the moments of other marginals of the maximum likelihood parameter are."
        ],
        [
            "Precisely the empirical ones.",
            "OK, so.",
            "In our picture."
        ],
        [
            "Just have you bar here and you say to email there, OK?"
        ],
        [
            "And there we go.",
            "OK, so that's a nice property.",
            "It means that the learned model manage is to capture at least these marginals of the empirical data.",
            "OK, so it mimics the empirical data in the sense of these marginals 1st and 2nd order.",
            "An actually could say if all you cared about are these marginals then all this does is just cash these marginals in the model, right?",
            "So there's no point in doing learning and then Martin Wainwright had some nicely observations in that regard.",
            "OK, and that only holds if you do exactly what happens if you do approximate learning.",
            "That's that's a key question here.",
            "So Martin is shown that if you use for certain approximation schemes, moment matching does hold OK.",
            "So for what people refer to this convex free energy.",
            "OK, so there are certain approximation schemes both learning and inference.",
            "If you do those, you get moment matching, but what's completely unclear or was unclear is what happens if you use belief propagation approaches.",
            "OK, so our key question was does belief propagation satisfying moment matching.",
            "When does it satisfy and when does it not?",
            "So initially we thought OK, this such a trivial property.",
            "Probably it should satisfy it, and actually so it's not.",
            "It's not that simple."
        ],
        [
            "So what does it mean to satisfy moment matching with approximate learning?",
            "So you take the data, you calculate the marginals, then you do your approximate learning scheme.",
            "You get some model and you run some approximate inference scheme to get marginals out of that and you'd like."
        ],
        [
            "If these two things to be equal, that would mean that you have a moment magic OK?"
        ],
        [
            "OK, but it's not clear that you do, and in particular for belief propagation, it's not."
        ],
        [
            "OK, So what does it mean to learn with belief propagation, or equivalently with better.",
            "So you told me I should say better, not better, but I'm so tuned to better.",
            "So I'm sorry, just stick with that.",
            "OK, so how do we do approximate maximum likelihood using better approximations or using belief propagation?",
            "So here's the scheme and it's been used by people in the past with some success.",
            "OK, so this is the original likelihood and there are two problems with it.",
            "One is that it's hard to calculate this log partition function.",
            "The other is that if you want to optimize it using, say gradient, the gradient is the marginal or the set of margins, and this is also hard to calculate for."
        ],
        [
            "Our models."
        ],
        [
            "OK, so the idea when using a.",
            "Better ability propagation based approaches to approx."
        ],
        [
            "These two things.",
            "OK, So what does the approximation look like?"
        ],
        [
            "So let's first think of."
        ],
        [
            "Calculating the partition function exactly so there's a variation.",
            "A formulation of calculating the partition function and better approximations built on that too."
        ],
        [
            "Approximation, so the exact position function can be calculated as follows.",
            "There is some function which I'll try not to."
        ],
        [
            "Some function called, let's call it FU Theta, is sometimes referred to as the."
        ],
        [
            "Free energy and there's some domain constraint on you."
        ],
        [
            "Such that if you maximize FU over this domain, you get the log partition function further."
        ],
        [
            "So the maximizer of the free energy is the correct set of marginals.",
            "OK, so this is for exact inference.",
            "OK, so the idea is better based approximations."
        ],
        [
            "Is to replace OK, but the problem is that both.",
            "Optimizing over this domain and the function itself are hard to do, so that's intractable, obviously."
        ],
        [
            "OK. OK."
        ],
        [
            "Here's the function, but."
        ],
        [
            "Let's not get into that.",
            "So the idea better approximations is to replace both the domain and the function with something that you can more or less they handle."
        ],
        [
            "So you replace the domain with the diff."
        ],
        [
            "Domain on you.",
            "Often called the local marginal."
        ],
        [
            "And the function with a different function.",
            "So this is misleading.",
            "The function should be simpler to evaluate, OK, just in one day it's bit misleading, but there is a different function which will call the better free energy.",
            "And both ML and the FBI are meant to approximate the original domain and function OK.",
            "The idea in better based."
        ],
        [
            "Observation is to maximize this FB under the AML constraints.",
            "OK, so that's the approximate."
        ],
        [
            "So you take the maximizer of this, so this is going to be the better partition function."
        ],
        [
            "And the maximizer.",
            "The argmax is going to be the better marginals.",
            "OK, so this is the better scheme for approximating both partition function calculation and marginal."
        ],
        [
            "Anne.",
            "Yeah, and it's been shown so you might ask, what's the relation to loopy belief propagation?",
            "It's been shown by a demon and wise and Heska say later refined a result that if you run loopy belief propagation on this model defined by Theta is what the fixed points of flipping belief propagation correspond to stable fixed points of this optimization problem."
        ],
        [
            "OK, so."
        ],
        [
            "What this means is that you have your original graph."
        ],
        [
            "You run a belief propagation on it, so essentially it's the protocol for passing messages around the graph.",
            "You run these messages for awhile until they converge.",
            "You hope they converge an once they could."
        ],
        [
            "Merged as a scheme for turning those into some marginals OK, and what I did a Freeman invoice have shown is that.",
            "These marginals that beep."
        ],
        [
            "Get caught."
        ],
        [
            "Sponde to."
        ],
        [
            "Stationary points of this optimization problem.",
            "OK, so that's the link.",
            "So in some sense, optimizing this better variational approximation is identical to running a loopy belief propagation in the sense of finding stationary points.",
            "OK, and what actually has cases shown is that the stable fixed points of BP correspond to local Maxima of this function.",
            "So if you run BP, you're very unlikely to find the local minima or other stationary points."
        ],
        [
            "OK. Yeah, so that would be an unstable fixed point of BP."
        ],
        [
            "OK, so we pin practices in many cases and effective approximation for partition function marginals.",
            "So for many problems of interest is actually provides very good approximations.",
            "As you all know its exact for Trigraphs an works well for other cases.",
            "So one caveat obviously is the previous slide showed is that it can return local optimal right?",
            "So it doesn't exactly solve this better variational problem.",
            "So let's assume for now that actually belief propagation will find you the global optimum of the better potential problem.",
            "And you'll see that this doesn't solve the problem.",
            "So actually the harder problem, the more the catastrophic failure doesn't have to do with that OK?",
            "OK, so we're going to be optimistic towards a BP and assume that it actually solves the better variational approximation.",
            "OK, so in this case.",
            "Oh, I don't want that again.",
            "You're going to have to see that again, sorry."
        ],
        [
            "Alright, so I'm actually going to assume that BP.",
            "If you run it will find you the global maximum.",
            "OK and this is not going to solve the problems.",
            "OK."
        ],
        [
            "Right, so better based maximum likelihood is just going to be the following simple thing.",
            "This is the original likelihood function and what we'll do is approximate the partition function with it's better approximation OK, and we can write OK, so the log partition function is just this thing.",
            "With a better approximation and the better likelihood is going to be the original likelihood with a better approximation for the partition function.",
            "OK, now it's sometimes overlooked, but actually this function is a function of beta Theta.",
            "Sorry is a concave function, so despite this non convexity therein HMU this function as a function of Theta is concave.",
            "So actually it if there's no problem calculating this argmax, which I assume there isn't, then you can maximize over this function efficiently.",
            "OK, so it's easy to see that it's a.",
            "It's a concave function.",
            "OK, so this better likelihood is actually a concave function, so there's no problem here."
        ],
        [
            "OK. And what's better inference?",
            "Well, that's the standard thing.",
            "Given a parameter vector you take, it's better marginal to be the maximum of the better free energy.",
            "OK, so that's the standard thing.",
            "So you take this parameter Theta.",
            "You construct the free energy, and you take its maximum.",
            "And as I said before, you can sort of calculate this thing using loopy belief propagation, assuming you don't get stuck in sub optimal points.",
            "OK, so let's assume as I said that there are no issues with local Optima.",
            "So BP is you catch BP on a good day and it always finds this optimum global optimum and we see that the problem doesn't come from there OK?",
            "Right?",
            "OK, so now we can."
        ],
        [
            "Think of approximately earning again, so there's the data you calculate the marginals, and let's assume you use better maximum likelihood to obtain some denoted by Theta of new bar, so that will be the optimum better parameters.",
            "You use better inference too."
        ],
        [
            "Jacketed marginals which means you take the free energy corresponding to this parameter."
        ],
        [
            "Take its maximum."
        ],
        [
            "And you get marginals.",
            "So the question is."
        ],
        [
            "Are these things?"
        ],
        [
            "OK, so the bad news is that there are cases.",
            "There are many, many cases where they are not and will characterize those now."
        ],
        [
            "OK, so the first problem actually comes from characterizing what it means to maximize the better maximum likelihood.",
            "So I'll give a characterization of what it means to maximize the better likelihood, and this characterization is not exactly moment matching.",
            "OK, and this is where the problem lies, so here it is.",
            "So let's say you have a problem with data.",
            "So now define a right there."
        ],
        [
            "Draw the free energy for that parameter and take all its maximizers.",
            "So take the set of these maximizers."
        ],
        [
            "So in this case there are three."
        ],
        [
            "OK, so these three are the set N. Theta is the set of these three vectors and the result is that."
        ],
        [
            "Dump is the optimum better likelihood parameter if."
        ],
        [
            "It is in the convex Hull of these three points.",
            "In this case.",
            "OK, so it's not simple moment matching.",
            "It says that Theta is the maximum better likelihood parameter if mu bar is in the convex Hull of the maximizers of.",
            "Free energy for Theta.",
            "OK, so it sounds a bit more complicated than standard moment matching and it is."
        ],
        [
            "OK, right?",
            "So the problem is going to be if we have multiple maximizers because OK."
        ],
        [
            "Anne."
        ],
        [
            "Right, so just do it."
        ],
        [
            "Straight, so if you have."
        ],
        [
            "Three maximize."
        ],
        [
            "Is then as long as new bar is in."
        ],
        [
            "The convex Hull of the."
        ],
        [
            "Then data is optimal for that new bar.",
            "OK."
        ],
        [
            "Right, so the good case is where you will.",
            "You will have something like moment matching is if you might manage to find the Theta that has a single maximizer.",
            "OK, so it's just a unique single maximizer such that new bar is its maximizer, right?",
            "'cause in that case, M. This set M of maximize, is just going to be one vector, and then if U bar is that vector, then we have optimality by what we had in the previous slide, and this will be if you manage to find such as data.",
            "This would be the optimum better likelihood parameter.",
            "And Furthermore, if you take this data, you can recover them.",
            "You bar from it, right?",
            "So if you take data this optimal data you draw the better likelihood and you take its maximum.",
            "You get new bar right?",
            "So you get it."
        ],
        [
            "At the moment matching right?",
            "OK, so you can take the better model and recover the original marginals from it muba.",
            "OK, so this is a good case."
        ],
        [
            "So it means that.",
            "If you have a single, if you manage to find a Theta such that it has a single maximum and at maximum is new bar, then you have moment matching and everything is fine.",
            "OK, but what if there is no such parameter, so it would actually think that this is the common case, right?",
            "Because to have two maximize that are exactly the same, it seems like a measure of 0 setting or something and you think, OK, this is not a real problem, but actually it's a very real problem.",
            "OK, 'cause the problem is that you don't get to choose these sailors, the thetas are the solution of this likelihood.",
            "And if they want to have this zero measure.",
            "Case of multiple maximizes.",
            "They will, and they actually do OK, so let me."
        ],
        [
            "Vince, you that this is the case?",
            "So this is the mu.",
            "I'm drawing them you plane here OK, and I'm drawing essentially the better free energy for the maximum likelihood parameter for some marginal new bar that I chose.",
            "So what happens is Bill Barr is this point there in the middle?",
            "OK?",
            "But the maximizers of this free energy are on the edges.",
            "OK so you see that new bar is not a maximizer of this free energy.",
            "So this is the catastrophic failure case.",
            "OK, it means that you took this new bar.",
            "These are your empirical marginals.",
            "You did learning.",
            "You got some data.",
            "Now you draw the free energy for that data and its maximizers are not your mu bar so you cannot recover new bar from just looking at.",
            "This image so if I if I erase this thing how you wouldn't be able to recover it from this reality, right?"
        ],
        [
            "So what we set out to do is to characterize the cases where you have this failure mode, and we've found that in many cases it's a.",
            "It's a very real life phenomenon.",
            "So let's let's define this more properly.",
            "So actually what you see from the data, the only thing that matters from the data is this marginal new bar, right?",
            "This is the only thing that matters for learning an.",
            "So what we did find it we say the marginal new bar is better learnable if learning with better achieves moment matching.",
            "OK so if after we've learned with better you can use your model to recover the marginal new bar, so that's the natural definition of.",
            "A learnability in this case so."
        ],
        [
            "Imagine you have this new bar space.",
            "Then there is a subset of this space."
        ],
        [
            "BL, which we call the learnable better learnable set of empirical marginals.",
            "So if you're outside the set, if you're in the honorable regime, you better not use BP for learning.",
            "OK, because it's not going to do even the basic thing of recovering your MMR."
        ],
        [
            "OK, so here's a learnable case where you have you've learned you found your date of new bar and it has a unique map."
        ],
        [
            "Some which is view bar.",
            "OK so that means you can use it to recover mewborn."
        ],
        [
            "Let's turn over the honorable case."
        ],
        [
            "Is."
        ],
        [
            "Where?"
        ],
        [
            "You learn you find Theta."
        ],
        [
            "And your.",
            "Free energy has multiple Maxima such that new bars in their convex Hull, but you can't recover it right?",
            "'cause it could equally well have been here or here anywhere in between these two points.",
            "OK, so that's a sad case."
        ],
        [
            "OK, and our goal was to characterize those.",
            "So I'll give you several characterization and I'll show you that in many cases they provide the complete characterization.",
            "OK, so what's the naive way of checking a few bars?",
            "Learnable, you do better.",
            "Maximum likelihood you find this optimal better parameter.",
            "You look at the free energy and you see if it has a single maximum, right?",
            "If it has a single maximum, then it has to be new bar.",
            "And then there's no matching, and that's learnable.",
            "Otherwise, it's undeniable.",
            "OK, obviously we don't want to do that, so we want the simple, preferably analytic characterization.",
            "OK, so to explain these characterization, I have to define."
        ],
        [
            "What we call the Canonical parameters.",
            "So if the graph if your graph is a tree, then the optimal better parameters are what we call and others have called Canonical parameters.",
            "So you take your menu bar and you calculate these Singleton and pairwise potentials.",
            "Just the log of the marginals and log of ratio of marginals.",
            "OK, so if your graph is a tree then these are the exact not only better optimal parameters, but maximum likelihood parameters.",
            "OK, so these are particular parameters that you calculate from U bar.",
            "OK, and again, if the graph is a tree then these are the exact maximum likelihood and better maximum likely parameters.",
            "The same in this case.",
            "OK. And in the general case, when the graph is not a tree, then some textbooks would actually lead you to believe that this is the optimal better maximum likelihood parameter, but it's not OK, so there's actually.",
            "OK, I'll tell you what you can tell about it next, but."
        ],
        [
            "It's a bit more complicated, so why you can say generally even for non trigraphs is that new bar is a stationary point of the free energy with these."
        ],
        [
            "Canonical parameters."
        ],
        [
            "So this."
        ],
        [
            "And is one of these OK?",
            "So if you take these Canonical parameters, they're not going to be the better Max maximum likelihood parameter, but you do know that there are new bar is a stationary point of the corresponding free energy.",
            "OK, that's all you know.",
            "Right, but in some sense they are special and we use the special set of parameters in this context and we lose them repeatedly, not follows.",
            "So whenever I say Theta, see I mean these parameters as obtained from you."
        ],
        [
            "OK, so here's a useful lemma that we have.",
            "We call it stationary point invariants.",
            "It says, let's say I have a different parameter data.",
            "That's not the Canonical one, such that new bar is a stationary point of its free energy.",
            "OK, so we borrow the stationary point of the energy for data then.",
            "If you."
        ],
        [
            "Now look at that.",
            "The free energy with the Canonical parameters it's going to have exactly the same form."
        ],
        [
            "OK, with exactly the same as stationary points.",
            "OK, so if you're interested in a parameters such that new bars or stationary points of these parameters, you might as well just."
        ],
        [
            "Get the Canonical ones OK. And that's fairly easy to show.",
            "So incense again if you're interested in parameters such a new bar is a stationary point, then you can focus just on Canonical on the Canonical parameters.",
            "They'll have exactly the same.",
            "In shape.",
            "OK.",
            "So in particular we're looking for Theta such that new bar is a single maximizer.",
            "It's enough to focus on the Canonical ones.",
            "OK, 'cause if you had a different parameter such that data is this maximizer, the Canonical one will have the same property."
        ],
        [
            "So here's our first message.",
            "You can use these Canonical parameters.",
            "Don't fool yourself to believe that their optimal, but if they are not optimal there nothing else is optimal.",
            "OK, so if they don't satisfy moment matching, not no other parameters will.",
            "So if you're going to do something, use these Canonical ones, but don't, you know, don't they overstate what they?"
        ],
        [
            "OK, so here's our first outer bound."
        ],
        [
            "In the learnable region."
        ],
        [
            "So we have this better learnable region so."
        ],
        [
            "Lion out about now.",
            "Mean characterization of points that are unlearnable.",
            "OK, so here's"
        ],
        [
            "Activation"
        ],
        [
            "OK."
        ],
        [
            "So that's another one."
        ],
        [
            "So take the Canonical parameters data.",
            "See of me."
        ],
        [
            "Bar.",
            "OK, and.",
            "Draw their free energy.",
            "Oops.",
            "Daughter"
        ],
        [
            "Free energy if you bar is not the global maximum of this free energy, then it's not learnable.",
            "OK, because of this a.",
            "Dilemma that I stated before.",
            "So for any other parameters such as your bar is a stationary point, it's going to look the same.",
            "The free energy.",
            "OK, so if new bar is not a global maximum of the Canonical free energy, then it's not a global maximum of anything."
        ],
        [
            "OK. Yeah.",
            "Good point.",
            "OK there we go.",
            "So one way of checking it, at least getting a sufficient condition for it.",
            "How do you check it good through?"
        ],
        [
            "So."
        ],
        [
            "One way of doing it is to run ABP several times to find other stationary points.",
            "For this Canonical parameters, and if one of them is better than that of new bar, then you know that new bar is suboptimal, right?",
            "So it's a Nessus."
        ],
        [
            "But not sufficient."
        ],
        [
            "OK. Yeah, so if we did."
        ],
        [
            "With the better Maxima then there is no chance that new bar is learnable.",
            "OK, because this is the case and then there's."
        ],
        [
            "Learnability OK. And OK, so that's the first case.",
            "But I agree, it's not very nice in the sense that you have to run air, BP and stuff, But this is actually that was our first sort of attempted doing it, and it revealed actually most of the region."
        ],
        [
            "For us.",
            "OK, so here's another take finding out of bounds so learnable marginals look like this where mu bar is the global optimum of this free energy.",
            "OK, So what if U bar cannot be a global optimum of any free energy?",
            "OK, so me bar is just it's a handicapped in the sense that it's just it doesn't have it in him to be a global maximum of anything.",
            "OK, it's not ambitious.",
            "It cannot for any Theta that you come up with new bar.",
            "Is not a global maximum.",
            "OK, so if you had if you could find such a mu bar, then it would not be learnable, right?",
            "Because you could never draw this picture for it.",
            "It would never be a global maximum of anything."
        ],
        [
            "OK, so it's not clear that such marginals exist, right?",
            "It's it's it's a.",
            "It's a humiliating property for marginal.",
            "It cannot be a maximum of this function for any failure, so you go around looking for Theta, and there's no.",
            "You can't find the Theta where.",
            "You'll see the picture."
        ],
        [
            "So actually there do exist such margins, and many of those we can characterize pretty nicely as our show."
        ],
        [
            "OK, so actually the fact that these margins exist, it has more radical an implications.",
            "So again, these are marginals that are never local Maxima.",
            "OK of any better pre energy.",
            "OK, and the fact that they're not local maximum of any better free energy means in particular that they are not.",
            "They cannot be stable fixed points of belief propagation, right?",
            "Because of what I said before?",
            "OK, so right, so there are.",
            "So these marginals, these poor marginals are in fact can never be obtained.",
            "A stable fixed points of BP and that means that if you use BP to calculate marginals, if these are your two marginals, you're not going to see them.",
            "OK, so it means that BP innocence has expressive power is limited, so there are marginals that cannot be the output of BP.",
            "So that's I don't think that was known and it was funny that in the same year that we discovered this set, Zach Bitcoin, Ken Miller essentially discovered the same phenomenon and they called it so they out copyrighted us.",
            "I would say they called it unbelievable marginals.",
            "OK, these margins that cannot be obtained as fixed points stable fixed points of BP."
        ],
        [
            "At.",
            "Write an."
        ],
        [
            "Yeah, so these points are mu bars that are not maximum of anything global or local."
        ],
        [
            "And the other could be points that are local Maxima.",
            "But never a global Maxima, but we don't discover this, so we only discovered the ones that are not local Maxima VP, ever."
        ],
        [
            "OK, so here's the second message."
        ],
        [
            "OK, so OK.",
            "So how can you find these these guys?",
            "So muse new bugs that are never a local Maxima of this function.",
            "So the lucky thing is that this function if you look at it so to understand if something is a local maximizer and easy ways to look at the Hessian, right?",
            "So the Hessian with respect to a.",
            "And you write it.",
            "OK, so the nice thing is that if you take the action of this thing, it doesn't depend on Theta because the dependence on Theta is just linear roughly because there are some constraints on you, but we overcame that somehow.",
            "OK, so it means if you take the Hessian of this thing then the linear term goes away and you only need to consider the Hessian of their better free energy.",
            "Annubar, so another question becomes easy.",
            "Essentially, as if this session of the better free energy does, it has non negative eigenvalues.",
            "OK then it means that this point is not a local maximizer 'cause it can escape upwards.",
            "OK, so now all you need to do is do some math.",
            "And for binary variables."
        ],
        [
            "It's easy to test, so just to make things simpler, I'll consider binary variables and I also consider sort of a cross section where all the Singleton variables have the same marginals and all the pairwise marginals have the same pairwise marginal.",
            "OK, so now because of that we can draw everything on a plane.",
            "OK, that was the plane which we saw before.",
            "And what we did is to calculate the hash and in this context and to look at the eigenvalues and to find the lower bound and now your values and check when it's not negative and that we could do analytically.",
            "And it's interesting.",
            "It's closely related to the spectrum of the graph, so of the adjacency matrix of the graph.",
            "So there's some interesting math there."
        ],
        [
            "OK, and what we found is that the following modules are learnable.",
            "So if you have new bar via new bar E that satisfy this inequality, they are unlearnable OK.",
            "So in particular, it means that you will never if you run VP, you will never get those as a stable fixed points, and if you do learning with those you will never be able to recover them from your parameters.",
            "OK, so that looks.",
            "I mean there's several sanity checks you can do with this, so trees so if you have a tree then nothing is going to be unlearnable as expected for cycles also.",
            "But let's see what happens.",
            "So if you have complete graphs with the.",
            "Infinitely take the graph size to Infinity.",
            "You get this this simple thing.",
            "OK, so new bar is greater than new bar V squared.",
            "So for the physicists among you, you might recognize this as the condition that characterizes attractive Ising models or ferromagnets.",
            "OK, so all fair magnets or attractive using models have this satisfied this property OK?",
            "Because it means that they, like I mean, the probability of both variables being one is larger than the product of individual one, so it characterizes attractive models.",
            "So that was out of bounds.",
            "I'll show you the."
        ],
        [
            "Graphically in a second so.",
            "But we can also talk about inner bounds."
        ],
        [
            "Inner bounds means."
        ],
        [
            "At."
        ],
        [
            "Give me a condition such that a point is learnable."
        ],
        [
            "So how can you guarantee that?"
        ],
        [
            "Bar is learnable, so we know that new bar is a local Optima or stationary point of the better free energy for the Canonical parameters.",
            "So when is it global?",
            "If you could guarantee that it's global, then you would know."
        ],
        [
            "It's learnable."
        ],
        [
            "So one way of guaranteeing that is to ask, in which case is this better for energy?",
            "For the Canonical parameter has a unique maximum point.",
            "OK, so no local Maxima."
        ],
        [
            "And actually there have been."
        ],
        [
            "Several works characters."
        ],
        [
            "Exactly this, so there were works by a movies, Kapanen, Rooster and Wainwright that asked the question if I give you a model and whether that BP on this model has a unique fixed point.",
            "OK, which is a equivalent to asking whether there better energy has a unique, has no local maximum, so we just use these results that plug in to get the universe."
        ],
        [
            "OK.",
            "So that's it in terms of the inner and outer bounds, and I'll just show you some results.",
            "Where are we on time?",
            "Sorry.",
            "Sorry.",
            "Well."
        ],
        [
            "I would say there are models for which they canonically.",
            "Partition function is well behaved.",
            "OK yeah.",
            "So.",
            "And it says that there are models for which BP is that are easy for BP where BP doesn't get stuck, etc.",
            "So you would think there are models without frustration and like nasty stuff like that.",
            "OK."
        ],
        [
            "Right, so let me just show you a couple of these inner and outer bounds, and I'm going to concentrate on the homogeneous case so same UV and UV for all nodes and edges."
        ],
        [
            "OK, so this is the example that I showed you before.",
            "Maybe it's clearer now, so if you take this particular view bar and you do a maximum better likelihood, this is the better energy that you get.",
            "A new bar is not a maximizer, so this is an unreasonable."
        ],
        [
            "Marginal.",
            "OK, so now I look at this new bar plain and I want to show you the inner and outer bounds.",
            "OK so this is view bar plane.",
            "All the all the possible empirical marginal Ann and I'm showing which ones are learnable and unlearnable according to our bounds.",
            "So this red region is unlearnable according to our bounds.",
            "OK, and this blue region is the inner bound, so these are learnable according to our bounds and this black region we don't.",
            "Our bounds don't say anything about it, but we empirically found that it is learnable.",
            "OK, So what this image shows?",
            "This is for three by three grid.",
            "What this shows is that are unavailable region is tight and says that we captured all the unlearnable points and the inner bound is not as tight.",
            "OK, but it's conceivable that people will come up with better results that will improve there."
        ],
        [
            "About OK, so it's tight again for the unlabeled number leveraging."
        ],
        [
            "OK, so this is for all these U GBM lovers out there and deeper networks.",
            "This is a bipartite graph an OK so like restricted Boltzmann machines an so this is rather shocking.",
            "I think it says that most of the actually most marginals are unlearnable OK and with a very thin question to learnable ones is actually gets worse as you increase the graph size.",
            "Yeah, so it's bad news if you want to use BP for learning our BMS, you probably shouldn't do it on complete graph.",
            "OK, it's not totally bad news because I think one of the conclusions from this is that you should carefully think about graph structure when using.",
            "BP in particular, probably other approximation algorithms as well.",
            "OK, but for a complete bipartite graph, this is that's the status.",
            "So most marginals are in fact under."
        ],
        [
            "OK so but what we actually want to do is calculate marginal's that are not the empirical margins we want to calculate other marginals, right?",
            "That's the whole point.",
            "So we wanted to ask.",
            "OK, so how?",
            "Well, so there is this region where BP is learnable.",
            "How well does it do?",
            "On this region.",
            "OK for other probabilistic queries.",
            "Anne.",
            "OK.",
            "So what we do is to test to calculate new marginals, not those annubar.",
            "OK, 'cause this is actually the thing that we want to typically ask about.",
            "So we used.",
            "We did an experiment with this in a grid graphs and sample models with varying field, an interaction strength, so different models and compare the results on these outer graph marginals to TRW.",
            "So TW actually does perfect moment matching.",
            "So in that sense it's better than BP, but it's not clear that generalization to other marginals.",
            "It's not clear that it's better there and I'll show you what happened.",
            "OK so this is so.",
            "Every point here.",
            "Chorus."
        ],
        [
            "Funds to a different interaction using model with different interaction feel their strengths and we actually sampled ten such models for each point and averaged OK.",
            "So this shows you the the error in calculating the marginals OK and blue is low, so this region is a good region for BP."
        ],
        [
            "And this is the same for TRW.",
            "So you see that TW has in terms of performance actually performs worse than BP on, you know, because there's more blue here than here, right?",
            "So the question is this region that BP does well on?",
            "How does it relate to learnability?"
        ],
        [
            "And as you might expect, so Reds, so this is the same parameter field but with red, indicating that these are learnable models and you see that where BP is learnable, it actually has very good performance and where it's not learnable, it has pretty bad performance.",
            "But so does so.",
            "Do other approximations.",
            "OK, so right."
        ],
        [
            "And another way of saying is that learnability is well correlated with performance.",
            "So if, at least for this in this case, if you have a region where BP is learnable, than you might expect it to work well there generally OK.",
            "So learnability is a good indicator of whether you should use PP there or not."
        ],
        [
            "OK so here are.",
            "So my messages from out of this one is that we find interesting is the marginals cannot be obtained with BP's stable fixed points.",
            "OK, so it says something about the expressive power of BP an we can characterize them analytically.",
            "Learning with BP will often not even achieve more matching.",
            "Right and in a sense, there's no reason to use BP in these cases if you use it in this particular use mode.",
            "An when you do have learnable marginals, then BP performs well, at least empirically.",
            "From these examples that we've seen.",
            "So there are many ways you can extend this.",
            "You can have a week."
        ],
        [
            "To get title characterizations of the learnable region and try to guide the model structure towards such structure.",
            "Where BP will work or most marginals are learnable Ann.",
            "And then you can think of work around.",
            "So maybe this combination of better maximum likelihood in better inference is not the right thing to do.",
            "So maybe you should target you're learning approach to its moment matching and there are various ways you might think of doing that.",
            "And of course you can use higher order approximations and our intuitions that would happen then is that the learnable.",
            "Region would increase.",
            "Obviously if you use the highest order approximation then everything will be learnable, OK and it's not clear how things change in between, so that's it, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so I changed the title slightly to what cannot be learned with belief propagation.",
                    "label": 0
                },
                {
                    "sent": "And this is joint work with my.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Student three hangman.",
                    "label": 0
                },
                {
                    "sent": "So there are multivariate signals everywhere and one of our key goals in machine learning is to have models for these.",
                    "label": 0
                },
                {
                    "sent": "Then in particular probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "And only distributions over high dimensional signals.",
                    "label": 1
                },
                {
                    "sent": "Anwan",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple way of doing that if we want to model distribution over N variables.",
                    "label": 0
                },
                {
                    "sent": "So if it is large that requires an exponential number of parameters, so we typically do in the context of graphical models is they assume that the probability over of these variables is given as a product of functions that are defined over a small subset of variables.",
                    "label": 1
                },
                {
                    "sent": "So say over three or two a variables here.",
                    "label": 1
                },
                {
                    "sent": "And I'll focus on pairwise.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The graphical models where we have a graph G with vertices and edges and some functions on the edges and then single variables.",
                    "label": 1
                },
                {
                    "sent": "And our distribution is given as an exponent of a sum over the potentials of the edges or product of edge potentials.",
                    "label": 0
                },
                {
                    "sent": "So this is the standard pairwise MRF, so that's going to this is going to be the model that we're interested in.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the learning problem.",
                    "label": 0
                },
                {
                    "sent": "So that's that's our focus.",
                    "label": 0
                },
                {
                    "sent": "There is given some real distribution, it doesn't.",
                    "label": 0
                },
                {
                    "sent": "I mean, we can assume that it comes from this model, but doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's not really important.",
                    "label": 0
                },
                {
                    "sent": "We have some data seen from that distribution.",
                    "label": 0
                },
                {
                    "sent": "This is not my baby, it's worries and then what we want to do is to have a learning algorithm that takes this data, is input and produces a model distribution and then actually most often were not interested in the distribution itself, but we want to ask questions or queries from this distribution.",
                    "label": 0
                },
                {
                    "sent": "For example, what's the marginal over particular variable XI, and then there's what we call an inference procedure that returns this.",
                    "label": 0
                },
                {
                    "sent": "Answer and what we'd like to do, what we like to happen is that if we ask the same question of the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Real distribution, we get similar answers obviously.",
                    "label": 0
                },
                {
                    "sent": "So in the case where we can do exactly running an inference, this is fairly well under.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do it, but the problem is that in many cases, for many problems of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just both the learning and the inference problems are hard computationally, in fact intractable.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a natural approach in this case is to approximate a either or.",
                    "label": 0
                },
                {
                    "sent": "Generally, both of these OK, but that raises a lot of interesting theoretical question as to what we can guarantee about a procedure that's inherently approximate in the sense of what we can say about the learn their model.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's been quite a lot of work on that.",
                    "label": 0
                },
                {
                    "sent": "Some of it we heard today.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our goal in particular here is to OK, Thanks.",
                    "label": 0
                },
                {
                    "sent": "Is to understand how well we can learn with approximate inference and learning.",
                    "label": 1
                },
                {
                    "sent": "OK, so we use these approximations and we want to be able to say something about it, and that's generally quite hard because even most approximations are not controlled and it's not easy to say how well an approximation does, so it seems like a fairly difficult problem, and there's been quite a lot of progress on it in recent years, with works from different groups, but our focus in particular here is on a problem that we thought.",
                    "label": 0
                },
                {
                    "sent": "Was very hard to begin with and now we think we understand it better is what happens if uses your approximation scheme.",
                    "label": 0
                },
                {
                    "sent": "A loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "So look, we lift up again.",
                    "label": 0
                },
                {
                    "sent": "On one hand is a fairly good approximation.",
                    "label": 0
                },
                {
                    "sent": "Algorithm works pretty well in practice.",
                    "label": 0
                },
                {
                    "sent": "In some cases it's understood theoretically, but mostly it's.",
                    "label": 0
                },
                {
                    "sent": "It's a fairly hard to analyze, and so we were interested in it because it's empirically fairly satisfying in many cases.",
                    "label": 0
                },
                {
                    "sent": "But learning with.",
                    "label": 0
                },
                {
                    "sent": "BP is rather poorly understood and that was our motivation.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 1
                },
                {
                    "sent": "Our motivation was that we tried learning with it.",
                    "label": 0
                },
                {
                    "sent": "In some cases they fantastically well, and in some it failed miserably and we set out to understand why that's the case.",
                    "label": 0
                },
                {
                    "sent": "So our results in it just say that one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "BP has what we call spectacular failure failure modes for learning.",
                    "label": 1
                },
                {
                    "sent": "So cases where it just fails miserably.",
                    "label": 1
                },
                {
                    "sent": "Explaining what sense we characterize those and we show that it's well correlated with empirical behavior.",
                    "label": 0
                },
                {
                    "sent": "So when the algorithm is in this spectacular failure mode, it does fair, but when it's not, it actually performs a pretty well.",
                    "label": 0
                },
                {
                    "sent": "So I'll show you that an it suggests our results suggest which model if you're going to learn with BP, which models should you an use in terms of structure and also a parameter constraints?",
                    "label": 0
                },
                {
                    "sent": "On the way, we've found some insights that we think are pretty interesting on what BP point fixed points can and cannot be, so it's some general results on BP regardless of learning.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's start.",
                    "label": 0
                },
                {
                    "sent": "So start with maximum likelihood, which we can think of.",
                    "label": 0
                },
                {
                    "sent": "It's an exact learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say you have M training instances where each one of those is has all the variables.",
                    "label": 1
                },
                {
                    "sent": "A right so each one is there has any variables in it and you'd like to find the parameter Theta that maximizes the likelihood written near here.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a standard maximum likelihood estimator.",
                    "label": 0
                },
                {
                    "sent": "It has nice properties, consistency, efficiency, etc.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's just rewrite that bit.",
                    "label": 0
                },
                {
                    "sent": "So first we take the empirical data so these merge points and calculate empirical marginals, Singleton and pairwise marginals OK. And we denote those by mu bar and they're going to be very important in what follows, and then it turns.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that you can write the likelihood as the dot product of new bar.",
                    "label": 0
                },
                {
                    "sent": "This sort of empirical marginals and the parameter Theta minus the log partition function at zed affair Theta.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Technically, they are computationally.",
                    "label": 0
                },
                {
                    "sent": "This is going to be hard generally because of the partition function that we see in there.",
                    "label": 0
                },
                {
                    "sent": "But let's forget about that for a moment and say we can maximize it efficiently.",
                    "label": 1
                },
                {
                    "sent": "So assuming you could maximize efficiently, there is a nice and simple characterization of what the optimum should satisfy.",
                    "label": 1
                },
                {
                    "sent": "The optimum where Theta and it's often referred to as the moment matching.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK moment matching.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "It means that in this setting, so we said we calculate marginals from the data.",
                    "label": 0
                },
                {
                    "sent": "These are the empirical marginals an if you do maximum likelihood learning and you get this model distribution and you do exact inference on it.",
                    "label": 0
                },
                {
                    "sent": "To get the same marginal.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then these would be equal.",
                    "label": 0
                },
                {
                    "sent": "OK, this is what moving matching means.",
                    "label": 0
                },
                {
                    "sent": "It means that the empirical marginals and those of your maximum likelihood model are the same, so that's a nice characterization of the.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just a bit more formally, if we denote by mu superscript data, the marginals corresponding to Model PX Theta.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then moment matching means that knew of data.",
                    "label": 1
                },
                {
                    "sent": "ML is just new bar.",
                    "label": 0
                },
                {
                    "sent": "OK, so the moments of other marginals of the maximum likelihood parameter are.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Precisely the empirical ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "In our picture.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just have you bar here and you say to email there, OK?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there we go.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a nice property.",
                    "label": 0
                },
                {
                    "sent": "It means that the learned model manage is to capture at least these marginals of the empirical data.",
                    "label": 1
                },
                {
                    "sent": "OK, so it mimics the empirical data in the sense of these marginals 1st and 2nd order.",
                    "label": 1
                },
                {
                    "sent": "An actually could say if all you cared about are these marginals then all this does is just cash these marginals in the model, right?",
                    "label": 0
                },
                {
                    "sent": "So there's no point in doing learning and then Martin Wainwright had some nicely observations in that regard.",
                    "label": 0
                },
                {
                    "sent": "OK, and that only holds if you do exactly what happens if you do approximate learning.",
                    "label": 1
                },
                {
                    "sent": "That's that's a key question here.",
                    "label": 0
                },
                {
                    "sent": "So Martin is shown that if you use for certain approximation schemes, moment matching does hold OK.",
                    "label": 0
                },
                {
                    "sent": "So for what people refer to this convex free energy.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are certain approximation schemes both learning and inference.",
                    "label": 1
                },
                {
                    "sent": "If you do those, you get moment matching, but what's completely unclear or was unclear is what happens if you use belief propagation approaches.",
                    "label": 0
                },
                {
                    "sent": "OK, so our key question was does belief propagation satisfying moment matching.",
                    "label": 0
                },
                {
                    "sent": "When does it satisfy and when does it not?",
                    "label": 0
                },
                {
                    "sent": "So initially we thought OK, this such a trivial property.",
                    "label": 0
                },
                {
                    "sent": "Probably it should satisfy it, and actually so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not that simple.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does it mean to satisfy moment matching with approximate learning?",
                    "label": 1
                },
                {
                    "sent": "So you take the data, you calculate the marginals, then you do your approximate learning scheme.",
                    "label": 0
                },
                {
                    "sent": "You get some model and you run some approximate inference scheme to get marginals out of that and you'd like.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If these two things to be equal, that would mean that you have a moment magic OK?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but it's not clear that you do, and in particular for belief propagation, it's not.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what does it mean to learn with belief propagation, or equivalently with better.",
                    "label": 0
                },
                {
                    "sent": "So you told me I should say better, not better, but I'm so tuned to better.",
                    "label": 0
                },
                {
                    "sent": "So I'm sorry, just stick with that.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we do approximate maximum likelihood using better approximations or using belief propagation?",
                    "label": 0
                },
                {
                    "sent": "So here's the scheme and it's been used by people in the past with some success.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the original likelihood and there are two problems with it.",
                    "label": 0
                },
                {
                    "sent": "One is that it's hard to calculate this log partition function.",
                    "label": 1
                },
                {
                    "sent": "The other is that if you want to optimize it using, say gradient, the gradient is the marginal or the set of margins, and this is also hard to calculate for.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our models.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the idea when using a.",
                    "label": 0
                },
                {
                    "sent": "Better ability propagation based approaches to approx.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These two things.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does the approximation look like?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's first think of.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Calculating the partition function exactly so there's a variation.",
                    "label": 0
                },
                {
                    "sent": "A formulation of calculating the partition function and better approximations built on that too.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximation, so the exact position function can be calculated as follows.",
                    "label": 0
                },
                {
                    "sent": "There is some function which I'll try not to.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some function called, let's call it FU Theta, is sometimes referred to as the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free energy and there's some domain constraint on you.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such that if you maximize FU over this domain, you get the log partition function further.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the maximizer of the free energy is the correct set of marginals.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is for exact inference.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is better based approximations.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to replace OK, but the problem is that both.",
                    "label": 0
                },
                {
                    "sent": "Optimizing over this domain and the function itself are hard to do, so that's intractable, obviously.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the function, but.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's not get into that.",
                    "label": 0
                },
                {
                    "sent": "So the idea better approximations is to replace both the domain and the function with something that you can more or less they handle.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you replace the domain with the diff.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Domain on you.",
                    "label": 0
                },
                {
                    "sent": "Often called the local marginal.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the function with a different function.",
                    "label": 0
                },
                {
                    "sent": "So this is misleading.",
                    "label": 0
                },
                {
                    "sent": "The function should be simpler to evaluate, OK, just in one day it's bit misleading, but there is a different function which will call the better free energy.",
                    "label": 0
                },
                {
                    "sent": "And both ML and the FBI are meant to approximate the original domain and function OK.",
                    "label": 0
                },
                {
                    "sent": "The idea in better based.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Observation is to maximize this FB under the AML constraints.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the approximate.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you take the maximizer of this, so this is going to be the better partition function.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the maximizer.",
                    "label": 0
                },
                {
                    "sent": "The argmax is going to be the better marginals.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the better scheme for approximating both partition function calculation and marginal.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and it's been shown so you might ask, what's the relation to loopy belief propagation?",
                    "label": 0
                },
                {
                    "sent": "It's been shown by a demon and wise and Heska say later refined a result that if you run loopy belief propagation on this model defined by Theta is what the fixed points of flipping belief propagation correspond to stable fixed points of this optimization problem.",
                    "label": 1
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What this means is that you have your original graph.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You run a belief propagation on it, so essentially it's the protocol for passing messages around the graph.",
                    "label": 1
                },
                {
                    "sent": "You run these messages for awhile until they converge.",
                    "label": 0
                },
                {
                    "sent": "You hope they converge an once they could.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Merged as a scheme for turning those into some marginals OK, and what I did a Freeman invoice have shown is that.",
                    "label": 0
                },
                {
                    "sent": "These marginals that beep.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get caught.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sponde to.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stationary points of this optimization problem.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's the link.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, optimizing this better variational approximation is identical to running a loopy belief propagation in the sense of finding stationary points.",
                    "label": 0
                },
                {
                    "sent": "OK, and what actually has cases shown is that the stable fixed points of BP correspond to local Maxima of this function.",
                    "label": 0
                },
                {
                    "sent": "So if you run BP, you're very unlikely to find the local minima or other stationary points.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Yeah, so that would be an unstable fixed point of BP.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we pin practices in many cases and effective approximation for partition function marginals.",
                    "label": 1
                },
                {
                    "sent": "So for many problems of interest is actually provides very good approximations.",
                    "label": 1
                },
                {
                    "sent": "As you all know its exact for Trigraphs an works well for other cases.",
                    "label": 0
                },
                {
                    "sent": "So one caveat obviously is the previous slide showed is that it can return local optimal right?",
                    "label": 0
                },
                {
                    "sent": "So it doesn't exactly solve this better variational problem.",
                    "label": 1
                },
                {
                    "sent": "So let's assume for now that actually belief propagation will find you the global optimum of the better potential problem.",
                    "label": 0
                },
                {
                    "sent": "And you'll see that this doesn't solve the problem.",
                    "label": 0
                },
                {
                    "sent": "So actually the harder problem, the more the catastrophic failure doesn't have to do with that OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to be optimistic towards a BP and assume that it actually solves the better variational approximation.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case.",
                    "label": 0
                },
                {
                    "sent": "Oh, I don't want that again.",
                    "label": 0
                },
                {
                    "sent": "You're going to have to see that again, sorry.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm actually going to assume that BP.",
                    "label": 0
                },
                {
                    "sent": "If you run it will find you the global maximum.",
                    "label": 0
                },
                {
                    "sent": "OK and this is not going to solve the problems.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so better based maximum likelihood is just going to be the following simple thing.",
                    "label": 0
                },
                {
                    "sent": "This is the original likelihood function and what we'll do is approximate the partition function with it's better approximation OK, and we can write OK, so the log partition function is just this thing.",
                    "label": 0
                },
                {
                    "sent": "With a better approximation and the better likelihood is going to be the original likelihood with a better approximation for the partition function.",
                    "label": 0
                },
                {
                    "sent": "OK, now it's sometimes overlooked, but actually this function is a function of beta Theta.",
                    "label": 0
                },
                {
                    "sent": "Sorry is a concave function, so despite this non convexity therein HMU this function as a function of Theta is concave.",
                    "label": 1
                },
                {
                    "sent": "So actually it if there's no problem calculating this argmax, which I assume there isn't, then you can maximize over this function efficiently.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's easy to see that it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a concave function.",
                    "label": 0
                },
                {
                    "sent": "OK, so this better likelihood is actually a concave function, so there's no problem here.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And what's better inference?",
                    "label": 0
                },
                {
                    "sent": "Well, that's the standard thing.",
                    "label": 0
                },
                {
                    "sent": "Given a parameter vector you take, it's better marginal to be the maximum of the better free energy.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's the standard thing.",
                    "label": 0
                },
                {
                    "sent": "So you take this parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "You construct the free energy, and you take its maximum.",
                    "label": 0
                },
                {
                    "sent": "And as I said before, you can sort of calculate this thing using loopy belief propagation, assuming you don't get stuck in sub optimal points.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's assume as I said that there are no issues with local Optima.",
                    "label": 0
                },
                {
                    "sent": "So BP is you catch BP on a good day and it always finds this optimum global optimum and we see that the problem doesn't come from there OK?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, so now we can.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think of approximately earning again, so there's the data you calculate the marginals, and let's assume you use better maximum likelihood to obtain some denoted by Theta of new bar, so that will be the optimum better parameters.",
                    "label": 0
                },
                {
                    "sent": "You use better inference too.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jacketed marginals which means you take the free energy corresponding to this parameter.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take its maximum.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you get marginals.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are these things?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the bad news is that there are cases.",
                    "label": 0
                },
                {
                    "sent": "There are many, many cases where they are not and will characterize those now.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first problem actually comes from characterizing what it means to maximize the better maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "So I'll give a characterization of what it means to maximize the better likelihood, and this characterization is not exactly moment matching.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is where the problem lies, so here it is.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have a problem with data.",
                    "label": 0
                },
                {
                    "sent": "So now define a right there.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Draw the free energy for that parameter and take all its maximizers.",
                    "label": 0
                },
                {
                    "sent": "So take the set of these maximizers.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case there are three.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so these three are the set N. Theta is the set of these three vectors and the result is that.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dump is the optimum better likelihood parameter if.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is in the convex Hull of these three points.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not simple moment matching.",
                    "label": 0
                },
                {
                    "sent": "It says that Theta is the maximum better likelihood parameter if mu bar is in the convex Hull of the maximizers of.",
                    "label": 0
                },
                {
                    "sent": "Free energy for Theta.",
                    "label": 0
                },
                {
                    "sent": "OK, so it sounds a bit more complicated than standard moment matching and it is.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, right?",
                    "label": 0
                },
                {
                    "sent": "So the problem is going to be if we have multiple maximizers because OK.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so just do it.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straight, so if you have.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three maximize.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is then as long as new bar is in.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The convex Hull of the.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then data is optimal for that new bar.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so the good case is where you will.",
                    "label": 0
                },
                {
                    "sent": "You will have something like moment matching is if you might manage to find the Theta that has a single maximizer.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just a unique single maximizer such that new bar is its maximizer, right?",
                    "label": 0
                },
                {
                    "sent": "'cause in that case, M. This set M of maximize, is just going to be one vector, and then if U bar is that vector, then we have optimality by what we had in the previous slide, and this will be if you manage to find such as data.",
                    "label": 0
                },
                {
                    "sent": "This would be the optimum better likelihood parameter.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, if you take this data, you can recover them.",
                    "label": 0
                },
                {
                    "sent": "You bar from it, right?",
                    "label": 0
                },
                {
                    "sent": "So if you take data this optimal data you draw the better likelihood and you take its maximum.",
                    "label": 0
                },
                {
                    "sent": "You get new bar right?",
                    "label": 0
                },
                {
                    "sent": "So you get it.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the moment matching right?",
                    "label": 0
                },
                {
                    "sent": "OK, so you can take the better model and recover the original marginals from it muba.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a good case.",
                    "label": 1
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it means that.",
                    "label": 0
                },
                {
                    "sent": "If you have a single, if you manage to find a Theta such that it has a single maximum and at maximum is new bar, then you have moment matching and everything is fine.",
                    "label": 1
                },
                {
                    "sent": "OK, but what if there is no such parameter, so it would actually think that this is the common case, right?",
                    "label": 1
                },
                {
                    "sent": "Because to have two maximize that are exactly the same, it seems like a measure of 0 setting or something and you think, OK, this is not a real problem, but actually it's a very real problem.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause the problem is that you don't get to choose these sailors, the thetas are the solution of this likelihood.",
                    "label": 0
                },
                {
                    "sent": "And if they want to have this zero measure.",
                    "label": 0
                },
                {
                    "sent": "Case of multiple maximizes.",
                    "label": 0
                },
                {
                    "sent": "They will, and they actually do OK, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vince, you that this is the case?",
                    "label": 0
                },
                {
                    "sent": "So this is the mu.",
                    "label": 0
                },
                {
                    "sent": "I'm drawing them you plane here OK, and I'm drawing essentially the better free energy for the maximum likelihood parameter for some marginal new bar that I chose.",
                    "label": 0
                },
                {
                    "sent": "So what happens is Bill Barr is this point there in the middle?",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "But the maximizers of this free energy are on the edges.",
                    "label": 0
                },
                {
                    "sent": "OK so you see that new bar is not a maximizer of this free energy.",
                    "label": 0
                },
                {
                    "sent": "So this is the catastrophic failure case.",
                    "label": 0
                },
                {
                    "sent": "OK, it means that you took this new bar.",
                    "label": 0
                },
                {
                    "sent": "These are your empirical marginals.",
                    "label": 0
                },
                {
                    "sent": "You did learning.",
                    "label": 0
                },
                {
                    "sent": "You got some data.",
                    "label": 0
                },
                {
                    "sent": "Now you draw the free energy for that data and its maximizers are not your mu bar so you cannot recover new bar from just looking at.",
                    "label": 0
                },
                {
                    "sent": "This image so if I if I erase this thing how you wouldn't be able to recover it from this reality, right?",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we set out to do is to characterize the cases where you have this failure mode, and we've found that in many cases it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a very real life phenomenon.",
                    "label": 0
                },
                {
                    "sent": "So let's let's define this more properly.",
                    "label": 0
                },
                {
                    "sent": "So actually what you see from the data, the only thing that matters from the data is this marginal new bar, right?",
                    "label": 0
                },
                {
                    "sent": "This is the only thing that matters for learning an.",
                    "label": 0
                },
                {
                    "sent": "So what we did find it we say the marginal new bar is better learnable if learning with better achieves moment matching.",
                    "label": 0
                },
                {
                    "sent": "OK so if after we've learned with better you can use your model to recover the marginal new bar, so that's the natural definition of.",
                    "label": 0
                },
                {
                    "sent": "A learnability in this case so.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imagine you have this new bar space.",
                    "label": 0
                },
                {
                    "sent": "Then there is a subset of this space.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "BL, which we call the learnable better learnable set of empirical marginals.",
                    "label": 0
                },
                {
                    "sent": "So if you're outside the set, if you're in the honorable regime, you better not use BP for learning.",
                    "label": 0
                },
                {
                    "sent": "OK, because it's not going to do even the basic thing of recovering your MMR.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's a learnable case where you have you've learned you found your date of new bar and it has a unique map.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some which is view bar.",
                    "label": 0
                },
                {
                    "sent": "OK so that means you can use it to recover mewborn.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's turn over the honorable case.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where?",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You learn you find Theta.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And your.",
                    "label": 0
                },
                {
                    "sent": "Free energy has multiple Maxima such that new bars in their convex Hull, but you can't recover it right?",
                    "label": 0
                },
                {
                    "sent": "'cause it could equally well have been here or here anywhere in between these two points.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a sad case.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and our goal was to characterize those.",
                    "label": 0
                },
                {
                    "sent": "So I'll give you several characterization and I'll show you that in many cases they provide the complete characterization.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the naive way of checking a few bars?",
                    "label": 0
                },
                {
                    "sent": "Learnable, you do better.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood you find this optimal better parameter.",
                    "label": 0
                },
                {
                    "sent": "You look at the free energy and you see if it has a single maximum, right?",
                    "label": 0
                },
                {
                    "sent": "If it has a single maximum, then it has to be new bar.",
                    "label": 0
                },
                {
                    "sent": "And then there's no matching, and that's learnable.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, it's undeniable.",
                    "label": 0
                },
                {
                    "sent": "OK, obviously we don't want to do that, so we want the simple, preferably analytic characterization.",
                    "label": 0
                },
                {
                    "sent": "OK, so to explain these characterization, I have to define.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we call the Canonical parameters.",
                    "label": 0
                },
                {
                    "sent": "So if the graph if your graph is a tree, then the optimal better parameters are what we call and others have called Canonical parameters.",
                    "label": 0
                },
                {
                    "sent": "So you take your menu bar and you calculate these Singleton and pairwise potentials.",
                    "label": 0
                },
                {
                    "sent": "Just the log of the marginals and log of ratio of marginals.",
                    "label": 0
                },
                {
                    "sent": "OK, so if your graph is a tree then these are the exact not only better optimal parameters, but maximum likelihood parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are particular parameters that you calculate from U bar.",
                    "label": 0
                },
                {
                    "sent": "OK, and again, if the graph is a tree then these are the exact maximum likelihood and better maximum likely parameters.",
                    "label": 0
                },
                {
                    "sent": "The same in this case.",
                    "label": 0
                },
                {
                    "sent": "OK. And in the general case, when the graph is not a tree, then some textbooks would actually lead you to believe that this is the optimal better maximum likelihood parameter, but it's not OK, so there's actually.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll tell you what you can tell about it next, but.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a bit more complicated, so why you can say generally even for non trigraphs is that new bar is a stationary point of the free energy with these.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Canonical parameters.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And is one of these OK?",
                    "label": 0
                },
                {
                    "sent": "So if you take these Canonical parameters, they're not going to be the better Max maximum likelihood parameter, but you do know that there are new bar is a stationary point of the corresponding free energy.",
                    "label": 1
                },
                {
                    "sent": "OK, that's all you know.",
                    "label": 0
                },
                {
                    "sent": "Right, but in some sense they are special and we use the special set of parameters in this context and we lose them repeatedly, not follows.",
                    "label": 0
                },
                {
                    "sent": "So whenever I say Theta, see I mean these parameters as obtained from you.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's a useful lemma that we have.",
                    "label": 0
                },
                {
                    "sent": "We call it stationary point invariants.",
                    "label": 0
                },
                {
                    "sent": "It says, let's say I have a different parameter data.",
                    "label": 0
                },
                {
                    "sent": "That's not the Canonical one, such that new bar is a stationary point of its free energy.",
                    "label": 1
                },
                {
                    "sent": "OK, so we borrow the stationary point of the energy for data then.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now look at that.",
                    "label": 0
                },
                {
                    "sent": "The free energy with the Canonical parameters it's going to have exactly the same form.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, with exactly the same as stationary points.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you're interested in a parameters such that new bars or stationary points of these parameters, you might as well just.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get the Canonical ones OK. And that's fairly easy to show.",
                    "label": 0
                },
                {
                    "sent": "So incense again if you're interested in parameters such a new bar is a stationary point, then you can focus just on Canonical on the Canonical parameters.",
                    "label": 1
                },
                {
                    "sent": "They'll have exactly the same.",
                    "label": 0
                },
                {
                    "sent": "In shape.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in particular we're looking for Theta such that new bar is a single maximizer.",
                    "label": 0
                },
                {
                    "sent": "It's enough to focus on the Canonical ones.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause if you had a different parameter such that data is this maximizer, the Canonical one will have the same property.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's our first message.",
                    "label": 0
                },
                {
                    "sent": "You can use these Canonical parameters.",
                    "label": 0
                },
                {
                    "sent": "Don't fool yourself to believe that their optimal, but if they are not optimal there nothing else is optimal.",
                    "label": 0
                },
                {
                    "sent": "OK, so if they don't satisfy moment matching, not no other parameters will.",
                    "label": 0
                },
                {
                    "sent": "So if you're going to do something, use these Canonical ones, but don't, you know, don't they overstate what they?",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's our first outer bound.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the learnable region.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have this better learnable region so.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lion out about now.",
                    "label": 0
                },
                {
                    "sent": "Mean characterization of points that are unlearnable.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Activation",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's another one.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So take the Canonical parameters data.",
                    "label": 0
                },
                {
                    "sent": "See of me.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bar.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "Draw their free energy.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "Daughter",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free energy if you bar is not the global maximum of this free energy, then it's not learnable.",
                    "label": 0
                },
                {
                    "sent": "OK, because of this a.",
                    "label": 0
                },
                {
                    "sent": "Dilemma that I stated before.",
                    "label": 0
                },
                {
                    "sent": "So for any other parameters such as your bar is a stationary point, it's going to look the same.",
                    "label": 0
                },
                {
                    "sent": "The free energy.",
                    "label": 0
                },
                {
                    "sent": "OK, so if new bar is not a global maximum of the Canonical free energy, then it's not a global maximum of anything.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Yeah.",
                    "label": 0
                },
                {
                    "sent": "Good point.",
                    "label": 0
                },
                {
                    "sent": "OK there we go.",
                    "label": 0
                },
                {
                    "sent": "So one way of checking it, at least getting a sufficient condition for it.",
                    "label": 0
                },
                {
                    "sent": "How do you check it good through?",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way of doing it is to run ABP several times to find other stationary points.",
                    "label": 0
                },
                {
                    "sent": "For this Canonical parameters, and if one of them is better than that of new bar, then you know that new bar is suboptimal, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a Nessus.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But not sufficient.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Yeah, so if we did.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the better Maxima then there is no chance that new bar is learnable.",
                    "label": 0
                },
                {
                    "sent": "OK, because this is the case and then there's.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learnability OK. And OK, so that's the first case.",
                    "label": 0
                },
                {
                    "sent": "But I agree, it's not very nice in the sense that you have to run air, BP and stuff, But this is actually that was our first sort of attempted doing it, and it revealed actually most of the region.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For us.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's another take finding out of bounds so learnable marginals look like this where mu bar is the global optimum of this free energy.",
                    "label": 1
                },
                {
                    "sent": "OK, So what if U bar cannot be a global optimum of any free energy?",
                    "label": 0
                },
                {
                    "sent": "OK, so me bar is just it's a handicapped in the sense that it's just it doesn't have it in him to be a global maximum of anything.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not ambitious.",
                    "label": 0
                },
                {
                    "sent": "It cannot for any Theta that you come up with new bar.",
                    "label": 0
                },
                {
                    "sent": "Is not a global maximum.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you had if you could find such a mu bar, then it would not be learnable, right?",
                    "label": 0
                },
                {
                    "sent": "Because you could never draw this picture for it.",
                    "label": 0
                },
                {
                    "sent": "It would never be a global maximum of anything.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it's not clear that such marginals exist, right?",
                    "label": 0
                },
                {
                    "sent": "It's it's it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a humiliating property for marginal.",
                    "label": 0
                },
                {
                    "sent": "It cannot be a maximum of this function for any failure, so you go around looking for Theta, and there's no.",
                    "label": 1
                },
                {
                    "sent": "You can't find the Theta where.",
                    "label": 0
                },
                {
                    "sent": "You'll see the picture.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually there do exist such margins, and many of those we can characterize pretty nicely as our show.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so actually the fact that these margins exist, it has more radical an implications.",
                    "label": 0
                },
                {
                    "sent": "So again, these are marginals that are never local Maxima.",
                    "label": 0
                },
                {
                    "sent": "OK of any better pre energy.",
                    "label": 0
                },
                {
                    "sent": "OK, and the fact that they're not local maximum of any better free energy means in particular that they are not.",
                    "label": 0
                },
                {
                    "sent": "They cannot be stable fixed points of belief propagation, right?",
                    "label": 0
                },
                {
                    "sent": "Because of what I said before?",
                    "label": 0
                },
                {
                    "sent": "OK, so right, so there are.",
                    "label": 0
                },
                {
                    "sent": "So these marginals, these poor marginals are in fact can never be obtained.",
                    "label": 0
                },
                {
                    "sent": "A stable fixed points of BP and that means that if you use BP to calculate marginals, if these are your two marginals, you're not going to see them.",
                    "label": 0
                },
                {
                    "sent": "OK, so it means that BP innocence has expressive power is limited, so there are marginals that cannot be the output of BP.",
                    "label": 0
                },
                {
                    "sent": "So that's I don't think that was known and it was funny that in the same year that we discovered this set, Zach Bitcoin, Ken Miller essentially discovered the same phenomenon and they called it so they out copyrighted us.",
                    "label": 0
                },
                {
                    "sent": "I would say they called it unbelievable marginals.",
                    "label": 0
                },
                {
                    "sent": "OK, these margins that cannot be obtained as fixed points stable fixed points of BP.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Write an.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so these points are mu bars that are not maximum of anything global or local.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the other could be points that are local Maxima.",
                    "label": 0
                },
                {
                    "sent": "But never a global Maxima, but we don't discover this, so we only discovered the ones that are not local Maxima VP, ever.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's the second message.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so OK.",
                    "label": 0
                },
                {
                    "sent": "So how can you find these these guys?",
                    "label": 0
                },
                {
                    "sent": "So muse new bugs that are never a local Maxima of this function.",
                    "label": 0
                },
                {
                    "sent": "So the lucky thing is that this function if you look at it so to understand if something is a local maximizer and easy ways to look at the Hessian, right?",
                    "label": 0
                },
                {
                    "sent": "So the Hessian with respect to a.",
                    "label": 0
                },
                {
                    "sent": "And you write it.",
                    "label": 0
                },
                {
                    "sent": "OK, so the nice thing is that if you take the action of this thing, it doesn't depend on Theta because the dependence on Theta is just linear roughly because there are some constraints on you, but we overcame that somehow.",
                    "label": 0
                },
                {
                    "sent": "OK, so it means if you take the Hessian of this thing then the linear term goes away and you only need to consider the Hessian of their better free energy.",
                    "label": 0
                },
                {
                    "sent": "Annubar, so another question becomes easy.",
                    "label": 0
                },
                {
                    "sent": "Essentially, as if this session of the better free energy does, it has non negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "OK then it means that this point is not a local maximizer 'cause it can escape upwards.",
                    "label": 0
                },
                {
                    "sent": "OK, so now all you need to do is do some math.",
                    "label": 0
                },
                {
                    "sent": "And for binary variables.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's easy to test, so just to make things simpler, I'll consider binary variables and I also consider sort of a cross section where all the Singleton variables have the same marginals and all the pairwise marginals have the same pairwise marginal.",
                    "label": 1
                },
                {
                    "sent": "OK, so now because of that we can draw everything on a plane.",
                    "label": 0
                },
                {
                    "sent": "OK, that was the plane which we saw before.",
                    "label": 0
                },
                {
                    "sent": "And what we did is to calculate the hash and in this context and to look at the eigenvalues and to find the lower bound and now your values and check when it's not negative and that we could do analytically.",
                    "label": 0
                },
                {
                    "sent": "And it's interesting.",
                    "label": 0
                },
                {
                    "sent": "It's closely related to the spectrum of the graph, so of the adjacency matrix of the graph.",
                    "label": 0
                },
                {
                    "sent": "So there's some interesting math there.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and what we found is that the following modules are learnable.",
                    "label": 0
                },
                {
                    "sent": "So if you have new bar via new bar E that satisfy this inequality, they are unlearnable OK.",
                    "label": 0
                },
                {
                    "sent": "So in particular, it means that you will never if you run VP, you will never get those as a stable fixed points, and if you do learning with those you will never be able to recover them from your parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so that looks.",
                    "label": 0
                },
                {
                    "sent": "I mean there's several sanity checks you can do with this, so trees so if you have a tree then nothing is going to be unlearnable as expected for cycles also.",
                    "label": 0
                },
                {
                    "sent": "But let's see what happens.",
                    "label": 0
                },
                {
                    "sent": "So if you have complete graphs with the.",
                    "label": 0
                },
                {
                    "sent": "Infinitely take the graph size to Infinity.",
                    "label": 0
                },
                {
                    "sent": "You get this this simple thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so new bar is greater than new bar V squared.",
                    "label": 0
                },
                {
                    "sent": "So for the physicists among you, you might recognize this as the condition that characterizes attractive Ising models or ferromagnets.",
                    "label": 0
                },
                {
                    "sent": "OK, so all fair magnets or attractive using models have this satisfied this property OK?",
                    "label": 0
                },
                {
                    "sent": "Because it means that they, like I mean, the probability of both variables being one is larger than the product of individual one, so it characterizes attractive models.",
                    "label": 0
                },
                {
                    "sent": "So that was out of bounds.",
                    "label": 0
                },
                {
                    "sent": "I'll show you the.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graphically in a second so.",
                    "label": 0
                },
                {
                    "sent": "But we can also talk about inner bounds.",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inner bounds means.",
                    "label": 0
                }
            ]
        },
        "clip_139": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_140": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give me a condition such that a point is learnable.",
                    "label": 0
                }
            ]
        },
        "clip_141": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how can you guarantee that?",
                    "label": 0
                }
            ]
        },
        "clip_142": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bar is learnable, so we know that new bar is a local Optima or stationary point of the better free energy for the Canonical parameters.",
                    "label": 0
                },
                {
                    "sent": "So when is it global?",
                    "label": 0
                },
                {
                    "sent": "If you could guarantee that it's global, then you would know.",
                    "label": 0
                }
            ]
        },
        "clip_143": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's learnable.",
                    "label": 0
                }
            ]
        },
        "clip_144": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one way of guaranteeing that is to ask, in which case is this better for energy?",
                    "label": 0
                },
                {
                    "sent": "For the Canonical parameter has a unique maximum point.",
                    "label": 0
                },
                {
                    "sent": "OK, so no local Maxima.",
                    "label": 0
                }
            ]
        },
        "clip_145": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually there have been.",
                    "label": 0
                }
            ]
        },
        "clip_146": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Several works characters.",
                    "label": 0
                }
            ]
        },
        "clip_147": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly this, so there were works by a movies, Kapanen, Rooster and Wainwright that asked the question if I give you a model and whether that BP on this model has a unique fixed point.",
                    "label": 0
                },
                {
                    "sent": "OK, which is a equivalent to asking whether there better energy has a unique, has no local maximum, so we just use these results that plug in to get the universe.",
                    "label": 0
                }
            ]
        },
        "clip_148": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's it in terms of the inner and outer bounds, and I'll just show you some results.",
                    "label": 0
                },
                {
                    "sent": "Where are we on time?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_149": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would say there are models for which they canonically.",
                    "label": 0
                },
                {
                    "sent": "Partition function is well behaved.",
                    "label": 0
                },
                {
                    "sent": "OK yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And it says that there are models for which BP is that are easy for BP where BP doesn't get stuck, etc.",
                    "label": 0
                },
                {
                    "sent": "So you would think there are models without frustration and like nasty stuff like that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_150": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so let me just show you a couple of these inner and outer bounds, and I'm going to concentrate on the homogeneous case so same UV and UV for all nodes and edges.",
                    "label": 0
                }
            ]
        },
        "clip_151": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the example that I showed you before.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's clearer now, so if you take this particular view bar and you do a maximum better likelihood, this is the better energy that you get.",
                    "label": 0
                },
                {
                    "sent": "A new bar is not a maximizer, so this is an unreasonable.",
                    "label": 0
                }
            ]
        },
        "clip_152": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Marginal.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I look at this new bar plain and I want to show you the inner and outer bounds.",
                    "label": 0
                },
                {
                    "sent": "OK so this is view bar plane.",
                    "label": 0
                },
                {
                    "sent": "All the all the possible empirical marginal Ann and I'm showing which ones are learnable and unlearnable according to our bounds.",
                    "label": 0
                },
                {
                    "sent": "So this red region is unlearnable according to our bounds.",
                    "label": 0
                },
                {
                    "sent": "OK, and this blue region is the inner bound, so these are learnable according to our bounds and this black region we don't.",
                    "label": 0
                },
                {
                    "sent": "Our bounds don't say anything about it, but we empirically found that it is learnable.",
                    "label": 0
                },
                {
                    "sent": "OK, So what this image shows?",
                    "label": 0
                },
                {
                    "sent": "This is for three by three grid.",
                    "label": 0
                },
                {
                    "sent": "What this shows is that are unavailable region is tight and says that we captured all the unlearnable points and the inner bound is not as tight.",
                    "label": 0
                },
                {
                    "sent": "OK, but it's conceivable that people will come up with better results that will improve there.",
                    "label": 0
                }
            ]
        },
        "clip_153": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About OK, so it's tight again for the unlabeled number leveraging.",
                    "label": 0
                }
            ]
        },
        "clip_154": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is for all these U GBM lovers out there and deeper networks.",
                    "label": 0
                },
                {
                    "sent": "This is a bipartite graph an OK so like restricted Boltzmann machines an so this is rather shocking.",
                    "label": 0
                },
                {
                    "sent": "I think it says that most of the actually most marginals are unlearnable OK and with a very thin question to learnable ones is actually gets worse as you increase the graph size.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's bad news if you want to use BP for learning our BMS, you probably shouldn't do it on complete graph.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not totally bad news because I think one of the conclusions from this is that you should carefully think about graph structure when using.",
                    "label": 0
                },
                {
                    "sent": "BP in particular, probably other approximation algorithms as well.",
                    "label": 0
                },
                {
                    "sent": "OK, but for a complete bipartite graph, this is that's the status.",
                    "label": 0
                },
                {
                    "sent": "So most marginals are in fact under.",
                    "label": 0
                }
            ]
        },
        "clip_155": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so but what we actually want to do is calculate marginal's that are not the empirical margins we want to calculate other marginals, right?",
                    "label": 0
                },
                {
                    "sent": "That's the whole point.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to ask.",
                    "label": 0
                },
                {
                    "sent": "OK, so how?",
                    "label": 0
                },
                {
                    "sent": "Well, so there is this region where BP is learnable.",
                    "label": 0
                },
                {
                    "sent": "How well does it do?",
                    "label": 0
                },
                {
                    "sent": "On this region.",
                    "label": 0
                },
                {
                    "sent": "OK for other probabilistic queries.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what we do is to test to calculate new marginals, not those annubar.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause this is actually the thing that we want to typically ask about.",
                    "label": 0
                },
                {
                    "sent": "So we used.",
                    "label": 0
                },
                {
                    "sent": "We did an experiment with this in a grid graphs and sample models with varying field, an interaction strength, so different models and compare the results on these outer graph marginals to TRW.",
                    "label": 0
                },
                {
                    "sent": "So TW actually does perfect moment matching.",
                    "label": 0
                },
                {
                    "sent": "So in that sense it's better than BP, but it's not clear that generalization to other marginals.",
                    "label": 0
                },
                {
                    "sent": "It's not clear that it's better there and I'll show you what happened.",
                    "label": 0
                },
                {
                    "sent": "OK so this is so.",
                    "label": 0
                },
                {
                    "sent": "Every point here.",
                    "label": 0
                },
                {
                    "sent": "Chorus.",
                    "label": 0
                }
            ]
        },
        "clip_156": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Funds to a different interaction using model with different interaction feel their strengths and we actually sampled ten such models for each point and averaged OK.",
                    "label": 0
                },
                {
                    "sent": "So this shows you the the error in calculating the marginals OK and blue is low, so this region is a good region for BP.",
                    "label": 0
                }
            ]
        },
        "clip_157": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the same for TRW.",
                    "label": 0
                },
                {
                    "sent": "So you see that TW has in terms of performance actually performs worse than BP on, you know, because there's more blue here than here, right?",
                    "label": 0
                },
                {
                    "sent": "So the question is this region that BP does well on?",
                    "label": 0
                },
                {
                    "sent": "How does it relate to learnability?",
                    "label": 0
                }
            ]
        },
        "clip_158": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as you might expect, so Reds, so this is the same parameter field but with red, indicating that these are learnable models and you see that where BP is learnable, it actually has very good performance and where it's not learnable, it has pretty bad performance.",
                    "label": 0
                },
                {
                    "sent": "But so does so.",
                    "label": 0
                },
                {
                    "sent": "Do other approximations.",
                    "label": 0
                },
                {
                    "sent": "OK, so right.",
                    "label": 0
                }
            ]
        },
        "clip_159": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another way of saying is that learnability is well correlated with performance.",
                    "label": 0
                },
                {
                    "sent": "So if, at least for this in this case, if you have a region where BP is learnable, than you might expect it to work well there generally OK.",
                    "label": 0
                },
                {
                    "sent": "So learnability is a good indicator of whether you should use PP there or not.",
                    "label": 0
                }
            ]
        },
        "clip_160": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here are.",
                    "label": 0
                },
                {
                    "sent": "So my messages from out of this one is that we find interesting is the marginals cannot be obtained with BP's stable fixed points.",
                    "label": 0
                },
                {
                    "sent": "OK, so it says something about the expressive power of BP an we can characterize them analytically.",
                    "label": 0
                },
                {
                    "sent": "Learning with BP will often not even achieve more matching.",
                    "label": 0
                },
                {
                    "sent": "Right and in a sense, there's no reason to use BP in these cases if you use it in this particular use mode.",
                    "label": 0
                },
                {
                    "sent": "An when you do have learnable marginals, then BP performs well, at least empirically.",
                    "label": 0
                },
                {
                    "sent": "From these examples that we've seen.",
                    "label": 0
                },
                {
                    "sent": "So there are many ways you can extend this.",
                    "label": 0
                },
                {
                    "sent": "You can have a week.",
                    "label": 0
                }
            ]
        },
        "clip_161": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get title characterizations of the learnable region and try to guide the model structure towards such structure.",
                    "label": 0
                },
                {
                    "sent": "Where BP will work or most marginals are learnable Ann.",
                    "label": 0
                },
                {
                    "sent": "And then you can think of work around.",
                    "label": 0
                },
                {
                    "sent": "So maybe this combination of better maximum likelihood in better inference is not the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "So maybe you should target you're learning approach to its moment matching and there are various ways you might think of doing that.",
                    "label": 0
                },
                {
                    "sent": "And of course you can use higher order approximations and our intuitions that would happen then is that the learnable.",
                    "label": 0
                },
                {
                    "sent": "Region would increase.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you use the highest order approximation then everything will be learnable, OK and it's not clear how things change in between, so that's it, thank you.",
                    "label": 0
                }
            ]
        }
    }
}