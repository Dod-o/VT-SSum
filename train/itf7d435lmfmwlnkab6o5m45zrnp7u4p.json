{
    "id": "itf7d435lmfmwlnkab6o5m45zrnp7u4p",
    "title": "Activized Learning: Transforming Passive to Active with Improved Label Complexity",
    "info": {
        "author": [
            "Steve Hanneke, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Jan. 15, 2009",
        "recorded": "October 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/cmulls08_hanneke_altp/",
    "segmentation": [
        [
            "OK, so this week I'm going to be talking.",
            "I'll be talking a bit about active learning, particular type of active learning.",
            "I've been working on recently and there's some will get to some fairly general and in my opinion extremely exciting.",
            "New results about the strength of active learning in relation to passive learning.",
            "So I'll start."
        ],
        [
            "Just by defining what do I mean by passive learning and then will contrast it and talk about active learning.",
            "So in passive learning this is something probably most of you have seen we have.",
            "This is just the supervised model of learning, so we have some data source and expert Anna learning algorithm and basically what happens is some raw unlabeled data comes from the data source and goes and goes to the expert.",
            "The expert labels those unlabeled examples passes the labeled data set into the learning algorithm, does some processing and then.",
            "Eventually outputs a classifier and what we hope is that this classifier will agree with the expert on future examples from that data source.",
            "So in contrast, in active learning, so the players are all the same, but the protocol is a little different that unlabeled data is."
        ],
        [
            "Go straight into the learning algorithm and we're going to let the learning algorithm choose examples.",
            "Particular examples from the set and ask the expert for the labels of those examples.",
            "The expert returns the label and then the algorithm chooses another example, and so this happens interactively and then we get the label of that example after a number of rounds.",
            "Eventually the algorithm outputs a classifier, and again we're hoping that this classifier is going to agree with the expert on most future examples from that data source.",
            "And the."
        ],
        [
            "Main quantity we're going to be interested in for active learning is how many label requests are required to learn.",
            "This is called the label complexity, and the hope is that by choosing those examples carefully by choosing only the informative examples to get the labels for, we can dramatically reduce the label complexity compared to what you would need in passive learning, where essentially you just essentially like requesting random labels, or all the labels that in the data set."
        ],
        [
            "So.",
            "OK, so that's active learning in general.",
            "What I want to talk about is a special type of active learning which I'm calling active eisd learning.",
            "So here the active learning algorithm has two components.",
            "It has an active visor which is sort of a meta algorithm, and then it also has a passive learning algorithm, and the idea is you can plug in any passive learning algorithm into this active visor, and the way the way the interaction works is the active visor algorithm is going to be constructing datasets and feeding them into this.",
            "Passive learning algorithm.",
            "Getting back classifiers and in the end we'll have we'll do this maybe several times and get a bunch of classifiers and then the classifier that we output is going to be either one of these, or maybe some combination of them, and so the idea is to keep this general that so that we can plug in a general passive learning algorithm.",
            "Into this active iseran, so I've written.",
            "It could be supervised or semi supervised.",
            "I'm not actually going to talk about the semi supervised case, but it's actually trivial extension to plug in semi supervised algorithms and the motivation for this is actually if you go through ICM, Lorne IFS and look at the active learning papers that are being published in what I might call the empirically driven research literature.",
            "There are almost all of this form.",
            "You typically start with something like logistic regression or SVM and the whole roll of this active learning component is just to construct the datasets that you're going to give to this algorithm, and you might be doing this sort of interactively train and then get some new examples and retrain something like that, but it definitely raises the theoretical question of whether you can always use this strategy and get some improvement over that passive learning subroutine that you're using, so that's exactly what we're going to look at."
        ],
        [
            "Here the main question here is, are there general purpose active izers that strictly improve the label complexity of any passive learning algorithm that you would plug in there?",
            "And so if they do, then we call it.",
            "We say that they're active Ising that algorithm.",
            "OK, so that's the general question we're going to look."
        ],
        [
            "That before I get into this sort of formalizing what is label complexity, what do I mean by active Ising?",
            "Let's just go through maybe an example of why we think that maybe this could happen.",
            "Why?",
            "This is an interesting thing to study and why it's possible in some instances at least.",
            "So the example I'm going to start with is just very simple.",
            "It's the threshold learning problem.",
            "So suppose that are examples are just on the real line and our classifiers look something like.",
            "Their threshold, right?",
            "So there's some point on the real line such that everything to the left is labeled plus and everything to the right is labeled everything to the left.",
            "They will minus and everything to the right is labeled plus.",
            "So this is what the classifiers that we're trying to learn look like.",
            "So let's take a look at an algorithm."
        ],
        [
            "For learning.",
            "These and it's going to be an active learning algorithm and that will see that this is going to be an example of where we can boost that label complexity.",
            "Improve the label complexity.",
            "OK, so.",
            "We're going to say is we're going to allow the algorithm to make this a end label request total, and then we'll figure out how big needs to be in relation to the passive learning label complexity.",
            "So the first step in the algorithm is just let's just use up half of our budget taken over 2 random unlabeled examples and request their labels and I.",
            "So I plotted them on the line and we're just going to find the closest negative and positive example, call them A&B and throw away the data set now.",
            "In the next step, we're going to use some unlabeled examples and estimate the probability of getting an example between A&B.",
            "Now just if you recall the way we chose them, you can already see the probability between A&B is going to be roughly 2 / N right so, but we're going to estimate it using unlabeled data and then we're going to take a sample of size roughly N / 4 times that probability.",
            "So by the reasoning I just mentioned, this is going to be order N ^2.",
            "Unlabeled examples OK.",
            "There they are, and for anything between A&B will request the labels.",
            "And then the interesting thing that happens is anything that's not between A&B anything, let's say to the left of a, so we know that a was a negative example.",
            "So everything to the left of a must also be labeled negative by whatever that target threshold is an.",
            "Similarly for be everything to the right of B, because B was a positive example.",
            "Everything to the right of it also needs to be a positive example, so we can just go ahead and label those ourselves.",
            "We don't have to request their labels, so.",
            "The interesting thing that happened here was.",
            "We took a sample of size order N squared and we requested the labels of.",
            "This ends up being something like N / 2 with high probability.",
            "There's only going to be about 10 /.",
            "2 examples between A&B.",
            "And yet we get labels for all N squared examples, so we'll just go ahead and take all those order N squared examples, feed them into the passive learning algorithm, and take whatever classifier it produces.",
            "So as I just mentioned, we used only end label request total.",
            "But we get a classifier that was trained on order N squared examples.",
            "OK, so whatever the passive learning label complexity was, the label complexity of this algorithm is going to be roughly square root of that.",
            "So that's a strict improvement in label complexity in this case.",
            "In fact, you can sort of apply this idea sequentially if you update A&B after every label request, you can actually boost that up to an exponential improvement, but for our purposes, just you get some improvement.",
            "That's what's important here.",
            "OK, so that's that's an example where you can do this.",
            "This is the type of ideas that we're going to be working with is generating labeled samples that we can feed into algorithms, and those samples are going to be larger than the number of label requests that were used to generate the samples, and that's how we're getting improvements in the label complexity."
        ],
        [
            "So for the rest of the talk, I'm going to start off just by talking about sort of formalizing.",
            "What do I mean by label complexity?",
            "What do I mean by improvements?",
            "Add to these new results which which I really like.",
            "If there's time, I'll get to dealing with noise.",
            "For most of this talk, I'm not going to talk about noise at all.",
            "And it's actually a big issue that hopefully I have time to address, and then I'll conclude with some open problems.",
            "OK, so.",
            "I should mention if there are any clarification questions, feel free to jump in.",
            "Former MoD."
        ],
        [
            "This sort of attack like framework will start off with some instant space, which is where our examples live.",
            "We have some concept space and this is just a set of classifiers and will denote by little D the VC dimension of the set of classifiers.",
            "And just.",
            "For the remainder of the talk will only be dealing with concept spaces where the VC dimension is finite.",
            "We'll also be talking about distribution.",
            "I'll denote by script Ian.",
            "This is the distribution.",
            "Are examples are sampled from.",
            "OK, there's also a special function in the concept space, which we call the target function, and this is essentially what we're trying to learn.",
            "This is what we're trying to approximate.",
            "An will define the error rate of any classifier as just the probability that disagrees with the target function.",
            "On a random example.",
            "Now for the learning problem, we're going to get a sequence of independent identically distributed training examples sampled according to the distribution Script D. Just to keep things simple in this analysis, these are unlabeled, so I'm just going to assume I have essentially inexhaustible supply of unlabeled examples.",
            "And then we'll just talk about the performance in terms of the number of labels that are needed.",
            "OK, so the protocol then is that the algorithm can pick any index in this sequence after it gets to observe.",
            "The ex is, and then it picks one of them and then it requests the label an it it gets back, is the target functions label on that example.",
            "And then basically we're going to be interested in counting how many of these requests it has to make.",
            "And of course the objective here is to find some classifiers such that the error rate of the classifier is small.",
            "OK, so that's."
        ],
        [
            "Model and here's the definition of.",
            "There's an arrow somehow in there.",
            "OK, so here's what label complexity when I say label complexity, this is what I mean.",
            "So we'll say that an algorithm achieves a label complexity Lambda, which has four parameters, epsilon, Delta, target function, and distribution for concept space.",
            "If it outputs some classifier HN after at most N label requests and for any target function distribution, epsilon, Delta for any and larger than the label complexity.",
            "With probability 1 minus Delta, the error rate of the classifier is better than epsilon.",
            "OK, so this is kind of a pack like definition.",
            "What it says is if we have enough, if we allow the algorithm enough label requests a large enough budget, particularly larger than this, then it's with high probability going to output a classifier with a small error rate.",
            "OK, and how small the error rate is is quantified in terms of epsilon now.",
            "This is, as I mentioned, sort of natural pack.",
            "Like definition.",
            "There's two things.",
            "There's one thing I want to mention though, and it's that the label complexity function is parameterized by the target function and the distribution.",
            "So this is not necessarily something you can actually calculate, but it's something that's interesting to analyze from theoretical perspective nonetheless, because it actually tells you something about how many labor requests the algorithm needs.",
            "OK, so that's label complexity.",
            "Now let's talk about what do I mean by improving.",
            "OK, So what will say is so passive learning algorithm.",
            "It's just an algorithm that so essentially you can think of a passive learning algorithm is just an active learning algorithm that requests the first N labels in the sequence.",
            "Or you could think of it as just an algorithm that you give.",
            "A labeled sample 2 and it will use that sample to learn.",
            "So suppose AP is a passive algorithm and it achieves some label complexity, Lambda P. For the concept space, so they will say meta algorithm ace of a active eyes is ASA P for that concept space.",
            "If when you plug in that passive learning algorithm a sub P it generates an active learning algorithm that achieves a label complexity Lambda a which for all non trivial target functions and distributions Lambda a the label complexity active learning algorithm is little O of the label complexity of the passive learning algorithm.",
            "Now I should mention the semantics of the little oh notation here.",
            "What I mean here is that we're going to hold these three parameters fixed Delta F and the distribution fixed, and let epsilon go to zero.",
            "And so if you take Lambda a divided by Lambda P, and let epsilon go to zero, the ratio will go to 0.",
            "Under dependencies.",
            "On the concept space.",
            "So yeah, sort of implicitly.",
            "I'm sorry so this is not a new thing that you're proposing where you only look at them, you know.",
            "That's right, that's right, yeah.",
            "So yeah, if you have a different concept space, you'll get a different value for the label complexity, right?",
            "OK, so five arguments.",
            "Right OK so.",
            "This is this is this is what I mean by improvement is we get an improved dependence, asymptotic dependence on epsilon."
        ],
        [
            "OK, so let's go ahead and look at.",
            "Revisit that algorithm that we looked at for the threshold learning problem and sort of generalize it and see whether this is going to do what we wanted to do.",
            "Is this going to always improve the performance to improve the label complexity of any passive learning algorithm using this active active learning idea?",
            "So the basic, if you recall the basic idea was let's just use half of our budget and request some random examples and request their labels and then we will define the version space just the set of classifiers that get all of them correct.",
            "And in the next step, we're going to estimate the probability of an example such that there are.",
            "Classifiers in the version space that label it plus an minus.",
            "So there's some disagreement about the label.",
            "Once we have that estimate, we're going to take roughly an over 4D.",
            "Labeled examples, random examples, unlabeled examples, and so with high probability, we know only about 10 /.",
            "2 of those are going to be examples such that there's some disagreement about their label in the version space, so we'll go ahead and request the labels of those examples.",
            "And for the rest of the examples, we can label them ourselves, right?",
            "Because all the classifiers in the version space agree on their labels, in particular the target functions in the version space, so those have to be the target functions labels, so we can just come up with them ourselves, and so we get a set of labeled examples drawn IID, which has size N / 4 Delta.",
            "And we feed that into our passive learning algorithm and output whatever.",
            "The result was.",
            "OK, so.",
            "First thing to notice is this produces a a set of labeled examples, their ID, and we can feed them into the passive learning algorithm.",
            "So we get sort of a natural fall back guarantee.",
            "It's never going to be more than Delta is never larger than one, so this is never going to be worse than, say, a factor of four were sample complexity than just using the passive learning algorithm.",
            "OK, but it sort of leaves open the question of.",
            "We have a fall back guarantee, but that's not an improvement guarantee, So what we're looking for is an improvement guarantee and.",
            "So let's take a look at weather."
        ],
        [
            "Actually happens, and it turns out it doesn't.",
            "So the simple algorithm, I mean sorry.",
            "The simple concept space.",
            "Simplest one I can think of where it doesn't happen is the intervals concept space.",
            "So here OK again, our points are just going to be on the real line.",
            "Let's say between zero and one.",
            "Let's say the distribution is just some smooth density and our concept space set of classifiers is just intervals such that it.",
            "Labels everything in the interval is plus and everything outside the interval as minus.",
            "So we have all of these intervals in the concept space.",
            "We're going to look at a very special case of the interval learning problem.",
            "The case where the target function is the empty interval."
        ],
        [
            "OK, so.",
            "So let's just go through what does this algorithm do when the target function is the empty interval.",
            "So we start off just with our random and over 2 examples.",
            "Here they are.",
            "They're all negative, of course, 'cause the target labels everything negative.",
            "OK, we have our version spaces, just the intervals that are consistent with this sample, so it's all intervals that don't contain one of these.",
            "We're going to estimate the probability that of getting a point such that there's some disagreement about its label.",
            "Now, here's where something interesting happens because.",
            "So let's say here's a point and any point that you know this is meant to represent any point that isn't one of the original data points.",
            "You can show that there is always some interval that doesn't contain it, right?",
            "That's pretty clear to see, so you can label it minus.",
            "And there's also some interval that does contain it so you can label it plus right and so you can kind of see that that that happens no matter where you pick the point, as long as it's not one of the initial points.",
            "How can I fix?",
            "Yeah, go ahead, I don't see where do you specify anything in this article.",
            "Oh yeah, so A&B are kind of.",
            "This set so everything between A&B.",
            "Is in the version space, and so there's kind of for thresholds.",
            "It's version space is equal to the region of disagreement, but.",
            "Does that make sense?",
            "Modify this inscriptions say where it where the NPR specified or farm.",
            "And so the NBA was just for the threshold learning problem and that's that was a way to sort of explicitly represent the version space, right?",
            "So the version space was all the thresholds between A&B.",
            "Does that make sense?",
            "So here the version space is all intervals that don't contain any of those points, so it's a different version space.",
            "You can't really write down at A&B for this for the interval learning problem.",
            "OK. No.",
            "Right, so it's just a different concept space, so there's no A&B necessarily.",
            "OK, so.",
            "Anyway, any point that's not one of these initial data points is going to be in this region of disagreement, so this estimate is just going to be one OK.",
            "So when we go to the next step and we sample, we're just going to get.",
            "Roughly an over 4 examples.",
            "And as I mentioned with probability one, all of them are in.",
            "This satisfy this condition that there's some classifier that labels it minus and some classifier that labels it.",
            "Plus so in fact we have to request the label of all over for these examples.",
            "And so we're not going to be able to label the rest ourselves.",
            "There's no, there's none left.",
            "And basically what this says is the set L has size N / 4, so it's still ordering examples.",
            "So this algorithm is not giving us any improvements compared to just running with random label requests.",
            "OK, so this doesn't work.",
            "Can any algorithm did improve?",
            "Can any so if you wanted to find an interval size, what you basically have to prove that there is no interval size epsilon right in this example.",
            "I.",
            "The true targets would be a zero in.",
            "So I put a successful classifier.",
            "You have to prove that there's no interval size apps.",
            "Now I understand why you would think that, but the interesting thing that happens in active learning is sometimes it's actually harder to verify that you have a good classifier then just to find a good classifier so you can find a good classifier, but it would take more examples to prove that.",
            "That it's a good classifier, so you would have to in order to prove that it's a good classifier.",
            "You're right, you have to check every every interval of size 2 epsilon in order to prove that you have an excellent good classifier that would take like one over epsilon samples.",
            "But just to find a good classifier.",
            "Or I can just I just output all negative, right?",
            "That's my default, but so will see an algorithm that works here.",
            "Also, that's more general than that.",
            "OK, so but this particular algorithm doesn't work.",
            "That's what we just established."
        ],
        [
            "So let's take a look at a slight modification of it, which.",
            "Does work for that example?",
            "And then we'll take a look at whether this is generally effective.",
            "So.",
            "OK, so we're going to start off very similar to what we did before.",
            "Will just take some random examples.",
            "This time will use a third of our budget and get labeled examples Q again.",
            "We'll just define the version space as those examples.",
            "Classifiers that get all those examples correct.",
            "An will will initialize a set S just to be the empty set, and that's S is going to be the difference between this and the previous algorithm and you'll see how that works.",
            "OK, so now we're going to go through D + 1 rounds where remember D is the VC dimension.",
            "And So what happens here is on each round again, we're going to estimate a probability, But this time we're going to use that set S and we're going to test whether the version space shatters.",
            "So we're going to find the probability of getting a point such that if you add it to the Set S, the version space will shatter the combined set.",
            "So those who don't recall.",
            "So when shattering means that you can label it.",
            "So if there's K points in the set, then shattering means that you can find 2 to the K classifiers, INV.",
            "That label.",
            "Those points in all two to the K different possible ways.",
            "So we're going to find the probability of an example, such as the version space shatters, and then we'll sample just as before, roughly N over set.",
            "This time will divide by day because of the loops.",
            "Get lichen over 60 times Delta examples so you can show with high probability that.",
            "At most roughly an over 2D of those examples are going to be points such that these shatters the combined set of S with that point.",
            "So for anything where that happens, we will request the label.",
            "For those that don't, that doesn't happen for what we're going to do is because we know that adding SX to S makes a set that can't be shattered.",
            "It means that there is some labeling of the points in S such that there is a forced label of X, so there's some way that you can label the points in S such that any classifier in the version space that labels the points in S that way must label the point X.",
            "In one particular way, and we'll just take that label and label the point ourselves.",
            "And then here's the new thing.",
            "We're going to take just another point at random.",
            "Subject to the fact that if you add it to S, you can still shatter this set using the version space.",
            "It doesn't exist.",
            "Don't worry bout that but.",
            "If it does exist, then we'll add it to us and we'll use that.",
            "Set us on the next round.",
            "OK, so this is this is the algorithm.",
            "And then at the end what we get is D + 1 different training sets L. So each one is Elsas K for the case round and we just feed them into the passive learning algorithm and we end up with D + 1 different classifiers and we have to pick between them basically.",
            "So this is just kind of a clean up step.",
            "If you go through the bounds here, each of these steps was using something like N. / 2 D samples labels.",
            "Sorry N / 3 D labels and so there were deer rounds where we actually request labels.",
            "And here's an over three up here, so we should have about a third of our budget leftover, and that's what we're using in the selection phase, and we're just going to take each pair, find a bunch of examples where they disagree, request the labels of those examples, and pick whichever classifier doesn't make too many mistakes on any of those pairwise comparisons.",
            "So this is just sort of a simple way to choose which of those classifiers we want to output is just do some sort of pairwise tournaments.",
            "Ascentia Lee."
        ],
        [
            "So.",
            "So that's not too important.",
            "You can see."
        ],
        [
            "The.",
            "If you have enough examples, you can use turn off balance to guarantee that.",
            "In fact, whatever the error rate of the best classifier in here is the one that you output will have an error rate.",
            "Let's say at most twice that."
        ],
        [
            "The big thing that we're going to be interested in is proving that one of these labeled samples has correct labels and is growing super linearly.",
            "And if that's true then we definitely get these improvements.",
            "OK, so let's take a look at how this algorithm behaves on the interval's problem.",
            "So again, just as before, we'll assume that labels everything is minus one.",
            "And OK, so our first samples are just this and over 3 random examples.",
            "Here they are.",
            "And our version spaces again just all the intervals that don't include any of those.",
            "Examples.",
            "So I'll just keep them up there just so we remember where they are.",
            "OK, so now first note that the first round of this algorithm when K = 1, this is just the naive algorithm from before, right?",
            "So testing whether V shatters emptyset Union X is just testing whether there exists a classifier that labels it.",
            "Plus in a classifier that labels it minus.",
            "And this is exactly what we did in the naive algorithm, so we know what's going to happen in the first round.",
            "We're going to sell one that is growing linearly in the number of samples.",
            "OK, so let's forget about that and let's just focus on what happens in the second round.",
            "So we see dimension here is 2, so we have a second round.",
            "And what's going to happen at the end of the first round?",
            "We get some point.",
            "Let's say it's this one.",
            "And that's this X one, and we're going to set, and that's the set that we're using on the second round.",
            "And so here we get to this point where we have to estimate the probability that the version space shatters.",
            "The set which contains this point with the point.",
            "So here's what happens.",
            "Take any point.",
            "Let's say we take this point.",
            "OK, so if.",
            "If we have a classifier that labels this point as minus, then that classifier.",
            "Could label this point as plus or minus right?",
            "On the other hand, if we have a classified label, this point as plus.",
            "There's no way it can label this point as plus also, because that would be an interval that contains a negative point, so.",
            "The classifiers in the version space cannot shatter this point at this point.",
            "So.",
            "The only points in fact that the classifiers in the version space can shatter are the ones that are between this negative example and this negative example, and not this point.",
            "Any point that's in here?",
            "You can add it to the set S and still shatter it.",
            "So the probability of getting a point such that we added to S and we can shatter is just the probability mass contained between these two points.",
            "That's going to be roughly 3 / N. So in this step we're just going to sample.",
            "Roughly, an over 60 Delta examples and we said Delta is roughly 3 / N, so this is going to be order N squared examples.",
            "And here we request the label of any of them such that the version space can shatter when you add them to S. So that's again just the points that are between here.",
            "It's these points, of course, they're negative, because everything is negative.",
            "For the other points.",
            "There.",
            "Outside, so remember when X was labeled.",
            "Plus we could only label these other points as minus, so in this step we just automatically infer that the label of those other points is minus.",
            "OK, let's just clear out the.",
            "The initial sample and X1 and what we're left with is the set L2.",
            "And remember that L2 was of size roughly N ^2.",
            "So and all the labels are correct, they're all minus as they should be.",
            "So we're feeding a set of examples that's growing super linearly in North into the passive learning algorithm, so the second classifier is going to achieve a strict improvement in label complexity.",
            "OK, so so this algorithm does improve label complexity, so we would say that this this algorithm, active eyes is any passive learning algorithm for the interval learning problem.",
            "OK."
        ],
        [
            "So we took this problem that looked like it was hard.",
            "And our naive algorithm certainly failed.",
            "I should also mention there's obvious ways to make this more efficient in terms of labels.",
            "Certainly you could update the version space after every label requests and get some improvements, so that's."
        ],
        [
            "Trivial little trick that you can do."
        ],
        [
            "This algorithm and you would get better label complexities, although already we had.",
            "We have this algorithm improving the label complexity compared to passive.",
            "So we can improve compared to passive for this interval learning problem, but we still have the general question right?",
            "Can we improve over the label complexities?",
            "For any passive algorithm for any concept space.",
            "Um?",
            "Right, so it turns out that we can."
        ],
        [
            "So this simple ACT Act Divisor algorithm does in fact active eyes.",
            "Any passive learning algorithm, so it improves the label complexity.",
            "For any concept space.",
            "So the simple corollary that we can get from this we know of a passive learning algorithm that for any concept space gets in order one over epsilon sample complexity.",
            "And so if we just plug that in to the active visor, we immediately get that you can always get an active learning label complexity that's little low of one over epsilon.",
            "So that's definitely something you can't prove about passive learning."
        ],
        [
            "OK, so.",
            "Unfortunately I don't have time and I doubt that you have the patience for me to go through the proof of this, but maybe I'll just stand up here and wave my hands for a little bit anyway.",
            "Just to give you a little bit of flavor of how the how the proof works and why this algorithm might give you these kinds of improvements.",
            "OK, so first of all, note that if.",
            "The probability of that region of disagreement probability that we can find classifiers in the version space that disagree on the label of a point is decreasing to 0 on that first round.",
            "Then we're done because we get a sample of size N divided by Delta, and so that's growing super linearly with N, and we can just take that first data set.",
            "So that's actually a condition where that naive algorithm will work.",
            "On the other hand, if that doesn't happen, what it means is that for large enough, N gets very large.",
            "You're going to the set is going to be decreasing, but not going past some point, so there's some constant such that we're not getting a set that's smaller than that constant.",
            "OK, so whatever that limiting set is.",
            "We know that for large enough sample size that that point that we're adding to S is going to go into.",
            "It's going to fall in that set, right?",
            "And if you think about it, what this means?",
            "What does it mean to be in that limiting set as N gets large?",
            "What it means is that there exists classifiers.",
            "That OK, so this should be X one.",
            "So what it means is there's this classifiers that label X1 as positive, which have an arbitrarily good error rate.",
            "There arbitrarily close to the target function, and there also exists classifiers that label X one is negative that are also arbitrarily close to the target function.",
            "OK.",
            "It's because N gets large.",
            "Our version space is getting closer and closer to the target function.",
            "All the classifiers are close to the target function.",
            "So in particular, with high probability, if you take sort with probably 1.",
            "Actually any example X that's agreed upon by all the classifiers that label X one is plus one or all the classifiers that label X One X -- 1.",
            "Fully agree upon has to be the target's label, because these are arbitrarily good classifiers, so.",
            "Basically what we can say here is that we know the label of any example such that if you add it to the set with X one and it's not sharable, whatever that remaining label is the label that we can't realize that has to be the label that we can realize, such that you can't label it the other way that has to be the target's label.",
            "OK. Now you can repeat this argument for the larger values of K and you can keep doing that until you get a value of Delta that doesn't get stuck at some constant.",
            "If you get a value of Delta that goes all the way to 0, then you're no longer guaranteed to find your shattered point in that limiting set.",
            "But that's OK, because if we get a value of K where Delta goes to 0, then we're sampling an over Delta examples.",
            "And so the set LK is going to be increasing super linearly, and so that's the set that we want that we want to be competitive with.",
            "That's the one that improves over passive.",
            "So that's kind of the slave."
        ],
        [
            "Of how the proof goes.",
            "Of course, this is a question of efficiency and OK, so I guess first of all, if you can't find even in passive learning if you can't find a separator or classifier that's consistent with your data.",
            "There's really nothing we can do here.",
            "Can't help you.",
            "But suppose you can do that efficiently.",
            "Then we're still left with the task of testing whether a set of at most D points is Shadow rible by the set of classifiers consistent with that initial sample.",
            "So for some spaces I would imagine this is going to be exponential in the VC dimension.",
            "And.",
            "But in many cases I think it might be efficient.",
            "Now, I haven't actually gone through the linear algebra, but I think for linear separators you can probably do this efficiently.",
            "Essentially, there's some relationship between shatter ability and linear independence.",
            "And so I think you can set up some kind of constraint satisfaction problem for linear separators that should.",
            "It should be efficiently solvable and many other concept spaces certainly should be able to test for shatter ability.",
            "Subject to these constraints."
        ],
        [
            "OK, so that's efficiency.",
            "The remaining issue, and since I have a couple of minutes I can talk about it.",
            "It's dealing with noise.",
            "I haven't mentioned it all about noise.",
            "It's actually really tricky problem for us.",
            "And.",
            "OK, so first of all, let's there's some modification to the definitions that are needed just to talk about it.",
            "So now what we're talking about is an arbitrary distribution over examples and labels.",
            "And, well, the label complexity is now just.",
            "I guess you could put the concept space in there also an it's just epsilon Delta and now that joint distribution over instance in label.",
            "And now we're measuring this epsilon is not error rate anymore, it's actually excess error rate over the best classifier in the concept space.",
            "So what we want is to guarantee with high probability that the error rate of the classifier we output is at most epsilon worse than the best error rate you could hope for from that concept space.",
            "OK, and then again active Ising would mean little oh of the passive learning sample complexity in terms of dependence on epsilon."
        ],
        [
            "So.",
            "OK, one simple modification that you probably definitely want to do is change this notion of version space, right?",
            "So if there's noisy examples, you definitely don't want to throw away classifiers that make one mistake.",
            "So in fact, suggestion here is to make a noise robust version space, which is just the set of classifiers such that the number of mistakes they make is not too much larger than any other classifier in the concept space, where too much larger is something like.",
            "One over root and or you could plug in some other bounds there as well, but basically uniform convergence bounds will imply that this version space still contains the reasonably good classifiers.",
            "OK, so.",
            "So it turns out that if you do this, you can get some results that are very nice if you plug in a particular passive learning algorithm and you use just this, just this version space instead of that one as your modified algorithm, and then you can get a label complexity that's always little of one over epsilon squared.",
            "So if you recall in passive learning theory call 701 you can always get big O of one over epsilon squared in passive learning.",
            "So here with active learning we're going to load that.",
            "And for those of you familiar with people cause noise conditions, you can get passive learning one over epsilon to the 2 -- 1 over cap.",
            "So here we can get a little of that.",
            "Also you have to be a little bit more careful about how you do certain steps in the algorithm, but.",
            "But in general, we don't yet have a solution to whether you can active eyes.",
            "Any passive learning algorithm, even with noise.",
            "This is really tough problem and interesting to resolve, either positive or negative.",
            "In fact, we don't even know if you can active eyes.",
            "Some empirical risk minimization, minimizing algorithm, so this would be interesting because at least there would be a little more general than this would be able to say that any theorem you can prove about empirical risk minimization, But the whole family of empirical risk.",
            "Minimization algorithms we would be able to do better than it with.",
            "The.",
            "Particular possible for this result?",
            "So particular passive algorithm that if you plug that in, you get an active learning algorithm.",
            "Yeah, it's basically empirical risk minimization subject to the examples that you automatically labeled are like constraints.",
            "You have to get those right and then you do empirical risk minimization on the rest.",
            "Any assumptions about the distribution of the noise here that's right, and if you work for example.",
            "Yeah, absolutely right.",
            "So this is a type of assumption about noise conditions.",
            "I don't want to get into the details of what it is, but.",
            "This tells you something about the noise around the decision boundary and you can get as you can see, this sort of interpolates if Kappa is bigger than one interpolates between one and two.",
            "Right, but in general for the for this problem, I don't even know about about that case.",
            "If you have some special if you have any kind of noise, I don't know how to act of eyes.",
            "Arbitrary passive learning algorithms.",
            "And it's just because you could have some funny dependence on the noise, right?",
            "OK, so."
        ],
        [
            "Just to wrap up, turns out you can activate any passive learning algorithm and we saw a simple method that does this in the 0 error finite VC dimension case.",
            "Several questions if we have infinite VC dimension is not clear what happens anymore.",
            "There's some ideas about maybe you could still run this algorithm and it would just have an infinite sequence of classifiers, or I guess more and more classifiers.",
            "It's an increases, but I don't have an analysis for that as of yet.",
            "There's also a question.",
            "OK, so we proved that you get improvements over passive learning, but we didn't say how big those improvements are.",
            "So this is another question that's interesting to look at, is what specific label complexity bounds can we get for these algorithms or variants of them?",
            "I mentioned some ways to improve the algorithms that I presented makes them a little bit more complicated, but better label complexities.",
            "And then of course the big question, what about when there's noise?",
            "Can we always active eyes any passive learning algorithm, even in the presence of noise?",
            "I think that's a tough problem."
        ],
        [
            "Thanks.",
            "Happy to take questions."
        ],
        [
            "Very theoretical question is.",
            "What this would mean when you actually apply to concrete examples?",
            "Let's say you want to do some part of speech tagging supplies, or some some traffic.",
            "So how much would we win?",
            "Under under this.",
            "Yeah, so I guess in general.",
            "The question you're asking is how much improvement in label complexity do we get for small samples.",
            "Basically, for values of epsilon that we could actually achieve with passive learning algorithms and not just in the limit and.",
            "So in some cases you can definitely get large improvements, but.",
            "In other cases, you don't in many cases, and so you can construct examples where essentially you don't see those improvements until epsilon is so small that maybe that's not a practical concern, but you can definitely also construct examples in thresholds.",
            "For example, and generalizations of that, such as linear separators where you see these improvements even for reasonable values of epsilon, and I think those have been coming out in experimental evaluations.",
            "Of other algorithms as well, but it would.",
            "I'm not sure exactly what this looks like in comparison.",
            "If you run it, it's something I am interested.",
            "I might I might run those experiments, will see.",
            "Question why is your work in general?",
            "I could learn how to fill in the area for other men.",
            "The basic idea is to try to find the the labeled examples around.",
            "It's around decision boundaries and then so, so that means the selection of the data points.",
            "It has to be has to be dependent on the distribution of two classes or multiclass examples, but the whole thing you are doing is trying to say well.",
            "Try to find those, think about what you are.",
            "Carrier to choose the subset of the data, points to label.",
            "Then.",
            "Basically you're saying that using the short ability OK, the shot ability.",
            "Basically, things that allow the subset of labels to the class labels.",
            "It would be arbitrary.",
            "So basically you lost your concentration on the truth is even boundary of given set of labeled data.",
            "So I don't understand the philosophy.",
            "I don't necessarily think that's true.",
            "I think actually you're going to be focusing still for sort of geometric concept spaces.",
            "You're still going to be focusing close to the decision boundary of the target function.",
            "So for example, if you just think about the naive algorithm that I mentioned, you're just sampling points such that you can disagree on the label within the version space, so the version space is actually just going to be sort of the set of classifiers that look like the target function, but maybe are slightly varied because after a certain number of labeled examples you can guarantee you're close to the target function.",
            "Your decision boundary isn't too far right, so the samples that you take are actually going to be.",
            "Within that region of disagreement of the set of classifiers that are close to the target functions.",
            "The boundary, so that's in the situation where the naive algorithm works.",
            "In the situation where it doesn't work, where you have to add these shattered points.",
            "It's a little bit more complicated.",
            "I haven't really thought too much about it, but I think you still get points that are sort of close to the target functions boundary in many cases, but I don't have to think a little bit more about it, but I think you still get some kind of geometric intuition.",
            "It's just that there's regions of the space that you couldn't rule out you couldn't figure out what the labels were from sampling from the region of disagreement, so you just sort of make up labels for those regions, and that allows you to focus more as you'd like to be able to focus.",
            "Quick question, I didn't quite get it.",
            "Efficiency point, so you will calculating the probability of the sample where you add that sample you cannot settle.",
            "While sub procedure you need to compute the shatter ability.",
            "So how do you actually compute the probability of the samples where you cannot shatter when you add the sample?",
            "Well, so all you need is the probability of the set that you can shatter, right?",
            "Um?",
            "That either implies the other, but yeah, so if you estimate the probability of the set of points that you can shatter, and that's actually what you needed in the algorithm, 'cause those are the points you're going to request labels for, right?",
            "So you just sample a bunch and then count how many you can shatter.",
            "Yep.",
            "Because it's only for something so easy to estimate the problem.",
            "Well, I mean, it's easy if you can test whether you can shatter set.",
            "'cause you just sample a bunch of unlabeled points and then for each one you add it to the set test whether you can shatter.",
            "That's an indicator function.",
            "You just count how many are 1.",
            "Great.",
            "Yeah, two strong suction that cost parking perfectly separate handles right and that's why I mentioned this deal."
        ],
        [
            "With noise problem such kind of problem.",
            "You do sometimes.",
            "There are NLP datasets that are linearly separable, for example, just because you have so many dimensions.",
            "But in general, you definitely want to deal with this noise problem, and I don't think it's such a big problem for some natural passive learning algorithms that you might want to plug in, but to get a general theorem just theoretically speaking, it's difficult.",
            "Algorithm why do you need to look through four from VC dimension?",
            "One 2D isn't enough.",
            "Today plus one is.",
            "Right so.",
            "Sometimes you want it, sometimes you want the plus one.",
            "So D + 1 is not going to automatically label all the points, right?",
            "Because you can't shatter D + 1 points, so every point is going to have some automatic label that you can assign to it.",
            "So it's just.",
            "There's just some cases where you want to use that last where they're all automatically labeled instead of just.",
            "You can construct spaces where you want to do that.",
            "Loop is for number of samples of the loop.",
            "Here let's go back."
        ],
        [
            "Yeah, so yeah, it's the loop is through values of K less than the D + 1, so Dee Dee is the VC dimension right?",
            "So you definitely can't shatter D + 1 points just by definition of VC dimension, so the last loop through this is just going to automatically label all the points in this sample.",
            "So just you can just there's some spaces where you want to use.",
            "This is the algorithm that this is the call to the algorithm that gives you the good label complexity.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this week I'm going to be talking.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking a bit about active learning, particular type of active learning.",
                    "label": 0
                },
                {
                    "sent": "I've been working on recently and there's some will get to some fairly general and in my opinion extremely exciting.",
                    "label": 0
                },
                {
                    "sent": "New results about the strength of active learning in relation to passive learning.",
                    "label": 0
                },
                {
                    "sent": "So I'll start.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just by defining what do I mean by passive learning and then will contrast it and talk about active learning.",
                    "label": 0
                },
                {
                    "sent": "So in passive learning this is something probably most of you have seen we have.",
                    "label": 0
                },
                {
                    "sent": "This is just the supervised model of learning, so we have some data source and expert Anna learning algorithm and basically what happens is some raw unlabeled data comes from the data source and goes and goes to the expert.",
                    "label": 1
                },
                {
                    "sent": "The expert labels those unlabeled examples passes the labeled data set into the learning algorithm, does some processing and then.",
                    "label": 0
                },
                {
                    "sent": "Eventually outputs a classifier and what we hope is that this classifier will agree with the expert on future examples from that data source.",
                    "label": 1
                },
                {
                    "sent": "So in contrast, in active learning, so the players are all the same, but the protocol is a little different that unlabeled data is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go straight into the learning algorithm and we're going to let the learning algorithm choose examples.",
                    "label": 0
                },
                {
                    "sent": "Particular examples from the set and ask the expert for the labels of those examples.",
                    "label": 0
                },
                {
                    "sent": "The expert returns the label and then the algorithm chooses another example, and so this happens interactively and then we get the label of that example after a number of rounds.",
                    "label": 1
                },
                {
                    "sent": "Eventually the algorithm outputs a classifier, and again we're hoping that this classifier is going to agree with the expert on most future examples from that data source.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Main quantity we're going to be interested in for active learning is how many label requests are required to learn.",
                    "label": 0
                },
                {
                    "sent": "This is called the label complexity, and the hope is that by choosing those examples carefully by choosing only the informative examples to get the labels for, we can dramatically reduce the label complexity compared to what you would need in passive learning, where essentially you just essentially like requesting random labels, or all the labels that in the data set.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's active learning in general.",
                    "label": 0
                },
                {
                    "sent": "What I want to talk about is a special type of active learning which I'm calling active eisd learning.",
                    "label": 0
                },
                {
                    "sent": "So here the active learning algorithm has two components.",
                    "label": 0
                },
                {
                    "sent": "It has an active visor which is sort of a meta algorithm, and then it also has a passive learning algorithm, and the idea is you can plug in any passive learning algorithm into this active visor, and the way the way the interaction works is the active visor algorithm is going to be constructing datasets and feeding them into this.",
                    "label": 0
                },
                {
                    "sent": "Passive learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Getting back classifiers and in the end we'll have we'll do this maybe several times and get a bunch of classifiers and then the classifier that we output is going to be either one of these, or maybe some combination of them, and so the idea is to keep this general that so that we can plug in a general passive learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Into this active iseran, so I've written.",
                    "label": 0
                },
                {
                    "sent": "It could be supervised or semi supervised.",
                    "label": 0
                },
                {
                    "sent": "I'm not actually going to talk about the semi supervised case, but it's actually trivial extension to plug in semi supervised algorithms and the motivation for this is actually if you go through ICM, Lorne IFS and look at the active learning papers that are being published in what I might call the empirically driven research literature.",
                    "label": 0
                },
                {
                    "sent": "There are almost all of this form.",
                    "label": 0
                },
                {
                    "sent": "You typically start with something like logistic regression or SVM and the whole roll of this active learning component is just to construct the datasets that you're going to give to this algorithm, and you might be doing this sort of interactively train and then get some new examples and retrain something like that, but it definitely raises the theoretical question of whether you can always use this strategy and get some improvement over that passive learning subroutine that you're using, so that's exactly what we're going to look at.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here the main question here is, are there general purpose active izers that strictly improve the label complexity of any passive learning algorithm that you would plug in there?",
                    "label": 1
                },
                {
                    "sent": "And so if they do, then we call it.",
                    "label": 0
                },
                {
                    "sent": "We say that they're active Ising that algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the general question we're going to look.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That before I get into this sort of formalizing what is label complexity, what do I mean by active Ising?",
                    "label": 0
                },
                {
                    "sent": "Let's just go through maybe an example of why we think that maybe this could happen.",
                    "label": 1
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "This is an interesting thing to study and why it's possible in some instances at least.",
                    "label": 0
                },
                {
                    "sent": "So the example I'm going to start with is just very simple.",
                    "label": 0
                },
                {
                    "sent": "It's the threshold learning problem.",
                    "label": 0
                },
                {
                    "sent": "So suppose that are examples are just on the real line and our classifiers look something like.",
                    "label": 0
                },
                {
                    "sent": "Their threshold, right?",
                    "label": 0
                },
                {
                    "sent": "So there's some point on the real line such that everything to the left is labeled plus and everything to the right is labeled everything to the left.",
                    "label": 0
                },
                {
                    "sent": "They will minus and everything to the right is labeled plus.",
                    "label": 0
                },
                {
                    "sent": "So this is what the classifiers that we're trying to learn look like.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look at an algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For learning.",
                    "label": 0
                },
                {
                    "sent": "These and it's going to be an active learning algorithm and that will see that this is going to be an example of where we can boost that label complexity.",
                    "label": 0
                },
                {
                    "sent": "Improve the label complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We're going to say is we're going to allow the algorithm to make this a end label request total, and then we'll figure out how big needs to be in relation to the passive learning label complexity.",
                    "label": 0
                },
                {
                    "sent": "So the first step in the algorithm is just let's just use up half of our budget taken over 2 random unlabeled examples and request their labels and I.",
                    "label": 0
                },
                {
                    "sent": "So I plotted them on the line and we're just going to find the closest negative and positive example, call them A&B and throw away the data set now.",
                    "label": 0
                },
                {
                    "sent": "In the next step, we're going to use some unlabeled examples and estimate the probability of getting an example between A&B.",
                    "label": 1
                },
                {
                    "sent": "Now just if you recall the way we chose them, you can already see the probability between A&B is going to be roughly 2 / N right so, but we're going to estimate it using unlabeled data and then we're going to take a sample of size roughly N / 4 times that probability.",
                    "label": 0
                },
                {
                    "sent": "So by the reasoning I just mentioned, this is going to be order N ^2.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled examples OK.",
                    "label": 0
                },
                {
                    "sent": "There they are, and for anything between A&B will request the labels.",
                    "label": 1
                },
                {
                    "sent": "And then the interesting thing that happens is anything that's not between A&B anything, let's say to the left of a, so we know that a was a negative example.",
                    "label": 0
                },
                {
                    "sent": "So everything to the left of a must also be labeled negative by whatever that target threshold is an.",
                    "label": 0
                },
                {
                    "sent": "Similarly for be everything to the right of B, because B was a positive example.",
                    "label": 0
                },
                {
                    "sent": "Everything to the right of it also needs to be a positive example, so we can just go ahead and label those ourselves.",
                    "label": 0
                },
                {
                    "sent": "We don't have to request their labels, so.",
                    "label": 1
                },
                {
                    "sent": "The interesting thing that happened here was.",
                    "label": 0
                },
                {
                    "sent": "We took a sample of size order N squared and we requested the labels of.",
                    "label": 0
                },
                {
                    "sent": "This ends up being something like N / 2 with high probability.",
                    "label": 0
                },
                {
                    "sent": "There's only going to be about 10 /.",
                    "label": 0
                },
                {
                    "sent": "2 examples between A&B.",
                    "label": 0
                },
                {
                    "sent": "And yet we get labels for all N squared examples, so we'll just go ahead and take all those order N squared examples, feed them into the passive learning algorithm, and take whatever classifier it produces.",
                    "label": 0
                },
                {
                    "sent": "So as I just mentioned, we used only end label request total.",
                    "label": 0
                },
                {
                    "sent": "But we get a classifier that was trained on order N squared examples.",
                    "label": 1
                },
                {
                    "sent": "OK, so whatever the passive learning label complexity was, the label complexity of this algorithm is going to be roughly square root of that.",
                    "label": 0
                },
                {
                    "sent": "So that's a strict improvement in label complexity in this case.",
                    "label": 1
                },
                {
                    "sent": "In fact, you can sort of apply this idea sequentially if you update A&B after every label request, you can actually boost that up to an exponential improvement, but for our purposes, just you get some improvement.",
                    "label": 0
                },
                {
                    "sent": "That's what's important here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's an example where you can do this.",
                    "label": 0
                },
                {
                    "sent": "This is the type of ideas that we're going to be working with is generating labeled samples that we can feed into algorithms, and those samples are going to be larger than the number of label requests that were used to generate the samples, and that's how we're getting improvements in the label complexity.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the rest of the talk, I'm going to start off just by talking about sort of formalizing.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by label complexity?",
                    "label": 0
                },
                {
                    "sent": "What do I mean by improvements?",
                    "label": 0
                },
                {
                    "sent": "Add to these new results which which I really like.",
                    "label": 1
                },
                {
                    "sent": "If there's time, I'll get to dealing with noise.",
                    "label": 1
                },
                {
                    "sent": "For most of this talk, I'm not going to talk about noise at all.",
                    "label": 0
                },
                {
                    "sent": "And it's actually a big issue that hopefully I have time to address, and then I'll conclude with some open problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I should mention if there are any clarification questions, feel free to jump in.",
                    "label": 0
                },
                {
                    "sent": "Former MoD.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This sort of attack like framework will start off with some instant space, which is where our examples live.",
                    "label": 0
                },
                {
                    "sent": "We have some concept space and this is just a set of classifiers and will denote by little D the VC dimension of the set of classifiers.",
                    "label": 0
                },
                {
                    "sent": "And just.",
                    "label": 0
                },
                {
                    "sent": "For the remainder of the talk will only be dealing with concept spaces where the VC dimension is finite.",
                    "label": 0
                },
                {
                    "sent": "We'll also be talking about distribution.",
                    "label": 0
                },
                {
                    "sent": "I'll denote by script Ian.",
                    "label": 0
                },
                {
                    "sent": "This is the distribution.",
                    "label": 0
                },
                {
                    "sent": "Are examples are sampled from.",
                    "label": 0
                },
                {
                    "sent": "OK, there's also a special function in the concept space, which we call the target function, and this is essentially what we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "This is what we're trying to approximate.",
                    "label": 0
                },
                {
                    "sent": "An will define the error rate of any classifier as just the probability that disagrees with the target function.",
                    "label": 0
                },
                {
                    "sent": "On a random example.",
                    "label": 0
                },
                {
                    "sent": "Now for the learning problem, we're going to get a sequence of independent identically distributed training examples sampled according to the distribution Script D. Just to keep things simple in this analysis, these are unlabeled, so I'm just going to assume I have essentially inexhaustible supply of unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "And then we'll just talk about the performance in terms of the number of labels that are needed.",
                    "label": 0
                },
                {
                    "sent": "OK, so the protocol then is that the algorithm can pick any index in this sequence after it gets to observe.",
                    "label": 0
                },
                {
                    "sent": "The ex is, and then it picks one of them and then it requests the label an it it gets back, is the target functions label on that example.",
                    "label": 0
                },
                {
                    "sent": "And then basically we're going to be interested in counting how many of these requests it has to make.",
                    "label": 0
                },
                {
                    "sent": "And of course the objective here is to find some classifiers such that the error rate of the classifier is small.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model and here's the definition of.",
                    "label": 0
                },
                {
                    "sent": "There's an arrow somehow in there.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's what label complexity when I say label complexity, this is what I mean.",
                    "label": 0
                },
                {
                    "sent": "So we'll say that an algorithm achieves a label complexity Lambda, which has four parameters, epsilon, Delta, target function, and distribution for concept space.",
                    "label": 0
                },
                {
                    "sent": "If it outputs some classifier HN after at most N label requests and for any target function distribution, epsilon, Delta for any and larger than the label complexity.",
                    "label": 0
                },
                {
                    "sent": "With probability 1 minus Delta, the error rate of the classifier is better than epsilon.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of a pack like definition.",
                    "label": 0
                },
                {
                    "sent": "What it says is if we have enough, if we allow the algorithm enough label requests a large enough budget, particularly larger than this, then it's with high probability going to output a classifier with a small error rate.",
                    "label": 0
                },
                {
                    "sent": "OK, and how small the error rate is is quantified in terms of epsilon now.",
                    "label": 0
                },
                {
                    "sent": "This is, as I mentioned, sort of natural pack.",
                    "label": 0
                },
                {
                    "sent": "Like definition.",
                    "label": 0
                },
                {
                    "sent": "There's two things.",
                    "label": 0
                },
                {
                    "sent": "There's one thing I want to mention though, and it's that the label complexity function is parameterized by the target function and the distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is not necessarily something you can actually calculate, but it's something that's interesting to analyze from theoretical perspective nonetheless, because it actually tells you something about how many labor requests the algorithm needs.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's label complexity.",
                    "label": 0
                },
                {
                    "sent": "Now let's talk about what do I mean by improving.",
                    "label": 0
                },
                {
                    "sent": "OK, So what will say is so passive learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's just an algorithm that so essentially you can think of a passive learning algorithm is just an active learning algorithm that requests the first N labels in the sequence.",
                    "label": 0
                },
                {
                    "sent": "Or you could think of it as just an algorithm that you give.",
                    "label": 0
                },
                {
                    "sent": "A labeled sample 2 and it will use that sample to learn.",
                    "label": 0
                },
                {
                    "sent": "So suppose AP is a passive algorithm and it achieves some label complexity, Lambda P. For the concept space, so they will say meta algorithm ace of a active eyes is ASA P for that concept space.",
                    "label": 0
                },
                {
                    "sent": "If when you plug in that passive learning algorithm a sub P it generates an active learning algorithm that achieves a label complexity Lambda a which for all non trivial target functions and distributions Lambda a the label complexity active learning algorithm is little O of the label complexity of the passive learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now I should mention the semantics of the little oh notation here.",
                    "label": 0
                },
                {
                    "sent": "What I mean here is that we're going to hold these three parameters fixed Delta F and the distribution fixed, and let epsilon go to zero.",
                    "label": 0
                },
                {
                    "sent": "And so if you take Lambda a divided by Lambda P, and let epsilon go to zero, the ratio will go to 0.",
                    "label": 0
                },
                {
                    "sent": "Under dependencies.",
                    "label": 0
                },
                {
                    "sent": "On the concept space.",
                    "label": 0
                },
                {
                    "sent": "So yeah, sort of implicitly.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry so this is not a new thing that you're proposing where you only look at them, you know.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right, yeah.",
                    "label": 0
                },
                {
                    "sent": "So yeah, if you have a different concept space, you'll get a different value for the label complexity, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so five arguments.",
                    "label": 0
                },
                {
                    "sent": "Right OK so.",
                    "label": 0
                },
                {
                    "sent": "This is this is this is what I mean by improvement is we get an improved dependence, asymptotic dependence on epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's go ahead and look at.",
                    "label": 0
                },
                {
                    "sent": "Revisit that algorithm that we looked at for the threshold learning problem and sort of generalize it and see whether this is going to do what we wanted to do.",
                    "label": 0
                },
                {
                    "sent": "Is this going to always improve the performance to improve the label complexity of any passive learning algorithm using this active active learning idea?",
                    "label": 0
                },
                {
                    "sent": "So the basic, if you recall the basic idea was let's just use half of our budget and request some random examples and request their labels and then we will define the version space just the set of classifiers that get all of them correct.",
                    "label": 0
                },
                {
                    "sent": "And in the next step, we're going to estimate the probability of an example such that there are.",
                    "label": 0
                },
                {
                    "sent": "Classifiers in the version space that label it plus an minus.",
                    "label": 0
                },
                {
                    "sent": "So there's some disagreement about the label.",
                    "label": 0
                },
                {
                    "sent": "Once we have that estimate, we're going to take roughly an over 4D.",
                    "label": 0
                },
                {
                    "sent": "Labeled examples, random examples, unlabeled examples, and so with high probability, we know only about 10 /.",
                    "label": 0
                },
                {
                    "sent": "2 of those are going to be examples such that there's some disagreement about their label in the version space, so we'll go ahead and request the labels of those examples.",
                    "label": 0
                },
                {
                    "sent": "And for the rest of the examples, we can label them ourselves, right?",
                    "label": 0
                },
                {
                    "sent": "Because all the classifiers in the version space agree on their labels, in particular the target functions in the version space, so those have to be the target functions labels, so we can just come up with them ourselves, and so we get a set of labeled examples drawn IID, which has size N / 4 Delta.",
                    "label": 0
                },
                {
                    "sent": "And we feed that into our passive learning algorithm and output whatever.",
                    "label": 0
                },
                {
                    "sent": "The result was.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "First thing to notice is this produces a a set of labeled examples, their ID, and we can feed them into the passive learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we get sort of a natural fall back guarantee.",
                    "label": 0
                },
                {
                    "sent": "It's never going to be more than Delta is never larger than one, so this is never going to be worse than, say, a factor of four were sample complexity than just using the passive learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, but it sort of leaves open the question of.",
                    "label": 0
                },
                {
                    "sent": "We have a fall back guarantee, but that's not an improvement guarantee, So what we're looking for is an improvement guarantee and.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look at weather.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually happens, and it turns out it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So the simple algorithm, I mean sorry.",
                    "label": 0
                },
                {
                    "sent": "The simple concept space.",
                    "label": 0
                },
                {
                    "sent": "Simplest one I can think of where it doesn't happen is the intervals concept space.",
                    "label": 0
                },
                {
                    "sent": "So here OK again, our points are just going to be on the real line.",
                    "label": 0
                },
                {
                    "sent": "Let's say between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Let's say the distribution is just some smooth density and our concept space set of classifiers is just intervals such that it.",
                    "label": 0
                },
                {
                    "sent": "Labels everything in the interval is plus and everything outside the interval as minus.",
                    "label": 0
                },
                {
                    "sent": "So we have all of these intervals in the concept space.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at a very special case of the interval learning problem.",
                    "label": 0
                },
                {
                    "sent": "The case where the target function is the empty interval.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So let's just go through what does this algorithm do when the target function is the empty interval.",
                    "label": 0
                },
                {
                    "sent": "So we start off just with our random and over 2 examples.",
                    "label": 0
                },
                {
                    "sent": "Here they are.",
                    "label": 0
                },
                {
                    "sent": "They're all negative, of course, 'cause the target labels everything negative.",
                    "label": 0
                },
                {
                    "sent": "OK, we have our version spaces, just the intervals that are consistent with this sample, so it's all intervals that don't contain one of these.",
                    "label": 0
                },
                {
                    "sent": "We're going to estimate the probability that of getting a point such that there's some disagreement about its label.",
                    "label": 0
                },
                {
                    "sent": "Now, here's where something interesting happens because.",
                    "label": 0
                },
                {
                    "sent": "So let's say here's a point and any point that you know this is meant to represent any point that isn't one of the original data points.",
                    "label": 0
                },
                {
                    "sent": "You can show that there is always some interval that doesn't contain it, right?",
                    "label": 0
                },
                {
                    "sent": "That's pretty clear to see, so you can label it minus.",
                    "label": 0
                },
                {
                    "sent": "And there's also some interval that does contain it so you can label it plus right and so you can kind of see that that that happens no matter where you pick the point, as long as it's not one of the initial points.",
                    "label": 0
                },
                {
                    "sent": "How can I fix?",
                    "label": 0
                },
                {
                    "sent": "Yeah, go ahead, I don't see where do you specify anything in this article.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so A&B are kind of.",
                    "label": 0
                },
                {
                    "sent": "This set so everything between A&B.",
                    "label": 0
                },
                {
                    "sent": "Is in the version space, and so there's kind of for thresholds.",
                    "label": 0
                },
                {
                    "sent": "It's version space is equal to the region of disagreement, but.",
                    "label": 0
                },
                {
                    "sent": "Does that make sense?",
                    "label": 0
                },
                {
                    "sent": "Modify this inscriptions say where it where the NPR specified or farm.",
                    "label": 0
                },
                {
                    "sent": "And so the NBA was just for the threshold learning problem and that's that was a way to sort of explicitly represent the version space, right?",
                    "label": 0
                },
                {
                    "sent": "So the version space was all the thresholds between A&B.",
                    "label": 0
                },
                {
                    "sent": "Does that make sense?",
                    "label": 0
                },
                {
                    "sent": "So here the version space is all intervals that don't contain any of those points, so it's a different version space.",
                    "label": 0
                },
                {
                    "sent": "You can't really write down at A&B for this for the interval learning problem.",
                    "label": 0
                },
                {
                    "sent": "OK. No.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's just a different concept space, so there's no A&B necessarily.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Anyway, any point that's not one of these initial data points is going to be in this region of disagreement, so this estimate is just going to be one OK.",
                    "label": 0
                },
                {
                    "sent": "So when we go to the next step and we sample, we're just going to get.",
                    "label": 0
                },
                {
                    "sent": "Roughly an over 4 examples.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned with probability one, all of them are in.",
                    "label": 0
                },
                {
                    "sent": "This satisfy this condition that there's some classifier that labels it minus and some classifier that labels it.",
                    "label": 0
                },
                {
                    "sent": "Plus so in fact we have to request the label of all over for these examples.",
                    "label": 0
                },
                {
                    "sent": "And so we're not going to be able to label the rest ourselves.",
                    "label": 0
                },
                {
                    "sent": "There's no, there's none left.",
                    "label": 0
                },
                {
                    "sent": "And basically what this says is the set L has size N / 4, so it's still ordering examples.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm is not giving us any improvements compared to just running with random label requests.",
                    "label": 0
                },
                {
                    "sent": "OK, so this doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Can any algorithm did improve?",
                    "label": 0
                },
                {
                    "sent": "Can any so if you wanted to find an interval size, what you basically have to prove that there is no interval size epsilon right in this example.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "The true targets would be a zero in.",
                    "label": 0
                },
                {
                    "sent": "So I put a successful classifier.",
                    "label": 0
                },
                {
                    "sent": "You have to prove that there's no interval size apps.",
                    "label": 0
                },
                {
                    "sent": "Now I understand why you would think that, but the interesting thing that happens in active learning is sometimes it's actually harder to verify that you have a good classifier then just to find a good classifier so you can find a good classifier, but it would take more examples to prove that.",
                    "label": 0
                },
                {
                    "sent": "That it's a good classifier, so you would have to in order to prove that it's a good classifier.",
                    "label": 0
                },
                {
                    "sent": "You're right, you have to check every every interval of size 2 epsilon in order to prove that you have an excellent good classifier that would take like one over epsilon samples.",
                    "label": 0
                },
                {
                    "sent": "But just to find a good classifier.",
                    "label": 0
                },
                {
                    "sent": "Or I can just I just output all negative, right?",
                    "label": 0
                },
                {
                    "sent": "That's my default, but so will see an algorithm that works here.",
                    "label": 0
                },
                {
                    "sent": "Also, that's more general than that.",
                    "label": 0
                },
                {
                    "sent": "OK, so but this particular algorithm doesn't work.",
                    "label": 0
                },
                {
                    "sent": "That's what we just established.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's take a look at a slight modification of it, which.",
                    "label": 0
                },
                {
                    "sent": "Does work for that example?",
                    "label": 0
                },
                {
                    "sent": "And then we'll take a look at whether this is generally effective.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to start off very similar to what we did before.",
                    "label": 0
                },
                {
                    "sent": "Will just take some random examples.",
                    "label": 0
                },
                {
                    "sent": "This time will use a third of our budget and get labeled examples Q again.",
                    "label": 0
                },
                {
                    "sent": "We'll just define the version space as those examples.",
                    "label": 0
                },
                {
                    "sent": "Classifiers that get all those examples correct.",
                    "label": 0
                },
                {
                    "sent": "An will will initialize a set S just to be the empty set, and that's S is going to be the difference between this and the previous algorithm and you'll see how that works.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we're going to go through D + 1 rounds where remember D is the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "And So what happens here is on each round again, we're going to estimate a probability, But this time we're going to use that set S and we're going to test whether the version space shatters.",
                    "label": 0
                },
                {
                    "sent": "So we're going to find the probability of getting a point such that if you add it to the Set S, the version space will shatter the combined set.",
                    "label": 0
                },
                {
                    "sent": "So those who don't recall.",
                    "label": 0
                },
                {
                    "sent": "So when shattering means that you can label it.",
                    "label": 0
                },
                {
                    "sent": "So if there's K points in the set, then shattering means that you can find 2 to the K classifiers, INV.",
                    "label": 0
                },
                {
                    "sent": "That label.",
                    "label": 0
                },
                {
                    "sent": "Those points in all two to the K different possible ways.",
                    "label": 0
                },
                {
                    "sent": "So we're going to find the probability of an example, such as the version space shatters, and then we'll sample just as before, roughly N over set.",
                    "label": 0
                },
                {
                    "sent": "This time will divide by day because of the loops.",
                    "label": 0
                },
                {
                    "sent": "Get lichen over 60 times Delta examples so you can show with high probability that.",
                    "label": 0
                },
                {
                    "sent": "At most roughly an over 2D of those examples are going to be points such that these shatters the combined set of S with that point.",
                    "label": 0
                },
                {
                    "sent": "So for anything where that happens, we will request the label.",
                    "label": 0
                },
                {
                    "sent": "For those that don't, that doesn't happen for what we're going to do is because we know that adding SX to S makes a set that can't be shattered.",
                    "label": 0
                },
                {
                    "sent": "It means that there is some labeling of the points in S such that there is a forced label of X, so there's some way that you can label the points in S such that any classifier in the version space that labels the points in S that way must label the point X.",
                    "label": 0
                },
                {
                    "sent": "In one particular way, and we'll just take that label and label the point ourselves.",
                    "label": 0
                },
                {
                    "sent": "And then here's the new thing.",
                    "label": 0
                },
                {
                    "sent": "We're going to take just another point at random.",
                    "label": 0
                },
                {
                    "sent": "Subject to the fact that if you add it to S, you can still shatter this set using the version space.",
                    "label": 0
                },
                {
                    "sent": "It doesn't exist.",
                    "label": 0
                },
                {
                    "sent": "Don't worry bout that but.",
                    "label": 0
                },
                {
                    "sent": "If it does exist, then we'll add it to us and we'll use that.",
                    "label": 0
                },
                {
                    "sent": "Set us on the next round.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then at the end what we get is D + 1 different training sets L. So each one is Elsas K for the case round and we just feed them into the passive learning algorithm and we end up with D + 1 different classifiers and we have to pick between them basically.",
                    "label": 0
                },
                {
                    "sent": "So this is just kind of a clean up step.",
                    "label": 0
                },
                {
                    "sent": "If you go through the bounds here, each of these steps was using something like N. / 2 D samples labels.",
                    "label": 0
                },
                {
                    "sent": "Sorry N / 3 D labels and so there were deer rounds where we actually request labels.",
                    "label": 0
                },
                {
                    "sent": "And here's an over three up here, so we should have about a third of our budget leftover, and that's what we're using in the selection phase, and we're just going to take each pair, find a bunch of examples where they disagree, request the labels of those examples, and pick whichever classifier doesn't make too many mistakes on any of those pairwise comparisons.",
                    "label": 0
                },
                {
                    "sent": "So this is just sort of a simple way to choose which of those classifiers we want to output is just do some sort of pairwise tournaments.",
                    "label": 1
                },
                {
                    "sent": "Ascentia Lee.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So that's not too important.",
                    "label": 0
                },
                {
                    "sent": "You can see.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "If you have enough examples, you can use turn off balance to guarantee that.",
                    "label": 0
                },
                {
                    "sent": "In fact, whatever the error rate of the best classifier in here is the one that you output will have an error rate.",
                    "label": 0
                },
                {
                    "sent": "Let's say at most twice that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The big thing that we're going to be interested in is proving that one of these labeled samples has correct labels and is growing super linearly.",
                    "label": 0
                },
                {
                    "sent": "And if that's true then we definitely get these improvements.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's take a look at how this algorithm behaves on the interval's problem.",
                    "label": 0
                },
                {
                    "sent": "So again, just as before, we'll assume that labels everything is minus one.",
                    "label": 0
                },
                {
                    "sent": "And OK, so our first samples are just this and over 3 random examples.",
                    "label": 0
                },
                {
                    "sent": "Here they are.",
                    "label": 0
                },
                {
                    "sent": "And our version spaces again just all the intervals that don't include any of those.",
                    "label": 0
                },
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "So I'll just keep them up there just so we remember where they are.",
                    "label": 0
                },
                {
                    "sent": "OK, so now first note that the first round of this algorithm when K = 1, this is just the naive algorithm from before, right?",
                    "label": 0
                },
                {
                    "sent": "So testing whether V shatters emptyset Union X is just testing whether there exists a classifier that labels it.",
                    "label": 0
                },
                {
                    "sent": "Plus in a classifier that labels it minus.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly what we did in the naive algorithm, so we know what's going to happen in the first round.",
                    "label": 0
                },
                {
                    "sent": "We're going to sell one that is growing linearly in the number of samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's forget about that and let's just focus on what happens in the second round.",
                    "label": 0
                },
                {
                    "sent": "So we see dimension here is 2, so we have a second round.",
                    "label": 0
                },
                {
                    "sent": "And what's going to happen at the end of the first round?",
                    "label": 0
                },
                {
                    "sent": "We get some point.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's this one.",
                    "label": 0
                },
                {
                    "sent": "And that's this X one, and we're going to set, and that's the set that we're using on the second round.",
                    "label": 0
                },
                {
                    "sent": "And so here we get to this point where we have to estimate the probability that the version space shatters.",
                    "label": 0
                },
                {
                    "sent": "The set which contains this point with the point.",
                    "label": 0
                },
                {
                    "sent": "So here's what happens.",
                    "label": 0
                },
                {
                    "sent": "Take any point.",
                    "label": 0
                },
                {
                    "sent": "Let's say we take this point.",
                    "label": 0
                },
                {
                    "sent": "OK, so if.",
                    "label": 0
                },
                {
                    "sent": "If we have a classifier that labels this point as minus, then that classifier.",
                    "label": 0
                },
                {
                    "sent": "Could label this point as plus or minus right?",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if we have a classified label, this point as plus.",
                    "label": 0
                },
                {
                    "sent": "There's no way it can label this point as plus also, because that would be an interval that contains a negative point, so.",
                    "label": 0
                },
                {
                    "sent": "The classifiers in the version space cannot shatter this point at this point.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The only points in fact that the classifiers in the version space can shatter are the ones that are between this negative example and this negative example, and not this point.",
                    "label": 0
                },
                {
                    "sent": "Any point that's in here?",
                    "label": 0
                },
                {
                    "sent": "You can add it to the set S and still shatter it.",
                    "label": 0
                },
                {
                    "sent": "So the probability of getting a point such that we added to S and we can shatter is just the probability mass contained between these two points.",
                    "label": 0
                },
                {
                    "sent": "That's going to be roughly 3 / N. So in this step we're just going to sample.",
                    "label": 0
                },
                {
                    "sent": "Roughly, an over 60 Delta examples and we said Delta is roughly 3 / N, so this is going to be order N squared examples.",
                    "label": 0
                },
                {
                    "sent": "And here we request the label of any of them such that the version space can shatter when you add them to S. So that's again just the points that are between here.",
                    "label": 0
                },
                {
                    "sent": "It's these points, of course, they're negative, because everything is negative.",
                    "label": 0
                },
                {
                    "sent": "For the other points.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Outside, so remember when X was labeled.",
                    "label": 0
                },
                {
                    "sent": "Plus we could only label these other points as minus, so in this step we just automatically infer that the label of those other points is minus.",
                    "label": 0
                },
                {
                    "sent": "OK, let's just clear out the.",
                    "label": 0
                },
                {
                    "sent": "The initial sample and X1 and what we're left with is the set L2.",
                    "label": 0
                },
                {
                    "sent": "And remember that L2 was of size roughly N ^2.",
                    "label": 0
                },
                {
                    "sent": "So and all the labels are correct, they're all minus as they should be.",
                    "label": 0
                },
                {
                    "sent": "So we're feeding a set of examples that's growing super linearly in North into the passive learning algorithm, so the second classifier is going to achieve a strict improvement in label complexity.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this algorithm does improve label complexity, so we would say that this this algorithm, active eyes is any passive learning algorithm for the interval learning problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we took this problem that looked like it was hard.",
                    "label": 0
                },
                {
                    "sent": "And our naive algorithm certainly failed.",
                    "label": 0
                },
                {
                    "sent": "I should also mention there's obvious ways to make this more efficient in terms of labels.",
                    "label": 0
                },
                {
                    "sent": "Certainly you could update the version space after every label requests and get some improvements, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trivial little trick that you can do.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This algorithm and you would get better label complexities, although already we had.",
                    "label": 0
                },
                {
                    "sent": "We have this algorithm improving the label complexity compared to passive.",
                    "label": 0
                },
                {
                    "sent": "So we can improve compared to passive for this interval learning problem, but we still have the general question right?",
                    "label": 0
                },
                {
                    "sent": "Can we improve over the label complexities?",
                    "label": 0
                },
                {
                    "sent": "For any passive algorithm for any concept space.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right, so it turns out that we can.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this simple ACT Act Divisor algorithm does in fact active eyes.",
                    "label": 0
                },
                {
                    "sent": "Any passive learning algorithm, so it improves the label complexity.",
                    "label": 1
                },
                {
                    "sent": "For any concept space.",
                    "label": 0
                },
                {
                    "sent": "So the simple corollary that we can get from this we know of a passive learning algorithm that for any concept space gets in order one over epsilon sample complexity.",
                    "label": 0
                },
                {
                    "sent": "And so if we just plug that in to the active visor, we immediately get that you can always get an active learning label complexity that's little low of one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So that's definitely something you can't prove about passive learning.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately I don't have time and I doubt that you have the patience for me to go through the proof of this, but maybe I'll just stand up here and wave my hands for a little bit anyway.",
                    "label": 0
                },
                {
                    "sent": "Just to give you a little bit of flavor of how the how the proof works and why this algorithm might give you these kinds of improvements.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, note that if.",
                    "label": 0
                },
                {
                    "sent": "The probability of that region of disagreement probability that we can find classifiers in the version space that disagree on the label of a point is decreasing to 0 on that first round.",
                    "label": 0
                },
                {
                    "sent": "Then we're done because we get a sample of size N divided by Delta, and so that's growing super linearly with N, and we can just take that first data set.",
                    "label": 0
                },
                {
                    "sent": "So that's actually a condition where that naive algorithm will work.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if that doesn't happen, what it means is that for large enough, N gets very large.",
                    "label": 0
                },
                {
                    "sent": "You're going to the set is going to be decreasing, but not going past some point, so there's some constant such that we're not getting a set that's smaller than that constant.",
                    "label": 0
                },
                {
                    "sent": "OK, so whatever that limiting set is.",
                    "label": 0
                },
                {
                    "sent": "We know that for large enough sample size that that point that we're adding to S is going to go into.",
                    "label": 0
                },
                {
                    "sent": "It's going to fall in that set, right?",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, what this means?",
                    "label": 0
                },
                {
                    "sent": "What does it mean to be in that limiting set as N gets large?",
                    "label": 0
                },
                {
                    "sent": "What it means is that there exists classifiers.",
                    "label": 0
                },
                {
                    "sent": "That OK, so this should be X one.",
                    "label": 0
                },
                {
                    "sent": "So what it means is there's this classifiers that label X1 as positive, which have an arbitrarily good error rate.",
                    "label": 0
                },
                {
                    "sent": "There arbitrarily close to the target function, and there also exists classifiers that label X one is negative that are also arbitrarily close to the target function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's because N gets large.",
                    "label": 0
                },
                {
                    "sent": "Our version space is getting closer and closer to the target function.",
                    "label": 0
                },
                {
                    "sent": "All the classifiers are close to the target function.",
                    "label": 0
                },
                {
                    "sent": "So in particular, with high probability, if you take sort with probably 1.",
                    "label": 0
                },
                {
                    "sent": "Actually any example X that's agreed upon by all the classifiers that label X one is plus one or all the classifiers that label X One X -- 1.",
                    "label": 0
                },
                {
                    "sent": "Fully agree upon has to be the target's label, because these are arbitrarily good classifiers, so.",
                    "label": 0
                },
                {
                    "sent": "Basically what we can say here is that we know the label of any example such that if you add it to the set with X one and it's not sharable, whatever that remaining label is the label that we can't realize that has to be the label that we can realize, such that you can't label it the other way that has to be the target's label.",
                    "label": 0
                },
                {
                    "sent": "OK. Now you can repeat this argument for the larger values of K and you can keep doing that until you get a value of Delta that doesn't get stuck at some constant.",
                    "label": 0
                },
                {
                    "sent": "If you get a value of Delta that goes all the way to 0, then you're no longer guaranteed to find your shattered point in that limiting set.",
                    "label": 0
                },
                {
                    "sent": "But that's OK, because if we get a value of K where Delta goes to 0, then we're sampling an over Delta examples.",
                    "label": 0
                },
                {
                    "sent": "And so the set LK is going to be increasing super linearly, and so that's the set that we want that we want to be competitive with.",
                    "label": 0
                },
                {
                    "sent": "That's the one that improves over passive.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of the slave.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of how the proof goes.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is a question of efficiency and OK, so I guess first of all, if you can't find even in passive learning if you can't find a separator or classifier that's consistent with your data.",
                    "label": 0
                },
                {
                    "sent": "There's really nothing we can do here.",
                    "label": 0
                },
                {
                    "sent": "Can't help you.",
                    "label": 0
                },
                {
                    "sent": "But suppose you can do that efficiently.",
                    "label": 0
                },
                {
                    "sent": "Then we're still left with the task of testing whether a set of at most D points is Shadow rible by the set of classifiers consistent with that initial sample.",
                    "label": 1
                },
                {
                    "sent": "So for some spaces I would imagine this is going to be exponential in the VC dimension.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "But in many cases I think it might be efficient.",
                    "label": 1
                },
                {
                    "sent": "Now, I haven't actually gone through the linear algebra, but I think for linear separators you can probably do this efficiently.",
                    "label": 0
                },
                {
                    "sent": "Essentially, there's some relationship between shatter ability and linear independence.",
                    "label": 1
                },
                {
                    "sent": "And so I think you can set up some kind of constraint satisfaction problem for linear separators that should.",
                    "label": 0
                },
                {
                    "sent": "It should be efficiently solvable and many other concept spaces certainly should be able to test for shatter ability.",
                    "label": 0
                },
                {
                    "sent": "Subject to these constraints.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's efficiency.",
                    "label": 0
                },
                {
                    "sent": "The remaining issue, and since I have a couple of minutes I can talk about it.",
                    "label": 0
                },
                {
                    "sent": "It's dealing with noise.",
                    "label": 0
                },
                {
                    "sent": "I haven't mentioned it all about noise.",
                    "label": 0
                },
                {
                    "sent": "It's actually really tricky problem for us.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, let's there's some modification to the definitions that are needed just to talk about it.",
                    "label": 0
                },
                {
                    "sent": "So now what we're talking about is an arbitrary distribution over examples and labels.",
                    "label": 0
                },
                {
                    "sent": "And, well, the label complexity is now just.",
                    "label": 0
                },
                {
                    "sent": "I guess you could put the concept space in there also an it's just epsilon Delta and now that joint distribution over instance in label.",
                    "label": 0
                },
                {
                    "sent": "And now we're measuring this epsilon is not error rate anymore, it's actually excess error rate over the best classifier in the concept space.",
                    "label": 0
                },
                {
                    "sent": "So what we want is to guarantee with high probability that the error rate of the classifier we output is at most epsilon worse than the best error rate you could hope for from that concept space.",
                    "label": 0
                },
                {
                    "sent": "OK, and then again active Ising would mean little oh of the passive learning sample complexity in terms of dependence on epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, one simple modification that you probably definitely want to do is change this notion of version space, right?",
                    "label": 0
                },
                {
                    "sent": "So if there's noisy examples, you definitely don't want to throw away classifiers that make one mistake.",
                    "label": 0
                },
                {
                    "sent": "So in fact, suggestion here is to make a noise robust version space, which is just the set of classifiers such that the number of mistakes they make is not too much larger than any other classifier in the concept space, where too much larger is something like.",
                    "label": 0
                },
                {
                    "sent": "One over root and or you could plug in some other bounds there as well, but basically uniform convergence bounds will imply that this version space still contains the reasonably good classifiers.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that if you do this, you can get some results that are very nice if you plug in a particular passive learning algorithm and you use just this, just this version space instead of that one as your modified algorithm, and then you can get a label complexity that's always little of one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "So if you recall in passive learning theory call 701 you can always get big O of one over epsilon squared in passive learning.",
                    "label": 0
                },
                {
                    "sent": "So here with active learning we're going to load that.",
                    "label": 0
                },
                {
                    "sent": "And for those of you familiar with people cause noise conditions, you can get passive learning one over epsilon to the 2 -- 1 over cap.",
                    "label": 0
                },
                {
                    "sent": "So here we can get a little of that.",
                    "label": 0
                },
                {
                    "sent": "Also you have to be a little bit more careful about how you do certain steps in the algorithm, but.",
                    "label": 0
                },
                {
                    "sent": "But in general, we don't yet have a solution to whether you can active eyes.",
                    "label": 0
                },
                {
                    "sent": "Any passive learning algorithm, even with noise.",
                    "label": 1
                },
                {
                    "sent": "This is really tough problem and interesting to resolve, either positive or negative.",
                    "label": 0
                },
                {
                    "sent": "In fact, we don't even know if you can active eyes.",
                    "label": 0
                },
                {
                    "sent": "Some empirical risk minimization, minimizing algorithm, so this would be interesting because at least there would be a little more general than this would be able to say that any theorem you can prove about empirical risk minimization, But the whole family of empirical risk.",
                    "label": 0
                },
                {
                    "sent": "Minimization algorithms we would be able to do better than it with.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Particular possible for this result?",
                    "label": 0
                },
                {
                    "sent": "So particular passive algorithm that if you plug that in, you get an active learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's basically empirical risk minimization subject to the examples that you automatically labeled are like constraints.",
                    "label": 0
                },
                {
                    "sent": "You have to get those right and then you do empirical risk minimization on the rest.",
                    "label": 0
                },
                {
                    "sent": "Any assumptions about the distribution of the noise here that's right, and if you work for example.",
                    "label": 0
                },
                {
                    "sent": "Yeah, absolutely right.",
                    "label": 0
                },
                {
                    "sent": "So this is a type of assumption about noise conditions.",
                    "label": 0
                },
                {
                    "sent": "I don't want to get into the details of what it is, but.",
                    "label": 0
                },
                {
                    "sent": "This tells you something about the noise around the decision boundary and you can get as you can see, this sort of interpolates if Kappa is bigger than one interpolates between one and two.",
                    "label": 0
                },
                {
                    "sent": "Right, but in general for the for this problem, I don't even know about about that case.",
                    "label": 0
                },
                {
                    "sent": "If you have some special if you have any kind of noise, I don't know how to act of eyes.",
                    "label": 0
                },
                {
                    "sent": "Arbitrary passive learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "And it's just because you could have some funny dependence on the noise, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to wrap up, turns out you can activate any passive learning algorithm and we saw a simple method that does this in the 0 error finite VC dimension case.",
                    "label": 0
                },
                {
                    "sent": "Several questions if we have infinite VC dimension is not clear what happens anymore.",
                    "label": 0
                },
                {
                    "sent": "There's some ideas about maybe you could still run this algorithm and it would just have an infinite sequence of classifiers, or I guess more and more classifiers.",
                    "label": 0
                },
                {
                    "sent": "It's an increases, but I don't have an analysis for that as of yet.",
                    "label": 0
                },
                {
                    "sent": "There's also a question.",
                    "label": 0
                },
                {
                    "sent": "OK, so we proved that you get improvements over passive learning, but we didn't say how big those improvements are.",
                    "label": 0
                },
                {
                    "sent": "So this is another question that's interesting to look at, is what specific label complexity bounds can we get for these algorithms or variants of them?",
                    "label": 0
                },
                {
                    "sent": "I mentioned some ways to improve the algorithms that I presented makes them a little bit more complicated, but better label complexities.",
                    "label": 0
                },
                {
                    "sent": "And then of course the big question, what about when there's noise?",
                    "label": 0
                },
                {
                    "sent": "Can we always active eyes any passive learning algorithm, even in the presence of noise?",
                    "label": 1
                },
                {
                    "sent": "I think that's a tough problem.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Happy to take questions.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very theoretical question is.",
                    "label": 0
                },
                {
                    "sent": "What this would mean when you actually apply to concrete examples?",
                    "label": 0
                },
                {
                    "sent": "Let's say you want to do some part of speech tagging supplies, or some some traffic.",
                    "label": 0
                },
                {
                    "sent": "So how much would we win?",
                    "label": 0
                },
                {
                    "sent": "Under under this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I guess in general.",
                    "label": 0
                },
                {
                    "sent": "The question you're asking is how much improvement in label complexity do we get for small samples.",
                    "label": 1
                },
                {
                    "sent": "Basically, for values of epsilon that we could actually achieve with passive learning algorithms and not just in the limit and.",
                    "label": 0
                },
                {
                    "sent": "So in some cases you can definitely get large improvements, but.",
                    "label": 0
                },
                {
                    "sent": "In other cases, you don't in many cases, and so you can construct examples where essentially you don't see those improvements until epsilon is so small that maybe that's not a practical concern, but you can definitely also construct examples in thresholds.",
                    "label": 0
                },
                {
                    "sent": "For example, and generalizations of that, such as linear separators where you see these improvements even for reasonable values of epsilon, and I think those have been coming out in experimental evaluations.",
                    "label": 0
                },
                {
                    "sent": "Of other algorithms as well, but it would.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure exactly what this looks like in comparison.",
                    "label": 0
                },
                {
                    "sent": "If you run it, it's something I am interested.",
                    "label": 0
                },
                {
                    "sent": "I might I might run those experiments, will see.",
                    "label": 0
                },
                {
                    "sent": "Question why is your work in general?",
                    "label": 0
                },
                {
                    "sent": "I could learn how to fill in the area for other men.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is to try to find the the labeled examples around.",
                    "label": 0
                },
                {
                    "sent": "It's around decision boundaries and then so, so that means the selection of the data points.",
                    "label": 0
                },
                {
                    "sent": "It has to be has to be dependent on the distribution of two classes or multiclass examples, but the whole thing you are doing is trying to say well.",
                    "label": 0
                },
                {
                    "sent": "Try to find those, think about what you are.",
                    "label": 0
                },
                {
                    "sent": "Carrier to choose the subset of the data, points to label.",
                    "label": 0
                },
                {
                    "sent": "Then.",
                    "label": 0
                },
                {
                    "sent": "Basically you're saying that using the short ability OK, the shot ability.",
                    "label": 0
                },
                {
                    "sent": "Basically, things that allow the subset of labels to the class labels.",
                    "label": 0
                },
                {
                    "sent": "It would be arbitrary.",
                    "label": 0
                },
                {
                    "sent": "So basically you lost your concentration on the truth is even boundary of given set of labeled data.",
                    "label": 0
                },
                {
                    "sent": "So I don't understand the philosophy.",
                    "label": 0
                },
                {
                    "sent": "I don't necessarily think that's true.",
                    "label": 0
                },
                {
                    "sent": "I think actually you're going to be focusing still for sort of geometric concept spaces.",
                    "label": 0
                },
                {
                    "sent": "You're still going to be focusing close to the decision boundary of the target function.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you just think about the naive algorithm that I mentioned, you're just sampling points such that you can disagree on the label within the version space, so the version space is actually just going to be sort of the set of classifiers that look like the target function, but maybe are slightly varied because after a certain number of labeled examples you can guarantee you're close to the target function.",
                    "label": 0
                },
                {
                    "sent": "Your decision boundary isn't too far right, so the samples that you take are actually going to be.",
                    "label": 0
                },
                {
                    "sent": "Within that region of disagreement of the set of classifiers that are close to the target functions.",
                    "label": 0
                },
                {
                    "sent": "The boundary, so that's in the situation where the naive algorithm works.",
                    "label": 0
                },
                {
                    "sent": "In the situation where it doesn't work, where you have to add these shattered points.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "I haven't really thought too much about it, but I think you still get points that are sort of close to the target functions boundary in many cases, but I don't have to think a little bit more about it, but I think you still get some kind of geometric intuition.",
                    "label": 0
                },
                {
                    "sent": "It's just that there's regions of the space that you couldn't rule out you couldn't figure out what the labels were from sampling from the region of disagreement, so you just sort of make up labels for those regions, and that allows you to focus more as you'd like to be able to focus.",
                    "label": 0
                },
                {
                    "sent": "Quick question, I didn't quite get it.",
                    "label": 0
                },
                {
                    "sent": "Efficiency point, so you will calculating the probability of the sample where you add that sample you cannot settle.",
                    "label": 0
                },
                {
                    "sent": "While sub procedure you need to compute the shatter ability.",
                    "label": 0
                },
                {
                    "sent": "So how do you actually compute the probability of the samples where you cannot shatter when you add the sample?",
                    "label": 0
                },
                {
                    "sent": "Well, so all you need is the probability of the set that you can shatter, right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That either implies the other, but yeah, so if you estimate the probability of the set of points that you can shatter, and that's actually what you needed in the algorithm, 'cause those are the points you're going to request labels for, right?",
                    "label": 0
                },
                {
                    "sent": "So you just sample a bunch and then count how many you can shatter.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Because it's only for something so easy to estimate the problem.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean, it's easy if you can test whether you can shatter set.",
                    "label": 0
                },
                {
                    "sent": "'cause you just sample a bunch of unlabeled points and then for each one you add it to the set test whether you can shatter.",
                    "label": 0
                },
                {
                    "sent": "That's an indicator function.",
                    "label": 0
                },
                {
                    "sent": "You just count how many are 1.",
                    "label": 0
                },
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "Yeah, two strong suction that cost parking perfectly separate handles right and that's why I mentioned this deal.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With noise problem such kind of problem.",
                    "label": 0
                },
                {
                    "sent": "You do sometimes.",
                    "label": 0
                },
                {
                    "sent": "There are NLP datasets that are linearly separable, for example, just because you have so many dimensions.",
                    "label": 0
                },
                {
                    "sent": "But in general, you definitely want to deal with this noise problem, and I don't think it's such a big problem for some natural passive learning algorithms that you might want to plug in, but to get a general theorem just theoretically speaking, it's difficult.",
                    "label": 0
                },
                {
                    "sent": "Algorithm why do you need to look through four from VC dimension?",
                    "label": 0
                },
                {
                    "sent": "One 2D isn't enough.",
                    "label": 0
                },
                {
                    "sent": "Today plus one is.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you want it, sometimes you want the plus one.",
                    "label": 0
                },
                {
                    "sent": "So D + 1 is not going to automatically label all the points, right?",
                    "label": 0
                },
                {
                    "sent": "Because you can't shatter D + 1 points, so every point is going to have some automatic label that you can assign to it.",
                    "label": 0
                },
                {
                    "sent": "So it's just.",
                    "label": 0
                },
                {
                    "sent": "There's just some cases where you want to use that last where they're all automatically labeled instead of just.",
                    "label": 0
                },
                {
                    "sent": "You can construct spaces where you want to do that.",
                    "label": 0
                },
                {
                    "sent": "Loop is for number of samples of the loop.",
                    "label": 0
                },
                {
                    "sent": "Here let's go back.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so yeah, it's the loop is through values of K less than the D + 1, so Dee Dee is the VC dimension right?",
                    "label": 0
                },
                {
                    "sent": "So you definitely can't shatter D + 1 points just by definition of VC dimension, so the last loop through this is just going to automatically label all the points in this sample.",
                    "label": 0
                },
                {
                    "sent": "So just you can just there's some spaces where you want to use.",
                    "label": 0
                },
                {
                    "sent": "This is the algorithm that this is the call to the algorithm that gives you the good label complexity.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}