{
    "id": "z5hzb34jh4zc5v3fwf7yqujbij2gbkdc",
    "title": "Trust Region Newton Methods for Large-Scale Logistic Regression",
    "info": {
        "author": [
            "Chin Jen Lin"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Mathematics->Operations Research"
        ]
    },
    "url": "http://videolectures.net/icml07_lin_chin_trnm/",
    "segmentation": [
        [
            "Trust region using methods for large scale logistic regression.",
            "Yeah, thank you.",
            "So this talk is about trust region.",
            "Newton method for large scale logistic regression.",
            "This is joint work with Ruby, Wynn and Associates."
        ],
        [
            "Percy so the goal here is to train very large linear classifiers so high linear, well, the reason is for certain problems, such as document classification, usually linear classifiers are as good as kernelized once.",
            "So we don't need to use things like nonlinear SVM.",
            "And because we focus on doing linear classification and we can train much larger datasets.",
            "So recently several papers they focus only just on training on linear.",
            "Yes, so for example, this is a Gmail app paper which which uses a used modified Newton method for 4L2 SVM and this is a KDD paper last year.",
            "And host and proposed an approximation algorithm for for training."
        ],
        [
            "Linear SVM.",
            "But here I am going to train logistic regression.",
            "Well, the reason is if we if we check the formulations of L1 and L2 SVM here I mean 01 loss function.",
            "Well now there are loads regularization term.",
            "So for for for this standard SVM then this is not differentiable and for L2 SVM it is differentiable, but it's not twice differentiable.",
            "If we use the loss function of logistic regression then it is twice differentiable.",
            "So I think that it may be easier to design optimization algorithm so we can train.",
            "We can more efficiently train very large datasets.",
            "If there performances are similar, what should be because their loss functions are not very different."
        ],
        [
            "But logistical regression is an old topic, so there are tons of methods for doing the training.",
            "So there are many existing methods, for example like iterative scaling.",
            "You can do nonlinear conjugate gradient or limited memory, cause I Newton.",
            "So currently in in in this Community people consider unlimited memory.",
            "'cause I Newton is the most efficient.",
            "But in this talk I'm I'm going to argue that for certain problems.",
            "Let a new technician is can be even faster."
        ],
        [
            "But usually people have some problems and they think that it is not easy to use Newton method because we need to get the 2nd order information.",
            "So that means the Haitian matrix.",
            "Well, in order to get the Newton direction we minimize the 2nd order approximation.",
            "So that's this quadratic function.",
            "Well, this is equivalent equivalent to sort to solving this Newton linear system.",
            "So this is the Hessian matrix.",
            "But usually this action matrix is too large to be stored.",
            "But for this logistic regression problem, we can calculate that the Hessian matrix has this special form well, so this is the multiplication of three matrix.",
            "Here X is the data matrix, and these are diagonal matrix.",
            "So if we multiply them out then the matrix will be too large to be stored.",
            "However, we don't need to do that in order to solve linear system to obtain a new ten direction.",
            "If we consider things like country gradient methods to solve.",
            "In your system, then all we need is the matrix vector product.",
            "So as long as we can do Haitian vector product then we can do country gradient and then we can get the Newton direction.",
            "So the trick is like this formulation.",
            "So instead of multiplying them out to get a location so we just sequentially do matrix vector product so that we can do count your gradients.",
            "But this has been known but now but we're hoping to do even better will you don't want to always use Newton directions.",
            "The reason is.",
            "Quadratic information is only useful in the end of the optimization procedure.",
            "Well, in the beginning the shape of the function is not very close to a quadratic function, so Newton direction may not be that good.",
            "But to find the Newton direction is expense."
        ],
        [
            "Leave so soon optimization.",
            "There is some technique called truncating the Newton methods is also called inexact Newton method.",
            "So the basic idea is that we do early stopping of the country gradient procedure.",
            "So so in early stage of the optimization procedure we only we only find an approximate Newton direction.",
            "So, Rick and more lately they have done some work on this for logistic regression, but somehow their procedure is a little bit ad hoc, so here we are trying to have a formal way of doing this, so over our requirements are that first we don't want to waste time in the beginning.",
            "OK, we only want to have an approximate Newton direction, but in the end we want to really use the exact Newton directions to get fast convergence.",
            "But we also want third, we also want to ensure the global convergence and even better is to prove the.",
            "The face."
        ],
        [
            "The local convergence rate.",
            "So here I'm going to use an optimization technique called Trust Region Newton method.",
            "It is a type of truncated Newton approach.",
            "It is popular in optimization and here I'm going to follow some of my earlier work on this method.",
            "So the basic idea is that at each iteration we have something called the trust region.",
            "So Delta K is the size of this trust region and we still minimize this quadratic model, but.",
            "With an additional constraint that our search direction must be inside this trust region.",
            "So in other words, we can find our search directions to be in this trust region.",
            "We only trust directions in that small."
        ],
        [
            "A small region.",
            "Len so after finding the direction we check check something called reduction ratio.",
            "So here this numerator is the function reduction and this is a quadratic approximation.",
            "Well if the function is close to quadratic and we take the full Newton step then this ratio should be close to one and then that is a good situation.",
            "So if this ratio is good enough, OK if it is greater than a certain prespecified threshold then we accept this direction.",
            "Well, otherwise we reject this direction, then, at least simultaneously we need to update our the size of the trust region.",
            "So the idea is is that if the function even reduction is is not good enough, then we should reduce this trust region size.",
            "Otherwise we can enlarge this still tacky."
        ],
        [
            "So overall, the trust region algorithm is like this, so we are given the initial initial point.",
            "Then we have outer iterations.",
            "So then if gradient is equal to 0 then then we start the procedure then then the main computational part of each iteration is to approximately solve the trust region subproblem.",
            "So we minimize the low quadratic.",
            "Another approximation subject to the condition that direction should be inside the trust region.",
            "Then we check the reduction ratio.",
            "Then we may update or keep the same iterate WK.",
            "We also update the trust region size.",
            "So then the key point is how to how to do this?",
            "How to solve this?"
        ],
        [
            "Region subproblem.",
            "So as I have said earlier, if without considering this, this constraint that minimizing this quadratic function is the same as solving the Newton linear system, so we use country gradient.",
            "But now we have two kids to take care of this additional condition."
        ],
        [
            "So we just use counter gradient iterations so we have some inner iterations.",
            "They're called as Part 1 S, Part 2 blah blah.",
            "So then Linda question is when should we stop?",
            "So so this so the country gradient procedure stops if either one of the two conditions is satisfied.",
            "So the first one is list.",
            "This is actually a relative stopping condition of solving the linear system."
        ],
        [
            "If we go back to see this Newton linear system, if we move the right hand side to the left, then the whole thing should be close to."
        ],
        [
            "There should be 0.",
            "So, so this thing should be close to 0 and we put the right hands than of the right hand side here.",
            "So this is a relative stopping condition of solving a linear system so you can see that in the beginning of the end of the whole procedure the gradient is still large, so this is a very loose stopping condition, so we only find approximate Newton direction.",
            "But in the end this value is small, so we find the accurate Newton direction we want.",
            "But now we have the trust region constraint.",
            "So we have a nice property that those inner iterations they let the length of those country country gradient iterates is increasing.",
            "So either this condition is satisfied or at some point our country gradient iterations reached the trust region boundary.",
            "So that's another situation we should stop and go to the next outer iteration.",
            "So overall, this is the whole trust region.",
            "Algorithm is a careful design to ensure convergence, so so this is just a rough picture.",
            "I don't show some other details, so over we can prove global convergence and even better is the local quadratic convergence rate."
        ],
        [
            "So now I'm going to do some experiments so we collect.",
            "Several large datasets, so those are for for classification and originally they may be multi class, so I transform them to just two class.",
            "That actually makes it raining more difficult, because if you if you do one versus terrorist, then each two colors trending is unbalanced.",
            "So then the training will be be shorter.",
            "Pretty large datasets like this one has 3 million features and the last to leave about half million number of instances and the number of nonzeros means in the whole data set in the whole data set.",
            "How many feature values are now?"
        ],
        [
            "Zeros.",
            "So I tried to compare several methods where the first one is the trust region.",
            "Newton method proposed here.",
            "This method is already either is a Newton method for L2 SVM but but it is not the truncated type one, so at each iteration the Newton linear system is exactly exactly solved.",
            "Then we also compared with LB FGS, it is a general purpose.",
            "Limited memory quasi Newton implementation.",
            "At the end of this SVM curve by Torsten is an approximation algorithm for four stand."
        ],
        [
            "This fear.",
            "So this table shows us some results of of two problems.",
            "So see the regularization parameters.",
            "So from small to large and the CV is the cross validation accuracy because our method and LB FGS label self logistical regression.",
            "So they gave the same cross validation accuracy and for L2 SVM it is slightly different but they are compareable.",
            "So if we compare our method an Airbnb GS you can see that ours is so under the same stopping condition that.",
            "This truncated this kind of trust region.",
            "Newton method is much faster.",
            "And for for this method, because even though it is a Newton method, but because it exactly solve each Newton linear system at each iteration, so it is also slower.",
            "The computational time here is in."
        ],
        [
            "Seconds.",
            "So then we talk.",
            "So here is a comparison between our method and SVM perv.",
            "the Y axis is the accuracy difference to the final model.",
            "Under the X axis is the training time, but this is log scaled, so we can see that for the red curve, let's.",
            "It shows the result of SVM curve, so it gives you a web model very quickly in the beginning, even though the model is not very good.",
            "But then it is a problem of slow final convergence and for our method for Newton method you need at least a few few iterations.",
            "OK, one or two iterations to give you a model.",
            "At least model is already very good, very good and we can see that it is very fast final convergence."
        ],
        [
            "Yeah, OK so.",
            "So in conclusion that.",
            "So here we have shown that trust original method is a very effective way for for large scale logistic regression.",
            "There are a lot of possible extensions, so one thing we have we have we have done is to extend the lease to unregularized logistic regression.",
            "So here I mean that the regularization term is 1.",
            "Here we are doing 2 classes document classification.",
            "But logistic regression is a special case of maximum entropy and conditional random fields.",
            "So it is possible to extend this method for for those models.",
            "So we have released a new package called Library Lebell.",
            "For the code of this work and actually all sources used for for experiments here also available on the same web address, so you can try to see how those different methods perform.",
            "Yes, so thank you very much.",
            "Yeah.",
            "Yeah, so I did confirm with lead in this work, so that's a different paper.",
            "So I have written a paper comparing our method with layers, but those are for everyone.",
            "Logest regularizer, logistic regression.",
            "But in this work we are doing a role to logistic regression, so that's why it is not compared here.",
            "SPM is just a proxy.",
            "You exact convex proxy to a decision problem, so you might be doing better with no job in finding a better solution for the for the convex problem, the question is what is actually the performances."
        ],
        [
            "Hands off, you know error rate.",
            "So do you have any graphs like this?",
            "Yeah, OK, so this one is.",
            "Accuracy difference accuracy is in the objective function now.",
            "OK so.",
            "Maybe a little bit misleading, so the accuracy difference means we we use a very, very small stopping tolerance for getting the final model and we check water testing accuracy of the final model is.",
            "OK, now that's that's considered the little base.",
            "Then, starting from the training time zero OK from the first iteration, we check which then we get the model.",
            "Then we check water.",
            "Testing accuracy is.",
            "Then we check the difference.",
            "Yeah, the error rate yeah oh.",
            "You don't get aerated hundreds OK.",
            "So for example, if if your final testing accuracy is 100.",
            "OK is 100 and then in the beginning you don't do anything, so 100% invested 100% OK and then in the beginning you're.",
            "In the beginning, suppose you predict everything everything incorrectly.",
            "Then your accuracy difference will be 100, so that's why this is bounded by by 1:50.",
            "Yeah yeah, yeah, percentage yeah, that's right, yeah.",
            "So this is really the difference to the final model.",
            "Yeah, yeah, yeah, yeah, yeah, yeah, that's right.",
            "Actually, the question related this, so do you think it how important it is to?",
            "It seems that your method is particularly good at getting that last bit.",
            "Yeah, no, no, let's yeah, that's the strong point of Newton message.",
            "Yeah, yeah, yeah.",
            "So for learning problems, yeah, right?",
            "Yep, Yep so.",
            "So usually people say that in in doing learning problems you don't need to solve optimization problems very accurately.",
            "Well, I agree that in quite a few situations that's the case.",
            "OK, the reason why I tried to work on this is it every time I I I have very large multiclass datasets so I have to use one versus rest.",
            "So I trained a lot of binary models.",
            "So for each model I have to specify certain stopping tolerance and I find out that.",
            "So if I use like things like iterative scaling, if I only specify a stopping condition then the number of iterations for that two class model maybe maybe very large and then so.",
            "So, so in that situation I still need a method to to have reasonably fast final convergence.",
            "Oh"
        ],
        [
            "Yeah, so you see the increased actually location matrix is more your conditioned so you can see that here the training time is more.",
            "Players, but there there's also an issue about the stopping condition.",
            "Here I am using an absolute way of doing the staffing, so if I can consider seeing my stopping condition and this training time may not increase that much, ASI increases, so stopping condition is actually a very very important issue."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trust region using methods for large scale logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "So this talk is about trust region.",
                    "label": 1
                },
                {
                    "sent": "Newton method for large scale logistic regression.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Ruby, Wynn and Associates.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Percy so the goal here is to train very large linear classifiers so high linear, well, the reason is for certain problems, such as document classification, usually linear classifiers are as good as kernelized once.",
                    "label": 1
                },
                {
                    "sent": "So we don't need to use things like nonlinear SVM.",
                    "label": 0
                },
                {
                    "sent": "And because we focus on doing linear classification and we can train much larger datasets.",
                    "label": 0
                },
                {
                    "sent": "So recently several papers they focus only just on training on linear.",
                    "label": 0
                },
                {
                    "sent": "Yes, so for example, this is a Gmail app paper which which uses a used modified Newton method for 4L2 SVM and this is a KDD paper last year.",
                    "label": 1
                },
                {
                    "sent": "And host and proposed an approximation algorithm for for training.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Linear SVM.",
                    "label": 0
                },
                {
                    "sent": "But here I am going to train logistic regression.",
                    "label": 1
                },
                {
                    "sent": "Well, the reason is if we if we check the formulations of L1 and L2 SVM here I mean 01 loss function.",
                    "label": 0
                },
                {
                    "sent": "Well now there are loads regularization term.",
                    "label": 0
                },
                {
                    "sent": "So for for for this standard SVM then this is not differentiable and for L2 SVM it is differentiable, but it's not twice differentiable.",
                    "label": 1
                },
                {
                    "sent": "If we use the loss function of logistic regression then it is twice differentiable.",
                    "label": 0
                },
                {
                    "sent": "So I think that it may be easier to design optimization algorithm so we can train.",
                    "label": 0
                },
                {
                    "sent": "We can more efficiently train very large datasets.",
                    "label": 0
                },
                {
                    "sent": "If there performances are similar, what should be because their loss functions are not very different.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But logistical regression is an old topic, so there are tons of methods for doing the training.",
                    "label": 0
                },
                {
                    "sent": "So there are many existing methods, for example like iterative scaling.",
                    "label": 1
                },
                {
                    "sent": "You can do nonlinear conjugate gradient or limited memory, cause I Newton.",
                    "label": 1
                },
                {
                    "sent": "So currently in in in this Community people consider unlimited memory.",
                    "label": 1
                },
                {
                    "sent": "'cause I Newton is the most efficient.",
                    "label": 0
                },
                {
                    "sent": "But in this talk I'm I'm going to argue that for certain problems.",
                    "label": 0
                },
                {
                    "sent": "Let a new technician is can be even faster.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But usually people have some problems and they think that it is not easy to use Newton method because we need to get the 2nd order information.",
                    "label": 0
                },
                {
                    "sent": "So that means the Haitian matrix.",
                    "label": 0
                },
                {
                    "sent": "Well, in order to get the Newton direction we minimize the 2nd order approximation.",
                    "label": 0
                },
                {
                    "sent": "So that's this quadratic function.",
                    "label": 0
                },
                {
                    "sent": "Well, this is equivalent equivalent to sort to solving this Newton linear system.",
                    "label": 1
                },
                {
                    "sent": "So this is the Hessian matrix.",
                    "label": 0
                },
                {
                    "sent": "But usually this action matrix is too large to be stored.",
                    "label": 0
                },
                {
                    "sent": "But for this logistic regression problem, we can calculate that the Hessian matrix has this special form well, so this is the multiplication of three matrix.",
                    "label": 0
                },
                {
                    "sent": "Here X is the data matrix, and these are diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "So if we multiply them out then the matrix will be too large to be stored.",
                    "label": 0
                },
                {
                    "sent": "However, we don't need to do that in order to solve linear system to obtain a new ten direction.",
                    "label": 0
                },
                {
                    "sent": "If we consider things like country gradient methods to solve.",
                    "label": 0
                },
                {
                    "sent": "In your system, then all we need is the matrix vector product.",
                    "label": 0
                },
                {
                    "sent": "So as long as we can do Haitian vector product then we can do country gradient and then we can get the Newton direction.",
                    "label": 0
                },
                {
                    "sent": "So the trick is like this formulation.",
                    "label": 0
                },
                {
                    "sent": "So instead of multiplying them out to get a location so we just sequentially do matrix vector product so that we can do count your gradients.",
                    "label": 0
                },
                {
                    "sent": "But this has been known but now but we're hoping to do even better will you don't want to always use Newton directions.",
                    "label": 0
                },
                {
                    "sent": "The reason is.",
                    "label": 0
                },
                {
                    "sent": "Quadratic information is only useful in the end of the optimization procedure.",
                    "label": 0
                },
                {
                    "sent": "Well, in the beginning the shape of the function is not very close to a quadratic function, so Newton direction may not be that good.",
                    "label": 0
                },
                {
                    "sent": "But to find the Newton direction is expense.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Leave so soon optimization.",
                    "label": 0
                },
                {
                    "sent": "There is some technique called truncating the Newton methods is also called inexact Newton method.",
                    "label": 1
                },
                {
                    "sent": "So the basic idea is that we do early stopping of the country gradient procedure.",
                    "label": 0
                },
                {
                    "sent": "So so in early stage of the optimization procedure we only we only find an approximate Newton direction.",
                    "label": 0
                },
                {
                    "sent": "So, Rick and more lately they have done some work on this for logistic regression, but somehow their procedure is a little bit ad hoc, so here we are trying to have a formal way of doing this, so over our requirements are that first we don't want to waste time in the beginning.",
                    "label": 0
                },
                {
                    "sent": "OK, we only want to have an approximate Newton direction, but in the end we want to really use the exact Newton directions to get fast convergence.",
                    "label": 0
                },
                {
                    "sent": "But we also want third, we also want to ensure the global convergence and even better is to prove the.",
                    "label": 0
                },
                {
                    "sent": "The face.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The local convergence rate.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going to use an optimization technique called Trust Region Newton method.",
                    "label": 1
                },
                {
                    "sent": "It is a type of truncated Newton approach.",
                    "label": 0
                },
                {
                    "sent": "It is popular in optimization and here I'm going to follow some of my earlier work on this method.",
                    "label": 1
                },
                {
                    "sent": "So the basic idea is that at each iteration we have something called the trust region.",
                    "label": 0
                },
                {
                    "sent": "So Delta K is the size of this trust region and we still minimize this quadratic model, but.",
                    "label": 0
                },
                {
                    "sent": "With an additional constraint that our search direction must be inside this trust region.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we can find our search directions to be in this trust region.",
                    "label": 0
                },
                {
                    "sent": "We only trust directions in that small.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A small region.",
                    "label": 0
                },
                {
                    "sent": "Len so after finding the direction we check check something called reduction ratio.",
                    "label": 1
                },
                {
                    "sent": "So here this numerator is the function reduction and this is a quadratic approximation.",
                    "label": 0
                },
                {
                    "sent": "Well if the function is close to quadratic and we take the full Newton step then this ratio should be close to one and then that is a good situation.",
                    "label": 1
                },
                {
                    "sent": "So if this ratio is good enough, OK if it is greater than a certain prespecified threshold then we accept this direction.",
                    "label": 0
                },
                {
                    "sent": "Well, otherwise we reject this direction, then, at least simultaneously we need to update our the size of the trust region.",
                    "label": 0
                },
                {
                    "sent": "So the idea is is that if the function even reduction is is not good enough, then we should reduce this trust region size.",
                    "label": 1
                },
                {
                    "sent": "Otherwise we can enlarge this still tacky.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So overall, the trust region algorithm is like this, so we are given the initial initial point.",
                    "label": 1
                },
                {
                    "sent": "Then we have outer iterations.",
                    "label": 0
                },
                {
                    "sent": "So then if gradient is equal to 0 then then we start the procedure then then the main computational part of each iteration is to approximately solve the trust region subproblem.",
                    "label": 1
                },
                {
                    "sent": "So we minimize the low quadratic.",
                    "label": 0
                },
                {
                    "sent": "Another approximation subject to the condition that direction should be inside the trust region.",
                    "label": 1
                },
                {
                    "sent": "Then we check the reduction ratio.",
                    "label": 0
                },
                {
                    "sent": "Then we may update or keep the same iterate WK.",
                    "label": 0
                },
                {
                    "sent": "We also update the trust region size.",
                    "label": 0
                },
                {
                    "sent": "So then the key point is how to how to do this?",
                    "label": 0
                },
                {
                    "sent": "How to solve this?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Region subproblem.",
                    "label": 0
                },
                {
                    "sent": "So as I have said earlier, if without considering this, this constraint that minimizing this quadratic function is the same as solving the Newton linear system, so we use country gradient.",
                    "label": 1
                },
                {
                    "sent": "But now we have two kids to take care of this additional condition.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we just use counter gradient iterations so we have some inner iterations.",
                    "label": 1
                },
                {
                    "sent": "They're called as Part 1 S, Part 2 blah blah.",
                    "label": 0
                },
                {
                    "sent": "So then Linda question is when should we stop?",
                    "label": 0
                },
                {
                    "sent": "So so this so the country gradient procedure stops if either one of the two conditions is satisfied.",
                    "label": 1
                },
                {
                    "sent": "So the first one is list.",
                    "label": 0
                },
                {
                    "sent": "This is actually a relative stopping condition of solving the linear system.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we go back to see this Newton linear system, if we move the right hand side to the left, then the whole thing should be close to.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There should be 0.",
                    "label": 0
                },
                {
                    "sent": "So, so this thing should be close to 0 and we put the right hands than of the right hand side here.",
                    "label": 0
                },
                {
                    "sent": "So this is a relative stopping condition of solving a linear system so you can see that in the beginning of the end of the whole procedure the gradient is still large, so this is a very loose stopping condition, so we only find approximate Newton direction.",
                    "label": 0
                },
                {
                    "sent": "But in the end this value is small, so we find the accurate Newton direction we want.",
                    "label": 0
                },
                {
                    "sent": "But now we have the trust region constraint.",
                    "label": 1
                },
                {
                    "sent": "So we have a nice property that those inner iterations they let the length of those country country gradient iterates is increasing.",
                    "label": 0
                },
                {
                    "sent": "So either this condition is satisfied or at some point our country gradient iterations reached the trust region boundary.",
                    "label": 0
                },
                {
                    "sent": "So that's another situation we should stop and go to the next outer iteration.",
                    "label": 0
                },
                {
                    "sent": "So overall, this is the whole trust region.",
                    "label": 0
                },
                {
                    "sent": "Algorithm is a careful design to ensure convergence, so so this is just a rough picture.",
                    "label": 1
                },
                {
                    "sent": "I don't show some other details, so over we can prove global convergence and even better is the local quadratic convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to do some experiments so we collect.",
                    "label": 0
                },
                {
                    "sent": "Several large datasets, so those are for for classification and originally they may be multi class, so I transform them to just two class.",
                    "label": 0
                },
                {
                    "sent": "That actually makes it raining more difficult, because if you if you do one versus terrorist, then each two colors trending is unbalanced.",
                    "label": 0
                },
                {
                    "sent": "So then the training will be be shorter.",
                    "label": 0
                },
                {
                    "sent": "Pretty large datasets like this one has 3 million features and the last to leave about half million number of instances and the number of nonzeros means in the whole data set in the whole data set.",
                    "label": 0
                },
                {
                    "sent": "How many feature values are now?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Zeros.",
                    "label": 0
                },
                {
                    "sent": "So I tried to compare several methods where the first one is the trust region.",
                    "label": 0
                },
                {
                    "sent": "Newton method proposed here.",
                    "label": 0
                },
                {
                    "sent": "This method is already either is a Newton method for L2 SVM but but it is not the truncated type one, so at each iteration the Newton linear system is exactly exactly solved.",
                    "label": 1
                },
                {
                    "sent": "Then we also compared with LB FGS, it is a general purpose.",
                    "label": 0
                },
                {
                    "sent": "Limited memory quasi Newton implementation.",
                    "label": 0
                },
                {
                    "sent": "At the end of this SVM curve by Torsten is an approximation algorithm for four stand.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This fear.",
                    "label": 0
                },
                {
                    "sent": "So this table shows us some results of of two problems.",
                    "label": 0
                },
                {
                    "sent": "So see the regularization parameters.",
                    "label": 0
                },
                {
                    "sent": "So from small to large and the CV is the cross validation accuracy because our method and LB FGS label self logistical regression.",
                    "label": 0
                },
                {
                    "sent": "So they gave the same cross validation accuracy and for L2 SVM it is slightly different but they are compareable.",
                    "label": 0
                },
                {
                    "sent": "So if we compare our method an Airbnb GS you can see that ours is so under the same stopping condition that.",
                    "label": 0
                },
                {
                    "sent": "This truncated this kind of trust region.",
                    "label": 0
                },
                {
                    "sent": "Newton method is much faster.",
                    "label": 0
                },
                {
                    "sent": "And for for this method, because even though it is a Newton method, but because it exactly solve each Newton linear system at each iteration, so it is also slower.",
                    "label": 0
                },
                {
                    "sent": "The computational time here is in.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seconds.",
                    "label": 0
                },
                {
                    "sent": "So then we talk.",
                    "label": 0
                },
                {
                    "sent": "So here is a comparison between our method and SVM perv.",
                    "label": 0
                },
                {
                    "sent": "the Y axis is the accuracy difference to the final model.",
                    "label": 1
                },
                {
                    "sent": "Under the X axis is the training time, but this is log scaled, so we can see that for the red curve, let's.",
                    "label": 0
                },
                {
                    "sent": "It shows the result of SVM curve, so it gives you a web model very quickly in the beginning, even though the model is not very good.",
                    "label": 0
                },
                {
                    "sent": "But then it is a problem of slow final convergence and for our method for Newton method you need at least a few few iterations.",
                    "label": 0
                },
                {
                    "sent": "OK, one or two iterations to give you a model.",
                    "label": 0
                },
                {
                    "sent": "At least model is already very good, very good and we can see that it is very fast final convergence.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, OK so.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion that.",
                    "label": 0
                },
                {
                    "sent": "So here we have shown that trust original method is a very effective way for for large scale logistic regression.",
                    "label": 1
                },
                {
                    "sent": "There are a lot of possible extensions, so one thing we have we have we have done is to extend the lease to unregularized logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So here I mean that the regularization term is 1.",
                    "label": 0
                },
                {
                    "sent": "Here we are doing 2 classes document classification.",
                    "label": 0
                },
                {
                    "sent": "But logistic regression is a special case of maximum entropy and conditional random fields.",
                    "label": 1
                },
                {
                    "sent": "So it is possible to extend this method for for those models.",
                    "label": 0
                },
                {
                    "sent": "So we have released a new package called Library Lebell.",
                    "label": 0
                },
                {
                    "sent": "For the code of this work and actually all sources used for for experiments here also available on the same web address, so you can try to see how those different methods perform.",
                    "label": 1
                },
                {
                    "sent": "Yes, so thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I did confirm with lead in this work, so that's a different paper.",
                    "label": 0
                },
                {
                    "sent": "So I have written a paper comparing our method with layers, but those are for everyone.",
                    "label": 0
                },
                {
                    "sent": "Logest regularizer, logistic regression.",
                    "label": 0
                },
                {
                    "sent": "But in this work we are doing a role to logistic regression, so that's why it is not compared here.",
                    "label": 0
                },
                {
                    "sent": "SPM is just a proxy.",
                    "label": 0
                },
                {
                    "sent": "You exact convex proxy to a decision problem, so you might be doing better with no job in finding a better solution for the for the convex problem, the question is what is actually the performances.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hands off, you know error rate.",
                    "label": 0
                },
                {
                    "sent": "So do you have any graphs like this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so this one is.",
                    "label": 0
                },
                {
                    "sent": "Accuracy difference accuracy is in the objective function now.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Maybe a little bit misleading, so the accuracy difference means we we use a very, very small stopping tolerance for getting the final model and we check water testing accuracy of the final model is.",
                    "label": 0
                },
                {
                    "sent": "OK, now that's that's considered the little base.",
                    "label": 0
                },
                {
                    "sent": "Then, starting from the training time zero OK from the first iteration, we check which then we get the model.",
                    "label": 0
                },
                {
                    "sent": "Then we check water.",
                    "label": 0
                },
                {
                    "sent": "Testing accuracy is.",
                    "label": 0
                },
                {
                    "sent": "Then we check the difference.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the error rate yeah oh.",
                    "label": 0
                },
                {
                    "sent": "You don't get aerated hundreds OK.",
                    "label": 0
                },
                {
                    "sent": "So for example, if if your final testing accuracy is 100.",
                    "label": 0
                },
                {
                    "sent": "OK is 100 and then in the beginning you don't do anything, so 100% invested 100% OK and then in the beginning you're.",
                    "label": 0
                },
                {
                    "sent": "In the beginning, suppose you predict everything everything incorrectly.",
                    "label": 0
                },
                {
                    "sent": "Then your accuracy difference will be 100, so that's why this is bounded by by 1:50.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah, percentage yeah, that's right, yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is really the difference to the final model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, yeah, yeah, yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Actually, the question related this, so do you think it how important it is to?",
                    "label": 0
                },
                {
                    "sent": "It seems that your method is particularly good at getting that last bit.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, no, let's yeah, that's the strong point of Newton message.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So for learning problems, yeah, right?",
                    "label": 0
                },
                {
                    "sent": "Yep, Yep so.",
                    "label": 0
                },
                {
                    "sent": "So usually people say that in in doing learning problems you don't need to solve optimization problems very accurately.",
                    "label": 0
                },
                {
                    "sent": "Well, I agree that in quite a few situations that's the case.",
                    "label": 0
                },
                {
                    "sent": "OK, the reason why I tried to work on this is it every time I I I have very large multiclass datasets so I have to use one versus rest.",
                    "label": 0
                },
                {
                    "sent": "So I trained a lot of binary models.",
                    "label": 0
                },
                {
                    "sent": "So for each model I have to specify certain stopping tolerance and I find out that.",
                    "label": 0
                },
                {
                    "sent": "So if I use like things like iterative scaling, if I only specify a stopping condition then the number of iterations for that two class model maybe maybe very large and then so.",
                    "label": 0
                },
                {
                    "sent": "So, so in that situation I still need a method to to have reasonably fast final convergence.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so you see the increased actually location matrix is more your conditioned so you can see that here the training time is more.",
                    "label": 0
                },
                {
                    "sent": "Players, but there there's also an issue about the stopping condition.",
                    "label": 0
                },
                {
                    "sent": "Here I am using an absolute way of doing the staffing, so if I can consider seeing my stopping condition and this training time may not increase that much, ASI increases, so stopping condition is actually a very very important issue.",
                    "label": 0
                }
            ]
        }
    }
}