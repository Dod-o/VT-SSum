{
    "id": "oayy32vig7ujfilaxrhgk4o5caevjen6",
    "title": "Concentration Inequalities",
    "info": {
        "author": [
            "Gabor Lugosi, Department of Economics and Business, Pompeu Fabra University"
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/uai08_lugosi_ci/",
    "segmentation": [
        [
            "OK, so it's a pleasure.",
            "Thanks Tongans.",
            "Thanks Rocco for having me here.",
            "Can you project so concentration inequalities?",
            "This is a toolbox that those of us who study.",
            "Machine learning learning theory in in a probabilistic framework find it useful.",
            "What is this about?",
            "So when we.",
            "When?",
            "Some of US study machine learning problems.",
            "We model data is as independent random variables.",
            "And and concentration inequalities is a very general way of handling complicated functions of independent random variables, so that's typically the kind of animal we face when we study machine learning algorithms or what the risk of some algorithm do.",
            "We have some really complicated thing that depends in a convoluted way on independent random variables and independence.",
            "Here is the keyword, so that's an assumption that I'm going to make.",
            "Throughout this talk, of course it's an idealizing assumption, but that's that's our basic starting point and what I'm going to tell you today is a potpourri of ideas that float around this.",
            "This general idea that.",
            "Complicated functions of independent random variables are concentrated.",
            "What does it mean that it's concentrated?",
            "It's they are close to their expected value.",
            "They are close to what we would get on the average if we repeated the experiment many many, many times.",
            "So that's the concentration phenomenon.",
            "And in the past few decades this has been huge, not only in learning theory, but but in a number of areas, obviously.",
            "Obviously this is not a limited to learning theory.",
            "Whenever we study random structures, these kind of tools come in handy and not only for studying random structures, but also in some areas of mathematics when when probabilistic methods are used.",
            "When randomness is is is introduced artificially, like in geometry of high dimensional spaces or or combinatorics, probabilistic methods have been very useful and a lot of the.",
            "Of the development come from these areas of mathematics, so their motivation motivations abound.",
            "Across.",
            "Lots of fields, and in particular in machine learning.",
            "I just as a user I found them very exciting and very very useful tool."
        ],
        [
            "So let's see, OK.",
            "So.",
            "Here's here's what we could call classical concentration.",
            "Let's say we have independent random variables.",
            "They don't really need to be need to have the same distribution.",
            "But let's assume just to.",
            "For simplicity that they are, they are bounded, and in this talk I I'm not making attempt an attempt to give you the most general possible.",
            "Theorems or results?",
            "I'm trying to give you an idea of how one can one can get around how one can maybe prove concentration inequalities.",
            "What type of concentration inequality's may be available, and what may be the convenient tools to study them.",
            "So if we have independent random variables then of course the law of large numbers tells us that their average will be close to their expected value and there's a bunch of ways of making this.",
            "To quantify this, for example the the simplest way of looking at concentration is to look at the variance of a random variable.",
            "Of course, we know that that the variance of the sum of independent random variables is the sum of the variances.",
            "So immediately you get if you if you have bounded an average of bounded random variables, it's less than.",
            "1 / 4 N. OK, if they are bounded between zero and one now this is the type of inequality we like in machine learning because on the right hand side we have something that doesn't depend on the distribution.",
            "Of course made an assumption that these guys are bounded between zero and one.",
            "This is not such a strong assumption.",
            "Of course independence that you can argue about in many many cases.",
            "But as I said, I'm not going to argue about this in this talk, so this is the simplest possible classical concentration inequality.",
            "But of course one can say much more so.",
            "Before I go on can just can anyone tell me a concentration inequality named after a Finnish mathematician?",
            "Mr."
        ],
        [
            "OK, here you go.",
            "Have things inequality?",
            "So, uh, basically have Ding was one of the first guys who studied exponential inequalities.",
            "And of course this is.",
            "This inequality tells us that the average if you want to look at the difference between the average and its expected value, and you don't only want to look at the variance, the expected squared deviation, but you really want to know about.",
            "Pair probabilities then then this gives you a sharp reason will be short estimate, which is again distribution free.",
            "So this is bread and butter of of learning theorists.",
            "Of course we deal with.",
            "It averages all the time and we want to know about deviations.",
            "And this is really close to what you would expect from the central Limit Theorem, which is an asymptotic statement.",
            "But we like non asymptotic statement.",
            "So somehow in learning theory.",
            "That's somehow.",
            "I guess it's because we like to study high dimensional problems where we don't have so many data points, so, so maybe the synthetic assumption is not always reasonable, so we really like non asymptotic inequality.",
            "So this is one of them and this is this is one of the one of the useful."
        ],
        [
            "OK, so the concentration phenomenon that I'm.",
            "I'm planning to talk talk about today.",
            "Is this kind of inequalities but in a more general framework, so not only we are of course not only interested in averages of independent random variables, but maybe more complicated functions, so that's.",
            "That's that's the what we call concentration inequalities, and there's a bunch of different ways, methods that have been worked out to prove such inequalities.",
            "Maybe the first ones are based on martingales and it's really difficult to say who was the 1st, but it looks like I, as far as I. I know you're in school was the first who used martingale techniques to he was interested in average is a vector values Banach space values, random variables and basically the techniques he worked out work for arbitrary functions.",
            "We will come back to this a little bit later and then there was a big breakthrough when Millman.",
            "In the 70s.",
            "Showed how these techniques.",
            "Can be used in a variety of ways in high dimensional geometry and martingale techniques.",
            "Got a lot of visibility through calling like the Army's work in 89 where he showed how all these inequalities can be used in an incredibly powerful way in a variety of areas in combinatorics.",
            "So that was that was Mickey armies work but basically parallel to this there was there was a development in.",
            "Information theory where information theoretic methods were used to come up with very similar inequalities and the first one was also with the Gaussian kernel 76 or about the same time as you risky, but these were parallel developments.",
            "They didn't really know about each other.",
            "And the big breakthrough in the theory came with telegrams work in the 1990s where he came up with completely ad hoc.",
            "Elementary.",
            "Very insightful method to prove concentration inequalities that went way beyond what one could do before using, using, for example, martingales.",
            "OK, so that was a very important development kind of mysterious, those techniques, but Luckily Ledoux Michel Ledoux showed an attractive, easy way of recovering many of diagrams, inequalities and that was based on logarithmic Sobolev inequalities with that family of tools that.",
            "Came from from analysis and this is the branch I'm going to talk about.",
            "Yeah, so I'll try to convince you that in spite of the scary name, this is not not such a complicated tool and one can get to these inequalities in a fairly easy elementary way, so that's that's one of my goals.",
            "In this talk.",
            "OK, so I put here some references, but of course this is far from being exhaustive.",
            "There's there's a big big literature of all kinds of concentration, so.",
            "So let's let me start with.",
            "With that we don't kind of weak concentration inequality, weak in the sense that it only tells us about the variance of a function of independent random variables, but it's incredibly powerful and easy to use, and somehow captures the essence of the major phenomenon.",
            "So we have."
        ],
        [
            "And it's random variables and of them.",
            "And again I emphasize it's only independence that we assume here.",
            "They don't need to have the same distribution and we have a function of these variables.",
            "So now we plug in these independent random variables here and then we get.",
            "We got a real valued random variable Z, so this is our.",
            "This is the object that we want to study.",
            "You can think about these end could be training data and Z maybe some kind of risk of some complicated algorithm you can dream up with lots of different than any complicated function you want of these of independent random variables.",
            "Now we have a random variable and we want to know how far this is from its expected value.",
            "So when we talk about concentration.",
            "Then we forget we were not studying how big.",
            "Typically this random variable is.",
            "We just want to know how far it is from its typical value.",
            "So that's the concentration phenomenon.",
            "So this inequality by Efron Stein.",
            "It says that let's look at what happens if we replace one of the one of the random variables by another one, an independent copy.",
            "So here we have RN random variables.",
            "Here we have the same ones except the guy was replaced by.",
            "Another one, which is the same distribution as XY.",
            "OK, so we keep all the other variables fixed and we replace one of them and we look at how much the value of this function changes.",
            "OK, we square it with summit and this guy.",
            "Here is an upper bound on the variance.",
            "OK, so the variance is always less than or equal to the expected 1/2 times the expected value of the sum of these squared differences.",
            "So what's what's the moral of this story?",
            "Well, that if this is a stable function in the sense that that individual variables don't cause a big effect on the outcome, then we have concentration.",
            "How good is it?",
            "Well, it's pretty good because you just think about the special case when this is a sum.",
            "If F is the sum of it of its argument, then of course Z -- Z I is just the difference between XI and XI prime.",
            "OK, and then you can see that we have.",
            "We actually have equality here.",
            "So in the special case of a sum, the variance equals this guy.",
            "And for any other random variable, the variance is actually smaller than this guy.",
            "So this is a very important observation that in some sense.",
            "Sums are the least concentrated of all possible functions.",
            "Well, in what sense?",
            "In this precise sense, OK, and that's very nice, because we know we know a lot about sums.",
            "I showed you how things inequality and the variance is easy to compute, but that's good news, OK?",
            "So if if by changing one variable of the function, the value of the function can change too much, then the variance can be bounded and I just put here an equivalent formulation.",
            "When we only look at one side of these deviations, this is just the positive part of Z minus ZI.",
            "That's useful when that will be useful later when we want to use this inequality.",
            "OK, so that's."
        ],
        [
            "The Efron Stein inequality and this is Efron Stein.",
            "Wait a minute, this is not stain, so this is Brad Efron from."
        ],
        [
            "And this is fine.",
            "There are two statistics professors from Stanford.",
            "Famous for many other things, this is not their major claim to fame, but I think they deserve this inequality."
        ],
        [
            "Named after them, and here's one example that comes up in learning theory a lot.",
            "Let's say we have a collection of sets.",
            "And we and we have N independent random variables.",
            "Let's say they have the same distribution and we want to compare the probability of a set.",
            "Be OK with the empirical probability of the set, so we want to compare them, but we actually want to compare the maximum of this deviation over this whole class of sets this in.",
            "Kind of 1st type of analysis of.",
            "For many machine learning algorithms or empirical risk minimization algorithms, this is.",
            "This is an important quantity that comes up all the time.",
            "OK, so let's how do we use the Efron Stein inequality?",
            "Well, we want to see what happens if we change one variable.",
            "What's the effect of this on on this guy on this?",
            "See?",
            "Well, if we change one variable then.",
            "No matter what, this Supreme can change by at most one over North.",
            "Right, this guy doesn't change here.",
            "All the averages can change by 1 / N so that the maximum deviation cannot change by more than N, so the variance is less than 1 / 2 N. No matter what, this is a beautiful result because it not only because this doesn't depend on the distribution, but also it doesn't depend on the size of this set.",
            "Right, so the concentration is easy, it's done.",
            "Finished story over.",
            "Now all you can all you need to do is to study the expected value of this and we expected values often.",
            "Not always, but often easier.",
            "Much easier to study than than just the whole complete behavior.",
            "And here we have a non synthetic distribution free free bound that doesn't depend on on on the size of this class.",
            "This was a big surprise to me when I saw it first.",
            "Because somehow it's not.",
            "Intuitively it's not clear why the deviation shouldn't depend on the size of the class.",
            "They don't, it's only the expected value that reflects that reflects the somehow the 1st order, the type of behavior of.",
            "Of this random variable?",
            "OK, so this is the.",
            "This is the kind of applications of concentration inequalities that make it make it."
        ],
        [
            "Extremely useful, so here's another.",
            "Example taken from learning theory, random VC dimension.",
            "So take take a set of endpoints.",
            "And I think the the largest subset of these endpoints that's shattered by this class of set.",
            "So we have the same class of sets as on the previous slide, and well, for those of you don't know.",
            "So we say that a set of points is shattered by a class if if.",
            "If the intersection of elements of this class with that set gives us all possible subsets of that set of all the two to the power of number of elements of that set, OK, so if these guys for any fixed endpoints, we can, we can check the set of the largest subset such that all possible we can intersect them.",
            "All possible ways with elements of this is the random.",
            "Why is it random?",
            "Because now we're going to put random variables here.",
            "We're going to take a random set and now we have a random variable Z.",
            "Now you can very easily check that deterministically no matter what.",
            "If you look at the quantity appearing on the right hand side of the Efron Stein inequality, that's always less than the random VC dimension itself.",
            "Well, why is it true?",
            "Well, here, so we take our set of our points and we fix the largest shattered subset.",
            "Now, what happens if I change?",
            "If I change some other points that's not in this fixed small subset, well?",
            "It's not.",
            "Nothing changes the largest shattered subset can only go up, but here I'm only looking at.",
            "Downwards changes, so if I change anything outside of this largest shattered subset then nothing changes and if I change one of these guys then the change can be at most one so.",
            "You get this.",
            "OK, so the variance is always less than the expected value.",
            "What's the expected value?",
            "We don't know.",
            "That depends on all kinds of the distribution, the size of the structure of this.",
            "It's a very complicated thing, but the variance is ups.",
            "Sorry, go back back back back back.",
            "The variance variance is always bounded by by the expected value.",
            "OK, and that's really good news because that means that we have a stable random variable.",
            "The typical deviations are of the order of the square root of the of the expected value.",
            "That means that the fluctuations are small compared to the expected value we call these stable random variables, so the random VC dimension is."
        ],
        [
            "Table and you already show my next slide.",
            "VC is Monica chairman in case they.",
            "They were The Pioneers of a statistical learning theory, way, way, way ahead of their time.",
            "In 70 four they wrote a wonderful book where you see you have.",
            "Empirical risk minimization.",
            "Large margin classifiers, perceptrons, everything, everything, super tractor machines, everything is there.",
            "So these two guys are."
        ],
        [
            "Yes, OK, so here's another thing that's not in the book of Ethnic, and Cermak is that could be it's implicitly, it's actually there Rademacher averages.",
            "This is rather marker.",
            "He has nothing to do with random averages, but.",
            "OK anyway.",
            "So here's.",
            "Here's what the other marker averages.",
            "It's we take the same class of sets as before we have.",
            "We take endpoints and now we look at how well we can correlate this random.",
            "The epsilon eyes are random plus plus minus one, so this is the correlation of this random noise with with the indicators of of the exercise in A and we look at the maximum correlation with a.",
            "Quantity is used in statistical learning theory for measuring somehow the effective size of.",
            "Of this class of sets, how well can you correlate random noise with with these indicator functions?",
            "That somehow if you can correlate very well, that means that somehow this class is too large, so this is something that's been used and we take the expected value with respect to the epsilon with respect to the the the noise variables.",
            "And now we have a random variable that.",
            "We have a function of these guys of these endpoints.",
            "Now once again in 2 lines you can prove that.",
            "Again, deterministically we have this relationship.",
            "The sum of the squared changes.",
            "The positive part is less than the the actual value of the random variable, so once again the variance is smaller than the expected value.",
            "This is again wonderful news because.",
            "Well, if if you know if you read the applicant Chairman and Kiss then you will know that this Class A is not too big.",
            "Then this guy here is of the order of square root of North.",
            "So the variances of over square root of N. That means the typical deviations of the order and to the 1/4 really small, very small, much smaller than what you would expect if you just naively wanted to use the Efron Stein inequality and say that all these differences are bounded by one.",
            "So this guy is less than an.",
            "Well here you actually get much something much better.",
            "So so this conditional rather marker averages are very very very concentrated.",
            "And again this this inequality is not asymptotically distribution free."
        ],
        [
            "OK, so the next step so far I've already shown I've only shown shown you how to bound the variance, but we can do more.",
            "We can.",
            "We can actually get exponential inequalities.",
            "Of the style of Holdings inequality that we saw on the second slide.",
            "For that we know that for averages we have a central limit theorem, and we have all these.",
            "Nice exponential inequality is a question can we extend?",
            "We kind of Efron Stein inequality's type of phenomenon to exponential inequalities and we can do it and for this we have to.",
            "We don't have to but we can go back to Shannon.",
            "This is Shannon statue in Mystic Michigan.",
            "Who of course his famous for for most famous for creative being the father of information theory and we can, we can just go back to the simplest.",
            "The first few pages of any information theory book and find the definition of the entropy.",
            "I don't think I have to go very deeply into this in this audience the conditional.",
            "So now X is discrete random variable.",
            "This is its entropy that depends on just only the distribution of this random variable if you have.",
            "A pair of random variables.",
            "You can define the conditional entropy.",
            "This way it's the entropy of the pair minus the entropy of Y.",
            "Is this expression, and by very simple convexity arguments you can.",
            "It's easy to see that the entropy is always less than log if if the variable X can take at most and values and the condition conditioning can only decrease the entropy.",
            "So basically only using this inequality you can prove."
        ],
        [
            "What does shorthand is?",
            "Japanese information theorists proved hands inequality, so what's hands inequality we have?",
            "A vector of random variables and this is the only slide in the talk where I don't assume that these are that they are independent, so this is a vector of this arbitrary discrete random variables.",
            "There's no independence here.",
            "These are completely arbitrary, and now we leave one guy out.",
            "We leave the variable out, so we get another vector, right?",
            "So this X this vector has some entropy, and then this XI has some entropy.",
            "Now if you sum the differences then.",
            "It's always bounded by the entropy of the first guy.",
            "This is really easy in two lines.",
            "You can prove it using using just just the fact that conditioning decreases the entropy.",
            "OK, so this is Hans inequality and we."
        ],
        [
            "Start working from here.",
            "Before before I do it, let me show you this is a nice.",
            "Immediate corollary of this very simple information theoretic inequality.",
            "It's kind of unexpected.",
            "Unexpected way, but I'm showing it to you because because isoperimetric inequalities have a lot to do with concentration, so isoparametric and concentration are.",
            "You can argue that they are basically the same thing.",
            "So here's here's a very simple isoperimetric inequality in the in the discrete cube.",
            "What's a nice parametric, the classical isoperimetric isoperimetric theorem says that among all sets on the plane with a given area, the the the ones with the.",
            "Minimal perimeter are circles.",
            "This is the classical isoperimetric theorem, so in isoperimetric you usually fix the size of a set, and you're asking which is the smallest which sets have the smallest boundary given the size, which are which have the smallest boundary, so we can ask this these questions in subsets of the of the discrete cube, so we have we have the discrete N dimensional discrete cube and think about this, the cubes as a graph now right?",
            "So too.",
            "The elements of the Q bar are the are the vertices and two of them are joined by an edge if they, if they're Hamming distance is one.",
            "OK, so think about this graph and now you can ask that given.",
            "Among those sets with a given size, which ones have the smallest boundary?",
            "Was the boundary.",
            "Well, you can define the boundary several ways.",
            "One of the natural ways is to count how many edges are there between the set and the complement of the set.",
            "This is called the edge boundary.",
            "OK, so this is the edge boundary that the number of edges that connect the set with the outside, so that the edges on the boundary.",
            "And now we can.",
            "We can just simply use hands inequality and buy.",
            "Taking a uniform distribution over this set a, then we know that the entropy of this guy is log and see what happens if I leave one guy out right?",
            "And you immediately get again 2 lines.",
            "You can do it.",
            "It has no mystery between here and here that the size of the edge boundary is always bigger than the size of the times.",
            "Log 2 to 1 divided by and this is optimal.",
            "You have equality here.",
            "Whenever, whenever a is a sub cube.",
            "OK, so this shows that the solutions of the edge isoperimetric problem in the in the hypercube are subcubes.",
            "This is a classical old result.",
            "It's just nice to see that you can get them from information theoretic principles and I'm OK we'll come back to isoperimetric inequality."
        ],
        [
            "A little bit later.",
            "OK, so the next step is.",
            "To use hands inequality to prove something something.",
            "Which is very much related to entropy and Efron Stein inequality.",
            "So here's here's what it is.",
            "The entropy and this is not the Shannon entropy, though this is it's very closely related.",
            "The entropy of a non negative random variable is defined as as.",
            "This difference.",
            "OK, so the expected value of Z log Z minus the expected value of Z times log expected value of Z. X log X is a convex function, so this is always non negative.",
            "And the entropies is somehow this gap in Jensen's inequality.",
            "Then using hands inequality and maybe 3 lines of calculus you can prove.",
            "That the entropy of Z squared.",
            "Now if Z is OK. Let me back up a little bit, so X now is a vector of independent random variables.",
            "We're back to independent and more over here.",
            "I assume that they actually are uniform.",
            "This is not so important.",
            "But I just don't want to complicate it.",
            "Constants and stuff like that, so let's just assume that X is uniformly distributed over the cube.",
            "That this means that the components are independent.",
            "Rademacher random variables.",
            "OK, so.",
            "And think any function on the cube.",
            "Then then Z is F of X.",
            "Then the entropy of Z squared.",
            "I have to squeeze because because the entropy is only the only defined for non negative random variables in NZ can be anything.",
            "So the entropy of Z squared is always less than and this is exactly the same quantity that we saw on Efron Stein inequality on the right hand side of the referenced an inequality.",
            "In fact you can prove you can see easily that this implies the Efron Stein inequality.",
            "Well in this.",
            "In this particular setup.",
            "And it also if you take an indicator function of a set, then this is this.",
            "This is actually the isoperimetric inequality.",
            "I showed you on the previous slide, so this is somehow a common generalization of Efron Stein and then isoperimetric inequality.",
            "Very easy from once you have hands, inequality is just two lines of calculus and this this has been been known.",
            "Of course in analysis, maybe for 20 years.",
            "This was used as a first step to prove an analogue statement for Gaussian random variables.",
            "The Gaussian, Gaussian, logarithmic Sobolev inequality with which has many many applications in a variety of areas.",
            "But here let's just stick to this very simple inequality.",
            "OK, this is our logarithmic Sobolev inequality.",
            "Now on the next slide, that's the most technique."
        ],
        [
            "Slide of the talk.",
            "I'll show you how this logarithmic sort of inequality can be used.",
            "To get exponential concentration inequalities so the trick and this is called the Herbst argument and I don't have a picture of herbs.",
            "Is.",
            "The main trick is that we use the.",
            "We have this function F and we want to prove concentration for this function F and we're on the hypercube, but it's just for simplicity.",
            "Of course, this can be generalized and then there are many variations for arbitrary independent random variables, But let's just stick to the hypercube for simplicity so.",
            "This is the function F for which we want concentration and what we do is is we define E to the Lambda F / 2.",
            "An why?",
            "Because because we want to get inequalities for the moment generating function E to the Lambda X.",
            "Easy to the Lambda F. And if I square this guy, then that's exactly what I get right into the Lambda F and we're looking at the entropy of that guy.",
            "The entropy of E to the Lambda F and we just tried it out and what you discover and that what?",
            "The entropy has the derivative of the moment generating function minus F log F. OK, so this is an expression that involves the moment generating function and and the derivative.",
            "So let's let's go back to the previous inequality."
        ],
        [
            "OK so here you see that the entropy is bounded by by this guy the entropy is bounded by.",
            "By this efference tank quantity so."
        ],
        [
            "So just assume that this quantity is bounded by a constant.",
            "Yeah, just just for simplicity, assume that we have.",
            "We have such a random variable.",
            "Such a function, for which there's some little squares can be bounded by some constant.",
            "There are many examples of this, and if you don't have this, you have to work a little bit harder.",
            "Just again, I just want to show you the simplest ideas.",
            "So if you have that then then the logarithmic Sobolev inequality becomes this differential inequality.",
            "Or for the moment, generating function on the right hand side, we have F appearing and in this quantity V which is V because it's kind of like an upper bound on the variance.",
            "That's why I chose V here and this can be solved very easily.",
            "And what you get is that F of Lambda is bounded by this exponential quantity, and once you have a bound on the moment generating function then you can just use Markov's inequality to get.",
            "To get an exponential inequality.",
            "OK, so from here to here.",
            "This is completely standard and what you get is that the probability of Z is greater than its expected value, plus Z is bounded by E to the minus T squared divided by this guy V. OK.",
            "So this is this is an extremely powerful concentration inequality that we can get in a few lines starting from."
        ],
        [
            "Algorithmics will have inequality, so for example we recover one of the classical concentration inequalities.",
            "That we could call the bounded differences inequality, but in the literature you see it as Azuma's inequality and like the Army's inequality, this is calling back the army in an FC Barcelona T shirt.",
            "And this this inequality extremely useful says that if.",
            "Function is such that by changing one variable, the function cannot change by more than, let's say one.",
            "Then you have exponential concentration.",
            "Then you have an inequality which looks exactly like have things inequality right?",
            "So for any function not only sums of independent variables, any function of independent variables the that has this kind of Lipschitz or this stability property, we have the same exponential concentration inequality that we had for some.",
            "It could be no.",
            "I don't think so.",
            "Now here, here, here is the same guy, but here I require this deterministically.",
            "Right, so in the Efron Stein inequality was something much better because because we only need it to bound the expected square of this thing here for the bounded differences inequality we need this hard constraint.",
            "So, so here we really.",
            "This inequality really assumes that changes are bounded deterministically, but you don't really need that.",
            "But you have to sacrifice something if you want to pass from variance inequalities to exponentially inequalities.",
            "It's obvious that you need some kind of extra conditions, this is."
        ],
        [
            "Conveniently.",
            "OK.",
            "So there are lots of variations of this.",
            "The scheme of the proof is the same.",
            "You start out with some kind of variations of the logarithmic Sobolev inequality.",
            "You arrive at a differential inequality and then you solve it.",
            "So for example, just going back to some of the examples I showed you, if Z is either the receive dimensional or the conditional Rademacher average.",
            "And here the only the only thing we need is that the Efron Stein quantity is bounded by Z itself.",
            "Then then you can get this.",
            "This type of sub Gaussian inequalities, the deviations.",
            "These are somehow the exact analogs of the variance inequalities we saw.",
            "You get nice exponential now."
        ],
        [
            "OK.",
            "So now in the remaining maybe 20 minutes, I want to show you some of the.",
            "Some completely different, maybe applications of the same kind of ideas, and this has to do with influences of functions.",
            "Influences of variables on set.",
            "So let's go back to the hypercube.",
            "We have a subset in the end dimensional hypercube and think take a uniform distribution on the hypercube.",
            "Again, this is not absolutely crucial.",
            "Independence is crucial, but things are simpler if we assume uniformity.",
            "The influence of a variable is.",
            "Probability that the I variable determine X variable is pivotal?",
            "What does it mean?",
            "It's pivotal that if we change the ice variable then and leave the others fixed, then you move in and out of the set.",
            "Right?",
            "So you can think about in social choice theory that you can think about we have voters and minus one is Hillary plus one is Obama and everyone casts his vote and we have a complicated voting scheme.",
            "Not necessarily majority, but really complicated and.",
            "American for example, and and so.",
            "A vector is in a.",
            "That means that that Obama wins.",
            "Vector is not in a dot means Hillary wins.",
            "So I'm I voter number I my influences the probability that I determine the outcome of the elections.",
            "OK, so this is the influence of the variable XI.",
            "Here is when I flip the bit.",
            "And the total influence is just the sum of the individual influences the sum of the individual influences just the edge boundary normalized of this set, right?",
            "It's just just you have this set and the number of ways number of edges this is connected to the outside."
        ],
        [
            "So here are some examples of dictator in a dictatorship.",
            "We would have the first guy.",
            "Determines the outcome.",
            "There's one, there's one voter, and whatever he says is the outcome.",
            "His influence is 1 or the other's influence is 0.",
            "The total influence equals one.",
            "The other extreme, we have the parity function when when we just we just count the number of votes and if it's even we are in the set.",
            "If it's odd than Hillary wins and here everybody has a big influence influence one.",
            "The total influence equals North.",
            "Of course this is not not a very good.",
            "I think you would get lots of legal problems if you.",
            "If you apply this, this voting scheme majority is more reasonable.",
            "In particular, voting scheme is usually it's good to have a monotone set right monotone, meaning that if you change your vote from from Obama to Hillary and Hillary won, then Hillary will still win.",
            "These are these are monotone sets.",
            "Majority is a monotone said that it's an important one.",
            "Here the central Limit Theorem tells you that the influence of every every voter is about 1 / sqrt 1 /, 2 / \u03c0 N, and the total influences just root 2 / \u03c0 asymptotically.",
            "Now the first time you can use the referenced an inequality for the indicator function of this guy and then what you get is that the variance.",
            "This is just the variance of the outcome is bounded by you get exactly the total influence divided by 4.",
            "So in particular Diophantine inequality tells you that dictatorship is the function with the smallest possible total influence, right?",
            "Because if you only consider kind of Fair voting schemes when the probability of winning is equal for the two.",
            "Voters, right?",
            "So because this is 1/4, so 1/4 is less than less than the total influence divided by 4.",
            "So the influence is always bigger than one.",
            "Dictatorship is the one that minimizes the toll."
        ],
        [
            "The influence of all guys.",
            "Now here's an inequality that that Falcon submarine ski proved based on the logarithmic Sobolev inequality.",
            "Once again, you take the logarithmic subway, have inequality on the hypercube, and in a few lines of calculus you get to this inequality.",
            "Don't don't really look at it, I'm just telling you that there there isn't such an inequality which involves the variance.",
            "B * 1 -- P is just the variance of the of the outcome and influences here, so you have the sum of the squared influences and the sum of the influences.",
            "So one of the easy consequences of this inequality is a classical result of calculating linear.",
            "That says that the maximum influence, no matter what the set is, the maximum influence is always bigger than the variance times log in over.",
            "Why is this surprising?",
            "Well, if all the influences are equal, and you know you want the voting scheme which has this property, if all the influences are equal, then the total influence is always bigger than login.",
            "No matter what the voting scheme is, the total influence is always bigger than login.",
            "By the way, this inequality and also this other color corollary, I'm going to tell you first were not proved using logarithmic syllable of inequalities.",
            "They are proved using discrete Fourier analysis, and this logarithmic solving equations give an interesting alternative for studying sets of subsets of the of the cube.",
            "Another corollary is that if the total influence is bounded is small, then a is.",
            "Basically there's a bounded number of voters that determine the outcome in some sense.",
            "In some approximate sense, these election schemes are called ahanta, so we have we have a small number of guys.",
            "That determines the outcome and all the rest.",
            "Don't don't count so that if you have any function whose total influence is small, then there's a small group of voters that determine the outcome.",
            "And if all this influences are equal, then the influence is always big.",
            "Big gross to Infinity and this."
        ],
        [
            "Will be important in a few slides, so why are these?",
            "Are these results?",
            "Nice because we can study threshold phenomenon with them.",
            "So what threshold final take now a subset of of the hypercube and assume that this is a monotone set.",
            "Monotone meaning that the indicator indicator function of this set is a monotone function of all the all the coordinates.",
            "Or if you wish by changing your vote from.",
            "From the loser to winner, the winner will stay the same.",
            "OK, that's what monotonicity means.",
            "Um?",
            "And assume now that that voters vote independently and the vote.",
            "Cast for Obama is everyone votes for Obama with probability P and to Hillary is probably 1 -- P. OK, so then you can ask what the probability of Obama winning is.",
            "You can just write it up.",
            "It's a nice, smooth nice.",
            "It's a complicated polynomial of P, but it's a.",
            "It's an increasing differential differentiable function.",
            "It's increasing because because this set is monotone.",
            "And it goes from zero to 1.",
            "So we can define the inverse of this function.",
            "Let's take a piece of AB, the probability for which the probability of Obama winning equals one.",
            "In particular, the critical probability is 1/2.",
            "Now this function grows from zero to 1.",
            "At some point it equals 1/2.",
            "We want to know how fast it grows from close to 0 to close to one, and then this week we call this the threshold width.",
            "So if epsilon is a small number then small positive number then you can you can ask.",
            "What's the difference between this probability when these probabilities give Obama winning with a big big probability and Obama winning with a very small probability?",
            "Yeah, this is the threshold with."
        ],
        [
            "So here are some examples.",
            "If you have a dictatorship then of course the probability of the outcome being one is equals P. So we have just this linear function.",
            "This is the critical value is 1/2 and transition is very very slow.",
            "We go as slowly as possible from zero to 1.",
            "If we have a majority then the transition is very quick.",
            "This is just a lot of large numbers.",
            "If we are slightly below.",
            "If the probability of voting to candidate number one is just is just slightly smaller than 1/2 then then candidate one will lose with a huge probably then if it's slightly bigger than he will win.",
            "Right, so transition is very quick.",
            "You can.",
            "You can see that by having things inequality for example that the the threshold interval is is of this order one over root, one over root N. So you can ask which are the sets for which we get this type of behavior.",
            "Which for which type?",
            "What are the properties of the monotone set that guarantee that we have this kind of it's in social choice theory?",
            "This is called asymptotic aggregation of information.",
            "We have a phase transition, a sharp threshold from very close to 0 to very close to 1."
        ],
        [
            "Well.",
            "The key to this is a simple equality that's called Russo's lemma, that tells us that the derivative of this function of this increasing function, that the probability of this set as a function of P. Equals the total influence and I put a superscript here just to indicate that this of course depends on the total influence, so the derivative equals the total influence.",
            "So this this is beautiful because it just tells us that if we want to study threshold phenomenon or we have to study these influences.",
            "The total influence is big, but that means that we have a quick transition.",
            "If the total influence is big around the critical value, then we have a quick transition.",
            "If it's small then then we have a slow transition.",
            "But but the result I showed you the calculated linear result.",
            "It tells us that transition is always quick.",
            "The influence is always big.",
            "If we have a nice.",
            "Fair voting scheme.",
            "If the influence if the set is symmetric in this very weak sense that the influence of every voter, every variable equals then, then the transition is always goes to zero within.",
            "Well, it goes to zero.",
            "In general, very slowly it's like one over log, but it's kind of beautiful that it's always we always have this fast transition and becomes large.",
            "The threshold with goes to 0.",
            "On the other hand, if you can prove using the other result, I showed you Rousseau's result that if the threshold is big, if the transition is slow, then we have a home to.",
            "Then there's a small number of variables that determines the outcome.",
            "For example, a dictatorship.",
            "When we had this really slow transition.",
            "OK, maybe I stop here, so I hope I give you more or less the flavor of some of the.",
            "The ways these concentration inequalities work, I only showed you one small corner of this.",
            "The arguments based on logarithmic Sobolev inequalities, but this is because because I like them so.",
            "OK.",
            "Question.",
            "Yeah so.",
            "About these Hammers like telegram, and everyone's time, the obvious nailing machine learning seems to be the cross validation problem.",
            "And even when you know when I look at my steel and how you scribes, then he says the assumptions are similar in spirit to bootstrapping or bagging.",
            "Yet somehow the problem is resisted analysis and yet question as to why these methods.",
            "Drag.",
            "Well, I one reason could be that that you don't have very good concentration.",
            "For.",
            "That we do.",
            "At least in some cases, in perfect seems like it does.",
            "So what's the gap in understanding what those cases are?",
            "It seems obviously it's taking out appointing a new car.",
            "Yeah, that's right.",
            "Well, in some cases I think you can.",
            "You can say things right if you have stability in some sense you can formulate condition.",
            "But yeah, it's a hard problem.",
            "I agree.",
            "I maybe some other tools should be developed, I'm not sure.",
            "I have money riding on one aspect of this problem.",
            "OK, good.",
            "Yes.",
            "No general.",
            "Martin is in general.",
            "I don't think that you can say things beautiful like this.",
            "For for things like.",
            "Rapidly mixing Markov chains that there exist results of this flavor becoming more from from.",
            "A method that I called information theoretic methods that are based on couplings and things like that.",
            "So for Markov chains I know there are some nice results, but it's very difficult to go beyond go significantly significantly beyond independence.",
            "Somehow independence is really crucial.",
            "Yeah.",
            "Random variables.",
            "About abroad.",
            "Right?",
            "Friday yeah, so in many cases there's no Gaussian limit in these, so the question was whether whether we can central limit type of results.",
            "But these inequalities are so general that in many cases we don't even have know if there's a limit distribution.",
            "So if you want to do something like that, you always need more general, more general assumptions.",
            "So one of one of one of the general tools is of course times method, the same Steiner Efron Stein inequality.",
            "And there exists.",
            "There are some new results by Saurav Chatterjee in Berkeley who who can show concentration inequalities in that flavor, but that's the best I know of.",
            "Yeah.",
            "There are generalizations about things inequality for the case of the variables are not, ideally terrible.",
            "Depends on the limited number of other variables that form a graph, yeah?",
            "Our average is yes, there are many qualities like this for general functions that would be a beautiful thing to have.",
            "If you have an independent dependence graph with small degree, that would be beautiful to have some concentration, some powerful concentration inequalities.",
            "Yeah.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it's a pleasure.",
                    "label": 0
                },
                {
                    "sent": "Thanks Tongans.",
                    "label": 0
                },
                {
                    "sent": "Thanks Rocco for having me here.",
                    "label": 0
                },
                {
                    "sent": "Can you project so concentration inequalities?",
                    "label": 1
                },
                {
                    "sent": "This is a toolbox that those of us who study.",
                    "label": 0
                },
                {
                    "sent": "Machine learning learning theory in in a probabilistic framework find it useful.",
                    "label": 0
                },
                {
                    "sent": "What is this about?",
                    "label": 0
                },
                {
                    "sent": "So when we.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                },
                {
                    "sent": "Some of US study machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "We model data is as independent random variables.",
                    "label": 0
                },
                {
                    "sent": "And and concentration inequalities is a very general way of handling complicated functions of independent random variables, so that's typically the kind of animal we face when we study machine learning algorithms or what the risk of some algorithm do.",
                    "label": 0
                },
                {
                    "sent": "We have some really complicated thing that depends in a convoluted way on independent random variables and independence.",
                    "label": 0
                },
                {
                    "sent": "Here is the keyword, so that's an assumption that I'm going to make.",
                    "label": 0
                },
                {
                    "sent": "Throughout this talk, of course it's an idealizing assumption, but that's that's our basic starting point and what I'm going to tell you today is a potpourri of ideas that float around this.",
                    "label": 0
                },
                {
                    "sent": "This general idea that.",
                    "label": 0
                },
                {
                    "sent": "Complicated functions of independent random variables are concentrated.",
                    "label": 0
                },
                {
                    "sent": "What does it mean that it's concentrated?",
                    "label": 0
                },
                {
                    "sent": "It's they are close to their expected value.",
                    "label": 0
                },
                {
                    "sent": "They are close to what we would get on the average if we repeated the experiment many many, many times.",
                    "label": 0
                },
                {
                    "sent": "So that's the concentration phenomenon.",
                    "label": 0
                },
                {
                    "sent": "And in the past few decades this has been huge, not only in learning theory, but but in a number of areas, obviously.",
                    "label": 0
                },
                {
                    "sent": "Obviously this is not a limited to learning theory.",
                    "label": 0
                },
                {
                    "sent": "Whenever we study random structures, these kind of tools come in handy and not only for studying random structures, but also in some areas of mathematics when when probabilistic methods are used.",
                    "label": 0
                },
                {
                    "sent": "When randomness is is is introduced artificially, like in geometry of high dimensional spaces or or combinatorics, probabilistic methods have been very useful and a lot of the.",
                    "label": 0
                },
                {
                    "sent": "Of the development come from these areas of mathematics, so their motivation motivations abound.",
                    "label": 0
                },
                {
                    "sent": "Across.",
                    "label": 0
                },
                {
                    "sent": "Lots of fields, and in particular in machine learning.",
                    "label": 0
                },
                {
                    "sent": "I just as a user I found them very exciting and very very useful tool.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's see, OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's here's what we could call classical concentration.",
                    "label": 1
                },
                {
                    "sent": "Let's say we have independent random variables.",
                    "label": 0
                },
                {
                    "sent": "They don't really need to be need to have the same distribution.",
                    "label": 0
                },
                {
                    "sent": "But let's assume just to.",
                    "label": 0
                },
                {
                    "sent": "For simplicity that they are, they are bounded, and in this talk I I'm not making attempt an attempt to give you the most general possible.",
                    "label": 0
                },
                {
                    "sent": "Theorems or results?",
                    "label": 0
                },
                {
                    "sent": "I'm trying to give you an idea of how one can one can get around how one can maybe prove concentration inequalities.",
                    "label": 0
                },
                {
                    "sent": "What type of concentration inequality's may be available, and what may be the convenient tools to study them.",
                    "label": 0
                },
                {
                    "sent": "So if we have independent random variables then of course the law of large numbers tells us that their average will be close to their expected value and there's a bunch of ways of making this.",
                    "label": 1
                },
                {
                    "sent": "To quantify this, for example the the simplest way of looking at concentration is to look at the variance of a random variable.",
                    "label": 0
                },
                {
                    "sent": "Of course, we know that that the variance of the sum of independent random variables is the sum of the variances.",
                    "label": 0
                },
                {
                    "sent": "So immediately you get if you if you have bounded an average of bounded random variables, it's less than.",
                    "label": 0
                },
                {
                    "sent": "1 / 4 N. OK, if they are bounded between zero and one now this is the type of inequality we like in machine learning because on the right hand side we have something that doesn't depend on the distribution.",
                    "label": 0
                },
                {
                    "sent": "Of course made an assumption that these guys are bounded between zero and one.",
                    "label": 0
                },
                {
                    "sent": "This is not such a strong assumption.",
                    "label": 0
                },
                {
                    "sent": "Of course independence that you can argue about in many many cases.",
                    "label": 0
                },
                {
                    "sent": "But as I said, I'm not going to argue about this in this talk, so this is the simplest possible classical concentration inequality.",
                    "label": 0
                },
                {
                    "sent": "But of course one can say much more so.",
                    "label": 0
                },
                {
                    "sent": "Before I go on can just can anyone tell me a concentration inequality named after a Finnish mathematician?",
                    "label": 0
                },
                {
                    "sent": "Mr.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here you go.",
                    "label": 0
                },
                {
                    "sent": "Have things inequality?",
                    "label": 0
                },
                {
                    "sent": "So, uh, basically have Ding was one of the first guys who studied exponential inequalities.",
                    "label": 0
                },
                {
                    "sent": "And of course this is.",
                    "label": 0
                },
                {
                    "sent": "This inequality tells us that the average if you want to look at the difference between the average and its expected value, and you don't only want to look at the variance, the expected squared deviation, but you really want to know about.",
                    "label": 0
                },
                {
                    "sent": "Pair probabilities then then this gives you a sharp reason will be short estimate, which is again distribution free.",
                    "label": 0
                },
                {
                    "sent": "So this is bread and butter of of learning theorists.",
                    "label": 0
                },
                {
                    "sent": "Of course we deal with.",
                    "label": 0
                },
                {
                    "sent": "It averages all the time and we want to know about deviations.",
                    "label": 0
                },
                {
                    "sent": "And this is really close to what you would expect from the central Limit Theorem, which is an asymptotic statement.",
                    "label": 0
                },
                {
                    "sent": "But we like non asymptotic statement.",
                    "label": 0
                },
                {
                    "sent": "So somehow in learning theory.",
                    "label": 0
                },
                {
                    "sent": "That's somehow.",
                    "label": 0
                },
                {
                    "sent": "I guess it's because we like to study high dimensional problems where we don't have so many data points, so, so maybe the synthetic assumption is not always reasonable, so we really like non asymptotic inequality.",
                    "label": 0
                },
                {
                    "sent": "So this is one of them and this is this is one of the one of the useful.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the concentration phenomenon that I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm planning to talk talk about today.",
                    "label": 0
                },
                {
                    "sent": "Is this kind of inequalities but in a more general framework, so not only we are of course not only interested in averages of independent random variables, but maybe more complicated functions, so that's.",
                    "label": 1
                },
                {
                    "sent": "That's that's the what we call concentration inequalities, and there's a bunch of different ways, methods that have been worked out to prove such inequalities.",
                    "label": 0
                },
                {
                    "sent": "Maybe the first ones are based on martingales and it's really difficult to say who was the 1st, but it looks like I, as far as I. I know you're in school was the first who used martingale techniques to he was interested in average is a vector values Banach space values, random variables and basically the techniques he worked out work for arbitrary functions.",
                    "label": 0
                },
                {
                    "sent": "We will come back to this a little bit later and then there was a big breakthrough when Millman.",
                    "label": 0
                },
                {
                    "sent": "In the 70s.",
                    "label": 0
                },
                {
                    "sent": "Showed how these techniques.",
                    "label": 0
                },
                {
                    "sent": "Can be used in a variety of ways in high dimensional geometry and martingale techniques.",
                    "label": 0
                },
                {
                    "sent": "Got a lot of visibility through calling like the Army's work in 89 where he showed how all these inequalities can be used in an incredibly powerful way in a variety of areas in combinatorics.",
                    "label": 0
                },
                {
                    "sent": "So that was that was Mickey armies work but basically parallel to this there was there was a development in.",
                    "label": 1
                },
                {
                    "sent": "Information theory where information theoretic methods were used to come up with very similar inequalities and the first one was also with the Gaussian kernel 76 or about the same time as you risky, but these were parallel developments.",
                    "label": 0
                },
                {
                    "sent": "They didn't really know about each other.",
                    "label": 0
                },
                {
                    "sent": "And the big breakthrough in the theory came with telegrams work in the 1990s where he came up with completely ad hoc.",
                    "label": 0
                },
                {
                    "sent": "Elementary.",
                    "label": 1
                },
                {
                    "sent": "Very insightful method to prove concentration inequalities that went way beyond what one could do before using, using, for example, martingales.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was a very important development kind of mysterious, those techniques, but Luckily Ledoux Michel Ledoux showed an attractive, easy way of recovering many of diagrams, inequalities and that was based on logarithmic Sobolev inequalities with that family of tools that.",
                    "label": 0
                },
                {
                    "sent": "Came from from analysis and this is the branch I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'll try to convince you that in spite of the scary name, this is not not such a complicated tool and one can get to these inequalities in a fairly easy elementary way, so that's that's one of my goals.",
                    "label": 0
                },
                {
                    "sent": "In this talk.",
                    "label": 0
                },
                {
                    "sent": "OK, so I put here some references, but of course this is far from being exhaustive.",
                    "label": 0
                },
                {
                    "sent": "There's there's a big big literature of all kinds of concentration, so.",
                    "label": 0
                },
                {
                    "sent": "So let's let me start with.",
                    "label": 0
                },
                {
                    "sent": "With that we don't kind of weak concentration inequality, weak in the sense that it only tells us about the variance of a function of independent random variables, but it's incredibly powerful and easy to use, and somehow captures the essence of the major phenomenon.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's random variables and of them.",
                    "label": 0
                },
                {
                    "sent": "And again I emphasize it's only independence that we assume here.",
                    "label": 0
                },
                {
                    "sent": "They don't need to have the same distribution and we have a function of these variables.",
                    "label": 0
                },
                {
                    "sent": "So now we plug in these independent random variables here and then we get.",
                    "label": 0
                },
                {
                    "sent": "We got a real valued random variable Z, so this is our.",
                    "label": 0
                },
                {
                    "sent": "This is the object that we want to study.",
                    "label": 0
                },
                {
                    "sent": "You can think about these end could be training data and Z maybe some kind of risk of some complicated algorithm you can dream up with lots of different than any complicated function you want of these of independent random variables.",
                    "label": 1
                },
                {
                    "sent": "Now we have a random variable and we want to know how far this is from its expected value.",
                    "label": 0
                },
                {
                    "sent": "So when we talk about concentration.",
                    "label": 0
                },
                {
                    "sent": "Then we forget we were not studying how big.",
                    "label": 0
                },
                {
                    "sent": "Typically this random variable is.",
                    "label": 0
                },
                {
                    "sent": "We just want to know how far it is from its typical value.",
                    "label": 0
                },
                {
                    "sent": "So that's the concentration phenomenon.",
                    "label": 0
                },
                {
                    "sent": "So this inequality by Efron Stein.",
                    "label": 0
                },
                {
                    "sent": "It says that let's look at what happens if we replace one of the one of the random variables by another one, an independent copy.",
                    "label": 0
                },
                {
                    "sent": "So here we have RN random variables.",
                    "label": 0
                },
                {
                    "sent": "Here we have the same ones except the guy was replaced by.",
                    "label": 0
                },
                {
                    "sent": "Another one, which is the same distribution as XY.",
                    "label": 0
                },
                {
                    "sent": "OK, so we keep all the other variables fixed and we replace one of them and we look at how much the value of this function changes.",
                    "label": 0
                },
                {
                    "sent": "OK, we square it with summit and this guy.",
                    "label": 0
                },
                {
                    "sent": "Here is an upper bound on the variance.",
                    "label": 0
                },
                {
                    "sent": "OK, so the variance is always less than or equal to the expected 1/2 times the expected value of the sum of these squared differences.",
                    "label": 0
                },
                {
                    "sent": "So what's what's the moral of this story?",
                    "label": 0
                },
                {
                    "sent": "Well, that if this is a stable function in the sense that that individual variables don't cause a big effect on the outcome, then we have concentration.",
                    "label": 0
                },
                {
                    "sent": "How good is it?",
                    "label": 0
                },
                {
                    "sent": "Well, it's pretty good because you just think about the special case when this is a sum.",
                    "label": 0
                },
                {
                    "sent": "If F is the sum of it of its argument, then of course Z -- Z I is just the difference between XI and XI prime.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you can see that we have.",
                    "label": 0
                },
                {
                    "sent": "We actually have equality here.",
                    "label": 0
                },
                {
                    "sent": "So in the special case of a sum, the variance equals this guy.",
                    "label": 0
                },
                {
                    "sent": "And for any other random variable, the variance is actually smaller than this guy.",
                    "label": 0
                },
                {
                    "sent": "So this is a very important observation that in some sense.",
                    "label": 0
                },
                {
                    "sent": "Sums are the least concentrated of all possible functions.",
                    "label": 0
                },
                {
                    "sent": "Well, in what sense?",
                    "label": 0
                },
                {
                    "sent": "In this precise sense, OK, and that's very nice, because we know we know a lot about sums.",
                    "label": 0
                },
                {
                    "sent": "I showed you how things inequality and the variance is easy to compute, but that's good news, OK?",
                    "label": 0
                },
                {
                    "sent": "So if if by changing one variable of the function, the value of the function can change too much, then the variance can be bounded and I just put here an equivalent formulation.",
                    "label": 0
                },
                {
                    "sent": "When we only look at one side of these deviations, this is just the positive part of Z minus ZI.",
                    "label": 0
                },
                {
                    "sent": "That's useful when that will be useful later when we want to use this inequality.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Efron Stein inequality and this is Efron Stein.",
                    "label": 0
                },
                {
                    "sent": "Wait a minute, this is not stain, so this is Brad Efron from.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is fine.",
                    "label": 0
                },
                {
                    "sent": "There are two statistics professors from Stanford.",
                    "label": 0
                },
                {
                    "sent": "Famous for many other things, this is not their major claim to fame, but I think they deserve this inequality.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Named after them, and here's one example that comes up in learning theory a lot.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have a collection of sets.",
                    "label": 1
                },
                {
                    "sent": "And we and we have N independent random variables.",
                    "label": 0
                },
                {
                    "sent": "Let's say they have the same distribution and we want to compare the probability of a set.",
                    "label": 0
                },
                {
                    "sent": "Be OK with the empirical probability of the set, so we want to compare them, but we actually want to compare the maximum of this deviation over this whole class of sets this in.",
                    "label": 0
                },
                {
                    "sent": "Kind of 1st type of analysis of.",
                    "label": 0
                },
                {
                    "sent": "For many machine learning algorithms or empirical risk minimization algorithms, this is.",
                    "label": 0
                },
                {
                    "sent": "This is an important quantity that comes up all the time.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's how do we use the Efron Stein inequality?",
                    "label": 0
                },
                {
                    "sent": "Well, we want to see what happens if we change one variable.",
                    "label": 0
                },
                {
                    "sent": "What's the effect of this on on this guy on this?",
                    "label": 0
                },
                {
                    "sent": "See?",
                    "label": 0
                },
                {
                    "sent": "Well, if we change one variable then.",
                    "label": 0
                },
                {
                    "sent": "No matter what, this Supreme can change by at most one over North.",
                    "label": 0
                },
                {
                    "sent": "Right, this guy doesn't change here.",
                    "label": 0
                },
                {
                    "sent": "All the averages can change by 1 / N so that the maximum deviation cannot change by more than N, so the variance is less than 1 / 2 N. No matter what, this is a beautiful result because it not only because this doesn't depend on the distribution, but also it doesn't depend on the size of this set.",
                    "label": 0
                },
                {
                    "sent": "Right, so the concentration is easy, it's done.",
                    "label": 0
                },
                {
                    "sent": "Finished story over.",
                    "label": 0
                },
                {
                    "sent": "Now all you can all you need to do is to study the expected value of this and we expected values often.",
                    "label": 0
                },
                {
                    "sent": "Not always, but often easier.",
                    "label": 0
                },
                {
                    "sent": "Much easier to study than than just the whole complete behavior.",
                    "label": 0
                },
                {
                    "sent": "And here we have a non synthetic distribution free free bound that doesn't depend on on on the size of this class.",
                    "label": 0
                },
                {
                    "sent": "This was a big surprise to me when I saw it first.",
                    "label": 0
                },
                {
                    "sent": "Because somehow it's not.",
                    "label": 0
                },
                {
                    "sent": "Intuitively it's not clear why the deviation shouldn't depend on the size of the class.",
                    "label": 0
                },
                {
                    "sent": "They don't, it's only the expected value that reflects that reflects the somehow the 1st order, the type of behavior of.",
                    "label": 0
                },
                {
                    "sent": "Of this random variable?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of applications of concentration inequalities that make it make it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Extremely useful, so here's another.",
                    "label": 0
                },
                {
                    "sent": "Example taken from learning theory, random VC dimension.",
                    "label": 1
                },
                {
                    "sent": "So take take a set of endpoints.",
                    "label": 0
                },
                {
                    "sent": "And I think the the largest subset of these endpoints that's shattered by this class of set.",
                    "label": 1
                },
                {
                    "sent": "So we have the same class of sets as on the previous slide, and well, for those of you don't know.",
                    "label": 0
                },
                {
                    "sent": "So we say that a set of points is shattered by a class if if.",
                    "label": 0
                },
                {
                    "sent": "If the intersection of elements of this class with that set gives us all possible subsets of that set of all the two to the power of number of elements of that set, OK, so if these guys for any fixed endpoints, we can, we can check the set of the largest subset such that all possible we can intersect them.",
                    "label": 0
                },
                {
                    "sent": "All possible ways with elements of this is the random.",
                    "label": 0
                },
                {
                    "sent": "Why is it random?",
                    "label": 0
                },
                {
                    "sent": "Because now we're going to put random variables here.",
                    "label": 0
                },
                {
                    "sent": "We're going to take a random set and now we have a random variable Z.",
                    "label": 0
                },
                {
                    "sent": "Now you can very easily check that deterministically no matter what.",
                    "label": 0
                },
                {
                    "sent": "If you look at the quantity appearing on the right hand side of the Efron Stein inequality, that's always less than the random VC dimension itself.",
                    "label": 0
                },
                {
                    "sent": "Well, why is it true?",
                    "label": 0
                },
                {
                    "sent": "Well, here, so we take our set of our points and we fix the largest shattered subset.",
                    "label": 0
                },
                {
                    "sent": "Now, what happens if I change?",
                    "label": 0
                },
                {
                    "sent": "If I change some other points that's not in this fixed small subset, well?",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "Nothing changes the largest shattered subset can only go up, but here I'm only looking at.",
                    "label": 0
                },
                {
                    "sent": "Downwards changes, so if I change anything outside of this largest shattered subset then nothing changes and if I change one of these guys then the change can be at most one so.",
                    "label": 0
                },
                {
                    "sent": "You get this.",
                    "label": 0
                },
                {
                    "sent": "OK, so the variance is always less than the expected value.",
                    "label": 0
                },
                {
                    "sent": "What's the expected value?",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 1
                },
                {
                    "sent": "That depends on all kinds of the distribution, the size of the structure of this.",
                    "label": 0
                },
                {
                    "sent": "It's a very complicated thing, but the variance is ups.",
                    "label": 0
                },
                {
                    "sent": "Sorry, go back back back back back.",
                    "label": 0
                },
                {
                    "sent": "The variance variance is always bounded by by the expected value.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's really good news because that means that we have a stable random variable.",
                    "label": 0
                },
                {
                    "sent": "The typical deviations are of the order of the square root of the of the expected value.",
                    "label": 0
                },
                {
                    "sent": "That means that the fluctuations are small compared to the expected value we call these stable random variables, so the random VC dimension is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Table and you already show my next slide.",
                    "label": 0
                },
                {
                    "sent": "VC is Monica chairman in case they.",
                    "label": 0
                },
                {
                    "sent": "They were The Pioneers of a statistical learning theory, way, way, way ahead of their time.",
                    "label": 0
                },
                {
                    "sent": "In 70 four they wrote a wonderful book where you see you have.",
                    "label": 0
                },
                {
                    "sent": "Empirical risk minimization.",
                    "label": 0
                },
                {
                    "sent": "Large margin classifiers, perceptrons, everything, everything, super tractor machines, everything is there.",
                    "label": 0
                },
                {
                    "sent": "So these two guys are.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, OK, so here's another thing that's not in the book of Ethnic, and Cermak is that could be it's implicitly, it's actually there Rademacher averages.",
                    "label": 0
                },
                {
                    "sent": "This is rather marker.",
                    "label": 0
                },
                {
                    "sent": "He has nothing to do with random averages, but.",
                    "label": 0
                },
                {
                    "sent": "OK anyway.",
                    "label": 0
                },
                {
                    "sent": "So here's.",
                    "label": 0
                },
                {
                    "sent": "Here's what the other marker averages.",
                    "label": 0
                },
                {
                    "sent": "It's we take the same class of sets as before we have.",
                    "label": 0
                },
                {
                    "sent": "We take endpoints and now we look at how well we can correlate this random.",
                    "label": 0
                },
                {
                    "sent": "The epsilon eyes are random plus plus minus one, so this is the correlation of this random noise with with the indicators of of the exercise in A and we look at the maximum correlation with a.",
                    "label": 0
                },
                {
                    "sent": "Quantity is used in statistical learning theory for measuring somehow the effective size of.",
                    "label": 0
                },
                {
                    "sent": "Of this class of sets, how well can you correlate random noise with with these indicator functions?",
                    "label": 0
                },
                {
                    "sent": "That somehow if you can correlate very well, that means that somehow this class is too large, so this is something that's been used and we take the expected value with respect to the epsilon with respect to the the the noise variables.",
                    "label": 0
                },
                {
                    "sent": "And now we have a random variable that.",
                    "label": 0
                },
                {
                    "sent": "We have a function of these guys of these endpoints.",
                    "label": 0
                },
                {
                    "sent": "Now once again in 2 lines you can prove that.",
                    "label": 0
                },
                {
                    "sent": "Again, deterministically we have this relationship.",
                    "label": 0
                },
                {
                    "sent": "The sum of the squared changes.",
                    "label": 0
                },
                {
                    "sent": "The positive part is less than the the actual value of the random variable, so once again the variance is smaller than the expected value.",
                    "label": 0
                },
                {
                    "sent": "This is again wonderful news because.",
                    "label": 0
                },
                {
                    "sent": "Well, if if you know if you read the applicant Chairman and Kiss then you will know that this Class A is not too big.",
                    "label": 0
                },
                {
                    "sent": "Then this guy here is of the order of square root of North.",
                    "label": 0
                },
                {
                    "sent": "So the variances of over square root of N. That means the typical deviations of the order and to the 1/4 really small, very small, much smaller than what you would expect if you just naively wanted to use the Efron Stein inequality and say that all these differences are bounded by one.",
                    "label": 0
                },
                {
                    "sent": "So this guy is less than an.",
                    "label": 0
                },
                {
                    "sent": "Well here you actually get much something much better.",
                    "label": 0
                },
                {
                    "sent": "So so this conditional rather marker averages are very very very concentrated.",
                    "label": 0
                },
                {
                    "sent": "And again this this inequality is not asymptotically distribution free.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the next step so far I've already shown I've only shown shown you how to bound the variance, but we can do more.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "We can actually get exponential inequalities.",
                    "label": 1
                },
                {
                    "sent": "Of the style of Holdings inequality that we saw on the second slide.",
                    "label": 0
                },
                {
                    "sent": "For that we know that for averages we have a central limit theorem, and we have all these.",
                    "label": 0
                },
                {
                    "sent": "Nice exponential inequality is a question can we extend?",
                    "label": 0
                },
                {
                    "sent": "We kind of Efron Stein inequality's type of phenomenon to exponential inequalities and we can do it and for this we have to.",
                    "label": 0
                },
                {
                    "sent": "We don't have to but we can go back to Shannon.",
                    "label": 0
                },
                {
                    "sent": "This is Shannon statue in Mystic Michigan.",
                    "label": 0
                },
                {
                    "sent": "Who of course his famous for for most famous for creative being the father of information theory and we can, we can just go back to the simplest.",
                    "label": 0
                },
                {
                    "sent": "The first few pages of any information theory book and find the definition of the entropy.",
                    "label": 0
                },
                {
                    "sent": "I don't think I have to go very deeply into this in this audience the conditional.",
                    "label": 0
                },
                {
                    "sent": "So now X is discrete random variable.",
                    "label": 0
                },
                {
                    "sent": "This is its entropy that depends on just only the distribution of this random variable if you have.",
                    "label": 0
                },
                {
                    "sent": "A pair of random variables.",
                    "label": 1
                },
                {
                    "sent": "You can define the conditional entropy.",
                    "label": 0
                },
                {
                    "sent": "This way it's the entropy of the pair minus the entropy of Y.",
                    "label": 0
                },
                {
                    "sent": "Is this expression, and by very simple convexity arguments you can.",
                    "label": 0
                },
                {
                    "sent": "It's easy to see that the entropy is always less than log if if the variable X can take at most and values and the condition conditioning can only decrease the entropy.",
                    "label": 0
                },
                {
                    "sent": "So basically only using this inequality you can prove.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What does shorthand is?",
                    "label": 0
                },
                {
                    "sent": "Japanese information theorists proved hands inequality, so what's hands inequality we have?",
                    "label": 0
                },
                {
                    "sent": "A vector of random variables and this is the only slide in the talk where I don't assume that these are that they are independent, so this is a vector of this arbitrary discrete random variables.",
                    "label": 0
                },
                {
                    "sent": "There's no independence here.",
                    "label": 0
                },
                {
                    "sent": "These are completely arbitrary, and now we leave one guy out.",
                    "label": 0
                },
                {
                    "sent": "We leave the variable out, so we get another vector, right?",
                    "label": 0
                },
                {
                    "sent": "So this X this vector has some entropy, and then this XI has some entropy.",
                    "label": 0
                },
                {
                    "sent": "Now if you sum the differences then.",
                    "label": 0
                },
                {
                    "sent": "It's always bounded by the entropy of the first guy.",
                    "label": 0
                },
                {
                    "sent": "This is really easy in two lines.",
                    "label": 0
                },
                {
                    "sent": "You can prove it using using just just the fact that conditioning decreases the entropy.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is Hans inequality and we.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start working from here.",
                    "label": 0
                },
                {
                    "sent": "Before before I do it, let me show you this is a nice.",
                    "label": 0
                },
                {
                    "sent": "Immediate corollary of this very simple information theoretic inequality.",
                    "label": 0
                },
                {
                    "sent": "It's kind of unexpected.",
                    "label": 0
                },
                {
                    "sent": "Unexpected way, but I'm showing it to you because because isoperimetric inequalities have a lot to do with concentration, so isoparametric and concentration are.",
                    "label": 0
                },
                {
                    "sent": "You can argue that they are basically the same thing.",
                    "label": 0
                },
                {
                    "sent": "So here's here's a very simple isoperimetric inequality in the in the discrete cube.",
                    "label": 1
                },
                {
                    "sent": "What's a nice parametric, the classical isoperimetric isoperimetric theorem says that among all sets on the plane with a given area, the the the ones with the.",
                    "label": 0
                },
                {
                    "sent": "Minimal perimeter are circles.",
                    "label": 0
                },
                {
                    "sent": "This is the classical isoperimetric theorem, so in isoperimetric you usually fix the size of a set, and you're asking which is the smallest which sets have the smallest boundary given the size, which are which have the smallest boundary, so we can ask this these questions in subsets of the of the discrete cube, so we have we have the discrete N dimensional discrete cube and think about this, the cubes as a graph now right?",
                    "label": 0
                },
                {
                    "sent": "So too.",
                    "label": 0
                },
                {
                    "sent": "The elements of the Q bar are the are the vertices and two of them are joined by an edge if they, if they're Hamming distance is one.",
                    "label": 0
                },
                {
                    "sent": "OK, so think about this graph and now you can ask that given.",
                    "label": 0
                },
                {
                    "sent": "Among those sets with a given size, which ones have the smallest boundary?",
                    "label": 0
                },
                {
                    "sent": "Was the boundary.",
                    "label": 0
                },
                {
                    "sent": "Well, you can define the boundary several ways.",
                    "label": 0
                },
                {
                    "sent": "One of the natural ways is to count how many edges are there between the set and the complement of the set.",
                    "label": 0
                },
                {
                    "sent": "This is called the edge boundary.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the edge boundary that the number of edges that connect the set with the outside, so that the edges on the boundary.",
                    "label": 0
                },
                {
                    "sent": "And now we can.",
                    "label": 0
                },
                {
                    "sent": "We can just simply use hands inequality and buy.",
                    "label": 0
                },
                {
                    "sent": "Taking a uniform distribution over this set a, then we know that the entropy of this guy is log and see what happens if I leave one guy out right?",
                    "label": 0
                },
                {
                    "sent": "And you immediately get again 2 lines.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "It has no mystery between here and here that the size of the edge boundary is always bigger than the size of the times.",
                    "label": 0
                },
                {
                    "sent": "Log 2 to 1 divided by and this is optimal.",
                    "label": 0
                },
                {
                    "sent": "You have equality here.",
                    "label": 1
                },
                {
                    "sent": "Whenever, whenever a is a sub cube.",
                    "label": 0
                },
                {
                    "sent": "OK, so this shows that the solutions of the edge isoperimetric problem in the in the hypercube are subcubes.",
                    "label": 0
                },
                {
                    "sent": "This is a classical old result.",
                    "label": 0
                },
                {
                    "sent": "It's just nice to see that you can get them from information theoretic principles and I'm OK we'll come back to isoperimetric inequality.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit later.",
                    "label": 0
                },
                {
                    "sent": "OK, so the next step is.",
                    "label": 0
                },
                {
                    "sent": "To use hands inequality to prove something something.",
                    "label": 0
                },
                {
                    "sent": "Which is very much related to entropy and Efron Stein inequality.",
                    "label": 0
                },
                {
                    "sent": "So here's here's what it is.",
                    "label": 0
                },
                {
                    "sent": "The entropy and this is not the Shannon entropy, though this is it's very closely related.",
                    "label": 0
                },
                {
                    "sent": "The entropy of a non negative random variable is defined as as.",
                    "label": 0
                },
                {
                    "sent": "This difference.",
                    "label": 0
                },
                {
                    "sent": "OK, so the expected value of Z log Z minus the expected value of Z times log expected value of Z. X log X is a convex function, so this is always non negative.",
                    "label": 0
                },
                {
                    "sent": "And the entropies is somehow this gap in Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "Then using hands inequality and maybe 3 lines of calculus you can prove.",
                    "label": 0
                },
                {
                    "sent": "That the entropy of Z squared.",
                    "label": 0
                },
                {
                    "sent": "Now if Z is OK. Let me back up a little bit, so X now is a vector of independent random variables.",
                    "label": 0
                },
                {
                    "sent": "We're back to independent and more over here.",
                    "label": 0
                },
                {
                    "sent": "I assume that they actually are uniform.",
                    "label": 0
                },
                {
                    "sent": "This is not so important.",
                    "label": 0
                },
                {
                    "sent": "But I just don't want to complicate it.",
                    "label": 0
                },
                {
                    "sent": "Constants and stuff like that, so let's just assume that X is uniformly distributed over the cube.",
                    "label": 0
                },
                {
                    "sent": "That this means that the components are independent.",
                    "label": 0
                },
                {
                    "sent": "Rademacher random variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And think any function on the cube.",
                    "label": 0
                },
                {
                    "sent": "Then then Z is F of X.",
                    "label": 0
                },
                {
                    "sent": "Then the entropy of Z squared.",
                    "label": 0
                },
                {
                    "sent": "I have to squeeze because because the entropy is only the only defined for non negative random variables in NZ can be anything.",
                    "label": 0
                },
                {
                    "sent": "So the entropy of Z squared is always less than and this is exactly the same quantity that we saw on Efron Stein inequality on the right hand side of the referenced an inequality.",
                    "label": 0
                },
                {
                    "sent": "In fact you can prove you can see easily that this implies the Efron Stein inequality.",
                    "label": 0
                },
                {
                    "sent": "Well in this.",
                    "label": 0
                },
                {
                    "sent": "In this particular setup.",
                    "label": 0
                },
                {
                    "sent": "And it also if you take an indicator function of a set, then this is this.",
                    "label": 0
                },
                {
                    "sent": "This is actually the isoperimetric inequality.",
                    "label": 1
                },
                {
                    "sent": "I showed you on the previous slide, so this is somehow a common generalization of Efron Stein and then isoperimetric inequality.",
                    "label": 0
                },
                {
                    "sent": "Very easy from once you have hands, inequality is just two lines of calculus and this this has been been known.",
                    "label": 0
                },
                {
                    "sent": "Of course in analysis, maybe for 20 years.",
                    "label": 0
                },
                {
                    "sent": "This was used as a first step to prove an analogue statement for Gaussian random variables.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian, Gaussian, logarithmic Sobolev inequality with which has many many applications in a variety of areas.",
                    "label": 0
                },
                {
                    "sent": "But here let's just stick to this very simple inequality.",
                    "label": 1
                },
                {
                    "sent": "OK, this is our logarithmic Sobolev inequality.",
                    "label": 0
                },
                {
                    "sent": "Now on the next slide, that's the most technique.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slide of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'll show you how this logarithmic sort of inequality can be used.",
                    "label": 0
                },
                {
                    "sent": "To get exponential concentration inequalities so the trick and this is called the Herbst argument and I don't have a picture of herbs.",
                    "label": 1
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "The main trick is that we use the.",
                    "label": 0
                },
                {
                    "sent": "We have this function F and we want to prove concentration for this function F and we're on the hypercube, but it's just for simplicity.",
                    "label": 0
                },
                {
                    "sent": "Of course, this can be generalized and then there are many variations for arbitrary independent random variables, But let's just stick to the hypercube for simplicity so.",
                    "label": 1
                },
                {
                    "sent": "This is the function F for which we want concentration and what we do is is we define E to the Lambda F / 2.",
                    "label": 0
                },
                {
                    "sent": "An why?",
                    "label": 0
                },
                {
                    "sent": "Because because we want to get inequalities for the moment generating function E to the Lambda X.",
                    "label": 1
                },
                {
                    "sent": "Easy to the Lambda F. And if I square this guy, then that's exactly what I get right into the Lambda F and we're looking at the entropy of that guy.",
                    "label": 0
                },
                {
                    "sent": "The entropy of E to the Lambda F and we just tried it out and what you discover and that what?",
                    "label": 0
                },
                {
                    "sent": "The entropy has the derivative of the moment generating function minus F log F. OK, so this is an expression that involves the moment generating function and and the derivative.",
                    "label": 0
                },
                {
                    "sent": "So let's let's go back to the previous inequality.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here you see that the entropy is bounded by by this guy the entropy is bounded by.",
                    "label": 0
                },
                {
                    "sent": "By this efference tank quantity so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just assume that this quantity is bounded by a constant.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just just for simplicity, assume that we have.",
                    "label": 0
                },
                {
                    "sent": "We have such a random variable.",
                    "label": 0
                },
                {
                    "sent": "Such a function, for which there's some little squares can be bounded by some constant.",
                    "label": 0
                },
                {
                    "sent": "There are many examples of this, and if you don't have this, you have to work a little bit harder.",
                    "label": 0
                },
                {
                    "sent": "Just again, I just want to show you the simplest ideas.",
                    "label": 0
                },
                {
                    "sent": "So if you have that then then the logarithmic Sobolev inequality becomes this differential inequality.",
                    "label": 0
                },
                {
                    "sent": "Or for the moment, generating function on the right hand side, we have F appearing and in this quantity V which is V because it's kind of like an upper bound on the variance.",
                    "label": 1
                },
                {
                    "sent": "That's why I chose V here and this can be solved very easily.",
                    "label": 0
                },
                {
                    "sent": "And what you get is that F of Lambda is bounded by this exponential quantity, and once you have a bound on the moment generating function then you can just use Markov's inequality to get.",
                    "label": 0
                },
                {
                    "sent": "To get an exponential inequality.",
                    "label": 0
                },
                {
                    "sent": "OK, so from here to here.",
                    "label": 0
                },
                {
                    "sent": "This is completely standard and what you get is that the probability of Z is greater than its expected value, plus Z is bounded by E to the minus T squared divided by this guy V. OK.",
                    "label": 0
                },
                {
                    "sent": "So this is this is an extremely powerful concentration inequality that we can get in a few lines starting from.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithmics will have inequality, so for example we recover one of the classical concentration inequalities.",
                    "label": 0
                },
                {
                    "sent": "That we could call the bounded differences inequality, but in the literature you see it as Azuma's inequality and like the Army's inequality, this is calling back the army in an FC Barcelona T shirt.",
                    "label": 1
                },
                {
                    "sent": "And this this inequality extremely useful says that if.",
                    "label": 1
                },
                {
                    "sent": "Function is such that by changing one variable, the function cannot change by more than, let's say one.",
                    "label": 0
                },
                {
                    "sent": "Then you have exponential concentration.",
                    "label": 0
                },
                {
                    "sent": "Then you have an inequality which looks exactly like have things inequality right?",
                    "label": 0
                },
                {
                    "sent": "So for any function not only sums of independent variables, any function of independent variables the that has this kind of Lipschitz or this stability property, we have the same exponential concentration inequality that we had for some.",
                    "label": 0
                },
                {
                    "sent": "It could be no.",
                    "label": 0
                },
                {
                    "sent": "I don't think so.",
                    "label": 0
                },
                {
                    "sent": "Now here, here, here is the same guy, but here I require this deterministically.",
                    "label": 0
                },
                {
                    "sent": "Right, so in the Efron Stein inequality was something much better because because we only need it to bound the expected square of this thing here for the bounded differences inequality we need this hard constraint.",
                    "label": 0
                },
                {
                    "sent": "So, so here we really.",
                    "label": 0
                },
                {
                    "sent": "This inequality really assumes that changes are bounded deterministically, but you don't really need that.",
                    "label": 0
                },
                {
                    "sent": "But you have to sacrifice something if you want to pass from variance inequalities to exponentially inequalities.",
                    "label": 0
                },
                {
                    "sent": "It's obvious that you need some kind of extra conditions, this is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conveniently.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of variations of this.",
                    "label": 0
                },
                {
                    "sent": "The scheme of the proof is the same.",
                    "label": 0
                },
                {
                    "sent": "You start out with some kind of variations of the logarithmic Sobolev inequality.",
                    "label": 0
                },
                {
                    "sent": "You arrive at a differential inequality and then you solve it.",
                    "label": 0
                },
                {
                    "sent": "So for example, just going back to some of the examples I showed you, if Z is either the receive dimensional or the conditional Rademacher average.",
                    "label": 1
                },
                {
                    "sent": "And here the only the only thing we need is that the Efron Stein quantity is bounded by Z itself.",
                    "label": 0
                },
                {
                    "sent": "Then then you can get this.",
                    "label": 0
                },
                {
                    "sent": "This type of sub Gaussian inequalities, the deviations.",
                    "label": 0
                },
                {
                    "sent": "These are somehow the exact analogs of the variance inequalities we saw.",
                    "label": 0
                },
                {
                    "sent": "You get nice exponential now.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now in the remaining maybe 20 minutes, I want to show you some of the.",
                    "label": 0
                },
                {
                    "sent": "Some completely different, maybe applications of the same kind of ideas, and this has to do with influences of functions.",
                    "label": 0
                },
                {
                    "sent": "Influences of variables on set.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to the hypercube.",
                    "label": 0
                },
                {
                    "sent": "We have a subset in the end dimensional hypercube and think take a uniform distribution on the hypercube.",
                    "label": 0
                },
                {
                    "sent": "Again, this is not absolutely crucial.",
                    "label": 0
                },
                {
                    "sent": "Independence is crucial, but things are simpler if we assume uniformity.",
                    "label": 0
                },
                {
                    "sent": "The influence of a variable is.",
                    "label": 1
                },
                {
                    "sent": "Probability that the I variable determine X variable is pivotal?",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "It's pivotal that if we change the ice variable then and leave the others fixed, then you move in and out of the set.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So you can think about in social choice theory that you can think about we have voters and minus one is Hillary plus one is Obama and everyone casts his vote and we have a complicated voting scheme.",
                    "label": 0
                },
                {
                    "sent": "Not necessarily majority, but really complicated and.",
                    "label": 0
                },
                {
                    "sent": "American for example, and and so.",
                    "label": 0
                },
                {
                    "sent": "A vector is in a.",
                    "label": 0
                },
                {
                    "sent": "That means that that Obama wins.",
                    "label": 0
                },
                {
                    "sent": "Vector is not in a dot means Hillary wins.",
                    "label": 0
                },
                {
                    "sent": "So I'm I voter number I my influences the probability that I determine the outcome of the elections.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the influence of the variable XI.",
                    "label": 0
                },
                {
                    "sent": "Here is when I flip the bit.",
                    "label": 1
                },
                {
                    "sent": "And the total influence is just the sum of the individual influences the sum of the individual influences just the edge boundary normalized of this set, right?",
                    "label": 0
                },
                {
                    "sent": "It's just just you have this set and the number of ways number of edges this is connected to the outside.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some examples of dictator in a dictatorship.",
                    "label": 0
                },
                {
                    "sent": "We would have the first guy.",
                    "label": 0
                },
                {
                    "sent": "Determines the outcome.",
                    "label": 0
                },
                {
                    "sent": "There's one, there's one voter, and whatever he says is the outcome.",
                    "label": 0
                },
                {
                    "sent": "His influence is 1 or the other's influence is 0.",
                    "label": 0
                },
                {
                    "sent": "The total influence equals one.",
                    "label": 0
                },
                {
                    "sent": "The other extreme, we have the parity function when when we just we just count the number of votes and if it's even we are in the set.",
                    "label": 0
                },
                {
                    "sent": "If it's odd than Hillary wins and here everybody has a big influence influence one.",
                    "label": 0
                },
                {
                    "sent": "The total influence equals North.",
                    "label": 0
                },
                {
                    "sent": "Of course this is not not a very good.",
                    "label": 0
                },
                {
                    "sent": "I think you would get lots of legal problems if you.",
                    "label": 0
                },
                {
                    "sent": "If you apply this, this voting scheme majority is more reasonable.",
                    "label": 0
                },
                {
                    "sent": "In particular, voting scheme is usually it's good to have a monotone set right monotone, meaning that if you change your vote from from Obama to Hillary and Hillary won, then Hillary will still win.",
                    "label": 0
                },
                {
                    "sent": "These are these are monotone sets.",
                    "label": 0
                },
                {
                    "sent": "Majority is a monotone said that it's an important one.",
                    "label": 0
                },
                {
                    "sent": "Here the central Limit Theorem tells you that the influence of every every voter is about 1 / sqrt 1 /, 2 / \u03c0 N, and the total influences just root 2 / \u03c0 asymptotically.",
                    "label": 0
                },
                {
                    "sent": "Now the first time you can use the referenced an inequality for the indicator function of this guy and then what you get is that the variance.",
                    "label": 0
                },
                {
                    "sent": "This is just the variance of the outcome is bounded by you get exactly the total influence divided by 4.",
                    "label": 0
                },
                {
                    "sent": "So in particular Diophantine inequality tells you that dictatorship is the function with the smallest possible total influence, right?",
                    "label": 0
                },
                {
                    "sent": "Because if you only consider kind of Fair voting schemes when the probability of winning is equal for the two.",
                    "label": 0
                },
                {
                    "sent": "Voters, right?",
                    "label": 0
                },
                {
                    "sent": "So because this is 1/4, so 1/4 is less than less than the total influence divided by 4.",
                    "label": 0
                },
                {
                    "sent": "So the influence is always bigger than one.",
                    "label": 0
                },
                {
                    "sent": "Dictatorship is the one that minimizes the toll.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The influence of all guys.",
                    "label": 0
                },
                {
                    "sent": "Now here's an inequality that that Falcon submarine ski proved based on the logarithmic Sobolev inequality.",
                    "label": 1
                },
                {
                    "sent": "Once again, you take the logarithmic subway, have inequality on the hypercube, and in a few lines of calculus you get to this inequality.",
                    "label": 0
                },
                {
                    "sent": "Don't don't really look at it, I'm just telling you that there there isn't such an inequality which involves the variance.",
                    "label": 0
                },
                {
                    "sent": "B * 1 -- P is just the variance of the of the outcome and influences here, so you have the sum of the squared influences and the sum of the influences.",
                    "label": 0
                },
                {
                    "sent": "So one of the easy consequences of this inequality is a classical result of calculating linear.",
                    "label": 0
                },
                {
                    "sent": "That says that the maximum influence, no matter what the set is, the maximum influence is always bigger than the variance times log in over.",
                    "label": 0
                },
                {
                    "sent": "Why is this surprising?",
                    "label": 0
                },
                {
                    "sent": "Well, if all the influences are equal, and you know you want the voting scheme which has this property, if all the influences are equal, then the total influence is always bigger than login.",
                    "label": 0
                },
                {
                    "sent": "No matter what the voting scheme is, the total influence is always bigger than login.",
                    "label": 0
                },
                {
                    "sent": "By the way, this inequality and also this other color corollary, I'm going to tell you first were not proved using logarithmic syllable of inequalities.",
                    "label": 0
                },
                {
                    "sent": "They are proved using discrete Fourier analysis, and this logarithmic solving equations give an interesting alternative for studying sets of subsets of the of the cube.",
                    "label": 0
                },
                {
                    "sent": "Another corollary is that if the total influence is bounded is small, then a is.",
                    "label": 1
                },
                {
                    "sent": "Basically there's a bounded number of voters that determine the outcome in some sense.",
                    "label": 1
                },
                {
                    "sent": "In some approximate sense, these election schemes are called ahanta, so we have we have a small number of guys.",
                    "label": 0
                },
                {
                    "sent": "That determines the outcome and all the rest.",
                    "label": 1
                },
                {
                    "sent": "Don't don't count so that if you have any function whose total influence is small, then there's a small group of voters that determine the outcome.",
                    "label": 0
                },
                {
                    "sent": "And if all this influences are equal, then the influence is always big.",
                    "label": 0
                },
                {
                    "sent": "Big gross to Infinity and this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will be important in a few slides, so why are these?",
                    "label": 0
                },
                {
                    "sent": "Are these results?",
                    "label": 0
                },
                {
                    "sent": "Nice because we can study threshold phenomenon with them.",
                    "label": 0
                },
                {
                    "sent": "So what threshold final take now a subset of of the hypercube and assume that this is a monotone set.",
                    "label": 1
                },
                {
                    "sent": "Monotone meaning that the indicator indicator function of this set is a monotone function of all the all the coordinates.",
                    "label": 0
                },
                {
                    "sent": "Or if you wish by changing your vote from.",
                    "label": 0
                },
                {
                    "sent": "From the loser to winner, the winner will stay the same.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what monotonicity means.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And assume now that that voters vote independently and the vote.",
                    "label": 0
                },
                {
                    "sent": "Cast for Obama is everyone votes for Obama with probability P and to Hillary is probably 1 -- P. OK, so then you can ask what the probability of Obama winning is.",
                    "label": 0
                },
                {
                    "sent": "You can just write it up.",
                    "label": 0
                },
                {
                    "sent": "It's a nice, smooth nice.",
                    "label": 1
                },
                {
                    "sent": "It's a complicated polynomial of P, but it's a.",
                    "label": 1
                },
                {
                    "sent": "It's an increasing differential differentiable function.",
                    "label": 0
                },
                {
                    "sent": "It's increasing because because this set is monotone.",
                    "label": 0
                },
                {
                    "sent": "And it goes from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So we can define the inverse of this function.",
                    "label": 0
                },
                {
                    "sent": "Let's take a piece of AB, the probability for which the probability of Obama winning equals one.",
                    "label": 0
                },
                {
                    "sent": "In particular, the critical probability is 1/2.",
                    "label": 0
                },
                {
                    "sent": "Now this function grows from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "At some point it equals 1/2.",
                    "label": 0
                },
                {
                    "sent": "We want to know how fast it grows from close to 0 to close to one, and then this week we call this the threshold width.",
                    "label": 0
                },
                {
                    "sent": "So if epsilon is a small number then small positive number then you can you can ask.",
                    "label": 0
                },
                {
                    "sent": "What's the difference between this probability when these probabilities give Obama winning with a big big probability and Obama winning with a very small probability?",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is the threshold with.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are some examples.",
                    "label": 0
                },
                {
                    "sent": "If you have a dictatorship then of course the probability of the outcome being one is equals P. So we have just this linear function.",
                    "label": 0
                },
                {
                    "sent": "This is the critical value is 1/2 and transition is very very slow.",
                    "label": 0
                },
                {
                    "sent": "We go as slowly as possible from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "If we have a majority then the transition is very quick.",
                    "label": 1
                },
                {
                    "sent": "This is just a lot of large numbers.",
                    "label": 0
                },
                {
                    "sent": "If we are slightly below.",
                    "label": 0
                },
                {
                    "sent": "If the probability of voting to candidate number one is just is just slightly smaller than 1/2 then then candidate one will lose with a huge probably then if it's slightly bigger than he will win.",
                    "label": 0
                },
                {
                    "sent": "Right, so transition is very quick.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "You can see that by having things inequality for example that the the threshold interval is is of this order one over root, one over root N. So you can ask which are the sets for which we get this type of behavior.",
                    "label": 0
                },
                {
                    "sent": "Which for which type?",
                    "label": 0
                },
                {
                    "sent": "What are the properties of the monotone set that guarantee that we have this kind of it's in social choice theory?",
                    "label": 0
                },
                {
                    "sent": "This is called asymptotic aggregation of information.",
                    "label": 0
                },
                {
                    "sent": "We have a phase transition, a sharp threshold from very close to 0 to very close to 1.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "The key to this is a simple equality that's called Russo's lemma, that tells us that the derivative of this function of this increasing function, that the probability of this set as a function of P. Equals the total influence and I put a superscript here just to indicate that this of course depends on the total influence, so the derivative equals the total influence.",
                    "label": 0
                },
                {
                    "sent": "So this this is beautiful because it just tells us that if we want to study threshold phenomenon or we have to study these influences.",
                    "label": 0
                },
                {
                    "sent": "The total influence is big, but that means that we have a quick transition.",
                    "label": 0
                },
                {
                    "sent": "If the total influence is big around the critical value, then we have a quick transition.",
                    "label": 0
                },
                {
                    "sent": "If it's small then then we have a slow transition.",
                    "label": 0
                },
                {
                    "sent": "But but the result I showed you the calculated linear result.",
                    "label": 0
                },
                {
                    "sent": "It tells us that transition is always quick.",
                    "label": 0
                },
                {
                    "sent": "The influence is always big.",
                    "label": 0
                },
                {
                    "sent": "If we have a nice.",
                    "label": 0
                },
                {
                    "sent": "Fair voting scheme.",
                    "label": 0
                },
                {
                    "sent": "If the influence if the set is symmetric in this very weak sense that the influence of every voter, every variable equals then, then the transition is always goes to zero within.",
                    "label": 0
                },
                {
                    "sent": "Well, it goes to zero.",
                    "label": 0
                },
                {
                    "sent": "In general, very slowly it's like one over log, but it's kind of beautiful that it's always we always have this fast transition and becomes large.",
                    "label": 0
                },
                {
                    "sent": "The threshold with goes to 0.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you can prove using the other result, I showed you Rousseau's result that if the threshold is big, if the transition is slow, then we have a home to.",
                    "label": 1
                },
                {
                    "sent": "Then there's a small number of variables that determines the outcome.",
                    "label": 0
                },
                {
                    "sent": "For example, a dictatorship.",
                    "label": 0
                },
                {
                    "sent": "When we had this really slow transition.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I stop here, so I hope I give you more or less the flavor of some of the.",
                    "label": 0
                },
                {
                    "sent": "The ways these concentration inequalities work, I only showed you one small corner of this.",
                    "label": 0
                },
                {
                    "sent": "The arguments based on logarithmic Sobolev inequalities, but this is because because I like them so.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "About these Hammers like telegram, and everyone's time, the obvious nailing machine learning seems to be the cross validation problem.",
                    "label": 0
                },
                {
                    "sent": "And even when you know when I look at my steel and how you scribes, then he says the assumptions are similar in spirit to bootstrapping or bagging.",
                    "label": 0
                },
                {
                    "sent": "Yet somehow the problem is resisted analysis and yet question as to why these methods.",
                    "label": 0
                },
                {
                    "sent": "Drag.",
                    "label": 0
                },
                {
                    "sent": "Well, I one reason could be that that you don't have very good concentration.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "That we do.",
                    "label": 0
                },
                {
                    "sent": "At least in some cases, in perfect seems like it does.",
                    "label": 0
                },
                {
                    "sent": "So what's the gap in understanding what those cases are?",
                    "label": 0
                },
                {
                    "sent": "It seems obviously it's taking out appointing a new car.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "Well, in some cases I think you can.",
                    "label": 0
                },
                {
                    "sent": "You can say things right if you have stability in some sense you can formulate condition.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's a hard problem.",
                    "label": 0
                },
                {
                    "sent": "I agree.",
                    "label": 0
                },
                {
                    "sent": "I maybe some other tools should be developed, I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I have money riding on one aspect of this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "No general.",
                    "label": 0
                },
                {
                    "sent": "Martin is in general.",
                    "label": 0
                },
                {
                    "sent": "I don't think that you can say things beautiful like this.",
                    "label": 0
                },
                {
                    "sent": "For for things like.",
                    "label": 0
                },
                {
                    "sent": "Rapidly mixing Markov chains that there exist results of this flavor becoming more from from.",
                    "label": 0
                },
                {
                    "sent": "A method that I called information theoretic methods that are based on couplings and things like that.",
                    "label": 0
                },
                {
                    "sent": "So for Markov chains I know there are some nice results, but it's very difficult to go beyond go significantly significantly beyond independence.",
                    "label": 0
                },
                {
                    "sent": "Somehow independence is really crucial.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Random variables.",
                    "label": 0
                },
                {
                    "sent": "About abroad.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Friday yeah, so in many cases there's no Gaussian limit in these, so the question was whether whether we can central limit type of results.",
                    "label": 0
                },
                {
                    "sent": "But these inequalities are so general that in many cases we don't even have know if there's a limit distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do something like that, you always need more general, more general assumptions.",
                    "label": 0
                },
                {
                    "sent": "So one of one of one of the general tools is of course times method, the same Steiner Efron Stein inequality.",
                    "label": 0
                },
                {
                    "sent": "And there exists.",
                    "label": 0
                },
                {
                    "sent": "There are some new results by Saurav Chatterjee in Berkeley who who can show concentration inequalities in that flavor, but that's the best I know of.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "There are generalizations about things inequality for the case of the variables are not, ideally terrible.",
                    "label": 0
                },
                {
                    "sent": "Depends on the limited number of other variables that form a graph, yeah?",
                    "label": 0
                },
                {
                    "sent": "Our average is yes, there are many qualities like this for general functions that would be a beautiful thing to have.",
                    "label": 0
                },
                {
                    "sent": "If you have an independent dependence graph with small degree, that would be beautiful to have some concentration, some powerful concentration inequalities.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}