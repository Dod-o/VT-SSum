{
    "id": "mz6l2xhrsptmjuhlnviilv73y3s5onfh",
    "title": "How Random is a Coin Toss? Bayesian Inference and the Symbolic Dynamics of Deterministic Chaos",
    "info": {
        "author": [
            "Christopher Strelioff, Center for Complex Systems Research and Department of Physics, University of Illinois at Urbana-Champaign"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/dsb06_strelioff_hrctb/",
    "segmentation": [
        [
            "To be here.",
            "This is my first time at NIPS, so I've been learning a lot about this community.",
            "What I'll be talking about is a departure from stochastic differential equations, but I hope that I can convince you that there's some connections to what's already been talked about, and also what's being discussed in general around NIPS.",
            "So what is the problem that I'm into?"
        ],
        [
            "Din I'm interested in the possibility that randomness might not just be due to stochastic elements.",
            "It might actually be.",
            "When you see something that looks random, there might actually be instead a low dimensional deterministic system which has a chaotic attractor and this will display properties that look.",
            "Random.",
            "Now when you want to consider this type of data, you have to be careful about the measurement process.",
            "And I mean this in a sort of abstract way, so I'll be more specific about that, but it's very sensitive to this measurement process and it turns out that this will be both a blessing and a curse, because it will mean that we can we measure correctly will get very powerful representations of the data.",
            "If you measure incorrectly, we can get very bad representations of the data.",
            "OK. And so for."
        ],
        [
            "Our model of measurement.",
            "I'm going to talk about things that are related to a field in dynamical systems called symbolic dynamics, which is the idea that you can take things that have a continuous state space and project them onto a finite set of symbols.",
            "In some sense, we can think of this as a reasonable thing to do that any instrument is going to measure with finite precision, and we should understand how this is going to work.",
            "So as I said, symbolic dynamics is a core screen view, and I'll use this as the framework to discuss instrument design.",
            "And so when we do this, there's going to be two things we're interested in.",
            "We're going to take this continuous state data and make it discreet."
        ],
        [
            "ETA and will be 2 sort of optimizations that go on here.",
            "The first will be the instrument design, which is how do we do this projection from continuous state space onto a discrete or finite alphabet?",
            "And we want to do this in such a way that we get the most information from each measurement.",
            "The model inference is once we have our sequence of a finite alphabet, we actually want to model this.",
            "So in this case I'll be doing finite Markov chain too.",
            "Analyze and model the resulting sequence of symbols, and in this case what I'll be doing is minimizing the entropy rate, making a search for determinism within the resulting symbols, and so one of the big themes to take away from the talk.",
            "If you don't remember anything else, is these two things that you have to optimize.",
            "Each of these individually, you have to get the most information from each measurement and then produce a model that is the simplest that you can.",
            "So might.",
            "Model data source that I'll be doing is a 1 dimensional chaotic map and so this is a simple map."
        ],
        [
            "Think where F is going to be.",
            "Some nonlinear function of the state at a previous time, and I will also consider when we get to the experiment at the end, adding stochastic noise so will have both the randomness of deterministic chaos plus stochastic elements, and I think probably in the long run this is the most realistic view of the world.",
            "Is that you're going to have some nonlinearities that are deterministic, but you're also going to have stochastic elements to it.",
            "But first I want to motivate why this is a relevant example at all, and I want to show how you can connect systems of ODS to 1 dimensional map.",
            "So this is sort of a whirlwind overview of deterministic chaos, so I'm going to take a different example.",
            "Then we saw earlier, which was a little run."
        ],
        [
            "I'm going to talk about the Rossler attractor that's described by this set of ODS.",
            "Fairly innocent looking, except for this one nonlinear term here, and there's three parameters AB&C depending on what you set those parameters, you can get constant time dynamics.",
            "You can get oscillatory behavior and then you can get times where all the periodic orbits become unstable and you get actually random looking behavior, and so typical examples of this if you set the parameters to these values.",
            "You'll get chaotic dynamics.",
            "So what does this actually look like?",
            "This."
        ],
        [
            "The Representative time series.",
            "X is in red.",
            "Why is in green and blue is Z and what you can see is the red and green look.",
            "Basically like auditory oscillatory behavior.",
            "But notice that the amplitudes aren't constant in time, so we have a bunch of different periodic behavior is going on in here.",
            "The blue is has a sort of spiking or threshold behavior.",
            "It only activates at certain points.",
            "And where does the randomness come from?",
            "Well, it's this sensitivity to perturbation or small changes in the initial condition, so the previous slide looked fairly simple.",
            "But in this case."
        ],
        [
            "I take a simulation an I perturb the initial conditions a little bit.",
            "I get two different trajectories overtime.",
            "So in a sense I can get randomness without any stochastic elements in it at all, because if I have a well defined, you can imagine a Gaussian distribution here that's very well defined by the time I get out here, my Gaussian has spread to a very large variance, so without any stochastic elements we can already have this kind of behavior.",
            "But there's also."
        ],
        [
            "Organization to these things so chaotic attractors have a geometry to them.",
            "And So what this is is I've removed time and XY and Z.",
            "We we actually put this in the state space.",
            "And so I've plotted X&Y.",
            "But time actually would be moving around in circles this way, and So what happens is you wind out from the center and once you get a certain way amount from the.",
            "The Center the Z direction will be activated and will actually come out in this direction and then come back down.",
            "The thing to notice here is that the two original trajectory and a perturbed trajectory still have the same global shape, so this attractor geometry is a stable thing, even though this sensitivity and randomness to particular trajectories.",
            "So we're going to exploit this in a typical way that you do that is too."
        ],
        [
            "For example, consider the sequence of Maxima in one of the variables.",
            "So we're just going to take X, and we're going to look at that every time we hit a maximum.",
            "And what would be interested in is the sequence of how this maximum goes to this.",
            "This one goes to this.",
            "This one goes to this.",
            "We can build up a return map.",
            "And here we have our one dimensional map."
        ],
        [
            "Alright, so this actually shows that if one cycle the maximum was at 6, the next I would expect the maximum to be around 10, so we described.",
            "Jumps in time.",
            "And how these Maxima are going to iterate?",
            "So that's a brief introduction to how we get from a system of odies to a 1 dimensional map, and I'll now talk about that, so the specifics of designing an instrument and so this is."
        ],
        [
            "Going to start out with assuming that there's no noise, so the stochastic element isn't present.",
            "Symbolic dynamics has been discussed in that framework and the basic idea is that you take a discrete time map, but there's a continuous state space and this map.",
            "Goes from the state space to itself and you consider a partition of the state space into a finite number of non overlapping regions.",
            "Can you do this in such a way that when you take the union of those regions, you get back your state space?",
            "And of course you don't want them to have any intersection, so they're not overlapping.",
            "Now it's well known how to do this for one dimensional Maps, we choose the decision points or the boundaries between our intervals to be at the critical points of the map.",
            "So an example of a good partition here is this choice, so everything."
        ],
        [
            "In my time series that would be between 0 and 1/2 would be labeled with the zero.",
            "Everything between 1/2 and one would be labeled with a one.",
            "And the reason that this is a sensible thing is that this basically looks at monotone branches of the map.",
            "So if this were a mapping that had multiple humps in it, you would choose each of those humps as a place to put your partition, and this will actually preserve the topological properties of the dynamics.",
            "So this is what I mean when it's very sensitive.",
            "If I choose to move this decision point little bit to the left, that would be a bad partition wouldn't preserve the topological properties, and we get a less representative projection from our continuous state space onto a sequence of just symbols."
        ],
        [
            "So part of my interest in this is what if we don't know F of X?",
            "What if we just have data and it's noisy?",
            "We still apply some of this stuff, so one way we can think about this is that you take.",
            "Your time series from your map.",
            "And you choose some partition.",
            "I'm not going to be specific about what it is or how many symbols.",
            "That will create a particular symbol sequence.",
            "And for noise free dynamics, so no stochastic elements here.",
            "There's a theorem that actually tells us that if we choose the correct.",
            "Partition, so this is the entropy rate given a particular partition of particular projection onto symbols.",
            "If we maximize that over all partitions, we will actually get the real entropy rate of the underlying continuous system and will use this as a way to analyze later how well we're doing.",
            "We can also relate this to known property."
        ],
        [
            "As of the dynamical system, there is an identity which tells us that the entropy rate should also be equal to the sum of the positive Lyapunov exponents, and this is something that you can calculate from a trajectory in the map.",
            "You basically take the absolute value of the slope of the map over the trajectory and all this is telling us is if you have two new nearby points.",
            "If the Lyapunov exponent is positive, those two nearby points will go away from each other overtime.",
            "If it's negative towards each other overtime.",
            "And so the equivalent here is that if you have a positive Lyapunov exponent, you'll have a nonzero entropy rate.",
            "If you have a negative Lyapunov exponent, you'll have zero entropy rate.",
            "So that's the connection to what's familiar probably to this community.",
            "So that."
        ],
        [
            "The discussion of how we actually go from continuous state space to a sequence of zeros and ones.",
            "For example, the next step is well.",
            "Now we have this symbolic data.",
            "We need to model it, for example, to estimate an entropy rate.",
            "And I will be doing Bayes theorem.",
            "Bayesian analysis of this typical thing we want to find a posterior density given a likelihood of prior and evidence an.",
            "In this problem.",
            "All this turns out to be analytic.",
            "This is done very much in the style of.",
            "Pier Baldis bioinformatics book on machine learning or even the guys look.",
            "And so the model basically says that now that I have my sequence of symbols, I'm going to predict the next symbol given the previous cases."
        ],
        [
            "Balls.",
            "And.",
            "This count up here is just the number of times I've seen words of length K plus one in my data stream, and this assumes two things, finite memory.",
            "So only K symbols to predict the next one an stationarity.",
            "So in principle for Chaotic dynamics, we can say it's going to be stationary in the sense that there's a stable chaotic attractor, but and finite memory may not be true, so we might actually be doing some out of class modeling here.",
            "Having attractive what if you don't have an attractor?",
            "Well, that's part of the assumption of what I'm doing here.",
            "Yeah, yeah.",
            "So prior here in the."
        ],
        [
            "Case is an analytic density, directly distribution, one for each of the histories of length K, and we have a set of hyperparameters here.",
            "Which we can set and in practice what I'm going to do is setting these all equal to 1, which gives me a uniform prior over each of the parameters of the of the Markov chain.",
            "The evidence which is."
        ],
        [
            "Is the normalization term in.",
            "Bayes theorem comes out to be basically just the normalization factor from the prior and what will become the inverse of the normalization factor from the posterior.",
            "But I want to point out that this is important in terms of the model pair comparison that will come later and also becomes a tool that we use to estimate the entropy rate because we can relate this to a partition function from statistical physics."
        ],
        [
            "And so finally we combined all these things and we get back.",
            "A richly distribution also.",
            "And this is because we chose the conjugate prior for the problem directly prior.",
            "Gives me address.",
            "Posterior and so you can see that it's the same form.",
            "But now I just have counts plus the parameters from the prior.",
            "Given this, in a certain sense, this is the Bayesian answer to what my parameters are.",
            "Gives me a distribution over them, but you can take means with this you can sample from it whatever you want to do.",
            "And so we know how to forgive and K and further parameters of the model.",
            "We might want to also select which of the orders K do we need, just the previous symbol to predict the next the previous two or three, and the way to do that is to consider a set of.",
            "Orders and use another variation on Bayes theorem."
        ],
        [
            "Where now the evidence term?",
            "From the estimation of parameters is right.",
            "Here we have a prior over orders and what I'm going to choose here is an exponential penalty for the number of parameters in here, so I'm going to force simpler models.",
            "Data has to prove that it really needs more memory to predict.",
            "So how do I actually estimate these entropy rates?"
        ],
        [
            "This is where the statistical physics comes in is that we can write down a partition function which is basically equal to the evidence, so it's equal to this integral that I would need to calculate the evidence, and I can rewrite this term.",
            "Here the joint distribution over the data and the parameters in this form that's motivated by type theory, for instance from.",
            "Covering Thomas is an example in information theory and what I have is.",
            "Terms here, where Q is my distribution that's defined by the posterior means and P is the actual parameters from the posterior distribution, and I have an entropy rate and then I have a KL divergent, but in this case it's slightly different because these are now conditional probabilities, so it's a relative conditional distance rather than the one that we are probably more familiar with.",
            "For a physicist, this looks exactly like an energy and a typical way of finding an energy is to define.",
            "Partition function which I do by plugging this up in here if I want to get an average of this quantity, I can take a derivative with respect to beta, which will bring this term down and I'll actually be able to get an average of this quantity with respect to the posterior distribution.",
            "And so if we do this.",
            "We can actually calculate these things out."
        ],
        [
            "Literally because of the forms known.",
            "So this is the derivative is talking about.",
            "We take a derivative of the log of the partition function and that will give our expectation of this quantity.",
            "The funny one over log 2 is because I defined my information theory quantities in terms of base two, so they're all in bits and this quantity is rather horrible looking, but it's fairly.",
            "Intuitive, these functions right here that I gammas are something that you get by taking.",
            "If you have a log of a gamma function, you take derivatives of that, you'll get digamma functions and these are something that you find in most numerical tool boxes in programming languages, Python or whatever, and so you can also expand this out and show that this quantity asymptotically will be equal to the entropy rate plus a term that goes is 1 / L, which is this distance here.",
            "So what we expect to happen when we actually get more and more data here.",
            "Is that this term is going to go to zero the distance and we're going to be left with an expectation of the entropy rate.",
            "OK, so."
        ],
        [
            "Now we're going to actually apply this.",
            "What can we learn by taking these two tools and putting them together?",
            "an A reminder of what we're looking at?",
            "We define an instrument which is given defined particular partition.",
            "We want to maximize that entropy rate.",
            "We also want to infer Markov chain model of the core screen data, and we want to minimize the entropy rate there.",
            "What did I use as the data?",
            "So here I took the logistic map, which is a typical single hump map like what we saw before, generated 10,000 data points.",
            "In this case I actually included."
        ],
        [
            "Stochastic perturbation so there is a noise level where we have a standard deviation of 10 to minus three.",
            "And so I have this single time series.",
            "Then I'm going to choose a decision point which defines a partition.",
            "So I go through my time series, everything between zero and D. I'm going to label with zero everything between D and one.",
            "I'm going to label with the one I'm going to take that symbolic sequence.",
            "Estimate parameters.",
            "Do model selection, get an entropy rate.",
            "Change the D. Do the whole thing again, change the, do the whole thing again.",
            "So we're going to basically test out each one of our instruments.",
            "What do we get well?"
        ],
        [
            "Now here I have plotted the decision point where I'm placing the the.",
            "Boundary between what's zero and what's one, and here I get the resulting entropy rate estimated using that average quantity, and so one thing you can see is that this is actually fairly non is quite nontrivial.",
            "As a function of the decision point.",
            "So here we maximize that according to the theorems from noise free symbolic dynamics.",
            "This should be where we want to place the partition to preserve all the topological properties of the underlying dynamics, and this red dotted line is actually the Lyapunov exponent that we calculate from the known map, so you wouldn't be able to calculate this red dotted line without knowledge of the map and so both of these agree here, and so in a certain sense, even though there's not theorems for symbolic dynamics when there's noise.",
            "We're showing that this is a consistent thing, that it does extrapolate well when you do add the noise.",
            "Now the interesting thing that to my knowledge it hasn't been done before.",
            "We can also look at how complicated the resulting model was given a particular partition.",
            "So the things that are interesting here is that of course, when my decision point is at 0.",
            "Or is that one?",
            "I'm looking at model orders from 1:00 to 8:00.",
            "I get very very simple models.",
            "I select the simplest one possible, but there's two other places where I get very, very simple models.",
            "One is at 1/2, which is where I actually want to place the thing.",
            "So I had an instrument that gave me the most information, but produced the simplest model.",
            "And I have this sort of mysterious one here, but if you look at it, it turns out that this is a pre image of 1/2.",
            "So if you map this backwards under the map, this actually will iterate to this point under 4 dynamics.",
            "So it's actually related to the appropriate thing we would choose this partition point because it maximizes entropy rate.",
            "And then the last thing."
        ],
        [
            "Is we can look at this in opposite way which is.",
            "That we want to minimize entropy of the resulting models, and So what I've done here is I have three different decision points that I've picked out point 2.3, point 4.5 and then on this axis I have the order of the particular Markov chain and this is a comment on how the model selection works.",
            "So for example 4.5, which was the correct decision point?",
            "I get an interview rate that at different orders doesn't change.",
            "It's a model selection, will just choose out the smallest model with the smallest entry rate, so it's select this one.",
            "This is the probability of that model, so.",
            "Basically has probability one at order one and probability 0 for all higher orders.",
            "For something where we have an incorrect decision point."
        ],
        [
            "Like .2 that actually has a minimum in the entropy rate estimated at different orders, and So what model selection does there is it finds this minimum.",
            "And once it's no longer going to get any reducing the apparent complexity, it will select out this model at that point, and the probability of that becomes one.",
            "So in conclusion, I hope that I've convinced you that randomness can be due to low dimensional.",
            "Chaos, not just stochastic properties, that symbolic dynamics can be a useful tool to analyze this data.",
            "And the two main takeaway themes are instruments should be designed in these types of systems to maximize the entropy rate.",
            "So get the most information from the observations and for modeling ferments, model inference we want to do exactly the opposite.",
            "Minimize entropy rate, and so the idea here is that you can combine this into one step, right?",
            "If you just did the model selection and made the partition the decision point part of the model selection, you get trivial answers.",
            "So you have to optimize over the two different parts to get the correct.",
            "Answer.",
            "Ticket.",
            "Yeah.",
            "Prior on the.",
            "Oh yeah, you don't actually, so I mean, Bayesian methods do have a sort of natural Occam's razor.",
            "Yes, in practice I've found that not in this context, but in other contexts that sometimes there tends to be sort of overfitting problems at very small data points is probably wouldn't be a problem here, and I could probably just say uniform overall model orders, and I still get the same results.",
            "In some cases you will get a sort of a slight overfitting at very small data points, and that's why you would potentially want to choose that kind of penalty term.",
            "Look when the noise is.",
            "Yeah, um.",
            "This actually I don't know.",
            "My guess is that because this is just preliminary or basically everything I know at this point is presented here, but my guess is that it's going to go up basically as a function of the stochastic term that's added.",
            "In fact, the coauthor here, Jim Crutchfield, has some really nice papers from the early 80s which are symbolic dynamics and noisy chaos.",
            "And he actually talks about these kinds of things using ideas from like renormalization group, and how how these things scale as a function of the noise that's applied.",
            "So I would I would look there as a good good place."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be here.",
                    "label": 0
                },
                {
                    "sent": "This is my first time at NIPS, so I've been learning a lot about this community.",
                    "label": 0
                },
                {
                    "sent": "What I'll be talking about is a departure from stochastic differential equations, but I hope that I can convince you that there's some connections to what's already been talked about, and also what's being discussed in general around NIPS.",
                    "label": 0
                },
                {
                    "sent": "So what is the problem that I'm into?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Din I'm interested in the possibility that randomness might not just be due to stochastic elements.",
                    "label": 0
                },
                {
                    "sent": "It might actually be.",
                    "label": 0
                },
                {
                    "sent": "When you see something that looks random, there might actually be instead a low dimensional deterministic system which has a chaotic attractor and this will display properties that look.",
                    "label": 1
                },
                {
                    "sent": "Random.",
                    "label": 1
                },
                {
                    "sent": "Now when you want to consider this type of data, you have to be careful about the measurement process.",
                    "label": 0
                },
                {
                    "sent": "And I mean this in a sort of abstract way, so I'll be more specific about that, but it's very sensitive to this measurement process and it turns out that this will be both a blessing and a curse, because it will mean that we can we measure correctly will get very powerful representations of the data.",
                    "label": 1
                },
                {
                    "sent": "If you measure incorrectly, we can get very bad representations of the data.",
                    "label": 0
                },
                {
                    "sent": "OK. And so for.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our model of measurement.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about things that are related to a field in dynamical systems called symbolic dynamics, which is the idea that you can take things that have a continuous state space and project them onto a finite set of symbols.",
                    "label": 0
                },
                {
                    "sent": "In some sense, we can think of this as a reasonable thing to do that any instrument is going to measure with finite precision, and we should understand how this is going to work.",
                    "label": 0
                },
                {
                    "sent": "So as I said, symbolic dynamics is a core screen view, and I'll use this as the framework to discuss instrument design.",
                    "label": 1
                },
                {
                    "sent": "And so when we do this, there's going to be two things we're interested in.",
                    "label": 0
                },
                {
                    "sent": "We're going to take this continuous state data and make it discreet.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "ETA and will be 2 sort of optimizations that go on here.",
                    "label": 0
                },
                {
                    "sent": "The first will be the instrument design, which is how do we do this projection from continuous state space onto a discrete or finite alphabet?",
                    "label": 1
                },
                {
                    "sent": "And we want to do this in such a way that we get the most information from each measurement.",
                    "label": 0
                },
                {
                    "sent": "The model inference is once we have our sequence of a finite alphabet, we actually want to model this.",
                    "label": 0
                },
                {
                    "sent": "So in this case I'll be doing finite Markov chain too.",
                    "label": 0
                },
                {
                    "sent": "Analyze and model the resulting sequence of symbols, and in this case what I'll be doing is minimizing the entropy rate, making a search for determinism within the resulting symbols, and so one of the big themes to take away from the talk.",
                    "label": 1
                },
                {
                    "sent": "If you don't remember anything else, is these two things that you have to optimize.",
                    "label": 1
                },
                {
                    "sent": "Each of these individually, you have to get the most information from each measurement and then produce a model that is the simplest that you can.",
                    "label": 0
                },
                {
                    "sent": "So might.",
                    "label": 0
                },
                {
                    "sent": "Model data source that I'll be doing is a 1 dimensional chaotic map and so this is a simple map.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think where F is going to be.",
                    "label": 0
                },
                {
                    "sent": "Some nonlinear function of the state at a previous time, and I will also consider when we get to the experiment at the end, adding stochastic noise so will have both the randomness of deterministic chaos plus stochastic elements, and I think probably in the long run this is the most realistic view of the world.",
                    "label": 0
                },
                {
                    "sent": "Is that you're going to have some nonlinearities that are deterministic, but you're also going to have stochastic elements to it.",
                    "label": 0
                },
                {
                    "sent": "But first I want to motivate why this is a relevant example at all, and I want to show how you can connect systems of ODS to 1 dimensional map.",
                    "label": 1
                },
                {
                    "sent": "So this is sort of a whirlwind overview of deterministic chaos, so I'm going to take a different example.",
                    "label": 0
                },
                {
                    "sent": "Then we saw earlier, which was a little run.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about the Rossler attractor that's described by this set of ODS.",
                    "label": 0
                },
                {
                    "sent": "Fairly innocent looking, except for this one nonlinear term here, and there's three parameters AB&C depending on what you set those parameters, you can get constant time dynamics.",
                    "label": 0
                },
                {
                    "sent": "You can get oscillatory behavior and then you can get times where all the periodic orbits become unstable and you get actually random looking behavior, and so typical examples of this if you set the parameters to these values.",
                    "label": 0
                },
                {
                    "sent": "You'll get chaotic dynamics.",
                    "label": 0
                },
                {
                    "sent": "So what does this actually look like?",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Representative time series.",
                    "label": 0
                },
                {
                    "sent": "X is in red.",
                    "label": 0
                },
                {
                    "sent": "Why is in green and blue is Z and what you can see is the red and green look.",
                    "label": 0
                },
                {
                    "sent": "Basically like auditory oscillatory behavior.",
                    "label": 0
                },
                {
                    "sent": "But notice that the amplitudes aren't constant in time, so we have a bunch of different periodic behavior is going on in here.",
                    "label": 0
                },
                {
                    "sent": "The blue is has a sort of spiking or threshold behavior.",
                    "label": 0
                },
                {
                    "sent": "It only activates at certain points.",
                    "label": 0
                },
                {
                    "sent": "And where does the randomness come from?",
                    "label": 0
                },
                {
                    "sent": "Well, it's this sensitivity to perturbation or small changes in the initial condition, so the previous slide looked fairly simple.",
                    "label": 0
                },
                {
                    "sent": "But in this case.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I take a simulation an I perturb the initial conditions a little bit.",
                    "label": 0
                },
                {
                    "sent": "I get two different trajectories overtime.",
                    "label": 0
                },
                {
                    "sent": "So in a sense I can get randomness without any stochastic elements in it at all, because if I have a well defined, you can imagine a Gaussian distribution here that's very well defined by the time I get out here, my Gaussian has spread to a very large variance, so without any stochastic elements we can already have this kind of behavior.",
                    "label": 0
                },
                {
                    "sent": "But there's also.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Organization to these things so chaotic attractors have a geometry to them.",
                    "label": 0
                },
                {
                    "sent": "And So what this is is I've removed time and XY and Z.",
                    "label": 0
                },
                {
                    "sent": "We we actually put this in the state space.",
                    "label": 0
                },
                {
                    "sent": "And so I've plotted X&Y.",
                    "label": 0
                },
                {
                    "sent": "But time actually would be moving around in circles this way, and So what happens is you wind out from the center and once you get a certain way amount from the.",
                    "label": 0
                },
                {
                    "sent": "The Center the Z direction will be activated and will actually come out in this direction and then come back down.",
                    "label": 0
                },
                {
                    "sent": "The thing to notice here is that the two original trajectory and a perturbed trajectory still have the same global shape, so this attractor geometry is a stable thing, even though this sensitivity and randomness to particular trajectories.",
                    "label": 0
                },
                {
                    "sent": "So we're going to exploit this in a typical way that you do that is too.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, consider the sequence of Maxima in one of the variables.",
                    "label": 0
                },
                {
                    "sent": "So we're just going to take X, and we're going to look at that every time we hit a maximum.",
                    "label": 0
                },
                {
                    "sent": "And what would be interested in is the sequence of how this maximum goes to this.",
                    "label": 0
                },
                {
                    "sent": "This one goes to this.",
                    "label": 0
                },
                {
                    "sent": "This one goes to this.",
                    "label": 0
                },
                {
                    "sent": "We can build up a return map.",
                    "label": 0
                },
                {
                    "sent": "And here we have our one dimensional map.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this actually shows that if one cycle the maximum was at 6, the next I would expect the maximum to be around 10, so we described.",
                    "label": 0
                },
                {
                    "sent": "Jumps in time.",
                    "label": 0
                },
                {
                    "sent": "And how these Maxima are going to iterate?",
                    "label": 0
                },
                {
                    "sent": "So that's a brief introduction to how we get from a system of odies to a 1 dimensional map, and I'll now talk about that, so the specifics of designing an instrument and so this is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to start out with assuming that there's no noise, so the stochastic element isn't present.",
                    "label": 0
                },
                {
                    "sent": "Symbolic dynamics has been discussed in that framework and the basic idea is that you take a discrete time map, but there's a continuous state space and this map.",
                    "label": 1
                },
                {
                    "sent": "Goes from the state space to itself and you consider a partition of the state space into a finite number of non overlapping regions.",
                    "label": 1
                },
                {
                    "sent": "Can you do this in such a way that when you take the union of those regions, you get back your state space?",
                    "label": 0
                },
                {
                    "sent": "And of course you don't want them to have any intersection, so they're not overlapping.",
                    "label": 0
                },
                {
                    "sent": "Now it's well known how to do this for one dimensional Maps, we choose the decision points or the boundaries between our intervals to be at the critical points of the map.",
                    "label": 0
                },
                {
                    "sent": "So an example of a good partition here is this choice, so everything.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In my time series that would be between 0 and 1/2 would be labeled with the zero.",
                    "label": 0
                },
                {
                    "sent": "Everything between 1/2 and one would be labeled with a one.",
                    "label": 0
                },
                {
                    "sent": "And the reason that this is a sensible thing is that this basically looks at monotone branches of the map.",
                    "label": 0
                },
                {
                    "sent": "So if this were a mapping that had multiple humps in it, you would choose each of those humps as a place to put your partition, and this will actually preserve the topological properties of the dynamics.",
                    "label": 0
                },
                {
                    "sent": "So this is what I mean when it's very sensitive.",
                    "label": 0
                },
                {
                    "sent": "If I choose to move this decision point little bit to the left, that would be a bad partition wouldn't preserve the topological properties, and we get a less representative projection from our continuous state space onto a sequence of just symbols.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So part of my interest in this is what if we don't know F of X?",
                    "label": 1
                },
                {
                    "sent": "What if we just have data and it's noisy?",
                    "label": 0
                },
                {
                    "sent": "We still apply some of this stuff, so one way we can think about this is that you take.",
                    "label": 0
                },
                {
                    "sent": "Your time series from your map.",
                    "label": 0
                },
                {
                    "sent": "And you choose some partition.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to be specific about what it is or how many symbols.",
                    "label": 0
                },
                {
                    "sent": "That will create a particular symbol sequence.",
                    "label": 0
                },
                {
                    "sent": "And for noise free dynamics, so no stochastic elements here.",
                    "label": 0
                },
                {
                    "sent": "There's a theorem that actually tells us that if we choose the correct.",
                    "label": 1
                },
                {
                    "sent": "Partition, so this is the entropy rate given a particular partition of particular projection onto symbols.",
                    "label": 0
                },
                {
                    "sent": "If we maximize that over all partitions, we will actually get the real entropy rate of the underlying continuous system and will use this as a way to analyze later how well we're doing.",
                    "label": 0
                },
                {
                    "sent": "We can also relate this to known property.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As of the dynamical system, there is an identity which tells us that the entropy rate should also be equal to the sum of the positive Lyapunov exponents, and this is something that you can calculate from a trajectory in the map.",
                    "label": 0
                },
                {
                    "sent": "You basically take the absolute value of the slope of the map over the trajectory and all this is telling us is if you have two new nearby points.",
                    "label": 0
                },
                {
                    "sent": "If the Lyapunov exponent is positive, those two nearby points will go away from each other overtime.",
                    "label": 0
                },
                {
                    "sent": "If it's negative towards each other overtime.",
                    "label": 0
                },
                {
                    "sent": "And so the equivalent here is that if you have a positive Lyapunov exponent, you'll have a nonzero entropy rate.",
                    "label": 0
                },
                {
                    "sent": "If you have a negative Lyapunov exponent, you'll have zero entropy rate.",
                    "label": 0
                },
                {
                    "sent": "So that's the connection to what's familiar probably to this community.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The discussion of how we actually go from continuous state space to a sequence of zeros and ones.",
                    "label": 0
                },
                {
                    "sent": "For example, the next step is well.",
                    "label": 0
                },
                {
                    "sent": "Now we have this symbolic data.",
                    "label": 1
                },
                {
                    "sent": "We need to model it, for example, to estimate an entropy rate.",
                    "label": 1
                },
                {
                    "sent": "And I will be doing Bayes theorem.",
                    "label": 0
                },
                {
                    "sent": "Bayesian analysis of this typical thing we want to find a posterior density given a likelihood of prior and evidence an.",
                    "label": 0
                },
                {
                    "sent": "In this problem.",
                    "label": 0
                },
                {
                    "sent": "All this turns out to be analytic.",
                    "label": 0
                },
                {
                    "sent": "This is done very much in the style of.",
                    "label": 0
                },
                {
                    "sent": "Pier Baldis bioinformatics book on machine learning or even the guys look.",
                    "label": 0
                },
                {
                    "sent": "And so the model basically says that now that I have my sequence of symbols, I'm going to predict the next symbol given the previous cases.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Balls.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This count up here is just the number of times I've seen words of length K plus one in my data stream, and this assumes two things, finite memory.",
                    "label": 0
                },
                {
                    "sent": "So only K symbols to predict the next one an stationarity.",
                    "label": 0
                },
                {
                    "sent": "So in principle for Chaotic dynamics, we can say it's going to be stationary in the sense that there's a stable chaotic attractor, but and finite memory may not be true, so we might actually be doing some out of class modeling here.",
                    "label": 0
                },
                {
                    "sent": "Having attractive what if you don't have an attractor?",
                    "label": 0
                },
                {
                    "sent": "Well, that's part of the assumption of what I'm doing here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So prior here in the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case is an analytic density, directly distribution, one for each of the histories of length K, and we have a set of hyperparameters here.",
                    "label": 0
                },
                {
                    "sent": "Which we can set and in practice what I'm going to do is setting these all equal to 1, which gives me a uniform prior over each of the parameters of the of the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "The evidence which is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the normalization term in.",
                    "label": 0
                },
                {
                    "sent": "Bayes theorem comes out to be basically just the normalization factor from the prior and what will become the inverse of the normalization factor from the posterior.",
                    "label": 0
                },
                {
                    "sent": "But I want to point out that this is important in terms of the model pair comparison that will come later and also becomes a tool that we use to estimate the entropy rate because we can relate this to a partition function from statistical physics.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so finally we combined all these things and we get back.",
                    "label": 0
                },
                {
                    "sent": "A richly distribution also.",
                    "label": 0
                },
                {
                    "sent": "And this is because we chose the conjugate prior for the problem directly prior.",
                    "label": 0
                },
                {
                    "sent": "Gives me address.",
                    "label": 0
                },
                {
                    "sent": "Posterior and so you can see that it's the same form.",
                    "label": 0
                },
                {
                    "sent": "But now I just have counts plus the parameters from the prior.",
                    "label": 0
                },
                {
                    "sent": "Given this, in a certain sense, this is the Bayesian answer to what my parameters are.",
                    "label": 0
                },
                {
                    "sent": "Gives me a distribution over them, but you can take means with this you can sample from it whatever you want to do.",
                    "label": 0
                },
                {
                    "sent": "And so we know how to forgive and K and further parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "We might want to also select which of the orders K do we need, just the previous symbol to predict the next the previous two or three, and the way to do that is to consider a set of.",
                    "label": 0
                },
                {
                    "sent": "Orders and use another variation on Bayes theorem.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where now the evidence term?",
                    "label": 0
                },
                {
                    "sent": "From the estimation of parameters is right.",
                    "label": 0
                },
                {
                    "sent": "Here we have a prior over orders and what I'm going to choose here is an exponential penalty for the number of parameters in here, so I'm going to force simpler models.",
                    "label": 0
                },
                {
                    "sent": "Data has to prove that it really needs more memory to predict.",
                    "label": 0
                },
                {
                    "sent": "So how do I actually estimate these entropy rates?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is where the statistical physics comes in is that we can write down a partition function which is basically equal to the evidence, so it's equal to this integral that I would need to calculate the evidence, and I can rewrite this term.",
                    "label": 0
                },
                {
                    "sent": "Here the joint distribution over the data and the parameters in this form that's motivated by type theory, for instance from.",
                    "label": 0
                },
                {
                    "sent": "Covering Thomas is an example in information theory and what I have is.",
                    "label": 0
                },
                {
                    "sent": "Terms here, where Q is my distribution that's defined by the posterior means and P is the actual parameters from the posterior distribution, and I have an entropy rate and then I have a KL divergent, but in this case it's slightly different because these are now conditional probabilities, so it's a relative conditional distance rather than the one that we are probably more familiar with.",
                    "label": 0
                },
                {
                    "sent": "For a physicist, this looks exactly like an energy and a typical way of finding an energy is to define.",
                    "label": 0
                },
                {
                    "sent": "Partition function which I do by plugging this up in here if I want to get an average of this quantity, I can take a derivative with respect to beta, which will bring this term down and I'll actually be able to get an average of this quantity with respect to the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "And so if we do this.",
                    "label": 0
                },
                {
                    "sent": "We can actually calculate these things out.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Literally because of the forms known.",
                    "label": 0
                },
                {
                    "sent": "So this is the derivative is talking about.",
                    "label": 0
                },
                {
                    "sent": "We take a derivative of the log of the partition function and that will give our expectation of this quantity.",
                    "label": 0
                },
                {
                    "sent": "The funny one over log 2 is because I defined my information theory quantities in terms of base two, so they're all in bits and this quantity is rather horrible looking, but it's fairly.",
                    "label": 0
                },
                {
                    "sent": "Intuitive, these functions right here that I gammas are something that you get by taking.",
                    "label": 0
                },
                {
                    "sent": "If you have a log of a gamma function, you take derivatives of that, you'll get digamma functions and these are something that you find in most numerical tool boxes in programming languages, Python or whatever, and so you can also expand this out and show that this quantity asymptotically will be equal to the entropy rate plus a term that goes is 1 / L, which is this distance here.",
                    "label": 0
                },
                {
                    "sent": "So what we expect to happen when we actually get more and more data here.",
                    "label": 0
                },
                {
                    "sent": "Is that this term is going to go to zero the distance and we're going to be left with an expectation of the entropy rate.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we're going to actually apply this.",
                    "label": 0
                },
                {
                    "sent": "What can we learn by taking these two tools and putting them together?",
                    "label": 0
                },
                {
                    "sent": "an A reminder of what we're looking at?",
                    "label": 0
                },
                {
                    "sent": "We define an instrument which is given defined particular partition.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize that entropy rate.",
                    "label": 0
                },
                {
                    "sent": "We also want to infer Markov chain model of the core screen data, and we want to minimize the entropy rate there.",
                    "label": 1
                },
                {
                    "sent": "What did I use as the data?",
                    "label": 0
                },
                {
                    "sent": "So here I took the logistic map, which is a typical single hump map like what we saw before, generated 10,000 data points.",
                    "label": 0
                },
                {
                    "sent": "In this case I actually included.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stochastic perturbation so there is a noise level where we have a standard deviation of 10 to minus three.",
                    "label": 1
                },
                {
                    "sent": "And so I have this single time series.",
                    "label": 1
                },
                {
                    "sent": "Then I'm going to choose a decision point which defines a partition.",
                    "label": 1
                },
                {
                    "sent": "So I go through my time series, everything between zero and D. I'm going to label with zero everything between D and one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to label with the one I'm going to take that symbolic sequence.",
                    "label": 0
                },
                {
                    "sent": "Estimate parameters.",
                    "label": 0
                },
                {
                    "sent": "Do model selection, get an entropy rate.",
                    "label": 0
                },
                {
                    "sent": "Change the D. Do the whole thing again, change the, do the whole thing again.",
                    "label": 0
                },
                {
                    "sent": "So we're going to basically test out each one of our instruments.",
                    "label": 0
                },
                {
                    "sent": "What do we get well?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now here I have plotted the decision point where I'm placing the the.",
                    "label": 1
                },
                {
                    "sent": "Boundary between what's zero and what's one, and here I get the resulting entropy rate estimated using that average quantity, and so one thing you can see is that this is actually fairly non is quite nontrivial.",
                    "label": 0
                },
                {
                    "sent": "As a function of the decision point.",
                    "label": 1
                },
                {
                    "sent": "So here we maximize that according to the theorems from noise free symbolic dynamics.",
                    "label": 1
                },
                {
                    "sent": "This should be where we want to place the partition to preserve all the topological properties of the underlying dynamics, and this red dotted line is actually the Lyapunov exponent that we calculate from the known map, so you wouldn't be able to calculate this red dotted line without knowledge of the map and so both of these agree here, and so in a certain sense, even though there's not theorems for symbolic dynamics when there's noise.",
                    "label": 0
                },
                {
                    "sent": "We're showing that this is a consistent thing, that it does extrapolate well when you do add the noise.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing that to my knowledge it hasn't been done before.",
                    "label": 0
                },
                {
                    "sent": "We can also look at how complicated the resulting model was given a particular partition.",
                    "label": 0
                },
                {
                    "sent": "So the things that are interesting here is that of course, when my decision point is at 0.",
                    "label": 0
                },
                {
                    "sent": "Or is that one?",
                    "label": 0
                },
                {
                    "sent": "I'm looking at model orders from 1:00 to 8:00.",
                    "label": 0
                },
                {
                    "sent": "I get very very simple models.",
                    "label": 0
                },
                {
                    "sent": "I select the simplest one possible, but there's two other places where I get very, very simple models.",
                    "label": 0
                },
                {
                    "sent": "One is at 1/2, which is where I actually want to place the thing.",
                    "label": 0
                },
                {
                    "sent": "So I had an instrument that gave me the most information, but produced the simplest model.",
                    "label": 1
                },
                {
                    "sent": "And I have this sort of mysterious one here, but if you look at it, it turns out that this is a pre image of 1/2.",
                    "label": 0
                },
                {
                    "sent": "So if you map this backwards under the map, this actually will iterate to this point under 4 dynamics.",
                    "label": 0
                },
                {
                    "sent": "So it's actually related to the appropriate thing we would choose this partition point because it maximizes entropy rate.",
                    "label": 0
                },
                {
                    "sent": "And then the last thing.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is we can look at this in opposite way which is.",
                    "label": 0
                },
                {
                    "sent": "That we want to minimize entropy of the resulting models, and So what I've done here is I have three different decision points that I've picked out point 2.3, point 4.5 and then on this axis I have the order of the particular Markov chain and this is a comment on how the model selection works.",
                    "label": 0
                },
                {
                    "sent": "So for example 4.5, which was the correct decision point?",
                    "label": 0
                },
                {
                    "sent": "I get an interview rate that at different orders doesn't change.",
                    "label": 0
                },
                {
                    "sent": "It's a model selection, will just choose out the smallest model with the smallest entry rate, so it's select this one.",
                    "label": 0
                },
                {
                    "sent": "This is the probability of that model, so.",
                    "label": 0
                },
                {
                    "sent": "Basically has probability one at order one and probability 0 for all higher orders.",
                    "label": 0
                },
                {
                    "sent": "For something where we have an incorrect decision point.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like .2 that actually has a minimum in the entropy rate estimated at different orders, and So what model selection does there is it finds this minimum.",
                    "label": 0
                },
                {
                    "sent": "And once it's no longer going to get any reducing the apparent complexity, it will select out this model at that point, and the probability of that becomes one.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion, I hope that I've convinced you that randomness can be due to low dimensional.",
                    "label": 1
                },
                {
                    "sent": "Chaos, not just stochastic properties, that symbolic dynamics can be a useful tool to analyze this data.",
                    "label": 1
                },
                {
                    "sent": "And the two main takeaway themes are instruments should be designed in these types of systems to maximize the entropy rate.",
                    "label": 1
                },
                {
                    "sent": "So get the most information from the observations and for modeling ferments, model inference we want to do exactly the opposite.",
                    "label": 0
                },
                {
                    "sent": "Minimize entropy rate, and so the idea here is that you can combine this into one step, right?",
                    "label": 0
                },
                {
                    "sent": "If you just did the model selection and made the partition the decision point part of the model selection, you get trivial answers.",
                    "label": 0
                },
                {
                    "sent": "So you have to optimize over the two different parts to get the correct.",
                    "label": 0
                },
                {
                    "sent": "Answer.",
                    "label": 0
                },
                {
                    "sent": "Ticket.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Prior on the.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, you don't actually, so I mean, Bayesian methods do have a sort of natural Occam's razor.",
                    "label": 0
                },
                {
                    "sent": "Yes, in practice I've found that not in this context, but in other contexts that sometimes there tends to be sort of overfitting problems at very small data points is probably wouldn't be a problem here, and I could probably just say uniform overall model orders, and I still get the same results.",
                    "label": 0
                },
                {
                    "sent": "In some cases you will get a sort of a slight overfitting at very small data points, and that's why you would potentially want to choose that kind of penalty term.",
                    "label": 0
                },
                {
                    "sent": "Look when the noise is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, um.",
                    "label": 0
                },
                {
                    "sent": "This actually I don't know.",
                    "label": 0
                },
                {
                    "sent": "My guess is that because this is just preliminary or basically everything I know at this point is presented here, but my guess is that it's going to go up basically as a function of the stochastic term that's added.",
                    "label": 0
                },
                {
                    "sent": "In fact, the coauthor here, Jim Crutchfield, has some really nice papers from the early 80s which are symbolic dynamics and noisy chaos.",
                    "label": 0
                },
                {
                    "sent": "And he actually talks about these kinds of things using ideas from like renormalization group, and how how these things scale as a function of the noise that's applied.",
                    "label": 0
                },
                {
                    "sent": "So I would I would look there as a good good place.",
                    "label": 0
                }
            ]
        }
    }
}