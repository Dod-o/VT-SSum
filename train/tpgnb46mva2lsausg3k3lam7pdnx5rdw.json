{
    "id": "tpgnb46mva2lsausg3k3lam7pdnx5rdw",
    "title": "No voodoo here! Learning discrete graphical models via inverse covariance estimation",
    "info": {
        "author": [
            "Po-Ling Loh, Department of Statistics, UC Berkeley"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/nips2012_loh_estimation/",
    "segmentation": [
        [
            "Hi everyone, thanks for coming to my talk.",
            "I'm really excited about this unexpected honor and I'm really thankful to my advisor for just being an awesome mentor and friend.",
            "And also all of my office mates at Berkeley for all the inspiration and encouragement.",
            "So today we'll be talking about discrete graphical models and let me just lay down some notation so throughout will."
        ],
        [
            "I have a graph which we represent by G and the vertex set is denoted by V and the edge set is E and I'll remind you that a graphical model is simply a pictorial representation of a joint distribution over several nodes over several random variables.",
            "And here the number of random variables is denoted by P and the edges will represent conditional."
        ],
        [
            "It's relations, in particular.",
            "Whenever there is an absence of an edge between nodes, S&T, that indicates that variables excess NXT are conditionally independent given all the other variables.",
            "This is actually a special case of the more general Markov property."
        ],
        [
            "States that the nodes in a set A are conditionally independent of the nodes in a set a set B given the nodes in a set S whenever S is a vertex cut set of the graph."
        ],
        [
            "So graphical models are well motivated in various areas of science and engineering.",
            "For instance, in Epidemiology, you can imagine that the various nodes represent people in a community, and the edges represent the propensity to spread disease from one person to another.",
            "So the question is based on random samples from the population.",
            "How can we infer the underlying edge structure of the graph?"
        ],
        [
            "And throughout this talk will be dealing with the high dimensional setting, which means that the number of nodes in the graph is much larger than the number of observations.",
            "Furthermore, will allow our observations to be observed on cleanly, so they might be corrupted by non IID Ness, or they might have systematic corruptions like missing data or some additive noise."
        ],
        [
            "So I'll remind you that the most well studied type of graphical model is a multivariate Gaussian, and in a Gaussian graphical model these nodes are related via multivariate normal distribution.",
            "Let's say with mean zero and covariance Sigma which we don't know.",
            "And by the well known Hammersley Clifford theorem, basically the inverse covariance matrix actually reflects the edge structure of the underlying graph, and what that means is that whenever there's a zero in position St of the inverse covariance matrix, that means that there's no edge in the underlying graph.",
            "Of course, this is assuming that our representation is faithful, which we'll assume for the sake of this discussion.",
            "And based on the relation."
        ],
        [
            "Ship between the inverse covariance matrix and the edge structure of the graph, there are many different methods that people have developed for inferring the underlying edge structure.",
            "When the variables are multivariate Gaussian.",
            "Some examples are the node wise regression method where we simply regress each node excess upon all the others and based on the support of that regression vector, we infer the neighborhood of that node.",
            "In the graph.",
            "There are also global estimation methods via the graphical lasso, which we'll talk about later in the talk.",
            "And more recently, there is a line of work on the so called non paranormal which basically is a way of recovering the edge structure when we can in fact push a multivariate distribution into a multivariate normal distribution via monotonic univariate transformations.",
            "But let me stress that all of these methods are inherently Gaussian and they depend very heavily on this first statement that the inverse covariance matrix actually tells us what the underlying edges of the graph are, and therefore it's enough to infer the inverse covariance matrix.",
            "Based on random samples and that will tell us the structure of the graph.",
            "But then the."
        ],
        [
            "Actual question arises, what if the data are not Gaussian?",
            "Can the inverse covariance matrix actually tell us anything about edge structure?",
            "And it's tempting to say yes, but in general, as you may know, the answer is actually no."
        ],
        [
            "So the purpose of our work is to establish a surprising and novel relationship between inverse covariance matrices.",
            "An graph structure in particular in the case of discrete graphical models, and in general the inverse covariance matrix is not sufficient.",
            "So as I will describe later, we look at a broader class of generalized inverse covariance matrices, and Furthermore based on these population level results where."
        ],
        [
            "Able to propose 2 new methods for edge recovery in the case of a discrete graphical model.",
            "But before I dive into the mathematics, let me give."
        ],
        [
            "Motivating example, so for the purpose of this example will be studying the binary eising model.",
            "And the form of the joint distribution is given here.",
            "Basically the distribution is parameterized by a vector Theta of edge and node potentials."
        ],
        [
            "And because we're in the binary case, each one of these variables will take values zero or one, and for the purpose of this example, we're going to."
        ],
        [
            "Set all of the node potentials equal 2.1 and all of the edge potentials equal to two."
        ],
        [
            "And will be studying these two particular graphs.",
            "Let's call the first wanna chain.",
            "Because if you string it out, you got a chain and the second one is a loop.",
            "Now I've given you enough information to go ahead and generate data from this distribution and also calculate the inverse covariance matrix.",
            "So the inverse."
        ],
        [
            "Covariance matrices look like this and something very interesting is happening here, because if we first just look at the top example, then it's almost as if we're looking at a Gaussian distribution on four nodes.",
            "Because I've highlighted the zeros here and there, zeros exactly in positions 1314 and 2, four, and you'll notice that there are exactly no edges in those three positions.",
            "So it's almost as if it's a Gaussian graphical model.",
            "However, for the second example, we're not quite so lucky, because even though there are no edges in positions 1, three, and two four, there aren't any zeros in this inverse covariance matrix.",
            "It's fully dense.",
            "In fact, this is what you probably would expect if I hadn't told you that there were some relationships between inverse covariance matrices and edge structure for discrete graphs.",
            "So in the language of our work, will say that Theta the inverse code."
        ],
        [
            "Terrence is graph structured in the case of a chain, but not for the loop.",
            "Now these examples are not unique as you might expect, and it turns out that something."
        ],
        [
            "Rather surprising happens, so in fact not just for this chain, but also for this thing that will call a Dinosaur graph.",
            "It turns out that the inverse covariance matrix actually is graph structured, so if you create the 13 by 13 covariance matrix of this Dinosaur and invert it, you'll exactly have the edge structure of the graph.",
            "However, in seemingly less complicated examples, for instance, the loop and this loop with a bar through it, we see that the inverse covariance matrix is not in fact graph structured.",
            "However, there is something."
        ],
        [
            "That happens in the case of the third graph this loop.",
            "So here we're going to again assume that we have the loop graph, but instead of just looking at the four by four covariance matrix, we're going to augment it by another statistic.",
            "So we have X, one X2X3X4, and now the product X, one X3, and so then we form a 5 by 5 covariance matrix and invert it, and all of a sudden we have zeros and the zeros aren't just in random places.",
            "These zeros are in position 2, four, and you'll see that even in this.",
            "Triangulated version of the graph.",
            "There's no edge between 2:00 and 4:00, so that somehow telling us information about the underlying graph.",
            "However, you might complain because there actually shouldn't be an edge between one and three in the loop graph either.",
            "And yet we have a 109 in position 1, three, but essentially what's going on is by augmenting with the extra statistic X One X3.",
            "We're introducing some relationships between one and three, so I'm saying that it's OK that there aren't zeros in those positions.",
            "Of course, we can make this more rigorous and more explicit.",
            "So."
        ],
        [
            "Now I can go into some of the more detailed, precise statements of our results, so we're going to assume that we're not just in the binary case.",
            "We could have up to M States, where M is some finite number an in the binary case, M is equal to two.",
            "So all of our variables are just 01 valued."
        ],
        [
            "And this point gets a little bit more technical, so for any subset of the node set will associate a vector which will call FIU of sufficient statistics corresponding to this subset.",
            "You now I don't want to go into detail about what this vector of sufficient statistics looks like in general, because it's kind of complicated, but I'm happy to explain it at the poster, but just to give you a sense, in simple cases when we are in the binary two state model and then it turns out that FIU basically just corresponds to all products."
        ],
        [
            "Of exercise which lie in the set you so if you is 1 two then it's X one X2 and the product X 1 * X Two.",
            "In the case when M is bigger than two, things are more complicated, but in the simple case, when are set, just has one element, then the set of sufficient statistics, the vector of sufficient statistics is an M -- 1 dim."
        ],
        [
            "Channel vector, which has indicators of all the non 0 States and the reason why it's M -- 1 dimensional instead of M dimensional has to do with the fact that we're talking about sufficient statistics.",
            "So in some sense M -- 1 parameters are enough to to define the distribution.",
            "Alright, so."
        ],
        [
            "I've given you enough information to describe how we actually go about augmenting these inverse covariance matrices, so we begin with an arbitrary graph."
        ],
        [
            "And the first step is to triangulate it, and I'll remind you that triangulation means that we add an edge is in such a way that the final graph we get has no cordless cycles of length bigger than four bigger than or equal."
        ],
        [
            "For the next step is we form a junction tree, and I'll remind you that a junction tree is created from a triangulated graph by taking the cliques to be the Max or taking the nodes to be the maximal cliques in the graph.",
            "Then we construct a tree over these maximal cliques, and we introduce these things called separator sets given by the box, which is basically the intersection of the two cliques lying on either side of the edge."
        ],
        [
            "Alright, and then we go ahead and augment, and in particular we augment by all of the sufficient statistics for the separator sets appearing in the junction tree.",
            "So this example is particularly simple because our junction tree just has two nodes in one separator set, and assuming we're in the binary case, the sufficient statistics for one three as I described RX1X3 and X 1 * X Three, which basically means that we only need to augment by the statistic X one X3.",
            "So we formed this 5 by 5.",
            "Variance matrix we invert it, and our general theorem says that this inverse covariance matrix is exactly graphs."
        ],
        [
            "Well, it's graph structured represent with respect to the triangulated graph."
        ],
        [
            "So that basically explains why in our earlier example we had the zeros because our theorem exactly applied to this loop graph with such a triangulation tells us that we have zeros exactly in position 24.",
            "Another couple of subtleties.",
            "So one subtlety is."
        ],
        [
            "That I have given you a method for constructing an inverse covariance matrix based on any triangulation, but in general a graph will have many triangulations.",
            "For instance, we could have drawn in a bar between 2:00 and 4:00 instead of one and three, and we would get a slightly different separator set, hence a slightly different augmented matrix with X 2X4 instead, and our theorem still holds.",
            "It still says that this 5 by 5 inverse covariance matrix is also graph structured, so will separate 1 from 3.",
            "So the other subtlety, which is a little bit more serious.",
            "Is that if we're actually talking about structural estimation, so our goal is to infer the edges of the graph, then we don't know the edges apriori, so there's no way of knowing what a triangulation would be our priority.",
            "Therefore, we can't actually construct this augmented inverse covariance 'cause we don't know what to augment.",
            "However, there is 1 case in which this is not an issue, and this case is when in fact the separator stats only have one element in them, so."
        ],
        [
            "This next slide I'm saying if there is a triangulation such that all the separator sets only have one element in them, then there's nothing to augment.",
            "So then the inverse covariance matrix is just graph structure."
        ],
        [
            "And that's what our corollary says.",
            "Actually, our corollary is a statement for a tree, 'cause that's easier to describe, because if you think about a tree, a tree is already triangulated, and it only has Singleton separator sets, and so this."
        ],
        [
            "There are nice corollary that's really easy to state, and it also describes Y in the chain and Dinosaur cases.",
            "The inverse covariance matrix is exactly graph structured because even though the Dinosaur is not quite a tree, if you draw the junction tree representation then the separator sets are in fact all singletons.",
            "All."
        ],
        [
            "So now we're ready to talk a little bit about structure learning, because all of these results I've just mentioned or what we call population level results.",
            "So their true regardless of data.",
            "There all just statements about the true inverse covariance matrix.",
            "But now we want to say, well, if we have random samples of data, then how can we actually learn the structure of the graph given these population level?"
        ],
        [
            "Results.",
            "So I'll remind you of the graphical lasso, which is generally used to infer the edge set in a Gaussian graphical model, and there have been many results about the consistency of this graphical lasso when the data are multivariate Gaussian.",
            "In fact, the way the graphical lasso arises from a penalized maximum likelihood estimator for a Gaussian graphical model.",
            "So what if the data aren't Gaussian?",
            "Well, as a practitioner it's tempting to say that even if the data aren't Gaussian, let's say their binary valued.",
            "I'll just pretend that they are Gaussian.",
            "Use the graphical lasso.",
            "Learn some edges and try to make sense out of that, but from a theoretical side, what we want to do is."
        ],
        [
            "Ask when it does the graphical lasso actually work for estimating the true edge set when the data aren't Gaussian and we didn't quite answer this question, but we have answered it in certain special cases of discrete graphical models."
        ],
        [
            "Namely, this corollary here follows directly from what I just stated about tree structured graphical models.",
            "So in the particular case of a binary eising model with Singleton separators, or you can think about it as a binary using model on a tree.",
            "Basically, we have shown that the inverse covariance matrix is always exactly graph structured."
        ],
        [
            "And the graphical lasso is actually a perfectly valid way of estimating the inverse covariance matrix regardless of the distribution."
        ],
        [
            "For the notes, so our corollary says that under the proper scaling, the graphical lasso is actually a valid way of inferring the edge structure of a binary using model over a tree."
        ],
        [
            "And this is very, very surprising.",
            "I mean, it's quite surprising to us, even after doing the theory, because the graphical lasso again is based on multivariate Gaussian.",
            "And in the case of a discrete graphical model, it has nothing to do with the penalized maximum likelihood.",
            "Estimator is also a cautionary message, which is that the graphical lasso doesn't always succeed, because for instance in our four node loop we know that the inverse covariance matrix is not graph structured.",
            "So we ran graphical lasso.",
            "We would get some nonsense."
        ],
        [
            "Now, in the case when we have more than two states, we can simply apply a type of group graphical lasso, and finally I'll also say that this method is actually very amenable to missing data or some kind of systematic corruption, because the graphical lasso actually the only data dependent component is the is the true as an estimate of the true covariance matrix Sigma hat, which we can correct for different types of corrupt."
        ],
        [
            "So we ran a simulation study on our favorite Dinosaur graph, which, as we claimed earlier, has an inverse covariance matrix which is graph structured and you can see that our method actually works.",
            "So there is a transition from success probability zero to success probability one, and a relatively sharp range as the sample size increases, and Furthermore we ran it for different levels of missing data, so you can see the curves kind of shifting to the right 'cause the problem is getting harder as there's more missing data.",
            "Solo."
        ],
        [
            "Honestly, I don't really have time to talk about this, although I'm happy to discuss it more at the poster.",
            "This kind of a teaser, and basically we do have methods for inference in discrete graphical models, even when we're not in the case of a tree, and this is particularly interesting because there previously been methods for learning the edge structure of a binary eising model on a tree, but not for general graphs.",
            "So our method is a node wise method, which means that we recover the neighborhood set of a particular vertex, and the gist of it is that."
        ],
        [
            "We form a triangulation simply by connecting everything that's not S."
        ],
        [
            "We construct a junction tree and through our methods we can basically recover the neighborhood set of S just by linear regression, which is kind of cool, so more details at the poster."
        ],
        [
            "So Lastly, just to summarize, we have established a clean and rather surprising relationship between inverse covariance matrices and the edge structure of an arbitrary discrete graphical model.",
            "We've demystified the relationship between."
        ],
        [
            "Beans and dinosaurs versus loops and loops with bars through them with nothing even remotely paranormal."
        ],
        [
            "We have proposed some structural learning methods for arbitrary discrete graphs.",
            "Both trees and non trees.",
            "And Lastly, although I haven't had time to discuss this in the presentation."
        ],
        [
            "Our results are actually theoretically rigorous and we can make clean statements about under what conditions with high probability.",
            "Can we actually recover the edge structure of the graph?",
            "Thank you.",
            "We have time for questions now, so you said that you don't necessarily have this corollary, say for chains, how easy to, how easy is it for your method to say, learn the structure of a graph if you know it's a chain but you don't know the ordering of the vertices?",
            "Oh, it's a chain, that's fine.",
            "'cause then it's a tree, right?",
            "Chain that's closed like say yeah loop.",
            "Yeah, so in that case we would have to resort to advise regression method an.",
            "I think the complexity is something like 2 to the dlog P, where D is the degree and P is the number of nodes.",
            "What's the degree in this context would be 2, so this method is only really efficient for bounded degree graphs.",
            "OK, thank you.",
            "Is there any other question?",
            "Yes.",
            "So what about the method where you would just do maximum likelihood with L1 penalization on the edges?",
            "Yeah, so you could do that, but I think at least in an easing model case it would be rather complicated.",
            "I mean so pretty, but other people have methods for logistic regression an already.",
            "I mean that's so it's a bit more complicated than just applying the graphical lasso or doing linear regression.",
            "OK, no more questions.",
            "Let's thank the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi everyone, thanks for coming to my talk.",
                    "label": 0
                },
                {
                    "sent": "I'm really excited about this unexpected honor and I'm really thankful to my advisor for just being an awesome mentor and friend.",
                    "label": 0
                },
                {
                    "sent": "And also all of my office mates at Berkeley for all the inspiration and encouragement.",
                    "label": 0
                },
                {
                    "sent": "So today we'll be talking about discrete graphical models and let me just lay down some notation so throughout will.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a graph which we represent by G and the vertex set is denoted by V and the edge set is E and I'll remind you that a graphical model is simply a pictorial representation of a joint distribution over several nodes over several random variables.",
                    "label": 0
                },
                {
                    "sent": "And here the number of random variables is denoted by P and the edges will represent conditional.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's relations, in particular.",
                    "label": 0
                },
                {
                    "sent": "Whenever there is an absence of an edge between nodes, S&T, that indicates that variables excess NXT are conditionally independent given all the other variables.",
                    "label": 0
                },
                {
                    "sent": "This is actually a special case of the more general Markov property.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "States that the nodes in a set A are conditionally independent of the nodes in a set a set B given the nodes in a set S whenever S is a vertex cut set of the graph.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So graphical models are well motivated in various areas of science and engineering.",
                    "label": 0
                },
                {
                    "sent": "For instance, in Epidemiology, you can imagine that the various nodes represent people in a community, and the edges represent the propensity to spread disease from one person to another.",
                    "label": 0
                },
                {
                    "sent": "So the question is based on random samples from the population.",
                    "label": 0
                },
                {
                    "sent": "How can we infer the underlying edge structure of the graph?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And throughout this talk will be dealing with the high dimensional setting, which means that the number of nodes in the graph is much larger than the number of observations.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, will allow our observations to be observed on cleanly, so they might be corrupted by non IID Ness, or they might have systematic corruptions like missing data or some additive noise.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll remind you that the most well studied type of graphical model is a multivariate Gaussian, and in a Gaussian graphical model these nodes are related via multivariate normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Let's say with mean zero and covariance Sigma which we don't know.",
                    "label": 0
                },
                {
                    "sent": "And by the well known Hammersley Clifford theorem, basically the inverse covariance matrix actually reflects the edge structure of the underlying graph, and what that means is that whenever there's a zero in position St of the inverse covariance matrix, that means that there's no edge in the underlying graph.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is assuming that our representation is faithful, which we'll assume for the sake of this discussion.",
                    "label": 0
                },
                {
                    "sent": "And based on the relation.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ship between the inverse covariance matrix and the edge structure of the graph, there are many different methods that people have developed for inferring the underlying edge structure.",
                    "label": 0
                },
                {
                    "sent": "When the variables are multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Some examples are the node wise regression method where we simply regress each node excess upon all the others and based on the support of that regression vector, we infer the neighborhood of that node.",
                    "label": 0
                },
                {
                    "sent": "In the graph.",
                    "label": 0
                },
                {
                    "sent": "There are also global estimation methods via the graphical lasso, which we'll talk about later in the talk.",
                    "label": 0
                },
                {
                    "sent": "And more recently, there is a line of work on the so called non paranormal which basically is a way of recovering the edge structure when we can in fact push a multivariate distribution into a multivariate normal distribution via monotonic univariate transformations.",
                    "label": 0
                },
                {
                    "sent": "But let me stress that all of these methods are inherently Gaussian and they depend very heavily on this first statement that the inverse covariance matrix actually tells us what the underlying edges of the graph are, and therefore it's enough to infer the inverse covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Based on random samples and that will tell us the structure of the graph.",
                    "label": 0
                },
                {
                    "sent": "But then the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actual question arises, what if the data are not Gaussian?",
                    "label": 0
                },
                {
                    "sent": "Can the inverse covariance matrix actually tell us anything about edge structure?",
                    "label": 0
                },
                {
                    "sent": "And it's tempting to say yes, but in general, as you may know, the answer is actually no.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the purpose of our work is to establish a surprising and novel relationship between inverse covariance matrices.",
                    "label": 1
                },
                {
                    "sent": "An graph structure in particular in the case of discrete graphical models, and in general the inverse covariance matrix is not sufficient.",
                    "label": 1
                },
                {
                    "sent": "So as I will describe later, we look at a broader class of generalized inverse covariance matrices, and Furthermore based on these population level results where.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Able to propose 2 new methods for edge recovery in the case of a discrete graphical model.",
                    "label": 0
                },
                {
                    "sent": "But before I dive into the mathematics, let me give.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motivating example, so for the purpose of this example will be studying the binary eising model.",
                    "label": 0
                },
                {
                    "sent": "And the form of the joint distribution is given here.",
                    "label": 0
                },
                {
                    "sent": "Basically the distribution is parameterized by a vector Theta of edge and node potentials.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And because we're in the binary case, each one of these variables will take values zero or one, and for the purpose of this example, we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set all of the node potentials equal 2.1 and all of the edge potentials equal to two.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And will be studying these two particular graphs.",
                    "label": 0
                },
                {
                    "sent": "Let's call the first wanna chain.",
                    "label": 0
                },
                {
                    "sent": "Because if you string it out, you got a chain and the second one is a loop.",
                    "label": 0
                },
                {
                    "sent": "Now I've given you enough information to go ahead and generate data from this distribution and also calculate the inverse covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So the inverse.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Covariance matrices look like this and something very interesting is happening here, because if we first just look at the top example, then it's almost as if we're looking at a Gaussian distribution on four nodes.",
                    "label": 0
                },
                {
                    "sent": "Because I've highlighted the zeros here and there, zeros exactly in positions 1314 and 2, four, and you'll notice that there are exactly no edges in those three positions.",
                    "label": 0
                },
                {
                    "sent": "So it's almost as if it's a Gaussian graphical model.",
                    "label": 0
                },
                {
                    "sent": "However, for the second example, we're not quite so lucky, because even though there are no edges in positions 1, three, and two four, there aren't any zeros in this inverse covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "It's fully dense.",
                    "label": 0
                },
                {
                    "sent": "In fact, this is what you probably would expect if I hadn't told you that there were some relationships between inverse covariance matrices and edge structure for discrete graphs.",
                    "label": 0
                },
                {
                    "sent": "So in the language of our work, will say that Theta the inverse code.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Terrence is graph structured in the case of a chain, but not for the loop.",
                    "label": 0
                },
                {
                    "sent": "Now these examples are not unique as you might expect, and it turns out that something.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rather surprising happens, so in fact not just for this chain, but also for this thing that will call a Dinosaur graph.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the inverse covariance matrix actually is graph structured, so if you create the 13 by 13 covariance matrix of this Dinosaur and invert it, you'll exactly have the edge structure of the graph.",
                    "label": 0
                },
                {
                    "sent": "However, in seemingly less complicated examples, for instance, the loop and this loop with a bar through it, we see that the inverse covariance matrix is not in fact graph structured.",
                    "label": 0
                },
                {
                    "sent": "However, there is something.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That happens in the case of the third graph this loop.",
                    "label": 0
                },
                {
                    "sent": "So here we're going to again assume that we have the loop graph, but instead of just looking at the four by four covariance matrix, we're going to augment it by another statistic.",
                    "label": 0
                },
                {
                    "sent": "So we have X, one X2X3X4, and now the product X, one X3, and so then we form a 5 by 5 covariance matrix and invert it, and all of a sudden we have zeros and the zeros aren't just in random places.",
                    "label": 0
                },
                {
                    "sent": "These zeros are in position 2, four, and you'll see that even in this.",
                    "label": 0
                },
                {
                    "sent": "Triangulated version of the graph.",
                    "label": 0
                },
                {
                    "sent": "There's no edge between 2:00 and 4:00, so that somehow telling us information about the underlying graph.",
                    "label": 0
                },
                {
                    "sent": "However, you might complain because there actually shouldn't be an edge between one and three in the loop graph either.",
                    "label": 0
                },
                {
                    "sent": "And yet we have a 109 in position 1, three, but essentially what's going on is by augmenting with the extra statistic X One X3.",
                    "label": 0
                },
                {
                    "sent": "We're introducing some relationships between one and three, so I'm saying that it's OK that there aren't zeros in those positions.",
                    "label": 0
                },
                {
                    "sent": "Of course, we can make this more rigorous and more explicit.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I can go into some of the more detailed, precise statements of our results, so we're going to assume that we're not just in the binary case.",
                    "label": 0
                },
                {
                    "sent": "We could have up to M States, where M is some finite number an in the binary case, M is equal to two.",
                    "label": 0
                },
                {
                    "sent": "So all of our variables are just 01 valued.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this point gets a little bit more technical, so for any subset of the node set will associate a vector which will call FIU of sufficient statistics corresponding to this subset.",
                    "label": 0
                },
                {
                    "sent": "You now I don't want to go into detail about what this vector of sufficient statistics looks like in general, because it's kind of complicated, but I'm happy to explain it at the poster, but just to give you a sense, in simple cases when we are in the binary two state model and then it turns out that FIU basically just corresponds to all products.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of exercise which lie in the set you so if you is 1 two then it's X one X2 and the product X 1 * X Two.",
                    "label": 0
                },
                {
                    "sent": "In the case when M is bigger than two, things are more complicated, but in the simple case, when are set, just has one element, then the set of sufficient statistics, the vector of sufficient statistics is an M -- 1 dim.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Channel vector, which has indicators of all the non 0 States and the reason why it's M -- 1 dimensional instead of M dimensional has to do with the fact that we're talking about sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "So in some sense M -- 1 parameters are enough to to define the distribution.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've given you enough information to describe how we actually go about augmenting these inverse covariance matrices, so we begin with an arbitrary graph.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the first step is to triangulate it, and I'll remind you that triangulation means that we add an edge is in such a way that the final graph we get has no cordless cycles of length bigger than four bigger than or equal.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the next step is we form a junction tree, and I'll remind you that a junction tree is created from a triangulated graph by taking the cliques to be the Max or taking the nodes to be the maximal cliques in the graph.",
                    "label": 0
                },
                {
                    "sent": "Then we construct a tree over these maximal cliques, and we introduce these things called separator sets given by the box, which is basically the intersection of the two cliques lying on either side of the edge.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, and then we go ahead and augment, and in particular we augment by all of the sufficient statistics for the separator sets appearing in the junction tree.",
                    "label": 1
                },
                {
                    "sent": "So this example is particularly simple because our junction tree just has two nodes in one separator set, and assuming we're in the binary case, the sufficient statistics for one three as I described RX1X3 and X 1 * X Three, which basically means that we only need to augment by the statistic X one X3.",
                    "label": 0
                },
                {
                    "sent": "So we formed this 5 by 5.",
                    "label": 1
                },
                {
                    "sent": "Variance matrix we invert it, and our general theorem says that this inverse covariance matrix is exactly graphs.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it's graph structured represent with respect to the triangulated graph.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that basically explains why in our earlier example we had the zeros because our theorem exactly applied to this loop graph with such a triangulation tells us that we have zeros exactly in position 24.",
                    "label": 0
                },
                {
                    "sent": "Another couple of subtleties.",
                    "label": 0
                },
                {
                    "sent": "So one subtlety is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That I have given you a method for constructing an inverse covariance matrix based on any triangulation, but in general a graph will have many triangulations.",
                    "label": 0
                },
                {
                    "sent": "For instance, we could have drawn in a bar between 2:00 and 4:00 instead of one and three, and we would get a slightly different separator set, hence a slightly different augmented matrix with X 2X4 instead, and our theorem still holds.",
                    "label": 0
                },
                {
                    "sent": "It still says that this 5 by 5 inverse covariance matrix is also graph structured, so will separate 1 from 3.",
                    "label": 0
                },
                {
                    "sent": "So the other subtlety, which is a little bit more serious.",
                    "label": 0
                },
                {
                    "sent": "Is that if we're actually talking about structural estimation, so our goal is to infer the edges of the graph, then we don't know the edges apriori, so there's no way of knowing what a triangulation would be our priority.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we can't actually construct this augmented inverse covariance 'cause we don't know what to augment.",
                    "label": 0
                },
                {
                    "sent": "However, there is 1 case in which this is not an issue, and this case is when in fact the separator stats only have one element in them, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This next slide I'm saying if there is a triangulation such that all the separator sets only have one element in them, then there's nothing to augment.",
                    "label": 0
                },
                {
                    "sent": "So then the inverse covariance matrix is just graph structure.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's what our corollary says.",
                    "label": 0
                },
                {
                    "sent": "Actually, our corollary is a statement for a tree, 'cause that's easier to describe, because if you think about a tree, a tree is already triangulated, and it only has Singleton separator sets, and so this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are nice corollary that's really easy to state, and it also describes Y in the chain and Dinosaur cases.",
                    "label": 0
                },
                {
                    "sent": "The inverse covariance matrix is exactly graph structured because even though the Dinosaur is not quite a tree, if you draw the junction tree representation then the separator sets are in fact all singletons.",
                    "label": 1
                },
                {
                    "sent": "All.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're ready to talk a little bit about structure learning, because all of these results I've just mentioned or what we call population level results.",
                    "label": 0
                },
                {
                    "sent": "So their true regardless of data.",
                    "label": 0
                },
                {
                    "sent": "There all just statements about the true inverse covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "But now we want to say, well, if we have random samples of data, then how can we actually learn the structure of the graph given these population level?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results.",
                    "label": 0
                },
                {
                    "sent": "So I'll remind you of the graphical lasso, which is generally used to infer the edge set in a Gaussian graphical model, and there have been many results about the consistency of this graphical lasso when the data are multivariate Gaussian.",
                    "label": 0
                },
                {
                    "sent": "In fact, the way the graphical lasso arises from a penalized maximum likelihood estimator for a Gaussian graphical model.",
                    "label": 1
                },
                {
                    "sent": "So what if the data aren't Gaussian?",
                    "label": 0
                },
                {
                    "sent": "Well, as a practitioner it's tempting to say that even if the data aren't Gaussian, let's say their binary valued.",
                    "label": 0
                },
                {
                    "sent": "I'll just pretend that they are Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Use the graphical lasso.",
                    "label": 0
                },
                {
                    "sent": "Learn some edges and try to make sense out of that, but from a theoretical side, what we want to do is.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask when it does the graphical lasso actually work for estimating the true edge set when the data aren't Gaussian and we didn't quite answer this question, but we have answered it in certain special cases of discrete graphical models.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Namely, this corollary here follows directly from what I just stated about tree structured graphical models.",
                    "label": 0
                },
                {
                    "sent": "So in the particular case of a binary eising model with Singleton separators, or you can think about it as a binary using model on a tree.",
                    "label": 1
                },
                {
                    "sent": "Basically, we have shown that the inverse covariance matrix is always exactly graph structured.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the graphical lasso is actually a perfectly valid way of estimating the inverse covariance matrix regardless of the distribution.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the notes, so our corollary says that under the proper scaling, the graphical lasso is actually a valid way of inferring the edge structure of a binary using model over a tree.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is very, very surprising.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's quite surprising to us, even after doing the theory, because the graphical lasso again is based on multivariate Gaussian.",
                    "label": 1
                },
                {
                    "sent": "And in the case of a discrete graphical model, it has nothing to do with the penalized maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Estimator is also a cautionary message, which is that the graphical lasso doesn't always succeed, because for instance in our four node loop we know that the inverse covariance matrix is not graph structured.",
                    "label": 1
                },
                {
                    "sent": "So we ran graphical lasso.",
                    "label": 0
                },
                {
                    "sent": "We would get some nonsense.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in the case when we have more than two states, we can simply apply a type of group graphical lasso, and finally I'll also say that this method is actually very amenable to missing data or some kind of systematic corruption, because the graphical lasso actually the only data dependent component is the is the true as an estimate of the true covariance matrix Sigma hat, which we can correct for different types of corrupt.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we ran a simulation study on our favorite Dinosaur graph, which, as we claimed earlier, has an inverse covariance matrix which is graph structured and you can see that our method actually works.",
                    "label": 1
                },
                {
                    "sent": "So there is a transition from success probability zero to success probability one, and a relatively sharp range as the sample size increases, and Furthermore we ran it for different levels of missing data, so you can see the curves kind of shifting to the right 'cause the problem is getting harder as there's more missing data.",
                    "label": 1
                },
                {
                    "sent": "Solo.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Honestly, I don't really have time to talk about this, although I'm happy to discuss it more at the poster.",
                    "label": 0
                },
                {
                    "sent": "This kind of a teaser, and basically we do have methods for inference in discrete graphical models, even when we're not in the case of a tree, and this is particularly interesting because there previously been methods for learning the edge structure of a binary eising model on a tree, but not for general graphs.",
                    "label": 0
                },
                {
                    "sent": "So our method is a node wise method, which means that we recover the neighborhood set of a particular vertex, and the gist of it is that.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We form a triangulation simply by connecting everything that's not S.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We construct a junction tree and through our methods we can basically recover the neighborhood set of S just by linear regression, which is kind of cool, so more details at the poster.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Lastly, just to summarize, we have established a clean and rather surprising relationship between inverse covariance matrices and the edge structure of an arbitrary discrete graphical model.",
                    "label": 0
                },
                {
                    "sent": "We've demystified the relationship between.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Beans and dinosaurs versus loops and loops with bars through them with nothing even remotely paranormal.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have proposed some structural learning methods for arbitrary discrete graphs.",
                    "label": 1
                },
                {
                    "sent": "Both trees and non trees.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, although I haven't had time to discuss this in the presentation.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our results are actually theoretically rigorous and we can make clean statements about under what conditions with high probability.",
                    "label": 1
                },
                {
                    "sent": "Can we actually recover the edge structure of the graph?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "We have time for questions now, so you said that you don't necessarily have this corollary, say for chains, how easy to, how easy is it for your method to say, learn the structure of a graph if you know it's a chain but you don't know the ordering of the vertices?",
                    "label": 0
                },
                {
                    "sent": "Oh, it's a chain, that's fine.",
                    "label": 0
                },
                {
                    "sent": "'cause then it's a tree, right?",
                    "label": 0
                },
                {
                    "sent": "Chain that's closed like say yeah loop.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in that case we would have to resort to advise regression method an.",
                    "label": 0
                },
                {
                    "sent": "I think the complexity is something like 2 to the dlog P, where D is the degree and P is the number of nodes.",
                    "label": 0
                },
                {
                    "sent": "What's the degree in this context would be 2, so this method is only really efficient for bounded degree graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Is there any other question?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So what about the method where you would just do maximum likelihood with L1 penalization on the edges?",
                    "label": 1
                },
                {
                    "sent": "Yeah, so you could do that, but I think at least in an easing model case it would be rather complicated.",
                    "label": 0
                },
                {
                    "sent": "I mean so pretty, but other people have methods for logistic regression an already.",
                    "label": 0
                },
                {
                    "sent": "I mean that's so it's a bit more complicated than just applying the graphical lasso or doing linear regression.",
                    "label": 0
                },
                {
                    "sent": "OK, no more questions.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker.",
                    "label": 0
                }
            ]
        }
    }
}