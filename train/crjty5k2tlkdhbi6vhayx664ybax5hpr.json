{
    "id": "crjty5k2tlkdhbi6vhayx664ybax5hpr",
    "title": "Machine Learning for Natural Languages Processing",
    "info": {
        "author": [
            "Joan Andreu Sanchez, Technical University of Valencia (UPV)"
        ],
        "published": "Aug. 5, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bootcamp2010_sanchez_mlnlp/",
    "segmentation": [
        [
            "In this talk in this sector we will introduce a.",
            "The problem of a probabilistic estimation of probabilistic syntactic models."
        ],
        [
            "So this is the.",
            "The the index that I will follow in this talk, so I will introduce first the problem.",
            "I will focus first the problem we will we will we will see we will study how to estimate the probabilities that are associated.",
            "Usually two syntactic models I'm in this talk we will focus mainly on probabilistic context, free grammar.",
            "So then we will see how to use.",
            "One of the main problems related to this, or anything that is the parsing problem.",
            "We will see how to use these models for language modeling.",
            "A possible way of using this kind of model for language modeling?",
            "We will see.",
            "Slightly.",
            "Not deeply how to use this kind of model for much interpolation.",
            "And then we will see the same notation.",
            "And we will explain one of the minor things that is the C key wire with him.",
            "Most of the estimation already that we will study in this lecture are based on this algorithm.",
            "Then we will see how to estimate the probabilities of a model using the classical classical algorithms.",
            "We will see we will mention some probability some.",
            "Properties of estimated models and we will see how to use them for language modeling.",
            "We'll see then how to.",
            "Similar models like that in this case will be a stochastic inversion.",
            "Traditional grammar can be used for can be used for much in translation.",
            "We we will devote short time to this topic because we have not too much time and at the end I will mention some interesting topics that.",
            "I have a titled Adventure Topics, so let's start."
        ],
        [
            "With."
        ],
        [
            "This part OK, so in this lecture we will talk about natural language processing, some basic basic techniques for natural language processing.",
            "So natural language processing can be considered like field or research field within computational linguistics and is closely related with other research topics like speech technologies, text technologies and language technologies so.",
            "I want mango."
        ],
        [
            "So in this talk with.",
            "Would be to develop systems that are able to process to understand and to produce natural language.",
            "Natural language is a broad area and we will focus in one kind of models.",
            "In our K we will focus mainly on, probably in context free grammar.",
            "OK, so the motivation.",
            "For dealing with this with this problem is that natural language is a natural way to represent and interested human language, natural ways of communicating information.",
            "So they also there exist a lot of information.",
            "In a knowledge in natural language speech text.",
            "Handwritten information, printed information and also a.",
            "Currently there is a more need even day by days there is more need for their more potential.",
            "They're more need for communicating with computers using natural language.",
            "This is a classic usual example."
        ],
        [
            "OK.",
            "So in this lecture we will introduce some concepts.",
            "Related to natural language that will be used for developing applications related.",
            "Like this, for example, system for information extraction systems.",
            "We will see tools for developing or developing systems for speech to text translation, text to speech translation.",
            "Like concretely, this kind of systems and system for communication with humans.",
            "In our case, in this our approach for solving these problems will take."
        ],
        [
            "Probabilistic approach.",
            "A probabilistic approach, so we will use a probabilistic framework in order to tackle with this kind of problems.",
            "So in we take the probabilistic approach.",
            "We have two tackle, at least with three issues, that is interpretation.",
            "Modeling and learning that is.",
            "Letter interpretation given an input to assistant, we must provide an output.",
            "In our case we will use.",
            "We will solve this problem.",
            "By using some the probabilistic decision rule.",
            "Then we have two defined some kind of modeling, another another some some.",
            "We will have to define models in order to solve these problems that in our case will be based on a statistical decision.",
            "Techniques and my Lyons formal language theory.",
            "And once we have defined and have decided the model that we will use, we have to to define methods for learning this kind of models.",
            "So we have two tackle at least these these three aspects."
        ],
        [
            "So, given that we will focus in coding syntactic models, the goal of the course will be to solve difficult problems related to natural language processing.",
            "With syntactic approach.",
            "There are a lot of literature about this topic, so we have choose one of these.",
            "A specific context, so we will speak mainly about context free grammars, OK?",
            "Additionally, we will see some fundamentals of compassion and linguistics and we will learn some basic techniques that are necessary to develop that.",
            "This kind of systems.",
            "OK.",
            "This is the tools that under technique that we will see in this."
        ],
        [
            "Selector will be useful for developing application like like this.",
            "Applications related to automatic speech recognition.",
            "Application related too much installation, dialogue system etc.",
            "And most of this application has a half the some abstract modules that some of them we have inaugurated.",
            "Some of them here, for example an abstract task is language modeling.",
            "We will see later what is.",
            "Language modeling in this context.",
            "Partial part of speech tagging.",
            "Parsing another interesting task.",
            "We will focus mainly on language modeling and parsing.",
            "So in order to tackle with this kind of problems with this approach, in order to develop these applications so.",
            "We, we will.",
            "We will use this or all the all this framework in order to develop natural language processing tools OK?",
            "So."
        ],
        [
            "So.",
            "When we tackle some some problems related to natural language, so there are several knowledge levels.",
            "That we have.",
            "Presented in this slide.",
            "This is and this is the low low level.",
            "Even there can be even more low levels depending on the application.",
            "For example phonology, we are.",
            "We're trying.",
            "We're trying to solve some problems related to speech recognition, but this is.",
            "This levels are related to more directly to natural language processing, so we have a problem.",
            "We will have a level of morphology, syntax, semantics, and in the high level we have a problems related to moral problems like the discourse.",
            "So for which we have to develop.",
            "To study dialogue.",
            "Techniques OK. We will.",
            "We will study Miley.",
            "This problem we will use techniques and models in order to sort to tackle syntax problems OK."
        ],
        [
            "So, given that and in our case we will use a specific kind of models, we will use context free grammar.",
            "For solving this problem.",
            "So the problem of syntax mainly can be expressed through this example.",
            "So the idea is we have a three sentence.",
            "Like this and our problem is to define to the define some kind of model that is able to to.",
            "Take into account the different relation that can be established between the different parts of a sentence, so our problem will be 2.",
            "Define models in order to show up to solve to define this kind of relations.",
            "So for example, we have a sentence like this.",
            "That is composed by the noun phrase at the very phrase and then the noun phrases composed in these two parts, and so on.",
            "So this is as true color as structural information in which we are interested.",
            "OK, so this.",
            "Part 3 this syntactic tree or parse tree.",
            "It will be our goal.",
            "Dealing with this kind of representation, we have our goal for this purpose.",
            "We will use a formalism that is known as grammar, formal grammar, context free grammar.",
            "In our case, a context free grammar is simple."
        ],
        [
            "Our former model like this.",
            "That is composed the set of rules ladies, for example, that defines the different relation that can be established between the different parts of a sentence.",
            "For example, we have a rule in order to express that a sentence is composed by known phrase and verb phrase, and we we expand this symbol, we obtain another structure and so on.",
            "Given that we are interested in dealing with.",
            "Probabilistic models we will."
        ],
        [
            "Try.",
            "The problem of ambiguity that is usual in natural language.",
            "We will try to solve this problem by."
        ],
        [
            "Incorporating some information to this simple model that in our case will be probability.",
            "So we will attach probabilities to the rules.",
            "And these probabilities will guide our search, our way of solving the problems, OK?"
        ],
        [
            "So.",
            "This is the the concept.",
            "Now we will use Miley on this on this talk.",
            "This is a context free grammar.",
            "Not accounts for this sentence in which we have incorporated that some probabilities OK.",
            "This kind of models have been used, for example, for language modeling an for.",
            "Much integration, let's see what the explain more in detail this."
        ],
        [
            "Problems OK, so the problem of recognition in pattern recognition can be toggled with this with this.",
            "With this system, with this structure, we have a recognition that receives an input and we must provide an possible interpretation of the input.",
            "So the way in which we can solve this problem is with this expression.",
            "In which if you see we have inverted the orientation of the recognition in such a way that first we must generate.",
            "An open sentence and then we compute how well this output sentence accounts for the input signal.",
            "The input of the system.",
            "OK.",
            "This is the.",
            "This is now our language model, and this is now's channel probability."
        ],
        [
            "In the case of speech recognition.",
            "We found our speech recognition system that receives an input, in this case signal and equally process it cleaned cleaned.",
            "So.",
            "With some kind of models, we provide a sequence of words.",
            "And this is usually solved with this, considering the recognition problem in this way, in which this is the language model.",
            "This language model.",
            "Is the probability of an output sentence and then.",
            "We combine this probability with the probability that this output sentence accounts for the input signal.",
            "OK, so.",
            "These expression can be discomposed in this way, so we have the probability of the first word.",
            "And we have this product in which each word depends on the previous words, OK?",
            "So the usual way."
        ],
        [
            "To solve this problem is user is using ngram models.",
            "In which we impose our restriction on the history so we restrict the probability of one word to taking into account just a few previous words that is.",
            "So this is the this is known as the ngram models so, but these models have some some restrictions, some the drawbacks.",
            "One of them is that these models are not able to capture long-term dependencies.",
            "Because we are only considering the previous words in order to decide the probability of of the following word.",
            "But they have interesting properties like they can't efficiently compute it in real time systems and there exist very efficient methods for estimating the parameters of the model.",
            "Another possibility that has been explored, some are grammatical models.",
            "In this kind of models, no restriction is imposed on the history, so the probability of one word when computing the probability of a word we take into account all the previous words.",
            "OK, but this kind of models well.",
            "The benefit of this kind of model is.",
            "They can capture long-term dependencies, but the drawback of the their spend shift to compute, especially to integrate, integrate this kind of models in real time systems.",
            "And although there exist efficient method to estimate the model parameters, they are.",
            "These methods are very expensive.",
            "OK. Well, a syntactic models that we will see in this lecture has been also used for.",
            "Much in translation."
        ],
        [
            "Especially in recent years.",
            "In which the problem so much installation has been tackled in different ways, for example using using the inverse approach, similar similar.",
            "Similarly as we have seen for speech recognition or other other approaches, for example the direct approach.",
            "A the most usual way of tackling with with this problem is using a log linear model, in which the search is carried out in a similar way of speech recognition.",
            "You can see the literature.",
            "So, but another possible way is to use syntactic models, where for much in translation there has been studied several tests."
        ],
        [
            "Cheeks.",
            "We have summarized here some of them.",
            "Most of the technique that's having developed for much international are based on the word alignments.",
            "Classical world alignments.",
            "The idea is to obtain a word on alignment between between the two sentences that we want to translate and then look for similar pairs of substrings.",
            "In each sentence.",
            "Another possibility is usually is using syntactic approach.",
            "Specially 4 languages in which the structure of the language is very different.",
            "For example, languages like Spanish and English have similar structure.",
            "For example, subject, verb and object.",
            "But Chinese has a different structure, so for this kind of languages, syntactic syntactic approach has been demonstrated to be very efficient.",
            "OK, we will see former formalism to tackle with this problem that in our case will be simple stochastic inversion.",
            "Do transition grammars OK?",
            "There are approaches not too much is pretty interesting, yet that are based on final stage approaches.",
            "OK.",
            "If you have any questions, please don't hesitate to interrupt me, OK?",
            "OK, so."
        ],
        [
            "Summarizing this this introduction we are interested in dealing with problems related to natural language processing using a syntactic approach, and in a more specifically stochastic syntactic approach, probabilistic approach.",
            "We will we will focus in one kind of models.",
            "In our case, context free grammar, stochastic wanted, remember so.",
            "So let's see some notation and definitions and some basic."
        ],
        [
            "Everything's OK, context free grammar.",
            "RA form is a formalism, very interesting formalism.",
            "That the.",
            "Was developed some time ago.",
            "OK, this this kind of models are simple and compact models for parsing.",
            "For giving for account for the structure of a sentence.",
            "Yeah there is.",
            "It is our well understood formal.",
            "There is exist well understood formal framework.",
            "Although this is the language is not.",
            "Regular is almost regular context free grammar, not regular languages, OK?",
            "Don't define regular languages OK. Yeah, quantify, are adequate for representing long-term syntactic structures, and you will incorporate probabilities to the model.",
            "They can deal adequately with the ambiguity that arise.",
            "In natural language processing so.",
            "Our our framework.",
            "Our framework.",
            "To work."
        ],
        [
            "Will be defined according to this.",
            "To this concept we will have a set of primitives that will that we will use in order to compose an object.",
            "In our case our primitives will be an alphabet.",
            "But a lot of it is simply a set of symbols.",
            "Warp words, punctuation symbols, etc that can be put together in order to compose sentences.",
            "In order to compose sentences.",
            "And if we have.",
            "Several sentences, then we have.",
            "Are a set of sentences that we will use 2.",
            "It's tracking information from the sentences and our problems will be related with syntactic analysis.",
            "Our goal is to define adequate methods in order to obtain a possible interpretation.",
            "From this kind of models.",
            "So this is, uh, this will be our input and we are interested in the defining methods and model models and methods in order to tackle with this kind of information.",
            "OK.",
            "So."
        ],
        [
            "More formally.",
            "We introduce some notation, more formal notation and alphabet.",
            "Is Affinity set of symbols.",
            "A string is composed, but a set of symbols.",
            "We put together 2 strings.",
            "We concatenate the strings.",
            "Then we we are able to construct larger.",
            "Strings sometimes it is convenient to deal with special strings like empty strings.",
            "This usually introduce some difficulties and in practical cases sometimes is avoided.",
            "So the closure of the of the alphabet.",
            "Is the set of all strings that can be cause composed combining the symbols in any way?",
            "And a language can be defined.",
            "It last last subset of this.",
            "Of these of this set OK. Well, this is this is this is our objects object will be sentences so now.",
            "We have our projects we now define.",
            "We need now to define a formalism that can account for this kind of obj."
        ],
        [
            "That in our case will be a grammar so agrama is simple can be defined it formally like a couple like this.",
            "In which this is a set of nonterminals that is the symbols that account for the internal structure of the sentence.",
            "A set of terminal symbols, that is, the words of the string.",
            "This is a set of rules that.",
            "Define how they don't.",
            "Terminals can be combined in order to compose sentences.",
            "But this is the initial symbol of the grammar.",
            "The actual matter grammar, all the all the all the sentences are composed from this non terminal so.",
            "This is the formalism that we will use in this in this that we will see in this talk.",
            "So we have strings.",
            "We have grammars.",
            "Now we need some way of relating strings and grammars.",
            "So for this propose we introduce the concept of derivation.",
            "Suppose that we have a string composed but non terminal symbols and terminal symbols.",
            "So the concept is derivation is our relation that is established between two strings in such a way?",
            "That is, we have this string composed but non terminal symbols and terminal symbols.",
            "We can substitute this nonterminal symbol, but this string if we have a rule.",
            "In the grammar like this.",
            "OK, I I will.",
            "I will illustrate this with some examples.",
            "So the concept of deviation is introduced in order to relate to in order to relate to strings.",
            "For example.",
            "Suppose that we have something like this.",
            "This is a string composed but terminal and non terminal symbols so.",
            "We take this known terminal symbol, look for a rule that has this non terminal on the left part of the rule which is the left part of the rule and we expand this symbol 62 in it.",
            "For this other two symbols.",
            "So we obtain.",
            "Something like this?",
            "OK.",
            "So we define a sentential form.",
            "Lost as a collection of.",
            "Derivation that.",
            "Start on the initial symbol on the action of the order grammar so.",
            "A sentential form looks like this.",
            "I'm sorry I need.",
            "I need some.",
            "Something like this?",
            "OK. We start with the initial symbol, produce these two symbols this symbol.",
            "It's just it buddy to single because we have a rule like this.",
            "OK. And then and then we shifted to this this terminal, but this terminal.",
            "OK.",
            "So.",
            "This is a sentential form becauses a set of strings and not strings, that are generated from this non terminal OK.",
            "So the language generated by the grammar can be defined in this way.",
            "Is the set of all strings that can be composed from derivations of the grammar.",
            "For example.",
            "If we if I continue with this derivation of things like santyl ideas.",
            "OK. Tell me.",
            "Faster screen.",
            "This one.",
            "The closer.",
            "OK, because if you have this dentist ring will start because you can start with within the industry.",
            "OK. You need a unit.",
            "Initially you have something like.",
            "This issues that we start with in this way.",
            "We have we don't have any derivation.",
            "Then we apply elevation the first elevation OK?",
            "Questions.",
            "OK. Show the gram the language.",
            "The language generated by G can be defined as all the strings that can be formed from.",
            "The starting with initial symbol on applying rules, spending rules until we achieved a set of only terminal symbols.",
            "OK.",
            "So.",
            "Well.",
            "This is closely related to formal to formal language is OK. We can see that the literature for.",
            "For more information.",
            "OK, so the grammars can be classified in order, taking into account the how the rules.",
            "How?",
            "The aspect of the rules so we can introduce at least that more than one type over grandmas.",
            "For example, counter free grammar and regular grammars in in the context, all the rules are of this way we have only here only one known non terminal that can produce.",
            "A string of terminals or non terminals.",
            "So we will focus only in this kind of models.",
            "OK, there exists another another kind of grammars for a simple.",
            "This grammar are regular grammars more simpler.",
            "OK, so we will focus in this kind of grammars.",
            "In this kind of grammars, the right part there is no restriction on the right part, but there exists authoring that demonstrated there exists a special way or writing the rules in such a way that all every general grammar can be right.",
            "In that way I'm talking about the chunky normal form.",
            "That means that every context free grammar.",
            "Can be writing in this way.",
            "Yeah.",
            "Removing the empty string.",
            "All every grammar can be writing in such a way that all the rules are of this kind or of this kind.",
            "OK, this is known as Chomsky normal form.",
            "OK.",
            "The algorithms that we will see here deal.",
            "With grammars in this format, in Chomsky normal form, OK?",
            "OK, there is a there is a closely relation between part three and derivations we can represent.",
            "The generation process resting with a derivation or with a partial tree is the same OK. Well, let's continue.",
            "Well, we have a strings.",
            "We have grammars and now we need a way or connect or relation or relating both concept."
        ],
        [
            "And this is the carry out through the concept of parsing.",
            "So the problem parsing relates agreement with a string agreement with a string.",
            "So there are several approaches in order to relate to.",
            "There are several approaches related to parsing.",
            "So for example, top down parsing down top, parsing, and another possibilities.",
            "In our case, we will focus in this algorithm, the CKY algorithm.",
            "That is, algorithm that is an algorithm.",
            "That in the in this case is on top there with him we will see this with him in the following slide is a dynamic programming are within just a dynamic programming algorithm.",
            "So let's see there with him I will explain."
        ],
        [
            "More detail.",
            "This is the Coke kasama yung algorithm.",
            "The writing was developed by these three outsource and the input to the algorithm is a grammar in Chomsky normal form and a string, and they are the algorithm is able to to tell us if the sentence belongs to the language generated by the grammar.",
            "OK, I will see an example in the in the following slide.",
            "Let's see my structure.",
            "It is a dynamic programming algorithm.",
            "So the idea is to fill in a table in which each cell.",
            "We, in which itself contains this non terminal and this means that these non terminal is able to generate this substring using one or more derivations.",
            "OK, you must take into account, for example that if we have.",
            "R. Context free grammar like this?",
            "Simply like this, we can generate three like this.",
            "In order to produce this string or we can use, we can have another string like this.",
            "So we have two parse trees for the same string.",
            "OK, two different structures for the same string.",
            "OK, so they always seem proceeds in the following way.",
            "It's a bottom up algorithm that take into account a.",
            "Short substrings of the of the of the sentence and.",
            "In fact, it starts with strings of sight one.",
            "Then of size 2 of size 3 etc until it accounts for all the string.",
            "So we consider first in the initialization.",
            "So a substrings of length one, so we include in this cell this non terminal.",
            "We have a rule like this.",
            "Shut this.",
            "That this symbol.",
            "Is the same of the similarities in this position, and then we start to analyze substrings of larger size, size 2, size 3.",
            "So this index.",
            "Ranges for all possible substrings.",
            "These index ranges for all possible positions and this index explore the possible ways in which we can divide 1 string, so finally.",
            "When the algorithm ends is able we are able to to answer this question.",
            "If this not this non terminal is in the top cell, so the string belongs to language OK.",
            "The important thing to note in this in this slide is that we have an algorithm that is cubic."
        ],
        [
            "Here we have an example, so this we have this this program A.",
            "This is the the the table that we are feeling through the algorithm.",
            "We start filling these cells for example.",
            "This means that these non terminal is able to to account for this symbol because we have a rule like this.",
            "OK the same for this and this information.",
            "For example this means that we have a rule that is able to account for.",
            "These non terminal and is non terminal S. Generate A and S. So if we include this non terminal in this in this cell and so on.",
            "OK, we proceed in this way completing the table at the end.",
            "If this symbol appears in this cell, we can say that the string belong to the language generated accepted by the.",
            "They extend string is accepted by the by the grammar, yes.",
            "Numbers, as you know.",
            "This is in order to clarify how this item has been composed.",
            "This sitting can be composed from this item.",
            "I'm from this item.",
            "OK is in order to clarify or something.",
            "They're everything OK, so they always has time complexity as quick and the space complexity is quadratic with the length of the sentence.",
            "But there is another dimension related with the number of symbols.",
            "OK questions.",
            "So we have strings.",
            "Note that we have strings, we have grammars and we have a way of relating the strings with grammars.",
            "Now we are."
        ],
        [
            "Adding some information to this formalism that we have introduced that in our case will be probabilities.",
            "We're incorporating probabilities to the to this kind of models.",
            "We are interested in.",
            "Adding tools for solving this problem, we would like to solve problem ladies.",
            "OK, so we extend the concept of language with stochastic language faster.",
            "Stochastic language is just language with that has apparently.",
            "A probability associated to each string in such a way that this this condition must be accomplished OK. For example, here we have a possible alphabet.",
            "This is a language.",
            "This is, this language is well known is the palindrome language.",
            "That has the same number, the same.",
            "Member of A and this OK.",
            "This is a context free language, so we define this this function.",
            "This is a probabilistic language.",
            "OK, so.",
            "We have incorporated probability to the language, so we need to incorporate probabilities to the model.",
            "OK, so."
        ],
        [
            "We extend the concept of of context free grammar.",
            "We in this way we introduce the concept of probabilistic context free grammar, probabilistic context free grammars just stop all define it.",
            "In this way G is a grammar, a context free grammar and P is a probability that is a function that associated probability to each role in such a way that this this condition must be accomplished.",
            "That is then that means that the we add the probabilities.",
            "Of the rules that has that have the same nonterminal on the left part of the rule, this value must be one.",
            "OK.",
            "So we have probability languages we have.",
            "Probabilistic models to account for this kind of languages.",
            "So we need to again to relate these concepts.",
            "In the same way.",
            "OK, so this concept is related through the introduces concepts in order to relate probability languages with the probabilistic context free grammar."
        ],
        [
            "So we we incorporate stochastic information to the concept of derivation.",
            "So if we have this derivation.",
            "In which we have used this rule.",
            "This rule this rule OK so.",
            "The probability of this derivation.",
            "Is defined as the probability of of the product of the probability of the rule that participate in it in that derivation product of this derivation with this event with this event with this event.",
            "So this is the this is the correct definition.",
            "But given that this is very difficult to compute, we introduce some restrictions and we shoot this especially with this expression.",
            "So we eliminate this dependency and we compute the probability of a derivation as the product.",
            "As the product of the rules that participate, and in that derivation.",
            "OK. Christians.",
            "Ignore.",
            "Like transitions for complex breeze.",
            "For the dependencies.",
            "Your properties on your grammars.",
            "Are you asking me in some way the model is consistent?",
            "I I. Yeah, I know, I know, I know, I know, I know, I know.",
            "But we will see some properties that prove that the model is consistent with these well defined.",
            "Let me let me see, OK?",
            "So."
        ],
        [
            "OK, given given this these concepts so we introduce this disconcerts probability of a delegation that is defined in this way.",
            "Sometimes interest is it is interesting to define the probability in this way.",
            "OK.",
            "So the probability of string can be defined as the addition of all of the probability of all the derivation that can compose this string.",
            "You must take into account that sometime this set, this is the set of possible derivation.",
            "This set can have a. Exponential size, so this is a way of representing this information OK.",
            "Sometimes it is interesting to compute, not the probability of the string, but the probability of the derivation that is able to two accounts for the string.",
            "So we must substitute this addition with maximization and sometimes we can take an intermediate position and sometimes it is interesting to find the probability of not taking into account.",
            "All the notable derivation and not only one derivation, but a set of derivation we have introduced this concept in order two general to see destination algorithms in a general way OK.",
            "So finally, the language generated by a context free grammar, probabilistic context, Can we define it in this way?",
            "That is, the this apologetic programmer is just the set of all the string that can be composed.",
            "Yeah, playing derivation and which probability is greater than 0.",
            "OK.",
            "I will try now to answer some of your doubts so.",
            "Yeah, if you see we have a grammars, probabilistic languages, probabilistic grammars away so."
        ],
        [
            "An interesting question is is.",
            "Every language that language can be represented by a probabilistic grammar.",
            "It's an interesting question.",
            "So the answer is no.",
            "You have here an example.",
            "This.",
            "This language processing language cannot be represented by the contest for grammar.",
            "Maybe because of the definition that we have introduced OK. Another interesting, interesting question is every probabilistic grammar is able to generate a probabilistic language.",
            "The answer is no.",
            "Again so.",
            "And there are some examples, for example.",
            "This grammar.",
            "Is not.",
            "Consistent if P is larger than zero point point.",
            "This comment is not able to generate a probabilistic language.",
            "The idea is clear if we.",
            "We start to generate symbols.",
            "Every time that we choose this this rule, we introduce those two nonterminals.",
            "Two new student terminals.",
            "So the generating process can may not finish.",
            "OK.",
            "But questions?",
            "OK, well now we'll see algorithms that can relate strings.",
            "Probably a stranger from a language from a political language with strings.",
            "We start with."
        ],
        [
            "So we introduced the Insight algorithm that is well known algorithm for parsing.",
            "That the.",
            "Was that became popular from this paper, but there are previous.",
            "There is a previous paper from Baker.",
            "And again, this this is an algorithm that is exactly the same as the CKY algorithm.",
            "If we compute this value, that is the probability that this non terminal generated this substring.",
            "So this is the algorithm is a dynamic.",
            "Again is a dynamic programming algorithm.",
            "This is the initialization.",
            "So and this is the, this is the initialization and this is the recursion.",
            "So we are in all these values and is the same algorithm and in this way we have an algorithm to solve this problem.",
            "To solve this problem, does in this case has a time complexity that again each cubic cubic with the length of the string linear with the size of the model.",
            "They.",
            "Note that this is a hard restriction in order to use this kind of models in real applications.",
            "OK, so there have been some improvements."
        ],
        [
            "With this algorithm, possible improvement is to incorporate some in formation in the strings in order to limit 8 the search space.",
            "This information was introduced in this paper, and the idea was to incorporate brackets to the string.",
            "Markets that sometimes are linguistically motivated.",
            "For example, if we are able to define a way of of bracket bracket the the noun phrase and a brief ratio, we have some information we can.",
            "For we can take profit.",
            "OK, so in this way we can modify the algorithm which simple function that is 1.",
            "If this is a.",
            "The division of the string does not overlap the restriction imposed by the bracket.",
            "For example, we could in this example we could not associate this word with this word because these division in this point OK.",
            "If we implement adequately this information, so we have a full bracketed sentence, then everything becomes.",
            "Linear.",
            "OK, that's big, why?",
            "Because we have the past three.",
            "OK.",
            "Simply OK."
        ],
        [
            "So."
        ],
        [
            "And this is the central theme that allows us to compute this probability."
        ],
        [
            "OK, and we want to obtain not the probability of the string, but the best interpretation of the string that is the best Part 3.",
            "The most probably tree.",
            "Then we can we can introduce on everything that we now know as bitter beer with him.",
            "This algorithm is similar to the Beat LVL rating that is used for hidden Markov models.",
            "OK the."
        ],
        [
            "This."
        ],
        [
            "He is similar to this, but we have we have substitute this.",
            "Addition by a maximisation.",
            "Simply OK, another interesting algorithm for parsing in this case is down top.",
            "Is the."
        ],
        [
            "The site algorithm we introduced this algorithm cause this algorithm is very important for the estimation of the model of the probability of the model so.",
            "Yeah, yeah, simply we define this probability that this is the probability that the initial and terminal generated string.",
            "Until let me see.",
            "The Insight algorithm compute this probability of this part of the string and the sight probability is able to compute this probability.",
            "Starting from this symbol.",
            "This is the inside probability.",
            "Compute this probability.",
            "Are they all the parts tree that appear in this in this place, and the same probability is able to compute all the trees that are that generate this part on this part and generate this string?",
            "The symbol and this symbol?",
            "Have to account for this substring.",
            "OK.",
            "It's again a dynamic programming algorithm.",
            "There the team has two parts, 1 four.",
            "This part, another one for this part, OK?",
            "OK, so this is everything that I repeat.",
            "I introduce each other thing because it is important for the estimation of the models.",
            "OK, if we have bracket information again, the only thing can be linearly 'cause if we don't have bracketing information.",
            "The algorithm is cubic.",
            "OK questions.",
            "Finally, we introduce an algorithm that is very important for."
        ],
        [
            "Language modeling that is that in this case is the left to right inside algorithm is again up or dynamic programming algorithm.",
            "And it'll wash, compute the probability that the initial non terminal of the grammar is able to compute an initial superstring.",
            "So it is there a team is able to compute this probability OK. We need some pre computations that are not related with the input, just with the model.",
            "And then we define algorithms similar to the to the to the previous algorithms.",
            "In this case we need the insight everything in this in this.",
            "In this computation, OK, there is also quick questions.",
            "No well.",
            "So let's continue with this."
        ],
        [
            "With the with this point and let's start to and let go to speak about probabilistic estimation, probabilistic estimation of probabilistic context, free grammar, OK?",
            "So.",
            "Well, as you have you have seen so such as we have seen, the grammar has two parts.",
            "The stochastic part and this too proud part the stochastic part are the probabilities.",
            "Is there is there are the qualities and the structural part is composed by the rules of the grammar we will focus here on the learning of the probabilities, not on the learning of the rules.",
            "The learning of the rules is related with other virtual field that is known as grammatical inference OK?",
            "We will focus repeat.",
            "With the learning of the of the probabilities, not the rules, not the structural part, OK?",
            "There exist some work, some words that combine the learning of both of both parts, but not.",
            "I will not review this in this lecture.",
            "OK, so."
        ],
        [
            "Our problem is to estimate the probabilities of the model so we we have several frameworks.",
            "For example, we could learn the probabilities of the model from in a supervisor right away.",
            "That is a. Analyzing taking the rules directly from the trees.",
            "A from the trees of a sentence.",
            "But note that in this case we need to annotate that information.",
            "And this is very expensive.",
            "So in this store in this part of the talk we will focus on non supervisor methods.",
            "That is, we don't have any information about the annotation of the string, so we have Brown arrow sentence.",
            "OK so for this we have a we have a annotated data that is trees.",
            "So the usual way is to take the rules directly from the from the tree and count and normalize and in this way.",
            "We have a a possible a very good woman.",
            "In fact, this is the usual way.",
            "But the problem is that it is very expensive to have available.",
            "Trees that is a tree bank OK. A tree for each sentence.",
            "OK, in fact there are few corpora annotated in this way.",
            "OK, so we will focus on non supervisor methods so that we don't have any information about the.",
            "They are not the the structural part of the of the string, so the algorithm algorithms that we will see here is.",
            "Exactly the same as the the EM algorithm.",
            "The spectation maximization algorithm, but I will present the estimation of it in another way.",
            "Under one of the problem of this method is that usually they are able to achieve a local optimum.",
            "OK, well let's see the framework in which we explain the."
        ],
        [
            "Estimation everything, so suppose that we have a grammar.",
            "With a set of parameters, that is, this set of parameters are the rules, or sorry, the priority of the rules and we have a sample of strings and we want to optimize this function.",
            "So we want to define a function to be optimized and we want to achieve the best parameters that account for state function.",
            "In our case we will this so we need a function and optimization framework.",
            "In our case, the optimization frameworks will be the growth transformations and the optimization function in our case will be then.",
            "Likelihood OK?",
            "OK, so let's see what this growth information."
        ],
        [
            "OK.",
            "Suppose that we have a function, are normal generous polynomial with non negative coefficients OK?",
            "And let sit and be appointed defined in this way.",
            "This is a point in this domain.",
            "Yeah.",
            "This is the problem.",
            "The probabilities of the rules I will.",
            "I will write an example.",
            "So in that way that the the addition of of some part of the of the of this of this point at one.",
            "OK, and we apply this information.",
            "Let me write an example.",
            "OK, this is theater, you know 113 to 12.",
            "13 sister 21622 Cedar Tree one.",
            "OK, yes.",
            "Sorry.",
            "Wait wait wait wait wait I need I need another Hunter.",
            "Yes.",
            "OK, thank you.",
            "OK.",
            "So this.",
            "This is.",
            "A possible point in that domain, OK?",
            "So we have a polynomial and we have at this domain.",
            "If we apply this transformation.",
            "This information takes a point on obtained another point.",
            "That is, take this point.",
            "And obtains another point.",
            "OK.",
            "In the same domain.",
            "OK, so if we apply this information the we can guarantee that the that we are able to increase the polynomial is the polynomial is the likelihood.",
            "OK.",
            "So applying successively this iteratively, these.",
            "This transformation we are optimizing the likelihood function until we achieve the convergence.",
            "This is just the EM algorithm.",
            "So it's like multiplication of the title.",
            "Maker app.",
            "This this is this is where we are.",
            "The differential this is a point, a specific point, and this is our derivation 62 in the current point.",
            "Who will let me see?",
            "Let me continue because I have the the I will we will use for defining.",
            "Estimation method OK so suppose."
        ],
        [
            "That we have this function.",
            "OK. That in this case we have we are using just not only not all the derivation, but just a few derivations OK. A few derivations.",
            "If we take all the derivations, then we have the probability of the string and if we take the best innovation, we have the Viterbi probability.",
            "OK, so we apply the."
        ],
        [
            "Transformation."
        ],
        [
            "To this.",
            "To this function we achieve this expression.",
            "We can see the demonstration in this slides please."
        ],
        [
            "Take a take a look OK?",
            "OK, see the details in the demonstration.",
            "OK, so we see here what we're doing is.",
            "The the probability of the rule is simply becauses only the this is the one divided by the probability of the of the string.",
            "With this set of deviation.",
            "This is the product of the division and this is the number of times that this rule has been used in this celebration.",
            "We are in this value for all possible derivation.",
            "We simply are counting an average use of the rule in the derivations.",
            "That we are using for defining the probability of the other string.",
            "This is the average count.",
            "The number of times that we are using the rule simply is OK.",
            "So if we introduce this probability here, this is a probability.",
            "This is a priority over the hidden data.",
            "In this case the derivation.",
            "And this is our statistic.",
            "We account we are at this information.",
            "We are obtaining sufficient statistics in order to compute the probability of the new probability of the rule.",
            "Simply this and this is just a normalization factor in which we are counting the number of times that it's non terminal appears in each position so.",
            "We only.",
            "Suppose that we have a string like this.",
            "So we are looking we are.",
            "We are looking this rule on this position on all positions.",
            "OK. And each the and it's time.",
            "That the rules are the rules appear in what division we had one, but not one.",
            "But the average weight of that derivation in the division, in that it participates OK.",
            "Yes.",
            "First we set the parameters like.",
            "We set the parameters for production.",
            "Then we we apply it information, we optimize the function and repeat the process.",
            "Again, like.",
            "I don't know, it says first we decompose it.",
            "First we parse the playing the part three and then we called the link and we update the wait a minute, wait a minute.",
            "The computation is another problem you must take into account that this computation can be carried out in this way because the number of delegation.",
            "If we take we take all the population in this set can be half exponential size, so this is a way of seeing the estimation process.",
            "OK, so we define the."
        ],
        [
            "Function.",
            "That is, the probability of the."
        ],
        [
            "Take all the possible derivations."
        ],
        [
            "We are able to write this expression in this other way.",
            "And this is the classical inside outside algorithm.",
            "OK, here we have.",
            "Their child probability, the inside probability and then share probability.",
            "By the property of the Rule 4.",
            "The demonstration for data is or about how to."
        ],
        [
            "How to transform this ship?"
        ],
        [
            "Asian in this expression is easy.",
            "OK, you can see demonstration in one of the pennies.",
            "OK.",
            "So.",
            "We have a possible way of estimating we only count.",
            "We we we use we apply the inside the site and this transformation the inside the site and this information.",
            "Then inside the site and information.",
            "This information has also the cubic complexity.",
            "OK."
        ],
        [
            "Well, if we use not the set of derivation, but the best."
        ],
        [
            "And then we have this function and destination destination expression that we obtained.",
            "Is this that in this case?"
        ],
        [
            "It's very easy because we are taking only one derivation.",
            "Here are one derivation in this additional so this this term Council with this term and we have here only the number of times that are all having used in the best derivation."
        ],
        [
            "And this is their expression that we obtain.",
            "OK.",
            "So, but well, but there will be only theme destination with the bit algorithm that is also that we also call.",
            "Everything.",
            "It's a little strange because in this in this expression there there is implicitly a maximization.",
            "So one can say, well, can we differentiate differentiate with this maximization in the process so the."
        ],
        [
            "Everything runs in the following way so.",
            "We obtained the best derivation for each.",
            "For each string, we obtain the reservation.",
            "So once we have obtained the restoration, the transformation, the growth transformation warranties that this value is less than the value.",
            "So this is the probability of each string with the battery derivation, and this is the probability of each thing with derivation with but with the new parameters and this and this is less or equal.",
            "Done this because of the transformation and in the following step we look for the best elevation with the new with the new parameters so we can guarantee that it is true that this is true.",
            "So we optimizing.",
            "We're maximizing and then we can guarantee that this expression is less than this, especially not the difference is in this case is here in the parameters and in this case is in the derivation, so we can guarantee that this value.",
            "Less than this value.",
            "OK, how they better be algorithm run?"
        ],
        [
            "In this way, this is the maximization the transformation the maximization.",
            "Letter formation and so on OK.",
            "Questions.",
            "So this everything has this estimation.",
            "Algorithms have very interesting properties I will summarize."
        ],
        [
            "Some of them.",
            "So some of the properties can be expressed introducing some concepts in this case is the concept of probabilistic estimate expectation matrix that can be defined in this way.",
            "That is, this means the number of.",
            "Non terminals labeled with this with this index that are introduced with non terminals.",
            "Elaborate with index this index so.",
            "A.",
            "This is the.",
            "This is the definition or space of expectation material of this.",
            "This is the definition of expectation matrix.",
            "So we can guarantee that if we estimate the probabilities with these models, the grammar is consistent.",
            "So we have well defined models, so we can see the details here.",
            "The idea is very simple.",
            "The idea is to show that this is that.",
            "That this matrix converter converts.",
            "Converges OK.",
            "There are another another interesting properties that are having that I have not writing here Becauses too large.",
            "But when we estimate a grammar with a sample, the grammar is able to learn not only the distribution but also the average length of the of the of the string of the sample is the same of the.",
            "Average length of the string generated by the grammar and another probabilities.",
            "You can see the details in this paper.",
            "OK.",
            "So well, this everything you have seen in some way are very similar becaused we have seen that they come derivated from the same expression.",
            "So in the practice, how, how?"
        ],
        [
            "Run OK, let's hear.",
            "Some experiments that show in practical in the in practice how they run.",
            "We have a used in this case at Toy example.",
            "In this case, that example is the palindrome language.",
            "So these are we have this language and we have this model that is able to generate the strings of this kind.",
            "So we what we carry out was to generate a set of strings and then we train we trained.",
            "A grammar from these strings.",
            "OK, for this purpose we generate a grammar.",
            "That in this question was an exotic model that is a model that has all the possible rules that can be composed with a given number of terminals and a given number of non terminals.",
            "So.",
            "For example.",
            "Here we have.",
            "This set of sorry.",
            "Of non terminals at this and this just only one terminal.",
            "So we compose all the possible rules.",
            "As produce as fast as produce produces.",
            "As a as etc all possible roles.",
            "OK.",
            "So in this case we are in the worst case because we don't have any information about the structure of the language, so we estimate this grammar and then we evaluate it.",
            "The goodness of the model by measuring the cool bag labeled emergencies in this case, so we can see that the inside or outside the rhythm is able to learn better the distribution.",
            "Then we also generated strings with the learned grammar and we then use this grammar to see how how many of the of the of the strings were palindromes.",
            "So in this case for this example.",
            "Yeah.",
            "The number of palindrome generated by a gram estimated with a little beer with him, or with insight or celebrating the difference is very large.",
            "OK, I think that I tried different.",
            "Initially change phones for for the grammar random initializations.",
            "I think that I took the better for for showing this value, but in general the number of linear motion the Poly offer in terms is is very very low.",
            "Not larger than this, not as large as this as this so.",
            "This this is a."
        ],
        [
            "One of the properties and another property is that, well, the algorithm, both algorithms for estimating grammar and they proceed in the following way.",
            "The insight to settle with him.",
            "Estimate the probability of the rules and if you must, take into account that if the model has no.",
            "Less symbols so all the symbols produce a string and participate in a derivation so.",
            "Then all the rules appear at least in one place of the one derivation.",
            "Show is sometimes the convergence process takes long time, very long time.",
            "As you can imagine, and this is illustrated in this.",
            "In this example we need this number of iterations to achieve some convergency, while the Viterbi algorithm the rhythm if rule doesn't appear in the best derivation.",
            "For the second, disappear has new probability and it is removed.",
            "OK. Show this everything.",
            "Convert more or convert faster than the than the inside outside algorithm.",
            "So in addition."
        ],
        [
            "These are these discovery themes.",
            "Tends in general, tends to accumulate the probability mass in the best derivation, just in few derivations.",
            "So for example, we evaluated the accumulated probability mass in the best derivation and in the five best derivations.",
            "So after the training process, most of the probability of the strings is accumulated just in a few derivations.",
            "Just in a few division, and this justifies if some of you know the speech recognition when you when we use a hidden Markov models, sometimes they hear the models are trained with insights with the theme or the forward water with him and then the recognition is carried out with the metabolism.",
            "So given that most of their priorities accumulate in just a few derivations, this justifies the use of this.",
            "Chatterjee, OK so this illustrate how their probabilities accumulated in just a few derivations, so both algorithms proceeds in a similar way.",
            "This plot.",
            "Means the following.",
            "So this is the probability of the derivation.",
            "The second best elevation, the Third Division the Stratera, and we we are training the model.",
            "The priority tends to be accumulated investigation something like this.",
            "Until the final most of the most of the probability is accumulated in just a few generations.",
            "This is a phenomenon that has not been very well studied, and I think that it will be interesting to see how this why this happens.",
            "OK, questions.",
            "OK, so let's continue, and now we're seeing how to use these models for language modeling.",
            "Probabilistic context free grammar have been used in several having the use of probabilistic context free grammar for language modeling has been explored in several works.",
            "We explored this formalism some time ago, so I will explain how the how we how we used."
        ],
        [
            "These are these models, so in our case a.",
            "We use probabilistic context free grammar for language modeling, but in our case we combine the grammar with an angle grammar.",
            "This is usual.",
            "This is useful.",
            "OK, so suppose that we have.",
            "This is the we are interested in computing this value.",
            "And this value and this value can be writing in this way.",
            "OK, so when you are using ngram models, we have this expression in such a way that the probability of overwork depends.",
            "If such as I have commented previously on the previous words.",
            "So.",
            "We explored the use of these models using this this combination.",
            "We combine linearly and anger model with a probabilistic model that in this case that in this case we divided the probabilistic model again in two parts.",
            "Because the training of these models is very hard, very time consuming.",
            "So what we do was two train a model that accounts only for until the pre terminal.",
            "Of the of the until the pro terminal of the.",
            "Of the words of the string that is the part of the speech tax so.",
            "Let me see."
        ],
        [
            "So here we have the sentence.",
            "These are syntactic syntactic.",
            "These are syntactic.",
            "Symbols and these are post tax symbols, part of speech symbols.",
            "This means that this is a instructor and this is Attack Attack for that word that the meaning, for example, is that this is a noun.",
            "A single noun, plural noun and adjective, etc.",
            "So in our language model we train the model in two parts.",
            "We divided the model, the syntactic model in two parts.",
            "We tried this model.",
            "Until this part, and then we combine this in a simple way."
        ],
        [
            "OK, so here we have the ngram model and here we have the probabilistic model.",
            "This model as I have commented is divided into parts."
        ],
        [
            "The whole graph the for the first we have the Ingram model and.",
            "We divided the model in two parts.",
            "1st, we generate a string until post tax and then we generate for each postdoc award with the distribution.",
            "OK, so we try.",
            "Say in in in two times.",
            "First, we estimate the grammar and 2nd we estimate this separately.",
            "OK.",
            "In this case, given the difficult to train a good grammar, we used the bracketed version of the algorithm, the bracketed version.",
            "So we took the information from the from at, from corpus that is annotated not only by parentheses, by also with this."
        ],
        [
            "Completed structure.",
            "So the so."
        ],
        [
            "So where in order to compute the."
        ],
        [
            "Use the language model we need to compute this value.",
            "That she we need to compute this value, and so we need.",
            "The probability of an initial string, but we have nothing to compute.",
            "This is the left to right inside algorithm that we have introduced."
        ],
        [
            "Previously, but in that in this case we had to modify a slightly in order to introduce this distribution of post tax into words.",
            "So the modification is very simple.",
            "We are only introducing this probability, so we generate a category, a POS tag, and then we generate a word.",
            "We need also to modify the in."
        ],
        [
            "Title Re theme is easy.",
            "So these are the likely modifications and then.",
            "We evaluate it this language."
        ],
        [
            "OK, and."
        ],
        [
            "Sardar shot so we we here we have an example of the of the corporate that we used.",
            "This is this corpus is there is the the Pentagon corpus that is a couple that is annotated in this way we have the sentence.",
            "We have a post tax and we have syntactic tax.",
            "OK so for the training process.",
            "So we separate the training of the grammar from this part to the top.",
            "Part and from this part to the bottom part from this part."
        ],
        [
            "OK, so these are the these are the characteristic of the corpus.",
            "This is the training.",
            "In this case we use this amount of sentences.",
            "Eh, we limited the training by removing those sentences that had more than 15 words.",
            "And this is we choose this directory's for tuning, and this number of sentences for test.",
            "We removed from the vocabulary all.",
            "We only took the most.",
            "Ten, 10,000 words.",
            "OK, so for the grammar we use free software.",
            "We reduced the linear discounting force museum.",
            "So and we we with this model.",
            "With this training data we obtain this perplexity in the tuning set, and this perplexity in the test set.",
            "OK.",
            "So we will."
        ],
        [
            "We were able."
        ],
        [
            "To improve in some way the the perplexity of the N gram, and this is the.",
            "This is the perplexity on the tuning set obtained by the three gram model.",
            "This is our baseline model and we adjust the Alpha parameter.",
            "The combination with the tuning set, and the best value is obtained here."
        ],
        [
            "And then we evaluate it."
        ],
        [
            "With this Alpha."
        ],
        [
            "What?",
            "With this Alpha."
        ],
        [
            "The tests are pretty and well, we have achieved some improvements in a similar way to other works.",
            "OK, when we tried the same by evaluating war error rate, the results were not as good as we expected.",
            "OK questions?",
            "Questions, so let's questions.",
            "Let's continue.",
            "I will finish the match integration part and then we will.",
            "We will stop for half an hour OK?"
        ],
        [
            "Let's see."
        ],
        [
            "OK, so now we are speaking about the use of probabilistic model for much in translation.",
            "OK, the usual way for dealing with for much intallation.",
            "Usual ways, is to use a kind of model that are now as a phrase based models, so the use of syntax has been used just for some kind of languages, so most of these most of these.",
            "Some approaches rely relies on on a model that we will."
        ],
        [
            "In this in this section.",
            "So we are now introducing the concept of stochastic inversion.",
            "Traditional grammars that was recently introduced by the guy who, but this formalism is closely related with syntax directed syntax directed in transaction grammars.",
            "So in this case the our primitives will be 2 alphabet, one for the input language and one for the output language.",
            "In this case, and in this case, our objects will be pirated strings.",
            "One is drink and each translation in other language.",
            "So this is an example of our object.",
            "For example, we have volume attorney report that are there can be translated in English by living today in the afternoon.",
            "OK so we have party the strings and our Internet and we're interested in solving problems problems like this.",
            "We are interested in relating substrings of both strings, substrings, or both strings different parts.",
            "OK, so for example, this is an example of a tree or obtaining by.",
            "Stochastic inversion tradition grammar.",
            "This is an example.",
            "In the world presented by The Who.",
            "He only used one non terminal and the idea was that that non terminal accounts for immersion in the language is OK.",
            "So let's introduce more formally this concept."
        ],
        [
            "And ITG invention tradition grammar is simply at a point define it like this we have a set of nonterminals.",
            "A set of terminals, the language, the input language.",
            "There's a set of another set of terminals, the output language, a set of rules, and the action of the grammar.",
            "So this is the rules that can be that these rules are very similar to the rule that we have seen.",
            "For context free grammar.",
            "Very similar, but in this in this time.",
            "The rules has a special mark in order to decide if the non terminals in the right part must be inverted or not.",
            "So if we have, for example, a rule like this.",
            "This means that.",
            "In the input language, the relation between strings is in this way.",
            "And in the output, the string is also in this way.",
            "This is the input string.",
            "This is the output string and this structure is related with this and this structure is relatively nice.",
            "OK.",
            "If we have a rule like this.",
            "I'm an investor and immersion rule like this that is not it in this way.",
            "So.",
            "No, sorry.",
            "Then this part, sorry.",
            "This part is related with this part, and this part is related with this part.",
            "OK, there is our inversion in the structure.",
            "In the in the open language with respect to the output.",
            "So the input language for the input sentence.",
            "OK.",
            "So we will see later on example.",
            "So."
        ],
        [
            "What we have here an example.",
            "This is an inversion rule.",
            "So this string in the English part goes to the.",
            "Initial order string.",
            "Goes to.",
            "Initial part OK."
        ],
        [
            "So.",
            "Well, these models for these models there exist also a normal form in such a way that.",
            "Every.",
            "Immersion transition grammar can be righted.",
            "All the rules can be writing in this way, and if the we need to introduce the empty string so we have also this rule, this is.",
            "This rule is usually omitted.",
            "So this is the usual rules that we have that we have in a traditional grammar so.",
            "Then again, we we attach probability to the rules OK in order to introduce from stochastic inversion tradition grammars.",
            "OK.",
            "So we have again of stochastic formalism to account for pilot strings, so we must be introduced, we want."
        ],
        [
            "Change the concept of a stochastic derivation so that this concept is defined.",
            "Define it in a similar way.",
            "Is defined in the similar way, so the probability of a derivation is the priority of the rules.",
            "The product of the probability of the rule that contribute to that derivation.",
            "So we have an example for this.",
            "Here we have fun in stochastic inversion traditional grammar that is able to produce this this tree.",
            "So in the input string for the important thing we have, this is the string that we have that we have generated and this is just the string that we have generated for the operator string, because in this case we are visiting first this symbol because.",
            "For the output part with first right.",
            "The symbol and then the symbol.",
            "Imposed order we visit when we are trying to recover the output sentence.",
            "We must visit in post order the tree when we arrive to an inverted animation animated rule.",
            "OK."
        ],
        [
            "So we can all again into this conscious probability of a string paid probability of the reservation in a similar way, and the language generated by stochastic invention to transition drama in a similar way.",
            "So one could think that this formalism is powerful enough to represent all possible alignment between words, but this is not true.",
            "But this is not true, for example."
        ],
        [
            "Take a look.",
            "This is a result reported in the original paper of in the Bush Paper whose paper.",
            "So suppose that we're trying to match all the words in one sentence with all the words in another sentence with the same with the same length.",
            "So for example a.",
            "We have we have forward we are.",
            "We can match this with this with this world with this work with with this work.",
            "So this kind of alignments.",
            "Can't not be represented by an ATG, but.",
            "This kind of alignments are very strange.",
            "So for example, who reports this table in which he measured the possible alignments, all matching with the alignment that can be obtained with an ITG and we can see, for example, that the maximum number of alignments with two sentences of length 10 is this value, But this is the amount of alignment that can be represented by an ATG in.",
            "Sometimes this is not.",
            "As this is not as bad as we can, one could think cause in something we are restricting this search space.",
            "OK.",
            "So again, we have the."
        ],
        [
            "Similar problems we have supporting problems, learning problems and translation problems, so the parsing for the parsing problems.",
            "They are already teams are very similar.",
            "Are there similar?",
            "We have the insight they will tell you with him and the outsider redeemed estimation algorithm are very similar.",
            "So again, we have a learning problem that we can divide into parts.",
            "The learning of the structure, the rules, or the probabilities for the probabilities.",
            "We can define similar algorithms to the inside outside algorithm and finally the collation process can be carried out using an adapted or anything similar to the CKY origin.",
            "OK, let's see more data in one of these algorithms."
        ],
        [
            "But these debited materially theme this is the original algorithm proposed by who?",
            "This is the initialization.",
            "The idea is similar.",
            "We are filling our as a table in in a down top.",
            "OK, we have a first sentences of length 123 etc.",
            "Now you see me."
        ],
        [
            "Very similar, so this is there.",
            "The recursion in which we are maximum.",
            "We took the maximum of these two values.",
            "This is when the relation is direct and this is when the relation is inverted.",
            "OK, and this is the Viterbi algorithm.",
            "If we keep track of the of the decisions that we have taken in each step, we are able later to recover the tree.",
            "OK, and obtain the past three.",
            "OK, we're actually proposed a modification on the algorithm because the original algorithm avoids some parse tree."
        ],
        [
            "The details can be seen in this work.",
            "OK, showed everything.",
            "Once we have defined once we have introduced this algorithm, all the algorithms are very similar.",
            "This is the victory theme.",
            "They shut.",
            "The Insight algorithm is similar, we just."
        ],
        [
            "Here, the maximization by an addition here, OK, but.",
            "As you can see.",
            "This algorithm has a time complexity.",
            "This is cubic with the input and cubic with the output.",
            "OK, so this is very very very expensive.",
            "OK, so we only can use this for limited tasks.",
            "OK.",
            "So we can also introduce a brackets in the algorithm in order to to improve the time complexity.",
            "In this time the introduction of the use of bracketing is even more important and even more interesting because we could use monolingual partial for one part and a monolingual partial further apart.",
            "So we can use a monolingual parser for English and and another monolingual parser for Spanish.",
            "For example Chinese, and then we can use the brackets.",
            "In order to train R&R on stochastic investment and issue grammar.",
            "OK.",
            "So again, if we use brackets, the time complexity can be linear with each string.",
            "OK, so given that these are."
        ],
        [
            "I think we are able to solve this problem.",
            "This problem.",
            "And the translation is carried out in a similar way, too personal."
        ],
        [
            "Sing"
        ],
        [
            "With him OK."
        ],
        [
            "You can see the details in this reference, so we have evaluated these models for translation for much integration, and this prevents that.",
            "We carry out consisted on 1st we take, we took a billingual corpus.",
            "We parse one of the one of the.",
            "Of the.",
            "Of the part of the one of the sides.",
            "Sorry, one of the parts.",
            "Or the other of the strings.",
            "Then we minimized the, given that some parsers obtained parse tree that had no binary, we must introduce submission process.",
            "Then we combine this information with the parsing.",
            "Combine all the information in order to estimate the grammar."
        ],
        [
            "So more or less, the idea is to this is the only way up the result obtained by a monolingual parser, and this is the result of tiny.",
            "With an ITG in which we used not only one non terminal but more non terminals and then we try to match each non terminal with this with each non terminal.",
            "In this way we are able to obtain agreement estimated with bitter beer with him and then we use this model for for inauguration."
        ],
        [
            "When the results are acceptable, so we participate in international task.",
            "So this is the the figures for the training data for development and for the test set.",
            "And in this case the baseline was a baseline.",
            "In this case the baseline was a phrase based translation system that is the state of the art and well in our case we we achieved to improve a little.",
            "This values these values.",
            "OK questions.",
            "Is there any question no.",
            "Well, we stop here and we will return here in half an hour, OK?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this talk in this sector we will introduce a.",
                    "label": 0
                },
                {
                    "sent": "The problem of a probabilistic estimation of probabilistic syntactic models.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "The the index that I will follow in this talk, so I will introduce first the problem.",
                    "label": 0
                },
                {
                    "sent": "I will focus first the problem we will we will we will see we will study how to estimate the probabilities that are associated.",
                    "label": 0
                },
                {
                    "sent": "Usually two syntactic models I'm in this talk we will focus mainly on probabilistic context, free grammar.",
                    "label": 0
                },
                {
                    "sent": "So then we will see how to use.",
                    "label": 0
                },
                {
                    "sent": "One of the main problems related to this, or anything that is the parsing problem.",
                    "label": 0
                },
                {
                    "sent": "We will see how to use these models for language modeling.",
                    "label": 0
                },
                {
                    "sent": "A possible way of using this kind of model for language modeling?",
                    "label": 0
                },
                {
                    "sent": "We will see.",
                    "label": 0
                },
                {
                    "sent": "Slightly.",
                    "label": 0
                },
                {
                    "sent": "Not deeply how to use this kind of model for much interpolation.",
                    "label": 0
                },
                {
                    "sent": "And then we will see the same notation.",
                    "label": 0
                },
                {
                    "sent": "And we will explain one of the minor things that is the C key wire with him.",
                    "label": 0
                },
                {
                    "sent": "Most of the estimation already that we will study in this lecture are based on this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then we will see how to estimate the probabilities of a model using the classical classical algorithms.",
                    "label": 0
                },
                {
                    "sent": "We will see we will mention some probability some.",
                    "label": 0
                },
                {
                    "sent": "Properties of estimated models and we will see how to use them for language modeling.",
                    "label": 1
                },
                {
                    "sent": "We'll see then how to.",
                    "label": 0
                },
                {
                    "sent": "Similar models like that in this case will be a stochastic inversion.",
                    "label": 0
                },
                {
                    "sent": "Traditional grammar can be used for can be used for much in translation.",
                    "label": 0
                },
                {
                    "sent": "We we will devote short time to this topic because we have not too much time and at the end I will mention some interesting topics that.",
                    "label": 0
                },
                {
                    "sent": "I have a titled Adventure Topics, so let's start.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This part OK, so in this lecture we will talk about natural language processing, some basic basic techniques for natural language processing.",
                    "label": 0
                },
                {
                    "sent": "So natural language processing can be considered like field or research field within computational linguistics and is closely related with other research topics like speech technologies, text technologies and language technologies so.",
                    "label": 1
                },
                {
                    "sent": "I want mango.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk with.",
                    "label": 0
                },
                {
                    "sent": "Would be to develop systems that are able to process to understand and to produce natural language.",
                    "label": 1
                },
                {
                    "sent": "Natural language is a broad area and we will focus in one kind of models.",
                    "label": 0
                },
                {
                    "sent": "In our K we will focus mainly on, probably in context free grammar.",
                    "label": 0
                },
                {
                    "sent": "OK, so the motivation.",
                    "label": 1
                },
                {
                    "sent": "For dealing with this with this problem is that natural language is a natural way to represent and interested human language, natural ways of communicating information.",
                    "label": 1
                },
                {
                    "sent": "So they also there exist a lot of information.",
                    "label": 1
                },
                {
                    "sent": "In a knowledge in natural language speech text.",
                    "label": 0
                },
                {
                    "sent": "Handwritten information, printed information and also a.",
                    "label": 0
                },
                {
                    "sent": "Currently there is a more need even day by days there is more need for their more potential.",
                    "label": 0
                },
                {
                    "sent": "They're more need for communicating with computers using natural language.",
                    "label": 0
                },
                {
                    "sent": "This is a classic usual example.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in this lecture we will introduce some concepts.",
                    "label": 0
                },
                {
                    "sent": "Related to natural language that will be used for developing applications related.",
                    "label": 0
                },
                {
                    "sent": "Like this, for example, system for information extraction systems.",
                    "label": 1
                },
                {
                    "sent": "We will see tools for developing or developing systems for speech to text translation, text to speech translation.",
                    "label": 0
                },
                {
                    "sent": "Like concretely, this kind of systems and system for communication with humans.",
                    "label": 1
                },
                {
                    "sent": "In our case, in this our approach for solving these problems will take.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "A probabilistic approach, so we will use a probabilistic framework in order to tackle with this kind of problems.",
                    "label": 0
                },
                {
                    "sent": "So in we take the probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "We have two tackle, at least with three issues, that is interpretation.",
                    "label": 0
                },
                {
                    "sent": "Modeling and learning that is.",
                    "label": 0
                },
                {
                    "sent": "Letter interpretation given an input to assistant, we must provide an output.",
                    "label": 0
                },
                {
                    "sent": "In our case we will use.",
                    "label": 0
                },
                {
                    "sent": "We will solve this problem.",
                    "label": 0
                },
                {
                    "sent": "By using some the probabilistic decision rule.",
                    "label": 1
                },
                {
                    "sent": "Then we have two defined some kind of modeling, another another some some.",
                    "label": 0
                },
                {
                    "sent": "We will have to define models in order to solve these problems that in our case will be based on a statistical decision.",
                    "label": 1
                },
                {
                    "sent": "Techniques and my Lyons formal language theory.",
                    "label": 0
                },
                {
                    "sent": "And once we have defined and have decided the model that we will use, we have to to define methods for learning this kind of models.",
                    "label": 0
                },
                {
                    "sent": "So we have two tackle at least these these three aspects.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, given that we will focus in coding syntactic models, the goal of the course will be to solve difficult problems related to natural language processing.",
                    "label": 1
                },
                {
                    "sent": "With syntactic approach.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of literature about this topic, so we have choose one of these.",
                    "label": 0
                },
                {
                    "sent": "A specific context, so we will speak mainly about context free grammars, OK?",
                    "label": 1
                },
                {
                    "sent": "Additionally, we will see some fundamentals of compassion and linguistics and we will learn some basic techniques that are necessary to develop that.",
                    "label": 0
                },
                {
                    "sent": "This kind of systems.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This is the tools that under technique that we will see in this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Selector will be useful for developing application like like this.",
                    "label": 0
                },
                {
                    "sent": "Applications related to automatic speech recognition.",
                    "label": 1
                },
                {
                    "sent": "Application related too much installation, dialogue system etc.",
                    "label": 0
                },
                {
                    "sent": "And most of this application has a half the some abstract modules that some of them we have inaugurated.",
                    "label": 0
                },
                {
                    "sent": "Some of them here, for example an abstract task is language modeling.",
                    "label": 0
                },
                {
                    "sent": "We will see later what is.",
                    "label": 1
                },
                {
                    "sent": "Language modeling in this context.",
                    "label": 0
                },
                {
                    "sent": "Partial part of speech tagging.",
                    "label": 1
                },
                {
                    "sent": "Parsing another interesting task.",
                    "label": 0
                },
                {
                    "sent": "We will focus mainly on language modeling and parsing.",
                    "label": 0
                },
                {
                    "sent": "So in order to tackle with this kind of problems with this approach, in order to develop these applications so.",
                    "label": 0
                },
                {
                    "sent": "We, we will.",
                    "label": 0
                },
                {
                    "sent": "We will use this or all the all this framework in order to develop natural language processing tools OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When we tackle some some problems related to natural language, so there are several knowledge levels.",
                    "label": 1
                },
                {
                    "sent": "That we have.",
                    "label": 0
                },
                {
                    "sent": "Presented in this slide.",
                    "label": 0
                },
                {
                    "sent": "This is and this is the low low level.",
                    "label": 0
                },
                {
                    "sent": "Even there can be even more low levels depending on the application.",
                    "label": 0
                },
                {
                    "sent": "For example phonology, we are.",
                    "label": 0
                },
                {
                    "sent": "We're trying.",
                    "label": 0
                },
                {
                    "sent": "We're trying to solve some problems related to speech recognition, but this is.",
                    "label": 0
                },
                {
                    "sent": "This levels are related to more directly to natural language processing, so we have a problem.",
                    "label": 0
                },
                {
                    "sent": "We will have a level of morphology, syntax, semantics, and in the high level we have a problems related to moral problems like the discourse.",
                    "label": 0
                },
                {
                    "sent": "So for which we have to develop.",
                    "label": 0
                },
                {
                    "sent": "To study dialogue.",
                    "label": 0
                },
                {
                    "sent": "Techniques OK. We will.",
                    "label": 0
                },
                {
                    "sent": "We will study Miley.",
                    "label": 0
                },
                {
                    "sent": "This problem we will use techniques and models in order to sort to tackle syntax problems OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, given that and in our case we will use a specific kind of models, we will use context free grammar.",
                    "label": 0
                },
                {
                    "sent": "For solving this problem.",
                    "label": 0
                },
                {
                    "sent": "So the problem of syntax mainly can be expressed through this example.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we have a three sentence.",
                    "label": 0
                },
                {
                    "sent": "Like this and our problem is to define to the define some kind of model that is able to to.",
                    "label": 0
                },
                {
                    "sent": "Take into account the different relation that can be established between the different parts of a sentence, so our problem will be 2.",
                    "label": 0
                },
                {
                    "sent": "Define models in order to show up to solve to define this kind of relations.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have a sentence like this.",
                    "label": 0
                },
                {
                    "sent": "That is composed by the noun phrase at the very phrase and then the noun phrases composed in these two parts, and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is as true color as structural information in which we are interested.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "Part 3 this syntactic tree or parse tree.",
                    "label": 0
                },
                {
                    "sent": "It will be our goal.",
                    "label": 0
                },
                {
                    "sent": "Dealing with this kind of representation, we have our goal for this purpose.",
                    "label": 0
                },
                {
                    "sent": "We will use a formalism that is known as grammar, formal grammar, context free grammar.",
                    "label": 0
                },
                {
                    "sent": "In our case, a context free grammar is simple.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our former model like this.",
                    "label": 0
                },
                {
                    "sent": "That is composed the set of rules ladies, for example, that defines the different relation that can be established between the different parts of a sentence.",
                    "label": 0
                },
                {
                    "sent": "For example, we have a rule in order to express that a sentence is composed by known phrase and verb phrase, and we we expand this symbol, we obtain another structure and so on.",
                    "label": 0
                },
                {
                    "sent": "Given that we are interested in dealing with.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic models we will.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try.",
                    "label": 0
                },
                {
                    "sent": "The problem of ambiguity that is usual in natural language.",
                    "label": 0
                },
                {
                    "sent": "We will try to solve this problem by.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Incorporating some information to this simple model that in our case will be probability.",
                    "label": 0
                },
                {
                    "sent": "So we will attach probabilities to the rules.",
                    "label": 0
                },
                {
                    "sent": "And these probabilities will guide our search, our way of solving the problems, OK?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the the concept.",
                    "label": 0
                },
                {
                    "sent": "Now we will use Miley on this on this talk.",
                    "label": 0
                },
                {
                    "sent": "This is a context free grammar.",
                    "label": 0
                },
                {
                    "sent": "Not accounts for this sentence in which we have incorporated that some probabilities OK.",
                    "label": 0
                },
                {
                    "sent": "This kind of models have been used, for example, for language modeling an for.",
                    "label": 0
                },
                {
                    "sent": "Much integration, let's see what the explain more in detail this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems OK, so the problem of recognition in pattern recognition can be toggled with this with this.",
                    "label": 1
                },
                {
                    "sent": "With this system, with this structure, we have a recognition that receives an input and we must provide an possible interpretation of the input.",
                    "label": 0
                },
                {
                    "sent": "So the way in which we can solve this problem is with this expression.",
                    "label": 0
                },
                {
                    "sent": "In which if you see we have inverted the orientation of the recognition in such a way that first we must generate.",
                    "label": 0
                },
                {
                    "sent": "An open sentence and then we compute how well this output sentence accounts for the input signal.",
                    "label": 0
                },
                {
                    "sent": "The input of the system.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is now our language model, and this is now's channel probability.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the case of speech recognition.",
                    "label": 0
                },
                {
                    "sent": "We found our speech recognition system that receives an input, in this case signal and equally process it cleaned cleaned.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "With some kind of models, we provide a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "And this is usually solved with this, considering the recognition problem in this way, in which this is the language model.",
                    "label": 0
                },
                {
                    "sent": "This language model.",
                    "label": 0
                },
                {
                    "sent": "Is the probability of an output sentence and then.",
                    "label": 0
                },
                {
                    "sent": "We combine this probability with the probability that this output sentence accounts for the input signal.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "These expression can be discomposed in this way, so we have the probability of the first word.",
                    "label": 0
                },
                {
                    "sent": "And we have this product in which each word depends on the previous words, OK?",
                    "label": 0
                },
                {
                    "sent": "So the usual way.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To solve this problem is user is using ngram models.",
                    "label": 1
                },
                {
                    "sent": "In which we impose our restriction on the history so we restrict the probability of one word to taking into account just a few previous words that is.",
                    "label": 1
                },
                {
                    "sent": "So this is the this is known as the ngram models so, but these models have some some restrictions, some the drawbacks.",
                    "label": 0
                },
                {
                    "sent": "One of them is that these models are not able to capture long-term dependencies.",
                    "label": 1
                },
                {
                    "sent": "Because we are only considering the previous words in order to decide the probability of of the following word.",
                    "label": 0
                },
                {
                    "sent": "But they have interesting properties like they can't efficiently compute it in real time systems and there exist very efficient methods for estimating the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "Another possibility that has been explored, some are grammatical models.",
                    "label": 0
                },
                {
                    "sent": "In this kind of models, no restriction is imposed on the history, so the probability of one word when computing the probability of a word we take into account all the previous words.",
                    "label": 1
                },
                {
                    "sent": "OK, but this kind of models well.",
                    "label": 0
                },
                {
                    "sent": "The benefit of this kind of model is.",
                    "label": 1
                },
                {
                    "sent": "They can capture long-term dependencies, but the drawback of the their spend shift to compute, especially to integrate, integrate this kind of models in real time systems.",
                    "label": 0
                },
                {
                    "sent": "And although there exist efficient method to estimate the model parameters, they are.",
                    "label": 1
                },
                {
                    "sent": "These methods are very expensive.",
                    "label": 0
                },
                {
                    "sent": "OK. Well, a syntactic models that we will see in this lecture has been also used for.",
                    "label": 0
                },
                {
                    "sent": "Much in translation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Especially in recent years.",
                    "label": 0
                },
                {
                    "sent": "In which the problem so much installation has been tackled in different ways, for example using using the inverse approach, similar similar.",
                    "label": 1
                },
                {
                    "sent": "Similarly as we have seen for speech recognition or other other approaches, for example the direct approach.",
                    "label": 0
                },
                {
                    "sent": "A the most usual way of tackling with with this problem is using a log linear model, in which the search is carried out in a similar way of speech recognition.",
                    "label": 1
                },
                {
                    "sent": "You can see the literature.",
                    "label": 0
                },
                {
                    "sent": "So, but another possible way is to use syntactic models, where for much in translation there has been studied several tests.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cheeks.",
                    "label": 0
                },
                {
                    "sent": "We have summarized here some of them.",
                    "label": 0
                },
                {
                    "sent": "Most of the technique that's having developed for much international are based on the word alignments.",
                    "label": 0
                },
                {
                    "sent": "Classical world alignments.",
                    "label": 0
                },
                {
                    "sent": "The idea is to obtain a word on alignment between between the two sentences that we want to translate and then look for similar pairs of substrings.",
                    "label": 0
                },
                {
                    "sent": "In each sentence.",
                    "label": 0
                },
                {
                    "sent": "Another possibility is usually is using syntactic approach.",
                    "label": 0
                },
                {
                    "sent": "Specially 4 languages in which the structure of the language is very different.",
                    "label": 0
                },
                {
                    "sent": "For example, languages like Spanish and English have similar structure.",
                    "label": 0
                },
                {
                    "sent": "For example, subject, verb and object.",
                    "label": 0
                },
                {
                    "sent": "But Chinese has a different structure, so for this kind of languages, syntactic syntactic approach has been demonstrated to be very efficient.",
                    "label": 0
                },
                {
                    "sent": "OK, we will see former formalism to tackle with this problem that in our case will be simple stochastic inversion.",
                    "label": 0
                },
                {
                    "sent": "Do transition grammars OK?",
                    "label": 0
                },
                {
                    "sent": "There are approaches not too much is pretty interesting, yet that are based on final stage approaches.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If you have any questions, please don't hesitate to interrupt me, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Summarizing this this introduction we are interested in dealing with problems related to natural language processing using a syntactic approach, and in a more specifically stochastic syntactic approach, probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "We will we will focus in one kind of models.",
                    "label": 0
                },
                {
                    "sent": "In our case, context free grammar, stochastic wanted, remember so.",
                    "label": 0
                },
                {
                    "sent": "So let's see some notation and definitions and some basic.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Everything's OK, context free grammar.",
                    "label": 0
                },
                {
                    "sent": "RA form is a formalism, very interesting formalism.",
                    "label": 0
                },
                {
                    "sent": "That the.",
                    "label": 0
                },
                {
                    "sent": "Was developed some time ago.",
                    "label": 0
                },
                {
                    "sent": "OK, this this kind of models are simple and compact models for parsing.",
                    "label": 1
                },
                {
                    "sent": "For giving for account for the structure of a sentence.",
                    "label": 0
                },
                {
                    "sent": "Yeah there is.",
                    "label": 0
                },
                {
                    "sent": "It is our well understood formal.",
                    "label": 1
                },
                {
                    "sent": "There is exist well understood formal framework.",
                    "label": 0
                },
                {
                    "sent": "Although this is the language is not.",
                    "label": 0
                },
                {
                    "sent": "Regular is almost regular context free grammar, not regular languages, OK?",
                    "label": 0
                },
                {
                    "sent": "Don't define regular languages OK. Yeah, quantify, are adequate for representing long-term syntactic structures, and you will incorporate probabilities to the model.",
                    "label": 1
                },
                {
                    "sent": "They can deal adequately with the ambiguity that arise.",
                    "label": 0
                },
                {
                    "sent": "In natural language processing so.",
                    "label": 0
                },
                {
                    "sent": "Our our framework.",
                    "label": 0
                },
                {
                    "sent": "Our framework.",
                    "label": 0
                },
                {
                    "sent": "To work.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will be defined according to this.",
                    "label": 1
                },
                {
                    "sent": "To this concept we will have a set of primitives that will that we will use in order to compose an object.",
                    "label": 0
                },
                {
                    "sent": "In our case our primitives will be an alphabet.",
                    "label": 0
                },
                {
                    "sent": "But a lot of it is simply a set of symbols.",
                    "label": 0
                },
                {
                    "sent": "Warp words, punctuation symbols, etc that can be put together in order to compose sentences.",
                    "label": 1
                },
                {
                    "sent": "In order to compose sentences.",
                    "label": 0
                },
                {
                    "sent": "And if we have.",
                    "label": 0
                },
                {
                    "sent": "Several sentences, then we have.",
                    "label": 1
                },
                {
                    "sent": "Are a set of sentences that we will use 2.",
                    "label": 0
                },
                {
                    "sent": "It's tracking information from the sentences and our problems will be related with syntactic analysis.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to define adequate methods in order to obtain a possible interpretation.",
                    "label": 0
                },
                {
                    "sent": "From this kind of models.",
                    "label": 0
                },
                {
                    "sent": "So this is, uh, this will be our input and we are interested in the defining methods and model models and methods in order to tackle with this kind of information.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More formally.",
                    "label": 0
                },
                {
                    "sent": "We introduce some notation, more formal notation and alphabet.",
                    "label": 1
                },
                {
                    "sent": "Is Affinity set of symbols.",
                    "label": 1
                },
                {
                    "sent": "A string is composed, but a set of symbols.",
                    "label": 1
                },
                {
                    "sent": "We put together 2 strings.",
                    "label": 0
                },
                {
                    "sent": "We concatenate the strings.",
                    "label": 0
                },
                {
                    "sent": "Then we we are able to construct larger.",
                    "label": 0
                },
                {
                    "sent": "Strings sometimes it is convenient to deal with special strings like empty strings.",
                    "label": 1
                },
                {
                    "sent": "This usually introduce some difficulties and in practical cases sometimes is avoided.",
                    "label": 0
                },
                {
                    "sent": "So the closure of the of the alphabet.",
                    "label": 1
                },
                {
                    "sent": "Is the set of all strings that can be cause composed combining the symbols in any way?",
                    "label": 0
                },
                {
                    "sent": "And a language can be defined.",
                    "label": 0
                },
                {
                    "sent": "It last last subset of this.",
                    "label": 0
                },
                {
                    "sent": "Of these of this set OK. Well, this is this is this is our objects object will be sentences so now.",
                    "label": 0
                },
                {
                    "sent": "We have our projects we now define.",
                    "label": 0
                },
                {
                    "sent": "We need now to define a formalism that can account for this kind of obj.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That in our case will be a grammar so agrama is simple can be defined it formally like a couple like this.",
                    "label": 0
                },
                {
                    "sent": "In which this is a set of nonterminals that is the symbols that account for the internal structure of the sentence.",
                    "label": 0
                },
                {
                    "sent": "A set of terminal symbols, that is, the words of the string.",
                    "label": 0
                },
                {
                    "sent": "This is a set of rules that.",
                    "label": 0
                },
                {
                    "sent": "Define how they don't.",
                    "label": 0
                },
                {
                    "sent": "Terminals can be combined in order to compose sentences.",
                    "label": 0
                },
                {
                    "sent": "But this is the initial symbol of the grammar.",
                    "label": 0
                },
                {
                    "sent": "The actual matter grammar, all the all the all the sentences are composed from this non terminal so.",
                    "label": 0
                },
                {
                    "sent": "This is the formalism that we will use in this in this that we will see in this talk.",
                    "label": 0
                },
                {
                    "sent": "So we have strings.",
                    "label": 0
                },
                {
                    "sent": "We have grammars.",
                    "label": 0
                },
                {
                    "sent": "Now we need some way of relating strings and grammars.",
                    "label": 0
                },
                {
                    "sent": "So for this propose we introduce the concept of derivation.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have a string composed but non terminal symbols and terminal symbols.",
                    "label": 0
                },
                {
                    "sent": "So the concept is derivation is our relation that is established between two strings in such a way?",
                    "label": 0
                },
                {
                    "sent": "That is, we have this string composed but non terminal symbols and terminal symbols.",
                    "label": 0
                },
                {
                    "sent": "We can substitute this nonterminal symbol, but this string if we have a rule.",
                    "label": 0
                },
                {
                    "sent": "In the grammar like this.",
                    "label": 0
                },
                {
                    "sent": "OK, I I will.",
                    "label": 0
                },
                {
                    "sent": "I will illustrate this with some examples.",
                    "label": 0
                },
                {
                    "sent": "So the concept of deviation is introduced in order to relate to in order to relate to strings.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have something like this.",
                    "label": 0
                },
                {
                    "sent": "This is a string composed but terminal and non terminal symbols so.",
                    "label": 0
                },
                {
                    "sent": "We take this known terminal symbol, look for a rule that has this non terminal on the left part of the rule which is the left part of the rule and we expand this symbol 62 in it.",
                    "label": 0
                },
                {
                    "sent": "For this other two symbols.",
                    "label": 0
                },
                {
                    "sent": "So we obtain.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we define a sentential form.",
                    "label": 1
                },
                {
                    "sent": "Lost as a collection of.",
                    "label": 0
                },
                {
                    "sent": "Derivation that.",
                    "label": 0
                },
                {
                    "sent": "Start on the initial symbol on the action of the order grammar so.",
                    "label": 0
                },
                {
                    "sent": "A sentential form looks like this.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I need.",
                    "label": 0
                },
                {
                    "sent": "I need some.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "OK. We start with the initial symbol, produce these two symbols this symbol.",
                    "label": 0
                },
                {
                    "sent": "It's just it buddy to single because we have a rule like this.",
                    "label": 0
                },
                {
                    "sent": "OK. And then and then we shifted to this this terminal, but this terminal.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is a sentential form becauses a set of strings and not strings, that are generated from this non terminal OK.",
                    "label": 1
                },
                {
                    "sent": "So the language generated by the grammar can be defined in this way.",
                    "label": 0
                },
                {
                    "sent": "Is the set of all strings that can be composed from derivations of the grammar.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "If we if I continue with this derivation of things like santyl ideas.",
                    "label": 0
                },
                {
                    "sent": "OK. Tell me.",
                    "label": 0
                },
                {
                    "sent": "Faster screen.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "The closer.",
                    "label": 0
                },
                {
                    "sent": "OK, because if you have this dentist ring will start because you can start with within the industry.",
                    "label": 0
                },
                {
                    "sent": "OK. You need a unit.",
                    "label": 0
                },
                {
                    "sent": "Initially you have something like.",
                    "label": 0
                },
                {
                    "sent": "This issues that we start with in this way.",
                    "label": 0
                },
                {
                    "sent": "We have we don't have any derivation.",
                    "label": 0
                },
                {
                    "sent": "Then we apply elevation the first elevation OK?",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "OK. Show the gram the language.",
                    "label": 0
                },
                {
                    "sent": "The language generated by G can be defined as all the strings that can be formed from.",
                    "label": 1
                },
                {
                    "sent": "The starting with initial symbol on applying rules, spending rules until we achieved a set of only terminal symbols.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "This is closely related to formal to formal language is OK. We can see that the literature for.",
                    "label": 0
                },
                {
                    "sent": "For more information.",
                    "label": 0
                },
                {
                    "sent": "OK, so the grammars can be classified in order, taking into account the how the rules.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "The aspect of the rules so we can introduce at least that more than one type over grandmas.",
                    "label": 0
                },
                {
                    "sent": "For example, counter free grammar and regular grammars in in the context, all the rules are of this way we have only here only one known non terminal that can produce.",
                    "label": 0
                },
                {
                    "sent": "A string of terminals or non terminals.",
                    "label": 1
                },
                {
                    "sent": "So we will focus only in this kind of models.",
                    "label": 0
                },
                {
                    "sent": "OK, there exists another another kind of grammars for a simple.",
                    "label": 0
                },
                {
                    "sent": "This grammar are regular grammars more simpler.",
                    "label": 1
                },
                {
                    "sent": "OK, so we will focus in this kind of grammars.",
                    "label": 0
                },
                {
                    "sent": "In this kind of grammars, the right part there is no restriction on the right part, but there exists authoring that demonstrated there exists a special way or writing the rules in such a way that all every general grammar can be right.",
                    "label": 0
                },
                {
                    "sent": "In that way I'm talking about the chunky normal form.",
                    "label": 0
                },
                {
                    "sent": "That means that every context free grammar.",
                    "label": 0
                },
                {
                    "sent": "Can be writing in this way.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Removing the empty string.",
                    "label": 0
                },
                {
                    "sent": "All every grammar can be writing in such a way that all the rules are of this kind or of this kind.",
                    "label": 0
                },
                {
                    "sent": "OK, this is known as Chomsky normal form.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The algorithms that we will see here deal.",
                    "label": 0
                },
                {
                    "sent": "With grammars in this format, in Chomsky normal form, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, there is a there is a closely relation between part three and derivations we can represent.",
                    "label": 0
                },
                {
                    "sent": "The generation process resting with a derivation or with a partial tree is the same OK. Well, let's continue.",
                    "label": 0
                },
                {
                    "sent": "Well, we have a strings.",
                    "label": 0
                },
                {
                    "sent": "We have grammars and now we need a way or connect or relation or relating both concept.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the carry out through the concept of parsing.",
                    "label": 0
                },
                {
                    "sent": "So the problem parsing relates agreement with a string agreement with a string.",
                    "label": 0
                },
                {
                    "sent": "So there are several approaches in order to relate to.",
                    "label": 0
                },
                {
                    "sent": "There are several approaches related to parsing.",
                    "label": 0
                },
                {
                    "sent": "So for example, top down parsing down top, parsing, and another possibilities.",
                    "label": 0
                },
                {
                    "sent": "In our case, we will focus in this algorithm, the CKY algorithm.",
                    "label": 0
                },
                {
                    "sent": "That is, algorithm that is an algorithm.",
                    "label": 0
                },
                {
                    "sent": "That in the in this case is on top there with him we will see this with him in the following slide is a dynamic programming are within just a dynamic programming algorithm.",
                    "label": 0
                },
                {
                    "sent": "So let's see there with him I will explain.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More detail.",
                    "label": 0
                },
                {
                    "sent": "This is the Coke kasama yung algorithm.",
                    "label": 0
                },
                {
                    "sent": "The writing was developed by these three outsource and the input to the algorithm is a grammar in Chomsky normal form and a string, and they are the algorithm is able to to tell us if the sentence belongs to the language generated by the grammar.",
                    "label": 0
                },
                {
                    "sent": "OK, I will see an example in the in the following slide.",
                    "label": 0
                },
                {
                    "sent": "Let's see my structure.",
                    "label": 0
                },
                {
                    "sent": "It is a dynamic programming algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to fill in a table in which each cell.",
                    "label": 0
                },
                {
                    "sent": "We, in which itself contains this non terminal and this means that these non terminal is able to generate this substring using one or more derivations.",
                    "label": 0
                },
                {
                    "sent": "OK, you must take into account, for example that if we have.",
                    "label": 0
                },
                {
                    "sent": "R. Context free grammar like this?",
                    "label": 0
                },
                {
                    "sent": "Simply like this, we can generate three like this.",
                    "label": 0
                },
                {
                    "sent": "In order to produce this string or we can use, we can have another string like this.",
                    "label": 0
                },
                {
                    "sent": "So we have two parse trees for the same string.",
                    "label": 0
                },
                {
                    "sent": "OK, two different structures for the same string.",
                    "label": 0
                },
                {
                    "sent": "OK, so they always seem proceeds in the following way.",
                    "label": 0
                },
                {
                    "sent": "It's a bottom up algorithm that take into account a.",
                    "label": 0
                },
                {
                    "sent": "Short substrings of the of the of the sentence and.",
                    "label": 0
                },
                {
                    "sent": "In fact, it starts with strings of sight one.",
                    "label": 0
                },
                {
                    "sent": "Then of size 2 of size 3 etc until it accounts for all the string.",
                    "label": 0
                },
                {
                    "sent": "So we consider first in the initialization.",
                    "label": 0
                },
                {
                    "sent": "So a substrings of length one, so we include in this cell this non terminal.",
                    "label": 0
                },
                {
                    "sent": "We have a rule like this.",
                    "label": 0
                },
                {
                    "sent": "Shut this.",
                    "label": 0
                },
                {
                    "sent": "That this symbol.",
                    "label": 0
                },
                {
                    "sent": "Is the same of the similarities in this position, and then we start to analyze substrings of larger size, size 2, size 3.",
                    "label": 0
                },
                {
                    "sent": "So this index.",
                    "label": 0
                },
                {
                    "sent": "Ranges for all possible substrings.",
                    "label": 0
                },
                {
                    "sent": "These index ranges for all possible positions and this index explore the possible ways in which we can divide 1 string, so finally.",
                    "label": 0
                },
                {
                    "sent": "When the algorithm ends is able we are able to to answer this question.",
                    "label": 0
                },
                {
                    "sent": "If this not this non terminal is in the top cell, so the string belongs to language OK.",
                    "label": 0
                },
                {
                    "sent": "The important thing to note in this in this slide is that we have an algorithm that is cubic.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have an example, so this we have this this program A.",
                    "label": 0
                },
                {
                    "sent": "This is the the the table that we are feeling through the algorithm.",
                    "label": 0
                },
                {
                    "sent": "We start filling these cells for example.",
                    "label": 0
                },
                {
                    "sent": "This means that these non terminal is able to to account for this symbol because we have a rule like this.",
                    "label": 0
                },
                {
                    "sent": "OK the same for this and this information.",
                    "label": 0
                },
                {
                    "sent": "For example this means that we have a rule that is able to account for.",
                    "label": 0
                },
                {
                    "sent": "These non terminal and is non terminal S. Generate A and S. So if we include this non terminal in this in this cell and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, we proceed in this way completing the table at the end.",
                    "label": 0
                },
                {
                    "sent": "If this symbol appears in this cell, we can say that the string belong to the language generated accepted by the.",
                    "label": 0
                },
                {
                    "sent": "They extend string is accepted by the by the grammar, yes.",
                    "label": 0
                },
                {
                    "sent": "Numbers, as you know.",
                    "label": 0
                },
                {
                    "sent": "This is in order to clarify how this item has been composed.",
                    "label": 0
                },
                {
                    "sent": "This sitting can be composed from this item.",
                    "label": 0
                },
                {
                    "sent": "I'm from this item.",
                    "label": 0
                },
                {
                    "sent": "OK is in order to clarify or something.",
                    "label": 0
                },
                {
                    "sent": "They're everything OK, so they always has time complexity as quick and the space complexity is quadratic with the length of the sentence.",
                    "label": 0
                },
                {
                    "sent": "But there is another dimension related with the number of symbols.",
                    "label": 0
                },
                {
                    "sent": "OK questions.",
                    "label": 0
                },
                {
                    "sent": "So we have strings.",
                    "label": 0
                },
                {
                    "sent": "Note that we have strings, we have grammars and we have a way of relating the strings with grammars.",
                    "label": 0
                },
                {
                    "sent": "Now we are.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adding some information to this formalism that we have introduced that in our case will be probabilities.",
                    "label": 0
                },
                {
                    "sent": "We're incorporating probabilities to the to this kind of models.",
                    "label": 0
                },
                {
                    "sent": "We are interested in.",
                    "label": 0
                },
                {
                    "sent": "Adding tools for solving this problem, we would like to solve problem ladies.",
                    "label": 0
                },
                {
                    "sent": "OK, so we extend the concept of language with stochastic language faster.",
                    "label": 0
                },
                {
                    "sent": "Stochastic language is just language with that has apparently.",
                    "label": 0
                },
                {
                    "sent": "A probability associated to each string in such a way that this this condition must be accomplished OK. For example, here we have a possible alphabet.",
                    "label": 0
                },
                {
                    "sent": "This is a language.",
                    "label": 0
                },
                {
                    "sent": "This is, this language is well known is the palindrome language.",
                    "label": 0
                },
                {
                    "sent": "That has the same number, the same.",
                    "label": 0
                },
                {
                    "sent": "Member of A and this OK.",
                    "label": 0
                },
                {
                    "sent": "This is a context free language, so we define this this function.",
                    "label": 0
                },
                {
                    "sent": "This is a probabilistic language.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We have incorporated probability to the language, so we need to incorporate probabilities to the model.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We extend the concept of of context free grammar.",
                    "label": 0
                },
                {
                    "sent": "We in this way we introduce the concept of probabilistic context free grammar, probabilistic context free grammars just stop all define it.",
                    "label": 0
                },
                {
                    "sent": "In this way G is a grammar, a context free grammar and P is a probability that is a function that associated probability to each role in such a way that this this condition must be accomplished.",
                    "label": 0
                },
                {
                    "sent": "That is then that means that the we add the probabilities.",
                    "label": 0
                },
                {
                    "sent": "Of the rules that has that have the same nonterminal on the left part of the rule, this value must be one.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have probability languages we have.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic models to account for this kind of languages.",
                    "label": 0
                },
                {
                    "sent": "So we need to again to relate these concepts.",
                    "label": 0
                },
                {
                    "sent": "In the same way.",
                    "label": 0
                },
                {
                    "sent": "OK, so this concept is related through the introduces concepts in order to relate probability languages with the probabilistic context free grammar.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we we incorporate stochastic information to the concept of derivation.",
                    "label": 0
                },
                {
                    "sent": "So if we have this derivation.",
                    "label": 0
                },
                {
                    "sent": "In which we have used this rule.",
                    "label": 0
                },
                {
                    "sent": "This rule this rule OK so.",
                    "label": 0
                },
                {
                    "sent": "The probability of this derivation.",
                    "label": 0
                },
                {
                    "sent": "Is defined as the probability of of the product of the probability of the rule that participate in it in that derivation product of this derivation with this event with this event with this event.",
                    "label": 1
                },
                {
                    "sent": "So this is the this is the correct definition.",
                    "label": 0
                },
                {
                    "sent": "But given that this is very difficult to compute, we introduce some restrictions and we shoot this especially with this expression.",
                    "label": 0
                },
                {
                    "sent": "So we eliminate this dependency and we compute the probability of a derivation as the product.",
                    "label": 0
                },
                {
                    "sent": "As the product of the rules that participate, and in that derivation.",
                    "label": 0
                },
                {
                    "sent": "OK. Christians.",
                    "label": 0
                },
                {
                    "sent": "Ignore.",
                    "label": 0
                },
                {
                    "sent": "Like transitions for complex breeze.",
                    "label": 0
                },
                {
                    "sent": "For the dependencies.",
                    "label": 0
                },
                {
                    "sent": "Your properties on your grammars.",
                    "label": 0
                },
                {
                    "sent": "Are you asking me in some way the model is consistent?",
                    "label": 0
                },
                {
                    "sent": "I I. Yeah, I know, I know, I know, I know, I know, I know.",
                    "label": 0
                },
                {
                    "sent": "But we will see some properties that prove that the model is consistent with these well defined.",
                    "label": 0
                },
                {
                    "sent": "Let me let me see, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, given given this these concepts so we introduce this disconcerts probability of a delegation that is defined in this way.",
                    "label": 1
                },
                {
                    "sent": "Sometimes interest is it is interesting to define the probability in this way.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the probability of string can be defined as the addition of all of the probability of all the derivation that can compose this string.",
                    "label": 1
                },
                {
                    "sent": "You must take into account that sometime this set, this is the set of possible derivation.",
                    "label": 0
                },
                {
                    "sent": "This set can have a. Exponential size, so this is a way of representing this information OK.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it is interesting to compute, not the probability of the string, but the probability of the derivation that is able to two accounts for the string.",
                    "label": 0
                },
                {
                    "sent": "So we must substitute this addition with maximization and sometimes we can take an intermediate position and sometimes it is interesting to find the probability of not taking into account.",
                    "label": 0
                },
                {
                    "sent": "All the notable derivation and not only one derivation, but a set of derivation we have introduced this concept in order two general to see destination algorithms in a general way OK.",
                    "label": 0
                },
                {
                    "sent": "So finally, the language generated by a context free grammar, probabilistic context, Can we define it in this way?",
                    "label": 1
                },
                {
                    "sent": "That is, the this apologetic programmer is just the set of all the string that can be composed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, playing derivation and which probability is greater than 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I will try now to answer some of your doubts so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you see we have a grammars, probabilistic languages, probabilistic grammars away so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An interesting question is is.",
                    "label": 0
                },
                {
                    "sent": "Every language that language can be represented by a probabilistic grammar.",
                    "label": 0
                },
                {
                    "sent": "It's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "So the answer is no.",
                    "label": 0
                },
                {
                    "sent": "You have here an example.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "This language processing language cannot be represented by the contest for grammar.",
                    "label": 0
                },
                {
                    "sent": "Maybe because of the definition that we have introduced OK. Another interesting, interesting question is every probabilistic grammar is able to generate a probabilistic language.",
                    "label": 0
                },
                {
                    "sent": "The answer is no.",
                    "label": 0
                },
                {
                    "sent": "Again so.",
                    "label": 0
                },
                {
                    "sent": "And there are some examples, for example.",
                    "label": 0
                },
                {
                    "sent": "This grammar.",
                    "label": 0
                },
                {
                    "sent": "Is not.",
                    "label": 0
                },
                {
                    "sent": "Consistent if P is larger than zero point point.",
                    "label": 0
                },
                {
                    "sent": "This comment is not able to generate a probabilistic language.",
                    "label": 0
                },
                {
                    "sent": "The idea is clear if we.",
                    "label": 0
                },
                {
                    "sent": "We start to generate symbols.",
                    "label": 0
                },
                {
                    "sent": "Every time that we choose this this rule, we introduce those two nonterminals.",
                    "label": 0
                },
                {
                    "sent": "Two new student terminals.",
                    "label": 0
                },
                {
                    "sent": "So the generating process can may not finish.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But questions?",
                    "label": 0
                },
                {
                    "sent": "OK, well now we'll see algorithms that can relate strings.",
                    "label": 0
                },
                {
                    "sent": "Probably a stranger from a language from a political language with strings.",
                    "label": 0
                },
                {
                    "sent": "We start with.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we introduced the Insight algorithm that is well known algorithm for parsing.",
                    "label": 0
                },
                {
                    "sent": "That the.",
                    "label": 0
                },
                {
                    "sent": "Was that became popular from this paper, but there are previous.",
                    "label": 0
                },
                {
                    "sent": "There is a previous paper from Baker.",
                    "label": 0
                },
                {
                    "sent": "And again, this this is an algorithm that is exactly the same as the CKY algorithm.",
                    "label": 0
                },
                {
                    "sent": "If we compute this value, that is the probability that this non terminal generated this substring.",
                    "label": 0
                },
                {
                    "sent": "So this is the algorithm is a dynamic.",
                    "label": 0
                },
                {
                    "sent": "Again is a dynamic programming algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is the initialization.",
                    "label": 0
                },
                {
                    "sent": "So and this is the, this is the initialization and this is the recursion.",
                    "label": 0
                },
                {
                    "sent": "So we are in all these values and is the same algorithm and in this way we have an algorithm to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem, does in this case has a time complexity that again each cubic cubic with the length of the string linear with the size of the model.",
                    "label": 0
                },
                {
                    "sent": "They.",
                    "label": 0
                },
                {
                    "sent": "Note that this is a hard restriction in order to use this kind of models in real applications.",
                    "label": 0
                },
                {
                    "sent": "OK, so there have been some improvements.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this algorithm, possible improvement is to incorporate some in formation in the strings in order to limit 8 the search space.",
                    "label": 1
                },
                {
                    "sent": "This information was introduced in this paper, and the idea was to incorporate brackets to the string.",
                    "label": 0
                },
                {
                    "sent": "Markets that sometimes are linguistically motivated.",
                    "label": 0
                },
                {
                    "sent": "For example, if we are able to define a way of of bracket bracket the the noun phrase and a brief ratio, we have some information we can.",
                    "label": 0
                },
                {
                    "sent": "For we can take profit.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this way we can modify the algorithm which simple function that is 1.",
                    "label": 0
                },
                {
                    "sent": "If this is a.",
                    "label": 0
                },
                {
                    "sent": "The division of the string does not overlap the restriction imposed by the bracket.",
                    "label": 1
                },
                {
                    "sent": "For example, we could in this example we could not associate this word with this word because these division in this point OK.",
                    "label": 1
                },
                {
                    "sent": "If we implement adequately this information, so we have a full bracketed sentence, then everything becomes.",
                    "label": 0
                },
                {
                    "sent": "Linear.",
                    "label": 0
                },
                {
                    "sent": "OK, that's big, why?",
                    "label": 0
                },
                {
                    "sent": "Because we have the past three.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Simply OK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the central theme that allows us to compute this probability.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and we want to obtain not the probability of the string, but the best interpretation of the string that is the best Part 3.",
                    "label": 0
                },
                {
                    "sent": "The most probably tree.",
                    "label": 0
                },
                {
                    "sent": "Then we can we can introduce on everything that we now know as bitter beer with him.",
                    "label": 0
                },
                {
                    "sent": "This algorithm is similar to the Beat LVL rating that is used for hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "OK the.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He is similar to this, but we have we have substitute this.",
                    "label": 0
                },
                {
                    "sent": "Addition by a maximisation.",
                    "label": 0
                },
                {
                    "sent": "Simply OK, another interesting algorithm for parsing in this case is down top.",
                    "label": 0
                },
                {
                    "sent": "Is the.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The site algorithm we introduced this algorithm cause this algorithm is very important for the estimation of the model of the probability of the model so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, simply we define this probability that this is the probability that the initial and terminal generated string.",
                    "label": 0
                },
                {
                    "sent": "Until let me see.",
                    "label": 0
                },
                {
                    "sent": "The Insight algorithm compute this probability of this part of the string and the sight probability is able to compute this probability.",
                    "label": 0
                },
                {
                    "sent": "Starting from this symbol.",
                    "label": 0
                },
                {
                    "sent": "This is the inside probability.",
                    "label": 0
                },
                {
                    "sent": "Compute this probability.",
                    "label": 0
                },
                {
                    "sent": "Are they all the parts tree that appear in this in this place, and the same probability is able to compute all the trees that are that generate this part on this part and generate this string?",
                    "label": 0
                },
                {
                    "sent": "The symbol and this symbol?",
                    "label": 0
                },
                {
                    "sent": "Have to account for this substring.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's again a dynamic programming algorithm.",
                    "label": 0
                },
                {
                    "sent": "There the team has two parts, 1 four.",
                    "label": 0
                },
                {
                    "sent": "This part, another one for this part, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is everything that I repeat.",
                    "label": 0
                },
                {
                    "sent": "I introduce each other thing because it is important for the estimation of the models.",
                    "label": 0
                },
                {
                    "sent": "OK, if we have bracket information again, the only thing can be linearly 'cause if we don't have bracketing information.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is cubic.",
                    "label": 0
                },
                {
                    "sent": "OK questions.",
                    "label": 0
                },
                {
                    "sent": "Finally, we introduce an algorithm that is very important for.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Language modeling that is that in this case is the left to right inside algorithm is again up or dynamic programming algorithm.",
                    "label": 0
                },
                {
                    "sent": "And it'll wash, compute the probability that the initial non terminal of the grammar is able to compute an initial superstring.",
                    "label": 0
                },
                {
                    "sent": "So it is there a team is able to compute this probability OK. We need some pre computations that are not related with the input, just with the model.",
                    "label": 0
                },
                {
                    "sent": "And then we define algorithms similar to the to the to the previous algorithms.",
                    "label": 0
                },
                {
                    "sent": "In this case we need the insight everything in this in this.",
                    "label": 0
                },
                {
                    "sent": "In this computation, OK, there is also quick questions.",
                    "label": 0
                },
                {
                    "sent": "No well.",
                    "label": 0
                },
                {
                    "sent": "So let's continue with this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With the with this point and let's start to and let go to speak about probabilistic estimation, probabilistic estimation of probabilistic context, free grammar, OK?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, as you have you have seen so such as we have seen, the grammar has two parts.",
                    "label": 0
                },
                {
                    "sent": "The stochastic part and this too proud part the stochastic part are the probabilities.",
                    "label": 1
                },
                {
                    "sent": "Is there is there are the qualities and the structural part is composed by the rules of the grammar we will focus here on the learning of the probabilities, not on the learning of the rules.",
                    "label": 0
                },
                {
                    "sent": "The learning of the rules is related with other virtual field that is known as grammatical inference OK?",
                    "label": 0
                },
                {
                    "sent": "We will focus repeat.",
                    "label": 0
                },
                {
                    "sent": "With the learning of the of the probabilities, not the rules, not the structural part, OK?",
                    "label": 0
                },
                {
                    "sent": "There exist some work, some words that combine the learning of both of both parts, but not.",
                    "label": 0
                },
                {
                    "sent": "I will not review this in this lecture.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our problem is to estimate the probabilities of the model so we we have several frameworks.",
                    "label": 0
                },
                {
                    "sent": "For example, we could learn the probabilities of the model from in a supervisor right away.",
                    "label": 0
                },
                {
                    "sent": "That is a. Analyzing taking the rules directly from the trees.",
                    "label": 0
                },
                {
                    "sent": "A from the trees of a sentence.",
                    "label": 0
                },
                {
                    "sent": "But note that in this case we need to annotate that information.",
                    "label": 0
                },
                {
                    "sent": "And this is very expensive.",
                    "label": 0
                },
                {
                    "sent": "So in this store in this part of the talk we will focus on non supervisor methods.",
                    "label": 0
                },
                {
                    "sent": "That is, we don't have any information about the annotation of the string, so we have Brown arrow sentence.",
                    "label": 0
                },
                {
                    "sent": "OK so for this we have a we have a annotated data that is trees.",
                    "label": 1
                },
                {
                    "sent": "So the usual way is to take the rules directly from the from the tree and count and normalize and in this way.",
                    "label": 0
                },
                {
                    "sent": "We have a a possible a very good woman.",
                    "label": 0
                },
                {
                    "sent": "In fact, this is the usual way.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that it is very expensive to have available.",
                    "label": 0
                },
                {
                    "sent": "Trees that is a tree bank OK. A tree for each sentence.",
                    "label": 0
                },
                {
                    "sent": "OK, in fact there are few corpora annotated in this way.",
                    "label": 0
                },
                {
                    "sent": "OK, so we will focus on non supervisor methods so that we don't have any information about the.",
                    "label": 0
                },
                {
                    "sent": "They are not the the structural part of the of the string, so the algorithm algorithms that we will see here is.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same as the the EM algorithm.",
                    "label": 1
                },
                {
                    "sent": "The spectation maximization algorithm, but I will present the estimation of it in another way.",
                    "label": 0
                },
                {
                    "sent": "Under one of the problem of this method is that usually they are able to achieve a local optimum.",
                    "label": 0
                },
                {
                    "sent": "OK, well let's see the framework in which we explain the.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Estimation everything, so suppose that we have a grammar.",
                    "label": 0
                },
                {
                    "sent": "With a set of parameters, that is, this set of parameters are the rules, or sorry, the priority of the rules and we have a sample of strings and we want to optimize this function.",
                    "label": 0
                },
                {
                    "sent": "So we want to define a function to be optimized and we want to achieve the best parameters that account for state function.",
                    "label": 0
                },
                {
                    "sent": "In our case we will this so we need a function and optimization framework.",
                    "label": 0
                },
                {
                    "sent": "In our case, the optimization frameworks will be the growth transformations and the optimization function in our case will be then.",
                    "label": 1
                },
                {
                    "sent": "Likelihood OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see what this growth information.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have a function, are normal generous polynomial with non negative coefficients OK?",
                    "label": 0
                },
                {
                    "sent": "And let sit and be appointed defined in this way.",
                    "label": 0
                },
                {
                    "sent": "This is a point in this domain.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This is the problem.",
                    "label": 0
                },
                {
                    "sent": "The probabilities of the rules I will.",
                    "label": 0
                },
                {
                    "sent": "I will write an example.",
                    "label": 0
                },
                {
                    "sent": "So in that way that the the addition of of some part of the of the of this of this point at one.",
                    "label": 0
                },
                {
                    "sent": "OK, and we apply this information.",
                    "label": 0
                },
                {
                    "sent": "Let me write an example.",
                    "label": 0
                },
                {
                    "sent": "OK, this is theater, you know 113 to 12.",
                    "label": 0
                },
                {
                    "sent": "13 sister 21622 Cedar Tree one.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Wait wait wait wait wait I need I need another Hunter.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "A possible point in that domain, OK?",
                    "label": 0
                },
                {
                    "sent": "So we have a polynomial and we have at this domain.",
                    "label": 0
                },
                {
                    "sent": "If we apply this transformation.",
                    "label": 0
                },
                {
                    "sent": "This information takes a point on obtained another point.",
                    "label": 1
                },
                {
                    "sent": "That is, take this point.",
                    "label": 0
                },
                {
                    "sent": "And obtains another point.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In the same domain.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we apply this information the we can guarantee that the that we are able to increase the polynomial is the polynomial is the likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So applying successively this iteratively, these.",
                    "label": 0
                },
                {
                    "sent": "This transformation we are optimizing the likelihood function until we achieve the convergence.",
                    "label": 0
                },
                {
                    "sent": "This is just the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it's like multiplication of the title.",
                    "label": 0
                },
                {
                    "sent": "Maker app.",
                    "label": 0
                },
                {
                    "sent": "This this is this is where we are.",
                    "label": 0
                },
                {
                    "sent": "The differential this is a point, a specific point, and this is our derivation 62 in the current point.",
                    "label": 0
                },
                {
                    "sent": "Who will let me see?",
                    "label": 0
                },
                {
                    "sent": "Let me continue because I have the the I will we will use for defining.",
                    "label": 0
                },
                {
                    "sent": "Estimation method OK so suppose.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we have this function.",
                    "label": 0
                },
                {
                    "sent": "OK. That in this case we have we are using just not only not all the derivation, but just a few derivations OK. A few derivations.",
                    "label": 0
                },
                {
                    "sent": "If we take all the derivations, then we have the probability of the string and if we take the best innovation, we have the Viterbi probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so we apply the.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transformation.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To this.",
                    "label": 0
                },
                {
                    "sent": "To this function we achieve this expression.",
                    "label": 0
                },
                {
                    "sent": "We can see the demonstration in this slides please.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a take a look OK?",
                    "label": 0
                },
                {
                    "sent": "OK, see the details in the demonstration.",
                    "label": 0
                },
                {
                    "sent": "OK, so we see here what we're doing is.",
                    "label": 0
                },
                {
                    "sent": "The the probability of the rule is simply becauses only the this is the one divided by the probability of the of the string.",
                    "label": 0
                },
                {
                    "sent": "With this set of deviation.",
                    "label": 0
                },
                {
                    "sent": "This is the product of the division and this is the number of times that this rule has been used in this celebration.",
                    "label": 0
                },
                {
                    "sent": "We are in this value for all possible derivation.",
                    "label": 0
                },
                {
                    "sent": "We simply are counting an average use of the rule in the derivations.",
                    "label": 0
                },
                {
                    "sent": "That we are using for defining the probability of the other string.",
                    "label": 0
                },
                {
                    "sent": "This is the average count.",
                    "label": 0
                },
                {
                    "sent": "The number of times that we are using the rule simply is OK.",
                    "label": 0
                },
                {
                    "sent": "So if we introduce this probability here, this is a probability.",
                    "label": 0
                },
                {
                    "sent": "This is a priority over the hidden data.",
                    "label": 0
                },
                {
                    "sent": "In this case the derivation.",
                    "label": 0
                },
                {
                    "sent": "And this is our statistic.",
                    "label": 0
                },
                {
                    "sent": "We account we are at this information.",
                    "label": 0
                },
                {
                    "sent": "We are obtaining sufficient statistics in order to compute the probability of the new probability of the rule.",
                    "label": 0
                },
                {
                    "sent": "Simply this and this is just a normalization factor in which we are counting the number of times that it's non terminal appears in each position so.",
                    "label": 0
                },
                {
                    "sent": "We only.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we have a string like this.",
                    "label": 0
                },
                {
                    "sent": "So we are looking we are.",
                    "label": 0
                },
                {
                    "sent": "We are looking this rule on this position on all positions.",
                    "label": 0
                },
                {
                    "sent": "OK. And each the and it's time.",
                    "label": 0
                },
                {
                    "sent": "That the rules are the rules appear in what division we had one, but not one.",
                    "label": 0
                },
                {
                    "sent": "But the average weight of that derivation in the division, in that it participates OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "First we set the parameters like.",
                    "label": 0
                },
                {
                    "sent": "We set the parameters for production.",
                    "label": 0
                },
                {
                    "sent": "Then we we apply it information, we optimize the function and repeat the process.",
                    "label": 0
                },
                {
                    "sent": "Again, like.",
                    "label": 0
                },
                {
                    "sent": "I don't know, it says first we decompose it.",
                    "label": 0
                },
                {
                    "sent": "First we parse the playing the part three and then we called the link and we update the wait a minute, wait a minute.",
                    "label": 0
                },
                {
                    "sent": "The computation is another problem you must take into account that this computation can be carried out in this way because the number of delegation.",
                    "label": 0
                },
                {
                    "sent": "If we take we take all the population in this set can be half exponential size, so this is a way of seeing the estimation process.",
                    "label": 0
                },
                {
                    "sent": "OK, so we define the.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "That is, the probability of the.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take all the possible derivations.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are able to write this expression in this other way.",
                    "label": 0
                },
                {
                    "sent": "And this is the classical inside outside algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, here we have.",
                    "label": 0
                },
                {
                    "sent": "Their child probability, the inside probability and then share probability.",
                    "label": 0
                },
                {
                    "sent": "By the property of the Rule 4.",
                    "label": 0
                },
                {
                    "sent": "The demonstration for data is or about how to.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to transform this ship?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asian in this expression is easy.",
                    "label": 0
                },
                {
                    "sent": "OK, you can see demonstration in one of the pennies.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have a possible way of estimating we only count.",
                    "label": 0
                },
                {
                    "sent": "We we we use we apply the inside the site and this transformation the inside the site and this information.",
                    "label": 0
                },
                {
                    "sent": "Then inside the site and information.",
                    "label": 0
                },
                {
                    "sent": "This information has also the cubic complexity.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, if we use not the set of derivation, but the best.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have this function and destination destination expression that we obtained.",
                    "label": 0
                },
                {
                    "sent": "Is this that in this case?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's very easy because we are taking only one derivation.",
                    "label": 0
                },
                {
                    "sent": "Here are one derivation in this additional so this this term Council with this term and we have here only the number of times that are all having used in the best derivation.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is their expression that we obtain.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So, but well, but there will be only theme destination with the bit algorithm that is also that we also call.",
                    "label": 0
                },
                {
                    "sent": "Everything.",
                    "label": 0
                },
                {
                    "sent": "It's a little strange because in this in this expression there there is implicitly a maximization.",
                    "label": 0
                },
                {
                    "sent": "So one can say, well, can we differentiate differentiate with this maximization in the process so the.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything runs in the following way so.",
                    "label": 0
                },
                {
                    "sent": "We obtained the best derivation for each.",
                    "label": 0
                },
                {
                    "sent": "For each string, we obtain the reservation.",
                    "label": 0
                },
                {
                    "sent": "So once we have obtained the restoration, the transformation, the growth transformation warranties that this value is less than the value.",
                    "label": 0
                },
                {
                    "sent": "So this is the probability of each string with the battery derivation, and this is the probability of each thing with derivation with but with the new parameters and this and this is less or equal.",
                    "label": 0
                },
                {
                    "sent": "Done this because of the transformation and in the following step we look for the best elevation with the new with the new parameters so we can guarantee that it is true that this is true.",
                    "label": 0
                },
                {
                    "sent": "So we optimizing.",
                    "label": 0
                },
                {
                    "sent": "We're maximizing and then we can guarantee that this expression is less than this, especially not the difference is in this case is here in the parameters and in this case is in the derivation, so we can guarantee that this value.",
                    "label": 0
                },
                {
                    "sent": "Less than this value.",
                    "label": 0
                },
                {
                    "sent": "OK, how they better be algorithm run?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this way, this is the maximization the transformation the maximization.",
                    "label": 0
                },
                {
                    "sent": "Letter formation and so on OK.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So this everything has this estimation.",
                    "label": 0
                },
                {
                    "sent": "Algorithms have very interesting properties I will summarize.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of them.",
                    "label": 0
                },
                {
                    "sent": "So some of the properties can be expressed introducing some concepts in this case is the concept of probabilistic estimate expectation matrix that can be defined in this way.",
                    "label": 0
                },
                {
                    "sent": "That is, this means the number of.",
                    "label": 0
                },
                {
                    "sent": "Non terminals labeled with this with this index that are introduced with non terminals.",
                    "label": 0
                },
                {
                    "sent": "Elaborate with index this index so.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the definition or space of expectation material of this.",
                    "label": 0
                },
                {
                    "sent": "This is the definition of expectation matrix.",
                    "label": 0
                },
                {
                    "sent": "So we can guarantee that if we estimate the probabilities with these models, the grammar is consistent.",
                    "label": 0
                },
                {
                    "sent": "So we have well defined models, so we can see the details here.",
                    "label": 0
                },
                {
                    "sent": "The idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "The idea is to show that this is that.",
                    "label": 0
                },
                {
                    "sent": "That this matrix converter converts.",
                    "label": 0
                },
                {
                    "sent": "Converges OK.",
                    "label": 0
                },
                {
                    "sent": "There are another another interesting properties that are having that I have not writing here Becauses too large.",
                    "label": 0
                },
                {
                    "sent": "But when we estimate a grammar with a sample, the grammar is able to learn not only the distribution but also the average length of the of the of the string of the sample is the same of the.",
                    "label": 0
                },
                {
                    "sent": "Average length of the string generated by the grammar and another probabilities.",
                    "label": 0
                },
                {
                    "sent": "You can see the details in this paper.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So well, this everything you have seen in some way are very similar becaused we have seen that they come derivated from the same expression.",
                    "label": 0
                },
                {
                    "sent": "So in the practice, how, how?",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Run OK, let's hear.",
                    "label": 0
                },
                {
                    "sent": "Some experiments that show in practical in the in practice how they run.",
                    "label": 0
                },
                {
                    "sent": "We have a used in this case at Toy example.",
                    "label": 0
                },
                {
                    "sent": "In this case, that example is the palindrome language.",
                    "label": 1
                },
                {
                    "sent": "So these are we have this language and we have this model that is able to generate the strings of this kind.",
                    "label": 0
                },
                {
                    "sent": "So we what we carry out was to generate a set of strings and then we train we trained.",
                    "label": 0
                },
                {
                    "sent": "A grammar from these strings.",
                    "label": 0
                },
                {
                    "sent": "OK, for this purpose we generate a grammar.",
                    "label": 0
                },
                {
                    "sent": "That in this question was an exotic model that is a model that has all the possible rules that can be composed with a given number of terminals and a given number of non terminals.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Here we have.",
                    "label": 0
                },
                {
                    "sent": "This set of sorry.",
                    "label": 0
                },
                {
                    "sent": "Of non terminals at this and this just only one terminal.",
                    "label": 0
                },
                {
                    "sent": "So we compose all the possible rules.",
                    "label": 0
                },
                {
                    "sent": "As produce as fast as produce produces.",
                    "label": 0
                },
                {
                    "sent": "As a as etc all possible roles.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "So in this case we are in the worst case because we don't have any information about the structure of the language, so we estimate this grammar and then we evaluate it.",
                    "label": 0
                },
                {
                    "sent": "The goodness of the model by measuring the cool bag labeled emergencies in this case, so we can see that the inside or outside the rhythm is able to learn better the distribution.",
                    "label": 0
                },
                {
                    "sent": "Then we also generated strings with the learned grammar and we then use this grammar to see how how many of the of the of the strings were palindromes.",
                    "label": 1
                },
                {
                    "sent": "So in this case for this example.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The number of palindrome generated by a gram estimated with a little beer with him, or with insight or celebrating the difference is very large.",
                    "label": 0
                },
                {
                    "sent": "OK, I think that I tried different.",
                    "label": 0
                },
                {
                    "sent": "Initially change phones for for the grammar random initializations.",
                    "label": 0
                },
                {
                    "sent": "I think that I took the better for for showing this value, but in general the number of linear motion the Poly offer in terms is is very very low.",
                    "label": 0
                },
                {
                    "sent": "Not larger than this, not as large as this as this so.",
                    "label": 0
                },
                {
                    "sent": "This this is a.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the properties and another property is that, well, the algorithm, both algorithms for estimating grammar and they proceed in the following way.",
                    "label": 0
                },
                {
                    "sent": "The insight to settle with him.",
                    "label": 0
                },
                {
                    "sent": "Estimate the probability of the rules and if you must, take into account that if the model has no.",
                    "label": 0
                },
                {
                    "sent": "Less symbols so all the symbols produce a string and participate in a derivation so.",
                    "label": 0
                },
                {
                    "sent": "Then all the rules appear at least in one place of the one derivation.",
                    "label": 0
                },
                {
                    "sent": "Show is sometimes the convergence process takes long time, very long time.",
                    "label": 0
                },
                {
                    "sent": "As you can imagine, and this is illustrated in this.",
                    "label": 0
                },
                {
                    "sent": "In this example we need this number of iterations to achieve some convergency, while the Viterbi algorithm the rhythm if rule doesn't appear in the best derivation.",
                    "label": 0
                },
                {
                    "sent": "For the second, disappear has new probability and it is removed.",
                    "label": 0
                },
                {
                    "sent": "OK. Show this everything.",
                    "label": 0
                },
                {
                    "sent": "Convert more or convert faster than the than the inside outside algorithm.",
                    "label": 0
                },
                {
                    "sent": "So in addition.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are these discovery themes.",
                    "label": 0
                },
                {
                    "sent": "Tends in general, tends to accumulate the probability mass in the best derivation, just in few derivations.",
                    "label": 0
                },
                {
                    "sent": "So for example, we evaluated the accumulated probability mass in the best derivation and in the five best derivations.",
                    "label": 0
                },
                {
                    "sent": "So after the training process, most of the probability of the strings is accumulated just in a few derivations.",
                    "label": 0
                },
                {
                    "sent": "Just in a few division, and this justifies if some of you know the speech recognition when you when we use a hidden Markov models, sometimes they hear the models are trained with insights with the theme or the forward water with him and then the recognition is carried out with the metabolism.",
                    "label": 0
                },
                {
                    "sent": "So given that most of their priorities accumulate in just a few derivations, this justifies the use of this.",
                    "label": 0
                },
                {
                    "sent": "Chatterjee, OK so this illustrate how their probabilities accumulated in just a few derivations, so both algorithms proceeds in a similar way.",
                    "label": 0
                },
                {
                    "sent": "This plot.",
                    "label": 0
                },
                {
                    "sent": "Means the following.",
                    "label": 0
                },
                {
                    "sent": "So this is the probability of the derivation.",
                    "label": 0
                },
                {
                    "sent": "The second best elevation, the Third Division the Stratera, and we we are training the model.",
                    "label": 0
                },
                {
                    "sent": "The priority tends to be accumulated investigation something like this.",
                    "label": 0
                },
                {
                    "sent": "Until the final most of the most of the probability is accumulated in just a few generations.",
                    "label": 0
                },
                {
                    "sent": "This is a phenomenon that has not been very well studied, and I think that it will be interesting to see how this why this happens.",
                    "label": 0
                },
                {
                    "sent": "OK, questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's continue, and now we're seeing how to use these models for language modeling.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic context free grammar have been used in several having the use of probabilistic context free grammar for language modeling has been explored in several works.",
                    "label": 0
                },
                {
                    "sent": "We explored this formalism some time ago, so I will explain how the how we how we used.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are these models, so in our case a.",
                    "label": 0
                },
                {
                    "sent": "We use probabilistic context free grammar for language modeling, but in our case we combine the grammar with an angle grammar.",
                    "label": 0
                },
                {
                    "sent": "This is usual.",
                    "label": 0
                },
                {
                    "sent": "This is useful.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose that we have.",
                    "label": 0
                },
                {
                    "sent": "This is the we are interested in computing this value.",
                    "label": 0
                },
                {
                    "sent": "And this value and this value can be writing in this way.",
                    "label": 0
                },
                {
                    "sent": "OK, so when you are using ngram models, we have this expression in such a way that the probability of overwork depends.",
                    "label": 1
                },
                {
                    "sent": "If such as I have commented previously on the previous words.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We explored the use of these models using this this combination.",
                    "label": 0
                },
                {
                    "sent": "We combine linearly and anger model with a probabilistic model that in this case that in this case we divided the probabilistic model again in two parts.",
                    "label": 0
                },
                {
                    "sent": "Because the training of these models is very hard, very time consuming.",
                    "label": 0
                },
                {
                    "sent": "So what we do was two train a model that accounts only for until the pre terminal.",
                    "label": 0
                },
                {
                    "sent": "Of the of the until the pro terminal of the.",
                    "label": 1
                },
                {
                    "sent": "Of the words of the string that is the part of the speech tax so.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have the sentence.",
                    "label": 0
                },
                {
                    "sent": "These are syntactic syntactic.",
                    "label": 0
                },
                {
                    "sent": "These are syntactic.",
                    "label": 0
                },
                {
                    "sent": "Symbols and these are post tax symbols, part of speech symbols.",
                    "label": 0
                },
                {
                    "sent": "This means that this is a instructor and this is Attack Attack for that word that the meaning, for example, is that this is a noun.",
                    "label": 0
                },
                {
                    "sent": "A single noun, plural noun and adjective, etc.",
                    "label": 0
                },
                {
                    "sent": "So in our language model we train the model in two parts.",
                    "label": 0
                },
                {
                    "sent": "We divided the model, the syntactic model in two parts.",
                    "label": 0
                },
                {
                    "sent": "We tried this model.",
                    "label": 0
                },
                {
                    "sent": "Until this part, and then we combine this in a simple way.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here we have the ngram model and here we have the probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "This model as I have commented is divided into parts.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The whole graph the for the first we have the Ingram model and.",
                    "label": 0
                },
                {
                    "sent": "We divided the model in two parts.",
                    "label": 0
                },
                {
                    "sent": "1st, we generate a string until post tax and then we generate for each postdoc award with the distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so we try.",
                    "label": 0
                },
                {
                    "sent": "Say in in in two times.",
                    "label": 0
                },
                {
                    "sent": "First, we estimate the grammar and 2nd we estimate this separately.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In this case, given the difficult to train a good grammar, we used the bracketed version of the algorithm, the bracketed version.",
                    "label": 0
                },
                {
                    "sent": "So we took the information from the from at, from corpus that is annotated not only by parentheses, by also with this.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Completed structure.",
                    "label": 0
                },
                {
                    "sent": "So the so.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So where in order to compute the.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use the language model we need to compute this value.",
                    "label": 0
                },
                {
                    "sent": "That she we need to compute this value, and so we need.",
                    "label": 0
                },
                {
                    "sent": "The probability of an initial string, but we have nothing to compute.",
                    "label": 0
                },
                {
                    "sent": "This is the left to right inside algorithm that we have introduced.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previously, but in that in this case we had to modify a slightly in order to introduce this distribution of post tax into words.",
                    "label": 0
                },
                {
                    "sent": "So the modification is very simple.",
                    "label": 0
                },
                {
                    "sent": "We are only introducing this probability, so we generate a category, a POS tag, and then we generate a word.",
                    "label": 0
                },
                {
                    "sent": "We need also to modify the in.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Title Re theme is easy.",
                    "label": 0
                },
                {
                    "sent": "So these are the likely modifications and then.",
                    "label": 0
                },
                {
                    "sent": "We evaluate it this language.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sardar shot so we we here we have an example of the of the corporate that we used.",
                    "label": 0
                },
                {
                    "sent": "This is this corpus is there is the the Pentagon corpus that is a couple that is annotated in this way we have the sentence.",
                    "label": 0
                },
                {
                    "sent": "We have a post tax and we have syntactic tax.",
                    "label": 0
                },
                {
                    "sent": "OK so for the training process.",
                    "label": 0
                },
                {
                    "sent": "So we separate the training of the grammar from this part to the top.",
                    "label": 0
                },
                {
                    "sent": "Part and from this part to the bottom part from this part.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so these are the these are the characteristic of the corpus.",
                    "label": 0
                },
                {
                    "sent": "This is the training.",
                    "label": 0
                },
                {
                    "sent": "In this case we use this amount of sentences.",
                    "label": 0
                },
                {
                    "sent": "Eh, we limited the training by removing those sentences that had more than 15 words.",
                    "label": 0
                },
                {
                    "sent": "And this is we choose this directory's for tuning, and this number of sentences for test.",
                    "label": 0
                },
                {
                    "sent": "We removed from the vocabulary all.",
                    "label": 0
                },
                {
                    "sent": "We only took the most.",
                    "label": 0
                },
                {
                    "sent": "Ten, 10,000 words.",
                    "label": 0
                },
                {
                    "sent": "OK, so for the grammar we use free software.",
                    "label": 0
                },
                {
                    "sent": "We reduced the linear discounting force museum.",
                    "label": 0
                },
                {
                    "sent": "So and we we with this model.",
                    "label": 0
                },
                {
                    "sent": "With this training data we obtain this perplexity in the tuning set, and this perplexity in the test set.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we will.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We were able.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To improve in some way the the perplexity of the N gram, and this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the perplexity on the tuning set obtained by the three gram model.",
                    "label": 0
                },
                {
                    "sent": "This is our baseline model and we adjust the Alpha parameter.",
                    "label": 0
                },
                {
                    "sent": "The combination with the tuning set, and the best value is obtained here.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we evaluate it.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "With this Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The tests are pretty and well, we have achieved some improvements in a similar way to other works.",
                    "label": 0
                },
                {
                    "sent": "OK, when we tried the same by evaluating war error rate, the results were not as good as we expected.",
                    "label": 0
                },
                {
                    "sent": "OK questions?",
                    "label": 0
                },
                {
                    "sent": "Questions, so let's questions.",
                    "label": 0
                },
                {
                    "sent": "Let's continue.",
                    "label": 0
                },
                {
                    "sent": "I will finish the match integration part and then we will.",
                    "label": 0
                },
                {
                    "sent": "We will stop for half an hour OK?",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we are speaking about the use of probabilistic model for much in translation.",
                    "label": 0
                },
                {
                    "sent": "OK, the usual way for dealing with for much intallation.",
                    "label": 0
                },
                {
                    "sent": "Usual ways, is to use a kind of model that are now as a phrase based models, so the use of syntax has been used just for some kind of languages, so most of these most of these.",
                    "label": 0
                },
                {
                    "sent": "Some approaches rely relies on on a model that we will.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this in this section.",
                    "label": 0
                },
                {
                    "sent": "So we are now introducing the concept of stochastic inversion.",
                    "label": 1
                },
                {
                    "sent": "Traditional grammars that was recently introduced by the guy who, but this formalism is closely related with syntax directed syntax directed in transaction grammars.",
                    "label": 0
                },
                {
                    "sent": "So in this case the our primitives will be 2 alphabet, one for the input language and one for the output language.",
                    "label": 0
                },
                {
                    "sent": "In this case, and in this case, our objects will be pirated strings.",
                    "label": 0
                },
                {
                    "sent": "One is drink and each translation in other language.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of our object.",
                    "label": 0
                },
                {
                    "sent": "For example, we have volume attorney report that are there can be translated in English by living today in the afternoon.",
                    "label": 1
                },
                {
                    "sent": "OK so we have party the strings and our Internet and we're interested in solving problems problems like this.",
                    "label": 0
                },
                {
                    "sent": "We are interested in relating substrings of both strings, substrings, or both strings different parts.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, this is an example of a tree or obtaining by.",
                    "label": 0
                },
                {
                    "sent": "Stochastic inversion tradition grammar.",
                    "label": 0
                },
                {
                    "sent": "This is an example.",
                    "label": 0
                },
                {
                    "sent": "In the world presented by The Who.",
                    "label": 0
                },
                {
                    "sent": "He only used one non terminal and the idea was that that non terminal accounts for immersion in the language is OK.",
                    "label": 0
                },
                {
                    "sent": "So let's introduce more formally this concept.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And ITG invention tradition grammar is simply at a point define it like this we have a set of nonterminals.",
                    "label": 0
                },
                {
                    "sent": "A set of terminals, the language, the input language.",
                    "label": 0
                },
                {
                    "sent": "There's a set of another set of terminals, the output language, a set of rules, and the action of the grammar.",
                    "label": 0
                },
                {
                    "sent": "So this is the rules that can be that these rules are very similar to the rule that we have seen.",
                    "label": 0
                },
                {
                    "sent": "For context free grammar.",
                    "label": 0
                },
                {
                    "sent": "Very similar, but in this in this time.",
                    "label": 0
                },
                {
                    "sent": "The rules has a special mark in order to decide if the non terminals in the right part must be inverted or not.",
                    "label": 0
                },
                {
                    "sent": "So if we have, for example, a rule like this.",
                    "label": 0
                },
                {
                    "sent": "This means that.",
                    "label": 0
                },
                {
                    "sent": "In the input language, the relation between strings is in this way.",
                    "label": 0
                },
                {
                    "sent": "And in the output, the string is also in this way.",
                    "label": 0
                },
                {
                    "sent": "This is the input string.",
                    "label": 0
                },
                {
                    "sent": "This is the output string and this structure is related with this and this structure is relatively nice.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "If we have a rule like this.",
                    "label": 0
                },
                {
                    "sent": "I'm an investor and immersion rule like this that is not it in this way.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "No, sorry.",
                    "label": 0
                },
                {
                    "sent": "Then this part, sorry.",
                    "label": 0
                },
                {
                    "sent": "This part is related with this part, and this part is related with this part.",
                    "label": 0
                },
                {
                    "sent": "OK, there is our inversion in the structure.",
                    "label": 0
                },
                {
                    "sent": "In the in the open language with respect to the output.",
                    "label": 0
                },
                {
                    "sent": "So the input language for the input sentence.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we will see later on example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we have here an example.",
                    "label": 0
                },
                {
                    "sent": "This is an inversion rule.",
                    "label": 0
                },
                {
                    "sent": "So this string in the English part goes to the.",
                    "label": 0
                },
                {
                    "sent": "Initial order string.",
                    "label": 0
                },
                {
                    "sent": "Goes to.",
                    "label": 0
                },
                {
                    "sent": "Initial part OK.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, these models for these models there exist also a normal form in such a way that.",
                    "label": 0
                },
                {
                    "sent": "Every.",
                    "label": 0
                },
                {
                    "sent": "Immersion transition grammar can be righted.",
                    "label": 0
                },
                {
                    "sent": "All the rules can be writing in this way, and if the we need to introduce the empty string so we have also this rule, this is.",
                    "label": 0
                },
                {
                    "sent": "This rule is usually omitted.",
                    "label": 0
                },
                {
                    "sent": "So this is the usual rules that we have that we have in a traditional grammar so.",
                    "label": 0
                },
                {
                    "sent": "Then again, we we attach probability to the rules OK in order to introduce from stochastic inversion tradition grammars.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have again of stochastic formalism to account for pilot strings, so we must be introduced, we want.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Change the concept of a stochastic derivation so that this concept is defined.",
                    "label": 0
                },
                {
                    "sent": "Define it in a similar way.",
                    "label": 0
                },
                {
                    "sent": "Is defined in the similar way, so the probability of a derivation is the priority of the rules.",
                    "label": 0
                },
                {
                    "sent": "The product of the probability of the rule that contribute to that derivation.",
                    "label": 0
                },
                {
                    "sent": "So we have an example for this.",
                    "label": 0
                },
                {
                    "sent": "Here we have fun in stochastic inversion traditional grammar that is able to produce this this tree.",
                    "label": 0
                },
                {
                    "sent": "So in the input string for the important thing we have, this is the string that we have that we have generated and this is just the string that we have generated for the operator string, because in this case we are visiting first this symbol because.",
                    "label": 0
                },
                {
                    "sent": "For the output part with first right.",
                    "label": 0
                },
                {
                    "sent": "The symbol and then the symbol.",
                    "label": 0
                },
                {
                    "sent": "Imposed order we visit when we are trying to recover the output sentence.",
                    "label": 0
                },
                {
                    "sent": "We must visit in post order the tree when we arrive to an inverted animation animated rule.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can all again into this conscious probability of a string paid probability of the reservation in a similar way, and the language generated by stochastic invention to transition drama in a similar way.",
                    "label": 0
                },
                {
                    "sent": "So one could think that this formalism is powerful enough to represent all possible alignment between words, but this is not true.",
                    "label": 0
                },
                {
                    "sent": "But this is not true, for example.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a look.",
                    "label": 0
                },
                {
                    "sent": "This is a result reported in the original paper of in the Bush Paper whose paper.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we're trying to match all the words in one sentence with all the words in another sentence with the same with the same length.",
                    "label": 0
                },
                {
                    "sent": "So for example a.",
                    "label": 0
                },
                {
                    "sent": "We have we have forward we are.",
                    "label": 0
                },
                {
                    "sent": "We can match this with this with this world with this work with with this work.",
                    "label": 0
                },
                {
                    "sent": "So this kind of alignments.",
                    "label": 0
                },
                {
                    "sent": "Can't not be represented by an ATG, but.",
                    "label": 0
                },
                {
                    "sent": "This kind of alignments are very strange.",
                    "label": 0
                },
                {
                    "sent": "So for example, who reports this table in which he measured the possible alignments, all matching with the alignment that can be obtained with an ITG and we can see, for example, that the maximum number of alignments with two sentences of length 10 is this value, But this is the amount of alignment that can be represented by an ATG in.",
                    "label": 0
                },
                {
                    "sent": "Sometimes this is not.",
                    "label": 0
                },
                {
                    "sent": "As this is not as bad as we can, one could think cause in something we are restricting this search space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So again, we have the.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar problems we have supporting problems, learning problems and translation problems, so the parsing for the parsing problems.",
                    "label": 0
                },
                {
                    "sent": "They are already teams are very similar.",
                    "label": 0
                },
                {
                    "sent": "Are there similar?",
                    "label": 0
                },
                {
                    "sent": "We have the insight they will tell you with him and the outsider redeemed estimation algorithm are very similar.",
                    "label": 0
                },
                {
                    "sent": "So again, we have a learning problem that we can divide into parts.",
                    "label": 0
                },
                {
                    "sent": "The learning of the structure, the rules, or the probabilities for the probabilities.",
                    "label": 0
                },
                {
                    "sent": "We can define similar algorithms to the inside outside algorithm and finally the collation process can be carried out using an adapted or anything similar to the CKY origin.",
                    "label": 0
                },
                {
                    "sent": "OK, let's see more data in one of these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But these debited materially theme this is the original algorithm proposed by who?",
                    "label": 0
                },
                {
                    "sent": "This is the initialization.",
                    "label": 0
                },
                {
                    "sent": "The idea is similar.",
                    "label": 0
                },
                {
                    "sent": "We are filling our as a table in in a down top.",
                    "label": 0
                },
                {
                    "sent": "OK, we have a first sentences of length 123 etc.",
                    "label": 0
                },
                {
                    "sent": "Now you see me.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very similar, so this is there.",
                    "label": 0
                },
                {
                    "sent": "The recursion in which we are maximum.",
                    "label": 0
                },
                {
                    "sent": "We took the maximum of these two values.",
                    "label": 0
                },
                {
                    "sent": "This is when the relation is direct and this is when the relation is inverted.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is the Viterbi algorithm.",
                    "label": 0
                },
                {
                    "sent": "If we keep track of the of the decisions that we have taken in each step, we are able later to recover the tree.",
                    "label": 0
                },
                {
                    "sent": "OK, and obtain the past three.",
                    "label": 0
                },
                {
                    "sent": "OK, we're actually proposed a modification on the algorithm because the original algorithm avoids some parse tree.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The details can be seen in this work.",
                    "label": 0
                },
                {
                    "sent": "OK, showed everything.",
                    "label": 0
                },
                {
                    "sent": "Once we have defined once we have introduced this algorithm, all the algorithms are very similar.",
                    "label": 0
                },
                {
                    "sent": "This is the victory theme.",
                    "label": 0
                },
                {
                    "sent": "They shut.",
                    "label": 0
                },
                {
                    "sent": "The Insight algorithm is similar, we just.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, the maximization by an addition here, OK, but.",
                    "label": 0
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                },
                {
                    "sent": "This algorithm has a time complexity.",
                    "label": 0
                },
                {
                    "sent": "This is cubic with the input and cubic with the output.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very very very expensive.",
                    "label": 0
                },
                {
                    "sent": "OK, so we only can use this for limited tasks.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we can also introduce a brackets in the algorithm in order to to improve the time complexity.",
                    "label": 0
                },
                {
                    "sent": "In this time the introduction of the use of bracketing is even more important and even more interesting because we could use monolingual partial for one part and a monolingual partial further apart.",
                    "label": 0
                },
                {
                    "sent": "So we can use a monolingual parser for English and and another monolingual parser for Spanish.",
                    "label": 0
                },
                {
                    "sent": "For example Chinese, and then we can use the brackets.",
                    "label": 0
                },
                {
                    "sent": "In order to train R&R on stochastic investment and issue grammar.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So again, if we use brackets, the time complexity can be linear with each string.",
                    "label": 0
                },
                {
                    "sent": "OK, so given that these are.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think we are able to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "This problem.",
                    "label": 0
                },
                {
                    "sent": "And the translation is carried out in a similar way, too personal.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With him OK.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see the details in this reference, so we have evaluated these models for translation for much integration, and this prevents that.",
                    "label": 0
                },
                {
                    "sent": "We carry out consisted on 1st we take, we took a billingual corpus.",
                    "label": 0
                },
                {
                    "sent": "We parse one of the one of the.",
                    "label": 0
                },
                {
                    "sent": "Of the.",
                    "label": 0
                },
                {
                    "sent": "Of the part of the one of the sides.",
                    "label": 0
                },
                {
                    "sent": "Sorry, one of the parts.",
                    "label": 0
                },
                {
                    "sent": "Or the other of the strings.",
                    "label": 0
                },
                {
                    "sent": "Then we minimized the, given that some parsers obtained parse tree that had no binary, we must introduce submission process.",
                    "label": 0
                },
                {
                    "sent": "Then we combine this information with the parsing.",
                    "label": 0
                },
                {
                    "sent": "Combine all the information in order to estimate the grammar.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So more or less, the idea is to this is the only way up the result obtained by a monolingual parser, and this is the result of tiny.",
                    "label": 0
                },
                {
                    "sent": "With an ITG in which we used not only one non terminal but more non terminals and then we try to match each non terminal with this with each non terminal.",
                    "label": 0
                },
                {
                    "sent": "In this way we are able to obtain agreement estimated with bitter beer with him and then we use this model for for inauguration.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When the results are acceptable, so we participate in international task.",
                    "label": 0
                },
                {
                    "sent": "So this is the the figures for the training data for development and for the test set.",
                    "label": 0
                },
                {
                    "sent": "And in this case the baseline was a baseline.",
                    "label": 0
                },
                {
                    "sent": "In this case the baseline was a phrase based translation system that is the state of the art and well in our case we we achieved to improve a little.",
                    "label": 0
                },
                {
                    "sent": "This values these values.",
                    "label": 0
                },
                {
                    "sent": "OK questions.",
                    "label": 0
                },
                {
                    "sent": "Is there any question no.",
                    "label": 0
                },
                {
                    "sent": "Well, we stop here and we will return here in half an hour, OK?",
                    "label": 0
                }
            ]
        }
    }
}