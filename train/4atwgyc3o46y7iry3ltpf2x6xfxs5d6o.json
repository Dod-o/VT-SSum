{
    "id": "4atwgyc3o46y7iry3ltpf2x6xfxs5d6o",
    "title": "The sample complexity of agnostic learning under deterministic labels",
    "info": {
        "author": [
            "Ruth Urner, Computer Science Department, Carnegie Mellon University"
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/colt2014_urner_labels/",
    "segmentation": [
        [
            "I'll be talking about the sample complexity of agnostic learning under deterministic labels, and this is joint work with shape, end of it and where do I?"
        ],
        [
            "OK, so here's kind of the very basic classical learning theory framework that we all know for binary classification, learning with respect to some pics I posted this class H. So in general we kind of we differentiate between 2 main cases, they're realisable case, there's the agnostic case.",
            "In the realizable case we assume that the true concept is a member of the class that we're trying to learn, and we know that the sample complexity in this case is roughly.",
            "VC over Epsilon in the agnostic case we make no assumptions on the data generating distribution and we also know.",
            "But in this case, the sample complexity is always BC of epsilon square.",
            "Um?",
            "And then people have also looked at some kind of intermediate scenarios.",
            "So scenarios where maybe the base is in the class only.",
            "Well approximated by the class and we have some kind of conditions on the on the noise in the labels and there have been weights that interpolate between the realisable setting in the agnostic setting.",
            "And these results seem to indicate that low noise makes learning easier, so this kind of LED us to investigate the remaining corner in the state will be at the upper right corner, which is we look at the setting where the true labeling function of the distribution is deterministic.",
            "The label is a deterministic function of the of the instances, but we're still agnostic.",
            "We don't assume that this is well approximated by the class.",
            "So I."
        ],
        [
            "We care about this at all, so let me give you 2 reasons.",
            "One is that I mean machine learning tools now.",
            "Not able to handle operate on very high dimensional spaces, and it's reasonable to assume that once you've collected sufficiently many many features of in your task, then the then the label.",
            "It is actually a deterministic function of all these features.",
            "However, we would still not want to learn the class of all functions of all these.",
            "All these features in order to to not overfit.",
            "So, for example, we might be interested in learning sparse predictors of all these features, and in that case, even though the label is deterministic, it is still not a member of the.",
            "Part of this class that we're interested to learn.",
            "And Furthermore, and we think that this is, I mean, it's a.",
            "It's a fundamental basic problem, offer, well understood framework, and we think it can lead to some insights on where the difficulty of learning comes from.",
            "Um?"
        ],
        [
            "Yeah, so so this is the framework that we look at and we think it exhibits some unexpected behavior.",
            "So the 1st result is we show that deterministic labels do not automatically imply fast rates.",
            "So there are classes that still have the receipt of VC over epsilon square sample complexity, and in particular that means if we have one cannot get low noise, fast rates and results without making some kind of additional assumptions.",
            "Um?"
        ],
        [
            "So Furthermore, we show that in the setting on the sample complexity of learning is not always fully determined by the VC dimension of the class, so this is somehow in contrast to what we're used to in the in the classic framework that I that I displayed on the 1st slide.",
            "In that framework, both in the realisable and in the agnostic case, the sample complexity is always off class of finite visa dimension.",
            "In the realizable cases always VC over epsilon.",
            "In the agnostic cases always PC over epsilon squared.",
            "So in in in the.",
            "Ignostic deterministic setting that we look at here we show that there are classes of any VC dimension that can be actually be learned with VC over epsilon with sample complexity VC over epsilon while they also classes of the same basic dimension that require the slower rates of VC over epsilon square.",
            "So both these phenomena happen and we provide a full characterization of when it class falls into which category."
        ],
        [
            "So Furthermore, we show that in this setting, ERM, is not always optimal, so this is also in contrast to what we used to in the in the classic framework.",
            "The optimal sample complexity is always achieved by empirical risk minimizers.",
            "So here we show that there are classes that although the sample complexity of learning these classes is VC over epsilon, any DRM or any proper learning algorithm actually.",
            "Need sample size of VCO website on square so this is also a difference that happens here that doesn't have.",
            "Maybe in the general agnostic framework."
        ],
        [
            "Lastly, we show that.",
            "In this settings where the VM is not not an op statistically optimal strategy, this can be overcome with with unlabeled data.",
            "So we propose a semi supervised version of a VRM.",
            "We show that in these in these for these classes, the when the standard DRAM is not optimal, the semi supervised version of ERM does achieve optimal weight, so that it does achieve their with the better rates in these cases.",
            "So.",
            "To summarize."
        ],
        [
            "Um?",
            "So we showed that hopefully we convince you that this is an interesting.",
            "Problem to look at.",
            "We show that it exhibits some phenomena that that we're not used to and.",
            "Moreover, we've shown that imposing low noise does not necessarily render learning easy, which means that if we want to understand what.",
            "Ha.",
            "Under which situations we do get better rates?",
            "We need to look at different data, data niceness assumptions, data properties so that come into play then and I'll be very happy to."
        ],
        [
            "Discuss all of this at the post."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll be talking about the sample complexity of agnostic learning under deterministic labels, and this is joint work with shape, end of it and where do I?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's kind of the very basic classical learning theory framework that we all know for binary classification, learning with respect to some pics I posted this class H. So in general we kind of we differentiate between 2 main cases, they're realisable case, there's the agnostic case.",
                    "label": 0
                },
                {
                    "sent": "In the realizable case we assume that the true concept is a member of the class that we're trying to learn, and we know that the sample complexity in this case is roughly.",
                    "label": 0
                },
                {
                    "sent": "VC over Epsilon in the agnostic case we make no assumptions on the data generating distribution and we also know.",
                    "label": 0
                },
                {
                    "sent": "But in this case, the sample complexity is always BC of epsilon square.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then people have also looked at some kind of intermediate scenarios.",
                    "label": 0
                },
                {
                    "sent": "So scenarios where maybe the base is in the class only.",
                    "label": 0
                },
                {
                    "sent": "Well approximated by the class and we have some kind of conditions on the on the noise in the labels and there have been weights that interpolate between the realisable setting in the agnostic setting.",
                    "label": 0
                },
                {
                    "sent": "And these results seem to indicate that low noise makes learning easier, so this kind of LED us to investigate the remaining corner in the state will be at the upper right corner, which is we look at the setting where the true labeling function of the distribution is deterministic.",
                    "label": 0
                },
                {
                    "sent": "The label is a deterministic function of the of the instances, but we're still agnostic.",
                    "label": 0
                },
                {
                    "sent": "We don't assume that this is well approximated by the class.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We care about this at all, so let me give you 2 reasons.",
                    "label": 0
                },
                {
                    "sent": "One is that I mean machine learning tools now.",
                    "label": 1
                },
                {
                    "sent": "Not able to handle operate on very high dimensional spaces, and it's reasonable to assume that once you've collected sufficiently many many features of in your task, then the then the label.",
                    "label": 0
                },
                {
                    "sent": "It is actually a deterministic function of all these features.",
                    "label": 0
                },
                {
                    "sent": "However, we would still not want to learn the class of all functions of all these.",
                    "label": 0
                },
                {
                    "sent": "All these features in order to to not overfit.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we might be interested in learning sparse predictors of all these features, and in that case, even though the label is deterministic, it is still not a member of the.",
                    "label": 0
                },
                {
                    "sent": "Part of this class that we're interested to learn.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, and we think that this is, I mean, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a fundamental basic problem, offer, well understood framework, and we think it can lead to some insights on where the difficulty of learning comes from.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so so this is the framework that we look at and we think it exhibits some unexpected behavior.",
                    "label": 0
                },
                {
                    "sent": "So the 1st result is we show that deterministic labels do not automatically imply fast rates.",
                    "label": 1
                },
                {
                    "sent": "So there are classes that still have the receipt of VC over epsilon square sample complexity, and in particular that means if we have one cannot get low noise, fast rates and results without making some kind of additional assumptions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Furthermore, we show that in the setting on the sample complexity of learning is not always fully determined by the VC dimension of the class, so this is somehow in contrast to what we're used to in the in the classic framework that I that I displayed on the 1st slide.",
                    "label": 1
                },
                {
                    "sent": "In that framework, both in the realisable and in the agnostic case, the sample complexity is always off class of finite visa dimension.",
                    "label": 0
                },
                {
                    "sent": "In the realizable cases always VC over epsilon.",
                    "label": 0
                },
                {
                    "sent": "In the agnostic cases always PC over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "So in in in the.",
                    "label": 0
                },
                {
                    "sent": "Ignostic deterministic setting that we look at here we show that there are classes of any VC dimension that can be actually be learned with VC over epsilon with sample complexity VC over epsilon while they also classes of the same basic dimension that require the slower rates of VC over epsilon square.",
                    "label": 1
                },
                {
                    "sent": "So both these phenomena happen and we provide a full characterization of when it class falls into which category.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Furthermore, we show that in this setting, ERM, is not always optimal, so this is also in contrast to what we used to in the in the classic framework.",
                    "label": 1
                },
                {
                    "sent": "The optimal sample complexity is always achieved by empirical risk minimizers.",
                    "label": 0
                },
                {
                    "sent": "So here we show that there are classes that although the sample complexity of learning these classes is VC over epsilon, any DRM or any proper learning algorithm actually.",
                    "label": 0
                },
                {
                    "sent": "Need sample size of VCO website on square so this is also a difference that happens here that doesn't have.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the general agnostic framework.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lastly, we show that.",
                    "label": 0
                },
                {
                    "sent": "In this settings where the VM is not not an op statistically optimal strategy, this can be overcome with with unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So we propose a semi supervised version of a VRM.",
                    "label": 0
                },
                {
                    "sent": "We show that in these in these for these classes, the when the standard DRAM is not optimal, the semi supervised version of ERM does achieve optimal weight, so that it does achieve their with the better rates in these cases.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To summarize.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we showed that hopefully we convince you that this is an interesting.",
                    "label": 0
                },
                {
                    "sent": "Problem to look at.",
                    "label": 0
                },
                {
                    "sent": "We show that it exhibits some phenomena that that we're not used to and.",
                    "label": 0
                },
                {
                    "sent": "Moreover, we've shown that imposing low noise does not necessarily render learning easy, which means that if we want to understand what.",
                    "label": 1
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "Under which situations we do get better rates?",
                    "label": 1
                },
                {
                    "sent": "We need to look at different data, data niceness assumptions, data properties so that come into play then and I'll be very happy to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discuss all of this at the post.",
                    "label": 0
                }
            ]
        }
    }
}