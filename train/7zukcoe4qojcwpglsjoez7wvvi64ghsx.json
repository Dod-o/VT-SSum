{
    "id": "7zukcoe4qojcwpglsjoez7wvvi64ghsx",
    "title": "Resampling Approaches for Handling Imbalanced Regression Tasks",
    "info": {
        "author": [
            "Luis Torgo, University of Porto"
        ],
        "published": "Jan. 23, 2017",
        "recorded": "February 2017",
        "category": [
            "Top->Computer Science",
            "Top->Data Science"
        ]
    },
    "url": "http://videolectures.net/solomon_torgo_regression_tasks/",
    "segmentation": [
        [
            "So as such I was mentioning I'll be presenting or be talking about this work that we have been doing together with Paola written Barnett Fahringer from Waikato University, on trying to handle imbalance regression tasks.",
            "OK, so just to give you a."
        ],
        [
            "A brief overview of the context of this work.",
            "So predictive modeling within balance distributions is a well known subject, and that occurs because many important predictive tasks involve handling a target variable that has a very imbalanced distribution in the available training data.",
            "OK, now this is a problem that has been extensively studied within the context of classification.",
            "Tasks OK where you have rare classes being the main goal of the user.",
            "OK, now this work tries to raise the attention to the community that the similar problems occur.",
            "Also on regression.",
            "That is, when you have a target numeric variable and so that in effect is connected to our link it with several real world important applications like in domains like finance, technology and so on and so forth.",
            "OK where?",
            "You have this sort of imbalance regression tasks.",
            "OK, so if you want to know more about, you know the existing methods for both for classification and regression.",
            "We have recently published a rather extensive survey on these problems and the methods that exist for finding these things OK."
        ],
        [
            "Now.",
            "Focus now on regression.",
            "So as I mentioned mentioned, it frequently occurs that the main goal of the end user is, you know, a subset of the range of the target variable, and frequently this has to do with extreme values either extremely high or extremely low values of the target variable.",
            "OK, for instance, on stock markets, you know unusually high or low returns of some stock car, an example, or an instance of this sort of problems, OK?",
            "Now, these extreme values typically are very rare OK, and that means that they are normally very poorly represented on the training data that we have available to obtain our models.",
            "OK, now as we did in imbalanced classification, where you have, you know a target variable with some rare class, typically the most frequent cases that you have the so called positive class, which is very rare, and then that the normal and very frequent class as, but as we didn't imbalanced classification.",
            "Models will naturally, because of the criteria that they use to obtain the models.",
            "They will naturally be biased towards the more frequent cases in the training data.",
            "OK, now of course, because that's not the main goal of the end user.",
            "That will of course will lead to a disappointing performance on these more relevant cases which are poorly represented on the data.",
            "OK, so that's the source of the problem, and that's the motivation for our.",
            "Work OK."
        ],
        [
            "Now I'm trying to define a bit more the problem, so this issue of trying to predict rare extreme values can be seen as in the general regression regression setting where you have, you know, an unknown function that Maps a set of predictors into a target variable.",
            "And of course we have a training set.",
            "OK, now the problem or the difference to standard regression occurs because you know a subset of the range of this target variable has higher importance to the end user.",
            "And that's one of the key differences, and the other difference is that this subset is under represented on these available training data.",
            "OK, so that's the context that we are facing."
        ],
        [
            "So more formally, it while on classification it is easy, easy to say, or to state what are the important things and the normal things, because the user just say, well, this is the red class, the positive class.",
            "This is the normal class.",
            "OK, on regression that's not so easy because you have a continuous variable with a potentially infinite domain.",
            "So to help us in defining what is relevant or not for the end user we resort to this notion of a relevance function OK?",
            "This is the function that essentially Maps the domain of the target variable.",
            "The continuous target variable into a range of relevance in the interval 01, where one means maximal importance for the end user and zero represents you know minimum relevance OK.",
            "Moreover, as in classification we ask the user to tell us well what is the target class.",
            "The more important class here we ask the user to set a threshold on these relevance.",
            "OK, so that we know what are the more important values for the user.",
            "OK, so using this relevance function on this threshold we can actually, you know split the original domain in two subsets or two ranges.",
            "OK, the one that contains the data for for which the relevance is higher than the threshold.",
            "So the ones that are the more important things for the user and the normal cases which are not so relevant because they have a relevance below this threshold.",
            "OK, now again we can do the same thing with our initial training set.",
            "We can partition it into subsets, the one containing the cases that have a target value within this domain and the remaining cases OK.",
            "So these are the rare cases, and these are the normal cases in our training set.",
            "OK, so imbalanced prediction basically is defined by these two assertions.",
            "OK, first of all, the user has a known uniform importance assigned to the predicted performance within the target variable.",
            "OK, so it's not.",
            "It's not in different for him whether you are making a mistake here or there.",
            "Some things are more important for the end user.",
            "OK, and the other thing is that the subset of cases which are more important are much less present on the training data than the normal cases.",
            "OK, it's only the conjunction of these two conditions that make an imbalance problem.",
            "OK, because if it is in different for the user whatever.",
            "Your mistakes are happening then you don't have a problem.",
            "OK, it's not a problem that some cases are poorly represented because for the user is indifferent, OK, and also if if the more important cases are not under represented, then that's not a problem also, so it's the conjunction of these two things that make this hard problem."
        ],
        [
            "So just to give you a very short illustration of this thing.",
            "So in classification these are just two toy examples.",
            "One typically has something like this, so a class, the so-called negative class, which is very present on our training data and then a few of these so called positive but more important for the use user examples OK. And so the problem is that the user really wants our models to be very accurate on this.",
            "Positive class and the other one is more or less or at least less relevant.",
            "OK, so in regression, for instance.",
            "Here we have the approximation of the distribution of a target parables of some problem and what we see is that the cases are concentrated over.",
            "In this case near 0 values and then we have a few extreme extremely high values of this target variable.",
            "Anna in this concrete example it happens that the user is really particularly interested in.",
            "You know accurate predictions on these very rare values OK, and so if you want here we have another hypothetical relevance function that, in a way, Maps these interests of the user.",
            "OK, it says that well over here it's more more or less you relevant, so very low values of these relevance.",
            "But then as we start to grow in terms of the range of the target variable, then our importance or the importance assigned by the user starts to grow.",
            "Also OK, and by the way this is, you know this is a relevance function that is automatically obtained by procedure that we have proposed which is based on, you know the distribution of the target variable and automatically derives this this function assuming that I extreme values are the more relevant for the for the user.",
            "OK so because of course it would be very demanding to ask for a user to define this function fully defined this function.",
            "But we we propose means to automatically derive it from the training data, OK?"
        ],
        [
            "Alright, so.",
            "As we all know, problems imbalanced distribution creates all sorts of problems to learning methods, and in particular they typically require special purpose evaluation metrics because things, like a curacy you know, do not give good feedback when you have imbalanced classification, and the same thing happens in regression.",
            "So things like mean squared error mean absolute deviation.",
            "I mean, they tend to be biased stores.",
            "The most frequent cases which are the less relevant for the user, so the same phenomenon.",
            "Occurs with this standard error metrics in regression.",
            "Then what happens in the standard error metrics in classification like accuracy or error rate.",
            "OK, so they do not reflect the performance of the models on the more important cases.",
            "OK now, so typically we need special purpose evaluation metrics and Moreover we need to somehow make the learning algorithms focus on these rare situations instead of focusing on the more.",
            "Pre valent situations OK, and that happens both on classification and regression.",
            "That's in a way, our motivation.",
            "So in this talk I'll be focused on this issue.",
            "How to make the learning algorithms focus on the more relevant cases?",
            "This was already addressed before by our group proposing no evaluation metrics for this sort of problems.",
            "OK, now."
        ],
        [
            "Just a brief taxonomy of this.",
            "Typical strategies for handling imbalanced domains both on classification and regression, so at higher level you have, you know, data pre processing strategies that tryin away either to change the distribution, which will be what we are going to talk here today or you know, wait the data space.",
            "OK so these involve a kind of preprocessing step.",
            "Then we have special purpose learning methods and also.",
            "Prediction post processing.",
            "So after the models get some prediction, we somehow change that to make them more in accordance to the user bias.",
            "OK. And then there are set of hybrid methods that somehow integrate both of these.",
            "Some of this approach."
        ],
        [
            "OK, so just to very briefly go through these things so data preprocessing.",
            "Essentially the goal is to change the distribution of the examples on the training data that we have before applying any learning algorithm.",
            "OK, so the main advantage is that if we apply use these methods then we can use whatever learning algorithm on these modified training data, so we don't need to.",
            "We can apply any learning algorithm.",
            "OK, so the disadvantages are that it may be difficult to decide the optimal distribution.",
            "That better copes with the user goals.",
            "And you know some of these strategies.",
            "Typically, you know either increase or decrease the total number of examples of the initial training sample, and that may not be desirable.",
            "If it is something computationally very demanding, or if we have very sparse data set OK."
        ],
        [
            "No special purpose learning methods, so here the goal is actually to change the learning algorithms OK, and you know, in a way to provide a better filter with this imbalance distribution.",
            "OK, the advantages that you know they are very effective and Moreover the models tend to be more comprehensible because the algorithms were changed in accordance to this bias.",
            "OK, so the models tend to be reflect this change.",
            "OK now.",
            "Of course, there are several disadvantages because we need actually to know a lot about the algorithms and be able to change them in order to make this adaptation OK."
        ],
        [
            "Now post processing here.",
            "The goal is to somehow change the predictions that the models output OK, and we do that in a way to be more in accordance with the user preference bias and the main advantage is that again, as in preprocessing.",
            "Again we can use whatever learning algorithm, OK.",
            "But the disadvantage is is we have a potential loss of the models interpretability.",
            "Why?",
            "Because we have a prediction which is modified, and so the thing that we showed to the user it's not a direct reflection of the model.",
            "So to justify this prediction we need to explain the modification which is not always very easy, OK?"
        ],
        [
            "Now.",
            "So the key issue, one of the key issues here is this notion of relevance function, and so because it's thanks to this function that we are able to determine what are the relevant ranges of the target variable OK, and so, as I mentioned, this is a mapping function that Maps the original domain of the target variable into a 01 scale, and so as I also mentioned, this was.",
            "You know already defined a few years back by my PhD student Ribadeo on our PhD thesis proposed this relevance function and also an automatic method to derive this from the training sample.",
            "Although it is also possible to of course manually specify this function by providing a set of key points within this this curve.",
            "OK, so."
        ],
        [
            "So we are going to use this within our proposals.",
            "OK, now let's now focus on risk concrete resampling strategies for handling imbalanced regression tasks.",
            "Namely, I'll be talking about this five proposals that we have made.",
            "OK, some of them are just, you know, a simple modifications of strategies that already exist for imbalance classification.",
            "OK, the last one is relatively novel.",
            "Uh, situation?",
            "OK now."
        ],
        [
            "We have also created an R package that implement that is available to the research community that implements all these methods and also most of the existing methods for imbalanced classification.",
            "OK, so only I will use it here for some short illustrations, but it implements not only regression but also classification approaches for imbalanced learning.",
            "OK, so if you wish to ever try if you want to play with some data, OK."
        ],
        [
            "Now let's start with random undersampling.",
            "So the idea is very simple, so all cases with the target fellow which have a very low relevance actually lower than the user defined threshold are candidates for undersampling.",
            "OK, so the user decides this threshold and also the proportion that target proportion that he wants on the final modified training set in terms of the rare and normal cases.",
            "OK, so and then.",
            "These resampled data set is basically obtained by randomly removing examples from these uninteresting subset.",
            "OK, those which have a relevance lower than the user defined threshold.",
            "OK, so that's a very simple idea.",
            "As I mentioned before, given the threshold and the relevance function, we can split the training data in two subsets, the ones with the rare unimportant values and the ones with the normal cases.",
            "So then we go to the normal cases and randomly remove some of them.",
            "That's what random.",
            "Undersampling does OK. Now."
        ],
        [
            "Just, you know, brief over examples of how to use that using this package are package that I mentioned, so you can simply state what is the target variable and in this case I'm using some algae blooms data set.",
            "Essentially what you have to know here is that this is the name of the target variable and it is a variable which has a distribution like the one I was showing before.",
            "That is, most of the values are around 0 and then we have some.",
            "Extreme values which correspond to algae blooms OK. And of course here the main goal is to try to be able to anticipate these blooms and so the main goal of the user is this extreme.",
            "High values of this target parable.",
            "OK, so this is the original unchanged an unbalanced data set.",
            "So here I'm just saying, well, do random undersampling on this task, forecasting this algae using this originally unbalanced data set by default this thing will create a new data set which.",
            "By default, will try to balance the number of rare and normal cases OK, so the goal here will be to have an equal number of rare and normal cases.",
            "By default.",
            "That's the behavior OK. Now you can also with some parameters tweak this thing.",
            "You can go for extreme where you say well just invert the distribution.",
            "OK, we have very small number of rare very high number of.",
            "Normal, so just inverted diffusion OK?",
            "And so that's a very extreme thing to do.",
            "Or you can explicitly say the amount of undersampling the percentage of undersampling that you want to do.",
            "OK, so that's just very briefly what you can do with this function."
        ],
        [
            "And this is kind of.",
            "A picture of the impact on the distribution when using these three approaches.",
            "OK, so here in black you have the original distribution.",
            "OK, here you have the.",
            "Relevance function automatically derived.",
            "So more important to the extreme, I hear it's more or less irrelevant and we are using as a default as a threshold, which is the default of these functions.",
            "Although you can of course change that 05 relevance.",
            "OK, that means that.",
            "These values are considered normal.",
            "These are considered important for the user OK. An from this assumption, then this is the effect of the balance, so it tries to somehow make the number of cases here equal to the number of cases over there.",
            "And then you see here the extreme.",
            "Of course, it pushes the distribution over there because it creates more rare cases and then this is just, you know.",
            "And tuned sink where I just set the amount of undersampling that I wanted.",
            "OK, so that's just an overall picture of the impact on the distribution of the target variable caused by this.",
            "In this case these three examples."
        ],
        [
            "Alright, so random oversampling, again, very simple idea.",
            "Basically we introduce random copies of the examples in the original data set OK, and this replicas are, you know.",
            "I mean, we leave the normal cases untouched and then we pick the the important cases and re replicate them randomly.",
            "Very simple idea.",
            "Again we need the relevance function to define these two sets and relevance threshold but then.",
            "It's very simple to implement."
        ],
        [
            "Sing again a few examples.",
            "Again, we have this option of balancing or making it extreme and then."
        ],
        [
            "We have similar.",
            "Impact.",
            "You know, shifting the distribution towards the more important or relevant cases for the user.",
            "OK, so that's the expected impact of this distribution change."
        ],
        [
            "Now another method that we have proposed is this Gaussian noise that essentially combines oversampling with undersampling OK, so it does oversampling by generating new synthetic examples.",
            "And it does that by picking one of the rare cases.",
            "OK, Ann, just randomly permuting the variables values, OK?",
            "And then that creates a new example, which in principle is very similar to any existing rare case.",
            "OK, that's the idea of these random small perturbations.",
            "And then at the same time we under sampled the more normal cases.",
            "Now the target variable of these knew generated cases is obtained.",
            "The gain by introducing some small random perturbation to the original target of the seat example that was used to create these random case.",
            "This new case.",
            "OK, so again very simple idea."
        ],
        [
            "Again, here are a few examples of how to use this through our package."
        ],
        [
            "And again we see this kind of shift of the distribution towards the more important cases for the user.",
            "OK."
        ],
        [
            "Now.",
            "We can also this.",
            "It's a bit irrelevant, but you can also change the amount of perturbation.",
            "This random perturbation you can with the parameter you know make it more aggressive or less aggressive, but."
        ],
        [
            "Essentially, we didn't observe, you know lots of variation in terms of the distribution, which means that it's a bit irrelevant this parameter OK?"
        ],
        [
            "No smart Smarties are very well known algorithm that was developed for imbalanced classification problems.",
            "OK again, it combines undersampling with oversampling OK. Now.",
            "We have adapted this smart algorithm for regression, which we call small tar and essentially the idea of under sampling is the same as random undersampling.",
            "That's nothing you and the part of oversampling instead of this thing of randomly perturb rating the cases like Gaussian noise.",
            "In this case what we do is that we pick from the.",
            "We pick one case from the rare cases and we pick the nearest neighbor.",
            "And then we somehow interpolate between the two.",
            "OK, with some random shuffling instead of picking the middle value, we kind of obtain a value within the the two values of each feature.",
            "OK, so that's more or less the general idea of smart.",
            "So pick some examples, some rare cases.",
            "Search for the most similar rare cases and then use them to kind of interpolate and create a new case.",
            "OK, so that's more or less the idea, so we did the same for regression.",
            "The main difference compared to classification is you decide the target variable value because in case of classification it's easy because all of them are positive class because they are rare, but here they are.",
            "They have differences on the target variable.",
            "OK, So what we have used is a kind of weighted average of the values of the target.",
            "Over the two seed examples you know to determine what is the value of the target variable.",
            "OK, and these weights are kind of inverse function of the distance to the generated case to the two CC examples.",
            "OK, so that's the idea of smart."
        ],
        [
            "And we have implemented that on our package here.",
            "It requires a bit more care because this issue of finding the neighbors of course requires a distance function to be defined OK, and so we're the.",
            "The function also requires the specification of a function to calculate the distances, and in particularly in principle it could be the cuisine, normal equation distance.",
            "But if you have like nominal and numeric variables, which is the case of this data, then you need more surface, slightly more sophisticated distance functions that end.",
            "And both handle both types of variables.",
            "OK, but you can specify a function through this parameter OK?"
        ],
        [
            "Again, we see the same sort of, you know, shift of the original distribution tours.",
            "The more important cases, and this is you know what you get with smarter OK."
        ],
        [
            "Now finally we have recently presented this works over weighted relevance based combination strategy.",
            "OK so the key idea here or the motivation was to try to make the life of a user of the user as simple as possible OK, and so we didn't like this issue of the user specifying a threshold.",
            "OK Y 05070.",
            "It may be easy for naive users to set this side on this thing OK?",
            "So what we have tried to do is whether we could use the shape of the relevance function as a way to determine whether we should.",
            "And the sample or over oversample an outmatch.",
            "OK So what we have to use is to use the score of the relevance function as a kind of probability of resampling an example.",
            "So an example which has a very high score of the relevance will be more probably oversampled.",
            "An example which has a very low score of these relevance will be more prone to be, you know, just removed from the data.",
            "OK, so that's the idea.",
            "So oversampling.",
            "Examples with higher relevance have higher probability an in undersampling, you know the randomly selected examples to be removed are you know you know the.",
            "Inverse or the complement of this relevance score?",
            "OK, now the main thing here are advantages that we don't ask the user to to set this relevance threshold because we use the information from the relevance function.",
            "OK now."
        ],
        [
            "These are just some examples uses of course, as I mentioned, the user doesn't need to set this threshold, but it can if he wants.",
            "You know it can set, but the simplest thing is you know without any parameters just the data did this gets a balance.",
            "A distribution changed data set.",
            "OK, so that's the easiest way where the user just say, well, this is the task.",
            "This is the data the original data give me a re sample data set.",
            "OK, and it uses.",
            "It infers the the relevance function and it uses to decide the amount of oversampling and undersampling and where to this take place is OK, so that's the from a user perspective.",
            "That's the simplest set."
        ],
        [
            "OK, and these are other variants OK, and again you get the same sort of."
        ],
        [
            "Effect or impact, you know, just have urufu OS.",
            "Examples and that's what we get."
        ],
        [
            "As we dismantled.",
            "OK, so of course the next question.",
            "Well so many methods.",
            "What is the best?",
            "What should I use?",
            "OK, so we have a made an extensive experimental analysis of these methods.",
            "More specifically, we have considered the following strategy, so do nothing.",
            "Just use the original data, which is kind of our baseline, then a few variants of random under sampling random oversampling, small tar, Goshen noise and also this works.",
            "Variant then we have tried several regression algorithms like linear regression, 8 variants of neural Nets, four variants of Mars, 12 VM's and six random forests.",
            "And we have selected 15 regression datasets with different amount of imbalance on the target variable OK.",
            "So that makes up, you know, a total of seven, roughly 8000 experimental settings.",
            "OK, now all models were compared to use this F measure for regression, which is one of these evaluation metrics that we have derived for this type of imbalanced regression tasks and by means of you know, 2 * 10 fold cross validation and then we have used Wilcoxon signed rank test to test for the statistical significance of the.",
            "Observe differences OK now."
        ],
        [
            "This is the kind of summary of the results.",
            "OK, So what you see here is the total number of wins on the left in blue, and losses on the right in Brown of each of the simplest sampling strategies against the baseline of doing nothing.",
            "OK, So what this means is that run one of the variance of random oversampling 300 roughly 300 times, got a better score than.",
            "Doing nothing OK. On this, for all these you know settings that we have tried OK, so the difference between the two colors is that these are the number of times these differences were statistically significant.",
            "According to this, Wilcoxon signed rank tests OK.",
            "So the more important are of course the darker bars.",
            "The other ones OK there are wins, but they are not statistically significant OK?",
            "So in general, what we could say is that this works.",
            "Methodist seems to be clearly the more advantages advantage of the proposals, particularly because it.",
            "Almost never loses compared to doing a nothing to the original data.",
            "OK, and so that's the main difference on a.",
            "That's our overall recommendation, OK?"
        ],
        [
            "So some of the conclusions, recommendations or summary of some of the results of these large sets of experiments.",
            "So we have observed that.",
            "Run them for us together with either of these three sampling strategies, achieve the best score on almost, or at least a big part of the datasets.",
            "OK, so that's one of the observations there were there were other curious things about some of the learning algorithms you know I can send you, then the paper it's actually being reviewed now, so I cannot put here the link, but hopefully it will get accepted and then we can because there are some other interesting effects on some of the learning algorithms.",
            "So as usual, it's not like this thing is the best way.",
            "Whatever you have.",
            "OK, as usually there is some variation, but overall we think that this proposal achieved a more consistent performance when comparing to using the original data.",
            "We respectively of the learning algorithm.",
            "OK, so that's overall our conclusion and also Moreover it is more user friendly because we don't ask the user to set this.",
            "You know, sometimes unintuitive.",
            "Thresholds and it is also computationally more efficient because it doesn't involve calculating the neighbors like smart, which sometimes it's computationally heavy an you know it doesn't generate new cases, just replicates them.",
            "And also it doesn't increase the training set size OK?",
            "Alright, so sorry it was the last slide.",
            "That's why somehow.",
            "Alright, so.",
            "I don't know if you have any questions.",
            "That you want to ask.",
            "Sorry about this.",
            "Buck no no no no.",
            "This is already for the classes.",
            "I mean I could, I.",
            "The demos are running this code that I've shown you, so it's not very interesting I think.",
            "Alright, so thank you.",
            "I don't know if you have any questions that you want to ask.",
            "There will be, but first let's thank Louise OK. We just put together the title.",
            "Questions.",
            "Yes people.",
            "All this.",
            "OK, you avoid it relevance threshold, but you still have to specify the relevance function.",
            "Yeah so.",
            "Said that, will generate.",
            "Yeah so we have these two options.",
            "Our hypothesis is that in most cases people will not be able to propose this function and that's why we have this automatic way of deriving the function.",
            "OK, and this works well, assuming that your goal are the extreme values.",
            "So these automatic way assumes that the interests of the user are the rare extreme values be just one.",
            "In case of this example, just had one or two OK, two Tails.",
            "OK, so under that exemption, which is the most frequent case on imbalanced regression under deck assumption, then we can provide an automatic way of deriving this function.",
            "OK, so the user doesn't need to do anything at all.",
            "OK, if not then we have other means which are a compromise where the user can just pinpoint a few points on this curve OK?",
            "Where you say well for this value, my relevance score would be something around 034 that is slightly higher for that, and with this few points then we have some methods to try to interpolate this and derive this this relevance function OK.",
            "So.",
            "There are different ranges of complexity.",
            "It depends on the domain knowledge that you have, but if you don't have anything at all.",
            "Which was the case in our experiments, because these are standard datasets available on repository's.",
            "If you know nothing about this thing, then we can use these automatic sync that assigns more importance to the rare extreme ranges of the target, OK?",
            "Do you really need to resample or can you use this advance scores as weights?",
            "Yeah, that's that's a good question, so we also try.",
            "I didn't show here this thing, but we also try this thing and using the weights generally doesn't work as well as resampling OK?",
            "Any intuition why?",
            "Well, I think that the intuition you mean using the weights when learning when learning.",
            "Well, in a way, first of all you are restricted to learning algorithms that allow you to.",
            "You know, input this weights right?",
            "OK, and actually if you use these weights as a form of resampling, which for instance other boosts can do similar things because it replicates the cases where the previous model failed, and I mean in theory other boost uses probabilities, But if your classifier doesn't use probabilities and then what it does is replicating this case is an.",
            "So you could do the same thing here so.",
            "If you use the relevance scores as a kind of probability and then replicate the cases and give that to the training to the learning algorithm.",
            "But that's exactly what works does.",
            "So it's using this relevance as a kind of weight to guide the resampling.",
            "OK, but that has the advantage that you can use whatever learning algorithm irrespective if it accepts weights or not on the training data, OK?",
            "But I would say that.",
            "The things should not be very different, but it is more.",
            "It has more limitations because it constrains you on the type of learning algorithms that you use.",
            "You know, let's say you're trying to do just parameter fitting.",
            "You have either linear or nonlinear regression, but the form of the function is known and you just try to fit the parameters.",
            "Then this relevant functions could be then mapped to the loss function.",
            "You know, so that you don't have uniform loss.",
            "You don't just measure the difference between the measured in the predicted value, but you know that's actually what this F1 four regression does.",
            "So this F1 for regression uses the information on the relevance function to wait the errors by this.",
            "Information in a way.",
            "Intuitively, that's what it does.",
            "It looks at the our important is a test case.",
            "Looking at these relevance and then the error committed by the model is more has gets more weight.",
            "If this is more relevant, OK.",
            "So it does that, but it's slightly more complex because you need to not only look at the predicted value, but also to the true value.",
            "Because you have kind of false positives and false negatives.",
            "Also, because your your model may be predicting a rare value, but in true, in reality it's a normal value, so it's A kind of false alarm or it can happen the opposite OK?",
            "That you know you have a rare event being happened, but your model say, well, no, no, no.",
            "This is a normal thing, so if you look at this algae blooms you could have a model, say, well, this we are going to have a algae bloom, but it didn't happen.",
            "OK, or you could say, well, no, no, it's everything is OK with the River, but then there was an algae bloom, so this sort of things.",
            "So you have to look at the relevance not only of the true value of the test case, but also of the predicted value, and you have to wait this.",
            "It cost functions.",
            "Yeah yeah, we we call it.",
            "Utility surface is where we take this thing.",
            "These two things these two dimensions into into account.",
            "But this F1 for regression uses this information.",
            "So it uses the relevance function to evaluate the predictions of the model in this way.",
            "So and this is then obviously related to cost sensitive learning.",
            "Yeah, it is very related, yeah?",
            "In a way you can look at these utility services as a continuous version of a cost benefit matrix.",
            "Instead of having a grid of situations, you have a surface.",
            "Like you, you still have like the predicted value, the true value, but then you have a surface.",
            "Of utility.",
            "Sometimes it's a cost, it's a negative value.",
            "Sometimes it's a benefit because it's a positive value.",
            "OK, but even positive values you know it's more important to forecasts correctly and algae bloom then to forecast correctly that nothing happened.",
            "So the benefit is also a different.",
            "On all of these surfaces are derived from this relevance function.",
            "So in the classification case, you would have a cost misclassification cost matrix and this would be just the constants there in this matrix.",
            "When you move to the regression case, obviously there is arguments for actually having a function in symbolic form that represents this, because otherwise, how do you represent the surface?",
            "You can represent it visually because it's a 2D sync.",
            "You can show it to the user and what we do is that we provide means to automatically derive them like we do for the relevance function.",
            "Of course we are not going to ask for the user to specify specify this surface, but again we have this hypothesis of him him or her giving some specific points, and then we interpolate the surface.",
            "Or we can derive it automatically from the relevance function.",
            "Any other questions?",
            "Comments.",
            "So what are the interesting applications of this?",
            "You mean this resampling till now you mean real world know till now it's an academic thing.",
            "So we have been playing with benchmark datasets so.",
            "No no stock market.",
            "That's non disclosable information.",
            "OK, thank you.",
            "Thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as such I was mentioning I'll be presenting or be talking about this work that we have been doing together with Paola written Barnett Fahringer from Waikato University, on trying to handle imbalance regression tasks.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to give you a.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A brief overview of the context of this work.",
                    "label": 0
                },
                {
                    "sent": "So predictive modeling within balance distributions is a well known subject, and that occurs because many important predictive tasks involve handling a target variable that has a very imbalanced distribution in the available training data.",
                    "label": 1
                },
                {
                    "sent": "OK, now this is a problem that has been extensively studied within the context of classification.",
                    "label": 0
                },
                {
                    "sent": "Tasks OK where you have rare classes being the main goal of the user.",
                    "label": 0
                },
                {
                    "sent": "OK, now this work tries to raise the attention to the community that the similar problems occur.",
                    "label": 0
                },
                {
                    "sent": "Also on regression.",
                    "label": 0
                },
                {
                    "sent": "That is, when you have a target numeric variable and so that in effect is connected to our link it with several real world important applications like in domains like finance, technology and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK where?",
                    "label": 0
                },
                {
                    "sent": "You have this sort of imbalance regression tasks.",
                    "label": 1
                },
                {
                    "sent": "OK, so if you want to know more about, you know the existing methods for both for classification and regression.",
                    "label": 0
                },
                {
                    "sent": "We have recently published a rather extensive survey on these problems and the methods that exist for finding these things OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Focus now on regression.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned mentioned, it frequently occurs that the main goal of the end user is, you know, a subset of the range of the target variable, and frequently this has to do with extreme values either extremely high or extremely low values of the target variable.",
                    "label": 0
                },
                {
                    "sent": "OK, for instance, on stock markets, you know unusually high or low returns of some stock car, an example, or an instance of this sort of problems, OK?",
                    "label": 0
                },
                {
                    "sent": "Now, these extreme values typically are very rare OK, and that means that they are normally very poorly represented on the training data that we have available to obtain our models.",
                    "label": 1
                },
                {
                    "sent": "OK, now as we did in imbalanced classification, where you have, you know a target variable with some rare class, typically the most frequent cases that you have the so called positive class, which is very rare, and then that the normal and very frequent class as, but as we didn't imbalanced classification.",
                    "label": 0
                },
                {
                    "sent": "Models will naturally, because of the criteria that they use to obtain the models.",
                    "label": 0
                },
                {
                    "sent": "They will naturally be biased towards the more frequent cases in the training data.",
                    "label": 1
                },
                {
                    "sent": "OK, now of course, because that's not the main goal of the end user.",
                    "label": 0
                },
                {
                    "sent": "That will of course will lead to a disappointing performance on these more relevant cases which are poorly represented on the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the source of the problem, and that's the motivation for our.",
                    "label": 0
                },
                {
                    "sent": "Work OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm trying to define a bit more the problem, so this issue of trying to predict rare extreme values can be seen as in the general regression regression setting where you have, you know, an unknown function that Maps a set of predictors into a target variable.",
                    "label": 0
                },
                {
                    "sent": "And of course we have a training set.",
                    "label": 1
                },
                {
                    "sent": "OK, now the problem or the difference to standard regression occurs because you know a subset of the range of this target variable has higher importance to the end user.",
                    "label": 1
                },
                {
                    "sent": "And that's one of the key differences, and the other difference is that this subset is under represented on these available training data.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the context that we are facing.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So more formally, it while on classification it is easy, easy to say, or to state what are the important things and the normal things, because the user just say, well, this is the red class, the positive class.",
                    "label": 0
                },
                {
                    "sent": "This is the normal class.",
                    "label": 0
                },
                {
                    "sent": "OK, on regression that's not so easy because you have a continuous variable with a potentially infinite domain.",
                    "label": 0
                },
                {
                    "sent": "So to help us in defining what is relevant or not for the end user we resort to this notion of a relevance function OK?",
                    "label": 1
                },
                {
                    "sent": "This is the function that essentially Maps the domain of the target variable.",
                    "label": 1
                },
                {
                    "sent": "The continuous target variable into a range of relevance in the interval 01, where one means maximal importance for the end user and zero represents you know minimum relevance OK.",
                    "label": 0
                },
                {
                    "sent": "Moreover, as in classification we ask the user to tell us well what is the target class.",
                    "label": 0
                },
                {
                    "sent": "The more important class here we ask the user to set a threshold on these relevance.",
                    "label": 0
                },
                {
                    "sent": "OK, so that we know what are the more important values for the user.",
                    "label": 0
                },
                {
                    "sent": "OK, so using this relevance function on this threshold we can actually, you know split the original domain in two subsets or two ranges.",
                    "label": 1
                },
                {
                    "sent": "OK, the one that contains the data for for which the relevance is higher than the threshold.",
                    "label": 0
                },
                {
                    "sent": "So the ones that are the more important things for the user and the normal cases which are not so relevant because they have a relevance below this threshold.",
                    "label": 0
                },
                {
                    "sent": "OK, now again we can do the same thing with our initial training set.",
                    "label": 1
                },
                {
                    "sent": "We can partition it into subsets, the one containing the cases that have a target value within this domain and the remaining cases OK.",
                    "label": 0
                },
                {
                    "sent": "So these are the rare cases, and these are the normal cases in our training set.",
                    "label": 0
                },
                {
                    "sent": "OK, so imbalanced prediction basically is defined by these two assertions.",
                    "label": 0
                },
                {
                    "sent": "OK, first of all, the user has a known uniform importance assigned to the predicted performance within the target variable.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not in different for him whether you are making a mistake here or there.",
                    "label": 0
                },
                {
                    "sent": "Some things are more important for the end user.",
                    "label": 0
                },
                {
                    "sent": "OK, and the other thing is that the subset of cases which are more important are much less present on the training data than the normal cases.",
                    "label": 0
                },
                {
                    "sent": "OK, it's only the conjunction of these two conditions that make an imbalance problem.",
                    "label": 0
                },
                {
                    "sent": "OK, because if it is in different for the user whatever.",
                    "label": 0
                },
                {
                    "sent": "Your mistakes are happening then you don't have a problem.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not a problem that some cases are poorly represented because for the user is indifferent, OK, and also if if the more important cases are not under represented, then that's not a problem also, so it's the conjunction of these two things that make this hard problem.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you a very short illustration of this thing.",
                    "label": 0
                },
                {
                    "sent": "So in classification these are just two toy examples.",
                    "label": 0
                },
                {
                    "sent": "One typically has something like this, so a class, the so-called negative class, which is very present on our training data and then a few of these so called positive but more important for the use user examples OK. And so the problem is that the user really wants our models to be very accurate on this.",
                    "label": 0
                },
                {
                    "sent": "Positive class and the other one is more or less or at least less relevant.",
                    "label": 0
                },
                {
                    "sent": "OK, so in regression, for instance.",
                    "label": 0
                },
                {
                    "sent": "Here we have the approximation of the distribution of a target parables of some problem and what we see is that the cases are concentrated over.",
                    "label": 0
                },
                {
                    "sent": "In this case near 0 values and then we have a few extreme extremely high values of this target variable.",
                    "label": 0
                },
                {
                    "sent": "Anna in this concrete example it happens that the user is really particularly interested in.",
                    "label": 0
                },
                {
                    "sent": "You know accurate predictions on these very rare values OK, and so if you want here we have another hypothetical relevance function that, in a way, Maps these interests of the user.",
                    "label": 0
                },
                {
                    "sent": "OK, it says that well over here it's more more or less you relevant, so very low values of these relevance.",
                    "label": 0
                },
                {
                    "sent": "But then as we start to grow in terms of the range of the target variable, then our importance or the importance assigned by the user starts to grow.",
                    "label": 0
                },
                {
                    "sent": "Also OK, and by the way this is, you know this is a relevance function that is automatically obtained by procedure that we have proposed which is based on, you know the distribution of the target variable and automatically derives this this function assuming that I extreme values are the more relevant for the for the user.",
                    "label": 0
                },
                {
                    "sent": "OK so because of course it would be very demanding to ask for a user to define this function fully defined this function.",
                    "label": 0
                },
                {
                    "sent": "But we we propose means to automatically derive it from the training data, OK?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "As we all know, problems imbalanced distribution creates all sorts of problems to learning methods, and in particular they typically require special purpose evaluation metrics because things, like a curacy you know, do not give good feedback when you have imbalanced classification, and the same thing happens in regression.",
                    "label": 0
                },
                {
                    "sent": "So things like mean squared error mean absolute deviation.",
                    "label": 0
                },
                {
                    "sent": "I mean, they tend to be biased stores.",
                    "label": 0
                },
                {
                    "sent": "The most frequent cases which are the less relevant for the user, so the same phenomenon.",
                    "label": 0
                },
                {
                    "sent": "Occurs with this standard error metrics in regression.",
                    "label": 0
                },
                {
                    "sent": "Then what happens in the standard error metrics in classification like accuracy or error rate.",
                    "label": 0
                },
                {
                    "sent": "OK, so they do not reflect the performance of the models on the more important cases.",
                    "label": 1
                },
                {
                    "sent": "OK now, so typically we need special purpose evaluation metrics and Moreover we need to somehow make the learning algorithms focus on these rare situations instead of focusing on the more.",
                    "label": 1
                },
                {
                    "sent": "Pre valent situations OK, and that happens both on classification and regression.",
                    "label": 0
                },
                {
                    "sent": "That's in a way, our motivation.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I'll be focused on this issue.",
                    "label": 0
                },
                {
                    "sent": "How to make the learning algorithms focus on the more relevant cases?",
                    "label": 0
                },
                {
                    "sent": "This was already addressed before by our group proposing no evaluation metrics for this sort of problems.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just a brief taxonomy of this.",
                    "label": 1
                },
                {
                    "sent": "Typical strategies for handling imbalanced domains both on classification and regression, so at higher level you have, you know, data pre processing strategies that tryin away either to change the distribution, which will be what we are going to talk here today or you know, wait the data space.",
                    "label": 1
                },
                {
                    "sent": "OK so these involve a kind of preprocessing step.",
                    "label": 0
                },
                {
                    "sent": "Then we have special purpose learning methods and also.",
                    "label": 0
                },
                {
                    "sent": "Prediction post processing.",
                    "label": 0
                },
                {
                    "sent": "So after the models get some prediction, we somehow change that to make them more in accordance to the user bias.",
                    "label": 0
                },
                {
                    "sent": "OK. And then there are set of hybrid methods that somehow integrate both of these.",
                    "label": 0
                },
                {
                    "sent": "Some of this approach.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to very briefly go through these things so data preprocessing.",
                    "label": 0
                },
                {
                    "sent": "Essentially the goal is to change the distribution of the examples on the training data that we have before applying any learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "OK, so the main advantage is that if we apply use these methods then we can use whatever learning algorithm on these modified training data, so we don't need to.",
                    "label": 1
                },
                {
                    "sent": "We can apply any learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "OK, so the disadvantages are that it may be difficult to decide the optimal distribution.",
                    "label": 0
                },
                {
                    "sent": "That better copes with the user goals.",
                    "label": 1
                },
                {
                    "sent": "And you know some of these strategies.",
                    "label": 0
                },
                {
                    "sent": "Typically, you know either increase or decrease the total number of examples of the initial training sample, and that may not be desirable.",
                    "label": 0
                },
                {
                    "sent": "If it is something computationally very demanding, or if we have very sparse data set OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No special purpose learning methods, so here the goal is actually to change the learning algorithms OK, and you know, in a way to provide a better filter with this imbalance distribution.",
                    "label": 1
                },
                {
                    "sent": "OK, the advantages that you know they are very effective and Moreover the models tend to be more comprehensible because the algorithms were changed in accordance to this bias.",
                    "label": 0
                },
                {
                    "sent": "OK, so the models tend to be reflect this change.",
                    "label": 0
                },
                {
                    "sent": "OK now.",
                    "label": 0
                },
                {
                    "sent": "Of course, there are several disadvantages because we need actually to know a lot about the algorithms and be able to change them in order to make this adaptation OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now post processing here.",
                    "label": 0
                },
                {
                    "sent": "The goal is to somehow change the predictions that the models output OK, and we do that in a way to be more in accordance with the user preference bias and the main advantage is that again, as in preprocessing.",
                    "label": 0
                },
                {
                    "sent": "Again we can use whatever learning algorithm, OK.",
                    "label": 1
                },
                {
                    "sent": "But the disadvantage is is we have a potential loss of the models interpretability.",
                    "label": 1
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because we have a prediction which is modified, and so the thing that we showed to the user it's not a direct reflection of the model.",
                    "label": 0
                },
                {
                    "sent": "So to justify this prediction we need to explain the modification which is not always very easy, OK?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So the key issue, one of the key issues here is this notion of relevance function, and so because it's thanks to this function that we are able to determine what are the relevant ranges of the target variable OK, and so, as I mentioned, this is a mapping function that Maps the original domain of the target variable into a 01 scale, and so as I also mentioned, this was.",
                    "label": 1
                },
                {
                    "sent": "You know already defined a few years back by my PhD student Ribadeo on our PhD thesis proposed this relevance function and also an automatic method to derive this from the training sample.",
                    "label": 0
                },
                {
                    "sent": "Although it is also possible to of course manually specify this function by providing a set of key points within this this curve.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we are going to use this within our proposals.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's now focus on risk concrete resampling strategies for handling imbalanced regression tasks.",
                    "label": 1
                },
                {
                    "sent": "Namely, I'll be talking about this five proposals that we have made.",
                    "label": 0
                },
                {
                    "sent": "OK, some of them are just, you know, a simple modifications of strategies that already exist for imbalance classification.",
                    "label": 0
                },
                {
                    "sent": "OK, the last one is relatively novel.",
                    "label": 0
                },
                {
                    "sent": "Uh, situation?",
                    "label": 0
                },
                {
                    "sent": "OK now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have also created an R package that implement that is available to the research community that implements all these methods and also most of the existing methods for imbalanced classification.",
                    "label": 1
                },
                {
                    "sent": "OK, so only I will use it here for some short illustrations, but it implements not only regression but also classification approaches for imbalanced learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you wish to ever try if you want to play with some data, OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's start with random undersampling.",
                    "label": 1
                },
                {
                    "sent": "So the idea is very simple, so all cases with the target fellow which have a very low relevance actually lower than the user defined threshold are candidates for undersampling.",
                    "label": 1
                },
                {
                    "sent": "OK, so the user decides this threshold and also the proportion that target proportion that he wants on the final modified training set in terms of the rare and normal cases.",
                    "label": 0
                },
                {
                    "sent": "OK, so and then.",
                    "label": 1
                },
                {
                    "sent": "These resampled data set is basically obtained by randomly removing examples from these uninteresting subset.",
                    "label": 1
                },
                {
                    "sent": "OK, those which have a relevance lower than the user defined threshold.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a very simple idea.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned before, given the threshold and the relevance function, we can split the training data in two subsets, the ones with the rare unimportant values and the ones with the normal cases.",
                    "label": 0
                },
                {
                    "sent": "So then we go to the normal cases and randomly remove some of them.",
                    "label": 0
                },
                {
                    "sent": "That's what random.",
                    "label": 0
                },
                {
                    "sent": "Undersampling does OK. Now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just, you know, brief over examples of how to use that using this package are package that I mentioned, so you can simply state what is the target variable and in this case I'm using some algae blooms data set.",
                    "label": 0
                },
                {
                    "sent": "Essentially what you have to know here is that this is the name of the target variable and it is a variable which has a distribution like the one I was showing before.",
                    "label": 0
                },
                {
                    "sent": "That is, most of the values are around 0 and then we have some.",
                    "label": 0
                },
                {
                    "sent": "Extreme values which correspond to algae blooms OK. And of course here the main goal is to try to be able to anticipate these blooms and so the main goal of the user is this extreme.",
                    "label": 0
                },
                {
                    "sent": "High values of this target parable.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the original unchanged an unbalanced data set.",
                    "label": 1
                },
                {
                    "sent": "So here I'm just saying, well, do random undersampling on this task, forecasting this algae using this originally unbalanced data set by default this thing will create a new data set which.",
                    "label": 0
                },
                {
                    "sent": "By default, will try to balance the number of rare and normal cases OK, so the goal here will be to have an equal number of rare and normal cases.",
                    "label": 0
                },
                {
                    "sent": "By default.",
                    "label": 0
                },
                {
                    "sent": "That's the behavior OK. Now you can also with some parameters tweak this thing.",
                    "label": 0
                },
                {
                    "sent": "You can go for extreme where you say well just invert the distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, we have very small number of rare very high number of.",
                    "label": 0
                },
                {
                    "sent": "Normal, so just inverted diffusion OK?",
                    "label": 0
                },
                {
                    "sent": "And so that's a very extreme thing to do.",
                    "label": 0
                },
                {
                    "sent": "Or you can explicitly say the amount of undersampling the percentage of undersampling that you want to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just very briefly what you can do with this function.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is kind of.",
                    "label": 0
                },
                {
                    "sent": "A picture of the impact on the distribution when using these three approaches.",
                    "label": 0
                },
                {
                    "sent": "OK, so here in black you have the original distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, here you have the.",
                    "label": 0
                },
                {
                    "sent": "Relevance function automatically derived.",
                    "label": 0
                },
                {
                    "sent": "So more important to the extreme, I hear it's more or less irrelevant and we are using as a default as a threshold, which is the default of these functions.",
                    "label": 0
                },
                {
                    "sent": "Although you can of course change that 05 relevance.",
                    "label": 0
                },
                {
                    "sent": "OK, that means that.",
                    "label": 0
                },
                {
                    "sent": "These values are considered normal.",
                    "label": 0
                },
                {
                    "sent": "These are considered important for the user OK. An from this assumption, then this is the effect of the balance, so it tries to somehow make the number of cases here equal to the number of cases over there.",
                    "label": 0
                },
                {
                    "sent": "And then you see here the extreme.",
                    "label": 0
                },
                {
                    "sent": "Of course, it pushes the distribution over there because it creates more rare cases and then this is just, you know.",
                    "label": 0
                },
                {
                    "sent": "And tuned sink where I just set the amount of undersampling that I wanted.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just an overall picture of the impact on the distribution of the target variable caused by this.",
                    "label": 0
                },
                {
                    "sent": "In this case these three examples.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so random oversampling, again, very simple idea.",
                    "label": 1
                },
                {
                    "sent": "Basically we introduce random copies of the examples in the original data set OK, and this replicas are, you know.",
                    "label": 1
                },
                {
                    "sent": "I mean, we leave the normal cases untouched and then we pick the the important cases and re replicate them randomly.",
                    "label": 0
                },
                {
                    "sent": "Very simple idea.",
                    "label": 1
                },
                {
                    "sent": "Again we need the relevance function to define these two sets and relevance threshold but then.",
                    "label": 0
                },
                {
                    "sent": "It's very simple to implement.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing again a few examples.",
                    "label": 0
                },
                {
                    "sent": "Again, we have this option of balancing or making it extreme and then.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have similar.",
                    "label": 0
                },
                {
                    "sent": "Impact.",
                    "label": 0
                },
                {
                    "sent": "You know, shifting the distribution towards the more important or relevant cases for the user.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the expected impact of this distribution change.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now another method that we have proposed is this Gaussian noise that essentially combines oversampling with undersampling OK, so it does oversampling by generating new synthetic examples.",
                    "label": 1
                },
                {
                    "sent": "And it does that by picking one of the rare cases.",
                    "label": 0
                },
                {
                    "sent": "OK, Ann, just randomly permuting the variables values, OK?",
                    "label": 0
                },
                {
                    "sent": "And then that creates a new example, which in principle is very similar to any existing rare case.",
                    "label": 1
                },
                {
                    "sent": "OK, that's the idea of these random small perturbations.",
                    "label": 0
                },
                {
                    "sent": "And then at the same time we under sampled the more normal cases.",
                    "label": 1
                },
                {
                    "sent": "Now the target variable of these knew generated cases is obtained.",
                    "label": 0
                },
                {
                    "sent": "The gain by introducing some small random perturbation to the original target of the seat example that was used to create these random case.",
                    "label": 0
                },
                {
                    "sent": "This new case.",
                    "label": 0
                },
                {
                    "sent": "OK, so again very simple idea.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, here are a few examples of how to use this through our package.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again we see this kind of shift of the distribution towards the more important cases for the user.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "We can also this.",
                    "label": 0
                },
                {
                    "sent": "It's a bit irrelevant, but you can also change the amount of perturbation.",
                    "label": 0
                },
                {
                    "sent": "This random perturbation you can with the parameter you know make it more aggressive or less aggressive, but.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially, we didn't observe, you know lots of variation in terms of the distribution, which means that it's a bit irrelevant this parameter OK?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No smart Smarties are very well known algorithm that was developed for imbalanced classification problems.",
                    "label": 0
                },
                {
                    "sent": "OK again, it combines undersampling with oversampling OK. Now.",
                    "label": 0
                },
                {
                    "sent": "We have adapted this smart algorithm for regression, which we call small tar and essentially the idea of under sampling is the same as random undersampling.",
                    "label": 0
                },
                {
                    "sent": "That's nothing you and the part of oversampling instead of this thing of randomly perturb rating the cases like Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "In this case what we do is that we pick from the.",
                    "label": 0
                },
                {
                    "sent": "We pick one case from the rare cases and we pick the nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "And then we somehow interpolate between the two.",
                    "label": 0
                },
                {
                    "sent": "OK, with some random shuffling instead of picking the middle value, we kind of obtain a value within the the two values of each feature.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's more or less the general idea of smart.",
                    "label": 0
                },
                {
                    "sent": "So pick some examples, some rare cases.",
                    "label": 0
                },
                {
                    "sent": "Search for the most similar rare cases and then use them to kind of interpolate and create a new case.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's more or less the idea, so we did the same for regression.",
                    "label": 0
                },
                {
                    "sent": "The main difference compared to classification is you decide the target variable value because in case of classification it's easy because all of them are positive class because they are rare, but here they are.",
                    "label": 0
                },
                {
                    "sent": "They have differences on the target variable.",
                    "label": 1
                },
                {
                    "sent": "OK, So what we have used is a kind of weighted average of the values of the target.",
                    "label": 1
                },
                {
                    "sent": "Over the two seed examples you know to determine what is the value of the target variable.",
                    "label": 1
                },
                {
                    "sent": "OK, and these weights are kind of inverse function of the distance to the generated case to the two CC examples.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's the idea of smart.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have implemented that on our package here.",
                    "label": 0
                },
                {
                    "sent": "It requires a bit more care because this issue of finding the neighbors of course requires a distance function to be defined OK, and so we're the.",
                    "label": 0
                },
                {
                    "sent": "The function also requires the specification of a function to calculate the distances, and in particularly in principle it could be the cuisine, normal equation distance.",
                    "label": 0
                },
                {
                    "sent": "But if you have like nominal and numeric variables, which is the case of this data, then you need more surface, slightly more sophisticated distance functions that end.",
                    "label": 0
                },
                {
                    "sent": "And both handle both types of variables.",
                    "label": 0
                },
                {
                    "sent": "OK, but you can specify a function through this parameter OK?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, we see the same sort of, you know, shift of the original distribution tours.",
                    "label": 0
                },
                {
                    "sent": "The more important cases, and this is you know what you get with smarter OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now finally we have recently presented this works over weighted relevance based combination strategy.",
                    "label": 0
                },
                {
                    "sent": "OK so the key idea here or the motivation was to try to make the life of a user of the user as simple as possible OK, and so we didn't like this issue of the user specifying a threshold.",
                    "label": 0
                },
                {
                    "sent": "OK Y 05070.",
                    "label": 0
                },
                {
                    "sent": "It may be easy for naive users to set this side on this thing OK?",
                    "label": 0
                },
                {
                    "sent": "So what we have tried to do is whether we could use the shape of the relevance function as a way to determine whether we should.",
                    "label": 0
                },
                {
                    "sent": "And the sample or over oversample an outmatch.",
                    "label": 0
                },
                {
                    "sent": "OK So what we have to use is to use the score of the relevance function as a kind of probability of resampling an example.",
                    "label": 1
                },
                {
                    "sent": "So an example which has a very high score of the relevance will be more probably oversampled.",
                    "label": 0
                },
                {
                    "sent": "An example which has a very low score of these relevance will be more prone to be, you know, just removed from the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the idea.",
                    "label": 0
                },
                {
                    "sent": "So oversampling.",
                    "label": 0
                },
                {
                    "sent": "Examples with higher relevance have higher probability an in undersampling, you know the randomly selected examples to be removed are you know you know the.",
                    "label": 1
                },
                {
                    "sent": "Inverse or the complement of this relevance score?",
                    "label": 0
                },
                {
                    "sent": "OK, now the main thing here are advantages that we don't ask the user to to set this relevance threshold because we use the information from the relevance function.",
                    "label": 0
                },
                {
                    "sent": "OK now.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are just some examples uses of course, as I mentioned, the user doesn't need to set this threshold, but it can if he wants.",
                    "label": 0
                },
                {
                    "sent": "You know it can set, but the simplest thing is you know without any parameters just the data did this gets a balance.",
                    "label": 0
                },
                {
                    "sent": "A distribution changed data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the easiest way where the user just say, well, this is the task.",
                    "label": 0
                },
                {
                    "sent": "This is the data the original data give me a re sample data set.",
                    "label": 0
                },
                {
                    "sent": "OK, and it uses.",
                    "label": 0
                },
                {
                    "sent": "It infers the the relevance function and it uses to decide the amount of oversampling and undersampling and where to this take place is OK, so that's the from a user perspective.",
                    "label": 0
                },
                {
                    "sent": "That's the simplest set.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and these are other variants OK, and again you get the same sort of.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Effect or impact, you know, just have urufu OS.",
                    "label": 0
                },
                {
                    "sent": "Examples and that's what we get.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we dismantled.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course the next question.",
                    "label": 0
                },
                {
                    "sent": "Well so many methods.",
                    "label": 0
                },
                {
                    "sent": "What is the best?",
                    "label": 0
                },
                {
                    "sent": "What should I use?",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a made an extensive experimental analysis of these methods.",
                    "label": 1
                },
                {
                    "sent": "More specifically, we have considered the following strategy, so do nothing.",
                    "label": 1
                },
                {
                    "sent": "Just use the original data, which is kind of our baseline, then a few variants of random under sampling random oversampling, small tar, Goshen noise and also this works.",
                    "label": 1
                },
                {
                    "sent": "Variant then we have tried several regression algorithms like linear regression, 8 variants of neural Nets, four variants of Mars, 12 VM's and six random forests.",
                    "label": 1
                },
                {
                    "sent": "And we have selected 15 regression datasets with different amount of imbalance on the target variable OK.",
                    "label": 0
                },
                {
                    "sent": "So that makes up, you know, a total of seven, roughly 8000 experimental settings.",
                    "label": 0
                },
                {
                    "sent": "OK, now all models were compared to use this F measure for regression, which is one of these evaluation metrics that we have derived for this type of imbalanced regression tasks and by means of you know, 2 * 10 fold cross validation and then we have used Wilcoxon signed rank test to test for the statistical significance of the.",
                    "label": 1
                },
                {
                    "sent": "Observe differences OK now.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the kind of summary of the results.",
                    "label": 1
                },
                {
                    "sent": "OK, So what you see here is the total number of wins on the left in blue, and losses on the right in Brown of each of the simplest sampling strategies against the baseline of doing nothing.",
                    "label": 1
                },
                {
                    "sent": "OK, So what this means is that run one of the variance of random oversampling 300 roughly 300 times, got a better score than.",
                    "label": 0
                },
                {
                    "sent": "Doing nothing OK. On this, for all these you know settings that we have tried OK, so the difference between the two colors is that these are the number of times these differences were statistically significant.",
                    "label": 0
                },
                {
                    "sent": "According to this, Wilcoxon signed rank tests OK.",
                    "label": 0
                },
                {
                    "sent": "So the more important are of course the darker bars.",
                    "label": 0
                },
                {
                    "sent": "The other ones OK there are wins, but they are not statistically significant OK?",
                    "label": 0
                },
                {
                    "sent": "So in general, what we could say is that this works.",
                    "label": 1
                },
                {
                    "sent": "Methodist seems to be clearly the more advantages advantage of the proposals, particularly because it.",
                    "label": 0
                },
                {
                    "sent": "Almost never loses compared to doing a nothing to the original data.",
                    "label": 0
                },
                {
                    "sent": "OK, and so that's the main difference on a.",
                    "label": 0
                },
                {
                    "sent": "That's our overall recommendation, OK?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some of the conclusions, recommendations or summary of some of the results of these large sets of experiments.",
                    "label": 0
                },
                {
                    "sent": "So we have observed that.",
                    "label": 0
                },
                {
                    "sent": "Run them for us together with either of these three sampling strategies, achieve the best score on almost, or at least a big part of the datasets.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's one of the observations there were there were other curious things about some of the learning algorithms you know I can send you, then the paper it's actually being reviewed now, so I cannot put here the link, but hopefully it will get accepted and then we can because there are some other interesting effects on some of the learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So as usual, it's not like this thing is the best way.",
                    "label": 0
                },
                {
                    "sent": "Whatever you have.",
                    "label": 0
                },
                {
                    "sent": "OK, as usually there is some variation, but overall we think that this proposal achieved a more consistent performance when comparing to using the original data.",
                    "label": 1
                },
                {
                    "sent": "We respectively of the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's overall our conclusion and also Moreover it is more user friendly because we don't ask the user to set this.",
                    "label": 0
                },
                {
                    "sent": "You know, sometimes unintuitive.",
                    "label": 1
                },
                {
                    "sent": "Thresholds and it is also computationally more efficient because it doesn't involve calculating the neighbors like smart, which sometimes it's computationally heavy an you know it doesn't generate new cases, just replicates them.",
                    "label": 1
                },
                {
                    "sent": "And also it doesn't increase the training set size OK?",
                    "label": 0
                },
                {
                    "sent": "Alright, so sorry it was the last slide.",
                    "label": 0
                },
                {
                    "sent": "That's why somehow.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you have any questions.",
                    "label": 0
                },
                {
                    "sent": "That you want to ask.",
                    "label": 0
                },
                {
                    "sent": "Sorry about this.",
                    "label": 0
                },
                {
                    "sent": "Buck no no no no.",
                    "label": 0
                },
                {
                    "sent": "This is already for the classes.",
                    "label": 0
                },
                {
                    "sent": "I mean I could, I.",
                    "label": 0
                },
                {
                    "sent": "The demos are running this code that I've shown you, so it's not very interesting I think.",
                    "label": 0
                },
                {
                    "sent": "Alright, so thank you.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you have any questions that you want to ask.",
                    "label": 0
                },
                {
                    "sent": "There will be, but first let's thank Louise OK. We just put together the title.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Yes people.",
                    "label": 0
                },
                {
                    "sent": "All this.",
                    "label": 0
                },
                {
                    "sent": "OK, you avoid it relevance threshold, but you still have to specify the relevance function.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Said that, will generate.",
                    "label": 0
                },
                {
                    "sent": "Yeah so we have these two options.",
                    "label": 0
                },
                {
                    "sent": "Our hypothesis is that in most cases people will not be able to propose this function and that's why we have this automatic way of deriving the function.",
                    "label": 0
                },
                {
                    "sent": "OK, and this works well, assuming that your goal are the extreme values.",
                    "label": 0
                },
                {
                    "sent": "So these automatic way assumes that the interests of the user are the rare extreme values be just one.",
                    "label": 0
                },
                {
                    "sent": "In case of this example, just had one or two OK, two Tails.",
                    "label": 0
                },
                {
                    "sent": "OK, so under that exemption, which is the most frequent case on imbalanced regression under deck assumption, then we can provide an automatic way of deriving this function.",
                    "label": 0
                },
                {
                    "sent": "OK, so the user doesn't need to do anything at all.",
                    "label": 0
                },
                {
                    "sent": "OK, if not then we have other means which are a compromise where the user can just pinpoint a few points on this curve OK?",
                    "label": 0
                },
                {
                    "sent": "Where you say well for this value, my relevance score would be something around 034 that is slightly higher for that, and with this few points then we have some methods to try to interpolate this and derive this this relevance function OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are different ranges of complexity.",
                    "label": 0
                },
                {
                    "sent": "It depends on the domain knowledge that you have, but if you don't have anything at all.",
                    "label": 0
                },
                {
                    "sent": "Which was the case in our experiments, because these are standard datasets available on repository's.",
                    "label": 0
                },
                {
                    "sent": "If you know nothing about this thing, then we can use these automatic sync that assigns more importance to the rare extreme ranges of the target, OK?",
                    "label": 0
                },
                {
                    "sent": "Do you really need to resample or can you use this advance scores as weights?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's a good question, so we also try.",
                    "label": 0
                },
                {
                    "sent": "I didn't show here this thing, but we also try this thing and using the weights generally doesn't work as well as resampling OK?",
                    "label": 0
                },
                {
                    "sent": "Any intuition why?",
                    "label": 0
                },
                {
                    "sent": "Well, I think that the intuition you mean using the weights when learning when learning.",
                    "label": 0
                },
                {
                    "sent": "Well, in a way, first of all you are restricted to learning algorithms that allow you to.",
                    "label": 0
                },
                {
                    "sent": "You know, input this weights right?",
                    "label": 0
                },
                {
                    "sent": "OK, and actually if you use these weights as a form of resampling, which for instance other boosts can do similar things because it replicates the cases where the previous model failed, and I mean in theory other boost uses probabilities, But if your classifier doesn't use probabilities and then what it does is replicating this case is an.",
                    "label": 0
                },
                {
                    "sent": "So you could do the same thing here so.",
                    "label": 0
                },
                {
                    "sent": "If you use the relevance scores as a kind of probability and then replicate the cases and give that to the training to the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "But that's exactly what works does.",
                    "label": 0
                },
                {
                    "sent": "So it's using this relevance as a kind of weight to guide the resampling.",
                    "label": 0
                },
                {
                    "sent": "OK, but that has the advantage that you can use whatever learning algorithm irrespective if it accepts weights or not on the training data, OK?",
                    "label": 0
                },
                {
                    "sent": "But I would say that.",
                    "label": 0
                },
                {
                    "sent": "The things should not be very different, but it is more.",
                    "label": 0
                },
                {
                    "sent": "It has more limitations because it constrains you on the type of learning algorithms that you use.",
                    "label": 0
                },
                {
                    "sent": "You know, let's say you're trying to do just parameter fitting.",
                    "label": 0
                },
                {
                    "sent": "You have either linear or nonlinear regression, but the form of the function is known and you just try to fit the parameters.",
                    "label": 0
                },
                {
                    "sent": "Then this relevant functions could be then mapped to the loss function.",
                    "label": 0
                },
                {
                    "sent": "You know, so that you don't have uniform loss.",
                    "label": 0
                },
                {
                    "sent": "You don't just measure the difference between the measured in the predicted value, but you know that's actually what this F1 four regression does.",
                    "label": 1
                },
                {
                    "sent": "So this F1 for regression uses the information on the relevance function to wait the errors by this.",
                    "label": 0
                },
                {
                    "sent": "Information in a way.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, that's what it does.",
                    "label": 0
                },
                {
                    "sent": "It looks at the our important is a test case.",
                    "label": 0
                },
                {
                    "sent": "Looking at these relevance and then the error committed by the model is more has gets more weight.",
                    "label": 0
                },
                {
                    "sent": "If this is more relevant, OK.",
                    "label": 0
                },
                {
                    "sent": "So it does that, but it's slightly more complex because you need to not only look at the predicted value, but also to the true value.",
                    "label": 0
                },
                {
                    "sent": "Because you have kind of false positives and false negatives.",
                    "label": 0
                },
                {
                    "sent": "Also, because your your model may be predicting a rare value, but in true, in reality it's a normal value, so it's A kind of false alarm or it can happen the opposite OK?",
                    "label": 0
                },
                {
                    "sent": "That you know you have a rare event being happened, but your model say, well, no, no, no.",
                    "label": 0
                },
                {
                    "sent": "This is a normal thing, so if you look at this algae blooms you could have a model, say, well, this we are going to have a algae bloom, but it didn't happen.",
                    "label": 0
                },
                {
                    "sent": "OK, or you could say, well, no, no, it's everything is OK with the River, but then there was an algae bloom, so this sort of things.",
                    "label": 0
                },
                {
                    "sent": "So you have to look at the relevance not only of the true value of the test case, but also of the predicted value, and you have to wait this.",
                    "label": 0
                },
                {
                    "sent": "It cost functions.",
                    "label": 1
                },
                {
                    "sent": "Yeah yeah, we we call it.",
                    "label": 0
                },
                {
                    "sent": "Utility surface is where we take this thing.",
                    "label": 0
                },
                {
                    "sent": "These two things these two dimensions into into account.",
                    "label": 0
                },
                {
                    "sent": "But this F1 for regression uses this information.",
                    "label": 0
                },
                {
                    "sent": "So it uses the relevance function to evaluate the predictions of the model in this way.",
                    "label": 0
                },
                {
                    "sent": "So and this is then obviously related to cost sensitive learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it is very related, yeah?",
                    "label": 0
                },
                {
                    "sent": "In a way you can look at these utility services as a continuous version of a cost benefit matrix.",
                    "label": 0
                },
                {
                    "sent": "Instead of having a grid of situations, you have a surface.",
                    "label": 0
                },
                {
                    "sent": "Like you, you still have like the predicted value, the true value, but then you have a surface.",
                    "label": 0
                },
                {
                    "sent": "Of utility.",
                    "label": 1
                },
                {
                    "sent": "Sometimes it's a cost, it's a negative value.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's a benefit because it's a positive value.",
                    "label": 0
                },
                {
                    "sent": "OK, but even positive values you know it's more important to forecasts correctly and algae bloom then to forecast correctly that nothing happened.",
                    "label": 0
                },
                {
                    "sent": "So the benefit is also a different.",
                    "label": 0
                },
                {
                    "sent": "On all of these surfaces are derived from this relevance function.",
                    "label": 0
                },
                {
                    "sent": "So in the classification case, you would have a cost misclassification cost matrix and this would be just the constants there in this matrix.",
                    "label": 0
                },
                {
                    "sent": "When you move to the regression case, obviously there is arguments for actually having a function in symbolic form that represents this, because otherwise, how do you represent the surface?",
                    "label": 0
                },
                {
                    "sent": "You can represent it visually because it's a 2D sync.",
                    "label": 0
                },
                {
                    "sent": "You can show it to the user and what we do is that we provide means to automatically derive them like we do for the relevance function.",
                    "label": 0
                },
                {
                    "sent": "Of course we are not going to ask for the user to specify specify this surface, but again we have this hypothesis of him him or her giving some specific points, and then we interpolate the surface.",
                    "label": 0
                },
                {
                    "sent": "Or we can derive it automatically from the relevance function.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Comments.",
                    "label": 0
                },
                {
                    "sent": "So what are the interesting applications of this?",
                    "label": 0
                },
                {
                    "sent": "You mean this resampling till now you mean real world know till now it's an academic thing.",
                    "label": 0
                },
                {
                    "sent": "So we have been playing with benchmark datasets so.",
                    "label": 0
                },
                {
                    "sent": "No no stock market.",
                    "label": 0
                },
                {
                    "sent": "That's non disclosable information.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}