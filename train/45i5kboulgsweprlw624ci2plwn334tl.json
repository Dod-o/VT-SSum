{
    "id": "45i5kboulgsweprlw624ci2plwn334tl",
    "title": "Living on the Edge - Phase Transitions in Random Convex Programs",
    "info": {
        "author": [
            "Joel Tropp, Department of Computing and Mathematical Sciences, California Institute of Technology (Caltech)"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Machine Learning->Regularization",
            "Top->Computer Science->Compressed Sensing"
        ]
    },
    "url": "http://videolectures.net/roks2013_tropp_optimization/",
    "segmentation": [
        [
            "Just want to start by thanking the organizers again for hosting such a great meeting in a beautiful location, and unfortunately there will be no discussion of matrix concentration in this talk.",
            "Apologies, so to begin, let me just take a note of my coauthors Michael McCoy, who is one of my students at Celtic, and.",
            "Just finished his degree and Dennis him along soon and Martin lots who are both going to Manchester and the fall.",
            "I'm also going to be talking a little bit about some work of my colleague Summer Toy.",
            "Mac is a graduate student at Caltech and Baka CB, so all of this is a reference to appropriately OK, so first let me tell you what we're talking about."
        ],
        [
            "Phase transitions"
        ],
        [
            "So what's a phase transition?",
            "So my definition is sort of heuristic.",
            "It's a sharp change in the behavior of a computational problem as you vary the parameters and one parameter regime, you see one behavior.",
            "You change the parameters a little bit and the behavior is completely different.",
            "So the example that is probably most familiar to anyone who's looked at a paper on compressed sensing or sparsity in the last decade is the phase transition that occurs in a sparse linear inverse problem with random data.",
            "This is also known as the compressed sensing problem.",
            "So we're going to suppose that there is a true unknown.",
            "It's a vector of length D that has S non zero entries, so it's sparse.",
            "We're going to acquire M linear measurements of this unknown vector by computing inner product with standard normal vectors.",
            "So take em random measurements, and then we'd like to reconstruct the.",
            "Target vector from our knowledge of the measurement vectors GI and the value zi of the measurement, and as you all know, we accomplish this by minimizing the L1 norm of the decision variable X subject to consistency with the measurements that we've acquired.",
            "And this is a proxy for sparsity.",
            "It's a regularizer that promotes sparsity.",
            "And when we solve this problem indeed, sometimes we are able to reconstruct the original sparse vector.",
            "Ex natural so."
        ],
        [
            "Here's a diagram of what happens.",
            "So on this axis we have 100 dimensional problem.",
            "On this axis we have the number of non zero entries in the target vector X natural and on this vertical axis we have the number of random measurements that we've acquired and the color map.",
            "Here the heat map indicates whether or not solving that 01 minimization problem successfully identified the unknown target vector or not.",
            "So in this black region epic failure.",
            "And in the white region uniform success and you can see that there is sort of a narrow ledge here where the probabilities are intermediate, but it's quite narrow and the only reason is really even visible is because I chose to show you a small example.",
            "So the message here is that at a given level of sparsity, a given number of non zero entries in the target vector.",
            "If we don't have enough measurements were just out of lock, we cannot reconstruct.",
            "The target vector by solving the L1 minimization problem, whereas if we have enough measurements, we are able to reconstruct the target vector and the change occurs very quickly over a very small number of measurements.",
            "And so when you look at this picture.",
            "Certainly motivates ones intellectual curiosity you'd really like to know what's going on here.",
            "What phenomenon is this?",
            "How can we explain this?",
            "Especially once you start looking at other papers on sparse regularization related problems and see the same picture in every paper.",
            "Always there's this phase transition between success and failure runway.",
            "Acquire enough information."
        ],
        [
            "So the research challenge that I like to talk to you about today is understanding and predicting phase transitions and random convex optimization problems so."
        ],
        [
            "Let me take just one more moment to motivate why it is you might actually care about random convex programs if you weren't already persuaded.",
            "So example I began with the sensing, so in this case we collect random measurements of some object.",
            "This out in the world and we'd like to reconstruct that object by solving an optimization problem.",
            "So the randomness and the problem comes from the randomness and the measurements that we've acquired, and so for any given.",
            "Draw of the random measurements.",
            "The instance of the optimization problem that we have to solve can be viewed as random.",
            "The randomness comes from the sensing measurements in statistics.",
            "This is a machine learning.",
            "This is of course completely standard.",
            "We model data as a random process and we want to fit a model for regression or estimation or classification or one of these things by solving an optimization problem and so the randomness in the data becomes a random component.",
            "Of the optimization problem that we use for estimation.",
            "So once again statistics and machine learning are full of random optimization problems.",
            "You see this thing in coding theory lots and lots of other applications, and some of the reasons that it might be interesting to actually write down one of these random models and try and understand how the optimization problem behaves is that you get fundamental bounds on behavior.",
            "You get opportunities for convex programs in the setting where you have some random.",
            "Data or random measurements and you find limits on the performance of these convex programs beyond the phase transition.",
            "This convex program will not solve the problem, so you'd really like to understand where that phase transition is.",
            "And I also think about this like an average case analysis.",
            "You can maybe think about randomness is describing typical or even favorable behavior, and the kinds of problems that arise in statistics and machine learning and so forth and so.",
            "By analyzing a random convex optimization problem, you can get a sense of what you might hope to achieve in practice rather than the worst case bounds, which tend to be very pessimistic in these settings.",
            "So.",
            "Now that I have persuaded you that this is."
        ],
        [
            "Worthwhile direction of study.",
            "I want to start out with an example that doesn't have a phase transition, but in which the geometry is extremely transparent and it will help you understand some of the quantities that show up in the analysis.",
            "So we're going to regularize denoising problems."
        ],
        [
            "So here's the setup so.",
            "We've got a target vector X natural one today which we believe to be structured, but we don't know what the vector is.",
            "So when I say structure I mean things like smooth or sparse or low rank or block sparse.",
            "One of these things that you see in the literature on sparsity structure, inverse problems, machine learning and we will construct a regularizer F which is a convex function that measures how structured a vector is when the value of the function F is small.",
            "We think the vector is very structured as low complexity when the value of the function F is high.",
            "We believe that the vector argument of the function is very complex, so by minimizing.",
            "F We can try to minimize the complexity of a vector, and so we think about F is somehow encoding what we mean by structure.",
            "So examples include the L1 norm.",
            "If we believe the sparsity is the structure of interest might be the Chattan one norm of the trace storm.",
            "If we believe that low rank is the structure of interest and so forth, I'm not going to talk about how to build regularizers today for the reason that you've probably seen a lot of them.",
            "You have probably have your favourites.",
            "And everything I'm telling you is completely generic.",
            "It works for essentially any non smooth regularizer you can think of.",
            "OK, so here's the problem.",
            "We observe X natural.",
            "This target vector corrupted with white Gaussian noise with power Sigma squared times dimension.",
            "So the variance of each component of the noise is Sigma squared and our goal is to remove the noise from X natural by solving an optimization problem.",
            "So the problem that we're going to pose.",
            "Is to find a vector X this close to our observation in Euclidean norm and at the same time is less complex than the target vector X natural?",
            "I'm going to assume for the purposes of this talk that we know the complexity in advance.",
            "That's roughly the same as having a really good choice.",
            "For the LaGrange parameter.",
            "So our hope now is that the minimizer X hat of this optimization problem is a good approximation to X natural.",
            "The target vector that we believe we are the corrupted version of, so we have an optimization problem here.",
            "The randomness comes in Z inside the observation, and we'd like to understand how much of the noise that we succeed in removing using this particular choice of regularization."
        ],
        [
            "So here's the geometry of this problem so.",
            "Here I'm looking at the sparse case, so think about X and autism, X naturals being a vector that has mostly zero entries in a relatively small number of non zero entries.",
            "So it's sitting up here at a vertex out sitting here on one of the axes and the regularizer in this example is the L1 norm, so I've indicated here with this darker blue region.",
            "The sub level set of the L1 norm at the target vector X natural.",
            "Now we don't observe X natural.",
            "We observe some point Z, which is a perturbed version of X natural perturbed by Sigma W. The noise and what you can see is that if you solve this optimization problem, we're looking inside this sublevel set for the best approximation of Z.",
            "The projection of Z onto the sublevel set, and that's the solution X hat to the optimization problem is our estimator.",
            "For the true vector, and so you can see that the error is just a discrepancy between X hat the estimate and next natural the ground truth.",
            "So what we've done here is we've projected the noise onto the sublevel set and the norm of this vector here that I've labeled error is the error that we incur in our estimate.",
            "And of course, if you want to study a statistical problem, you average over choices of the error, and so you'd like to understand the average size of the error in this problem.",
            "Now I've drawn something else here.",
            "A white blue region here, which is a cone.",
            "This is the cone of descent directions from X natural shifted up so that it's apex is X natural, and so you can see that this Kona descent directions as an outer approximation to the sublevel set.",
            "And so the size of the projection of the noise onto this cone is never any smaller than the projection onto the sublevel set.",
            "OK, so this is sort of evident in this picture.",
            "The projection of the noise onto the cone is always bigger than the projection onto the sublevel set, and so if we look at the expected squared magnitude of the projection of the noise onto this cone we get a.",
            "Upper estimate for the error that we incur in this problem, but something else you can see is that if Sigma is really, really small, we're up here in this region.",
            "Here in the Conan the sublevel set look the same and so there's not really any different between looking at this cone of descent directions and looking at the sublevel set.",
            "Provided that the noise is small, OK, any questions on this diagram?",
            "Somehow this encapsulates everything there is to say about denoising.",
            "Yeah.",
            "With this inside the.",
            "Inside the apartment, then the projection onto convex that doesn't go anywhere nor onto the descent cone.",
            "Code.",
            "So in that case there's nothing you can do about the noise.",
            "So what does that tell you?",
            "Tells you that if this cone is big, you eat a lot of noise.",
            "There's not much that you can remove if the cone is small, well, maybe you stand a chance.",
            "There's not much likelihood you're going to land in the cone, and in fact, the projection onto the cone might be quite small.",
            "Small cone.",
            "Lots of denoising, big cone, not so much denoising.",
            "So that's really the geometric picture here, alright?"
        ],
        [
            "So it turns out that what I told you heuristically is exactly the truth.",
            "So if we observe a noisy of vector X natural corrupted with some amount of white noise, we solve this optimization problem to denoise it, then the mean squared error in the estimator.",
            "Relative to the variance of the noise is equal to some number.",
            "Delta the statistical dimension of the descent cone of the regularizer at the target vector.",
            "I'm going to tell you what this is an excruciating detail in just a moment.",
            "So what this tells you is that when the cones big.",
            "You eat a lot of noise.",
            "You have a lot of trouble removing noise from the problem when this cone, the descent cone is small, you're in good shape.",
            "The statistical dimension is a measure of how big a cone is and what you can see from this diagram is that the statistical dimension of this code governs the performance of denoising, with the regularizer F. So if you no.",
            "This tester will dimension you know exactly what the worst mean squared error is for any noise power.",
            "So this is a result due to my colleague some salmon oil mackenbach asebe and what they've shown here is that if you know the statistical dimension of a dissent cone of your regularizer, you know everything about the performance of this denoising method.",
            "Now it turns out that this statistical dimension governs the behavior of all sorts of optimization problems with random data, and the goal of this lecture is to explain how this arises in linear inverse problems, demixing problems and random comb programs among other examples."
        ],
        [
            "So now I need to tell you what the statistical dimension is of sort of hinted at this, but we need a death."
        ],
        [
            "So.",
            "We're going to let K be a closed convex cone and will define the statistical dimension of this cone as the squared magnitude of the Euclidean projection of a standard normal vector onto the cone.",
            "In average.",
            "So take a Gaussian vector projection on the cone, compute the squared magnitude, average overall Gaussians, OK.",
            "So this is exactly what's happening in this picture."
        ],
        [
            "Here we've taken a. Gaussian vector W. We projected it onto the cone.",
            "The error here is this mean squared error is exactly the statistical dimension of this descent cone that I've drawn here.",
            "So the statistical mentioned."
        ],
        [
            "Think about really as a description of what happens in denoising problem and you should have the intuition that again, when that descent cone or when the cone K is big, the statistical dimension is big when the cone K is very narrow, the statistical dimension is small.",
            "And in fact, the reason that we call this thing's statistical dimension is because it is a Canonical extension of the dimension of a linear subspace to convex cones.",
            "So if you plug a subspace in here, you get the dimension of the subspace.",
            "But you can define this for any cone whatsoever, and the intuition you should take about this quantity is that in a stochastic geometry problem, a convex cone with statistical dimension Delta behaves a sensually the same way as a random subspace with the same dimension.",
            "So this is sort of a surprising fact about geometry.",
            "Cones behave like subspaces when they're randomly oriented, and the randomness in this example just comes from the error and some of the other problems will see the randomness will come in through the.",
            "Construction of the optimization prob."
        ],
        [
            "OK, So what is this thing?",
            "Can we compute it?",
            "The answer is yes and it's got very natural interpretations.",
            "A lot of examples, so a subspace has statistical dimension equal to the dimension of the subspace.",
            "The non negative orthant has statistical mentioned equal to half the ambient dimension.",
            "Bunch of these other examples.",
            "The statistical dimension is half the ambient dimension for the Lawrence Cone.",
            "The real PSD.",
            "Conan the complex PSD cone.",
            "So all of these can be computed exactly without much trouble.",
            "OK, so again it's a measure of how big a cone is.",
            "OK."
        ],
        [
            "Circular cones, so this is a cone.",
            "Generated by the .0 and a spherical cap of a given angle Alpha and you can compute this statistical mention pretty accurately here and normalized it by the ambient dimension here so you see.",
            "But as the angle grosses statistical mentioned traces out the shape of a sine curve, and again, I've normalized us by the ambient dimension here.",
            "OK, so you compute this."
        ],
        [
            "Imagine the circular cones descent cones.",
            "So remember that cone of a function is all of the directions where the function goes downhill and.",
            "Saint Cone is in fact a cone, so if you've got a non smooth function F here I've drawn the sublevel, set.",
            "The descent cone is really just this cone of directions that I've shaded here in Gray.",
            "And you can compute the statistical mention of all sorts of dissent cones.",
            "There's a recipe for doing that."
        ],
        [
            "So here are a couple examples, so if you look at the L1 norm in a sparse vector, you can.",
            "Compute the statistical dimension of US descent cone.",
            "The parameterisation in this picture here is the number of nonzeros per as a function of.",
            "Sorry the number of nonzeros relative to the dimension of the vector and on this axis we have this statistical dimension divided by the.",
            "Dimension of the vector space and you can see it races this lovely curve here and this orange curve underneath is what happens if you add a non negativity constraint to the L1 norm.",
            "OK, so you can compute still mentions for the L1 norm.",
            "You can compute the statistical."
        ],
        [
            "Mention for the Chattan one norm, the trace norm at a low rank matrix, and we've parameterized curves here.",
            "Depending on the aspect ratio, the matrix, how broad it is, so the message here is really just this is this is a quantity that we can understand.",
            "It's something that we can compute and is something that helps us understand.",
            "At the very least, the performance of denoising problems.",
            "Gives the exact performance of regularised denoising OK?"
        ],
        [
            "But here's the thing.",
            "The statistical mention also plays a role in the understanding of phase transitions.",
            "For random convex programs.",
            "The key fact that you need to understand here, which I'll try and illustrate when we come to some specific examples, is.",
            "So the key question here is task one to two randomly oriented cones share array.",
            "So you take 2 cones, you adjoin them at their vertex, and then you smack one of 'em so that spins around like a roulette wheel.",
            "The question is when it stops, do the two cones have array in common or is the only point in common the vertex?",
            "And.",
            "If you want to understand the answer to this question, the intuition you should take is to ask when two randomly oriented subspaces share a line.",
            "So when do they intersect and the answer to that question is easy if the total dimension of the subspace is smaller than the ambient dimension, they don't intersect 2 random subspaces.",
            "Whose dimension is smaller than the ambient dimension?",
            "Do not share a line with probability one.",
            "OK, so you can see this by performing a little thought experiment.",
            "If you have two lines and R3 and you spin them around.",
            "They only intersect at zero if you have a line in a plane and are three, you spin them around.",
            "They only intersect at zero.",
            "OK, so those are the two cases where the dimension sum to less than or equal to ambient dimension.",
            "But if you have two planes in R3 and you spend one of them around at random.",
            "And always shares a line.",
            "So that's the case where the ambient dimensions.",
            "Or the sum of the two dimensions exceeds the ambient dimension.",
            "And in that case you have an intersection between the subspace with probability one.",
            "Turns out the intersections exactly correct for cones as well.",
            "So here's the result with this is called the approximate kinematic formula to.",
            "Simplification of a an exact formula that goes back to the 1940s with the work of Charron, Blaschko, Santalo, and others.",
            "An integral geometry.",
            "So if we have two closed convex cones, SNK in D dimensions and the total dimension of the cones is smaller than the ambient dimension.",
            "Then with probability one, the cone C and they randomly rotated copy of the cone K. Have a trivial intersection, so the total dimension small two randomly oriented cones do not intersect.",
            "Conversely.",
            "If the total statistical dimension of the two cones exceeds the ambient dimension, then with high probability.",
            "If we Orient one of the cones at random and intersect with the other cone, the chance of a trivial intersection is essentially 0.",
            "So you see a sharp transition into this probability from one to zero.",
            "When the total statistical dimension of the two cones changes from smaller than the ambient dimension to larger than the ambient dimension.",
            "So you're starting to see where this phase transition might come from.",
            "We just have to connect this fact with the geometry of some optimization problems that arise in practice.",
            "Any questions on the statistical mention or?",
            "These facts that have labeled the approximate kinematic formula.",
            "So in a lot of examples you can actually prove nice things like it's an analytic curve.",
            "Things like that.",
            "I think perhaps a more interesting question is what am I hiding here in this approximate equal to?",
            "It turns out that it's terms of lower order, something like the square root of the dimension.",
            "So this really is negligible for large problems.",
            "OK, so."
        ],
        [
            "Let's talk about regularize linear inverse problems, also known as compressed sensing."
        ],
        [
            "So here's the setup for these problems.",
            "We've already seen one example of this problem on the 1st slide, the phase transition and compressed sensing, so it turns out that all of these problems have a phase transition and we can compute where that is.",
            "So, whoops, so X natural is a structured unknown vector that we would like to acquire.",
            "The way we do that is by taking linear measurements of X natural using some observation process, and we observe data Z.",
            "So vectors E of M measurements of this D dimensional vector and the way that we're going to try and estimate the unknown vector X natural, is by solving an optimization problem.",
            "We're going to insist that the decision variable X reproduce the measurements that we've acquired, so this is a consistency with the data constraint and.",
            "Out of all of the possibilities, we're going to look for the one that is the least complex, so we need to tune the regularizer F to the type of structure we're looking for, just as always.",
            "So if we're looking for sparse things, this is the L1 norm for looking for low rank things.",
            "It's a shot in one norm, and so forth, so we solve this optimization problem and we really hope that the solution X hat equals.",
            "The ground truth X natural.",
            "OK, so if we pick an appropriate regularizer, maybe we can achieve this goal.",
            "Now."
        ],
        [
            "The geometry of this problem is familiar to anyone who's seen a talk on compressed sensing, so we've got this vector X natural here.",
            "We've got the sublevel set of the L1 ball.",
            "And I've drawn in the descent cone of the L1 norm, translated up to, so that is Apex is at X natural OK.",
            "So.",
            "X Natural is the unique solution of that optimization problem if and only if.",
            "The kernel of a + X natural, so this shifted.",
            "This afine space doesn't intersect this cone here.",
            "So we need to make sure that this.",
            "The null space of the measurement operator doesn't intersect the descent cone, otherwise there will be directions we can move to improve the complexity of our estimate.",
            "That are inside the descent cone of X natural, so we could reduce the complexity.",
            "Without changing.",
            "The observations that would be the bad situation.",
            "OK, now in a compressed sensing problem we think about this."
        ],
        [
            "German operation A as being random were joining random measurements of the data and."
        ],
        [
            "Under reasonable models, that's equivalent to the kernel of the Matrix a being a random subspace.",
            "What you should think about here is we've got X natural sitting here at one of the vertices of the L1 ball.",
            "This cone is emanating away from that, and we've spun this affine space randomly around this hinge here at X natural, and then the question is.",
            "Does this line intersect this cone or not?",
            "And the approximate account komatik formula tells you the answer to that question.",
            "If the total dimension of the null space and the cone is smaller than the ambient dimension you unlock at the total mentions bigger than the ambient dimension.",
            "Epic failure this optimization problem will not reconstruct the ground truth and so you can express this in a theorem."
        ],
        [
            "So.",
            "This is the same setup I just showed you a moment before, but now we're assuming the measurements are a standard normal matrix.",
            "And the question is when does the regularised optimization problem reconstruct the ground truth?",
            "And the answer is that if the number of measurements exceeds the statistical dimension of the descent cone of the regularizer at the ground truth, then the optimization problem works, and if the number of measurements is smaller, the optimization problem doesn't work and the probabilities are overwhelming.",
            "So right at this point here?",
            "When the number of measurements is too small.",
            "It's hopeless, you will not reconstruct.",
            "The target vector, if you have enough measurements, you're Golden and this happens very quickly.",
            "Failure, failure, failure, failure, success."
        ],
        [
            "So here are a couple of pictures to convince you that what we're doing here actually corresponds with.",
            "Results of empirical experiments.",
            "So this is the same picture I showed you before for the compressed sensing problem, so we're trying to reconstruct a sparse vector by minimizing the L1 norm and.",
            "On the horizontal axis we have the level of sparsity, the number of nonzero entries in X natural on the vertical axis.",
            "We have the number of measurements, and as I showed you before, when the number of measurements are small.",
            "Epic failure when the number of measurements is big, overwhelming success.",
            "The color map indicates the probability, so this red curve that I've drawn in here is the 50% success isocline computed from the data.",
            "So this is the line where you get it right half the time, so the yellow curve underneath that you can't really see is the theoretical prediction.",
            "So this is really describes exactly what happens in this setting, and I've also drawn in the 5% in the 95% isoclines to give you a sense of how wide the transition is, and you can see that it's quite narrow.",
            "Any questions on this?",
            "So this is a very idealized analysis.",
            "Realistically, numerical experiments suggest that everything behaves the same and that there is an invariant principle here as well, but no ones identified that except in some limited cases for the L1 norm, but certainly not in the generality I'm talking about here."
        ],
        [
            "This applies to any regularised linear inverse problem.",
            "I mean, with the caveat that the regularizer ought to be non smooth, otherwise its descent cones are trivial, they're all half spaces, so you don't really care about that example very much.",
            "That's by the way is why you don't see phase transitions for least squares.",
            "But if you have a nonsmooth regularizer, this analysis essentially goes through without any assumptions, and you got a very clear prediction of how many measurements you need to reconstruct the unknown vector, assuming that the measurements are in fact isotropic.",
            "How is it changing when you apply wait to tell one.",
            "We haven't made the calculation so it's fairly straightforward, but it depends in a sort of detailed why on the choice of weights.",
            "So you could work it out for a specific set of weights, but I don't know if there's going to be any kind of simple formula.",
            "In fact, the curves here."
        ],
        [
            "Are already there analytic functions?",
            "But they involve error functions and you know implicit equations and things solution.",
            "Yeah, if you do reweighting, it's sort of clearly.",
            "You can make the descent cone smaller and as a result you need fewer measurements.",
            "So heuristically it's clear what you need to accomplish with the RE weighting.",
            "You need to make descent cone smaller.",
            "If you do that then.",
            "You'll do better.",
            "OK, here's another example just to convince you that this isn't a one off."
        ],
        [
            "This is the recovery of a low rank square matrix from random measurements.",
            "By minimizing the Schatten one norm.",
            "So on this axis we have the rank of the target matrix on this vertical axis we have the number of random measurements we've drawn, and the red curve.",
            "Here is the 50% success Isocline.",
            "The yellow curve is the theoretical prediction, so you can see that they coincide exactly.",
            "And by the way, you can see that the phase transition is actually a lot narrower here, for the reason that it's a much bigger problem.",
            "So remember that the phase transition has with square root of the ambient dimension and so it's getting narrower narrower as the problems are getting bigger.",
            "OK, so we can understand regularised linear inverse problems using this statistical dimension analysis.",
            "And as I showed you before, we can compute these in all sorts."
        ],
        [
            "Examples, so here's another example demixing.",
            "So this is the problem of separating two superimposed signals from each other."
        ],
        [
            "And this method.",
            "Provides a clean analysis of this problem as well, so let X not and why not be too structured?",
            "Unknown vectors, so maybe this is sparse and one basis, and this is also sparse and we're going to let you be a known orthogonal matrix, so it's going to model the relative orientation of these two signals.",
            "In practice, this is often just the identity.",
            "Or it's.",
            "Somehow implicit in the choice of structure, so we're going to assume that we observe a superposition of X natural and a rotated copy of Y natural.",
            "OK, so we see a superposition of these two signals, one of which has been.",
            "Reoriented and our goal is to pull them apart.",
            "Given the observation, OK, there's a natural convex of program that can achieve this.",
            "So the decision variables are X&Y, and we're going to insist that they are consistent with the observed data, so X plus you I had better reproduce what we saw.",
            "We're going to insist that why be less complicated than Y natural.",
            "The second of the target vectors, and subject to that constraint, we're going to minimize the complexity of the first vector.",
            "And our hope is that the solution to this problem, X hat Y hat coincides with the ground truth exponential Y natural OK."
        ],
        [
            "So what's the geometry here?",
            "Well.",
            "In this case, it's a lot more complicated, but suffice it to say that we have two descent cones.",
            "The descent cone of the regularizer for X, and the descent cone of the regularizer for Y. Rotated.",
            "By the Matrix you so we have two cones, one of which is rotated, and we succeed in identifying the ground truth.",
            "When these two cones here do not share array, that's the situation where there is no simultaneous descent direction for both F&G.",
            "On the other hand, if these two cones do share array, there is a simultaneous descent direction for both of the regularizers and.",
            "We fail to reconstruct the ground truth, so in the case where we model random incoherence, so we think about the two structures being oblique with each other, which is the usual setting and signal processing problems.",
            "It corresponds to rotating this red cone.",
            "At random.",
            "And so then the question is when does this red cone?",
            "When it's randomly oriented, strike this blue cone.",
            "Well, we already know the answer to that question."
        ],
        [
            "The kinematic formula tells us and it says that if the total dimension of the two descent codes is small, we can D mix the signals.",
            "So if the two signals aren't very complicated, we can pull them apart, provided that there don't have orientation in common.",
            "And if the complexity of the two signals is large, we can't pull them apart.",
            "Makes good intuitive sense, and we have a detailed numerical prediction of when we can do this, and we can't so."
        ],
        [
            "Here's an example.",
            "We've got a vector that's Parson one basis and sparse in a random basis plus another vector.",
            "The sparse in random basis, and we're going to try and.",
            "Do you mix them by solving a problem using 2L1 norms one to promote sparsity in the first vector, the other to promote sparsity in the second vector.",
            "So in this diagram Weaver aid the number of nonzeros in X natural in this axis and the number of nonzeros and why natural in the vertical axis.",
            "So we see that when the two vectors are both very sparse, we can pull them apart without much trouble, and when their dense.",
            "Impossible.",
            "Once again, the red curve is a 50% success isocline the yellow curves of theoretical prediction.",
            "So this tells us exactly when we can separate these two types of.",
            "Structured vectors."
        ],
        [
            "Here's an example for rank sparsity decomposition, sometimes known as robust PCA.",
            "In this case we have a low rank matrix for which we use the shotgun one norm regularizer and we have a sparse matrix for which we use the L1 norm regularizer and when we frame this demixing problem, we see that when the rank is small and the sparsity is small, everything works fine.",
            "And when the rank is high and the sparsity is high.",
            "The demixing problem fails once again.",
            "The theoretical prediction coincides perfectly with the empirical 50% isocline.",
            "So any questions on Demixing?",
            "OK."
        ],
        [
            "One more example, sokon programs with random constraints."
        ],
        [
            "So I'm just going to state the result here.",
            "The geometry is actually a little bit more complicated and you have to do a bit more work to get this result, but assume that we have a cone program, so the decision variable logs to a proper cone.",
            "And we have M random affine constraints, so that means that the Matrix A is got standard normal entries, the right hand side B has standard normal entries and our aim is to minimize a random linear form subject to these constraints.",
            "So it turns out that cone programs exhibit a phase transition depending on how many affine constraints you enforce.",
            "So when the number of affine constraints is smaller than the statistical dimension of the cone.",
            "The cone program is feasible and unbounded with high probability.",
            "So if there aren't too many constraints, there's a feasible point.",
            "On the other hand, if the number of constraints is larger than the dimension of the cone, the cone program is infeasible with high probability.",
            "There's no point that satisfies the.",
            "Um, equality the affine equality constraints in the common constraint.",
            "So this is a phase transition.",
            "I don't think that's been observed before in the behavior of any cone program K as a proper code.",
            "So once again, the statistical dimension the size of this cone which we were able to compute, governs of performance or governs the behavior of this type of random optimization problem.",
            "And just to indicate that this analysis really is.",
            "Correct?"
        ],
        [
            "We did some examples with various numbers of cones.",
            "These heavy dashed lines here are the empirical 50% marks for these logistic regressions and the heavy dashed lines that are right next to them are the theoretical estimates for.",
            "The number of constraints you're going to need to see before you move from feasibility and feasibility.",
            "Yeah.",
            "Is a little bit fancier because this."
        ],
        [
            "These are all fine now.",
            "Yeah.",
            "So there's a one more degree or one fewer degrees of freedom I guess."
        ],
        [
            "Alright."
        ],
        [
            "So.",
            "In 40 minutes is a little bit hard to do.",
            "Full justice to this entire theory, but I hope I've persuaded you that this analysis really provides for the first time a consistent set of tools for analyzing the performance of a wide class of random optimization problems that we see all the time in statistics, machine learning, and lots of other areas of modern inquiry.",
            "So most of the work I've described today is in a paper joint with Dennis Amalong, some Martin lots.",
            "Michael McCoy called living on the edge the edge referring to this.",
            "Ledger, the phase transition, and there's also work from my colleagues.",
            "Some annoy mackenbach asebe that's available in archive that describes the role of the statistical dimension and.",
            "Denoising problems and there are more papers on this on the way, so I appreciate your attention and thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just want to start by thanking the organizers again for hosting such a great meeting in a beautiful location, and unfortunately there will be no discussion of matrix concentration in this talk.",
                    "label": 0
                },
                {
                    "sent": "Apologies, so to begin, let me just take a note of my coauthors Michael McCoy, who is one of my students at Celtic, and.",
                    "label": 0
                },
                {
                    "sent": "Just finished his degree and Dennis him along soon and Martin lots who are both going to Manchester and the fall.",
                    "label": 1
                },
                {
                    "sent": "I'm also going to be talking a little bit about some work of my colleague Summer Toy.",
                    "label": 0
                },
                {
                    "sent": "Mac is a graduate student at Caltech and Baka CB, so all of this is a reference to appropriately OK, so first let me tell you what we're talking about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Phase transitions",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's a phase transition?",
                    "label": 1
                },
                {
                    "sent": "So my definition is sort of heuristic.",
                    "label": 0
                },
                {
                    "sent": "It's a sharp change in the behavior of a computational problem as you vary the parameters and one parameter regime, you see one behavior.",
                    "label": 1
                },
                {
                    "sent": "You change the parameters a little bit and the behavior is completely different.",
                    "label": 1
                },
                {
                    "sent": "So the example that is probably most familiar to anyone who's looked at a paper on compressed sensing or sparsity in the last decade is the phase transition that occurs in a sparse linear inverse problem with random data.",
                    "label": 0
                },
                {
                    "sent": "This is also known as the compressed sensing problem.",
                    "label": 1
                },
                {
                    "sent": "So we're going to suppose that there is a true unknown.",
                    "label": 0
                },
                {
                    "sent": "It's a vector of length D that has S non zero entries, so it's sparse.",
                    "label": 0
                },
                {
                    "sent": "We're going to acquire M linear measurements of this unknown vector by computing inner product with standard normal vectors.",
                    "label": 0
                },
                {
                    "sent": "So take em random measurements, and then we'd like to reconstruct the.",
                    "label": 0
                },
                {
                    "sent": "Target vector from our knowledge of the measurement vectors GI and the value zi of the measurement, and as you all know, we accomplish this by minimizing the L1 norm of the decision variable X subject to consistency with the measurements that we've acquired.",
                    "label": 0
                },
                {
                    "sent": "And this is a proxy for sparsity.",
                    "label": 0
                },
                {
                    "sent": "It's a regularizer that promotes sparsity.",
                    "label": 0
                },
                {
                    "sent": "And when we solve this problem indeed, sometimes we are able to reconstruct the original sparse vector.",
                    "label": 0
                },
                {
                    "sent": "Ex natural so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a diagram of what happens.",
                    "label": 0
                },
                {
                    "sent": "So on this axis we have 100 dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "On this axis we have the number of non zero entries in the target vector X natural and on this vertical axis we have the number of random measurements that we've acquired and the color map.",
                    "label": 0
                },
                {
                    "sent": "Here the heat map indicates whether or not solving that 01 minimization problem successfully identified the unknown target vector or not.",
                    "label": 0
                },
                {
                    "sent": "So in this black region epic failure.",
                    "label": 0
                },
                {
                    "sent": "And in the white region uniform success and you can see that there is sort of a narrow ledge here where the probabilities are intermediate, but it's quite narrow and the only reason is really even visible is because I chose to show you a small example.",
                    "label": 0
                },
                {
                    "sent": "So the message here is that at a given level of sparsity, a given number of non zero entries in the target vector.",
                    "label": 0
                },
                {
                    "sent": "If we don't have enough measurements were just out of lock, we cannot reconstruct.",
                    "label": 0
                },
                {
                    "sent": "The target vector by solving the L1 minimization problem, whereas if we have enough measurements, we are able to reconstruct the target vector and the change occurs very quickly over a very small number of measurements.",
                    "label": 0
                },
                {
                    "sent": "And so when you look at this picture.",
                    "label": 0
                },
                {
                    "sent": "Certainly motivates ones intellectual curiosity you'd really like to know what's going on here.",
                    "label": 0
                },
                {
                    "sent": "What phenomenon is this?",
                    "label": 0
                },
                {
                    "sent": "How can we explain this?",
                    "label": 0
                },
                {
                    "sent": "Especially once you start looking at other papers on sparse regularization related problems and see the same picture in every paper.",
                    "label": 0
                },
                {
                    "sent": "Always there's this phase transition between success and failure runway.",
                    "label": 0
                },
                {
                    "sent": "Acquire enough information.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the research challenge that I like to talk to you about today is understanding and predicting phase transitions and random convex optimization problems so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me take just one more moment to motivate why it is you might actually care about random convex programs if you weren't already persuaded.",
                    "label": 0
                },
                {
                    "sent": "So example I began with the sensing, so in this case we collect random measurements of some object.",
                    "label": 1
                },
                {
                    "sent": "This out in the world and we'd like to reconstruct that object by solving an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So the randomness and the problem comes from the randomness and the measurements that we've acquired, and so for any given.",
                    "label": 0
                },
                {
                    "sent": "Draw of the random measurements.",
                    "label": 0
                },
                {
                    "sent": "The instance of the optimization problem that we have to solve can be viewed as random.",
                    "label": 0
                },
                {
                    "sent": "The randomness comes from the sensing measurements in statistics.",
                    "label": 0
                },
                {
                    "sent": "This is a machine learning.",
                    "label": 0
                },
                {
                    "sent": "This is of course completely standard.",
                    "label": 0
                },
                {
                    "sent": "We model data as a random process and we want to fit a model for regression or estimation or classification or one of these things by solving an optimization problem and so the randomness in the data becomes a random component.",
                    "label": 0
                },
                {
                    "sent": "Of the optimization problem that we use for estimation.",
                    "label": 0
                },
                {
                    "sent": "So once again statistics and machine learning are full of random optimization problems.",
                    "label": 0
                },
                {
                    "sent": "You see this thing in coding theory lots and lots of other applications, and some of the reasons that it might be interesting to actually write down one of these random models and try and understand how the optimization problem behaves is that you get fundamental bounds on behavior.",
                    "label": 0
                },
                {
                    "sent": "You get opportunities for convex programs in the setting where you have some random.",
                    "label": 1
                },
                {
                    "sent": "Data or random measurements and you find limits on the performance of these convex programs beyond the phase transition.",
                    "label": 1
                },
                {
                    "sent": "This convex program will not solve the problem, so you'd really like to understand where that phase transition is.",
                    "label": 0
                },
                {
                    "sent": "And I also think about this like an average case analysis.",
                    "label": 0
                },
                {
                    "sent": "You can maybe think about randomness is describing typical or even favorable behavior, and the kinds of problems that arise in statistics and machine learning and so forth and so.",
                    "label": 0
                },
                {
                    "sent": "By analyzing a random convex optimization problem, you can get a sense of what you might hope to achieve in practice rather than the worst case bounds, which tend to be very pessimistic in these settings.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now that I have persuaded you that this is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Worthwhile direction of study.",
                    "label": 0
                },
                {
                    "sent": "I want to start out with an example that doesn't have a phase transition, but in which the geometry is extremely transparent and it will help you understand some of the quantities that show up in the analysis.",
                    "label": 0
                },
                {
                    "sent": "So we're going to regularize denoising problems.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the setup so.",
                    "label": 0
                },
                {
                    "sent": "We've got a target vector X natural one today which we believe to be structured, but we don't know what the vector is.",
                    "label": 1
                },
                {
                    "sent": "So when I say structure I mean things like smooth or sparse or low rank or block sparse.",
                    "label": 0
                },
                {
                    "sent": "One of these things that you see in the literature on sparsity structure, inverse problems, machine learning and we will construct a regularizer F which is a convex function that measures how structured a vector is when the value of the function F is small.",
                    "label": 1
                },
                {
                    "sent": "We think the vector is very structured as low complexity when the value of the function F is high.",
                    "label": 0
                },
                {
                    "sent": "We believe that the vector argument of the function is very complex, so by minimizing.",
                    "label": 0
                },
                {
                    "sent": "F We can try to minimize the complexity of a vector, and so we think about F is somehow encoding what we mean by structure.",
                    "label": 0
                },
                {
                    "sent": "So examples include the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "If we believe the sparsity is the structure of interest might be the Chattan one norm of the trace storm.",
                    "label": 0
                },
                {
                    "sent": "If we believe that low rank is the structure of interest and so forth, I'm not going to talk about how to build regularizers today for the reason that you've probably seen a lot of them.",
                    "label": 0
                },
                {
                    "sent": "You have probably have your favourites.",
                    "label": 0
                },
                {
                    "sent": "And everything I'm telling you is completely generic.",
                    "label": 0
                },
                {
                    "sent": "It works for essentially any non smooth regularizer you can think of.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the problem.",
                    "label": 0
                },
                {
                    "sent": "We observe X natural.",
                    "label": 0
                },
                {
                    "sent": "This target vector corrupted with white Gaussian noise with power Sigma squared times dimension.",
                    "label": 0
                },
                {
                    "sent": "So the variance of each component of the noise is Sigma squared and our goal is to remove the noise from X natural by solving an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So the problem that we're going to pose.",
                    "label": 1
                },
                {
                    "sent": "Is to find a vector X this close to our observation in Euclidean norm and at the same time is less complex than the target vector X natural?",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume for the purposes of this talk that we know the complexity in advance.",
                    "label": 0
                },
                {
                    "sent": "That's roughly the same as having a really good choice.",
                    "label": 0
                },
                {
                    "sent": "For the LaGrange parameter.",
                    "label": 1
                },
                {
                    "sent": "So our hope now is that the minimizer X hat of this optimization problem is a good approximation to X natural.",
                    "label": 0
                },
                {
                    "sent": "The target vector that we believe we are the corrupted version of, so we have an optimization problem here.",
                    "label": 0
                },
                {
                    "sent": "The randomness comes in Z inside the observation, and we'd like to understand how much of the noise that we succeed in removing using this particular choice of regularization.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the geometry of this problem so.",
                    "label": 1
                },
                {
                    "sent": "Here I'm looking at the sparse case, so think about X and autism, X naturals being a vector that has mostly zero entries in a relatively small number of non zero entries.",
                    "label": 0
                },
                {
                    "sent": "So it's sitting up here at a vertex out sitting here on one of the axes and the regularizer in this example is the L1 norm, so I've indicated here with this darker blue region.",
                    "label": 0
                },
                {
                    "sent": "The sub level set of the L1 norm at the target vector X natural.",
                    "label": 0
                },
                {
                    "sent": "Now we don't observe X natural.",
                    "label": 0
                },
                {
                    "sent": "We observe some point Z, which is a perturbed version of X natural perturbed by Sigma W. The noise and what you can see is that if you solve this optimization problem, we're looking inside this sublevel set for the best approximation of Z.",
                    "label": 0
                },
                {
                    "sent": "The projection of Z onto the sublevel set, and that's the solution X hat to the optimization problem is our estimator.",
                    "label": 0
                },
                {
                    "sent": "For the true vector, and so you can see that the error is just a discrepancy between X hat the estimate and next natural the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So what we've done here is we've projected the noise onto the sublevel set and the norm of this vector here that I've labeled error is the error that we incur in our estimate.",
                    "label": 0
                },
                {
                    "sent": "And of course, if you want to study a statistical problem, you average over choices of the error, and so you'd like to understand the average size of the error in this problem.",
                    "label": 0
                },
                {
                    "sent": "Now I've drawn something else here.",
                    "label": 0
                },
                {
                    "sent": "A white blue region here, which is a cone.",
                    "label": 0
                },
                {
                    "sent": "This is the cone of descent directions from X natural shifted up so that it's apex is X natural, and so you can see that this Kona descent directions as an outer approximation to the sublevel set.",
                    "label": 0
                },
                {
                    "sent": "And so the size of the projection of the noise onto this cone is never any smaller than the projection onto the sublevel set.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is sort of evident in this picture.",
                    "label": 0
                },
                {
                    "sent": "The projection of the noise onto the cone is always bigger than the projection onto the sublevel set, and so if we look at the expected squared magnitude of the projection of the noise onto this cone we get a.",
                    "label": 0
                },
                {
                    "sent": "Upper estimate for the error that we incur in this problem, but something else you can see is that if Sigma is really, really small, we're up here in this region.",
                    "label": 0
                },
                {
                    "sent": "Here in the Conan the sublevel set look the same and so there's not really any different between looking at this cone of descent directions and looking at the sublevel set.",
                    "label": 0
                },
                {
                    "sent": "Provided that the noise is small, OK, any questions on this diagram?",
                    "label": 0
                },
                {
                    "sent": "Somehow this encapsulates everything there is to say about denoising.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "With this inside the.",
                    "label": 0
                },
                {
                    "sent": "Inside the apartment, then the projection onto convex that doesn't go anywhere nor onto the descent cone.",
                    "label": 0
                },
                {
                    "sent": "Code.",
                    "label": 0
                },
                {
                    "sent": "So in that case there's nothing you can do about the noise.",
                    "label": 0
                },
                {
                    "sent": "So what does that tell you?",
                    "label": 0
                },
                {
                    "sent": "Tells you that if this cone is big, you eat a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "There's not much that you can remove if the cone is small, well, maybe you stand a chance.",
                    "label": 0
                },
                {
                    "sent": "There's not much likelihood you're going to land in the cone, and in fact, the projection onto the cone might be quite small.",
                    "label": 0
                },
                {
                    "sent": "Small cone.",
                    "label": 1
                },
                {
                    "sent": "Lots of denoising, big cone, not so much denoising.",
                    "label": 0
                },
                {
                    "sent": "So that's really the geometric picture here, alright?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that what I told you heuristically is exactly the truth.",
                    "label": 0
                },
                {
                    "sent": "So if we observe a noisy of vector X natural corrupted with some amount of white noise, we solve this optimization problem to denoise it, then the mean squared error in the estimator.",
                    "label": 0
                },
                {
                    "sent": "Relative to the variance of the noise is equal to some number.",
                    "label": 0
                },
                {
                    "sent": "Delta the statistical dimension of the descent cone of the regularizer at the target vector.",
                    "label": 1
                },
                {
                    "sent": "I'm going to tell you what this is an excruciating detail in just a moment.",
                    "label": 0
                },
                {
                    "sent": "So what this tells you is that when the cones big.",
                    "label": 0
                },
                {
                    "sent": "You eat a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of trouble removing noise from the problem when this cone, the descent cone is small, you're in good shape.",
                    "label": 0
                },
                {
                    "sent": "The statistical dimension is a measure of how big a cone is and what you can see from this diagram is that the statistical dimension of this code governs the performance of denoising, with the regularizer F. So if you no.",
                    "label": 0
                },
                {
                    "sent": "This tester will dimension you know exactly what the worst mean squared error is for any noise power.",
                    "label": 0
                },
                {
                    "sent": "So this is a result due to my colleague some salmon oil mackenbach asebe and what they've shown here is that if you know the statistical dimension of a dissent cone of your regularizer, you know everything about the performance of this denoising method.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that this statistical dimension governs the behavior of all sorts of optimization problems with random data, and the goal of this lecture is to explain how this arises in linear inverse problems, demixing problems and random comb programs among other examples.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I need to tell you what the statistical dimension is of sort of hinted at this, but we need a death.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're going to let K be a closed convex cone and will define the statistical dimension of this cone as the squared magnitude of the Euclidean projection of a standard normal vector onto the cone.",
                    "label": 1
                },
                {
                    "sent": "In average.",
                    "label": 0
                },
                {
                    "sent": "So take a Gaussian vector projection on the cone, compute the squared magnitude, average overall Gaussians, OK.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly what's happening in this picture.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we've taken a. Gaussian vector W. We projected it onto the cone.",
                    "label": 0
                },
                {
                    "sent": "The error here is this mean squared error is exactly the statistical dimension of this descent cone that I've drawn here.",
                    "label": 0
                },
                {
                    "sent": "So the statistical mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think about really as a description of what happens in denoising problem and you should have the intuition that again, when that descent cone or when the cone K is big, the statistical dimension is big when the cone K is very narrow, the statistical dimension is small.",
                    "label": 1
                },
                {
                    "sent": "And in fact, the reason that we call this thing's statistical dimension is because it is a Canonical extension of the dimension of a linear subspace to convex cones.",
                    "label": 0
                },
                {
                    "sent": "So if you plug a subspace in here, you get the dimension of the subspace.",
                    "label": 1
                },
                {
                    "sent": "But you can define this for any cone whatsoever, and the intuition you should take about this quantity is that in a stochastic geometry problem, a convex cone with statistical dimension Delta behaves a sensually the same way as a random subspace with the same dimension.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a surprising fact about geometry.",
                    "label": 0
                },
                {
                    "sent": "Cones behave like subspaces when they're randomly oriented, and the randomness in this example just comes from the error and some of the other problems will see the randomness will come in through the.",
                    "label": 0
                },
                {
                    "sent": "Construction of the optimization prob.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is this thing?",
                    "label": 0
                },
                {
                    "sent": "Can we compute it?",
                    "label": 0
                },
                {
                    "sent": "The answer is yes and it's got very natural interpretations.",
                    "label": 0
                },
                {
                    "sent": "A lot of examples, so a subspace has statistical dimension equal to the dimension of the subspace.",
                    "label": 0
                },
                {
                    "sent": "The non negative orthant has statistical mentioned equal to half the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "Bunch of these other examples.",
                    "label": 0
                },
                {
                    "sent": "The statistical dimension is half the ambient dimension for the Lawrence Cone.",
                    "label": 0
                },
                {
                    "sent": "The real PSD.",
                    "label": 0
                },
                {
                    "sent": "Conan the complex PSD cone.",
                    "label": 1
                },
                {
                    "sent": "So all of these can be computed exactly without much trouble.",
                    "label": 0
                },
                {
                    "sent": "OK, so again it's a measure of how big a cone is.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Circular cones, so this is a cone.",
                    "label": 1
                },
                {
                    "sent": "Generated by the .0 and a spherical cap of a given angle Alpha and you can compute this statistical mention pretty accurately here and normalized it by the ambient dimension here so you see.",
                    "label": 0
                },
                {
                    "sent": "But as the angle grosses statistical mentioned traces out the shape of a sine curve, and again, I've normalized us by the ambient dimension here.",
                    "label": 0
                },
                {
                    "sent": "OK, so you compute this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Imagine the circular cones descent cones.",
                    "label": 0
                },
                {
                    "sent": "So remember that cone of a function is all of the directions where the function goes downhill and.",
                    "label": 0
                },
                {
                    "sent": "Saint Cone is in fact a cone, so if you've got a non smooth function F here I've drawn the sublevel, set.",
                    "label": 0
                },
                {
                    "sent": "The descent cone is really just this cone of directions that I've shaded here in Gray.",
                    "label": 1
                },
                {
                    "sent": "And you can compute the statistical mention of all sorts of dissent cones.",
                    "label": 0
                },
                {
                    "sent": "There's a recipe for doing that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are a couple examples, so if you look at the L1 norm in a sparse vector, you can.",
                    "label": 1
                },
                {
                    "sent": "Compute the statistical dimension of US descent cone.",
                    "label": 1
                },
                {
                    "sent": "The parameterisation in this picture here is the number of nonzeros per as a function of.",
                    "label": 0
                },
                {
                    "sent": "Sorry the number of nonzeros relative to the dimension of the vector and on this axis we have this statistical dimension divided by the.",
                    "label": 0
                },
                {
                    "sent": "Dimension of the vector space and you can see it races this lovely curve here and this orange curve underneath is what happens if you add a non negativity constraint to the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can compute still mentions for the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "You can compute the statistical.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mention for the Chattan one norm, the trace norm at a low rank matrix, and we've parameterized curves here.",
                    "label": 1
                },
                {
                    "sent": "Depending on the aspect ratio, the matrix, how broad it is, so the message here is really just this is this is a quantity that we can understand.",
                    "label": 0
                },
                {
                    "sent": "It's something that we can compute and is something that helps us understand.",
                    "label": 0
                },
                {
                    "sent": "At the very least, the performance of denoising problems.",
                    "label": 0
                },
                {
                    "sent": "Gives the exact performance of regularised denoising OK?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But here's the thing.",
                    "label": 0
                },
                {
                    "sent": "The statistical mention also plays a role in the understanding of phase transitions.",
                    "label": 0
                },
                {
                    "sent": "For random convex programs.",
                    "label": 0
                },
                {
                    "sent": "The key fact that you need to understand here, which I'll try and illustrate when we come to some specific examples, is.",
                    "label": 0
                },
                {
                    "sent": "So the key question here is task one to two randomly oriented cones share array.",
                    "label": 1
                },
                {
                    "sent": "So you take 2 cones, you adjoin them at their vertex, and then you smack one of 'em so that spins around like a roulette wheel.",
                    "label": 0
                },
                {
                    "sent": "The question is when it stops, do the two cones have array in common or is the only point in common the vertex?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If you want to understand the answer to this question, the intuition you should take is to ask when two randomly oriented subspaces share a line.",
                    "label": 0
                },
                {
                    "sent": "So when do they intersect and the answer to that question is easy if the total dimension of the subspace is smaller than the ambient dimension, they don't intersect 2 random subspaces.",
                    "label": 0
                },
                {
                    "sent": "Whose dimension is smaller than the ambient dimension?",
                    "label": 0
                },
                {
                    "sent": "Do not share a line with probability one.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can see this by performing a little thought experiment.",
                    "label": 0
                },
                {
                    "sent": "If you have two lines and R3 and you spin them around.",
                    "label": 0
                },
                {
                    "sent": "They only intersect at zero if you have a line in a plane and are three, you spin them around.",
                    "label": 0
                },
                {
                    "sent": "They only intersect at zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are the two cases where the dimension sum to less than or equal to ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "But if you have two planes in R3 and you spend one of them around at random.",
                    "label": 0
                },
                {
                    "sent": "And always shares a line.",
                    "label": 0
                },
                {
                    "sent": "So that's the case where the ambient dimensions.",
                    "label": 0
                },
                {
                    "sent": "Or the sum of the two dimensions exceeds the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "And in that case you have an intersection between the subspace with probability one.",
                    "label": 0
                },
                {
                    "sent": "Turns out the intersections exactly correct for cones as well.",
                    "label": 0
                },
                {
                    "sent": "So here's the result with this is called the approximate kinematic formula to.",
                    "label": 0
                },
                {
                    "sent": "Simplification of a an exact formula that goes back to the 1940s with the work of Charron, Blaschko, Santalo, and others.",
                    "label": 0
                },
                {
                    "sent": "An integral geometry.",
                    "label": 1
                },
                {
                    "sent": "So if we have two closed convex cones, SNK in D dimensions and the total dimension of the cones is smaller than the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "Then with probability one, the cone C and they randomly rotated copy of the cone K. Have a trivial intersection, so the total dimension small two randomly oriented cones do not intersect.",
                    "label": 0
                },
                {
                    "sent": "Conversely.",
                    "label": 0
                },
                {
                    "sent": "If the total statistical dimension of the two cones exceeds the ambient dimension, then with high probability.",
                    "label": 0
                },
                {
                    "sent": "If we Orient one of the cones at random and intersect with the other cone, the chance of a trivial intersection is essentially 0.",
                    "label": 0
                },
                {
                    "sent": "So you see a sharp transition into this probability from one to zero.",
                    "label": 0
                },
                {
                    "sent": "When the total statistical dimension of the two cones changes from smaller than the ambient dimension to larger than the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "So you're starting to see where this phase transition might come from.",
                    "label": 0
                },
                {
                    "sent": "We just have to connect this fact with the geometry of some optimization problems that arise in practice.",
                    "label": 1
                },
                {
                    "sent": "Any questions on the statistical mention or?",
                    "label": 1
                },
                {
                    "sent": "These facts that have labeled the approximate kinematic formula.",
                    "label": 0
                },
                {
                    "sent": "So in a lot of examples you can actually prove nice things like it's an analytic curve.",
                    "label": 0
                },
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "I think perhaps a more interesting question is what am I hiding here in this approximate equal to?",
                    "label": 0
                },
                {
                    "sent": "It turns out that it's terms of lower order, something like the square root of the dimension.",
                    "label": 0
                },
                {
                    "sent": "So this really is negligible for large problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's talk about regularize linear inverse problems, also known as compressed sensing.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the setup for these problems.",
                    "label": 1
                },
                {
                    "sent": "We've already seen one example of this problem on the 1st slide, the phase transition and compressed sensing, so it turns out that all of these problems have a phase transition and we can compute where that is.",
                    "label": 0
                },
                {
                    "sent": "So, whoops, so X natural is a structured unknown vector that we would like to acquire.",
                    "label": 1
                },
                {
                    "sent": "The way we do that is by taking linear measurements of X natural using some observation process, and we observe data Z.",
                    "label": 0
                },
                {
                    "sent": "So vectors E of M measurements of this D dimensional vector and the way that we're going to try and estimate the unknown vector X natural, is by solving an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We're going to insist that the decision variable X reproduce the measurements that we've acquired, so this is a consistency with the data constraint and.",
                    "label": 0
                },
                {
                    "sent": "Out of all of the possibilities, we're going to look for the one that is the least complex, so we need to tune the regularizer F to the type of structure we're looking for, just as always.",
                    "label": 0
                },
                {
                    "sent": "So if we're looking for sparse things, this is the L1 norm for looking for low rank things.",
                    "label": 0
                },
                {
                    "sent": "It's a shot in one norm, and so forth, so we solve this optimization problem and we really hope that the solution X hat equals.",
                    "label": 0
                },
                {
                    "sent": "The ground truth X natural.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we pick an appropriate regularizer, maybe we can achieve this goal.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The geometry of this problem is familiar to anyone who's seen a talk on compressed sensing, so we've got this vector X natural here.",
                    "label": 0
                },
                {
                    "sent": "We've got the sublevel set of the L1 ball.",
                    "label": 0
                },
                {
                    "sent": "And I've drawn in the descent cone of the L1 norm, translated up to, so that is Apex is at X natural OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "X Natural is the unique solution of that optimization problem if and only if.",
                    "label": 0
                },
                {
                    "sent": "The kernel of a + X natural, so this shifted.",
                    "label": 0
                },
                {
                    "sent": "This afine space doesn't intersect this cone here.",
                    "label": 0
                },
                {
                    "sent": "So we need to make sure that this.",
                    "label": 0
                },
                {
                    "sent": "The null space of the measurement operator doesn't intersect the descent cone, otherwise there will be directions we can move to improve the complexity of our estimate.",
                    "label": 0
                },
                {
                    "sent": "That are inside the descent cone of X natural, so we could reduce the complexity.",
                    "label": 0
                },
                {
                    "sent": "Without changing.",
                    "label": 0
                },
                {
                    "sent": "The observations that would be the bad situation.",
                    "label": 0
                },
                {
                    "sent": "OK, now in a compressed sensing problem we think about this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "German operation A as being random were joining random measurements of the data and.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Under reasonable models, that's equivalent to the kernel of the Matrix a being a random subspace.",
                    "label": 0
                },
                {
                    "sent": "What you should think about here is we've got X natural sitting here at one of the vertices of the L1 ball.",
                    "label": 0
                },
                {
                    "sent": "This cone is emanating away from that, and we've spun this affine space randomly around this hinge here at X natural, and then the question is.",
                    "label": 0
                },
                {
                    "sent": "Does this line intersect this cone or not?",
                    "label": 0
                },
                {
                    "sent": "And the approximate account komatik formula tells you the answer to that question.",
                    "label": 0
                },
                {
                    "sent": "If the total dimension of the null space and the cone is smaller than the ambient dimension you unlock at the total mentions bigger than the ambient dimension.",
                    "label": 0
                },
                {
                    "sent": "Epic failure this optimization problem will not reconstruct the ground truth and so you can express this in a theorem.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the same setup I just showed you a moment before, but now we're assuming the measurements are a standard normal matrix.",
                    "label": 0
                },
                {
                    "sent": "And the question is when does the regularised optimization problem reconstruct the ground truth?",
                    "label": 0
                },
                {
                    "sent": "And the answer is that if the number of measurements exceeds the statistical dimension of the descent cone of the regularizer at the ground truth, then the optimization problem works, and if the number of measurements is smaller, the optimization problem doesn't work and the probabilities are overwhelming.",
                    "label": 0
                },
                {
                    "sent": "So right at this point here?",
                    "label": 0
                },
                {
                    "sent": "When the number of measurements is too small.",
                    "label": 0
                },
                {
                    "sent": "It's hopeless, you will not reconstruct.",
                    "label": 0
                },
                {
                    "sent": "The target vector, if you have enough measurements, you're Golden and this happens very quickly.",
                    "label": 0
                },
                {
                    "sent": "Failure, failure, failure, failure, success.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are a couple of pictures to convince you that what we're doing here actually corresponds with.",
                    "label": 0
                },
                {
                    "sent": "Results of empirical experiments.",
                    "label": 0
                },
                {
                    "sent": "So this is the same picture I showed you before for the compressed sensing problem, so we're trying to reconstruct a sparse vector by minimizing the L1 norm and.",
                    "label": 0
                },
                {
                    "sent": "On the horizontal axis we have the level of sparsity, the number of nonzero entries in X natural on the vertical axis.",
                    "label": 0
                },
                {
                    "sent": "We have the number of measurements, and as I showed you before, when the number of measurements are small.",
                    "label": 0
                },
                {
                    "sent": "Epic failure when the number of measurements is big, overwhelming success.",
                    "label": 0
                },
                {
                    "sent": "The color map indicates the probability, so this red curve that I've drawn in here is the 50% success isocline computed from the data.",
                    "label": 0
                },
                {
                    "sent": "So this is the line where you get it right half the time, so the yellow curve underneath that you can't really see is the theoretical prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is really describes exactly what happens in this setting, and I've also drawn in the 5% in the 95% isoclines to give you a sense of how wide the transition is, and you can see that it's quite narrow.",
                    "label": 0
                },
                {
                    "sent": "Any questions on this?",
                    "label": 0
                },
                {
                    "sent": "So this is a very idealized analysis.",
                    "label": 0
                },
                {
                    "sent": "Realistically, numerical experiments suggest that everything behaves the same and that there is an invariant principle here as well, but no ones identified that except in some limited cases for the L1 norm, but certainly not in the generality I'm talking about here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This applies to any regularised linear inverse problem.",
                    "label": 0
                },
                {
                    "sent": "I mean, with the caveat that the regularizer ought to be non smooth, otherwise its descent cones are trivial, they're all half spaces, so you don't really care about that example very much.",
                    "label": 0
                },
                {
                    "sent": "That's by the way is why you don't see phase transitions for least squares.",
                    "label": 0
                },
                {
                    "sent": "But if you have a nonsmooth regularizer, this analysis essentially goes through without any assumptions, and you got a very clear prediction of how many measurements you need to reconstruct the unknown vector, assuming that the measurements are in fact isotropic.",
                    "label": 0
                },
                {
                    "sent": "How is it changing when you apply wait to tell one.",
                    "label": 0
                },
                {
                    "sent": "We haven't made the calculation so it's fairly straightforward, but it depends in a sort of detailed why on the choice of weights.",
                    "label": 0
                },
                {
                    "sent": "So you could work it out for a specific set of weights, but I don't know if there's going to be any kind of simple formula.",
                    "label": 0
                },
                {
                    "sent": "In fact, the curves here.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are already there analytic functions?",
                    "label": 0
                },
                {
                    "sent": "But they involve error functions and you know implicit equations and things solution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you do reweighting, it's sort of clearly.",
                    "label": 0
                },
                {
                    "sent": "You can make the descent cone smaller and as a result you need fewer measurements.",
                    "label": 0
                },
                {
                    "sent": "So heuristically it's clear what you need to accomplish with the RE weighting.",
                    "label": 0
                },
                {
                    "sent": "You need to make descent cone smaller.",
                    "label": 0
                },
                {
                    "sent": "If you do that then.",
                    "label": 0
                },
                {
                    "sent": "You'll do better.",
                    "label": 0
                },
                {
                    "sent": "OK, here's another example just to convince you that this isn't a one off.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the recovery of a low rank square matrix from random measurements.",
                    "label": 0
                },
                {
                    "sent": "By minimizing the Schatten one norm.",
                    "label": 0
                },
                {
                    "sent": "So on this axis we have the rank of the target matrix on this vertical axis we have the number of random measurements we've drawn, and the red curve.",
                    "label": 0
                },
                {
                    "sent": "Here is the 50% success Isocline.",
                    "label": 0
                },
                {
                    "sent": "The yellow curve is the theoretical prediction, so you can see that they coincide exactly.",
                    "label": 0
                },
                {
                    "sent": "And by the way, you can see that the phase transition is actually a lot narrower here, for the reason that it's a much bigger problem.",
                    "label": 0
                },
                {
                    "sent": "So remember that the phase transition has with square root of the ambient dimension and so it's getting narrower narrower as the problems are getting bigger.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can understand regularised linear inverse problems using this statistical dimension analysis.",
                    "label": 0
                },
                {
                    "sent": "And as I showed you before, we can compute these in all sorts.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples, so here's another example demixing.",
                    "label": 0
                },
                {
                    "sent": "So this is the problem of separating two superimposed signals from each other.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this method.",
                    "label": 0
                },
                {
                    "sent": "Provides a clean analysis of this problem as well, so let X not and why not be too structured?",
                    "label": 0
                },
                {
                    "sent": "Unknown vectors, so maybe this is sparse and one basis, and this is also sparse and we're going to let you be a known orthogonal matrix, so it's going to model the relative orientation of these two signals.",
                    "label": 1
                },
                {
                    "sent": "In practice, this is often just the identity.",
                    "label": 0
                },
                {
                    "sent": "Or it's.",
                    "label": 0
                },
                {
                    "sent": "Somehow implicit in the choice of structure, so we're going to assume that we observe a superposition of X natural and a rotated copy of Y natural.",
                    "label": 0
                },
                {
                    "sent": "OK, so we see a superposition of these two signals, one of which has been.",
                    "label": 0
                },
                {
                    "sent": "Reoriented and our goal is to pull them apart.",
                    "label": 0
                },
                {
                    "sent": "Given the observation, OK, there's a natural convex of program that can achieve this.",
                    "label": 0
                },
                {
                    "sent": "So the decision variables are X&Y, and we're going to insist that they are consistent with the observed data, so X plus you I had better reproduce what we saw.",
                    "label": 0
                },
                {
                    "sent": "We're going to insist that why be less complicated than Y natural.",
                    "label": 0
                },
                {
                    "sent": "The second of the target vectors, and subject to that constraint, we're going to minimize the complexity of the first vector.",
                    "label": 0
                },
                {
                    "sent": "And our hope is that the solution to this problem, X hat Y hat coincides with the ground truth exponential Y natural OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's the geometry here?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's a lot more complicated, but suffice it to say that we have two descent cones.",
                    "label": 0
                },
                {
                    "sent": "The descent cone of the regularizer for X, and the descent cone of the regularizer for Y. Rotated.",
                    "label": 0
                },
                {
                    "sent": "By the Matrix you so we have two cones, one of which is rotated, and we succeed in identifying the ground truth.",
                    "label": 0
                },
                {
                    "sent": "When these two cones here do not share array, that's the situation where there is no simultaneous descent direction for both F&G.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if these two cones do share array, there is a simultaneous descent direction for both of the regularizers and.",
                    "label": 0
                },
                {
                    "sent": "We fail to reconstruct the ground truth, so in the case where we model random incoherence, so we think about the two structures being oblique with each other, which is the usual setting and signal processing problems.",
                    "label": 0
                },
                {
                    "sent": "It corresponds to rotating this red cone.",
                    "label": 0
                },
                {
                    "sent": "At random.",
                    "label": 0
                },
                {
                    "sent": "And so then the question is when does this red cone?",
                    "label": 0
                },
                {
                    "sent": "When it's randomly oriented, strike this blue cone.",
                    "label": 0
                },
                {
                    "sent": "Well, we already know the answer to that question.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The kinematic formula tells us and it says that if the total dimension of the two descent codes is small, we can D mix the signals.",
                    "label": 0
                },
                {
                    "sent": "So if the two signals aren't very complicated, we can pull them apart, provided that there don't have orientation in common.",
                    "label": 0
                },
                {
                    "sent": "And if the complexity of the two signals is large, we can't pull them apart.",
                    "label": 0
                },
                {
                    "sent": "Makes good intuitive sense, and we have a detailed numerical prediction of when we can do this, and we can't so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "We've got a vector that's Parson one basis and sparse in a random basis plus another vector.",
                    "label": 0
                },
                {
                    "sent": "The sparse in random basis, and we're going to try and.",
                    "label": 0
                },
                {
                    "sent": "Do you mix them by solving a problem using 2L1 norms one to promote sparsity in the first vector, the other to promote sparsity in the second vector.",
                    "label": 0
                },
                {
                    "sent": "So in this diagram Weaver aid the number of nonzeros in X natural in this axis and the number of nonzeros and why natural in the vertical axis.",
                    "label": 0
                },
                {
                    "sent": "So we see that when the two vectors are both very sparse, we can pull them apart without much trouble, and when their dense.",
                    "label": 0
                },
                {
                    "sent": "Impossible.",
                    "label": 0
                },
                {
                    "sent": "Once again, the red curve is a 50% success isocline the yellow curves of theoretical prediction.",
                    "label": 0
                },
                {
                    "sent": "So this tells us exactly when we can separate these two types of.",
                    "label": 0
                },
                {
                    "sent": "Structured vectors.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example for rank sparsity decomposition, sometimes known as robust PCA.",
                    "label": 0
                },
                {
                    "sent": "In this case we have a low rank matrix for which we use the shotgun one norm regularizer and we have a sparse matrix for which we use the L1 norm regularizer and when we frame this demixing problem, we see that when the rank is small and the sparsity is small, everything works fine.",
                    "label": 0
                },
                {
                    "sent": "And when the rank is high and the sparsity is high.",
                    "label": 0
                },
                {
                    "sent": "The demixing problem fails once again.",
                    "label": 0
                },
                {
                    "sent": "The theoretical prediction coincides perfectly with the empirical 50% isocline.",
                    "label": 0
                },
                {
                    "sent": "So any questions on Demixing?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One more example, sokon programs with random constraints.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm just going to state the result here.",
                    "label": 0
                },
                {
                    "sent": "The geometry is actually a little bit more complicated and you have to do a bit more work to get this result, but assume that we have a cone program, so the decision variable logs to a proper cone.",
                    "label": 0
                },
                {
                    "sent": "And we have M random affine constraints, so that means that the Matrix A is got standard normal entries, the right hand side B has standard normal entries and our aim is to minimize a random linear form subject to these constraints.",
                    "label": 1
                },
                {
                    "sent": "So it turns out that cone programs exhibit a phase transition depending on how many affine constraints you enforce.",
                    "label": 0
                },
                {
                    "sent": "So when the number of affine constraints is smaller than the statistical dimension of the cone.",
                    "label": 0
                },
                {
                    "sent": "The cone program is feasible and unbounded with high probability.",
                    "label": 1
                },
                {
                    "sent": "So if there aren't too many constraints, there's a feasible point.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if the number of constraints is larger than the dimension of the cone, the cone program is infeasible with high probability.",
                    "label": 1
                },
                {
                    "sent": "There's no point that satisfies the.",
                    "label": 0
                },
                {
                    "sent": "Um, equality the affine equality constraints in the common constraint.",
                    "label": 0
                },
                {
                    "sent": "So this is a phase transition.",
                    "label": 0
                },
                {
                    "sent": "I don't think that's been observed before in the behavior of any cone program K as a proper code.",
                    "label": 0
                },
                {
                    "sent": "So once again, the statistical dimension the size of this cone which we were able to compute, governs of performance or governs the behavior of this type of random optimization problem.",
                    "label": 0
                },
                {
                    "sent": "And just to indicate that this analysis really is.",
                    "label": 0
                },
                {
                    "sent": "Correct?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did some examples with various numbers of cones.",
                    "label": 0
                },
                {
                    "sent": "These heavy dashed lines here are the empirical 50% marks for these logistic regressions and the heavy dashed lines that are right next to them are the theoretical estimates for.",
                    "label": 0
                },
                {
                    "sent": "The number of constraints you're going to need to see before you move from feasibility and feasibility.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Is a little bit fancier because this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are all fine now.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So there's a one more degree or one fewer degrees of freedom I guess.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In 40 minutes is a little bit hard to do.",
                    "label": 0
                },
                {
                    "sent": "Full justice to this entire theory, but I hope I've persuaded you that this analysis really provides for the first time a consistent set of tools for analyzing the performance of a wide class of random optimization problems that we see all the time in statistics, machine learning, and lots of other areas of modern inquiry.",
                    "label": 0
                },
                {
                    "sent": "So most of the work I've described today is in a paper joint with Dennis Amalong, some Martin lots.",
                    "label": 0
                },
                {
                    "sent": "Michael McCoy called living on the edge the edge referring to this.",
                    "label": 1
                },
                {
                    "sent": "Ledger, the phase transition, and there's also work from my colleagues.",
                    "label": 0
                },
                {
                    "sent": "Some annoy mackenbach asebe that's available in archive that describes the role of the statistical dimension and.",
                    "label": 0
                },
                {
                    "sent": "Denoising problems and there are more papers on this on the way, so I appreciate your attention and thank you.",
                    "label": 0
                }
            ]
        }
    }
}