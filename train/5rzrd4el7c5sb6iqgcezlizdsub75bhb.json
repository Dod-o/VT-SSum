{
    "id": "5rzrd4el7c5sb6iqgcezlizdsub75bhb",
    "title": "Probabilistic Graphical Models and Structured Prediction",
    "info": {
        "author": [
            "Ben Taskar, University of Pennsylvania"
        ],
        "published": "Dec. 14, 2007",
        "recorded": "October 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/aop07_taskar_pgm/",
    "segmentation": [
        [
            "Hi, my name is Ben Ben Tasker.",
            "I'm from U Penn and I'll talk about structured prediction actually now as opposed to later in the afternoon we switched the order for various reasons so.",
            "Hopefully if you have any questions about graphical models, graphical models in particular will be answered late in the afternoon, so this is kind of a teaser about some of those issues, but I think you should be able to follow most of the stuff doesn't depend on graphical models.",
            "Please ask questions early so I know where to slow down when to speed up, so I want questions.",
            "OK, so I'll talk about structured prediction and one particular view of it using large margin kind of approaches like SVM that we've seen before."
        ],
        [
            "And start with sharing with you guys cartoon.",
            "I ran across a while back and this is.",
            "New Yorker cartoon.",
            "So don't worry Howard.",
            "The big questions are multiple choice and I would argue that the machine learning especially supervised machine learning.",
            "Community is spent way too much time worrying about binary prediction and not thinking about the big questions or the where the big questions are ones that have a lot of answers.",
            "Too many answers to enumerate."
        ],
        [
            "And this is exactly where sort of structured prediction comes in.",
            "The kind of questions that we want to address.",
            "Our prediction problems, where the output is not a single bit, yes or no, but a sequence, for example sequence of outputs like here.",
            "So instead of predicting each letter I'm predicting into."
        ],
        [
            "A word.",
            "Or the output might have spatial structure.",
            "So the problem here is taking this 3D point cloud and classifying.",
            "Say basically mapping out the buildings which are in red.",
            "The trees are in green, the Bush is in blue, there's ground sitter etc.",
            "Mapping out the objects in the scene."
        ],
        [
            "The outputs might have.",
            "Recursive structure as we find find in natural language parsing.",
            "So the input is a sentence and the output is.",
            "Context free grammar type of parse of the sentence.",
            "Right, this doesn't look like."
        ],
        [
            "Single bit some of the more exotic prediction problems that I've been working on.",
            "The output might be an actual combinatorial object like a matching, so this is a problem in natural language machine translation, so part of the pipeline of building a machine translation system is doing word alignment, taking pairs of sentences that are supposedly translations of each other, and establishing correspondences.",
            "Roughly speaking word to word.",
            "So I'm going to make a simplifying assumption that.",
            "You know one word is going to map to at most one word here and for English French it's not such a horrible assumption about 9095% of words can be kind of map 1 to one as well.",
            "There's exceptions, but let's just think of it this way.",
            "So what I'm looking for is, given this pair of sentences, give me a matching partial matching of each word for each word.",
            "OK, so again, complex input, complex output."
        ],
        [
            "Another problem that's somewhat similar, but a different kind of control structure comes up in computational biology.",
            "The problem of protein prediction in particular is, you know taking a string of I, me, know acids an from the string predicting the three dimensional structure, extremely hard problem and I'm not going to take it today.",
            "But a subset of the problem is determining what are called disulfide bonds.",
            "These bonds are basically the strongest bonds there are in this in this structure, and they hold together a fold this this this protein into into shape.",
            "And what happens with these bonds?",
            "They form between cysteine molecules.",
            "So we have our string.",
            "We have the locations of the Seas and we know that these seas are going to find a pair and match up.",
            "We just don't know which one.",
            "So the prediction problem is to figure out which C is going to match to which other see.",
            "Again, it's a matching.",
            "It's not bipartite matching like in Word alignment, but it's a non bipartite matching matuska material structure.",
            "Turns out it's a harder problem than the bipartite matching, but but we can still sort of use the same kinds of."
        ],
        [
            "Methods.",
            "So a lot of approaches to these kinds of structure problems basically take the tools we know things.",
            "the Hammers we have in our hands and apply them essentially by breaking down, divide and conquer approach you take your problem, we break it up into pieces.",
            "We predict each piece individually, right?",
            "So we know how to build a classifier for each letter.",
            "Same thing.",
            "In this case we might take a sliding window that.",
            "Classifies each pixel by looking at surrounding independently.",
            "The problem with this approach is that it doesn't use the correlations in the data, and the fact that certain letters are follow certain other letters much more commonly, or that, for example, a pixel that's next to a tree pixel is going to be more likely to be next to a tree pixel.",
            "More generally, we might have constraints if we're trying to predict, say, a parse tree from a sentence right there, we cannot predict each.",
            "Each part of the parse tree independently, because they have to fit together into recursive structure.",
            "If we're predicting matchings again, we cannot predict each edge independently.",
            "Their constraints that make sure that it's a matching right, so this kind of local prediction is not going to solve all their problems."
        ],
        [
            "But let's see what it does on this somewhat controlled problem.",
            "So this is again problem of segmentation of 3D point cloud data, and So what we've done here is basically taken an SVM, trained it on local windows to try to predict what there's a tree at, red or green.",
            "Sorry, building red treat in green and blue is Bush is the ground is Gray.",
            "So as you can see it does pretty well.",
            "I mean that Chris here is about 70% or something like that, but there's tons of these kind of inconsistency issues actually.",
            "Actually, I haven't highlighted so there's trees on top of Windows.",
            "There's trees on top of roofs.",
            "There is building setup of trees etc etc and this is a problem that you cannot really solve with a bigger window or not right because of.",
            "All kinds of sampling issue.",
            "Density of sampling diversity and just diversity of scale.",
            "There's no right window for everything, and you can.",
            "Also.",
            "We cannot also solve it as well sure later by just say voting or something like that.",
            "Taking a local neighborhood and and taking a majority or something like that.",
            "Local approaches do really have these inconsistent kinds of predict."
        ],
        [
            "So instead, we're going to advocate is structured prediction is models that take the whole input and try to predict the entire output.",
            "That sort of is self consistent that uses the local information like the local windows and local signal, but also exploit correlations and respect constraints."
        ],
        [
            "And so, for example, in this problem, if we use one of the models and talk about later.",
            "This is what it does there, and this is sort of comparison right?",
            "So it really what it does is stress to enforce the the fact that nearby pixels are correlated, so so it really smooth, smooth out this area in this area and that area.",
            "I mean it's not perfect, but.",
            "In the setting we don't have to make these decisions individually.",
            "We can essentially borrow strength of virus signal bar information from surrounding area from things you're interacting with to do prediction better.",
            "OK, so that's the straw."
        ],
        [
            "Prediction setting and then there are many different instantiations of that.",
            "So we can think about sequences and so people have looked at what are called CRF models that essentially are kind of like Hmm's, but there's sort of a discriminative version of that there.",
            "If you think about naive Bayes versus logistic regression, the Hmm's interior sort of the same, the same way.",
            "So starting with that model and I'll explain a little bit now and then, I'll explain it more later in the afternoon.",
            "You can think about doing structured prediction on sequences.",
            "You can do structured prediction on trees.",
            "These guys, which I'll talk about soon.",
            "There are special kind of wraps, matchings, etc.",
            "So that's the problem.",
            "Mapping from complex input to complex output an trying to exploit everything we can about correlations, constraints, everything that clear.",
            "Good.",
            "So this is the rough outline I'm going to talk about, so I'm going to talk about how we can formulate this as a large margin problem, and how sort of the structure can be exploited and respected.",
            "And there's two different formulations for this that we need to actually solve.",
            "Some of these problems.",
            "Yeah, so that's the rough outline."
        ],
        [
            "And let me.",
            "Try to abstract from all these different models so we have trees.",
            "We have these hmm type thing sequences.",
            "We have matchings all of these things look kind of.",
            "Diverse and hard to sort of handle in one unified way, so this is an attempt to do that is to say OK, our structured model definition is just this kind of.",
            "Well, in some sense, right now it's a black box.",
            "Basically it takes an input X and output Y.",
            "This thing is, you know some complex object.",
            "This thing is also complex.",
            "Object finds the best one according to the scoring function.",
            "OK, that's our definition of a structured model, right?",
            "And there's a space of feasible outputs, and it might depend on X, which is the space of possible matchings or trees.",
            "Note that this space is generally very large.",
            "It's exponential in the size of of.",
            "Input the number of save points in your point cloud right in that point cloud, there's about actually 15 million points or so, so.",
            "It's basically you say 4 to that to that power for the number of outputs in general, so we cannot enumerate that.",
            "Enumerate that set efficiently, right?",
            "So we're going to be looking at, you know, going to specialize the definition to models where we can do this.",
            "Argmax in somewhat efficient way.",
            "Otherwise, we're sort of stuck at being too abstract, so.",
            "We can assume that the score is our favorite linear combination of features, right?",
            "It's very much like you doing this VMS, but now the features are not only dependent on X.",
            "This feature mapping function, but it also depends on the output.",
            "So we get together are.",
            "The sequence of images with a potential labeling an we create features on that and I'll show exactly what they look like, right?",
            "Same thing for matchings and trees, right?",
            "There's a joint feature map that takes input and output and produces a big vector of.",
            "You can think of them as sufficient statistics of X&Y.",
            "You can think of him as basically some kind of counts of what's happening with X&Y, just the vector, and EE is the thing we're trying to learn.",
            "The parameter vector that basically prefers or just prefers certain structures and forms the score.",
            "OK so question.",
            "Is.",
            "Based on this very specific model of a great like, well, it's not very restrictive, I mean.",
            "I'm contemplating considers only Saturday.",
            "So I'm saying this is a. I'm claiming this is a mild assumption, meaning that if you.",
            "I mean, if you would want to score to be something else, I can.",
            "Usually you know, basically put that function into the feature mapping and then make it linear, right?",
            "It's the same thing as and I can use kernels here too later.",
            "Nothing is given to you in the party to learn is coming to value.",
            "Yeah, I mean, it's a standard supervised, yes?",
            "And once you fix your feature space, this is a limited set of functions, yeah?",
            "So am I sure?",
            "Which is based in everything except yes yes yes.",
            "I mean in the nonparametric setting when we make these things, we can kernelized these guys.",
            "Then you can kind of grow with these data set.",
            "Right, so it potentially could be on any arbitrary function.",
            "Not quite arbitrary, but a function that's.",
            "Well, I mean, we're going to assume in later on that this thing decomposes nicely, right?",
            "We need to be able to.",
            "This still doesn't give us the ability to find argmax.",
            "What's going to that abilities?",
            "This function further decomposes into parts, and then we can maximize over that.",
            "I mean, these these parts might be in grammar.",
            "Little productions in a sequence, pairs of input, output, and output, output, consecutive outputs, etc etc.",
            "So, so we're going to restrict further the function to be a sum of parts.",
            "That's a much stronger restriction, but the dependent.",
            "So the dependence on X could be potentially arbitrary, right?",
            "We can make a nonparametric."
        ],
        [
            "OK, so let me map this chain Markov net CRF into this sort of WRF thing.",
            "Which is, you know it's not done.",
            "It's not really usually written that way.",
            "I love.",
            "Uh, it's pretty easy to see so.",
            "Yeah, so let me just start with this.",
            "OK so we have our X right?",
            "It's a sequence of images.",
            "We have our Y which is a sequence of labels.",
            "26 possibilities.",
            "And then this conditional random fields are probabilistic models that define distribution by take by sort of decomposing it into parts and then.",
            "Through of multiplying those parts together.",
            "More on that later today so the parts are these node potentials, which is potentials between image and the label and edge potentials which are between consecutive labels and also include the input as well.",
            "If you'd like to.",
            "OK, so if you are more familiar with HMMS right?",
            "This would kind of correspond to a.",
            "Emission probabilities and this would be the transition probabilities, right?",
            "So the difference here is that these guys don't need to be probabilities, they don't need to sum to one, they just need to be positive so.",
            "The the only restriction again there there are non negative and we're going to further say that they're positive right there, just represented through this E to the again linear combination of things.",
            "Attention.",
            "Sequences what is the complete relationship between elements in which I don't know all the things sequence?",
            "Yes.",
            "Yeah, so this is more of an example and so the for example we do the segmentation of point Cloud.",
            "It's a 3D grid.",
            "Or I mean it's not a regular grid, but it's roughly speaking 3 degrees.",
            "So yeah, so this is just an illustration of what it looks like in.",
            "In Metro language and by informatics condition biology, alot of the problems are changed.",
            "So this special case is a very important case and write this.",
            "All of this extends to other things so again so our potential is log linear kind of representation right?",
            "X to the WF OK and then it depends on XJ and YJ so we're going to basically the index of the WS.",
            "The correspondence subvector of parameters that correspond to the node parameters.",
            "And the same thing here actually.",
            "So I wanted to mention what are the features here, so the features might be something like this pixel in 3rd row and 4th column is on and the letter label is Z OK, right?",
            "So that's a feature that's basically saying I mean and so if that Pixel likes to be on when letter Z, then you probably will have positive wait.",
            "If it doesn't then it will probably be a negative weight because positive negative weights or this feature will affect.",
            "The potential and so the larger the potential is, the more likely it is to be that label.",
            "Right, so right?",
            "And again, yeah, so for edges you might have features like what's the previous letter and what's the next.",
            "OK, so here it doesn't depend on X, but in general we can.",
            "OK, yes, 2 letters.",
            "Obviously there are some combinations that don't happen, but I mean most combinations.",
            "It doesn't sound like you're getting a lot of information from the like.",
            "I mean, it was three letters, lot more information than just too.",
            "But I mean sure.",
            "So again, this is example only so we can deal with non pairwise potentials and so I could have a pairwise micro network.",
            "But I could have XYJKL and then I have larger features and everything would just go through so.",
            "Yeah, exactly so if you have, say a second order model then you know you just blow up your state space by instead of being 2626 will be 26 ^3.",
            "But yeah, but it's all still doable as long as those connections are short range, right?",
            "As long as you just depend on the previous couple of things.",
            "So."
        ],
        [
            "Um?",
            "OK, this is a little bit of notation manipulation which but I just wanted to get what I want to get.",
            "Is this right?",
            "I want to show that these guys look like this E to the scoring function that I want.",
            "OK so this is all just about trying to.",
            "Where geographical model into this sort of form?",
            "OK, so how do I do that?",
            "I mean they just say OK, so let me look at the product of the potentials.",
            "That's just X of the sum of those guys, because individual ones were just easier that OK, which is X2 WFN of XY, where I've defined a new thing FN of XY.",
            "Is this some of these things?",
            "OK, so this is what I mean by sufficient statistics.",
            "So we had an individual feature that looked at particular particular position.",
            "And letter and said OK, this on, and this is the OK. Now what this guy does says OK, some over the sequence.",
            "How many times do I see this position on an letter Z?",
            "So that's a sufficient statistic?",
            "OK, and that's what's going to wait that wait, and the same thing here, right?",
            "I have you know this feature and then now I just.",
            "Compute how many times is a followed by Z, so that's again sufficient statistic.",
            "Alright man so.",
            "Now my this whole product is just this.",
            "This whole product is just that.",
            "So if I just going to stack up F this way and that way I can write it this way.",
            "So all I'm saying is that the probability of Y given X. Alright, is proportional to Y to this WF of FXY where FXY is this feature function feature mapping function.",
            "It takes a full labeling.",
            "It takes a full input and gives me some vector statistics.",
            "That's all it is and this is what this thing is.",
            "Hopefully that's clear and it's sometimes it's sort of hard to you know.",
            "Visualize it and kind of get comfortable with it, but it's.",
            "It's a, it's a lot easier once you get to use this representation.",
            "Is that clear, so just one.",
            "I want to be labored if everybody's.",
            "OK good.",
            "I think people are."
        ],
        [
            "People are happy.",
            "So the same thing happens in so it doesn't have to be a chain, right?",
            "So if I have just a general product of node potential, edge potentials is not a chain, you know, I just have, so we're going to talk about specific kinds of Markov networks that essentially try to make.",
            "Nodes that are connected by an edge so usually say for example we have a grid 3D grid that we're trying to label things that are nearby tend to be the same label.",
            "So what happens in the potential is that everything is 1, meaning that doesn't do anything, and then the diagonal entries are greater than one.",
            "So basically the model right?",
            "If this is greater than one, it prefers the labels to be.",
            "The same right so?",
            "A lot of assignments that have labels nearby labels the same.",
            "Are more likely OK.",
            "Turns out this is a special model because it allows the argmax to be done efficiently.",
            "So just examples of these features would be in this.",
            "In this case this would be.",
            "Instead of from computer vision, there are some types of descriptors are used for classifying essentially 3D texture, right?",
            "We can look at, say, a normal to the surface and essentially a histogram of the densities around this with respect to this normal.",
            "So this is one of kind of descriptors that's used to determine what kind of texture I see at particular point.",
            "So for example, you know walls and columns are pretty flat, while trees are much more dispersed than Bush is are having different.",
            "Texture etc etc and then edge features might be, you know, kind of how far to things are away from each other, how?",
            "What are the normals of the two of the two surface points etc etc.",
            "OK, so there is some arbitrary feature function."
        ],
        [
            "Feature mapping.",
            "Diagnosed"
        ],
        [
            "So this is a restriction.",
            "This is a restriction we going to make and the model in order to make it tractable.",
            "So this is this is this is just an example that I'm going to use later, But it turns out this is a special kind of model that no matter so.",
            "Well, do you wanna ask your question?",
            "Yes.",
            "So usually when you don't have chains or trees, right?",
            "The question of inference.",
            "Finding the argmax is hard.",
            "In case of this kind of model, especially for K = 2.",
            "No matter what the topology is, the argmax is tractable OK, and then if K is greater than two, you have an approximation guarantee.",
            "I'll come back to that, but this is one kind of model that's useful in a lot of computer vision applications."
        ],
        [
            "OK, let me also try to put this kind of a parsing grammar type.",
            "Model into the same framework.",
            "So usually if I want to have distribution over parse trees given input sentence, I do it in terms of productions.",
            "This is a production that takes a non terminal and produces Alpha is either say a non terminal and non terminal or terminal or whatever.",
            "It's some sequence of other things.",
            "OK so these are these productions of my grammar.",
            "And so when I want to, when I want to calculate the probability of this guy, what I do is I take all the productions that are in this X&Y.",
            "So the productions here would be S goes to N PvP exclusive I. Map them out.",
            "Yeah, so production here is S goes to in PvP.",
            "Here DT goes to the etc etc.",
            "And so this feature mapping just takes this.",
            "Sequence in a tree and Maps it into the statistics.",
            "How many times did I see in Pico de TNN?",
            "ETC etc.",
            "So this is noun phrase, determiner, verb, etc.",
            "OK, so again the probability is just going to be proportional to.",
            "If you know if we have features that are like this, features that depend on the non terminal.",
            "And the production, what it produces.",
            "It's the scoring."
        ],
        [
            "OK, I think this is the last thing.",
            "So for those those things I just described before are commonly.",
            "No specified as probabilistic model, so we kind of started with conditional probability distribution and said oh look it's can be written.",
            "This log linear form.",
            "OK so forward.",
            "I mean for word alignment for matchings you don't actually start with probabilistic model, and there's a good reason for that and I'll get back to that.",
            "But basically here we just assume that the score of a matching is just something that basically looks at each edge and see that is present in the matching and says what's the score and then just.",
            "Heads up OK, so for each YJK.",
            "JK, that's in the matching I calculated score so a little bit of abuse notation here.",
            "F of X JK and then I. I'm going to note that by that OK and so the features here, just to be concrete, might the so the relative position of these two words.",
            "So if you talk about languages are pretty close in the word order, things tend to be don't move too far orthography.",
            "If these are again languages are closed, then say a string at a distance between these two things might give you some clue Association, which is, you know you might be able to have a bunch of unlabeled data where you can kind of say what's the mutual information between occurrence of this word in this word.",
            "OK, so that might be a feature that you got from unlabeled data.",
            "So there's a whole bunch of features in your way.",
            "You're learning, trying to learn the weights that give you the right."
        ],
        [
            "Payments.",
            "OK, and then finally I guess this is the number part that matching model you have your season.",
            "Then the scores might depend.",
            "Essentially.",
            "I mean, what we've done is looked at a window around a window.",
            "Amino acids around there and then the window analysis around there and we're learning a function which is you can think of an approximate energy function for matching this."
        ],
        [
            "Yes, this is some of the.",
            "Features that we use, right?",
            "So we have for matching system four and six.",
            "This is the kind of the appropriate part of the input that we're considering, and then we have some sort of feature mapping on this input, which looks at amino acid entities it looks at.",
            "Basically, physical chemical properties of those things, whether they like water, where they don't like water etc etc.",
            "OK, the point is, in all of these models the score decomposes into parts whether there are edges productions, pairs of consecutive labels, etc."
        ],
        [
            "Alright, so this is.",
            "This further assumption is that their score is not only linear, but it's also decomposed into parts.",
            "OK, yes."
        ],
        [
            "Just to get back here, you thinking, hearing decomposing sort of corresponding.",
            "Things in the sequence being matched mean is not the way to school.",
            "Yeah so so OK, so let me just I went quickly over this one, but I'm considering OK so there's six cystines.",
            "OK, so there's going to be 3 edges if I'm looking at perfect matchings.",
            "A score of each matching is going to be just double well.",
            "This right?",
            "So I'm going to sum over those three, take my W, and multiply that.",
            "So how am I going to compute whether for like 6?",
            "It depends on the properties of the string around there OK and so.",
            "You know we.",
            "It's actually it is decomposed.",
            "I mean, it's we're using something there that's that's based on identity letters, but it's not.",
            "I mean, it's not just it's too sparse to just take the entire thing so it basically looks at the identical letters question.",
            "This page.",
            "There's lots of constraints from the topology of the change, so you assume the first one.",
            "You can get the second one first.",
            "Things that help their system probably don't make yes exactly exactly good good.",
            "Right good, so I mean there is a question of yeah, so whether you actually try to satisfy all the constraints, right?",
            "So if you're trying to solve the entire 3D3D protein folding problem, there's tons of physical constraints about these things that are very hard to do right.",
            "And usually the problem of you know, given even if somebody gives you an energy function, you're not trying to learn it.",
            "Finding the optimal thing, the argmax is something that takes on the order of, you know, months CPU time, right?",
            "So?",
            "Sorry yeah, so.",
            "So what we're looking for?",
            "I mean, what is this useful for?",
            "I mean, right now it's kind of useful as an illustration of what kind of structures are there.",
            "But this, as once we learn such a model, what it can be useful for for fast throughput kind of filtering of proteins, right?",
            "So what we try to do actually with this thing is basically try to predict which systems form proteins 'cause.",
            "This is very fast and then use those constraints to say.",
            "Rosetta, which is one of the sort of Premier protein folding.",
            "Software out there use that those constraints to help it in the search.",
            "OK, so this thing is sort of a preprocessing step to trust.",
            "Identify these things we can handle and we can handle the fact that it might be not a perfect matching that some of these guys actually matching to other other things.",
            "This is a sub chain and smashing to another part.",
            "We can handle a non perfect matching as well.",
            "It's basically.",
            "Yeah, I'll talk about that.",
            "Yes.",
            "Additional function air conclude.",
            "Sooner is not PlayStation given.",
            "Yes, so this is kind of.",
            "Yes, straight parameter estimation right?",
            "You could OK.",
            "So yes and no.",
            "I mean, you could also say for example, if I'm trying to do this with Mark of random fields or something like that.",
            "I could introduce a lot of possible cliques and do selection using some kind of sparsifying prior, right?",
            "If I later, when I learned my WS with you, if I put a. L2 norm I don't get something sparse for put up like L1 type normal or sparsifying norm.",
            "I can get some kind of structure selection as well and we played around with that as well, but usually the structure is given.",
            "I mean it's hard enough with structure given so, but it's a good question.",
            "I mean really, you would really want to be doing is is doing those two things at once."
        ],
        [
            "OK, so that's the assumption for structure models and you know the.",
            "This is assumption about the score, and then there's also assumption about wise being.",
            "Inter constraint, right?",
            "It's not like every you can just take a bag of parts in.",
            "These parts are completely independent of each other.",
            "These parts yps have to somehow fit together well and you will see that happens with grammars that happens with sequences.",
            "Happens with everything."
        ],
        [
            "OK, so here's the picture.",
            "We have a model.",
            "It's kind of like that.",
            "I mean, you can think of it either as a score like this, or if you want to think about it as a probability distribution over the outputs, you just exponentiate the score.",
            "It's super cute super setting.",
            "We know all the apps we know all the we know what doubles we want to learn.",
            "We have labeled data X&Y's so this is a matching.",
            "We're going to estimate W during learning at prediction time.",
            "We're going to the argmax of this probability distribution, which is the same as the argmax of the score.",
            "I haven't changed anything.",
            "This is sort of.",
            "Probabilistic view of this thing, and so.",
            "An example of this would be I run weighted matching here.",
            "More generally, I do some kind of internal optimization or dynamic programming to compute the best parse or the best ytterbium HMM or I do.",
            "Turns out I do a minimum weight cut in the that.",
            "Call Damon associative potentials so we know how to do this part.",
            "It's some kind of computer chips is a optimization given RW.",
            "So how do we learn this?",
            "So one is local.",
            "Just basically you learn ignoring all structure and you might actually there's two variants of this.",
            "One is ignored structure during learning and then also ignore structure during inference.",
            "So I mean, we're assuming the first one is totally brain dead, the second one is a potential candidate where you.",
            "I'll show exactly how it works.",
            "It doesn't work that well, but it's a candidate.",
            "There's likelihood so I can have my data right?",
            "I can calculate what's the probability of Y given X, and they can maximize the log likelihood.",
            "The data for a lot of models like chains and parse trees.",
            "It's doable for a bunch of other problems like matchings, and these sort of grid grid type models.",
            "It's intractable, and then the third method is what we're advocating is basically a margin based method that's kind of generalizes the SVM hinge loss.",
            "To the setting and it.",
            "Your means tractable for all of the problems that I mentioned, even in places where likelihood is not.",
            "OK, so this is this is the pipeline.",
            "This is the box we're going to talk about.",
            "Where?",
            "Yeah, OK."
        ],
        [
            "OK, So what do we want?",
            "Given our label data, right so this is our label, this is our input.",
            "This is our label.",
            "We want the argmax to give us back the right answer.",
            "OK, so we want to find WS that satisfy this condition.",
            "It's kind of a strange condition, so it helps to actually try to rewrite it in some way.",
            "You know we'd like to sort of write something that's kind of, you know, there's some linear constraints and W or something nice and useful.",
            "So, for example, we could write down for every possibility of five letter words we can write those down and say the score of this guy.",
            "The truth is better than the other, and so we make this list.",
            "The list is going to be.",
            "You know 26 letters to the power of five more generally is going to be something very very large, right?",
            "But this is at least an attempt to write down what we want in some kind of declarative form.",
            "That's the easier to do."
        ],
        [
            "This.",
            "In parsing it's very similar, right?",
            "So if this is a sentence that we want this, you know we just enumerate all parse trees, and there's a lot of the Catalan number of those.",
            "In terms of the."
        ],
        [
            "The length of the string for alignment.",
            "Again, you know this is the truth.",
            "Say for this input and we enumerate all the rest."
        ],
        [
            "OK, so.",
            "One challenge with this, in addition to the fact that there are exponentially many alternatives we need list is that the loss that we actually usually sort of measure when we compute accuracy on our prediction is not simple.",
            "01 loss very often.",
            "So for example in character kind of recognition you care about character error, right?",
            "So how many positions you get wrong?",
            "OK, so here you got two wrong here.",
            "You got one wrong.",
            "Other cases as well, you might.",
            "You know this is correct.",
            "Then you got one wrong here.",
            "Here it's two wrong because you also you mislabeled this guy and also you got the structure wrong, right?",
            "If you're counting productions, this is actually three wrong.",
            "'cause you got the structure wrong and you mislabeled the things right, so different different candidates are scored differently with respect to error, right?",
            "So for example, for parsing you know, usually people care, but it's kind of bracket recall kind of measure.",
            "So how many?",
            "Brackets as you get right, roughly speaking, precision recall on that right for matchings.",
            "Again, it's something that like decomposes into into.",
            "Into into individual parts, right?",
            "So you got here, you got 1 missing.",
            "That's penalty of 1 if you got 2 missing, it's that, etc etc.",
            "So we have a structured loss in addition to.",
            "Having experienced many things and for for us and actually for mathematical convenience, we're going to assume that the loss decomposes in the same way that the model does, and that's key for making it efficient, so.",
            "You know, basically it's a Hamming kind of loss, right number of parts that are raw."
        ],
        [
            "OK, so let's write down some cubes.",
            "Here.",
            "We have training examples X&Y and this is what we want for every alternative, we want to maximize this margin gamma, right?",
            "Because this is strictly greater and.",
            "What we've we've done here is make the margin be weighted by the number of mistakes.",
            "So the intuition here is that you know the more wrong a particular output is, the bigger the margin should be, right?",
            "So that you want to reject that, reject that possibility with sort of much higher confidence there.",
            "Other sort of justifications for a more, more more reasonable justification for that is that.",
            "Once you transform everything, it turns out that this is going to be an upper bound, like a convex upper bound on the structured loss, which is the number of things wrong.",
            "OK, so it's in the same way as hinge loss upper bounds here.",
            "01 error.",
            "This kind of.",
            "Mistake weighted margin upper bounds your."
        ],
        [
            "Hamming distance loss.",
            "So this is a problem, right?",
            "So maximizing gamma, we need to constrain W some number of W so that this thing is coherent.",
            "Otherwise you know you can just scale up everything and have a better gamma.",
            "So we make this right and the usual kind of SVM thing.",
            "We eliminate gamma and translate that into a minimizing of the norm of W ^2.",
            "Right by standard substitution, so it looks very much like an SVM right now, as VM without Slack variables, except these constraints.",
            "Now you have for each example an for each alternate.",
            "For that example, you have some kind of constraint.",
            "Essentially, the constraint is that the truth beats out the alternate.",
            "This runner up by.",
            "I ordered the margin in the case where we cannot do this we need to add slacks the same way as we do it in this VMS we had a slack for each example.",
            "OK, so say I and they're essentially allow us to violate this constraint at a penalty FC.",
            "OK."
        ],
        [
            "So OK, so now we have this problem.",
            "At least we've sort of wrote down exactly what we wanted.",
            "And we could potentially just do brute force enumeration of all these constraints and try to solve it.",
            "So this doesn't really scale, of course, and what we tried we tried to do instead is actually do this thing exactly.",
            "The same problem through what's called, so we call a min Max formulation that.",
            "Is going to be.",
            "Turns out and with some algebra is going to turn out to be polynomial, so polynomial in all the things you care about, the number of wise number of parts in the input.",
            "So we're going to go from this kind of exponential enumeration into something that's that much nicer, and we're going to do this is by doing the following trick.",
            "It's a very simple trick.",
            "Essentially, what we're saying here is that the truth beats out everybody else by this amount.",
            "Everybody else.",
            "For that example.",
            "OK, so it must beat the Max.",
            "OK, is that clear.",
            "Good man.",
            "Then we're going to just take this Max.",
            "So now we have something weird, right?",
            "We have a minimization over continuous space and then embedded in this thing is a discrete maximization over Y.",
            "Right, and then the second trick is to take this embedded Max an take go from a discrete optimization into a continuous one.",
            "If we can do that without loss, we can essentially just take that.",
            "Continuous optimization, plug it in and have a joint continuous optimization for both and that's exactly what we do.",
            "OK, is there questions about that?"
        ],
        [
            "OK. Alright, so OK good.",
            "The assumptions to make this work are this structured loss.",
            "Like I said, Hamming loss that it decomposes in the same way as the model, so I sum of her parts and I compare whether I got the part right.",
            "So this could be 01 right basically, but it doesn't have to be one.",
            "It could be something waited as well.",
            "I mean certain part making mistake in a certain part might be more costly than another part, so it's something that decomposes into parts.",
            "So it's kind of weighted Hamming distance, say OK, so that's T. And then our this guy just decomposes again into this thing into parts.",
            "This is the part of the score and this is the part of the.",
            "Loss.",
            "What we're going to do then is this mapping from discrete to continuous.",
            "So what we're going to do is basically have some variable Z that roughly correspond to Y. OK, there are continuous relaxations of Y, and then Q is going to be whatever all of this sort of coefficients that affect that.",
            "So I'm going to explain what Q is, but essentially we're going to write down this this this discrete optimization thing, which usually you would solve, either would say.",
            "Weighted matching organic program to do that.",
            "Irby or parsing thing, etc etc.",
            "We would write it down as a linear program.",
            "This is not to say this is we're going to prediction right.",
            "This is a tool to do the learning part and kind of get the reductions the way you do your prediction at runtime is still up to you, right?",
            "So this is just a tool to actually be able to map these problems into something solvable.",
            "And then once we can kind of see that these guys are equivalent, the value I mean, as long as they return the same value.",
            "This in this for what Q depends basically on WF&L.",
            "As long as they dismiss in value, we can just plug this thing in and go with that."
        ],
        [
            "OK, so now I have to.",
            "I'm going to sort of take a step back and explain how to do this argmax using linear programming for a few of these problems, OK?",
            "Let's see.",
            "And then we can come back and plug it in."
        ],
        [
            "See what happens.",
            "OK, so.",
            "The plan is to, you know, take our whatever we were going to do with the dynamic program and now write it down in a more actually declarative way than a dynamic program you know usually is thought of.",
            "OK, so we're going to do that is to take a Y, right?",
            "So in this case, let's take the thing with sequences, right?",
            "We take a Y and it's just, you know, five sequence, five letter sequence, and we're going to.",
            "Encode that with five variables.",
            "OK, well, five.",
            "Actually, this kind of vector variables, these variables.",
            "Your question, so these variables correspond to just kind of.",
            "I have a what I really want to be is a binary variable for each of these.",
            "Each of these possibilities OK, and then this corresponds to the first character.",
            "This correspond to the second character, etc etc.",
            "So I have this five by kind of 26 array.",
            "This very sparse.",
            "It only has like 55 ones, that encodes my Z.",
            "My why OK?",
            "The clear the other part that I need is.",
            "Now encoding Sir, actually.",
            "Pairs so consecutive pairs.",
            "Where am I going to go with this?",
            "I'm going to write down the objective function instead of just in terms of why I'm going to write it down in terms of these, because, you know, we're doing arguments over why I'm going to be doing now arguments over Z, so I want to express the objective in terms of these, so this helps me in code.",
            "Why in this way next step is, you know, I'm going to take care in this sequence model of pairwise terms.",
            "So what I'm going to do is just this in this in one matrix.",
            "So the pair is a followed by B, right?",
            "So there is going to be one position on in this matrix, so this is just you can think of it as an indicator variable that says first position is a in the second position is B. Alright, and then there's you know, 26 by 26 and so there's there's.",
            "Four of these write an.",
            "In these four, there's only four ones.",
            "Turn on.",
            "OK, so this is our discrete Y.",
            "This is how it Maps into this kind of the 01 variable Z. OK."
        ],
        [
            "Now we're ready to write down the optimization in terms of Z.",
            "So what we do is basically take whatever it was was.",
            "Whatever objective was hanging on particular YJ, we write it now in terms of CJ.",
            "OK, so GJ FM said, is a position J has label M and so we take the score of you know part of the potential that talks about M and we.",
            "That's the coefficient of Z maximization linear's.",
            "So these are maximizing.",
            "This is a constant for me.",
            "Same thing with the edge variables, right?",
            "So I have these edge variables that tell me Eminem.",
            "And then these are.",
            "This is a constant.",
            "With this, when I'm doing I'm definition OK, so now my objective.",
            "This objective is the same as the objective with the discrete Y.",
            "But now you know I need to actually give the Z some meaning, right?",
            "Otherwise, this maximization is completely arbitrary.",
            "So what I want.",
            "I mean these these have to be really 01.",
            "If I if I write him down with 01, I have an integer program which is not what I wanted.",
            "I wanted something continuous, so it turns out it's enough to do the following.",
            "It's enough to say that their positive to say that to each one sums to 1, meaning that only one positions are.",
            "Only one position is set in each consecutive slot OK.",
            "So exactly right?",
            "So basically, we're encoding this thing and then we need to ensure that the Jays and DJ case are consistent with each other, right?",
            "So that they they encode the same Y, right?",
            "Because there's a score on the JKS and there's a score in the JS and those need to be coordinated.",
            "Otherwise you can, just, you know.",
            "Maximize The edge score separately from the note score and then you don't have a consistent why you're talking about.",
            "So in order to synchronize with Jake, Jake, think and Wednesdays think it's enough to do the following.",
            "Turns out you just basically enforce agreement meaning, so I'm looking at particular JK and as a sum over one side I should get the sum here.",
            "Basically marginalized one way.",
            "And this is some of the other way I should also get that.",
            "Right, so if you think of this kind of as a pairwise representation, that pairwise representation has to agree with the unary representation.",
            "So these are linear constraints on Z.",
            "This is sort of linear inequality is right, so we haven't said.",
            "We haven't made these discrete, we just they're still continuous.",
            "But it turns out that you can.",
            "You can show that if our original network is a chain or a tree, the answer you get is always going to be integral.",
            "Right, so so going from.",
            "The picture of this is something like this.",
            "If you have, you know this is a.",
            "Suppose you have some integer points in the lattice.",
            "OK, and then I have, you know, a relaxation of that lattice.",
            "So let me see if I draw this.",
            "Right?",
            "We have a point here.",
            "So.",
            "OK, so the points are the integer things that I'm really interested in, right?",
            "So this isn't some 2 dimensional space and doesn't correspond to these exactly OK, and what I'm doing is I'm optimizing a linear function in terms of these right?",
            "And the question is when I optimize function right, I'm going to end up at some corner, could end up end up at a face as well, but.",
            "Let's just think about non degenerate case.",
            "When I'm at the corner OK, and so if all the corners are discrete, then I'm guaranteed that I can find an optimal solution as discrete.",
            "The case in the face is not a problem, why?",
            "Because?",
            "Well, it basically it means that either this or this one solution have the same score, so I can just pick one of them right?",
            "So you can show that this this.",
            "Polytope right, this kind of.",
            "Set of linear constraints in the space of these makes a structure such that all of its corners are discrete.",
            "So when we optimize a linear function over that space, we're always going to end up at a corner and get an interest solution.",
            "So this has to do so I'll come back to that in certain graphical models.",
            "Why that's easy?",
            "Why that happens, but you know?",
            "Yeah, so maybe I'll come back to why that happens.",
            "But this is kind of a common scenario.",
            "If you can.",
            "Essentially, if you can do something with the dynamic program, you can write it down as a linear in some sense is a linear as a linear program as well, and that linear program is going to give you integer solutions if the dynamic program gave you in their solutions, so this is not true if we have a more complicated network.",
            "If we have an arbitrary network and no.",
            "Constraints on what the potentials are right?",
            "Then we cannot guarantee that this is going to give us an integer answer.",
            "So what happens then if we get a fractional answer, what happens is in we're maximizing, right?",
            "That means we got some Z that doesn't correspond to Y, and usually that's going to mean that this is going to be bigger than any real Y. OK, so this is going to give us an upper bound on the real way."
        ],
        [
            "Let's look at this associative Markov network LP.",
            "So there again, we have these variables.",
            "EJ.",
            "Which are you know?",
            "For each position, then for GJK turns out we only need to track the ones that are on the diagonal.",
            "OK, 'cause everything else is 0 right?",
            "Well, whatever we take the log there were ones.",
            "I took the log.",
            "That thing was zero.",
            "OK so I just have a variable unbalances.",
            "Is this position in this position?",
            "Are they both on?",
            "OK, and then if they're both on, I get this bonus WFE plus loss.",
            "OK, so that's the writing down linear objective.",
            "To write down the constraints you do the thing we did before ZJ M = 1, meaning that.",
            "Each position has only one label.",
            "And then for the edge you do something slightly different, and that's because you assume that this weight is positive.",
            "OK, if that weight is positive, what happens is as I optimize, ZJK is going to become the minimum of the tool, right?",
            "Because it's trying to push against both of them is going to get stopped by the smallest of the two.",
            "If they were actually integers, right 01, then the minimum is going to be just the end of the tool.",
            "OK, so if everything is happy and DJ's are all integral, then DJK written down this way right?",
            "With these constraints at the maximum is going to mean exactly what we wanted.",
            "We wanted came to mean the end of these two things.",
            "OK, so this is a set of linear constraints.",
            "This is a set of.",
            "This is the objective.",
            "If K = 2.",
            "Right, so we have just a binary problem.",
            "Then you can also show that this thing has integer corners and it's not something that let me.",
            "I'm not claiming that I did this right, so this is a well known fact that come into optimization.",
            "It has to do with the constraint matrix.",
            "OK, so you can show it for the other one as well.",
            "There's a different condition for that matrix A.",
            "It's called totally imaginal total unimodularity.",
            "You know this one way to write it and that means that every if you look at this matrix A right, you basically lined all these up in a vector.",
            "You have a matrix hitting that.",
            "That vector and then you have some constraint.",
            "Say it's less than or equal to be OK as long as this is integral and a is totally modular, meaning that every sub determinant of a.",
            "Is either 1 -- 1 or 0?",
            "OK, so this is something you can verify for this matrix an for a bunch of other other ones.",
            "Every step determine if this thing is that.",
            "It turns out then for any objective whatever Q is, it doesn't matter the corners of this guy are going to be integral, and the way that sort of works is because at this solution right?",
            "I'm going to have some of these constraints be active, so there's going to be some submatrix of that.",
            "Matrix, it's called B that hits Z with equality.",
            "Right, and you know if this is full rank, assuming everything is full rank, then you know it's just the universe.",
            "If this is integral in the sub determinants of this are 1 -- 1 and 0, then this is going to be integral as well.",
            "OK, so that means pretty few line proof of this thing, but recognizing that a matrix is 2 totally new modular is actually a hard problem.",
            "But we know that these kinds of problems, the ones that arise from.",
            "Either like network flow problems or matching problems, things like that they have a special structure that that you know we can just apply known theorems and say this is true.",
            "It's totally new modular an for any objective we're going to get into the answer.",
            "Yeah.",
            "That clear.",
            "OK, so.",
            "Good so yeah.",
            "A good reference for that, and I think but, but.",
            "Nemhauser and Woolsey.",
            "Very good book that talks about about this kind of stuff and there's a few other conditions where you can guarantee integrality, but but this is by far the most the most.",
            "Sort of unrestricted, OK good, so we have a way to write down the objective and the constraints and get a linear program QZ such that a is less than B.",
            "That gives us an integral answer, meaning that answers.",
            "Has the same score as the Why score so we."
        ],
        [
            "And lost anything.",
            "Same thing here.",
            "Not sure how many of you do parsing so go fast to this.",
            "So here you have these basic context ruleset goes to BC.",
            "And then where it starts, ends and where service splits.",
            "OK.",
            "So basically B spends S2, M&C spends N + 1, two East, so there's a chart representation of this right where you basically say the span from three to five is covered by NP OK, and then so you will have variables for each span.",
            "So there's going to be N squared of these variables.",
            "Free span there's going to be variable an for each possible non terminal.",
            "A variable that says is that span covered by this non terminal.",
            "Same thing that for productions so for each.",
            "Triplet actually of start, middle and end.",
            "So there would be.",
            "2 seven and actually 1 two and seven so.",
            "What production was there right?",
            "So this kind of thing ABC start so the details are not super."
        ],
        [
            "Important, the idea is though you can write down the score again like this, you have two sets of variables.",
            "At the root, you make sure that the entire sentence from zero to N is covered by some non terminal.",
            "So some over a is equal to 1.",
            "This is going inside constraint where you say.",
            "That if I have a covering S 2 E then a is going to produce something.",
            "So as a sum over all possible midpoints and all possible productions are being C, This is going to be alright.",
            "If this is 1 then there's going to be exactly one of these fires.",
            "One way to think about it, and then the other way around if if S between S&E is covered by a, it must be that this was generated by somebody else, either from from East am so somewhere you know somewhere after M somebody generated a or somewhere before it was generated.",
            "OK, so basically this is that a is going to produce offspring and this says that a is not an orphan.",
            "You know is generated by somebody, roughly speaking.",
            "OK, so it turns out that these linear constraints again ensure that this linear program is going to give us integral solutions when we parse again.",
            "I'm not suggesting you parse with linear programs.",
            "That would be silly, so, but the point is that you can write down the.",
            "The objective the constraints in this way and guarantee that the solutions are integral.",
            "The other thing to note in all of these things that I was talking about the number of variables and number of constraints is linear in the number of parts, right?",
            "So everything is, you know.",
            "Small as compared to the number of possibilities that we should consider.",
            "How we doing in time?",
            "I'm.",
            "OK. Sir.",
            "Euphoria sweet dreams."
        ],
        [
            "20 two o'clock OK. Taken vestalia movie.",
            "OK, well known so this is.",
            "The last thing actually.",
            "Again, see this is a very simple one.",
            "We have the JK essentially mapping to yjk.",
            "There is a one to one mapping between Weizens ease their positive and then there's a degree constraint, right?",
            "But at most one of the Maps and at most one of the Maps OK, and we again we don't constrain equals to one.",
            "So for the.",
            "And we can show that has this.",
            "This A is also totally unimodular.",
            "OK so by the way, the problem of the non bipartite matching.",
            "So the system molecule matching there is no linear programming formulation.",
            "It looks very similar right?",
            "We have nodes and I want to get a matching right and I want to say constrain the degree to be equal to 1 or less than one.",
            "It turns out for that problem of a non bipartite matchings.",
            "The number of constraints you need to add to the linear program for it to actually give you an integral solution is exponential, or at least there is no known polynomial one.",
            "You know, for the last 40 years so.",
            "So that problem we're going to have to attack with some other method, right?",
            "So this kind of method applies when we can write down the LP and the number of constraints in LP and number variables in LP is."
        ],
        [
            "This polynomial.",
            "OK, so the other piece of.",
            "Piece of technology we need is is just duality, right?",
            "You seem valid in the context of EMS.",
            "If this is our LP.",
            "In this is a primal.",
            "The dual is that looks very similar.",
            "Just swap C&B transpose the A and.",
            "Add a Lambda right?",
            "So this is a mapping.",
            "I guess you flip the science too, so.",
            "We we wrote all their inference problems like this.",
            "Now we're going to actually use this guy, which is.",
            "So if this problem is feasible.",
            "Then you know this guy is going to be bounded and so this value in this value are equal in all the problems that we've considered.",
            "This set of constraints is always feasible, so we're in the good case where the strong dollar holds an we have this Max and this men are exactly equal, so we can just plug in then plug them in.",
            "Interchangeably."
        ],
        [
            "OK, so this is our formulation right?",
            "We plugged in the Max discrete Max.",
            "What we're going to do now is plug in.",
            "The continuous one.",
            "OK Qi, just abbreviate it.",
            "It's F5W plus Li.",
            "OK, so it's taking everything that kind of hits Y and putting it into this matrix that now it's F right?",
            "So there was W hitting FXY.",
            "Now we can write it down as W hitting FI OK in the same thing for loss so W hits this and then it hits Z and that gives you the score and then.",
            "This hits hits that and gives you the loss, right?",
            "So here you see why the loss was also important to be decomposable, because now it's going to be a linear function of these right visa vector and this guy is a vector and so that's going to get you know that that computes our loss for us.",
            "Biofeed reality is interchangeable, so we can just.",
            "Stick in this guy, hear them in and turns out we can just move them in up there, right?",
            "So basically this minimization over everything at the same time, right?",
            "So the only variables that appear in this minimization appear down below, so you might as well just move them in out here.",
            "So now we have a joint man over W. The slacks, parameter slacks and these lambdas, which is basically the lambdas.",
            "Try to compute for us implicitly.",
            "What is sort of the most violated constraint is what is the the highest scoring guy that's making us?",
            "Look bad, right?",
            "And all of that is sort of encoded into in this.",
            "Because that's the dual of the Max.",
            "Right, that's it.",
            "That's the whole trick.",
            "You go from discrete to continuous, take the dual and then you have a QP that has polynomial number of variables, right WS number parameters.",
            "Lambda is number of roughly corresponds to the number of constraints in your formulation which is.",
            "Number of parts."
        ],
        [
            "This thing is polynomial."
        ],
        [
            "Just to this transformation, you can.",
            "Basically you can eliminate this guy and put it up into the objective by simple manipulation.",
            "Basically this is being minimized, so this is being minimized, so the minimum what happens is this.",
            "If you move this over there, right?",
            "This is going to be exactly equal to that side, so we can eliminate."
        ],
        [
            "In the slack by just putting it like that.",
            "OK, so this is just kind of the cleanest formulation of the problem is minimizing norm this thing?",
            "Which is essentially the you know.",
            "Hinge loss and constraints in Lambda.",
            "That enforce the semantics that we want.",
            "No question yes."
        ],
        [
            "This time the.",
            "Is it the same thing about Methodism?",
            "But no method sense that you could, I mean.",
            "I mean, you could see you know.",
            "So the Lambda rise along to 0, right?",
            "So there's just going to be.",
            "Or maybe they, or they're going to be potentially.",
            "Actually did not necessarily going to be zero.",
            "They are not the same as.",
            "Well, OK, sure.",
            "So they are a little bit like the alphas in this in this VMS, but there are going to be.",
            "Much less sparse.",
            "But OK, so continuous.",
            "So suppose there are somewhat sparse.",
            "You know you could the other message.",
            "You might think of doing this is to sort of, you know, solve it for at some stage, and then look for awhile.",
            "That sort of is the maximum that add that into your settings, so like construction yet.",
            "So I'll talk about that this is.",
            "Yeah, at some point, yeah.",
            "So so that would be OK.",
            "So I have slides about that later, but that would be sort of a greedy approach where you just basically.",
            "How a current W right?",
            "You do the Max and you add in that constraint right?",
            "So it's constrained generation.",
            "You know, column generation.",
            "There's lots of inferred is bundled methods is also related to that.",
            "So that I mean, that's an approximate method.",
            "The problem is you keep resolving this QP multiple times and people have suggested this is exactly what Thomas Hoffman and his students did, and so on.",
            "And so it's a feasible method is just.",
            "I think it's much slower.",
            "We much slower and you know we the goal of this was to write the entire problem in polynomial form and then you have all kinds of options for how to solve it, right?",
            "So when you have just constraint generation, you're sort of stuck to one approach when you just write down the whole thing explicitly and in small you can use 2nd order methods.",
            "You could use, you can use whatever interior point methods to solve it fast etc etc.",
            "So there is advantage of writing the entire thing declaratively.",
            "As a small.",
            "Program because then you can use the entire toolbox as opposed to sticking to a particular method from beginning.",
            "So that's advantage and.",
            "Um?",
            "Right, so this is it, right?",
            "Basically, this form of you're kind of done right?",
            "So all you have to do you have a structured problem, you write down the LP for it.",
            "You take the dual.",
            "You formulate this QP, you stick it into, you know, Mosaic.",
            "Or simplex or whatever your favorite optimizer is, and then you're done, right?",
            "You don't need to write anymore code really.",
            "You just need to.",
            "Deciding the feature functions right down the LP tickets tool and the problem is solved, right.",
            "You know general kind of a tool box like like simplex or music that will solve structured prediction problems for you.",
            "At least in principle, right?",
            "Not clear.",
            "So in practice, of course you want to.",
            "I mean methods that actually optimize for particular structure a much better and then something that I've worked on.",
            "Quite a lot is trying to exploit the structure in this QP to do this solve these things faster than off the shelf optimizer does, but but the convenience in hand in being able to write something down declaratively and not having to write code.",
            "Suppose when you're decided to work on a new problem, you're trying out different models.",
            "Writing down declaratively and just having an answer pop out as much easier than writing writing procedures that solve it for your particular problem every time."
        ],
        [
            "OK, so it works for Markov networks like the serious.",
            "We talked about low treewidth means.",
            "I mean, think of the most trees right?",
            "If there are trees then we can write down the optimization and it's going to be integral.",
            "Associated marketing networks.",
            "It's exact for K = 2 context, free grammars, bipartite matchings, and.",
            "When we have untranslated Markham network so so Michael Networks that are Santana Grid but don't have this restriction.",
            "OK or they have restriction but K is greater than two then the LP relaxation in general is not going to be exact right?",
            "It's actually doing a Max so it's going to be bigger than the true one.",
            "And this one actually interesting open questions is what can you see in that?",
            "In that case, suppose for emens with K equals greater than two.",
            "We haven't actually a constant factor guarantee that the inference produces something that's factored.",
            "Two of the optimal.",
            "Does it translate into does that translate into a guarantee and learning and actually turns out it doesn't?",
            "But, well, certain practice that it works on.",
            "A whole range of problems.",
            "It worked remarkably well to deal with these approximate formulations."
        ],
        [
            "OK, so let's just unpack this a little bit what's going on so?",
            "This is our unfactored primal right enumeration of all the constraints.",
            "If we take the dual right this is.",
            "It looks a lot like SVM's.",
            "Just there weird things going on with the loss function in this VM is 01 S. This doesn't sort of pop out this way, but if you squint it kind of looks like a VM, right?",
            "You have some of alphas and then you have alphas hitting your support vectors here.",
            "The support vectors are not just.",
            "F so I'm suppressing X right?",
            "So I'm not sure when I started doing that.",
            "Oh yeah here.",
            "OK so I'm suppressing dependence on X just to make it more.",
            "Concise notation.",
            "OK so but FIY means F of XY.",
            "OK, so this is our support vectors.",
            "These are the support vectors are the differences between the true labeling and the alternative right?",
            "And it's not surprising 'cause those were there are constraints.",
            "So and then this thing comes to see.",
            "That's because of our slack variable.",
            "OK, so this site has expansion constrains.",
            "This one has exponentially many variables, but you can see from here that it's you know Colonel Isable.",
            "Everything is sort of nice in that sense.",
            "I.",
            "Now what have?"
        ],
        [
            "Opens in the factored primal dual when we actually, you know, gone to the trouble of writing down this this LP.",
            "So this is the factored one.",
            "If you take the dual, you kind of looks like this.",
            "And.",
            "So the question is OK, so those lambdas are kind of hard to interpret.",
            "Those lambdas are sort of the thing you know.",
            "Things that compute for you the argmax of the of the other guy of the other alternative.",
            "Right here, the mus.",
            "In the dual right, actually correspond to something that's fairly intuitive in terms of the original alphas, right?",
            "So the alphas we had, so this is going to be evident in the second, but they essentially take the alphas which correspond to entire sequence labeling or entire parse tree, and they are kind of a summary of a whole set of alphas, OK?"
        ],
        [
            "So in particular, if we look at OK, this is.",
            "The factor dual this is the unfactored tool and let's look at the case of sequences.",
            "So we have suppose these are support vectors, right?",
            "So I mean this is the truth and these are the three guys that turned out to be the support vectors.",
            "OK, so the alphas for them.",
            "Let's just say our point 4.25 point one 5.2 because C = 1, right?",
            "Good, so that's the solution.",
            "Say to that here this guy here, which is the exponential thing.",
            "And now if you think about what is actually going to be, if you workout with the solution is going to be in terms of Muse.",
            "It turns out that Mus are going to be essentially kind of alphas marginalized.",
            "So what does that mean so?",
            "There's going to be a mew.",
            "MU is actually a vector is going to be new for each position and each value for that position.",
            "OK, so here is going to be 26 muse for this position.",
            "It's kind of the same things as disease we had sewing 26 mus here and then the one that corresponds to B is going to value one here.",
            "There's going to be in 26 of them with zero except for RNC.",
            "Huawei because those are .2 of Alpha in here.",
            "So it's kind of marginalizing this down here.",
            "I have point.",
            "Or because it's taking this one and this one, and this is the sort of the only the only alphas that are active, right?",
            "So if you think of essentially these guys is normalized counts of the alphas.",
            "Weighted normalized counts of alphas.",
            "That's the correspondence so you can think of this marginals of this distribution over entire sequences.",
            "That clear so.",
            "Miss you so so these mus in the dual are going to be exactly disease that we had.",
            "Remember we we had a Z that encoded like each for each variable in its value or.",
            "Each edge etc etc because we took we took the.",
            "We had the Max, then we took the dual to get them in and then we took the dual here again of the resulting thing.",
            "So we took the dual twice so them use, you know expand to original disease.",
            "So whatever structure we had in code in the model is going to kind of pop out and amuse in his new variables.",
            "And they turned out to basically correspond to essentially kind of marginalization of Alpha.",
            "Which is kind of surprising fact, but but.",
            "But yeah, it's basically.",
            "I mean, maybe it's not so surprising because the all of the feature functions decomposed according to disease and according to sorry according to the parts.",
            "And so once you sort of.",
            "Optimize over these things should be really in terms of in terms of these part variables and going through the LP potentially derive them for you in some sense, automatically right with that solution is.",
            "Is that clear or is that?",
            "Somewhat clear.",
            "OK."
        ],
        [
            "OK, so you know in this exponential dual it was pretty easy.",
            "You know you take the.",
            "You take the this guy and we basically can kernelized kernelized this right.",
            "This function could be written as a kernel.",
            "Right, because the only thing you care about is just the dot product.",
            "OK, so you write this out and I have this time this this time, this this time, this so that product.",
            "You know you can do it for that.",
            "It turns out you can do this also in the factored dual.",
            "And there you're going to have kernels now that take in some part of the input some part of input on the sort of the thing you're comparing to the label and the label, and then so this is your kernel, right?",
            "So this is a way to go.",
            "Kind of nonparametric.",
            "You can just write down any kind of object, any sort of kernel you like there."
        ],
        [
            "OK, so this is so the alternatives are cheaper ways to do this right?",
            "So one way to do it is we talked about Perceptron.",
            "You know what a couple days ago, right?",
            "Whynott perception for this and there is one of the first algorithms for restricted prediction was a perception type algorithm where we basically you have a current W. You do the argmax right so it's a little more complicated than a binary one.",
            "And then what you do is basically if you made a mistake then you.",
            "Tweak your weights so that you go towards F of XY.",
            "OK, so you know that's your gradient direction.",
            "So the problem is with using this and that kind of works for some problems and breaks down for the others, and I think structured output is a particularly difficult case because what happens with structured output a lot of times you have very few instances, but each instance is big.",
            "So for example in the labeling labeling 3D point clouds we might have actually one or two instances, so each instance is a scene, right?",
            "Or we're doing these matchings?",
            "You know we only have 10 or 15 of them.",
            "There are lots of little parts in them, but the number of instances is small.",
            "And so when you do the argmax and you do the update, these updates are pretty large and you know it's very sort of noisy and this is a.",
            "Plot of this thing.",
            "Perceptron versus sort of a different method of ours that we were doing, and so it's a very, very noisy kind of.",
            "Isolation and you know the ways to deal with that and it's you could basically do average perceptron, which is your on your perceptron and then you take all of the weights that you've seen all the distinct weight vectors and you average them together and that helps you reduce the variance and regularize things.",
            "It still doesn't do quite as well as sort of doing the QP, but it's a good cheap alternative if you really need to scale up to very large problems."
        ],
        [
            "So the other the other methods is what you are suggesting is basically add the most violated constraints, so you have you do the argmax, and you add it.",
            "And.",
            "The advantages of that is you might be able to do the argmax and handle more general loss function.",
            "If you can do this, argmax with some other function that maybe doesn't decompose linearly, but you can still do the argmax somehow nicely.",
            "You can add that constraint, then people have used this to say, do something like.",
            "F1 here, as opposed to Hamming loss.",
            "So you can show actually this is work by.",
            "Took a TARDIS at all, so the number of constraints you need to add in order to solve this to epsilon is actually polynomial.",
            "And this has to do with the fact that you know we have an Oracle that sort of polynomial time Oracle that gives us the optimal violated constraint.",
            "So whenever you have a situation where you can get the most valid constraint.",
            "With an Oracle that Oracle is argmax, then you can get these sort of polynomial solution time.",
            "The problem is that you know if you look at the worst case number of constraints, it's going much larger than what we have and also your resolving.",
            "This could be many times, so it's a nice result but but it also gives this kind of polynomial solution, but but at least in the in terms of guarantees guarantees it's a much more expensive one.",
            "In practice, it's a very very useful tool as well.",
            "So then there's actually software for doing this.",
            "Um?",
            "So, so it's a good."
        ],
        [
            "Alternative to so.",
            "Time wise, but there's some results.",
            "This is some early stuff that we did with the CRF's.",
            "You know doing something that's local, predicting each each letter independently.",
            "Doing a CRF where we optimize log likelihood when things are connected doing what we call em cube Nets.",
            "Which is this Max margin Markov network.",
            "Basically the same features but just different loss function.",
            "Turns out that it helped.",
            "And then we looked at sort of adding kernels, right?",
            "So adding?",
            "Quadratic and cubic kernels on the pixels.",
            "Up to Lawton for CRF's.",
            "It's actually kind of hard to do this with serious because the nice thing about using kernels with a with a hinge loss is that the solutions are fairly sparse with CRF, so you can kernelized CRF's as well.",
            "But the solutions are not sparse, and so you basically kind of memorize the entire data set.",
            "So we could actually do that.",
            "This is the result for that."
        ],
        [
            "I want to get to the other formulation, so I'm going to go a little fast on these, so this is a text classification problem.",
            "But exhibition problem we have sort of web pages and the web pages are connected by hyperlinks, so we do independent classification.",
            "We do kind of a market random field on that using likelihood an using M cube Nets, so this is sort of the comparison of those and.",
            "Again, here we were using was this linear relaxation that's not tight because the model is unconstrained.",
            "I mean there's four class or five classes.",
            "How many pages and links the LP can give you fractional solutions, but in practice, what happens is what happens.",
            "It works fine, works better than our previous results in this data set that we've."
        ],
        [
            "Send OK, so I wanted to show so this part.",
            "This is the the point.",
            "Clouds is collected by this robot that roamed around Stanford and collected 3 point clouds.",
            "So there's about 3 million points we labeled by hand some small sub."
        ],
        [
            "Set of those and."
        ],
        [
            "Again, this is what I showed before flat.",
            "This is an attempt to do this kind of voting in order to smooth things out."
        ],
        [
            "It works, but not doesn't work as well as.",
            "Doing the full MQ net."
        ],
        [
            "Um?"
        ],
        [
            "And so this is the accuracies showing example of what it actually looks like.",
            "If it works.",
            "No it doesn't.",
            "Shoot OK, I don't know something is, so there's a fly through of the thing.",
            "But now here we go.",
            "So this is what the data set looks like.",
            "So you know again, there's tons of sort of sparsity issues with points sampled.",
            "Nonuniformly everywhere, and so this is a result of income net.",
            "On this where you have.",
            "Local classifieds local features as well as points connected to its nearest nearest couple of neighbors.",
            "Nearest couple of neighbors in the kind of vertical thing and it works fairly well.",
            "The places where it doesn't work is where you have really long range dependency, so if you look at those poms over there some of the problems that gets right.",
            "Basically by propagating the information about the Crown of the palm down to the the all the way down to the bottom of the tree.",
            "And some of them sort of the trunk wins and it thinks that.",
            "That palm is is a building.",
            "I mean this is Stanford campus.",
            "A lot of columns look like palms etc.",
            "But you know this is a limitation of course of a again a fairly local model.",
            "I mean the model is MRF that only looks at nearest neighbors and.",
            "And when you have very very long structures, things can kind of go either way.",
            "So that's that I can.",
            "Explain how we did this.",
            "If we have time, you can't run an LP in this thing to solve it, and so it's using a minimum weight cut algorithm to do this efficiently so."
        ],
        [
            "OK, word alignment results.",
            "So this is supervised setting.",
            "We have train 100 sentences tests in 350 and then we compared several methods.",
            "Here again local learning.",
            "So this is what I meant by this kind of 2nd approach where you say I'm going to learn for each edge whether it's present or not, ignoring the structure.",
            "And then I'm going to take the output of that and use it as my edge score so it doesn't work that well as opposed to sort of doing the global learning and global prediction.",
            "But then we also compared it to something that's basically uses a lot more data, but in an unsupervised way.",
            "In that both both of these approaches.",
            "This approach and this is a standard software that basically uses a lot of unsupervised data.",
            "An EM to get get this accuracy.",
            "If we use this predictions of this guy is a feature, then you know we can look again look at local learning.",
            "In our approach, we still kind of beat it and then the other model that we tried sort of after that is is a model that actually tries to not just have a score on each edge but also have scores on pairs of edges.",
            "OK, so you can model things that you have.",
            "Consecutive words mapping two consecutive words over here.",
            "In that case, what happens is it's no longer a tractable problem, it's actually it's a quadratic assignment problem, which is as hard as traveling salesman.",
            "So again, we use this linear relaxations an empirically.",
            "It works really well even now there are no guarantees on how well it does.",
            "Yeah, so it turns out actually."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, my name is Ben Ben Tasker.",
                    "label": 0
                },
                {
                    "sent": "I'm from U Penn and I'll talk about structured prediction actually now as opposed to later in the afternoon we switched the order for various reasons so.",
                    "label": 0
                },
                {
                    "sent": "Hopefully if you have any questions about graphical models, graphical models in particular will be answered late in the afternoon, so this is kind of a teaser about some of those issues, but I think you should be able to follow most of the stuff doesn't depend on graphical models.",
                    "label": 0
                },
                {
                    "sent": "Please ask questions early so I know where to slow down when to speed up, so I want questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll talk about structured prediction and one particular view of it using large margin kind of approaches like SVM that we've seen before.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And start with sharing with you guys cartoon.",
                    "label": 0
                },
                {
                    "sent": "I ran across a while back and this is.",
                    "label": 0
                },
                {
                    "sent": "New Yorker cartoon.",
                    "label": 0
                },
                {
                    "sent": "So don't worry Howard.",
                    "label": 0
                },
                {
                    "sent": "The big questions are multiple choice and I would argue that the machine learning especially supervised machine learning.",
                    "label": 1
                },
                {
                    "sent": "Community is spent way too much time worrying about binary prediction and not thinking about the big questions or the where the big questions are ones that have a lot of answers.",
                    "label": 0
                },
                {
                    "sent": "Too many answers to enumerate.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is exactly where sort of structured prediction comes in.",
                    "label": 0
                },
                {
                    "sent": "The kind of questions that we want to address.",
                    "label": 0
                },
                {
                    "sent": "Our prediction problems, where the output is not a single bit, yes or no, but a sequence, for example sequence of outputs like here.",
                    "label": 0
                },
                {
                    "sent": "So instead of predicting each letter I'm predicting into.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A word.",
                    "label": 0
                },
                {
                    "sent": "Or the output might have spatial structure.",
                    "label": 1
                },
                {
                    "sent": "So the problem here is taking this 3D point cloud and classifying.",
                    "label": 0
                },
                {
                    "sent": "Say basically mapping out the buildings which are in red.",
                    "label": 0
                },
                {
                    "sent": "The trees are in green, the Bush is in blue, there's ground sitter etc.",
                    "label": 0
                },
                {
                    "sent": "Mapping out the objects in the scene.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The outputs might have.",
                    "label": 0
                },
                {
                    "sent": "Recursive structure as we find find in natural language parsing.",
                    "label": 1
                },
                {
                    "sent": "So the input is a sentence and the output is.",
                    "label": 0
                },
                {
                    "sent": "Context free grammar type of parse of the sentence.",
                    "label": 0
                },
                {
                    "sent": "Right, this doesn't look like.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Single bit some of the more exotic prediction problems that I've been working on.",
                    "label": 0
                },
                {
                    "sent": "The output might be an actual combinatorial object like a matching, so this is a problem in natural language machine translation, so part of the pipeline of building a machine translation system is doing word alignment, taking pairs of sentences that are supposedly translations of each other, and establishing correspondences.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking word to word.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to make a simplifying assumption that.",
                    "label": 0
                },
                {
                    "sent": "You know one word is going to map to at most one word here and for English French it's not such a horrible assumption about 9095% of words can be kind of map 1 to one as well.",
                    "label": 0
                },
                {
                    "sent": "There's exceptions, but let's just think of it this way.",
                    "label": 0
                },
                {
                    "sent": "So what I'm looking for is, given this pair of sentences, give me a matching partial matching of each word for each word.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, complex input, complex output.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another problem that's somewhat similar, but a different kind of control structure comes up in computational biology.",
                    "label": 0
                },
                {
                    "sent": "The problem of protein prediction in particular is, you know taking a string of I, me, know acids an from the string predicting the three dimensional structure, extremely hard problem and I'm not going to take it today.",
                    "label": 0
                },
                {
                    "sent": "But a subset of the problem is determining what are called disulfide bonds.",
                    "label": 0
                },
                {
                    "sent": "These bonds are basically the strongest bonds there are in this in this structure, and they hold together a fold this this this protein into into shape.",
                    "label": 1
                },
                {
                    "sent": "And what happens with these bonds?",
                    "label": 0
                },
                {
                    "sent": "They form between cysteine molecules.",
                    "label": 0
                },
                {
                    "sent": "So we have our string.",
                    "label": 0
                },
                {
                    "sent": "We have the locations of the Seas and we know that these seas are going to find a pair and match up.",
                    "label": 0
                },
                {
                    "sent": "We just don't know which one.",
                    "label": 0
                },
                {
                    "sent": "So the prediction problem is to figure out which C is going to match to which other see.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a matching.",
                    "label": 0
                },
                {
                    "sent": "It's not bipartite matching like in Word alignment, but it's a non bipartite matching matuska material structure.",
                    "label": 0
                },
                {
                    "sent": "Turns out it's a harder problem than the bipartite matching, but but we can still sort of use the same kinds of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Methods.",
                    "label": 0
                },
                {
                    "sent": "So a lot of approaches to these kinds of structure problems basically take the tools we know things.",
                    "label": 0
                },
                {
                    "sent": "the Hammers we have in our hands and apply them essentially by breaking down, divide and conquer approach you take your problem, we break it up into pieces.",
                    "label": 0
                },
                {
                    "sent": "We predict each piece individually, right?",
                    "label": 0
                },
                {
                    "sent": "So we know how to build a classifier for each letter.",
                    "label": 0
                },
                {
                    "sent": "Same thing.",
                    "label": 0
                },
                {
                    "sent": "In this case we might take a sliding window that.",
                    "label": 0
                },
                {
                    "sent": "Classifies each pixel by looking at surrounding independently.",
                    "label": 0
                },
                {
                    "sent": "The problem with this approach is that it doesn't use the correlations in the data, and the fact that certain letters are follow certain other letters much more commonly, or that, for example, a pixel that's next to a tree pixel is going to be more likely to be next to a tree pixel.",
                    "label": 0
                },
                {
                    "sent": "More generally, we might have constraints if we're trying to predict, say, a parse tree from a sentence right there, we cannot predict each.",
                    "label": 0
                },
                {
                    "sent": "Each part of the parse tree independently, because they have to fit together into recursive structure.",
                    "label": 0
                },
                {
                    "sent": "If we're predicting matchings again, we cannot predict each edge independently.",
                    "label": 0
                },
                {
                    "sent": "Their constraints that make sure that it's a matching right, so this kind of local prediction is not going to solve all their problems.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But let's see what it does on this somewhat controlled problem.",
                    "label": 0
                },
                {
                    "sent": "So this is again problem of segmentation of 3D point cloud data, and So what we've done here is basically taken an SVM, trained it on local windows to try to predict what there's a tree at, red or green.",
                    "label": 0
                },
                {
                    "sent": "Sorry, building red treat in green and blue is Bush is the ground is Gray.",
                    "label": 0
                },
                {
                    "sent": "So as you can see it does pretty well.",
                    "label": 0
                },
                {
                    "sent": "I mean that Chris here is about 70% or something like that, but there's tons of these kind of inconsistency issues actually.",
                    "label": 0
                },
                {
                    "sent": "Actually, I haven't highlighted so there's trees on top of Windows.",
                    "label": 0
                },
                {
                    "sent": "There's trees on top of roofs.",
                    "label": 0
                },
                {
                    "sent": "There is building setup of trees etc etc and this is a problem that you cannot really solve with a bigger window or not right because of.",
                    "label": 0
                },
                {
                    "sent": "All kinds of sampling issue.",
                    "label": 0
                },
                {
                    "sent": "Density of sampling diversity and just diversity of scale.",
                    "label": 0
                },
                {
                    "sent": "There's no right window for everything, and you can.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "We cannot also solve it as well sure later by just say voting or something like that.",
                    "label": 0
                },
                {
                    "sent": "Taking a local neighborhood and and taking a majority or something like that.",
                    "label": 0
                },
                {
                    "sent": "Local approaches do really have these inconsistent kinds of predict.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So instead, we're going to advocate is structured prediction is models that take the whole input and try to predict the entire output.",
                    "label": 0
                },
                {
                    "sent": "That sort of is self consistent that uses the local information like the local windows and local signal, but also exploit correlations and respect constraints.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, for example, in this problem, if we use one of the models and talk about later.",
                    "label": 0
                },
                {
                    "sent": "This is what it does there, and this is sort of comparison right?",
                    "label": 0
                },
                {
                    "sent": "So it really what it does is stress to enforce the the fact that nearby pixels are correlated, so so it really smooth, smooth out this area in this area and that area.",
                    "label": 0
                },
                {
                    "sent": "I mean it's not perfect, but.",
                    "label": 0
                },
                {
                    "sent": "In the setting we don't have to make these decisions individually.",
                    "label": 0
                },
                {
                    "sent": "We can essentially borrow strength of virus signal bar information from surrounding area from things you're interacting with to do prediction better.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the straw.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prediction setting and then there are many different instantiations of that.",
                    "label": 0
                },
                {
                    "sent": "So we can think about sequences and so people have looked at what are called CRF models that essentially are kind of like Hmm's, but there's sort of a discriminative version of that there.",
                    "label": 0
                },
                {
                    "sent": "If you think about naive Bayes versus logistic regression, the Hmm's interior sort of the same, the same way.",
                    "label": 0
                },
                {
                    "sent": "So starting with that model and I'll explain a little bit now and then, I'll explain it more later in the afternoon.",
                    "label": 0
                },
                {
                    "sent": "You can think about doing structured prediction on sequences.",
                    "label": 0
                },
                {
                    "sent": "You can do structured prediction on trees.",
                    "label": 1
                },
                {
                    "sent": "These guys, which I'll talk about soon.",
                    "label": 0
                },
                {
                    "sent": "There are special kind of wraps, matchings, etc.",
                    "label": 0
                },
                {
                    "sent": "So that's the problem.",
                    "label": 0
                },
                {
                    "sent": "Mapping from complex input to complex output an trying to exploit everything we can about correlations, constraints, everything that clear.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "So this is the rough outline I'm going to talk about, so I'm going to talk about how we can formulate this as a large margin problem, and how sort of the structure can be exploited and respected.",
                    "label": 1
                },
                {
                    "sent": "And there's two different formulations for this that we need to actually solve.",
                    "label": 0
                },
                {
                    "sent": "Some of these problems.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's the rough outline.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let me.",
                    "label": 0
                },
                {
                    "sent": "Try to abstract from all these different models so we have trees.",
                    "label": 0
                },
                {
                    "sent": "We have these hmm type thing sequences.",
                    "label": 0
                },
                {
                    "sent": "We have matchings all of these things look kind of.",
                    "label": 0
                },
                {
                    "sent": "Diverse and hard to sort of handle in one unified way, so this is an attempt to do that is to say OK, our structured model definition is just this kind of.",
                    "label": 0
                },
                {
                    "sent": "Well, in some sense, right now it's a black box.",
                    "label": 0
                },
                {
                    "sent": "Basically it takes an input X and output Y.",
                    "label": 0
                },
                {
                    "sent": "This thing is, you know some complex object.",
                    "label": 0
                },
                {
                    "sent": "This thing is also complex.",
                    "label": 0
                },
                {
                    "sent": "Object finds the best one according to the scoring function.",
                    "label": 0
                },
                {
                    "sent": "OK, that's our definition of a structured model, right?",
                    "label": 0
                },
                {
                    "sent": "And there's a space of feasible outputs, and it might depend on X, which is the space of possible matchings or trees.",
                    "label": 0
                },
                {
                    "sent": "Note that this space is generally very large.",
                    "label": 0
                },
                {
                    "sent": "It's exponential in the size of of.",
                    "label": 0
                },
                {
                    "sent": "Input the number of save points in your point cloud right in that point cloud, there's about actually 15 million points or so, so.",
                    "label": 0
                },
                {
                    "sent": "It's basically you say 4 to that to that power for the number of outputs in general, so we cannot enumerate that.",
                    "label": 0
                },
                {
                    "sent": "Enumerate that set efficiently, right?",
                    "label": 0
                },
                {
                    "sent": "So we're going to be looking at, you know, going to specialize the definition to models where we can do this.",
                    "label": 0
                },
                {
                    "sent": "Argmax in somewhat efficient way.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, we're sort of stuck at being too abstract, so.",
                    "label": 0
                },
                {
                    "sent": "We can assume that the score is our favorite linear combination of features, right?",
                    "label": 0
                },
                {
                    "sent": "It's very much like you doing this VMS, but now the features are not only dependent on X.",
                    "label": 0
                },
                {
                    "sent": "This feature mapping function, but it also depends on the output.",
                    "label": 0
                },
                {
                    "sent": "So we get together are.",
                    "label": 0
                },
                {
                    "sent": "The sequence of images with a potential labeling an we create features on that and I'll show exactly what they look like, right?",
                    "label": 0
                },
                {
                    "sent": "Same thing for matchings and trees, right?",
                    "label": 0
                },
                {
                    "sent": "There's a joint feature map that takes input and output and produces a big vector of.",
                    "label": 0
                },
                {
                    "sent": "You can think of them as sufficient statistics of X&Y.",
                    "label": 0
                },
                {
                    "sent": "You can think of him as basically some kind of counts of what's happening with X&Y, just the vector, and EE is the thing we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "The parameter vector that basically prefers or just prefers certain structures and forms the score.",
                    "label": 0
                },
                {
                    "sent": "OK so question.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Based on this very specific model of a great like, well, it's not very restrictive, I mean.",
                    "label": 0
                },
                {
                    "sent": "I'm contemplating considers only Saturday.",
                    "label": 0
                },
                {
                    "sent": "So I'm saying this is a. I'm claiming this is a mild assumption, meaning that if you.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you would want to score to be something else, I can.",
                    "label": 0
                },
                {
                    "sent": "Usually you know, basically put that function into the feature mapping and then make it linear, right?",
                    "label": 0
                },
                {
                    "sent": "It's the same thing as and I can use kernels here too later.",
                    "label": 0
                },
                {
                    "sent": "Nothing is given to you in the party to learn is coming to value.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, it's a standard supervised, yes?",
                    "label": 0
                },
                {
                    "sent": "And once you fix your feature space, this is a limited set of functions, yeah?",
                    "label": 0
                },
                {
                    "sent": "So am I sure?",
                    "label": 0
                },
                {
                    "sent": "Which is based in everything except yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "I mean in the nonparametric setting when we make these things, we can kernelized these guys.",
                    "label": 0
                },
                {
                    "sent": "Then you can kind of grow with these data set.",
                    "label": 0
                },
                {
                    "sent": "Right, so it potentially could be on any arbitrary function.",
                    "label": 0
                },
                {
                    "sent": "Not quite arbitrary, but a function that's.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean, we're going to assume in later on that this thing decomposes nicely, right?",
                    "label": 0
                },
                {
                    "sent": "We need to be able to.",
                    "label": 0
                },
                {
                    "sent": "This still doesn't give us the ability to find argmax.",
                    "label": 0
                },
                {
                    "sent": "What's going to that abilities?",
                    "label": 0
                },
                {
                    "sent": "This function further decomposes into parts, and then we can maximize over that.",
                    "label": 0
                },
                {
                    "sent": "I mean, these these parts might be in grammar.",
                    "label": 0
                },
                {
                    "sent": "Little productions in a sequence, pairs of input, output, and output, output, consecutive outputs, etc etc.",
                    "label": 0
                },
                {
                    "sent": "So, so we're going to restrict further the function to be a sum of parts.",
                    "label": 0
                },
                {
                    "sent": "That's a much stronger restriction, but the dependent.",
                    "label": 0
                },
                {
                    "sent": "So the dependence on X could be potentially arbitrary, right?",
                    "label": 0
                },
                {
                    "sent": "We can make a nonparametric.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me map this chain Markov net CRF into this sort of WRF thing.",
                    "label": 0
                },
                {
                    "sent": "Which is, you know it's not done.",
                    "label": 0
                },
                {
                    "sent": "It's not really usually written that way.",
                    "label": 0
                },
                {
                    "sent": "I love.",
                    "label": 0
                },
                {
                    "sent": "Uh, it's pretty easy to see so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so let me just start with this.",
                    "label": 0
                },
                {
                    "sent": "OK so we have our X right?",
                    "label": 0
                },
                {
                    "sent": "It's a sequence of images.",
                    "label": 0
                },
                {
                    "sent": "We have our Y which is a sequence of labels.",
                    "label": 0
                },
                {
                    "sent": "26 possibilities.",
                    "label": 0
                },
                {
                    "sent": "And then this conditional random fields are probabilistic models that define distribution by take by sort of decomposing it into parts and then.",
                    "label": 0
                },
                {
                    "sent": "Through of multiplying those parts together.",
                    "label": 0
                },
                {
                    "sent": "More on that later today so the parts are these node potentials, which is potentials between image and the label and edge potentials which are between consecutive labels and also include the input as well.",
                    "label": 0
                },
                {
                    "sent": "If you'd like to.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you are more familiar with HMMS right?",
                    "label": 0
                },
                {
                    "sent": "This would kind of correspond to a.",
                    "label": 0
                },
                {
                    "sent": "Emission probabilities and this would be the transition probabilities, right?",
                    "label": 0
                },
                {
                    "sent": "So the difference here is that these guys don't need to be probabilities, they don't need to sum to one, they just need to be positive so.",
                    "label": 0
                },
                {
                    "sent": "The the only restriction again there there are non negative and we're going to further say that they're positive right there, just represented through this E to the again linear combination of things.",
                    "label": 0
                },
                {
                    "sent": "Attention.",
                    "label": 0
                },
                {
                    "sent": "Sequences what is the complete relationship between elements in which I don't know all the things sequence?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is more of an example and so the for example we do the segmentation of point Cloud.",
                    "label": 0
                },
                {
                    "sent": "It's a 3D grid.",
                    "label": 0
                },
                {
                    "sent": "Or I mean it's not a regular grid, but it's roughly speaking 3 degrees.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so this is just an illustration of what it looks like in.",
                    "label": 0
                },
                {
                    "sent": "In Metro language and by informatics condition biology, alot of the problems are changed.",
                    "label": 0
                },
                {
                    "sent": "So this special case is a very important case and write this.",
                    "label": 0
                },
                {
                    "sent": "All of this extends to other things so again so our potential is log linear kind of representation right?",
                    "label": 0
                },
                {
                    "sent": "X to the WF OK and then it depends on XJ and YJ so we're going to basically the index of the WS.",
                    "label": 0
                },
                {
                    "sent": "The correspondence subvector of parameters that correspond to the node parameters.",
                    "label": 0
                },
                {
                    "sent": "And the same thing here actually.",
                    "label": 0
                },
                {
                    "sent": "So I wanted to mention what are the features here, so the features might be something like this pixel in 3rd row and 4th column is on and the letter label is Z OK, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a feature that's basically saying I mean and so if that Pixel likes to be on when letter Z, then you probably will have positive wait.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't then it will probably be a negative weight because positive negative weights or this feature will affect.",
                    "label": 0
                },
                {
                    "sent": "The potential and so the larger the potential is, the more likely it is to be that label.",
                    "label": 0
                },
                {
                    "sent": "Right, so right?",
                    "label": 0
                },
                {
                    "sent": "And again, yeah, so for edges you might have features like what's the previous letter and what's the next.",
                    "label": 0
                },
                {
                    "sent": "OK, so here it doesn't depend on X, but in general we can.",
                    "label": 0
                },
                {
                    "sent": "OK, yes, 2 letters.",
                    "label": 0
                },
                {
                    "sent": "Obviously there are some combinations that don't happen, but I mean most combinations.",
                    "label": 0
                },
                {
                    "sent": "It doesn't sound like you're getting a lot of information from the like.",
                    "label": 0
                },
                {
                    "sent": "I mean, it was three letters, lot more information than just too.",
                    "label": 0
                },
                {
                    "sent": "But I mean sure.",
                    "label": 0
                },
                {
                    "sent": "So again, this is example only so we can deal with non pairwise potentials and so I could have a pairwise micro network.",
                    "label": 0
                },
                {
                    "sent": "But I could have XYJKL and then I have larger features and everything would just go through so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly so if you have, say a second order model then you know you just blow up your state space by instead of being 2626 will be 26 ^3.",
                    "label": 0
                },
                {
                    "sent": "But yeah, but it's all still doable as long as those connections are short range, right?",
                    "label": 0
                },
                {
                    "sent": "As long as you just depend on the previous couple of things.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, this is a little bit of notation manipulation which but I just wanted to get what I want to get.",
                    "label": 0
                },
                {
                    "sent": "Is this right?",
                    "label": 0
                },
                {
                    "sent": "I want to show that these guys look like this E to the scoring function that I want.",
                    "label": 0
                },
                {
                    "sent": "OK so this is all just about trying to.",
                    "label": 0
                },
                {
                    "sent": "Where geographical model into this sort of form?",
                    "label": 0
                },
                {
                    "sent": "OK, so how do I do that?",
                    "label": 0
                },
                {
                    "sent": "I mean they just say OK, so let me look at the product of the potentials.",
                    "label": 0
                },
                {
                    "sent": "That's just X of the sum of those guys, because individual ones were just easier that OK, which is X2 WFN of XY, where I've defined a new thing FN of XY.",
                    "label": 0
                },
                {
                    "sent": "Is this some of these things?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what I mean by sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "So we had an individual feature that looked at particular particular position.",
                    "label": 0
                },
                {
                    "sent": "And letter and said OK, this on, and this is the OK. Now what this guy does says OK, some over the sequence.",
                    "label": 0
                },
                {
                    "sent": "How many times do I see this position on an letter Z?",
                    "label": 0
                },
                {
                    "sent": "So that's a sufficient statistic?",
                    "label": 0
                },
                {
                    "sent": "OK, and that's what's going to wait that wait, and the same thing here, right?",
                    "label": 0
                },
                {
                    "sent": "I have you know this feature and then now I just.",
                    "label": 0
                },
                {
                    "sent": "Compute how many times is a followed by Z, so that's again sufficient statistic.",
                    "label": 0
                },
                {
                    "sent": "Alright man so.",
                    "label": 0
                },
                {
                    "sent": "Now my this whole product is just this.",
                    "label": 0
                },
                {
                    "sent": "This whole product is just that.",
                    "label": 0
                },
                {
                    "sent": "So if I just going to stack up F this way and that way I can write it this way.",
                    "label": 0
                },
                {
                    "sent": "So all I'm saying is that the probability of Y given X. Alright, is proportional to Y to this WF of FXY where FXY is this feature function feature mapping function.",
                    "label": 0
                },
                {
                    "sent": "It takes a full labeling.",
                    "label": 0
                },
                {
                    "sent": "It takes a full input and gives me some vector statistics.",
                    "label": 0
                },
                {
                    "sent": "That's all it is and this is what this thing is.",
                    "label": 0
                },
                {
                    "sent": "Hopefully that's clear and it's sometimes it's sort of hard to you know.",
                    "label": 0
                },
                {
                    "sent": "Visualize it and kind of get comfortable with it, but it's.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a lot easier once you get to use this representation.",
                    "label": 0
                },
                {
                    "sent": "Is that clear, so just one.",
                    "label": 0
                },
                {
                    "sent": "I want to be labored if everybody's.",
                    "label": 0
                },
                {
                    "sent": "OK good.",
                    "label": 0
                },
                {
                    "sent": "I think people are.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People are happy.",
                    "label": 0
                },
                {
                    "sent": "So the same thing happens in so it doesn't have to be a chain, right?",
                    "label": 0
                },
                {
                    "sent": "So if I have just a general product of node potential, edge potentials is not a chain, you know, I just have, so we're going to talk about specific kinds of Markov networks that essentially try to make.",
                    "label": 0
                },
                {
                    "sent": "Nodes that are connected by an edge so usually say for example we have a grid 3D grid that we're trying to label things that are nearby tend to be the same label.",
                    "label": 0
                },
                {
                    "sent": "So what happens in the potential is that everything is 1, meaning that doesn't do anything, and then the diagonal entries are greater than one.",
                    "label": 0
                },
                {
                    "sent": "So basically the model right?",
                    "label": 0
                },
                {
                    "sent": "If this is greater than one, it prefers the labels to be.",
                    "label": 0
                },
                {
                    "sent": "The same right so?",
                    "label": 0
                },
                {
                    "sent": "A lot of assignments that have labels nearby labels the same.",
                    "label": 0
                },
                {
                    "sent": "Are more likely OK.",
                    "label": 0
                },
                {
                    "sent": "Turns out this is a special model because it allows the argmax to be done efficiently.",
                    "label": 0
                },
                {
                    "sent": "So just examples of these features would be in this.",
                    "label": 0
                },
                {
                    "sent": "In this case this would be.",
                    "label": 0
                },
                {
                    "sent": "Instead of from computer vision, there are some types of descriptors are used for classifying essentially 3D texture, right?",
                    "label": 0
                },
                {
                    "sent": "We can look at, say, a normal to the surface and essentially a histogram of the densities around this with respect to this normal.",
                    "label": 0
                },
                {
                    "sent": "So this is one of kind of descriptors that's used to determine what kind of texture I see at particular point.",
                    "label": 0
                },
                {
                    "sent": "So for example, you know walls and columns are pretty flat, while trees are much more dispersed than Bush is are having different.",
                    "label": 0
                },
                {
                    "sent": "Texture etc etc and then edge features might be, you know, kind of how far to things are away from each other, how?",
                    "label": 0
                },
                {
                    "sent": "What are the normals of the two of the two surface points etc etc.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is some arbitrary feature function.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Feature mapping.",
                    "label": 0
                },
                {
                    "sent": "Diagnosed",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a restriction.",
                    "label": 0
                },
                {
                    "sent": "This is a restriction we going to make and the model in order to make it tractable.",
                    "label": 0
                },
                {
                    "sent": "So this is this is this is just an example that I'm going to use later, But it turns out this is a special kind of model that no matter so.",
                    "label": 0
                },
                {
                    "sent": "Well, do you wanna ask your question?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So usually when you don't have chains or trees, right?",
                    "label": 0
                },
                {
                    "sent": "The question of inference.",
                    "label": 0
                },
                {
                    "sent": "Finding the argmax is hard.",
                    "label": 0
                },
                {
                    "sent": "In case of this kind of model, especially for K = 2.",
                    "label": 0
                },
                {
                    "sent": "No matter what the topology is, the argmax is tractable OK, and then if K is greater than two, you have an approximation guarantee.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to that, but this is one kind of model that's useful in a lot of computer vision applications.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let me also try to put this kind of a parsing grammar type.",
                    "label": 0
                },
                {
                    "sent": "Model into the same framework.",
                    "label": 0
                },
                {
                    "sent": "So usually if I want to have distribution over parse trees given input sentence, I do it in terms of productions.",
                    "label": 0
                },
                {
                    "sent": "This is a production that takes a non terminal and produces Alpha is either say a non terminal and non terminal or terminal or whatever.",
                    "label": 0
                },
                {
                    "sent": "It's some sequence of other things.",
                    "label": 0
                },
                {
                    "sent": "OK so these are these productions of my grammar.",
                    "label": 0
                },
                {
                    "sent": "And so when I want to, when I want to calculate the probability of this guy, what I do is I take all the productions that are in this X&Y.",
                    "label": 0
                },
                {
                    "sent": "So the productions here would be S goes to N PvP exclusive I. Map them out.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so production here is S goes to in PvP.",
                    "label": 0
                },
                {
                    "sent": "Here DT goes to the etc etc.",
                    "label": 0
                },
                {
                    "sent": "And so this feature mapping just takes this.",
                    "label": 0
                },
                {
                    "sent": "Sequence in a tree and Maps it into the statistics.",
                    "label": 0
                },
                {
                    "sent": "How many times did I see in Pico de TNN?",
                    "label": 0
                },
                {
                    "sent": "ETC etc.",
                    "label": 0
                },
                {
                    "sent": "So this is noun phrase, determiner, verb, etc.",
                    "label": 0
                },
                {
                    "sent": "OK, so again the probability is just going to be proportional to.",
                    "label": 0
                },
                {
                    "sent": "If you know if we have features that are like this, features that depend on the non terminal.",
                    "label": 0
                },
                {
                    "sent": "And the production, what it produces.",
                    "label": 0
                },
                {
                    "sent": "It's the scoring.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I think this is the last thing.",
                    "label": 1
                },
                {
                    "sent": "So for those those things I just described before are commonly.",
                    "label": 0
                },
                {
                    "sent": "No specified as probabilistic model, so we kind of started with conditional probability distribution and said oh look it's can be written.",
                    "label": 0
                },
                {
                    "sent": "This log linear form.",
                    "label": 0
                },
                {
                    "sent": "OK so forward.",
                    "label": 1
                },
                {
                    "sent": "I mean for word alignment for matchings you don't actually start with probabilistic model, and there's a good reason for that and I'll get back to that.",
                    "label": 0
                },
                {
                    "sent": "But basically here we just assume that the score of a matching is just something that basically looks at each edge and see that is present in the matching and says what's the score and then just.",
                    "label": 0
                },
                {
                    "sent": "Heads up OK, so for each YJK.",
                    "label": 0
                },
                {
                    "sent": "JK, that's in the matching I calculated score so a little bit of abuse notation here.",
                    "label": 0
                },
                {
                    "sent": "F of X JK and then I. I'm going to note that by that OK and so the features here, just to be concrete, might the so the relative position of these two words.",
                    "label": 0
                },
                {
                    "sent": "So if you talk about languages are pretty close in the word order, things tend to be don't move too far orthography.",
                    "label": 0
                },
                {
                    "sent": "If these are again languages are closed, then say a string at a distance between these two things might give you some clue Association, which is, you know you might be able to have a bunch of unlabeled data where you can kind of say what's the mutual information between occurrence of this word in this word.",
                    "label": 0
                },
                {
                    "sent": "OK, so that might be a feature that you got from unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So there's a whole bunch of features in your way.",
                    "label": 0
                },
                {
                    "sent": "You're learning, trying to learn the weights that give you the right.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Payments.",
                    "label": 0
                },
                {
                    "sent": "OK, and then finally I guess this is the number part that matching model you have your season.",
                    "label": 0
                },
                {
                    "sent": "Then the scores might depend.",
                    "label": 0
                },
                {
                    "sent": "Essentially.",
                    "label": 0
                },
                {
                    "sent": "I mean, what we've done is looked at a window around a window.",
                    "label": 0
                },
                {
                    "sent": "Amino acids around there and then the window analysis around there and we're learning a function which is you can think of an approximate energy function for matching this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, this is some of the.",
                    "label": 0
                },
                {
                    "sent": "Features that we use, right?",
                    "label": 0
                },
                {
                    "sent": "So we have for matching system four and six.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of the appropriate part of the input that we're considering, and then we have some sort of feature mapping on this input, which looks at amino acid entities it looks at.",
                    "label": 0
                },
                {
                    "sent": "Basically, physical chemical properties of those things, whether they like water, where they don't like water etc etc.",
                    "label": 0
                },
                {
                    "sent": "OK, the point is, in all of these models the score decomposes into parts whether there are edges productions, pairs of consecutive labels, etc.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is.",
                    "label": 0
                },
                {
                    "sent": "This further assumption is that their score is not only linear, but it's also decomposed into parts.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to get back here, you thinking, hearing decomposing sort of corresponding.",
                    "label": 0
                },
                {
                    "sent": "Things in the sequence being matched mean is not the way to school.",
                    "label": 0
                },
                {
                    "sent": "Yeah so so OK, so let me just I went quickly over this one, but I'm considering OK so there's six cystines.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's going to be 3 edges if I'm looking at perfect matchings.",
                    "label": 0
                },
                {
                    "sent": "A score of each matching is going to be just double well.",
                    "label": 0
                },
                {
                    "sent": "This right?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to sum over those three, take my W, and multiply that.",
                    "label": 0
                },
                {
                    "sent": "So how am I going to compute whether for like 6?",
                    "label": 0
                },
                {
                    "sent": "It depends on the properties of the string around there OK and so.",
                    "label": 0
                },
                {
                    "sent": "You know we.",
                    "label": 0
                },
                {
                    "sent": "It's actually it is decomposed.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's we're using something there that's that's based on identity letters, but it's not.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not just it's too sparse to just take the entire thing so it basically looks at the identical letters question.",
                    "label": 0
                },
                {
                    "sent": "This page.",
                    "label": 0
                },
                {
                    "sent": "There's lots of constraints from the topology of the change, so you assume the first one.",
                    "label": 0
                },
                {
                    "sent": "You can get the second one first.",
                    "label": 0
                },
                {
                    "sent": "Things that help their system probably don't make yes exactly exactly good good.",
                    "label": 0
                },
                {
                    "sent": "Right good, so I mean there is a question of yeah, so whether you actually try to satisfy all the constraints, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're trying to solve the entire 3D3D protein folding problem, there's tons of physical constraints about these things that are very hard to do right.",
                    "label": 0
                },
                {
                    "sent": "And usually the problem of you know, given even if somebody gives you an energy function, you're not trying to learn it.",
                    "label": 0
                },
                {
                    "sent": "Finding the optimal thing, the argmax is something that takes on the order of, you know, months CPU time, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "Sorry yeah, so.",
                    "label": 0
                },
                {
                    "sent": "So what we're looking for?",
                    "label": 0
                },
                {
                    "sent": "I mean, what is this useful for?",
                    "label": 0
                },
                {
                    "sent": "I mean, right now it's kind of useful as an illustration of what kind of structures are there.",
                    "label": 0
                },
                {
                    "sent": "But this, as once we learn such a model, what it can be useful for for fast throughput kind of filtering of proteins, right?",
                    "label": 0
                },
                {
                    "sent": "So what we try to do actually with this thing is basically try to predict which systems form proteins 'cause.",
                    "label": 0
                },
                {
                    "sent": "This is very fast and then use those constraints to say.",
                    "label": 0
                },
                {
                    "sent": "Rosetta, which is one of the sort of Premier protein folding.",
                    "label": 0
                },
                {
                    "sent": "Software out there use that those constraints to help it in the search.",
                    "label": 0
                },
                {
                    "sent": "OK, so this thing is sort of a preprocessing step to trust.",
                    "label": 0
                },
                {
                    "sent": "Identify these things we can handle and we can handle the fact that it might be not a perfect matching that some of these guys actually matching to other other things.",
                    "label": 0
                },
                {
                    "sent": "This is a sub chain and smashing to another part.",
                    "label": 0
                },
                {
                    "sent": "We can handle a non perfect matching as well.",
                    "label": 0
                },
                {
                    "sent": "It's basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Additional function air conclude.",
                    "label": 0
                },
                {
                    "sent": "Sooner is not PlayStation given.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is kind of.",
                    "label": 0
                },
                {
                    "sent": "Yes, straight parameter estimation right?",
                    "label": 0
                },
                {
                    "sent": "You could OK.",
                    "label": 0
                },
                {
                    "sent": "So yes and no.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could also say for example, if I'm trying to do this with Mark of random fields or something like that.",
                    "label": 0
                },
                {
                    "sent": "I could introduce a lot of possible cliques and do selection using some kind of sparsifying prior, right?",
                    "label": 0
                },
                {
                    "sent": "If I later, when I learned my WS with you, if I put a. L2 norm I don't get something sparse for put up like L1 type normal or sparsifying norm.",
                    "label": 0
                },
                {
                    "sent": "I can get some kind of structure selection as well and we played around with that as well, but usually the structure is given.",
                    "label": 0
                },
                {
                    "sent": "I mean it's hard enough with structure given so, but it's a good question.",
                    "label": 0
                },
                {
                    "sent": "I mean really, you would really want to be doing is is doing those two things at once.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the assumption for structure models and you know the.",
                    "label": 0
                },
                {
                    "sent": "This is assumption about the score, and then there's also assumption about wise being.",
                    "label": 0
                },
                {
                    "sent": "Inter constraint, right?",
                    "label": 0
                },
                {
                    "sent": "It's not like every you can just take a bag of parts in.",
                    "label": 0
                },
                {
                    "sent": "These parts are completely independent of each other.",
                    "label": 0
                },
                {
                    "sent": "These parts yps have to somehow fit together well and you will see that happens with grammars that happens with sequences.",
                    "label": 0
                },
                {
                    "sent": "Happens with everything.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's the picture.",
                    "label": 0
                },
                {
                    "sent": "We have a model.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like that.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can think of it either as a score like this, or if you want to think about it as a probability distribution over the outputs, you just exponentiate the score.",
                    "label": 0
                },
                {
                    "sent": "It's super cute super setting.",
                    "label": 0
                },
                {
                    "sent": "We know all the apps we know all the we know what doubles we want to learn.",
                    "label": 0
                },
                {
                    "sent": "We have labeled data X&Y's so this is a matching.",
                    "label": 0
                },
                {
                    "sent": "We're going to estimate W during learning at prediction time.",
                    "label": 0
                },
                {
                    "sent": "We're going to the argmax of this probability distribution, which is the same as the argmax of the score.",
                    "label": 0
                },
                {
                    "sent": "I haven't changed anything.",
                    "label": 0
                },
                {
                    "sent": "This is sort of.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic view of this thing, and so.",
                    "label": 0
                },
                {
                    "sent": "An example of this would be I run weighted matching here.",
                    "label": 0
                },
                {
                    "sent": "More generally, I do some kind of internal optimization or dynamic programming to compute the best parse or the best ytterbium HMM or I do.",
                    "label": 0
                },
                {
                    "sent": "Turns out I do a minimum weight cut in the that.",
                    "label": 0
                },
                {
                    "sent": "Call Damon associative potentials so we know how to do this part.",
                    "label": 0
                },
                {
                    "sent": "It's some kind of computer chips is a optimization given RW.",
                    "label": 0
                },
                {
                    "sent": "So how do we learn this?",
                    "label": 0
                },
                {
                    "sent": "So one is local.",
                    "label": 0
                },
                {
                    "sent": "Just basically you learn ignoring all structure and you might actually there's two variants of this.",
                    "label": 0
                },
                {
                    "sent": "One is ignored structure during learning and then also ignore structure during inference.",
                    "label": 0
                },
                {
                    "sent": "So I mean, we're assuming the first one is totally brain dead, the second one is a potential candidate where you.",
                    "label": 0
                },
                {
                    "sent": "I'll show exactly how it works.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work that well, but it's a candidate.",
                    "label": 0
                },
                {
                    "sent": "There's likelihood so I can have my data right?",
                    "label": 0
                },
                {
                    "sent": "I can calculate what's the probability of Y given X, and they can maximize the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "The data for a lot of models like chains and parse trees.",
                    "label": 0
                },
                {
                    "sent": "It's doable for a bunch of other problems like matchings, and these sort of grid grid type models.",
                    "label": 0
                },
                {
                    "sent": "It's intractable, and then the third method is what we're advocating is basically a margin based method that's kind of generalizes the SVM hinge loss.",
                    "label": 0
                },
                {
                    "sent": "To the setting and it.",
                    "label": 0
                },
                {
                    "sent": "Your means tractable for all of the problems that I mentioned, even in places where likelihood is not.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is the pipeline.",
                    "label": 0
                },
                {
                    "sent": "This is the box we're going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what do we want?",
                    "label": 1
                },
                {
                    "sent": "Given our label data, right so this is our label, this is our input.",
                    "label": 0
                },
                {
                    "sent": "This is our label.",
                    "label": 0
                },
                {
                    "sent": "We want the argmax to give us back the right answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so we want to find WS that satisfy this condition.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a strange condition, so it helps to actually try to rewrite it in some way.",
                    "label": 0
                },
                {
                    "sent": "You know we'd like to sort of write something that's kind of, you know, there's some linear constraints and W or something nice and useful.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we could write down for every possibility of five letter words we can write those down and say the score of this guy.",
                    "label": 0
                },
                {
                    "sent": "The truth is better than the other, and so we make this list.",
                    "label": 0
                },
                {
                    "sent": "The list is going to be.",
                    "label": 0
                },
                {
                    "sent": "You know 26 letters to the power of five more generally is going to be something very very large, right?",
                    "label": 0
                },
                {
                    "sent": "But this is at least an attempt to write down what we want in some kind of declarative form.",
                    "label": 0
                },
                {
                    "sent": "That's the easier to do.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "In parsing it's very similar, right?",
                    "label": 0
                },
                {
                    "sent": "So if this is a sentence that we want this, you know we just enumerate all parse trees, and there's a lot of the Catalan number of those.",
                    "label": 0
                },
                {
                    "sent": "In terms of the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The length of the string for alignment.",
                    "label": 0
                },
                {
                    "sent": "Again, you know this is the truth.",
                    "label": 0
                },
                {
                    "sent": "Say for this input and we enumerate all the rest.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "One challenge with this, in addition to the fact that there are exponentially many alternatives we need list is that the loss that we actually usually sort of measure when we compute accuracy on our prediction is not simple.",
                    "label": 0
                },
                {
                    "sent": "01 loss very often.",
                    "label": 0
                },
                {
                    "sent": "So for example in character kind of recognition you care about character error, right?",
                    "label": 0
                },
                {
                    "sent": "So how many positions you get wrong?",
                    "label": 0
                },
                {
                    "sent": "OK, so here you got two wrong here.",
                    "label": 0
                },
                {
                    "sent": "You got one wrong.",
                    "label": 0
                },
                {
                    "sent": "Other cases as well, you might.",
                    "label": 0
                },
                {
                    "sent": "You know this is correct.",
                    "label": 0
                },
                {
                    "sent": "Then you got one wrong here.",
                    "label": 0
                },
                {
                    "sent": "Here it's two wrong because you also you mislabeled this guy and also you got the structure wrong, right?",
                    "label": 0
                },
                {
                    "sent": "If you're counting productions, this is actually three wrong.",
                    "label": 0
                },
                {
                    "sent": "'cause you got the structure wrong and you mislabeled the things right, so different different candidates are scored differently with respect to error, right?",
                    "label": 0
                },
                {
                    "sent": "So for example, for parsing you know, usually people care, but it's kind of bracket recall kind of measure.",
                    "label": 0
                },
                {
                    "sent": "So how many?",
                    "label": 0
                },
                {
                    "sent": "Brackets as you get right, roughly speaking, precision recall on that right for matchings.",
                    "label": 0
                },
                {
                    "sent": "Again, it's something that like decomposes into into.",
                    "label": 0
                },
                {
                    "sent": "Into into individual parts, right?",
                    "label": 0
                },
                {
                    "sent": "So you got here, you got 1 missing.",
                    "label": 0
                },
                {
                    "sent": "That's penalty of 1 if you got 2 missing, it's that, etc etc.",
                    "label": 0
                },
                {
                    "sent": "So we have a structured loss in addition to.",
                    "label": 0
                },
                {
                    "sent": "Having experienced many things and for for us and actually for mathematical convenience, we're going to assume that the loss decomposes in the same way that the model does, and that's key for making it efficient, so.",
                    "label": 0
                },
                {
                    "sent": "You know, basically it's a Hamming kind of loss, right number of parts that are raw.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's write down some cubes.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We have training examples X&Y and this is what we want for every alternative, we want to maximize this margin gamma, right?",
                    "label": 0
                },
                {
                    "sent": "Because this is strictly greater and.",
                    "label": 0
                },
                {
                    "sent": "What we've we've done here is make the margin be weighted by the number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "So the intuition here is that you know the more wrong a particular output is, the bigger the margin should be, right?",
                    "label": 0
                },
                {
                    "sent": "So that you want to reject that, reject that possibility with sort of much higher confidence there.",
                    "label": 0
                },
                {
                    "sent": "Other sort of justifications for a more, more more reasonable justification for that is that.",
                    "label": 0
                },
                {
                    "sent": "Once you transform everything, it turns out that this is going to be an upper bound, like a convex upper bound on the structured loss, which is the number of things wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's in the same way as hinge loss upper bounds here.",
                    "label": 0
                },
                {
                    "sent": "01 error.",
                    "label": 0
                },
                {
                    "sent": "This kind of.",
                    "label": 0
                },
                {
                    "sent": "Mistake weighted margin upper bounds your.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hamming distance loss.",
                    "label": 0
                },
                {
                    "sent": "So this is a problem, right?",
                    "label": 0
                },
                {
                    "sent": "So maximizing gamma, we need to constrain W some number of W so that this thing is coherent.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you know you can just scale up everything and have a better gamma.",
                    "label": 0
                },
                {
                    "sent": "So we make this right and the usual kind of SVM thing.",
                    "label": 0
                },
                {
                    "sent": "We eliminate gamma and translate that into a minimizing of the norm of W ^2.",
                    "label": 0
                },
                {
                    "sent": "Right by standard substitution, so it looks very much like an SVM right now, as VM without Slack variables, except these constraints.",
                    "label": 0
                },
                {
                    "sent": "Now you have for each example an for each alternate.",
                    "label": 0
                },
                {
                    "sent": "For that example, you have some kind of constraint.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the constraint is that the truth beats out the alternate.",
                    "label": 0
                },
                {
                    "sent": "This runner up by.",
                    "label": 0
                },
                {
                    "sent": "I ordered the margin in the case where we cannot do this we need to add slacks the same way as we do it in this VMS we had a slack for each example.",
                    "label": 0
                },
                {
                    "sent": "OK, so say I and they're essentially allow us to violate this constraint at a penalty FC.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so now we have this problem.",
                    "label": 0
                },
                {
                    "sent": "At least we've sort of wrote down exactly what we wanted.",
                    "label": 0
                },
                {
                    "sent": "And we could potentially just do brute force enumeration of all these constraints and try to solve it.",
                    "label": 1
                },
                {
                    "sent": "So this doesn't really scale, of course, and what we tried we tried to do instead is actually do this thing exactly.",
                    "label": 0
                },
                {
                    "sent": "The same problem through what's called, so we call a min Max formulation that.",
                    "label": 0
                },
                {
                    "sent": "Is going to be.",
                    "label": 0
                },
                {
                    "sent": "Turns out and with some algebra is going to turn out to be polynomial, so polynomial in all the things you care about, the number of wise number of parts in the input.",
                    "label": 0
                },
                {
                    "sent": "So we're going to go from this kind of exponential enumeration into something that's that much nicer, and we're going to do this is by doing the following trick.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple trick.",
                    "label": 0
                },
                {
                    "sent": "Essentially, what we're saying here is that the truth beats out everybody else by this amount.",
                    "label": 0
                },
                {
                    "sent": "Everybody else.",
                    "label": 0
                },
                {
                    "sent": "For that example.",
                    "label": 0
                },
                {
                    "sent": "OK, so it must beat the Max.",
                    "label": 0
                },
                {
                    "sent": "OK, is that clear.",
                    "label": 0
                },
                {
                    "sent": "Good man.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to just take this Max.",
                    "label": 0
                },
                {
                    "sent": "So now we have something weird, right?",
                    "label": 0
                },
                {
                    "sent": "We have a minimization over continuous space and then embedded in this thing is a discrete maximization over Y.",
                    "label": 0
                },
                {
                    "sent": "Right, and then the second trick is to take this embedded Max an take go from a discrete optimization into a continuous one.",
                    "label": 0
                },
                {
                    "sent": "If we can do that without loss, we can essentially just take that.",
                    "label": 0
                },
                {
                    "sent": "Continuous optimization, plug it in and have a joint continuous optimization for both and that's exactly what we do.",
                    "label": 0
                },
                {
                    "sent": "OK, is there questions about that?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Alright, so OK good.",
                    "label": 0
                },
                {
                    "sent": "The assumptions to make this work are this structured loss.",
                    "label": 1
                },
                {
                    "sent": "Like I said, Hamming loss that it decomposes in the same way as the model, so I sum of her parts and I compare whether I got the part right.",
                    "label": 0
                },
                {
                    "sent": "So this could be 01 right basically, but it doesn't have to be one.",
                    "label": 0
                },
                {
                    "sent": "It could be something waited as well.",
                    "label": 0
                },
                {
                    "sent": "I mean certain part making mistake in a certain part might be more costly than another part, so it's something that decomposes into parts.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of weighted Hamming distance, say OK, so that's T. And then our this guy just decomposes again into this thing into parts.",
                    "label": 0
                },
                {
                    "sent": "This is the part of the score and this is the part of the.",
                    "label": 0
                },
                {
                    "sent": "Loss.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do then is this mapping from discrete to continuous.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is basically have some variable Z that roughly correspond to Y. OK, there are continuous relaxations of Y, and then Q is going to be whatever all of this sort of coefficients that affect that.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to explain what Q is, but essentially we're going to write down this this this discrete optimization thing, which usually you would solve, either would say.",
                    "label": 0
                },
                {
                    "sent": "Weighted matching organic program to do that.",
                    "label": 0
                },
                {
                    "sent": "Irby or parsing thing, etc etc.",
                    "label": 0
                },
                {
                    "sent": "We would write it down as a linear program.",
                    "label": 0
                },
                {
                    "sent": "This is not to say this is we're going to prediction right.",
                    "label": 0
                },
                {
                    "sent": "This is a tool to do the learning part and kind of get the reductions the way you do your prediction at runtime is still up to you, right?",
                    "label": 0
                },
                {
                    "sent": "So this is just a tool to actually be able to map these problems into something solvable.",
                    "label": 0
                },
                {
                    "sent": "And then once we can kind of see that these guys are equivalent, the value I mean, as long as they return the same value.",
                    "label": 0
                },
                {
                    "sent": "This in this for what Q depends basically on WF&L.",
                    "label": 0
                },
                {
                    "sent": "As long as they dismiss in value, we can just plug this thing in and go with that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I have to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of take a step back and explain how to do this argmax using linear programming for a few of these problems, OK?",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "And then we can come back and plug it in.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See what happens.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The plan is to, you know, take our whatever we were going to do with the dynamic program and now write it down in a more actually declarative way than a dynamic program you know usually is thought of.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to do that is to take a Y, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case, let's take the thing with sequences, right?",
                    "label": 0
                },
                {
                    "sent": "We take a Y and it's just, you know, five sequence, five letter sequence, and we're going to.",
                    "label": 0
                },
                {
                    "sent": "Encode that with five variables.",
                    "label": 0
                },
                {
                    "sent": "OK, well, five.",
                    "label": 0
                },
                {
                    "sent": "Actually, this kind of vector variables, these variables.",
                    "label": 0
                },
                {
                    "sent": "Your question, so these variables correspond to just kind of.",
                    "label": 0
                },
                {
                    "sent": "I have a what I really want to be is a binary variable for each of these.",
                    "label": 0
                },
                {
                    "sent": "Each of these possibilities OK, and then this corresponds to the first character.",
                    "label": 0
                },
                {
                    "sent": "This correspond to the second character, etc etc.",
                    "label": 0
                },
                {
                    "sent": "So I have this five by kind of 26 array.",
                    "label": 0
                },
                {
                    "sent": "This very sparse.",
                    "label": 0
                },
                {
                    "sent": "It only has like 55 ones, that encodes my Z.",
                    "label": 0
                },
                {
                    "sent": "My why OK?",
                    "label": 0
                },
                {
                    "sent": "The clear the other part that I need is.",
                    "label": 0
                },
                {
                    "sent": "Now encoding Sir, actually.",
                    "label": 0
                },
                {
                    "sent": "Pairs so consecutive pairs.",
                    "label": 0
                },
                {
                    "sent": "Where am I going to go with this?",
                    "label": 0
                },
                {
                    "sent": "I'm going to write down the objective function instead of just in terms of why I'm going to write it down in terms of these, because, you know, we're doing arguments over why I'm going to be doing now arguments over Z, so I want to express the objective in terms of these, so this helps me in code.",
                    "label": 0
                },
                {
                    "sent": "Why in this way next step is, you know, I'm going to take care in this sequence model of pairwise terms.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is just this in this in one matrix.",
                    "label": 0
                },
                {
                    "sent": "So the pair is a followed by B, right?",
                    "label": 0
                },
                {
                    "sent": "So there is going to be one position on in this matrix, so this is just you can think of it as an indicator variable that says first position is a in the second position is B. Alright, and then there's you know, 26 by 26 and so there's there's.",
                    "label": 0
                },
                {
                    "sent": "Four of these write an.",
                    "label": 0
                },
                {
                    "sent": "In these four, there's only four ones.",
                    "label": 0
                },
                {
                    "sent": "Turn on.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is our discrete Y.",
                    "label": 0
                },
                {
                    "sent": "This is how it Maps into this kind of the 01 variable Z. OK.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're ready to write down the optimization in terms of Z.",
                    "label": 0
                },
                {
                    "sent": "So what we do is basically take whatever it was was.",
                    "label": 0
                },
                {
                    "sent": "Whatever objective was hanging on particular YJ, we write it now in terms of CJ.",
                    "label": 0
                },
                {
                    "sent": "OK, so GJ FM said, is a position J has label M and so we take the score of you know part of the potential that talks about M and we.",
                    "label": 0
                },
                {
                    "sent": "That's the coefficient of Z maximization linear's.",
                    "label": 0
                },
                {
                    "sent": "So these are maximizing.",
                    "label": 0
                },
                {
                    "sent": "This is a constant for me.",
                    "label": 0
                },
                {
                    "sent": "Same thing with the edge variables, right?",
                    "label": 0
                },
                {
                    "sent": "So I have these edge variables that tell me Eminem.",
                    "label": 0
                },
                {
                    "sent": "And then these are.",
                    "label": 0
                },
                {
                    "sent": "This is a constant.",
                    "label": 0
                },
                {
                    "sent": "With this, when I'm doing I'm definition OK, so now my objective.",
                    "label": 0
                },
                {
                    "sent": "This objective is the same as the objective with the discrete Y.",
                    "label": 0
                },
                {
                    "sent": "But now you know I need to actually give the Z some meaning, right?",
                    "label": 0
                },
                {
                    "sent": "Otherwise, this maximization is completely arbitrary.",
                    "label": 0
                },
                {
                    "sent": "So what I want.",
                    "label": 0
                },
                {
                    "sent": "I mean these these have to be really 01.",
                    "label": 0
                },
                {
                    "sent": "If I if I write him down with 01, I have an integer program which is not what I wanted.",
                    "label": 0
                },
                {
                    "sent": "I wanted something continuous, so it turns out it's enough to do the following.",
                    "label": 0
                },
                {
                    "sent": "It's enough to say that their positive to say that to each one sums to 1, meaning that only one positions are.",
                    "label": 0
                },
                {
                    "sent": "Only one position is set in each consecutive slot OK.",
                    "label": 0
                },
                {
                    "sent": "So exactly right?",
                    "label": 0
                },
                {
                    "sent": "So basically, we're encoding this thing and then we need to ensure that the Jays and DJ case are consistent with each other, right?",
                    "label": 0
                },
                {
                    "sent": "So that they they encode the same Y, right?",
                    "label": 0
                },
                {
                    "sent": "Because there's a score on the JKS and there's a score in the JS and those need to be coordinated.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can, just, you know.",
                    "label": 0
                },
                {
                    "sent": "Maximize The edge score separately from the note score and then you don't have a consistent why you're talking about.",
                    "label": 0
                },
                {
                    "sent": "So in order to synchronize with Jake, Jake, think and Wednesdays think it's enough to do the following.",
                    "label": 0
                },
                {
                    "sent": "Turns out you just basically enforce agreement meaning, so I'm looking at particular JK and as a sum over one side I should get the sum here.",
                    "label": 0
                },
                {
                    "sent": "Basically marginalized one way.",
                    "label": 0
                },
                {
                    "sent": "And this is some of the other way I should also get that.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you think of this kind of as a pairwise representation, that pairwise representation has to agree with the unary representation.",
                    "label": 0
                },
                {
                    "sent": "So these are linear constraints on Z.",
                    "label": 0
                },
                {
                    "sent": "This is sort of linear inequality is right, so we haven't said.",
                    "label": 0
                },
                {
                    "sent": "We haven't made these discrete, we just they're still continuous.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that you can.",
                    "label": 0
                },
                {
                    "sent": "You can show that if our original network is a chain or a tree, the answer you get is always going to be integral.",
                    "label": 0
                },
                {
                    "sent": "Right, so so going from.",
                    "label": 0
                },
                {
                    "sent": "The picture of this is something like this.",
                    "label": 0
                },
                {
                    "sent": "If you have, you know this is a.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have some integer points in the lattice.",
                    "label": 0
                },
                {
                    "sent": "OK, and then I have, you know, a relaxation of that lattice.",
                    "label": 0
                },
                {
                    "sent": "So let me see if I draw this.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "We have a point here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so the points are the integer things that I'm really interested in, right?",
                    "label": 0
                },
                {
                    "sent": "So this isn't some 2 dimensional space and doesn't correspond to these exactly OK, and what I'm doing is I'm optimizing a linear function in terms of these right?",
                    "label": 0
                },
                {
                    "sent": "And the question is when I optimize function right, I'm going to end up at some corner, could end up end up at a face as well, but.",
                    "label": 0
                },
                {
                    "sent": "Let's just think about non degenerate case.",
                    "label": 0
                },
                {
                    "sent": "When I'm at the corner OK, and so if all the corners are discrete, then I'm guaranteed that I can find an optimal solution as discrete.",
                    "label": 0
                },
                {
                    "sent": "The case in the face is not a problem, why?",
                    "label": 0
                },
                {
                    "sent": "Because?",
                    "label": 0
                },
                {
                    "sent": "Well, it basically it means that either this or this one solution have the same score, so I can just pick one of them right?",
                    "label": 0
                },
                {
                    "sent": "So you can show that this this.",
                    "label": 0
                },
                {
                    "sent": "Polytope right, this kind of.",
                    "label": 0
                },
                {
                    "sent": "Set of linear constraints in the space of these makes a structure such that all of its corners are discrete.",
                    "label": 0
                },
                {
                    "sent": "So when we optimize a linear function over that space, we're always going to end up at a corner and get an interest solution.",
                    "label": 0
                },
                {
                    "sent": "So this has to do so I'll come back to that in certain graphical models.",
                    "label": 0
                },
                {
                    "sent": "Why that's easy?",
                    "label": 0
                },
                {
                    "sent": "Why that happens, but you know?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so maybe I'll come back to why that happens.",
                    "label": 0
                },
                {
                    "sent": "But this is kind of a common scenario.",
                    "label": 0
                },
                {
                    "sent": "If you can.",
                    "label": 0
                },
                {
                    "sent": "Essentially, if you can do something with the dynamic program, you can write it down as a linear in some sense is a linear as a linear program as well, and that linear program is going to give you integer solutions if the dynamic program gave you in their solutions, so this is not true if we have a more complicated network.",
                    "label": 0
                },
                {
                    "sent": "If we have an arbitrary network and no.",
                    "label": 0
                },
                {
                    "sent": "Constraints on what the potentials are right?",
                    "label": 0
                },
                {
                    "sent": "Then we cannot guarantee that this is going to give us an integer answer.",
                    "label": 0
                },
                {
                    "sent": "So what happens then if we get a fractional answer, what happens is in we're maximizing, right?",
                    "label": 0
                },
                {
                    "sent": "That means we got some Z that doesn't correspond to Y, and usually that's going to mean that this is going to be bigger than any real Y. OK, so this is going to give us an upper bound on the real way.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at this associative Markov network LP.",
                    "label": 0
                },
                {
                    "sent": "So there again, we have these variables.",
                    "label": 0
                },
                {
                    "sent": "EJ.",
                    "label": 0
                },
                {
                    "sent": "Which are you know?",
                    "label": 0
                },
                {
                    "sent": "For each position, then for GJK turns out we only need to track the ones that are on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause everything else is 0 right?",
                    "label": 0
                },
                {
                    "sent": "Well, whatever we take the log there were ones.",
                    "label": 0
                },
                {
                    "sent": "I took the log.",
                    "label": 0
                },
                {
                    "sent": "That thing was zero.",
                    "label": 0
                },
                {
                    "sent": "OK so I just have a variable unbalances.",
                    "label": 0
                },
                {
                    "sent": "Is this position in this position?",
                    "label": 0
                },
                {
                    "sent": "Are they both on?",
                    "label": 0
                },
                {
                    "sent": "OK, and then if they're both on, I get this bonus WFE plus loss.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the writing down linear objective.",
                    "label": 0
                },
                {
                    "sent": "To write down the constraints you do the thing we did before ZJ M = 1, meaning that.",
                    "label": 0
                },
                {
                    "sent": "Each position has only one label.",
                    "label": 0
                },
                {
                    "sent": "And then for the edge you do something slightly different, and that's because you assume that this weight is positive.",
                    "label": 0
                },
                {
                    "sent": "OK, if that weight is positive, what happens is as I optimize, ZJK is going to become the minimum of the tool, right?",
                    "label": 0
                },
                {
                    "sent": "Because it's trying to push against both of them is going to get stopped by the smallest of the two.",
                    "label": 0
                },
                {
                    "sent": "If they were actually integers, right 01, then the minimum is going to be just the end of the tool.",
                    "label": 0
                },
                {
                    "sent": "OK, so if everything is happy and DJ's are all integral, then DJK written down this way right?",
                    "label": 0
                },
                {
                    "sent": "With these constraints at the maximum is going to mean exactly what we wanted.",
                    "label": 0
                },
                {
                    "sent": "We wanted came to mean the end of these two things.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a set of linear constraints.",
                    "label": 0
                },
                {
                    "sent": "This is a set of.",
                    "label": 0
                },
                {
                    "sent": "This is the objective.",
                    "label": 0
                },
                {
                    "sent": "If K = 2.",
                    "label": 0
                },
                {
                    "sent": "Right, so we have just a binary problem.",
                    "label": 0
                },
                {
                    "sent": "Then you can also show that this thing has integer corners and it's not something that let me.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming that I did this right, so this is a well known fact that come into optimization.",
                    "label": 0
                },
                {
                    "sent": "It has to do with the constraint matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can show it for the other one as well.",
                    "label": 0
                },
                {
                    "sent": "There's a different condition for that matrix A.",
                    "label": 0
                },
                {
                    "sent": "It's called totally imaginal total unimodularity.",
                    "label": 0
                },
                {
                    "sent": "You know this one way to write it and that means that every if you look at this matrix A right, you basically lined all these up in a vector.",
                    "label": 0
                },
                {
                    "sent": "You have a matrix hitting that.",
                    "label": 0
                },
                {
                    "sent": "That vector and then you have some constraint.",
                    "label": 0
                },
                {
                    "sent": "Say it's less than or equal to be OK as long as this is integral and a is totally modular, meaning that every sub determinant of a.",
                    "label": 0
                },
                {
                    "sent": "Is either 1 -- 1 or 0?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is something you can verify for this matrix an for a bunch of other other ones.",
                    "label": 0
                },
                {
                    "sent": "Every step determine if this thing is that.",
                    "label": 0
                },
                {
                    "sent": "It turns out then for any objective whatever Q is, it doesn't matter the corners of this guy are going to be integral, and the way that sort of works is because at this solution right?",
                    "label": 0
                },
                {
                    "sent": "I'm going to have some of these constraints be active, so there's going to be some submatrix of that.",
                    "label": 0
                },
                {
                    "sent": "Matrix, it's called B that hits Z with equality.",
                    "label": 0
                },
                {
                    "sent": "Right, and you know if this is full rank, assuming everything is full rank, then you know it's just the universe.",
                    "label": 0
                },
                {
                    "sent": "If this is integral in the sub determinants of this are 1 -- 1 and 0, then this is going to be integral as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so that means pretty few line proof of this thing, but recognizing that a matrix is 2 totally new modular is actually a hard problem.",
                    "label": 0
                },
                {
                    "sent": "But we know that these kinds of problems, the ones that arise from.",
                    "label": 0
                },
                {
                    "sent": "Either like network flow problems or matching problems, things like that they have a special structure that that you know we can just apply known theorems and say this is true.",
                    "label": 0
                },
                {
                    "sent": "It's totally new modular an for any objective we're going to get into the answer.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That clear.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Good so yeah.",
                    "label": 0
                },
                {
                    "sent": "A good reference for that, and I think but, but.",
                    "label": 0
                },
                {
                    "sent": "Nemhauser and Woolsey.",
                    "label": 0
                },
                {
                    "sent": "Very good book that talks about about this kind of stuff and there's a few other conditions where you can guarantee integrality, but but this is by far the most the most.",
                    "label": 0
                },
                {
                    "sent": "Sort of unrestricted, OK good, so we have a way to write down the objective and the constraints and get a linear program QZ such that a is less than B.",
                    "label": 0
                },
                {
                    "sent": "That gives us an integral answer, meaning that answers.",
                    "label": 0
                },
                {
                    "sent": "Has the same score as the Why score so we.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And lost anything.",
                    "label": 0
                },
                {
                    "sent": "Same thing here.",
                    "label": 0
                },
                {
                    "sent": "Not sure how many of you do parsing so go fast to this.",
                    "label": 0
                },
                {
                    "sent": "So here you have these basic context ruleset goes to BC.",
                    "label": 0
                },
                {
                    "sent": "And then where it starts, ends and where service splits.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically B spends S2, M&C spends N + 1, two East, so there's a chart representation of this right where you basically say the span from three to five is covered by NP OK, and then so you will have variables for each span.",
                    "label": 0
                },
                {
                    "sent": "So there's going to be N squared of these variables.",
                    "label": 0
                },
                {
                    "sent": "Free span there's going to be variable an for each possible non terminal.",
                    "label": 0
                },
                {
                    "sent": "A variable that says is that span covered by this non terminal.",
                    "label": 0
                },
                {
                    "sent": "Same thing that for productions so for each.",
                    "label": 0
                },
                {
                    "sent": "Triplet actually of start, middle and end.",
                    "label": 0
                },
                {
                    "sent": "So there would be.",
                    "label": 0
                },
                {
                    "sent": "2 seven and actually 1 two and seven so.",
                    "label": 0
                },
                {
                    "sent": "What production was there right?",
                    "label": 0
                },
                {
                    "sent": "So this kind of thing ABC start so the details are not super.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Important, the idea is though you can write down the score again like this, you have two sets of variables.",
                    "label": 0
                },
                {
                    "sent": "At the root, you make sure that the entire sentence from zero to N is covered by some non terminal.",
                    "label": 0
                },
                {
                    "sent": "So some over a is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "This is going inside constraint where you say.",
                    "label": 0
                },
                {
                    "sent": "That if I have a covering S 2 E then a is going to produce something.",
                    "label": 0
                },
                {
                    "sent": "So as a sum over all possible midpoints and all possible productions are being C, This is going to be alright.",
                    "label": 0
                },
                {
                    "sent": "If this is 1 then there's going to be exactly one of these fires.",
                    "label": 0
                },
                {
                    "sent": "One way to think about it, and then the other way around if if S between S&E is covered by a, it must be that this was generated by somebody else, either from from East am so somewhere you know somewhere after M somebody generated a or somewhere before it was generated.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically this is that a is going to produce offspring and this says that a is not an orphan.",
                    "label": 0
                },
                {
                    "sent": "You know is generated by somebody, roughly speaking.",
                    "label": 0
                },
                {
                    "sent": "OK, so it turns out that these linear constraints again ensure that this linear program is going to give us integral solutions when we parse again.",
                    "label": 1
                },
                {
                    "sent": "I'm not suggesting you parse with linear programs.",
                    "label": 0
                },
                {
                    "sent": "That would be silly, so, but the point is that you can write down the.",
                    "label": 0
                },
                {
                    "sent": "The objective the constraints in this way and guarantee that the solutions are integral.",
                    "label": 0
                },
                {
                    "sent": "The other thing to note in all of these things that I was talking about the number of variables and number of constraints is linear in the number of parts, right?",
                    "label": 0
                },
                {
                    "sent": "So everything is, you know.",
                    "label": 0
                },
                {
                    "sent": "Small as compared to the number of possibilities that we should consider.",
                    "label": 0
                },
                {
                    "sent": "How we doing in time?",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "OK. Sir.",
                    "label": 0
                },
                {
                    "sent": "Euphoria sweet dreams.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "20 two o'clock OK. Taken vestalia movie.",
                    "label": 0
                },
                {
                    "sent": "OK, well known so this is.",
                    "label": 0
                },
                {
                    "sent": "The last thing actually.",
                    "label": 0
                },
                {
                    "sent": "Again, see this is a very simple one.",
                    "label": 0
                },
                {
                    "sent": "We have the JK essentially mapping to yjk.",
                    "label": 0
                },
                {
                    "sent": "There is a one to one mapping between Weizens ease their positive and then there's a degree constraint, right?",
                    "label": 0
                },
                {
                    "sent": "But at most one of the Maps and at most one of the Maps OK, and we again we don't constrain equals to one.",
                    "label": 0
                },
                {
                    "sent": "So for the.",
                    "label": 0
                },
                {
                    "sent": "And we can show that has this.",
                    "label": 0
                },
                {
                    "sent": "This A is also totally unimodular.",
                    "label": 0
                },
                {
                    "sent": "OK so by the way, the problem of the non bipartite matching.",
                    "label": 0
                },
                {
                    "sent": "So the system molecule matching there is no linear programming formulation.",
                    "label": 0
                },
                {
                    "sent": "It looks very similar right?",
                    "label": 0
                },
                {
                    "sent": "We have nodes and I want to get a matching right and I want to say constrain the degree to be equal to 1 or less than one.",
                    "label": 0
                },
                {
                    "sent": "It turns out for that problem of a non bipartite matchings.",
                    "label": 0
                },
                {
                    "sent": "The number of constraints you need to add to the linear program for it to actually give you an integral solution is exponential, or at least there is no known polynomial one.",
                    "label": 0
                },
                {
                    "sent": "You know, for the last 40 years so.",
                    "label": 0
                },
                {
                    "sent": "So that problem we're going to have to attack with some other method, right?",
                    "label": 0
                },
                {
                    "sent": "So this kind of method applies when we can write down the LP and the number of constraints in LP and number variables in LP is.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This polynomial.",
                    "label": 0
                },
                {
                    "sent": "OK, so the other piece of.",
                    "label": 0
                },
                {
                    "sent": "Piece of technology we need is is just duality, right?",
                    "label": 0
                },
                {
                    "sent": "You seem valid in the context of EMS.",
                    "label": 0
                },
                {
                    "sent": "If this is our LP.",
                    "label": 0
                },
                {
                    "sent": "In this is a primal.",
                    "label": 0
                },
                {
                    "sent": "The dual is that looks very similar.",
                    "label": 0
                },
                {
                    "sent": "Just swap C&B transpose the A and.",
                    "label": 0
                },
                {
                    "sent": "Add a Lambda right?",
                    "label": 0
                },
                {
                    "sent": "So this is a mapping.",
                    "label": 0
                },
                {
                    "sent": "I guess you flip the science too, so.",
                    "label": 0
                },
                {
                    "sent": "We we wrote all their inference problems like this.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to actually use this guy, which is.",
                    "label": 0
                },
                {
                    "sent": "So if this problem is feasible.",
                    "label": 0
                },
                {
                    "sent": "Then you know this guy is going to be bounded and so this value in this value are equal in all the problems that we've considered.",
                    "label": 0
                },
                {
                    "sent": "This set of constraints is always feasible, so we're in the good case where the strong dollar holds an we have this Max and this men are exactly equal, so we can just plug in then plug them in.",
                    "label": 0
                },
                {
                    "sent": "Interchangeably.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is our formulation right?",
                    "label": 0
                },
                {
                    "sent": "We plugged in the Max discrete Max.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do now is plug in.",
                    "label": 0
                },
                {
                    "sent": "The continuous one.",
                    "label": 0
                },
                {
                    "sent": "OK Qi, just abbreviate it.",
                    "label": 0
                },
                {
                    "sent": "It's F5W plus Li.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's taking everything that kind of hits Y and putting it into this matrix that now it's F right?",
                    "label": 0
                },
                {
                    "sent": "So there was W hitting FXY.",
                    "label": 0
                },
                {
                    "sent": "Now we can write it down as W hitting FI OK in the same thing for loss so W hits this and then it hits Z and that gives you the score and then.",
                    "label": 0
                },
                {
                    "sent": "This hits hits that and gives you the loss, right?",
                    "label": 0
                },
                {
                    "sent": "So here you see why the loss was also important to be decomposable, because now it's going to be a linear function of these right visa vector and this guy is a vector and so that's going to get you know that that computes our loss for us.",
                    "label": 0
                },
                {
                    "sent": "Biofeed reality is interchangeable, so we can just.",
                    "label": 0
                },
                {
                    "sent": "Stick in this guy, hear them in and turns out we can just move them in up there, right?",
                    "label": 0
                },
                {
                    "sent": "So basically this minimization over everything at the same time, right?",
                    "label": 0
                },
                {
                    "sent": "So the only variables that appear in this minimization appear down below, so you might as well just move them in out here.",
                    "label": 0
                },
                {
                    "sent": "So now we have a joint man over W. The slacks, parameter slacks and these lambdas, which is basically the lambdas.",
                    "label": 0
                },
                {
                    "sent": "Try to compute for us implicitly.",
                    "label": 0
                },
                {
                    "sent": "What is sort of the most violated constraint is what is the the highest scoring guy that's making us?",
                    "label": 0
                },
                {
                    "sent": "Look bad, right?",
                    "label": 0
                },
                {
                    "sent": "And all of that is sort of encoded into in this.",
                    "label": 0
                },
                {
                    "sent": "Because that's the dual of the Max.",
                    "label": 0
                },
                {
                    "sent": "Right, that's it.",
                    "label": 0
                },
                {
                    "sent": "That's the whole trick.",
                    "label": 0
                },
                {
                    "sent": "You go from discrete to continuous, take the dual and then you have a QP that has polynomial number of variables, right WS number parameters.",
                    "label": 0
                },
                {
                    "sent": "Lambda is number of roughly corresponds to the number of constraints in your formulation which is.",
                    "label": 0
                },
                {
                    "sent": "Number of parts.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This thing is polynomial.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to this transformation, you can.",
                    "label": 0
                },
                {
                    "sent": "Basically you can eliminate this guy and put it up into the objective by simple manipulation.",
                    "label": 0
                },
                {
                    "sent": "Basically this is being minimized, so this is being minimized, so the minimum what happens is this.",
                    "label": 0
                },
                {
                    "sent": "If you move this over there, right?",
                    "label": 0
                },
                {
                    "sent": "This is going to be exactly equal to that side, so we can eliminate.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the slack by just putting it like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just kind of the cleanest formulation of the problem is minimizing norm this thing?",
                    "label": 0
                },
                {
                    "sent": "Which is essentially the you know.",
                    "label": 0
                },
                {
                    "sent": "Hinge loss and constraints in Lambda.",
                    "label": 0
                },
                {
                    "sent": "That enforce the semantics that we want.",
                    "label": 0
                },
                {
                    "sent": "No question yes.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This time the.",
                    "label": 0
                },
                {
                    "sent": "Is it the same thing about Methodism?",
                    "label": 0
                },
                {
                    "sent": "But no method sense that you could, I mean.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could see you know.",
                    "label": 0
                },
                {
                    "sent": "So the Lambda rise along to 0, right?",
                    "label": 0
                },
                {
                    "sent": "So there's just going to be.",
                    "label": 0
                },
                {
                    "sent": "Or maybe they, or they're going to be potentially.",
                    "label": 0
                },
                {
                    "sent": "Actually did not necessarily going to be zero.",
                    "label": 0
                },
                {
                    "sent": "They are not the same as.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, sure.",
                    "label": 0
                },
                {
                    "sent": "So they are a little bit like the alphas in this in this VMS, but there are going to be.",
                    "label": 0
                },
                {
                    "sent": "Much less sparse.",
                    "label": 0
                },
                {
                    "sent": "But OK, so continuous.",
                    "label": 0
                },
                {
                    "sent": "So suppose there are somewhat sparse.",
                    "label": 0
                },
                {
                    "sent": "You know you could the other message.",
                    "label": 0
                },
                {
                    "sent": "You might think of doing this is to sort of, you know, solve it for at some stage, and then look for awhile.",
                    "label": 0
                },
                {
                    "sent": "That sort of is the maximum that add that into your settings, so like construction yet.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk about that this is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, at some point, yeah.",
                    "label": 0
                },
                {
                    "sent": "So so that would be OK.",
                    "label": 0
                },
                {
                    "sent": "So I have slides about that later, but that would be sort of a greedy approach where you just basically.",
                    "label": 0
                },
                {
                    "sent": "How a current W right?",
                    "label": 0
                },
                {
                    "sent": "You do the Max and you add in that constraint right?",
                    "label": 0
                },
                {
                    "sent": "So it's constrained generation.",
                    "label": 0
                },
                {
                    "sent": "You know, column generation.",
                    "label": 0
                },
                {
                    "sent": "There's lots of inferred is bundled methods is also related to that.",
                    "label": 0
                },
                {
                    "sent": "So that I mean, that's an approximate method.",
                    "label": 0
                },
                {
                    "sent": "The problem is you keep resolving this QP multiple times and people have suggested this is exactly what Thomas Hoffman and his students did, and so on.",
                    "label": 0
                },
                {
                    "sent": "And so it's a feasible method is just.",
                    "label": 0
                },
                {
                    "sent": "I think it's much slower.",
                    "label": 0
                },
                {
                    "sent": "We much slower and you know we the goal of this was to write the entire problem in polynomial form and then you have all kinds of options for how to solve it, right?",
                    "label": 0
                },
                {
                    "sent": "So when you have just constraint generation, you're sort of stuck to one approach when you just write down the whole thing explicitly and in small you can use 2nd order methods.",
                    "label": 0
                },
                {
                    "sent": "You could use, you can use whatever interior point methods to solve it fast etc etc.",
                    "label": 0
                },
                {
                    "sent": "So there is advantage of writing the entire thing declaratively.",
                    "label": 0
                },
                {
                    "sent": "As a small.",
                    "label": 0
                },
                {
                    "sent": "Program because then you can use the entire toolbox as opposed to sticking to a particular method from beginning.",
                    "label": 0
                },
                {
                    "sent": "So that's advantage and.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right, so this is it, right?",
                    "label": 0
                },
                {
                    "sent": "Basically, this form of you're kind of done right?",
                    "label": 0
                },
                {
                    "sent": "So all you have to do you have a structured problem, you write down the LP for it.",
                    "label": 0
                },
                {
                    "sent": "You take the dual.",
                    "label": 0
                },
                {
                    "sent": "You formulate this QP, you stick it into, you know, Mosaic.",
                    "label": 0
                },
                {
                    "sent": "Or simplex or whatever your favorite optimizer is, and then you're done, right?",
                    "label": 0
                },
                {
                    "sent": "You don't need to write anymore code really.",
                    "label": 0
                },
                {
                    "sent": "You just need to.",
                    "label": 0
                },
                {
                    "sent": "Deciding the feature functions right down the LP tickets tool and the problem is solved, right.",
                    "label": 0
                },
                {
                    "sent": "You know general kind of a tool box like like simplex or music that will solve structured prediction problems for you.",
                    "label": 0
                },
                {
                    "sent": "At least in principle, right?",
                    "label": 0
                },
                {
                    "sent": "Not clear.",
                    "label": 0
                },
                {
                    "sent": "So in practice, of course you want to.",
                    "label": 0
                },
                {
                    "sent": "I mean methods that actually optimize for particular structure a much better and then something that I've worked on.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot is trying to exploit the structure in this QP to do this solve these things faster than off the shelf optimizer does, but but the convenience in hand in being able to write something down declaratively and not having to write code.",
                    "label": 0
                },
                {
                    "sent": "Suppose when you're decided to work on a new problem, you're trying out different models.",
                    "label": 0
                },
                {
                    "sent": "Writing down declaratively and just having an answer pop out as much easier than writing writing procedures that solve it for your particular problem every time.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it works for Markov networks like the serious.",
                    "label": 0
                },
                {
                    "sent": "We talked about low treewidth means.",
                    "label": 0
                },
                {
                    "sent": "I mean, think of the most trees right?",
                    "label": 0
                },
                {
                    "sent": "If there are trees then we can write down the optimization and it's going to be integral.",
                    "label": 0
                },
                {
                    "sent": "Associated marketing networks.",
                    "label": 0
                },
                {
                    "sent": "It's exact for K = 2 context, free grammars, bipartite matchings, and.",
                    "label": 1
                },
                {
                    "sent": "When we have untranslated Markham network so so Michael Networks that are Santana Grid but don't have this restriction.",
                    "label": 0
                },
                {
                    "sent": "OK or they have restriction but K is greater than two then the LP relaxation in general is not going to be exact right?",
                    "label": 0
                },
                {
                    "sent": "It's actually doing a Max so it's going to be bigger than the true one.",
                    "label": 0
                },
                {
                    "sent": "And this one actually interesting open questions is what can you see in that?",
                    "label": 0
                },
                {
                    "sent": "In that case, suppose for emens with K equals greater than two.",
                    "label": 0
                },
                {
                    "sent": "We haven't actually a constant factor guarantee that the inference produces something that's factored.",
                    "label": 0
                },
                {
                    "sent": "Two of the optimal.",
                    "label": 0
                },
                {
                    "sent": "Does it translate into does that translate into a guarantee and learning and actually turns out it doesn't?",
                    "label": 0
                },
                {
                    "sent": "But, well, certain practice that it works on.",
                    "label": 0
                },
                {
                    "sent": "A whole range of problems.",
                    "label": 0
                },
                {
                    "sent": "It worked remarkably well to deal with these approximate formulations.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's just unpack this a little bit what's going on so?",
                    "label": 0
                },
                {
                    "sent": "This is our unfactored primal right enumeration of all the constraints.",
                    "label": 0
                },
                {
                    "sent": "If we take the dual right this is.",
                    "label": 0
                },
                {
                    "sent": "It looks a lot like SVM's.",
                    "label": 0
                },
                {
                    "sent": "Just there weird things going on with the loss function in this VM is 01 S. This doesn't sort of pop out this way, but if you squint it kind of looks like a VM, right?",
                    "label": 0
                },
                {
                    "sent": "You have some of alphas and then you have alphas hitting your support vectors here.",
                    "label": 0
                },
                {
                    "sent": "The support vectors are not just.",
                    "label": 0
                },
                {
                    "sent": "F so I'm suppressing X right?",
                    "label": 0
                },
                {
                    "sent": "So I'm not sure when I started doing that.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah here.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm suppressing dependence on X just to make it more.",
                    "label": 0
                },
                {
                    "sent": "Concise notation.",
                    "label": 0
                },
                {
                    "sent": "OK so but FIY means F of XY.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is our support vectors.",
                    "label": 0
                },
                {
                    "sent": "These are the support vectors are the differences between the true labeling and the alternative right?",
                    "label": 0
                },
                {
                    "sent": "And it's not surprising 'cause those were there are constraints.",
                    "label": 0
                },
                {
                    "sent": "So and then this thing comes to see.",
                    "label": 0
                },
                {
                    "sent": "That's because of our slack variable.",
                    "label": 0
                },
                {
                    "sent": "OK, so this site has expansion constrains.",
                    "label": 0
                },
                {
                    "sent": "This one has exponentially many variables, but you can see from here that it's you know Colonel Isable.",
                    "label": 1
                },
                {
                    "sent": "Everything is sort of nice in that sense.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Now what have?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Opens in the factored primal dual when we actually, you know, gone to the trouble of writing down this this LP.",
                    "label": 0
                },
                {
                    "sent": "So this is the factored one.",
                    "label": 0
                },
                {
                    "sent": "If you take the dual, you kind of looks like this.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the question is OK, so those lambdas are kind of hard to interpret.",
                    "label": 0
                },
                {
                    "sent": "Those lambdas are sort of the thing you know.",
                    "label": 0
                },
                {
                    "sent": "Things that compute for you the argmax of the of the other guy of the other alternative.",
                    "label": 0
                },
                {
                    "sent": "Right here, the mus.",
                    "label": 0
                },
                {
                    "sent": "In the dual right, actually correspond to something that's fairly intuitive in terms of the original alphas, right?",
                    "label": 1
                },
                {
                    "sent": "So the alphas we had, so this is going to be evident in the second, but they essentially take the alphas which correspond to entire sequence labeling or entire parse tree, and they are kind of a summary of a whole set of alphas, OK?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular, if we look at OK, this is.",
                    "label": 0
                },
                {
                    "sent": "The factor dual this is the unfactored tool and let's look at the case of sequences.",
                    "label": 0
                },
                {
                    "sent": "So we have suppose these are support vectors, right?",
                    "label": 0
                },
                {
                    "sent": "So I mean this is the truth and these are the three guys that turned out to be the support vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so the alphas for them.",
                    "label": 0
                },
                {
                    "sent": "Let's just say our point 4.25 point one 5.2 because C = 1, right?",
                    "label": 0
                },
                {
                    "sent": "Good, so that's the solution.",
                    "label": 0
                },
                {
                    "sent": "Say to that here this guy here, which is the exponential thing.",
                    "label": 0
                },
                {
                    "sent": "And now if you think about what is actually going to be, if you workout with the solution is going to be in terms of Muse.",
                    "label": 0
                },
                {
                    "sent": "It turns out that Mus are going to be essentially kind of alphas marginalized.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean so?",
                    "label": 0
                },
                {
                    "sent": "There's going to be a mew.",
                    "label": 0
                },
                {
                    "sent": "MU is actually a vector is going to be new for each position and each value for that position.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is going to be 26 muse for this position.",
                    "label": 0
                },
                {
                    "sent": "It's kind of the same things as disease we had sewing 26 mus here and then the one that corresponds to B is going to value one here.",
                    "label": 0
                },
                {
                    "sent": "There's going to be in 26 of them with zero except for RNC.",
                    "label": 0
                },
                {
                    "sent": "Huawei because those are .2 of Alpha in here.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of marginalizing this down here.",
                    "label": 0
                },
                {
                    "sent": "I have point.",
                    "label": 0
                },
                {
                    "sent": "Or because it's taking this one and this one, and this is the sort of the only the only alphas that are active, right?",
                    "label": 0
                },
                {
                    "sent": "So if you think of essentially these guys is normalized counts of the alphas.",
                    "label": 0
                },
                {
                    "sent": "Weighted normalized counts of alphas.",
                    "label": 0
                },
                {
                    "sent": "That's the correspondence so you can think of this marginals of this distribution over entire sequences.",
                    "label": 0
                },
                {
                    "sent": "That clear so.",
                    "label": 0
                },
                {
                    "sent": "Miss you so so these mus in the dual are going to be exactly disease that we had.",
                    "label": 0
                },
                {
                    "sent": "Remember we we had a Z that encoded like each for each variable in its value or.",
                    "label": 0
                },
                {
                    "sent": "Each edge etc etc because we took we took the.",
                    "label": 0
                },
                {
                    "sent": "We had the Max, then we took the dual to get them in and then we took the dual here again of the resulting thing.",
                    "label": 0
                },
                {
                    "sent": "So we took the dual twice so them use, you know expand to original disease.",
                    "label": 0
                },
                {
                    "sent": "So whatever structure we had in code in the model is going to kind of pop out and amuse in his new variables.",
                    "label": 0
                },
                {
                    "sent": "And they turned out to basically correspond to essentially kind of marginalization of Alpha.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of surprising fact, but but.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's basically.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe it's not so surprising because the all of the feature functions decomposed according to disease and according to sorry according to the parts.",
                    "label": 0
                },
                {
                    "sent": "And so once you sort of.",
                    "label": 0
                },
                {
                    "sent": "Optimize over these things should be really in terms of in terms of these part variables and going through the LP potentially derive them for you in some sense, automatically right with that solution is.",
                    "label": 0
                },
                {
                    "sent": "Is that clear or is that?",
                    "label": 0
                },
                {
                    "sent": "Somewhat clear.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so you know in this exponential dual it was pretty easy.",
                    "label": 0
                },
                {
                    "sent": "You know you take the.",
                    "label": 0
                },
                {
                    "sent": "You take the this guy and we basically can kernelized kernelized this right.",
                    "label": 0
                },
                {
                    "sent": "This function could be written as a kernel.",
                    "label": 0
                },
                {
                    "sent": "Right, because the only thing you care about is just the dot product.",
                    "label": 0
                },
                {
                    "sent": "OK, so you write this out and I have this time this this time, this this time, this so that product.",
                    "label": 0
                },
                {
                    "sent": "You know you can do it for that.",
                    "label": 0
                },
                {
                    "sent": "It turns out you can do this also in the factored dual.",
                    "label": 1
                },
                {
                    "sent": "And there you're going to have kernels now that take in some part of the input some part of input on the sort of the thing you're comparing to the label and the label, and then so this is your kernel, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a way to go.",
                    "label": 0
                },
                {
                    "sent": "Kind of nonparametric.",
                    "label": 0
                },
                {
                    "sent": "You can just write down any kind of object, any sort of kernel you like there.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is so the alternatives are cheaper ways to do this right?",
                    "label": 0
                },
                {
                    "sent": "So one way to do it is we talked about Perceptron.",
                    "label": 0
                },
                {
                    "sent": "You know what a couple days ago, right?",
                    "label": 0
                },
                {
                    "sent": "Whynott perception for this and there is one of the first algorithms for restricted prediction was a perception type algorithm where we basically you have a current W. You do the argmax right so it's a little more complicated than a binary one.",
                    "label": 0
                },
                {
                    "sent": "And then what you do is basically if you made a mistake then you.",
                    "label": 0
                },
                {
                    "sent": "Tweak your weights so that you go towards F of XY.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know that's your gradient direction.",
                    "label": 0
                },
                {
                    "sent": "So the problem is with using this and that kind of works for some problems and breaks down for the others, and I think structured output is a particularly difficult case because what happens with structured output a lot of times you have very few instances, but each instance is big.",
                    "label": 0
                },
                {
                    "sent": "So for example in the labeling labeling 3D point clouds we might have actually one or two instances, so each instance is a scene, right?",
                    "label": 0
                },
                {
                    "sent": "Or we're doing these matchings?",
                    "label": 0
                },
                {
                    "sent": "You know we only have 10 or 15 of them.",
                    "label": 0
                },
                {
                    "sent": "There are lots of little parts in them, but the number of instances is small.",
                    "label": 0
                },
                {
                    "sent": "And so when you do the argmax and you do the update, these updates are pretty large and you know it's very sort of noisy and this is a.",
                    "label": 0
                },
                {
                    "sent": "Plot of this thing.",
                    "label": 0
                },
                {
                    "sent": "Perceptron versus sort of a different method of ours that we were doing, and so it's a very, very noisy kind of.",
                    "label": 0
                },
                {
                    "sent": "Isolation and you know the ways to deal with that and it's you could basically do average perceptron, which is your on your perceptron and then you take all of the weights that you've seen all the distinct weight vectors and you average them together and that helps you reduce the variance and regularize things.",
                    "label": 0
                },
                {
                    "sent": "It still doesn't do quite as well as sort of doing the QP, but it's a good cheap alternative if you really need to scale up to very large problems.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the other the other methods is what you are suggesting is basically add the most violated constraints, so you have you do the argmax, and you add it.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The advantages of that is you might be able to do the argmax and handle more general loss function.",
                    "label": 1
                },
                {
                    "sent": "If you can do this, argmax with some other function that maybe doesn't decompose linearly, but you can still do the argmax somehow nicely.",
                    "label": 0
                },
                {
                    "sent": "You can add that constraint, then people have used this to say, do something like.",
                    "label": 0
                },
                {
                    "sent": "F1 here, as opposed to Hamming loss.",
                    "label": 0
                },
                {
                    "sent": "So you can show actually this is work by.",
                    "label": 1
                },
                {
                    "sent": "Took a TARDIS at all, so the number of constraints you need to add in order to solve this to epsilon is actually polynomial.",
                    "label": 0
                },
                {
                    "sent": "And this has to do with the fact that you know we have an Oracle that sort of polynomial time Oracle that gives us the optimal violated constraint.",
                    "label": 0
                },
                {
                    "sent": "So whenever you have a situation where you can get the most valid constraint.",
                    "label": 0
                },
                {
                    "sent": "With an Oracle that Oracle is argmax, then you can get these sort of polynomial solution time.",
                    "label": 0
                },
                {
                    "sent": "The problem is that you know if you look at the worst case number of constraints, it's going much larger than what we have and also your resolving.",
                    "label": 1
                },
                {
                    "sent": "This could be many times, so it's a nice result but but it also gives this kind of polynomial solution, but but at least in the in terms of guarantees guarantees it's a much more expensive one.",
                    "label": 0
                },
                {
                    "sent": "In practice, it's a very very useful tool as well.",
                    "label": 0
                },
                {
                    "sent": "So then there's actually software for doing this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So, so it's a good.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alternative to so.",
                    "label": 0
                },
                {
                    "sent": "Time wise, but there's some results.",
                    "label": 0
                },
                {
                    "sent": "This is some early stuff that we did with the CRF's.",
                    "label": 0
                },
                {
                    "sent": "You know doing something that's local, predicting each each letter independently.",
                    "label": 0
                },
                {
                    "sent": "Doing a CRF where we optimize log likelihood when things are connected doing what we call em cube Nets.",
                    "label": 0
                },
                {
                    "sent": "Which is this Max margin Markov network.",
                    "label": 0
                },
                {
                    "sent": "Basically the same features but just different loss function.",
                    "label": 0
                },
                {
                    "sent": "Turns out that it helped.",
                    "label": 0
                },
                {
                    "sent": "And then we looked at sort of adding kernels, right?",
                    "label": 0
                },
                {
                    "sent": "So adding?",
                    "label": 0
                },
                {
                    "sent": "Quadratic and cubic kernels on the pixels.",
                    "label": 0
                },
                {
                    "sent": "Up to Lawton for CRF's.",
                    "label": 0
                },
                {
                    "sent": "It's actually kind of hard to do this with serious because the nice thing about using kernels with a with a hinge loss is that the solutions are fairly sparse with CRF, so you can kernelized CRF's as well.",
                    "label": 0
                },
                {
                    "sent": "But the solutions are not sparse, and so you basically kind of memorize the entire data set.",
                    "label": 0
                },
                {
                    "sent": "So we could actually do that.",
                    "label": 0
                },
                {
                    "sent": "This is the result for that.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to get to the other formulation, so I'm going to go a little fast on these, so this is a text classification problem.",
                    "label": 0
                },
                {
                    "sent": "But exhibition problem we have sort of web pages and the web pages are connected by hyperlinks, so we do independent classification.",
                    "label": 0
                },
                {
                    "sent": "We do kind of a market random field on that using likelihood an using M cube Nets, so this is sort of the comparison of those and.",
                    "label": 0
                },
                {
                    "sent": "Again, here we were using was this linear relaxation that's not tight because the model is unconstrained.",
                    "label": 0
                },
                {
                    "sent": "I mean there's four class or five classes.",
                    "label": 0
                },
                {
                    "sent": "How many pages and links the LP can give you fractional solutions, but in practice, what happens is what happens.",
                    "label": 0
                },
                {
                    "sent": "It works fine, works better than our previous results in this data set that we've.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Send OK, so I wanted to show so this part.",
                    "label": 0
                },
                {
                    "sent": "This is the the point.",
                    "label": 0
                },
                {
                    "sent": "Clouds is collected by this robot that roamed around Stanford and collected 3 point clouds.",
                    "label": 0
                },
                {
                    "sent": "So there's about 3 million points we labeled by hand some small sub.",
                    "label": 1
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set of those and.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, this is what I showed before flat.",
                    "label": 0
                },
                {
                    "sent": "This is an attempt to do this kind of voting in order to smooth things out.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It works, but not doesn't work as well as.",
                    "label": 0
                },
                {
                    "sent": "Doing the full MQ net.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is the accuracies showing example of what it actually looks like.",
                    "label": 0
                },
                {
                    "sent": "If it works.",
                    "label": 0
                },
                {
                    "sent": "No it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Shoot OK, I don't know something is, so there's a fly through of the thing.",
                    "label": 0
                },
                {
                    "sent": "But now here we go.",
                    "label": 0
                },
                {
                    "sent": "So this is what the data set looks like.",
                    "label": 0
                },
                {
                    "sent": "So you know again, there's tons of sort of sparsity issues with points sampled.",
                    "label": 0
                },
                {
                    "sent": "Nonuniformly everywhere, and so this is a result of income net.",
                    "label": 0
                },
                {
                    "sent": "On this where you have.",
                    "label": 0
                },
                {
                    "sent": "Local classifieds local features as well as points connected to its nearest nearest couple of neighbors.",
                    "label": 0
                },
                {
                    "sent": "Nearest couple of neighbors in the kind of vertical thing and it works fairly well.",
                    "label": 0
                },
                {
                    "sent": "The places where it doesn't work is where you have really long range dependency, so if you look at those poms over there some of the problems that gets right.",
                    "label": 0
                },
                {
                    "sent": "Basically by propagating the information about the Crown of the palm down to the the all the way down to the bottom of the tree.",
                    "label": 0
                },
                {
                    "sent": "And some of them sort of the trunk wins and it thinks that.",
                    "label": 0
                },
                {
                    "sent": "That palm is is a building.",
                    "label": 0
                },
                {
                    "sent": "I mean this is Stanford campus.",
                    "label": 0
                },
                {
                    "sent": "A lot of columns look like palms etc.",
                    "label": 0
                },
                {
                    "sent": "But you know this is a limitation of course of a again a fairly local model.",
                    "label": 0
                },
                {
                    "sent": "I mean the model is MRF that only looks at nearest neighbors and.",
                    "label": 0
                },
                {
                    "sent": "And when you have very very long structures, things can kind of go either way.",
                    "label": 0
                },
                {
                    "sent": "So that's that I can.",
                    "label": 0
                },
                {
                    "sent": "Explain how we did this.",
                    "label": 0
                },
                {
                    "sent": "If we have time, you can't run an LP in this thing to solve it, and so it's using a minimum weight cut algorithm to do this efficiently so.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, word alignment results.",
                    "label": 0
                },
                {
                    "sent": "So this is supervised setting.",
                    "label": 0
                },
                {
                    "sent": "We have train 100 sentences tests in 350 and then we compared several methods.",
                    "label": 0
                },
                {
                    "sent": "Here again local learning.",
                    "label": 0
                },
                {
                    "sent": "So this is what I meant by this kind of 2nd approach where you say I'm going to learn for each edge whether it's present or not, ignoring the structure.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to take the output of that and use it as my edge score so it doesn't work that well as opposed to sort of doing the global learning and global prediction.",
                    "label": 0
                },
                {
                    "sent": "But then we also compared it to something that's basically uses a lot more data, but in an unsupervised way.",
                    "label": 0
                },
                {
                    "sent": "In that both both of these approaches.",
                    "label": 0
                },
                {
                    "sent": "This approach and this is a standard software that basically uses a lot of unsupervised data.",
                    "label": 0
                },
                {
                    "sent": "An EM to get get this accuracy.",
                    "label": 0
                },
                {
                    "sent": "If we use this predictions of this guy is a feature, then you know we can look again look at local learning.",
                    "label": 0
                },
                {
                    "sent": "In our approach, we still kind of beat it and then the other model that we tried sort of after that is is a model that actually tries to not just have a score on each edge but also have scores on pairs of edges.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can model things that you have.",
                    "label": 0
                },
                {
                    "sent": "Consecutive words mapping two consecutive words over here.",
                    "label": 0
                },
                {
                    "sent": "In that case, what happens is it's no longer a tractable problem, it's actually it's a quadratic assignment problem, which is as hard as traveling salesman.",
                    "label": 0
                },
                {
                    "sent": "So again, we use this linear relaxations an empirically.",
                    "label": 0
                },
                {
                    "sent": "It works really well even now there are no guarantees on how well it does.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it turns out actually.",
                    "label": 0
                }
            ]
        }
    }
}