{
    "id": "xnmzxsiyvi5l25boksxqgg6phvselov5",
    "title": "Experimental Design for Efficient Identification of Gene Regulatory Networks using Sparse Bayesian Models",
    "info": {
        "author": [
            "Matthias W. Seeger, Laboratory for Probabilistic Machine Learning, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"
        ],
        "published": "April 4, 2007",
        "recorded": "March 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pesb07_seeger_ilg/",
    "segmentation": [
        [
            "So thanks for the intro.",
            "This is actually joined work with two of my colleagues, flowage tanker and Koji Suda, who are both here.",
            "Doesn't work."
        ],
        [
            "So I think that could help you be a better transition to my talk then the last question by Dirk who's my about how do we actually encode sparsity?",
            "If we know this is important property for any problem, let's say 4G network identification.",
            "So this is exactly what this work is about.",
            "We're not doing this by using latent variables, but we actually using the other idea by using a specific prior for doing this, I would be actually very interested in combining these two things and then see what you get out of this.",
            "But this is actually the opposite idea.",
            "You want to have a.",
            "Prior, which directly enforces sparsity is the outline of my talk.",
            "I'm just going to talk about the problem, and I'm going to talk about the assumptions that people do, usually to approach this problem.",
            "To actually get measurements, and then it's clearly motivated that you could model these measurements by our standard linear model.",
            "Then we are going to talk about the sparse Bayesian linear model, which enforces sparsity directly on the para meters.",
            "You want to infer, and I'm going to talk about approximate inference for this model, and I'm going to clearly motivate why we're doing all this thing because we're interested in experimental design, which would probably require you to have a Bayesian model in the 1st place.",
            "So in this case, Bayesian inference is really clearly motivated.",
            "By the problem and I'm showing you some experiments there in silico again, and I'm talking about some related work in conclusions so."
        ],
        [
            "Problem is that.",
            "Genes can actually regulate other genes.",
            "This is a little bit of a bird eye view because I'm not a biologist, so forgive me if something is not completely accurate here.",
            "So proteins that come from some gene A can be actually be a transcription factor in the sense that if they're actually there they can up or down, regulate the transcription of another gene B and that would actually mean that we have a causal link in the gene regulatory network that we would want to infer.",
            "Now we have to measure something and we should ask about what.",
            "Can we actually measure affordably in the moment and one of the most maybe affordable things are microarrays which measure the amount of memory in a in a sample, which is then directly related to the expression level at the time point of the given gene.",
            "Now, if you want to go from these measurements to identify a network, then there are several approaches for doing that, and the approach that we choose is the systems identification approach, which is not just looking at the network for a long time, not doing anything, but it's interventionist so you actually disturb the system.",
            "You change the system, you disturb it without breaking it without knocking things out.",
            "In this case, and then you try to look at that disturb system again and you compare the measurements you get from the undisturbed system with the ones you get from the disturbed system.",
            "And from these changes, you're trying to infer back characteristics of the system.",
            "In our case, the structure, the connectivity structure in the system.",
            "This is a classical approach in systems identification and so within this framework we actually very much interested in optimal experimental design, and this is just really simply within a given model that you have agreed upon.",
            "No question about that anymore.",
            "So within this model you can then ask the question about what is the shortest sequence of possibly experiments.",
            "You could do all the experiments about cost you the same amount of money to do, which actually leads to identification of the para meters you're interested in.",
            "And this seems like a very ambitious thing to do, but within a model that you've agreed upon.",
            "You can actually do this."
        ],
        [
            "So this is the framework that we are working in its linearized ODI model.",
            "So you start by saying that OK, the gene.",
            "The gene expression levels at time TXT in their model by a ordinary differential equation here, and so you have some nonlinear model.",
            "And you have some.",
            "I mean, it's a stochastic differential equations.",
            "You have some noise here and then you wait.",
            "In our case it's a steady state approach.",
            "You wait until you the system settles down into a steady state X nought, and this is the undisturbed system and now you make this key assumption that you linearize the system around.",
            "This given steady state and we also transforming our variables XT so that they are now differences from this steady state for simplicity.",
            "So this is the system matrix, which is just the Jacobian at this steady state.",
            "And this thing we're interested in learning, and from that we want to infer back the network structure.",
            "So we do this again by disturbing the system and disturbing in this case means that we apply an external disturbance, which in an experiment would for example be the down regulation by external means of a certain number of genes in this system.",
            "By so experimentally you can do this by interference Erin A or gene switches, and why this is not very accurate.",
            "In the moment.",
            "I mean we are certain that there will be lots of things happening in the next years because this is an important approach that people really have to find the experiment for doing these approaches.",
            "And then you actually looking at the disturbed linearized system here.",
            "And then you're waiting.",
            "Now this system, because there's an external disturbance here, which is not zero.",
            "It's going to settle into a new steady state X star.",
            "And this is the thing we are interested in measuring.",
            "So we have a data pair used which is the constant disturbance.",
            "We apply an extra which is the new steady state, and now the whole set up, with the linearisation with the system matrix.",
            "A clearly motivates to model these measurements.",
            "Every experiment gives me a pair of these guys to model them within the simple linear model that's very very classical in statistics, so you simply say that you star so they disturbance.",
            "Actually given as a linear mapping of the X star.",
            "Your new steady state measurement.",
            "And then there's some noise which in this case we assume to be uncorrelated Gaussian.",
            "So all this all this setup is innocence classical in the sense that, uh, quite a few papers have been written which exactly use the same assumptions and talk about them later, whether they actually realistic or not."
        ],
        [
            "Now within this setup, we are actually interested in experimental design and this is really nice because it's a very clear motivation for doing Bayesian inference in this whole system.",
            "OK, let me just tell you why that is a clear motivation for doing base in infants.",
            "I mean, first of all, these disturbance experiments are really, really expensive.",
            "There fairly recently, one only has the possibility of instead of just knocking out a gene completely to down, regulate it by a fairly small amount.",
            "So we need.",
            "This is a fairly small disturbance of a system and the other thing is quite crucial.",
            "We have to actually quantitatively know by how much we actually down regulate these jeans.",
            "You cannot just say or we down regulate them.",
            "I don't know by how much we have to sort of know by how much we actually down regulate them.",
            "So given that these guys are expensive, we should really not do more than we really need.",
            "So they should be very informative experiments.",
            "So if you want to quantify information, there's a very classical way of doing it, but that requires you to actually quantify your belief in the para meters you want to infer in the 1st place, and then information is just a decrease in uncertainty in that belief before and after doing an experiment.",
            "You can quantify your belief by the Bayesian posterior.",
            "So again, the value of information is basically defined by Shannon.",
            "Is that sort of the difference?",
            "I'm going to talk about that more later of Q, the posterior before an Q prime after you've made a new measurement and the other difficulty in this setup is of course that you don't know the outcome of the experiment cause before you've done it in the Bayesian sense.",
            "If you don't know something, you're always going to marginalise over it.",
            "So we have now motivated Bayesian inference.",
            "We need some.",
            "We need some representation of uncertainty or belief, which is the posterior and we also need to innocence make a statement about the outcome of an experiment which which is again going to be the Bayesian posterior of the experimental outcome.",
            "So we want to do all this.",
            "We need a fast, accurate way of doing Bayesian inference for the system matrix in the linear model and really important to point out that it's not enough to just.",
            "Estimate this a matrix by kind of maximum likelihood of penalized maximum likelihood, because that would not give us an empty.",
            "Any quantification of the belief of uncertainty in a we need to do inference.",
            "Little punch line would be.",
            "Then I mean in this case it's not PESB, but it's PISV OK. Parameter inference and system biology.",
            "In all case, we don't.",
            "We need more than estimation here."
        ],
        [
            "OK, so doing Bayesian inference you crank the Bayesian handle.",
            "You take the likelihood which is Gaussian here from the linear model, multiply it with a prior and you normalize it.",
            "You have to posterior not all the textbooks tell you well, just use the Gaussian prior for a its conjugate.",
            "You don't have any trouble, but the problem here is that we want to do experimental design.",
            "So we have to make fairly good decisions about the next experiment to be done, and we have to do them very early when we've only observed very few data points.",
            "And this is especially tough in biology because in biology people always do actually.",
            "Actually many less measurements than they actually have variables to estimate.",
            "And I mean, we've had all these talks about identifiability.",
            "Remember this is the case where we are non identifiable.",
            "We know that and we're all the way through until the end.",
            "We are always non identifiable.",
            "Maximum likelihood estimator would never be identifiable until the very end here.",
            "And only the prior is going to push you into the right direction.",
            "Here the prior encodes prior knowledge that biologists basically have found out by doing a lot of experiments.",
            "So a strong prior is here really essential.",
            "In this framework, you're going to see this in the experimental outcomes.",
            "Well, one.",
            "Thing was really alluded to already by Professor Wilden, and also in the questions that we know that all biological regulatory networks are always sparsely connected.",
            "So if we really want to put a prior in here that everybody agrees upon in every Journal in every article, then it should be a sparsity prior on the Matrix A.",
            "No question about that, so we don't only need a fast, accurate inference method in the linear model, but we also need to do that for sparsity prior on the matrix A and a sparsity prior is simply simply a prior that allows some entries of A to be fairly large such that most other entries can be made very, very small.",
            "Close to 0.",
            "That's a sparsity prior.",
            "And we're using Laplacian."
        ],
        [
            "Sparsity prior.",
            "That's that's the density of the Laplace and let."
        ],
        [
            "Just motivate you that from a picture here.",
            "So suppose now we only have two para meters.",
            "I cannot really plot anymore and this is a contour plot of a prior sort of the density seen from above.",
            "OK, so these are the two para meters A1 and A2 here and now we've made a single measurement, which is this noisy kind of linear constraint here, and we multiply the density with the likelihood coming from the measurement and we get the posterior to posterior for each priors plotted down here.",
            "So this is a Gaussian prior.",
            "It's completely the contours are all spherical.",
            "And now if you if you then compute the posterior, because the likelihood is Gaussian, you get you get a Gaussian posterior as well.",
            "So notice that in this posterior we don't encode any sparsity, because we're going from the prior to the posterior and the principle mass in this is still spread around.",
            "Sparsity would mean that the mass is concentrated along at least one of these axes.",
            "That's what it means, right?",
            "And access the where one of the components is 0 if you want to have sparsity, you want to make sure that the posterior concentrates along the axis.",
            "Now this is the Laplace prior.",
            "The contours are all like these squares here.",
            "And if you now do the posterior, you see that the mass is skewed towards the closer edge in this case.",
            "So the mass is not like spread out like the Gaussian anymore, but it's skewed towards the X.",
            "So if you know sample from this you would get solutions that have many more.",
            "So entries in XA two are much closer to 0 here.",
            "Now this is a sparsity prior that's.",
            "Very well.",
            "So this is a sparsity prior like a student T. That's even more that's even sparser.",
            "The problem here is that you have concentration along the axis, but you create multiple modes and this is very hard to do for approximate inference.",
            "To approximate such a multimodal distribution.",
            "So if you want to do approximate inference, you would have a big problem here."
        ],
        [
            "I'm just going much too slow.",
            "Approximate inference is done by a specific technique that has been shown to be pretty accurate.",
            "Now this is called expectation propagation and the idea is that you iterate moment matchings here.",
            "Just don't have the time to go into that now.",
            "It's a.",
            "You"
        ],
        [
            "You basically iterate consistency equation until they don't give you any changes anymore method is also from statistical physics and in our case we have a problem by just applying this algorithm because we have many less measurements than variables.",
            "So the system is strongly underdetermined and then there are some algorithmic algorithmic problems of stability which are so severe that you can run it anymore.",
            "So we had to do some modified a little bit weaker form of expectation propagation which still gives you a good solution.",
            "The final method is then very robust, has a predictable running time, so you can actually run it even if you're not an expert, which is not the case for many Markov chain Monte Carlo methods for example, now then."
        ],
        [
            "And that we've done posterior inference, we want to do experimental design which which needs us to quantify the value of information.",
            "And this is the score we're going to use this the information gain, which roughly means how much new information you gain by adding one more experiment from Q2 Q prime, because you don't know the outcome of the experiment, you're going to average it out over the posterior.",
            "The Bayesian answer of doing things with latent variables is to marginalized them over the posterior.",
            "You can think about it as something like the best Bayesian randomized guess.",
            "Now how can we do that?",
            "In this case we can actually sample from this distribution of X star given you start by first sampling these a matrices and then just solving for X star so we can do this marginalization by sampling and we can do this very, very efficiently.",
            "So we can score very many candidates very quickly and then pick the best one that's going to be our next experiment.",
            "We're going to do."
        ],
        [
            "So the in silico setup is like that.",
            "So we sample a network which is sparse.",
            "It is a small world property.",
            "It has in this case 50 jeans.",
            "Then we have a realistic model with Hill type kinetics which is the same as done in previous work on the same problem by cholodenko tile and then we sample a pool of 200 candidates for experiments.",
            "They're all unit Norman, their sparse because in biology biological disturbance experiments you cannot simply downregulate all jeans, you might just.",
            "Be able to do that for very few of them, which was three non zeros and then we estimate the open para meters here in a moment in a dumb way.",
            "But we could also do marginal likelihood type things like like spoken by Professor Wilde."
        ],
        [
            "And then we actually.",
            "How do we decide on a network?",
            "We look at the posterior and we basically simply say what is the probability for this element to be significantly different from zero.",
            "And that's going to pop up out of ranking over the edges.",
            "And we're then going to score this ranking in an RC analysis by basically computing the area under the RC curve.",
            "Under this Cora random ranking has a very low area, so it's oh .02 that would be the so that would be the dumb thing just to pop out a random ranking and we also have to discard about 1/4 or 1 / 4 of the edges because they cannot be inferred from the linear setting.",
            "This is a weakness of the linearisation assumption.",
            "They don't.",
            "Basically their effect is nonlinear, they don't show up in the linearisation matrix."
        ],
        [
            "You don't.",
            "Don't squad him either, so this is the different methods.",
            "Here L means that we use the Laplace prior while G means that we use the Gaussian prior that all the textbooks tell you you should use because it's simple.",
            "It's a conjugate prior D is designed, so let me remind you we do one experiment at random.",
            "Then we actually update the posterior.",
            "We're going to then score all the candidates we picked, the one that gives gives us the most information bio score.",
            "We put that in.",
            "We update the posterior and we iterate that.",
            "And after each experiment we measure the the area under the RC curve and that's plotted at the Y axis here.",
            "So down here we had random guessing when we have done experiments.",
            "So now the method that we basically proposes slow plus design, which is the blue curve here, which goes up here very rapidly and after having done about 30 experiments, you're almost certain that you found a good best ranking here.",
            "Now, if you compare the designed against the random experiment, that's the red curve here.",
            "You basically would need even more experiments.",
            "Then you have genes to actually identify this network with a very high confidence.",
            "So this is the gain you get from doing experimental design in this setting.",
            "Then we have to Gauss prior which the textbooks tell you you should use even if you do design, you're doing pretty much worse than the Laplace prior.",
            "This is simply a failure of the prior.",
            "The prior doesn't encode sparsity, and you don't encode something you should really do, and I'm running out of time here, so this is basically the gap from a Gaussian tuala plus model here and then.",
            "If you do random design, random design means you don't select the experiments, you just pick the next one at random.",
            "You always do worse than in the.",
            "In the experimental design setting."
        ],
        [
            "We've also compared it to another method that has been.",
            "That's basically the most cited work on experimental design is a paper.",
            "By taking it all in Pinas, and this is our curve and it's there, so it's a huge difference.",
            "Apart from that, our method is also kind of two orders of magnitude faster, and they require a pretty unrealistic constraint on the network, be they have to have abounded in degree, which is actually not true and true networks."
        ],
        [
            "There's some related work I don't have time for.",
            "This is basically sparse Bayesian learning.",
            "Martial law means here who's done some work on this.",
            "All these people didn't do experimental design."
        ],
        [
            "Conclusions are we have proposed a very fast method which leads to very good results in the experimental design.",
            "The network sparsity is a very key prior assumption that has been alluded in the last talk as well, and we basically don't use latent variables to encode it, but we use directly Laplace prior to actually get get the improvements here.",
            "Also, experimental design leads to large savings.",
            "We could do all this not in the steady state case, but with time cost measurements as well.",
            "It's a robust, very easy to use method.",
            "You don't really need to be an expert in sampling or anything to use it, and we also release some code pretty soon, I hope.",
            "There are limitations to the whole linearized or the approach.",
            "Basically, we actually need small disturbances so that we actually this approach is still valid, but we would also like to have large disturbances because there's noise and we have to basically or signal has to be above the noise.",
            "There's also no modeling of saturation, Michaelis, Menten or whatever, so this is just a linearized system here.",
            "So the only thing you learn from the network is kind of the linear effects around at steady state.",
            "And I mean then of course it would be nice weather 1 could do Bayesian inference for nonlinear model.",
            "Maybe a simple one?"
        ],
        [
            "And many other applications of this I think, I think, even going towards dynamical models, there would be very nice applications of using this approximate way of using a Laplace prior.",
            "Maybe nonparametric methods.",
            "And there are some details in this.",
            "In this paper you can get from my home page.",
            "So thank you.",
            "Question.",
            "You need to know the size of the perturbation, but there are approximately approaches where also linearized where you don't have to know this as a preservation.",
            "Rather than looking at the derivative of.",
            "The effect of the preservation of the perturbation you look at ratios of these and that crosses out the preservation size.",
            "So called published a paper with a method like that.",
            "And it's also still a linearizations probably, so you minion.",
            "But that may leave one of the constraints there.",
            "Oh, that's interesting.",
            "You can look at how large it could be before you see deviations and 50% perturbations are still well within boundaries over.",
            "Providing good result.",
            "I mean, we've done some experiment with larger perturbation.",
            "It's not.",
            "We don't say this completely breaks it.",
            "I mean, flooring has done nice experiments.",
            "You can make the perturbations pretty large, but up from some point you basically.",
            "I mean you basically cannot identify the nonlinear parts in this network anymore.",
            "But this is interesting.",
            "I would like to know what the reference that is.",
            "Very similar method.",
            "OK, Florian may be common.",
            "Design.",
            "Then you have.",
            "I mean system is not true, right?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks for the intro.",
                    "label": 0
                },
                {
                    "sent": "This is actually joined work with two of my colleagues, flowage tanker and Koji Suda, who are both here.",
                    "label": 0
                },
                {
                    "sent": "Doesn't work.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I think that could help you be a better transition to my talk then the last question by Dirk who's my about how do we actually encode sparsity?",
                    "label": 0
                },
                {
                    "sent": "If we know this is important property for any problem, let's say 4G network identification.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly what this work is about.",
                    "label": 0
                },
                {
                    "sent": "We're not doing this by using latent variables, but we actually using the other idea by using a specific prior for doing this, I would be actually very interested in combining these two things and then see what you get out of this.",
                    "label": 0
                },
                {
                    "sent": "But this is actually the opposite idea.",
                    "label": 0
                },
                {
                    "sent": "You want to have a.",
                    "label": 0
                },
                {
                    "sent": "Prior, which directly enforces sparsity is the outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to talk about the problem, and I'm going to talk about the assumptions that people do, usually to approach this problem.",
                    "label": 0
                },
                {
                    "sent": "To actually get measurements, and then it's clearly motivated that you could model these measurements by our standard linear model.",
                    "label": 0
                },
                {
                    "sent": "Then we are going to talk about the sparse Bayesian linear model, which enforces sparsity directly on the para meters.",
                    "label": 0
                },
                {
                    "sent": "You want to infer, and I'm going to talk about approximate inference for this model, and I'm going to clearly motivate why we're doing all this thing because we're interested in experimental design, which would probably require you to have a Bayesian model in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So in this case, Bayesian inference is really clearly motivated.",
                    "label": 0
                },
                {
                    "sent": "By the problem and I'm showing you some experiments there in silico again, and I'm talking about some related work in conclusions so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem is that.",
                    "label": 0
                },
                {
                    "sent": "Genes can actually regulate other genes.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit of a bird eye view because I'm not a biologist, so forgive me if something is not completely accurate here.",
                    "label": 0
                },
                {
                    "sent": "So proteins that come from some gene A can be actually be a transcription factor in the sense that if they're actually there they can up or down, regulate the transcription of another gene B and that would actually mean that we have a causal link in the gene regulatory network that we would want to infer.",
                    "label": 1
                },
                {
                    "sent": "Now we have to measure something and we should ask about what.",
                    "label": 0
                },
                {
                    "sent": "Can we actually measure affordably in the moment and one of the most maybe affordable things are microarrays which measure the amount of memory in a in a sample, which is then directly related to the expression level at the time point of the given gene.",
                    "label": 0
                },
                {
                    "sent": "Now, if you want to go from these measurements to identify a network, then there are several approaches for doing that, and the approach that we choose is the systems identification approach, which is not just looking at the network for a long time, not doing anything, but it's interventionist so you actually disturb the system.",
                    "label": 0
                },
                {
                    "sent": "You change the system, you disturb it without breaking it without knocking things out.",
                    "label": 0
                },
                {
                    "sent": "In this case, and then you try to look at that disturb system again and you compare the measurements you get from the undisturbed system with the ones you get from the disturbed system.",
                    "label": 0
                },
                {
                    "sent": "And from these changes, you're trying to infer back characteristics of the system.",
                    "label": 0
                },
                {
                    "sent": "In our case, the structure, the connectivity structure in the system.",
                    "label": 0
                },
                {
                    "sent": "This is a classical approach in systems identification and so within this framework we actually very much interested in optimal experimental design, and this is just really simply within a given model that you have agreed upon.",
                    "label": 0
                },
                {
                    "sent": "No question about that anymore.",
                    "label": 0
                },
                {
                    "sent": "So within this model you can then ask the question about what is the shortest sequence of possibly experiments.",
                    "label": 0
                },
                {
                    "sent": "You could do all the experiments about cost you the same amount of money to do, which actually leads to identification of the para meters you're interested in.",
                    "label": 0
                },
                {
                    "sent": "And this seems like a very ambitious thing to do, but within a model that you've agreed upon.",
                    "label": 0
                },
                {
                    "sent": "You can actually do this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the framework that we are working in its linearized ODI model.",
                    "label": 0
                },
                {
                    "sent": "So you start by saying that OK, the gene.",
                    "label": 0
                },
                {
                    "sent": "The gene expression levels at time TXT in their model by a ordinary differential equation here, and so you have some nonlinear model.",
                    "label": 1
                },
                {
                    "sent": "And you have some.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a stochastic differential equations.",
                    "label": 0
                },
                {
                    "sent": "You have some noise here and then you wait.",
                    "label": 0
                },
                {
                    "sent": "In our case it's a steady state approach.",
                    "label": 0
                },
                {
                    "sent": "You wait until you the system settles down into a steady state X nought, and this is the undisturbed system and now you make this key assumption that you linearize the system around.",
                    "label": 1
                },
                {
                    "sent": "This given steady state and we also transforming our variables XT so that they are now differences from this steady state for simplicity.",
                    "label": 0
                },
                {
                    "sent": "So this is the system matrix, which is just the Jacobian at this steady state.",
                    "label": 0
                },
                {
                    "sent": "And this thing we're interested in learning, and from that we want to infer back the network structure.",
                    "label": 0
                },
                {
                    "sent": "So we do this again by disturbing the system and disturbing in this case means that we apply an external disturbance, which in an experiment would for example be the down regulation by external means of a certain number of genes in this system.",
                    "label": 0
                },
                {
                    "sent": "By so experimentally you can do this by interference Erin A or gene switches, and why this is not very accurate.",
                    "label": 0
                },
                {
                    "sent": "In the moment.",
                    "label": 0
                },
                {
                    "sent": "I mean we are certain that there will be lots of things happening in the next years because this is an important approach that people really have to find the experiment for doing these approaches.",
                    "label": 0
                },
                {
                    "sent": "And then you actually looking at the disturbed linearized system here.",
                    "label": 0
                },
                {
                    "sent": "And then you're waiting.",
                    "label": 0
                },
                {
                    "sent": "Now this system, because there's an external disturbance here, which is not zero.",
                    "label": 0
                },
                {
                    "sent": "It's going to settle into a new steady state X star.",
                    "label": 0
                },
                {
                    "sent": "And this is the thing we are interested in measuring.",
                    "label": 0
                },
                {
                    "sent": "So we have a data pair used which is the constant disturbance.",
                    "label": 0
                },
                {
                    "sent": "We apply an extra which is the new steady state, and now the whole set up, with the linearisation with the system matrix.",
                    "label": 1
                },
                {
                    "sent": "A clearly motivates to model these measurements.",
                    "label": 0
                },
                {
                    "sent": "Every experiment gives me a pair of these guys to model them within the simple linear model that's very very classical in statistics, so you simply say that you star so they disturbance.",
                    "label": 0
                },
                {
                    "sent": "Actually given as a linear mapping of the X star.",
                    "label": 0
                },
                {
                    "sent": "Your new steady state measurement.",
                    "label": 0
                },
                {
                    "sent": "And then there's some noise which in this case we assume to be uncorrelated Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So all this all this setup is innocence classical in the sense that, uh, quite a few papers have been written which exactly use the same assumptions and talk about them later, whether they actually realistic or not.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now within this setup, we are actually interested in experimental design and this is really nice because it's a very clear motivation for doing Bayesian inference in this whole system.",
                    "label": 0
                },
                {
                    "sent": "OK, let me just tell you why that is a clear motivation for doing base in infants.",
                    "label": 0
                },
                {
                    "sent": "I mean, first of all, these disturbance experiments are really, really expensive.",
                    "label": 0
                },
                {
                    "sent": "There fairly recently, one only has the possibility of instead of just knocking out a gene completely to down, regulate it by a fairly small amount.",
                    "label": 0
                },
                {
                    "sent": "So we need.",
                    "label": 0
                },
                {
                    "sent": "This is a fairly small disturbance of a system and the other thing is quite crucial.",
                    "label": 0
                },
                {
                    "sent": "We have to actually quantitatively know by how much we actually down regulate these jeans.",
                    "label": 0
                },
                {
                    "sent": "You cannot just say or we down regulate them.",
                    "label": 0
                },
                {
                    "sent": "I don't know by how much we have to sort of know by how much we actually down regulate them.",
                    "label": 0
                },
                {
                    "sent": "So given that these guys are expensive, we should really not do more than we really need.",
                    "label": 0
                },
                {
                    "sent": "So they should be very informative experiments.",
                    "label": 0
                },
                {
                    "sent": "So if you want to quantify information, there's a very classical way of doing it, but that requires you to actually quantify your belief in the para meters you want to infer in the 1st place, and then information is just a decrease in uncertainty in that belief before and after doing an experiment.",
                    "label": 0
                },
                {
                    "sent": "You can quantify your belief by the Bayesian posterior.",
                    "label": 0
                },
                {
                    "sent": "So again, the value of information is basically defined by Shannon.",
                    "label": 0
                },
                {
                    "sent": "Is that sort of the difference?",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about that more later of Q, the posterior before an Q prime after you've made a new measurement and the other difficulty in this setup is of course that you don't know the outcome of the experiment cause before you've done it in the Bayesian sense.",
                    "label": 0
                },
                {
                    "sent": "If you don't know something, you're always going to marginalise over it.",
                    "label": 0
                },
                {
                    "sent": "So we have now motivated Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "We need some.",
                    "label": 0
                },
                {
                    "sent": "We need some representation of uncertainty or belief, which is the posterior and we also need to innocence make a statement about the outcome of an experiment which which is again going to be the Bayesian posterior of the experimental outcome.",
                    "label": 0
                },
                {
                    "sent": "So we want to do all this.",
                    "label": 0
                },
                {
                    "sent": "We need a fast, accurate way of doing Bayesian inference for the system matrix in the linear model and really important to point out that it's not enough to just.",
                    "label": 1
                },
                {
                    "sent": "Estimate this a matrix by kind of maximum likelihood of penalized maximum likelihood, because that would not give us an empty.",
                    "label": 0
                },
                {
                    "sent": "Any quantification of the belief of uncertainty in a we need to do inference.",
                    "label": 0
                },
                {
                    "sent": "Little punch line would be.",
                    "label": 0
                },
                {
                    "sent": "Then I mean in this case it's not PESB, but it's PISV OK. Parameter inference and system biology.",
                    "label": 0
                },
                {
                    "sent": "In all case, we don't.",
                    "label": 0
                },
                {
                    "sent": "We need more than estimation here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so doing Bayesian inference you crank the Bayesian handle.",
                    "label": 1
                },
                {
                    "sent": "You take the likelihood which is Gaussian here from the linear model, multiply it with a prior and you normalize it.",
                    "label": 0
                },
                {
                    "sent": "You have to posterior not all the textbooks tell you well, just use the Gaussian prior for a its conjugate.",
                    "label": 0
                },
                {
                    "sent": "You don't have any trouble, but the problem here is that we want to do experimental design.",
                    "label": 0
                },
                {
                    "sent": "So we have to make fairly good decisions about the next experiment to be done, and we have to do them very early when we've only observed very few data points.",
                    "label": 0
                },
                {
                    "sent": "And this is especially tough in biology because in biology people always do actually.",
                    "label": 0
                },
                {
                    "sent": "Actually many less measurements than they actually have variables to estimate.",
                    "label": 0
                },
                {
                    "sent": "And I mean, we've had all these talks about identifiability.",
                    "label": 0
                },
                {
                    "sent": "Remember this is the case where we are non identifiable.",
                    "label": 0
                },
                {
                    "sent": "We know that and we're all the way through until the end.",
                    "label": 0
                },
                {
                    "sent": "We are always non identifiable.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood estimator would never be identifiable until the very end here.",
                    "label": 0
                },
                {
                    "sent": "And only the prior is going to push you into the right direction.",
                    "label": 0
                },
                {
                    "sent": "Here the prior encodes prior knowledge that biologists basically have found out by doing a lot of experiments.",
                    "label": 0
                },
                {
                    "sent": "So a strong prior is here really essential.",
                    "label": 0
                },
                {
                    "sent": "In this framework, you're going to see this in the experimental outcomes.",
                    "label": 0
                },
                {
                    "sent": "Well, one.",
                    "label": 0
                },
                {
                    "sent": "Thing was really alluded to already by Professor Wilden, and also in the questions that we know that all biological regulatory networks are always sparsely connected.",
                    "label": 1
                },
                {
                    "sent": "So if we really want to put a prior in here that everybody agrees upon in every Journal in every article, then it should be a sparsity prior on the Matrix A.",
                    "label": 0
                },
                {
                    "sent": "No question about that, so we don't only need a fast, accurate inference method in the linear model, but we also need to do that for sparsity prior on the matrix A and a sparsity prior is simply simply a prior that allows some entries of A to be fairly large such that most other entries can be made very, very small.",
                    "label": 1
                },
                {
                    "sent": "Close to 0.",
                    "label": 0
                },
                {
                    "sent": "That's a sparsity prior.",
                    "label": 0
                },
                {
                    "sent": "And we're using Laplacian.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sparsity prior.",
                    "label": 0
                },
                {
                    "sent": "That's that's the density of the Laplace and let.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just motivate you that from a picture here.",
                    "label": 0
                },
                {
                    "sent": "So suppose now we only have two para meters.",
                    "label": 0
                },
                {
                    "sent": "I cannot really plot anymore and this is a contour plot of a prior sort of the density seen from above.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the two para meters A1 and A2 here and now we've made a single measurement, which is this noisy kind of linear constraint here, and we multiply the density with the likelihood coming from the measurement and we get the posterior to posterior for each priors plotted down here.",
                    "label": 0
                },
                {
                    "sent": "So this is a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "It's completely the contours are all spherical.",
                    "label": 0
                },
                {
                    "sent": "And now if you if you then compute the posterior, because the likelihood is Gaussian, you get you get a Gaussian posterior as well.",
                    "label": 0
                },
                {
                    "sent": "So notice that in this posterior we don't encode any sparsity, because we're going from the prior to the posterior and the principle mass in this is still spread around.",
                    "label": 0
                },
                {
                    "sent": "Sparsity would mean that the mass is concentrated along at least one of these axes.",
                    "label": 0
                },
                {
                    "sent": "That's what it means, right?",
                    "label": 0
                },
                {
                    "sent": "And access the where one of the components is 0 if you want to have sparsity, you want to make sure that the posterior concentrates along the axis.",
                    "label": 0
                },
                {
                    "sent": "Now this is the Laplace prior.",
                    "label": 1
                },
                {
                    "sent": "The contours are all like these squares here.",
                    "label": 0
                },
                {
                    "sent": "And if you now do the posterior, you see that the mass is skewed towards the closer edge in this case.",
                    "label": 0
                },
                {
                    "sent": "So the mass is not like spread out like the Gaussian anymore, but it's skewed towards the X.",
                    "label": 0
                },
                {
                    "sent": "So if you know sample from this you would get solutions that have many more.",
                    "label": 0
                },
                {
                    "sent": "So entries in XA two are much closer to 0 here.",
                    "label": 0
                },
                {
                    "sent": "Now this is a sparsity prior that's.",
                    "label": 0
                },
                {
                    "sent": "Very well.",
                    "label": 0
                },
                {
                    "sent": "So this is a sparsity prior like a student T. That's even more that's even sparser.",
                    "label": 0
                },
                {
                    "sent": "The problem here is that you have concentration along the axis, but you create multiple modes and this is very hard to do for approximate inference.",
                    "label": 0
                },
                {
                    "sent": "To approximate such a multimodal distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do approximate inference, you would have a big problem here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just going much too slow.",
                    "label": 0
                },
                {
                    "sent": "Approximate inference is done by a specific technique that has been shown to be pretty accurate.",
                    "label": 0
                },
                {
                    "sent": "Now this is called expectation propagation and the idea is that you iterate moment matchings here.",
                    "label": 0
                },
                {
                    "sent": "Just don't have the time to go into that now.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You basically iterate consistency equation until they don't give you any changes anymore method is also from statistical physics and in our case we have a problem by just applying this algorithm because we have many less measurements than variables.",
                    "label": 0
                },
                {
                    "sent": "So the system is strongly underdetermined and then there are some algorithmic algorithmic problems of stability which are so severe that you can run it anymore.",
                    "label": 1
                },
                {
                    "sent": "So we had to do some modified a little bit weaker form of expectation propagation which still gives you a good solution.",
                    "label": 0
                },
                {
                    "sent": "The final method is then very robust, has a predictable running time, so you can actually run it even if you're not an expert, which is not the case for many Markov chain Monte Carlo methods for example, now then.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that we've done posterior inference, we want to do experimental design which which needs us to quantify the value of information.",
                    "label": 0
                },
                {
                    "sent": "And this is the score we're going to use this the information gain, which roughly means how much new information you gain by adding one more experiment from Q2 Q prime, because you don't know the outcome of the experiment, you're going to average it out over the posterior.",
                    "label": 0
                },
                {
                    "sent": "The Bayesian answer of doing things with latent variables is to marginalized them over the posterior.",
                    "label": 0
                },
                {
                    "sent": "You can think about it as something like the best Bayesian randomized guess.",
                    "label": 0
                },
                {
                    "sent": "Now how can we do that?",
                    "label": 0
                },
                {
                    "sent": "In this case we can actually sample from this distribution of X star given you start by first sampling these a matrices and then just solving for X star so we can do this marginalization by sampling and we can do this very, very efficiently.",
                    "label": 0
                },
                {
                    "sent": "So we can score very many candidates very quickly and then pick the best one that's going to be our next experiment.",
                    "label": 0
                },
                {
                    "sent": "We're going to do.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the in silico setup is like that.",
                    "label": 0
                },
                {
                    "sent": "So we sample a network which is sparse.",
                    "label": 0
                },
                {
                    "sent": "It is a small world property.",
                    "label": 0
                },
                {
                    "sent": "It has in this case 50 jeans.",
                    "label": 0
                },
                {
                    "sent": "Then we have a realistic model with Hill type kinetics which is the same as done in previous work on the same problem by cholodenko tile and then we sample a pool of 200 candidates for experiments.",
                    "label": 0
                },
                {
                    "sent": "They're all unit Norman, their sparse because in biology biological disturbance experiments you cannot simply downregulate all jeans, you might just.",
                    "label": 0
                },
                {
                    "sent": "Be able to do that for very few of them, which was three non zeros and then we estimate the open para meters here in a moment in a dumb way.",
                    "label": 0
                },
                {
                    "sent": "But we could also do marginal likelihood type things like like spoken by Professor Wilde.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we actually.",
                    "label": 0
                },
                {
                    "sent": "How do we decide on a network?",
                    "label": 0
                },
                {
                    "sent": "We look at the posterior and we basically simply say what is the probability for this element to be significantly different from zero.",
                    "label": 0
                },
                {
                    "sent": "And that's going to pop up out of ranking over the edges.",
                    "label": 0
                },
                {
                    "sent": "And we're then going to score this ranking in an RC analysis by basically computing the area under the RC curve.",
                    "label": 0
                },
                {
                    "sent": "Under this Cora random ranking has a very low area, so it's oh .02 that would be the so that would be the dumb thing just to pop out a random ranking and we also have to discard about 1/4 or 1 / 4 of the edges because they cannot be inferred from the linear setting.",
                    "label": 0
                },
                {
                    "sent": "This is a weakness of the linearisation assumption.",
                    "label": 0
                },
                {
                    "sent": "They don't.",
                    "label": 0
                },
                {
                    "sent": "Basically their effect is nonlinear, they don't show up in the linearisation matrix.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You don't.",
                    "label": 0
                },
                {
                    "sent": "Don't squad him either, so this is the different methods.",
                    "label": 0
                },
                {
                    "sent": "Here L means that we use the Laplace prior while G means that we use the Gaussian prior that all the textbooks tell you you should use because it's simple.",
                    "label": 0
                },
                {
                    "sent": "It's a conjugate prior D is designed, so let me remind you we do one experiment at random.",
                    "label": 0
                },
                {
                    "sent": "Then we actually update the posterior.",
                    "label": 0
                },
                {
                    "sent": "We're going to then score all the candidates we picked, the one that gives gives us the most information bio score.",
                    "label": 0
                },
                {
                    "sent": "We put that in.",
                    "label": 0
                },
                {
                    "sent": "We update the posterior and we iterate that.",
                    "label": 0
                },
                {
                    "sent": "And after each experiment we measure the the area under the RC curve and that's plotted at the Y axis here.",
                    "label": 0
                },
                {
                    "sent": "So down here we had random guessing when we have done experiments.",
                    "label": 0
                },
                {
                    "sent": "So now the method that we basically proposes slow plus design, which is the blue curve here, which goes up here very rapidly and after having done about 30 experiments, you're almost certain that you found a good best ranking here.",
                    "label": 0
                },
                {
                    "sent": "Now, if you compare the designed against the random experiment, that's the red curve here.",
                    "label": 0
                },
                {
                    "sent": "You basically would need even more experiments.",
                    "label": 0
                },
                {
                    "sent": "Then you have genes to actually identify this network with a very high confidence.",
                    "label": 0
                },
                {
                    "sent": "So this is the gain you get from doing experimental design in this setting.",
                    "label": 0
                },
                {
                    "sent": "Then we have to Gauss prior which the textbooks tell you you should use even if you do design, you're doing pretty much worse than the Laplace prior.",
                    "label": 0
                },
                {
                    "sent": "This is simply a failure of the prior.",
                    "label": 0
                },
                {
                    "sent": "The prior doesn't encode sparsity, and you don't encode something you should really do, and I'm running out of time here, so this is basically the gap from a Gaussian tuala plus model here and then.",
                    "label": 0
                },
                {
                    "sent": "If you do random design, random design means you don't select the experiments, you just pick the next one at random.",
                    "label": 0
                },
                {
                    "sent": "You always do worse than in the.",
                    "label": 0
                },
                {
                    "sent": "In the experimental design setting.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've also compared it to another method that has been.",
                    "label": 0
                },
                {
                    "sent": "That's basically the most cited work on experimental design is a paper.",
                    "label": 0
                },
                {
                    "sent": "By taking it all in Pinas, and this is our curve and it's there, so it's a huge difference.",
                    "label": 0
                },
                {
                    "sent": "Apart from that, our method is also kind of two orders of magnitude faster, and they require a pretty unrealistic constraint on the network, be they have to have abounded in degree, which is actually not true and true networks.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's some related work I don't have time for.",
                    "label": 0
                },
                {
                    "sent": "This is basically sparse Bayesian learning.",
                    "label": 0
                },
                {
                    "sent": "Martial law means here who's done some work on this.",
                    "label": 0
                },
                {
                    "sent": "All these people didn't do experimental design.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusions are we have proposed a very fast method which leads to very good results in the experimental design.",
                    "label": 0
                },
                {
                    "sent": "The network sparsity is a very key prior assumption that has been alluded in the last talk as well, and we basically don't use latent variables to encode it, but we use directly Laplace prior to actually get get the improvements here.",
                    "label": 1
                },
                {
                    "sent": "Also, experimental design leads to large savings.",
                    "label": 1
                },
                {
                    "sent": "We could do all this not in the steady state case, but with time cost measurements as well.",
                    "label": 0
                },
                {
                    "sent": "It's a robust, very easy to use method.",
                    "label": 0
                },
                {
                    "sent": "You don't really need to be an expert in sampling or anything to use it, and we also release some code pretty soon, I hope.",
                    "label": 0
                },
                {
                    "sent": "There are limitations to the whole linearized or the approach.",
                    "label": 0
                },
                {
                    "sent": "Basically, we actually need small disturbances so that we actually this approach is still valid, but we would also like to have large disturbances because there's noise and we have to basically or signal has to be above the noise.",
                    "label": 0
                },
                {
                    "sent": "There's also no modeling of saturation, Michaelis, Menten or whatever, so this is just a linearized system here.",
                    "label": 1
                },
                {
                    "sent": "So the only thing you learn from the network is kind of the linear effects around at steady state.",
                    "label": 0
                },
                {
                    "sent": "And I mean then of course it would be nice weather 1 could do Bayesian inference for nonlinear model.",
                    "label": 0
                },
                {
                    "sent": "Maybe a simple one?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And many other applications of this I think, I think, even going towards dynamical models, there would be very nice applications of using this approximate way of using a Laplace prior.",
                    "label": 1
                },
                {
                    "sent": "Maybe nonparametric methods.",
                    "label": 0
                },
                {
                    "sent": "And there are some details in this.",
                    "label": 0
                },
                {
                    "sent": "In this paper you can get from my home page.",
                    "label": 0
                },
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "You need to know the size of the perturbation, but there are approximately approaches where also linearized where you don't have to know this as a preservation.",
                    "label": 0
                },
                {
                    "sent": "Rather than looking at the derivative of.",
                    "label": 0
                },
                {
                    "sent": "The effect of the preservation of the perturbation you look at ratios of these and that crosses out the preservation size.",
                    "label": 0
                },
                {
                    "sent": "So called published a paper with a method like that.",
                    "label": 0
                },
                {
                    "sent": "And it's also still a linearizations probably, so you minion.",
                    "label": 0
                },
                {
                    "sent": "But that may leave one of the constraints there.",
                    "label": 0
                },
                {
                    "sent": "Oh, that's interesting.",
                    "label": 0
                },
                {
                    "sent": "You can look at how large it could be before you see deviations and 50% perturbations are still well within boundaries over.",
                    "label": 0
                },
                {
                    "sent": "Providing good result.",
                    "label": 0
                },
                {
                    "sent": "I mean, we've done some experiment with larger perturbation.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "We don't say this completely breaks it.",
                    "label": 0
                },
                {
                    "sent": "I mean, flooring has done nice experiments.",
                    "label": 0
                },
                {
                    "sent": "You can make the perturbations pretty large, but up from some point you basically.",
                    "label": 0
                },
                {
                    "sent": "I mean you basically cannot identify the nonlinear parts in this network anymore.",
                    "label": 0
                },
                {
                    "sent": "But this is interesting.",
                    "label": 0
                },
                {
                    "sent": "I would like to know what the reference that is.",
                    "label": 0
                },
                {
                    "sent": "Very similar method.",
                    "label": 0
                },
                {
                    "sent": "OK, Florian may be common.",
                    "label": 0
                },
                {
                    "sent": "Design.",
                    "label": 0
                },
                {
                    "sent": "Then you have.",
                    "label": 0
                },
                {
                    "sent": "I mean system is not true, right?",
                    "label": 0
                }
            ]
        }
    }
}