{
    "id": "dbbz6ek6y773435h3zdjngrmz5mzgoom",
    "title": "Large-Scale Semi-Supervised Learning",
    "info": {
        "author": [
            "Jason Weston, NEC Laboratories America, Inc."
        ],
        "published": "Nov. 26, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/mmdss07_weston_lsssl/",
    "segmentation": [
        [
            "So hello.",
            "So hello, I'm Jason Weston.",
            "I'm going to talk about Semi supervised learning and in particular trying to scale that towards large problems.",
            "But actually a lot of the work done in semi supervised learning isn't that large yet.",
            "I will try to discuss some ways of making that, making those algorithms faster."
        ],
        [
            "So how do I?",
            "Next page.",
            "OK, so um so supervised learning, which we've mostly talked about at this workshop, involves some examples, and a teacher who's telling you, you know, teaching you about those examples.",
            "So typically we in our frameworks we say our examples are labeled, so we have classification problems where we have a class label.",
            "We have regression, and we're also going to have discussion about structured output learning, I think tomorrow.",
            "So where you know you do things like you have a sentence in the input and the output you tried to predict a parse tree for that sentence.",
            "So it's a much more complicated label within classification.",
            "And now if you don't have a teacher and you just have examples, then that's called unsupervised learning, and it's not so clear what to do.",
            "I mean, in supervised learning, people just make a nice methodology where they say you're going to measure how good you are by measuring error rate on the test set.",
            "In unsupervised learning where you don't have labels, so people have made sort of lots of different systems.",
            "Like you know clustering and outlier detection and dimensionality reduction, and all of those things is more.",
            "Can you make an analysis that will be useful for you later on?",
            "And maybe it's not so easy to to analyze how well it's doing.",
            "I mean there are metrics but still, but the most classical one is probably density estimation.",
            "You know coming from statistics, but then a lot of machine learning people don't.",
            "Tend to use density estimation.",
            "But anyway, semi supervised learning.",
            "Which thing I'm going to talk about?",
            "Well, that's kind of somewhere sitting in between the two.",
            "You've got examples and you could say a part time teacher, so your teachers teaching you about some of your examples.",
            "But then they have the day off the rest of the time and you're on your own.",
            "You've just got unlabeled data again.",
            "So, for example, you could have a set of labeled images and a huge collection of unlabeled images.",
            "And then you want to learn.",
            "And in a typical setting.",
            "The people use is more like the supervised settings, so they measure how well they're doing using test set, which is fully labeled.",
            "So then the problem is just to try to improve your generalization ability on a classification task using unlabeled data.",
            "Yeah, I'm talking about yeah mixing using classification, but of course you could use any of these supervised learning tasks with unlabeled data.",
            "So naively you could think well, I've got my list of supervised learning tasks and my lists of unsupervised learning tasks, and I could just pick one from one list and one from the other and add them together and semi supervised learning is kind of like that, maybe naively, so for example is semi supervised learning doing classification and clustering?",
            "Or is it doing structured output learning with some dimensionality reduction?",
            "Well, I'm not really sure about actually people.",
            "There's publications of people doing all these variations and.",
            "We will."
        ],
        [
            "We'll see.",
            "So first, a word about supervised learning.",
            "So as we've seen many times before, you have I'm using, I think the same notations, most people and input X and output Y.",
            "And then you've got training data xiy I see normally I'm going to say have L training points and there's two main families of methods.",
            "Generative models really estimated joint probability and discriminative models where you're just.",
            "Interested in the conditional probability or in things like support vector machines, you're just looking at which class is the most probable.",
            "So here in a two class problem, so you sort of dispense with the probabilities and you just look at say F of X is just a.",
            "A classification label directly and you don't give an estimate of probability.",
            "In this talk.",
            "I'm only going to discuss discriminative methods and some issues about all of supervised learning.",
            "Many algorithms don't scale that well as Leon was saying in his talk, you have a lot of algorithms that use, you know matrices and they have this sort of N cubed behavior and things like that.",
            "But you could say for many problems actually a lot of labeled data doesn't even exist.",
            "You know it's expensive to obtain, so maybe in a lot of supervised learning problems you don't even need to scale that well, but it's true in some you do, but it's more clear in semi supervised learning because unlabeled data is so much easier obtain while large scale learning is more important.",
            "But perversely, so far most people haven't addressed that too."
        ],
        [
            "It is coming but I guess people are still truly trying to understand what's the best actual techniques to use in semi supervised learning and that's why they're still concentrating a lot on small, small problems.",
            "I don't know.",
            "So what is semi supervised?",
            "Learning just a bit more detail here.",
            "So now I've got my labeled training set of xiy pairs, but I also have unlabeled data, which I call XI star and.",
            "And I'm going to measure success with a labeled test set.",
            "So that's the typical setting of semi supervised learning that people use.",
            "But actually there's there's other interpretations that you could make.",
            "For example, you could.",
            "You could treat this semi supervised learning problem as a clustering problem where you're given must Lincoln must not link constraints.",
            "So that's the teacher who's giving you those constraints.",
            "So it's a bit more than clustering, and that if you do it that way, you can actually do some tasks.",
            "You can be solved that you can't solve.",
            "In the.",
            "In the setting above where you just treat as a classification problem, because for example, if you have a missing class in your in your labeled set, you don't have a class, then you could still deal with it because you're doing clustering.",
            "So for example, in protein classification, many of the classes of super family or family aren't even known yet.",
            "So if you just do classification, you'll never going to predict one of those classes, so you're in a bit of a problem.",
            "So you could treat this in a different way and another way of looking at semi supervised learning perhaps is that you could think of having your part time teacher is sometimes giving quite detailed label or teaching on some examples and sometimes much less detailed and sometimes none at all.",
            "So they have a kind of fine grained knice of label on your examples and you could see.",
            "You know some real cases like that, for example in text.",
            "Processing you could have a label that just says this is a sentence from nature or this is this is a garbage collection of words, so that would be a very sort of loose grained label or more fine grained.",
            "You could have.",
            "These are the parts of speech of the words in that sentence, so this is a noun.",
            "This is verb and then even more detail.",
            "This is the powers tree of that sentence and then even more detailed or or going from syntax to semantics.",
            "You could have some semantic labels of that sentence.",
            "So you can see sort of lots of different you know levels of labeling there and you could imagine some big training set where you have all different kind of fine grained use of labels in some completely missing and then you have to handle that problem and I think that's kind of interesting and you could see that in speech another applique."
        ],
        [
            "Oceans as well.",
            "So I've already said a little bit why.",
            "Why do you semi supervised learning?",
            "Well supervised data can be expensive to obtain in some applications.",
            "I mean, you might have to hire A labeler to to actually come and label your data.",
            "That is the teacher, human teacher, and depending on the problem, they might have to be very well trained.",
            "So, for example, labeling sentences with parse trees can't be done by just anyone.",
            "But also there's other cases where, for example, if you're trying to predict protein 3D structure, then you can't even do that just like that.",
            "I mean you have to have some biology spending a lot of time in the lab trying to find that.",
            "And they might not even be successful.",
            "Where is unlabeled data can be in some applications very cheap to collect.",
            "So particularly I mean everything that humans perceive where you know, collecting this data all the time.",
            "So we kind of can get an infinite amount of that, like audio and vision and text.",
            "But there's also lots of other natural data not.",
            "Humans perceive directly from our senses like primary protein structures, network traffic and so on, so there's a lot of places where you can get unlabeled data.",
            "So, so that's the reason why do you send me supervised learning?",
            "You can get this stuff much easier than than labeled data where another argument is that.",
            "My mouse, a true AI that mimics humans, wouldn't have a strong teaching signal because.",
            "Humans tend to, well, I think that humans tend to learn a lot from limited teaching, so probably we do semi supervised learning.",
            "So for example in natural language processing linguists label senses we parse trees, but human learn humans learn about language without these labels.",
            "Well, they don't."
        ],
        [
            "Eat them.",
            "So so that's why semi supervised learning.",
            "And now when can it work?",
            "And typically people say that, well.",
            "I mean your unlabeled data.",
            "It gives you knowledge about the distribution of X, right?",
            "P of X.",
            "So if you had infinite amount of data, you can know the density P of X, but that doesn't necessarily tell you anything about P of Y given X.",
            "That I mean in a classification problem so.",
            "Typically people say it will help if some assumptions about the distribution of the data are true.",
            "And the two typical ones that people quote accord the cluster assumption and manifold assumption by I've included another two that may or may not be true.",
            "See what you think and.",
            "Just to say this thing about making assumptions and hoping they hold true for data.",
            "Sort of used in in supervised learning as well.",
            "I mean, it's not like it's just being made up for this because in a lot of algorithms, for example, in supervised learning you assume the label doesn't change when you move a little bit in input space.",
            "Most algorithms this unit."
        ],
        [
            "So here's these different assumptions.",
            "So the cluster assumption it just says that I hope on my particular problem that examples in the same cluster have the same class.",
            "So in this in this plot here I have.",
            "I have three clusters.",
            "OK, we have oh I've got a like a laser pointer or something, right?",
            "We have three clusters and I've only got 3 label examples.",
            "There's this one of one class and these two of the other class, and so if I just train a support vector machine or something like that on just these three points, I get like a hyperplane like this if it's a linear classifier and you notice I cut through this cluster.",
            "But if I had unlabeled data and I knew this assumption was true, so I saw these images, then I probably wouldn't make this decision.",
            "Alright, I'll probably let go like this or something.",
            "So the so this cluster assumption.",
            "People also have another name for it, they call it low density separation and they say that the decision rule should lie in a region of low density.",
            "So in other words it doesn't cut through a cluster because.",
            "Examples in cluster of the same class.",
            "So then this would be a region of low density here.",
            "So again if I need P of X, yeah is P of X is high here, here and here where the clusters."
        ],
        [
            "And people also have something else called the manifold assumption, which is pretty similar.",
            "Kind of thing really.",
            "They just say examples in the same manifold have the same class so.",
            "Typically this this inspires different algorithms because what people do is they say, oh, there's some manifolds in my data and they have the same class.",
            "So I'm going to do do something where I. I do dimensionality reduction.",
            "I try to re represent this space so that the manifolds are like unfolded into something this nice and easy to separate and there's a lot of algorithms like that."
        ],
        [
            "But here's another couple of possibilities of why it could be.",
            "Could be good to do semi supervised learning so.",
            "Yes.",
            "Yes."
        ],
        [
            "Bottom bottom.",
            "Oh this one now, but this is labeled this point, right?",
            "I have two labels there, an another label there, so three labels and this whole cluster imagini Samp.",
            "I had lots of unlabeled data.",
            "Then I have lots of samples from inside this cluster.",
            "Label yeah, yeah I'm sorry company so yeah I have my training set is 3 points so I have X one X2X3 and Y one is 1 Y two is minus one Y-3 is minus one.",
            "But then I have excise star.",
            "I called it unlabeled data.",
            "If I draw lots of those when I draw them they're going to mostly be from here, here and here."
        ],
        [
            "So he is.",
            "Yeah, just another way that you can can help zips law which people talk about a lot.",
            "I think in natural language processing in a corpus of natural language utterances the frequency of any word is roughly inversely proportional to its rank in the frequency table.",
            "So they say something like.",
            "10% of the words occur 90% of the time in in normal text, so you keep seeing the all the time, for example.",
            "But then words that might really help you to do document classification.",
            "For example, you can be content to be rare words like you'll only see them in a couple of times in the document and there will be really useful for the document classification or or or or at least.",
            "Another way of saying if you have a training set of documents.",
            "Because of this distribution of words, there seems you take another document.",
            "There's probably going to be words in it that you haven't seen before, so that would be a reason why semi supervised learning could be good, because if you're always in that sort of situation where you keep finding new words well, you're never going to be able to label enough text documents to know all those words are.",
            "So you better try and use unlabeled data to understand what they are."
        ],
        [
            "And another thing which I think Leon was alluding to a little bit at the end of his talk is about non IID data.",
            "So everything I've more or less been talking about IID data here, but in real life it's not necessary necessarily like that.",
            "And let's say for example, your test set is drawn from a different distribution to your training set, then normal supervised learning with these algorithms that we use which make this IID assumption like SVM's and so on.",
            "Necessary into work very well, but if you again if you use semi supervised learning and you had access to that test set of as unlabeled data, you might be able to do better.",
            "So for example, if you if you trained a parser on the Wall Street Journal and then you apply it to Moby Dick or some piece of fiction is not necessarily going to work well.",
            "But if you'd seen that as unlabeled data, maybe you can improve your parser somehow."
        ],
        [
            "So why large scale semi supervised learning?",
            "This is a plot taken from a paper of tossed in your achenes.",
            "He's showing, I mean the whole point that people do this is that they have actually found algorithms that show unlabeled data helps for compared to, you know, just using labeled data alone, so you typically get these plots where you compare your semi supervised algorithm to a supervised algorithm.",
            "According to the number of unlabeled data in that you use an, you want the improvement.",
            "Sorry, this plot actually is about the number of labeled data.",
            "So yeah, here, the number of unlabeled data is.",
            "Fixed I think, but then the number of labeled data is changing and what he shows is so transductive SVM is an algorithm for doing semi supervised learning in SVM.",
            "Is a normal supervised classifier and when you have a small number of labeled data there's there's an improvement here in break even point on a text classification problem.",
            "So you're really utilizing this unlabeled data and getting an improvement.",
            "But as the number of examples labeled examples increases.",
            "You see that gap diminishing?",
            "With the same number of unlabeled data, so there's a problem that a lot of these algorithms so far in the literature they've shown, like results with 50 labeled training points and a few thousand unlabeled and said, oh, it works better.",
            "But when you increase the number of labeled points, because I'm not sure that's really a very realistic case, you need to increase the number of unlabeled points, like drastically.",
            "It's not like a linear relation.",
            "Because somehow an unlabeled point isn't.",
            "As informative as a labeled one is much less so, so that's a good argument why you need large scale semi supervised learning 'cause you have a massive amount of this unlabeled data in each unlabeled point is somehow not worth very much.",
            "So you have to use them all.",
            "I took this from another paper.",
            "I think it's something like a few thousand unlabeled.",
            "So like 3000 or something, but I could be wrong.",
            "Yeah.",
            "No, I'm is sorry.",
            "Say that again.",
            "Oh OK, OK, so the number of unlabeled is going down then, OK?",
            "Simon, if that answers your question so.",
            "Yeah.",
            "But still, yeah."
        ],
        [
            "The same, my argument still holds.",
            "People use that a lot in text classification.",
            "Yeah.",
            "Sorry.",
            "You don't think it's a good metric.",
            "OK, I think I mean, yeah, I mean it's better than classification error in these kind of things right?",
            "Yeah, I mean, I guess he uses it in his papers because other people use it too.",
            "But I mean I mean normally I see in these text classification problems you quote several metrics like F1 as well and RSC right?",
            "But I think you know this will hold this kind of plot will hold with other metrics.",
            "Sorry, I feel bad now introduce."
        ],
        [
            "This graph of results where I didn't really know what it was.",
            "I just wanted to make."
        ],
        [
            "Some argument about you need a lot of unlabeled data.",
            "Will we get to other experiments later?",
            "So so.",
            "So a bit more about this.",
            "Why large scale semi supervised learning.",
            "So there's loads of unlabeled data researchers.",
            "So far I've come up with slow algorithms.",
            "They are typically trained on simple problems with like 50 labeled examples and.",
            "500 or 5000.",
            "Unlabored example with speakers.",
            "A lot of these methods are too slow to run otherwise.",
            "But there's a relationship here.",
            "That's I mean, I guess this problem independent.",
            "I mean, it depends on how strong the assumption about P of X you've made is true.",
            "But this relation between a number of unlabeled data you would need to label to really make some strong improvement probably isn't linear.",
            "So for say 5000 labeled examples, you might need 500,000 unlabeled to make some improvement.",
            "But a lot of these problems that are people are measuring the success rates on a kind of simple problems.",
            "I mean, compared to really hard classification problems that humans can solve.",
            "So I would guess that for really hard high dimensional problems like Cena analysis and language understanding, then if you only have 5000 labeled examples, that's you know it's in a very high dimensional space, so it's it looks actually very sparse an it's equivalent to having like 50 labeled examples in.",
            "One of these simple problems, so again, you're back in a situation where the semi supervised learning really might help."
        ],
        [
            "I would hope.",
            "Anne.",
            "He's a bit of history of semi supervised learning, so one of the first things that people tried is called self learning, where you have a supervised learner classifier and you train it on some data and then you have some unlabeled data and you apply it to that data.",
            "So you compute these predictions and then somehow you take those predictions as true labels.",
            "Some of them maybe some of them that you're confident about and put them in the training set and.",
            "And train on those as well and there's some success with that, but it's maybe a bit heuristic.",
            "Or you know how you actually choose which examples you're going to self learn on is kind of problem, because if you know you're making mistakes and you're putting those into your training set, then you're going to train on your mistakes and not do too well.",
            "And then in the 70s Vapnik came up with Trans Duction an you know people are still using transductive SVM now.",
            "And is it you can see a lot of relationships with the self learning algorithm, but somehow it's just a little bit more principled way of doing it and I'll get to that.",
            "And then the generative model approaches, which I'm not going to talk about and Co training.",
            "Um?",
            "Which yeah, I don't know if I should mention what what it does, but 'cause these these three are actually the ones I'm going to talk about, so.",
            "People started seeing the you could see the semi supervised learning problem is regularization based.",
            "So you take your supervised learning algorithm.",
            "Your add on an extra regularizer and that tries to take into account one of these assumptions that was talking about like the cluster assumption and then you can make an algorithm from that and transductive transduction.",
            "So T SVM is kind of in that framework, but then there's other algorithms in that framework too.",
            "And then there is also these graphed based approaches.",
            "So you you well, I'm going to get to them too.",
            "You build a graph out of the actual data and then try to do some kind of diffusion normally along the graph to to spread the supervised labels to the unlabeled data.",
            "And use them that way.",
            "And then there's also kind of change of representation methods as well, which are kind of a way of doing the regularization based, but in a simpler two stage approach.",
            "So first you re represent your data in a new metric, so you have a original input space you go through and you space and you try and use the unlabeled data to do that change and then you just run a supervised classifier in that."
        ],
        [
            "Space.",
            "Typically.",
            "Pin.",
            "But yeah, a general approach for discriminative semi supervised learning.",
            "Is just to write this.",
            "This is kind of the regularization approach I guess.",
            "So in supervised learning.",
            "This is a typical way of of optimizing.",
            "You minimize the empirical loss, and then you have some regularizer Sue too.",
            "To reduce the capacity.",
            "And find a smoother function.",
            "And then you you kind of implement your SRM using this two terms and in semi supervised learning you simply stick on a third term basically which.",
            "I will capture the function will change the function that you're going to find out if your set of functions according to the unlabeled data.",
            "So then the question is, how do you?",
            "What do you choose there?",
            "And there's lots of different choices and I think people haven't found maybe the best ones yet, but you know, typically you could encode, say, the cluster assumption with that, and you could make it a kind of pointwise loss, so it decomposes to a sum over unlabeled data.",
            "So my unlabeled data here is.",
            "You unlabeled points and have L labeled points.",
            "So so T SVM looks like that."
        ],
        [
            "So yeah, I'm going to talk about these last three.",
            "They happened to be the last three in my history slide.",
            "And you can kind of fit them all.",
            "A special case cases of that slide before really, so they're all kind of regularization approaches, but they just look a little bit different when you describe them.",
            "And a good reference book for this.",
            "For this, for semi supervised learning, is this book.",
            "Here I mean really this whole field is is kind of in its infancy and that's just a collection of contributed chapters, but I don't think there is.",
            "A better book right now.",
            "So first I'm going to talk about this.",
            "Is the graph based.",
            "This is a particular instance of a graph."
        ],
        [
            "Base method called label propagation.",
            "So if you have if you have some training data, so these circles two different, there will be two different classes were actually you don't know that in the beginning these are all just unlabeled apart from the ones that I've put with little crosses.",
            "Those are the three labeled points.",
            "So then this algorithm works like this.",
            "You find the nearest neighbors of each of the points, the unlabeled and labeled points.",
            "And you build some sort of graph like this.",
            "I mean here I was just I drew it so that it was lucky that there wasn't any edges between the two manifolds actually, but that could happen.",
            "But still the algorithm could work because there's very few edges between and.",
            "Then the way this algorithm works is that these three points are going to kind of send messages to all the other points using these edges as their paths of sending message.",
            "So they kind of, I mean this called like diffusion."
        ],
        [
            "So I mean you people call it like.",
            "Pumping out a signal or energy or something like that.",
            "So like this, this labeled point says to all his neighbors I'm.",
            "I'm label.",
            "You know I'm class one and this this point says to his neighbors.",
            "I'm class minus one, and then the algorithm works that all of the nodes are actually sending those messages.",
            "So these guys are also going to say to their neighbors, so I'm mostly class one and if they receive messages from both, then they'll whichever.",
            "This signal is loudest if you like from the two messages.",
            "The hearing will be the one that they."
        ],
        [
            "Take they believe they belong to that class.",
            "So this thing kind of spreads an you can see this kind of this label class one will spread along all these neighbors and go along this manifold."
        ],
        [
            "And and then that can.",
            "Correctly, well it will take it will take into account this kind of assumption that these examples should be the same class, whereas you know again if you train."
        ],
        [
            "Something like a support vector machine on just these labeled points.",
            "Three labeled points.",
            "You would get a hyperplane cutting across here that would label this part of this manifold belonging to this class, so you get a quite different.",
            "Testera if we now easier cluster assumption or manifold assumption is true.",
            "This house."
        ],
        [
            "And it's going to be much better.",
            "And the idea of just showing actually.",
            "What that looks like, yeah.",
            "So like an SVM will give you labeling like this in this label propagation algorithm.",
            "If you're labeling like this and K nearest neighbor doesn't work."
        ],
        [
            "And just in actual the actual algorithm, I mean, I just explained it intuitively.",
            "It looks like this.",
            "You construct this matrix matrix K where KIJ will tell you about the edge between example I&J.",
            "And then each example has a vector YYYJI mean why I was the example why I and then there will be one dimension for each class in that vector and you'll set.",
            "The the that dimension to one if it if it's labeled with that class, and 0 otherwise, so an unlabeled point will just have a vector of all zeros and then you just do these updates iteratively, which is basically sending those messages on the graph they were showing.",
            "So the label of a point becomes the weighted combination of the labels of the points that are sending messages to it.",
            "If you like and then at the end you can take the argmax over the vector to say which is the label prediction for an unlabeled point.",
            "And.",
            "You can show this algorithm converges and you can.",
            "Actually you don't have to write it as an iterative algorithm.",
            "You can write it in closed form.",
            "No.",
            "No, yeah you.",
            "I mean you can, but generally people don't.",
            "The only way?",
            "Yeah, the only way using the labels is that they're kind of pumping this information across the graph if you like so.",
            "Yeah, you can compute this in closed form, but that can be slower actually than doing it in the iterative way, because you could do some kind of stopping after only a few iterations.",
            "And also you can take account of sparsity in this matrix.",
            "Try and make it faster so you just set a lot of the kij to zero when you think they're too far away from each other.",
            "So this is kind of simple, but some about it can can be slow as I mean here you have to invert this matrix.",
            "And if you actually analyze this algorithm, what it's really doing is it's kind of a K nearest neighbor or powers on Windows, with an extra regularizer that uses the unlabeled data regularizer that looks kind of like this.",
            "There's some extra normalizations that it says that if two points are closer than, they should have a similar label basically, so you kind of added this regularizer to to a supervised learning algorithm, and so that's kind of a con because I mean KNN isn't seen as the best.",
            "Algorithm to use."
        ],
        [
            "In supervised classification anyway?",
            "So here's here's another method.",
            "This is general set of methods, change of representation methods where you basically do an unsupervised re representation of your data and then you do supervised learning in the new space so.",
            "I imagine you had data like this an you you might try to re represent it.",
            "I mean if you could to something simpler where you've captured this manifold structure in a simpler way, and then you can run a classifier and you know people use things like spectral clustering.",
            "So to do that?",
            "Yeah, so spectral clustering on the unlabeled data."
        ],
        [
            "So and then you could run something like an SVM.",
            "As in the second stage, I won't talk about SVM's.",
            "We just to say, yeah, part of what they do is they use kernel functions.",
            "When you're optimizing the jewel, so you get this K matrix again.",
            "It looks like the K matrix of the graph."
        ],
        [
            "Algorithm I just talked about.",
            "So because a lot of people have done these things called cluster kernels, which is basically just this change of representation techniques that I was talking about.",
            "So because, yeah, they're just define the kernel matrix, which is just the way of defining distances to give to a support vector machine that tried to use the unlabeled data.",
            "So two versions of one is a random walk kernel where you try to make kij, so the dot product between point Iron Point J to be something like the probability of walking.",
            "Along a graph and from I and ending up at J.",
            "So taking into account all the paths so this could be a good idea, because if you're in a manifold or a cluster there will be lots of paths to get from.",
            "I to J and you'll have a higher probability so.",
            "OK, so there's a way of doing that.",
            "You basically do like it and I'll be F Kern or something like that and normalize it so they look kind of like probabilities and take it to the power of T and then that will give you walking T steps so it will be the probability of walking T steps.",
            "There's some other bits you have to do, but it's kind of like that an and that kind of works and then another way is just using spectral clustering.",
            "Which I won't really talk about, but.",
            "That gives you spectral clustering doesn't actually give you a clustering per say.",
            "I mean, it doesn't say this point belongs to class one or two, it gives you.",
            "I mean the whole steps of the algorithm.",
            "People normally use K means at the end or something like that.",
            "Just just before that, what you get is a new representation of your data.",
            "Where, where K means or something works better than in the beginning.",
            "So you could just stop there and then give that to your support vector machine.",
            "That's what people have tried and that also can work.",
            "So these methods have shown could performance, but they can be slow because they involve diagonalizing a matrix."
        ],
        [
            "They have a lot of parameters.",
            "And then there's tears, themes, transductor there seems.",
            "So again, they want to do something.",
            "Like that, they all want to do this kind of thing, so where where you find a classification in region of low density and the way they do that is well efms they try to maximize the margin on labeled data, so you find a large margin here between these two points.",
            "But transductive SVM is find a large margin on both the labeled an unlabeled data.",
            "So here this decision rule, which is a nonlinear one doesn't pass close to the unlabeled points.",
            "So you have a large margin on them as well, so if you can implement this principle, you're effectively implementing the cluster assumption and it you know it looks."
        ],
        [
            "Like it's going to do something like the graph based methods, but it doesn't use KNN or K nearest neighbor type algorithm as a base algorithm.",
            "It's using SVM, so maybe it's a little better.",
            "The trouble is, this optimization problem is non convex, so this is the normal SVM optimization problem in the primal.",
            "So you just minimizing the training error and then you you minimize W squared which gives you a large margin.",
            "But then you add on this extra term.",
            "Which basically is for the unlabeled points only an it's a loss function on the.",
            "The prediction of an unlabeled point.",
            "So if here along this graph I've got the prediction of an unlabeled point, so F of XI star, if it's zero, it means it passes."
        ],
        [
            "Exactly through my hyper hyper power or decision rule, so it's like here so you don't want that right?",
            "You're trying to push it."
        ],
        [
            "Why so?",
            "You have a large margin between these points, so if you have a high penalty when it's zero, and so that's basically it.",
            "So if your point.",
            "And unlike what point is in the middle of the margin, it sort of here taking a high cost and as soon as it chooses one side or the other, and you don't know which side, you don't know which label is you get less cost, so it pushes it one way or the other.",
            "And that's why it's nonconvex.",
            "Because you don't know which side if you like so.",
            "And then there's several implementations of that.",
            "One way to optimize it is to do this kind of simple switch.",
            "You switch labels so you train on your unlabeled data as if it's labeled, and then you switch the labels to try and find the right ones, so."
        ],
        [
            "So you try to find that this one you know belongs to the same class as these guys and this."
        ],
        [
            "Online same classes, these ones and you switch them to opt to find the minimum of this function, which is a bit.",
            "Heuristic and kind of slow, but you could also.",
            "Just try to.",
            "I mean, that's what Chappelle and Xena have done.",
            "Just optimize this primal thing directly with gradient descent, so they use this function so it's smooth and just did just compute the gradient of this thing and try to minimize it.",
            "And that works a little bit better, but.",
            "It still has this.",
            "Cubic speed in the number of labeled and unlabeled points.",
            "And they have to do some tricks like use kernel PCA for nonlinearity as well.",
            "'cause this is only in the for the linear case."
        ],
        [
            "OK, so so far I've just talked about.",
            "These three sort of typical methods that people are using right now for a semi supervised learning and here's a little comparison of them on a couple of problems.",
            "Small scale problems.",
            "So, uh, on USPS, this is digit recognition problem with only 32 labeled points and 4000 unlabeled points.",
            "And for document classification problem, these are both just two class problems where you have two 2000 unlabeled points and 16 labeled points.",
            "So you can see that compared to a support vector machine you can get an improvement in accuracy, but it's kind of not that impressive for the SVM here and actually the label propagation does well.",
            "Everything is better than nearest neighbor though, and for the text classification problem the SVM is doing well and the label propagation doesn't work.",
            "So I mean this one people argue is because label propagation doesn't really work in high dimensions 'cause it's kind of based on density estimation which kind of breaks breaks down in high dimension.",
            "But this one why it's so good.",
            "I'm not so sure.",
            "But essentially there will.",
            "They are all helping to improve.",
            "Performance on these problems, but it looks like you really need.",
            "Have more more unlabeled data to make your bigger impact."
        ],
        [
            "So you're going to need some large scale semi supervised learning, so speeding up these algorithms I think I mentioned briefly label propagation.",
            "You can do a few things like sparsifying the graph and some people have tried approximating the K matrix by making some examples expressed as linear combinations of others trying to try to make this key matrix easier to deal with, so it's much faster, but I'm still.",
            "Not so impressed that that's going to go anywhere to be honest, but still.",
            "You can try and.",
            "Further cluster kernel so the re representation approach, I mean one obvious thing is basically you split the problem into 2 problems.",
            "You're running dimensionally dimensionality reduction algorithm or clustering algorithm.",
            "Then you're running a supervised learning algorithm.",
            "So to make it fast you just make both things really fast right?",
            "So I mean why not use something fast like K means in the first stage and then try and use a fast classifier in the second stage?",
            "So that's kind of obvious.",
            "But then a little bit more tricky.",
            "If you want to speed up the the regularization based methods like T SVM which have this extra.",
            "Regularizer to deal with the unlabeled data because for things like T SVM is not convex anymore so.",
            "A lot of the tricks that people are using to speed up SVN's and things like that aren't aren't going to work.",
            "Um?",
            "So 11 obvious way that people have tried is simply to take this switching heuristic and trying and fix it by switching lots of things at once and showed that there's some speed up there.",
            "But what I'm going to talk about is their concave convex minimization procedure that's been applied to T SVM and seems to be.",
            "Really, a nice clean way of optimizing that.",
            "That's that's quite fast.",
            "And just to say about scaling, because all these things here I'm talking about support vector machines.",
            "If you have a linear classifier, so no kernels, then you can get linear scaling with a number of examples.",
            "So and there's things like SVM, Lin like you can download that do things like that.",
            "But if you have a nonlinear classifier using kernels, then it's not so great.",
            "Then you have more like.",
            "A quadratic tendency.",
            "So that's just to say well, but a lot of the things I'm talking about, or in this nonlinear case.",
            "But if you want to go faster, you can just go back to the linear case at least, or else you something like.",
            "Your networks."
        ],
        [
            "Austin an so.",
            "For fast cluster kernels, one.",
            "One easy trick is to take her fast clustering algorithm, for example, like K means.",
            "So that can be much faster in spectral clustering where you have to, you know dialogue, diagonalize a big matrix.",
            "So I won't talk about K means.",
            "But just to say what you do with it afterwards.",
            "So here's some quick thing that you can try.",
            "You can run K means end times and then you construct a kernel matrix for a support vector machine where you say the kernel between I&J is the number of times inj or in the same cluster over the number of.",
            "Runs you tried the end over end so that kind of gives you like a probability that they rely on the same cluster, because K means every time you run it you get different result and then you can do something like take the product between two kernels, which is still a valid kernel.",
            "So your original kernel and this new.",
            "Bags K means kernel will effectively scale your original kernel, taking into account of the unlabeled data.",
            "So it's kind of the original kernels rescaled if you like by the probability that two points are in the same cluster.",
            "So you're if they're not in the same cluster, you're going to lower that down to zero.",
            "That dot product and and it's not going to try to relate them to each other, so it's going to take account of this clustering."
        ],
        [
            "For May shun."
        ],
        [
            "In the original space, yeah.",
            "Yeah.",
            "Yes.",
            "Yes, your assumption that you can do a good clustering with K means has to be true, otherwise K means."
        ],
        [
            "Profile so and I tried this in the context of protein super family classification, so this is averaged over 54 target families.",
            "So you try to predict whether a protein belongs to super family or not.",
            "So I know if you know anything about proteins, but they are organizing to families, super families and folds.",
            "It's kind of hierarchy of labels and well, you don't really need to know that to look at this.",
            "These results, but the error rate is measured using either RAC or RAC 50 where you look up to the first 50 false positives in your rank list so you care more about the top of the ranking.",
            "So this kind of more relevant this rock 50 result and I compared efms so you get Rs so a large score is good here.",
            "If .4 and then I was trying.",
            "Spectral clustering cone or the random walk kernel and you get like .5 eight .69.",
            "And with this camins kernel, you don't actually do that back you're doing.",
            "Pretty much like these other slower cluster kernels amazingly, but you can be much faster and they also happen to be better than than T SVM.",
            "But that was with the.",
            "3000 labeled points and 3000 unlabeled points.",
            "But then if I go up to 30,000 unlabeled points, which I can't run, the spectral clustering algorithm on, I get.",
            "Better results again.",
            "So now this is really quite significantly.",
            "Better than the axiom, sorry.",
            "Starting to do something useful."
        ],
        [
            "I guess I'm way overtime.",
            "No.",
            "Oh OK, so and then, here's a way of speeding up transductive SVM's.",
            "So as I said, the problem is is non convex and actually happens that you can split that problem into a convex and concave part.",
            "So the whole SVM part is convex, but then the other part.",
            "That's this function.",
            "You can split that into a convex and concave park.",
            "And then there exists this algorithm that if you can do that for optimizing such problems, so you can optimize some some objective J, where if you can split it into these two parts and the way it works is iteratively, you optimize basically the convex part fully plus an approximation of the concave part is just by linearizing it using its tangent.",
            "So this whole thing is still convex.",
            "And you can prove the running that iteratively.",
            "You will converge to to the.",
            "A local minimum of the whole non convex problem.",
            "And also decreases each iteration.",
            "So armed with that, we can simply apply it to the T SVM problem and we avoid these kind of switching label heuristic."
        ],
        [
            "Books and things like that, and it turns out that that's actually much faster.",
            "The algorithm looks like this, but I don't think I'll talk about that.",
            "Is just to say though, in each iteration it kind of looks like an SVM, so."
        ],
        [
            "The speed of this whole algorithm is basically.",
            "If you, I mean it's a constant times the speed of the SVM 'cause it is typically 5 to 10 iterations of running in SVM, so an you can use in each iteration.",
            "You're kind of like the last one, so you can use some restarts as well, so maybe it's not just five times slower like that.",
            "And that also.",
            "Works.",
            "Very well compared to SVM like see SVM and the nabla TSV M of Chappelle and Zoom on these are just a few small scale datasets.",
            "We're just taken from from the paper of Chappelle.",
            "You just replied to the same problems so."
        ],
        [
            "It works.",
            "And then if you look at the training time there as a function of the number of unlabeled examples, this is actually.",
            "So this is the time so.",
            "SVM Lite, transductive SVM, is getting much lower as the number of training examples increases.",
            "Well, we're also getting slower, but we're down here.",
            "This is the CCC PTSM an same behavior on two different datasets.",
            "And like for example here with 2000 unlabeled examples, it's 133 times faster than SVM light, and that gap is going to get bigger as well, with more unlabeled examples.",
            "So it's really.",
            "Working much better than."
        ],
        [
            "The heuristic.",
            "And this is just a plot to show that that's the iterations of the CCP.",
            "We're plotting the objective function and the test error, and you show this converges very quickly actually, because the algorithm for CCP only said that there is a local minima and it decreases on each iteration.",
            "You didn't say how many iterations, so this is just empirically to show.",
            "Actually is quite only few iterations."
        ],
        [
            "In practice.",
            "So you can try that now on some some bigger datasets.",
            "And.",
            "The result here is not not so great, I would say actually because so here with 100 labeled points, an increasing number of unlabeled points, you can see a drop in error is quite nice, but if you have only if you have 1000 labeled training points, so more more training points that are labeled, then this decrease in error is not very big anymore so.",
            "Snow.",
            "That's overall.",
            "That's just a two class problem.",
            "No.",
            "Sorry, I didn't say."
        ],
        [
            "Which one is?",
            "An hour MNIST you get something something similar.",
            "With with 1000 labeled points, you can get a decrease.",
            "In error rate, if you go up to 40,000 unlabeled examples."
        ],
        [
            "So.",
            "And it's just to show their training time is basically has a quadratic tendency with their number of unlabeled examples on these problems, which is as good as you can get with.",
            "Support that nonlinear support vector machines because they already have that tendency.",
            "So basically we have the same scaling as them.",
            "If we did this in the linear case though, we would have a linear."
        ],
        [
            "Scaling.",
            "So, but what you might be able to do is to avoid this kind of quadratic tendency is to do something like neural networks instead.",
            "'cause there the problem with support vector machines is that F of X.",
            "Uses, it grows as the number of unlabeled as a number of examples grow right?",
            "Because there are a number of support vectors grow, so somehow if you were doing online learning, your model is always getting slower and slower, which isn't a very good property for large scale so.",
            "Things like neural networks, on the other hand, have a fixed capacity.",
            "They have a fixed size network, so you know how fast you're going to be.",
            "Seems the nicer property, and you could easily apply this same kind of semi supervised regularizers to that as well.",
            "And then you'd have better scaling an that's just a quick plot to show.",
            "We just started doing that and it's working, but I don't have full."
        ],
        [
            "Results of that.",
            "So then the summary.",
            "Semi supervised learning WHI because labels are expensive or incomplete and unlabeled data is cheap.",
            "When when, when can you use it when P of X is useful basically so one of these assumptions has to be true.",
            "I don't know quite how many datasets that's true and how many.",
            "It's not true.",
            "So obviously semi supervised learning researchers tend to pick the datasets where it where it works so.",
            "I asked actually alleviation power is the.",
            "Author of the semi supervised learning book.",
            "How many he thought and he said maybe half half the datasets have these assumptions but I don't know.",
            "And then how can we do it?",
            "Well, we can use a semi supervised regularizer which we add to our supervised learning.",
            "Or we can implement that in a number of ways and.",
            "We're not sure which is which is the best regularizer yet.",
            "For example, it's been shown that if you add some of these regularizers together like the one of the TSV M, which pushes the examples away from the margin plus the one of the graph diffusion, which says two labels should be 2, two outputs should be the same if their points are close.",
            "If you add these two regularizers together then actually you're better than using one or the other.",
            "So somehow people haven't found what the best best regularizer is yet.",
            "And then why?",
            "Large scale?",
            "Because lots lots of you need lots of unlabeled data because each unlabeled point has very little impact.",
            "And then just a couple of ways of doing that.",
            "Large scale.",
            "A two stage approach of doing fast unsupervised learning plus fast supervised learning.",
            "And then the problem of the large scaling is no longer the problem for semi supervised learning research is it's more problem for people researching supervised and unsupervised learning.",
            "Or else if you do a direct regularizer then you might need to do something clever like this.",
            "SCCP optimization, so non convex optimization techniques can apply then and things are more tricky.",
            "That's it.",
            "Yeah.",
            "Detect yeah, certainly if you have better methods for unsupervised learning then you might be able to slot them right in.",
            "Well the reason well semi supervised learning researchers hope the reason is because.",
            "They have this cluster assumption that you know the the decision boundary lies.",
            "In a region of low density so that even though the classes are overlapping, it's still lower density.",
            "There where the overlaps occurring than than where the you know the cluster is there you know the centers of the clusters if you don't have that.",
            "If the clusters overlap that there isn't a region of low density, then the cluster assumption isn't true, and those things won't help I guess, but because that is hopefully true, those those kernels are giving better results.",
            "Yeah, you can easily make them, I mean free.",
            "You can easily construct them.",
            "Real world yeah, definitely.",
            "I mean, I've tried it on some things and.",
            "It didn't help, I mean.",
            "Well, I was doing.",
            "I was I was like trying to classify preferences of like movie preferences an I didn't.",
            "I didn't get an improvement there.",
            "I thought I mean no, I didn't.",
            "I mean The thing is, it might not be that problem per se.",
            "It might have been my representation of the problem as well.",
            "You know, like the feature representation I had was the people I guess along that's the normal 1, isn't it?",
            "Which people like this movie kind of thing and you have that vector.",
            "For each movie.",
            "Oh, why didn't I don't know, but I was.",
            "I was just saying it might not be the problem per say, but just my representation.",
            "You know the features that I took for that problem, which were basically for each movie I'd have like which person liked it, which user liked it.",
            "So then you know, if you take a dot product between 2 movies, it's high.",
            "If the same users like them, right?",
            "But whi that representation wasn't good.",
            "I don't know.",
            "Or maybe there's there's no representation for that problem, although I believe there is because I think you know their Netflix competition.",
            "Mintons results, which are one of the best or the best.",
            "He used to kind of transduction where he used, like the unlabeled set the test set.",
            "To improve his results.",
            "But he didn't say exactly so I don't think he said you.",
            "The improvement from either using or not using it, but just extrapolating that you beat everybody else was maybe that was one of the reasons."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So hello.",
                    "label": 0
                },
                {
                    "sent": "So hello, I'm Jason Weston.",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk about Semi supervised learning and in particular trying to scale that towards large problems.",
                    "label": 0
                },
                {
                    "sent": "But actually a lot of the work done in semi supervised learning isn't that large yet.",
                    "label": 0
                },
                {
                    "sent": "I will try to discuss some ways of making that, making those algorithms faster.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do I?",
                    "label": 0
                },
                {
                    "sent": "Next page.",
                    "label": 0
                },
                {
                    "sent": "OK, so um so supervised learning, which we've mostly talked about at this workshop, involves some examples, and a teacher who's telling you, you know, teaching you about those examples.",
                    "label": 0
                },
                {
                    "sent": "So typically we in our frameworks we say our examples are labeled, so we have classification problems where we have a class label.",
                    "label": 0
                },
                {
                    "sent": "We have regression, and we're also going to have discussion about structured output learning, I think tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So where you know you do things like you have a sentence in the input and the output you tried to predict a parse tree for that sentence.",
                    "label": 0
                },
                {
                    "sent": "So it's a much more complicated label within classification.",
                    "label": 0
                },
                {
                    "sent": "And now if you don't have a teacher and you just have examples, then that's called unsupervised learning, and it's not so clear what to do.",
                    "label": 0
                },
                {
                    "sent": "I mean, in supervised learning, people just make a nice methodology where they say you're going to measure how good you are by measuring error rate on the test set.",
                    "label": 0
                },
                {
                    "sent": "In unsupervised learning where you don't have labels, so people have made sort of lots of different systems.",
                    "label": 0
                },
                {
                    "sent": "Like you know clustering and outlier detection and dimensionality reduction, and all of those things is more.",
                    "label": 0
                },
                {
                    "sent": "Can you make an analysis that will be useful for you later on?",
                    "label": 0
                },
                {
                    "sent": "And maybe it's not so easy to to analyze how well it's doing.",
                    "label": 0
                },
                {
                    "sent": "I mean there are metrics but still, but the most classical one is probably density estimation.",
                    "label": 0
                },
                {
                    "sent": "You know coming from statistics, but then a lot of machine learning people don't.",
                    "label": 0
                },
                {
                    "sent": "Tend to use density estimation.",
                    "label": 1
                },
                {
                    "sent": "But anyway, semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Which thing I'm going to talk about?",
                    "label": 0
                },
                {
                    "sent": "Well, that's kind of somewhere sitting in between the two.",
                    "label": 0
                },
                {
                    "sent": "You've got examples and you could say a part time teacher, so your teachers teaching you about some of your examples.",
                    "label": 0
                },
                {
                    "sent": "But then they have the day off the rest of the time and you're on your own.",
                    "label": 0
                },
                {
                    "sent": "You've just got unlabeled data again.",
                    "label": 0
                },
                {
                    "sent": "So, for example, you could have a set of labeled images and a huge collection of unlabeled images.",
                    "label": 1
                },
                {
                    "sent": "And then you want to learn.",
                    "label": 0
                },
                {
                    "sent": "And in a typical setting.",
                    "label": 0
                },
                {
                    "sent": "The people use is more like the supervised settings, so they measure how well they're doing using test set, which is fully labeled.",
                    "label": 0
                },
                {
                    "sent": "So then the problem is just to try to improve your generalization ability on a classification task using unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm talking about yeah mixing using classification, but of course you could use any of these supervised learning tasks with unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So naively you could think well, I've got my list of supervised learning tasks and my lists of unsupervised learning tasks, and I could just pick one from one list and one from the other and add them together and semi supervised learning is kind of like that, maybe naively, so for example is semi supervised learning doing classification and clustering?",
                    "label": 1
                },
                {
                    "sent": "Or is it doing structured output learning with some dimensionality reduction?",
                    "label": 0
                },
                {
                    "sent": "Well, I'm not really sure about actually people.",
                    "label": 0
                },
                {
                    "sent": "There's publications of people doing all these variations and.",
                    "label": 0
                },
                {
                    "sent": "We will.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We'll see.",
                    "label": 0
                },
                {
                    "sent": "So first, a word about supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So as we've seen many times before, you have I'm using, I think the same notations, most people and input X and output Y.",
                    "label": 0
                },
                {
                    "sent": "And then you've got training data xiy I see normally I'm going to say have L training points and there's two main families of methods.",
                    "label": 1
                },
                {
                    "sent": "Generative models really estimated joint probability and discriminative models where you're just.",
                    "label": 0
                },
                {
                    "sent": "Interested in the conditional probability or in things like support vector machines, you're just looking at which class is the most probable.",
                    "label": 0
                },
                {
                    "sent": "So here in a two class problem, so you sort of dispense with the probabilities and you just look at say F of X is just a.",
                    "label": 0
                },
                {
                    "sent": "A classification label directly and you don't give an estimate of probability.",
                    "label": 0
                },
                {
                    "sent": "In this talk.",
                    "label": 0
                },
                {
                    "sent": "I'm only going to discuss discriminative methods and some issues about all of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Many algorithms don't scale that well as Leon was saying in his talk, you have a lot of algorithms that use, you know matrices and they have this sort of N cubed behavior and things like that.",
                    "label": 0
                },
                {
                    "sent": "But you could say for many problems actually a lot of labeled data doesn't even exist.",
                    "label": 0
                },
                {
                    "sent": "You know it's expensive to obtain, so maybe in a lot of supervised learning problems you don't even need to scale that well, but it's true in some you do, but it's more clear in semi supervised learning because unlabeled data is so much easier obtain while large scale learning is more important.",
                    "label": 1
                },
                {
                    "sent": "But perversely, so far most people haven't addressed that too.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is coming but I guess people are still truly trying to understand what's the best actual techniques to use in semi supervised learning and that's why they're still concentrating a lot on small, small problems.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "So what is semi supervised?",
                    "label": 0
                },
                {
                    "sent": "Learning just a bit more detail here.",
                    "label": 0
                },
                {
                    "sent": "So now I've got my labeled training set of xiy pairs, but I also have unlabeled data, which I call XI star and.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to measure success with a labeled test set.",
                    "label": 1
                },
                {
                    "sent": "So that's the typical setting of semi supervised learning that people use.",
                    "label": 1
                },
                {
                    "sent": "But actually there's there's other interpretations that you could make.",
                    "label": 0
                },
                {
                    "sent": "For example, you could.",
                    "label": 0
                },
                {
                    "sent": "You could treat this semi supervised learning problem as a clustering problem where you're given must Lincoln must not link constraints.",
                    "label": 0
                },
                {
                    "sent": "So that's the teacher who's giving you those constraints.",
                    "label": 0
                },
                {
                    "sent": "So it's a bit more than clustering, and that if you do it that way, you can actually do some tasks.",
                    "label": 0
                },
                {
                    "sent": "You can be solved that you can't solve.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "In the setting above where you just treat as a classification problem, because for example, if you have a missing class in your in your labeled set, you don't have a class, then you could still deal with it because you're doing clustering.",
                    "label": 0
                },
                {
                    "sent": "So for example, in protein classification, many of the classes of super family or family aren't even known yet.",
                    "label": 0
                },
                {
                    "sent": "So if you just do classification, you'll never going to predict one of those classes, so you're in a bit of a problem.",
                    "label": 0
                },
                {
                    "sent": "So you could treat this in a different way and another way of looking at semi supervised learning perhaps is that you could think of having your part time teacher is sometimes giving quite detailed label or teaching on some examples and sometimes much less detailed and sometimes none at all.",
                    "label": 0
                },
                {
                    "sent": "So they have a kind of fine grained knice of label on your examples and you could see.",
                    "label": 0
                },
                {
                    "sent": "You know some real cases like that, for example in text.",
                    "label": 0
                },
                {
                    "sent": "Processing you could have a label that just says this is a sentence from nature or this is this is a garbage collection of words, so that would be a very sort of loose grained label or more fine grained.",
                    "label": 0
                },
                {
                    "sent": "You could have.",
                    "label": 0
                },
                {
                    "sent": "These are the parts of speech of the words in that sentence, so this is a noun.",
                    "label": 0
                },
                {
                    "sent": "This is verb and then even more detail.",
                    "label": 0
                },
                {
                    "sent": "This is the powers tree of that sentence and then even more detailed or or going from syntax to semantics.",
                    "label": 0
                },
                {
                    "sent": "You could have some semantic labels of that sentence.",
                    "label": 0
                },
                {
                    "sent": "So you can see sort of lots of different you know levels of labeling there and you could imagine some big training set where you have all different kind of fine grained use of labels in some completely missing and then you have to handle that problem and I think that's kind of interesting and you could see that in speech another applique.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oceans as well.",
                    "label": 0
                },
                {
                    "sent": "So I've already said a little bit why.",
                    "label": 0
                },
                {
                    "sent": "Why do you semi supervised learning?",
                    "label": 0
                },
                {
                    "sent": "Well supervised data can be expensive to obtain in some applications.",
                    "label": 0
                },
                {
                    "sent": "I mean, you might have to hire A labeler to to actually come and label your data.",
                    "label": 0
                },
                {
                    "sent": "That is the teacher, human teacher, and depending on the problem, they might have to be very well trained.",
                    "label": 0
                },
                {
                    "sent": "So, for example, labeling sentences with parse trees can't be done by just anyone.",
                    "label": 0
                },
                {
                    "sent": "But also there's other cases where, for example, if you're trying to predict protein 3D structure, then you can't even do that just like that.",
                    "label": 0
                },
                {
                    "sent": "I mean you have to have some biology spending a lot of time in the lab trying to find that.",
                    "label": 0
                },
                {
                    "sent": "And they might not even be successful.",
                    "label": 0
                },
                {
                    "sent": "Where is unlabeled data can be in some applications very cheap to collect.",
                    "label": 0
                },
                {
                    "sent": "So particularly I mean everything that humans perceive where you know, collecting this data all the time.",
                    "label": 0
                },
                {
                    "sent": "So we kind of can get an infinite amount of that, like audio and vision and text.",
                    "label": 0
                },
                {
                    "sent": "But there's also lots of other natural data not.",
                    "label": 0
                },
                {
                    "sent": "Humans perceive directly from our senses like primary protein structures, network traffic and so on, so there's a lot of places where you can get unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So, so that's the reason why do you send me supervised learning?",
                    "label": 0
                },
                {
                    "sent": "You can get this stuff much easier than than labeled data where another argument is that.",
                    "label": 0
                },
                {
                    "sent": "My mouse, a true AI that mimics humans, wouldn't have a strong teaching signal because.",
                    "label": 1
                },
                {
                    "sent": "Humans tend to, well, I think that humans tend to learn a lot from limited teaching, so probably we do semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So for example in natural language processing linguists label senses we parse trees, but human learn humans learn about language without these labels.",
                    "label": 0
                },
                {
                    "sent": "Well, they don't.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eat them.",
                    "label": 0
                },
                {
                    "sent": "So so that's why semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And now when can it work?",
                    "label": 1
                },
                {
                    "sent": "And typically people say that, well.",
                    "label": 1
                },
                {
                    "sent": "I mean your unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "It gives you knowledge about the distribution of X, right?",
                    "label": 0
                },
                {
                    "sent": "P of X.",
                    "label": 0
                },
                {
                    "sent": "So if you had infinite amount of data, you can know the density P of X, but that doesn't necessarily tell you anything about P of Y given X.",
                    "label": 0
                },
                {
                    "sent": "That I mean in a classification problem so.",
                    "label": 1
                },
                {
                    "sent": "Typically people say it will help if some assumptions about the distribution of the data are true.",
                    "label": 0
                },
                {
                    "sent": "And the two typical ones that people quote accord the cluster assumption and manifold assumption by I've included another two that may or may not be true.",
                    "label": 0
                },
                {
                    "sent": "See what you think and.",
                    "label": 1
                },
                {
                    "sent": "Just to say this thing about making assumptions and hoping they hold true for data.",
                    "label": 0
                },
                {
                    "sent": "Sort of used in in supervised learning as well.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not like it's just being made up for this because in a lot of algorithms, for example, in supervised learning you assume the label doesn't change when you move a little bit in input space.",
                    "label": 0
                },
                {
                    "sent": "Most algorithms this unit.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's these different assumptions.",
                    "label": 0
                },
                {
                    "sent": "So the cluster assumption it just says that I hope on my particular problem that examples in the same cluster have the same class.",
                    "label": 1
                },
                {
                    "sent": "So in this in this plot here I have.",
                    "label": 0
                },
                {
                    "sent": "I have three clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, we have oh I've got a like a laser pointer or something, right?",
                    "label": 0
                },
                {
                    "sent": "We have three clusters and I've only got 3 label examples.",
                    "label": 0
                },
                {
                    "sent": "There's this one of one class and these two of the other class, and so if I just train a support vector machine or something like that on just these three points, I get like a hyperplane like this if it's a linear classifier and you notice I cut through this cluster.",
                    "label": 0
                },
                {
                    "sent": "But if I had unlabeled data and I knew this assumption was true, so I saw these images, then I probably wouldn't make this decision.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'll probably let go like this or something.",
                    "label": 0
                },
                {
                    "sent": "So the so this cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "People also have another name for it, they call it low density separation and they say that the decision rule should lie in a region of low density.",
                    "label": 0
                },
                {
                    "sent": "So in other words it doesn't cut through a cluster because.",
                    "label": 0
                },
                {
                    "sent": "Examples in cluster of the same class.",
                    "label": 1
                },
                {
                    "sent": "So then this would be a region of low density here.",
                    "label": 0
                },
                {
                    "sent": "So again if I need P of X, yeah is P of X is high here, here and here where the clusters.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And people also have something else called the manifold assumption, which is pretty similar.",
                    "label": 0
                },
                {
                    "sent": "Kind of thing really.",
                    "label": 0
                },
                {
                    "sent": "They just say examples in the same manifold have the same class so.",
                    "label": 1
                },
                {
                    "sent": "Typically this this inspires different algorithms because what people do is they say, oh, there's some manifolds in my data and they have the same class.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to do do something where I. I do dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "I try to re represent this space so that the manifolds are like unfolded into something this nice and easy to separate and there's a lot of algorithms like that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But here's another couple of possibilities of why it could be.",
                    "label": 0
                },
                {
                    "sent": "Could be good to do semi supervised learning so.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bottom bottom.",
                    "label": 0
                },
                {
                    "sent": "Oh this one now, but this is labeled this point, right?",
                    "label": 0
                },
                {
                    "sent": "I have two labels there, an another label there, so three labels and this whole cluster imagini Samp.",
                    "label": 0
                },
                {
                    "sent": "I had lots of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Then I have lots of samples from inside this cluster.",
                    "label": 0
                },
                {
                    "sent": "Label yeah, yeah I'm sorry company so yeah I have my training set is 3 points so I have X one X2X3 and Y one is 1 Y two is minus one Y-3 is minus one.",
                    "label": 0
                },
                {
                    "sent": "But then I have excise star.",
                    "label": 0
                },
                {
                    "sent": "I called it unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "If I draw lots of those when I draw them they're going to mostly be from here, here and here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So he is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just another way that you can can help zips law which people talk about a lot.",
                    "label": 0
                },
                {
                    "sent": "I think in natural language processing in a corpus of natural language utterances the frequency of any word is roughly inversely proportional to its rank in the frequency table.",
                    "label": 1
                },
                {
                    "sent": "So they say something like.",
                    "label": 0
                },
                {
                    "sent": "10% of the words occur 90% of the time in in normal text, so you keep seeing the all the time, for example.",
                    "label": 0
                },
                {
                    "sent": "But then words that might really help you to do document classification.",
                    "label": 0
                },
                {
                    "sent": "For example, you can be content to be rare words like you'll only see them in a couple of times in the document and there will be really useful for the document classification or or or or at least.",
                    "label": 0
                },
                {
                    "sent": "Another way of saying if you have a training set of documents.",
                    "label": 0
                },
                {
                    "sent": "Because of this distribution of words, there seems you take another document.",
                    "label": 0
                },
                {
                    "sent": "There's probably going to be words in it that you haven't seen before, so that would be a reason why semi supervised learning could be good, because if you're always in that sort of situation where you keep finding new words well, you're never going to be able to label enough text documents to know all those words are.",
                    "label": 0
                },
                {
                    "sent": "So you better try and use unlabeled data to understand what they are.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And another thing which I think Leon was alluding to a little bit at the end of his talk is about non IID data.",
                    "label": 0
                },
                {
                    "sent": "So everything I've more or less been talking about IID data here, but in real life it's not necessary necessarily like that.",
                    "label": 0
                },
                {
                    "sent": "And let's say for example, your test set is drawn from a different distribution to your training set, then normal supervised learning with these algorithms that we use which make this IID assumption like SVM's and so on.",
                    "label": 0
                },
                {
                    "sent": "Necessary into work very well, but if you again if you use semi supervised learning and you had access to that test set of as unlabeled data, you might be able to do better.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you if you trained a parser on the Wall Street Journal and then you apply it to Moby Dick or some piece of fiction is not necessarily going to work well.",
                    "label": 1
                },
                {
                    "sent": "But if you'd seen that as unlabeled data, maybe you can improve your parser somehow.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why large scale semi supervised learning?",
                    "label": 0
                },
                {
                    "sent": "This is a plot taken from a paper of tossed in your achenes.",
                    "label": 0
                },
                {
                    "sent": "He's showing, I mean the whole point that people do this is that they have actually found algorithms that show unlabeled data helps for compared to, you know, just using labeled data alone, so you typically get these plots where you compare your semi supervised algorithm to a supervised algorithm.",
                    "label": 0
                },
                {
                    "sent": "According to the number of unlabeled data in that you use an, you want the improvement.",
                    "label": 0
                },
                {
                    "sent": "Sorry, this plot actually is about the number of labeled data.",
                    "label": 0
                },
                {
                    "sent": "So yeah, here, the number of unlabeled data is.",
                    "label": 1
                },
                {
                    "sent": "Fixed I think, but then the number of labeled data is changing and what he shows is so transductive SVM is an algorithm for doing semi supervised learning in SVM.",
                    "label": 0
                },
                {
                    "sent": "Is a normal supervised classifier and when you have a small number of labeled data there's there's an improvement here in break even point on a text classification problem.",
                    "label": 0
                },
                {
                    "sent": "So you're really utilizing this unlabeled data and getting an improvement.",
                    "label": 0
                },
                {
                    "sent": "But as the number of examples labeled examples increases.",
                    "label": 0
                },
                {
                    "sent": "You see that gap diminishing?",
                    "label": 0
                },
                {
                    "sent": "With the same number of unlabeled data, so there's a problem that a lot of these algorithms so far in the literature they've shown, like results with 50 labeled training points and a few thousand unlabeled and said, oh, it works better.",
                    "label": 0
                },
                {
                    "sent": "But when you increase the number of labeled points, because I'm not sure that's really a very realistic case, you need to increase the number of unlabeled points, like drastically.",
                    "label": 0
                },
                {
                    "sent": "It's not like a linear relation.",
                    "label": 0
                },
                {
                    "sent": "Because somehow an unlabeled point isn't.",
                    "label": 0
                },
                {
                    "sent": "As informative as a labeled one is much less so, so that's a good argument why you need large scale semi supervised learning 'cause you have a massive amount of this unlabeled data in each unlabeled point is somehow not worth very much.",
                    "label": 0
                },
                {
                    "sent": "So you have to use them all.",
                    "label": 0
                },
                {
                    "sent": "I took this from another paper.",
                    "label": 0
                },
                {
                    "sent": "I think it's something like a few thousand unlabeled.",
                    "label": 0
                },
                {
                    "sent": "So like 3000 or something, but I could be wrong.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, I'm is sorry.",
                    "label": 0
                },
                {
                    "sent": "Say that again.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, OK, so the number of unlabeled is going down then, OK?",
                    "label": 0
                },
                {
                    "sent": "Simon, if that answers your question so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But still, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same, my argument still holds.",
                    "label": 0
                },
                {
                    "sent": "People use that a lot in text classification.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "You don't think it's a good metric.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I mean, yeah, I mean it's better than classification error in these kind of things right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, I guess he uses it in his papers because other people use it too.",
                    "label": 0
                },
                {
                    "sent": "But I mean I mean normally I see in these text classification problems you quote several metrics like F1 as well and RSC right?",
                    "label": 0
                },
                {
                    "sent": "But I think you know this will hold this kind of plot will hold with other metrics.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I feel bad now introduce.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This graph of results where I didn't really know what it was.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to make.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some argument about you need a lot of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Will we get to other experiments later?",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So a bit more about this.",
                    "label": 0
                },
                {
                    "sent": "Why large scale semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So there's loads of unlabeled data researchers.",
                    "label": 1
                },
                {
                    "sent": "So far I've come up with slow algorithms.",
                    "label": 0
                },
                {
                    "sent": "They are typically trained on simple problems with like 50 labeled examples and.",
                    "label": 1
                },
                {
                    "sent": "500 or 5000.",
                    "label": 0
                },
                {
                    "sent": "Unlabored example with speakers.",
                    "label": 0
                },
                {
                    "sent": "A lot of these methods are too slow to run otherwise.",
                    "label": 0
                },
                {
                    "sent": "But there's a relationship here.",
                    "label": 0
                },
                {
                    "sent": "That's I mean, I guess this problem independent.",
                    "label": 0
                },
                {
                    "sent": "I mean, it depends on how strong the assumption about P of X you've made is true.",
                    "label": 0
                },
                {
                    "sent": "But this relation between a number of unlabeled data you would need to label to really make some strong improvement probably isn't linear.",
                    "label": 0
                },
                {
                    "sent": "So for say 5000 labeled examples, you might need 500,000 unlabeled to make some improvement.",
                    "label": 1
                },
                {
                    "sent": "But a lot of these problems that are people are measuring the success rates on a kind of simple problems.",
                    "label": 0
                },
                {
                    "sent": "I mean, compared to really hard classification problems that humans can solve.",
                    "label": 0
                },
                {
                    "sent": "So I would guess that for really hard high dimensional problems like Cena analysis and language understanding, then if you only have 5000 labeled examples, that's you know it's in a very high dimensional space, so it's it looks actually very sparse an it's equivalent to having like 50 labeled examples in.",
                    "label": 1
                },
                {
                    "sent": "One of these simple problems, so again, you're back in a situation where the semi supervised learning really might help.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I would hope.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "He's a bit of history of semi supervised learning, so one of the first things that people tried is called self learning, where you have a supervised learner classifier and you train it on some data and then you have some unlabeled data and you apply it to that data.",
                    "label": 0
                },
                {
                    "sent": "So you compute these predictions and then somehow you take those predictions as true labels.",
                    "label": 0
                },
                {
                    "sent": "Some of them maybe some of them that you're confident about and put them in the training set and.",
                    "label": 0
                },
                {
                    "sent": "And train on those as well and there's some success with that, but it's maybe a bit heuristic.",
                    "label": 0
                },
                {
                    "sent": "Or you know how you actually choose which examples you're going to self learn on is kind of problem, because if you know you're making mistakes and you're putting those into your training set, then you're going to train on your mistakes and not do too well.",
                    "label": 0
                },
                {
                    "sent": "And then in the 70s Vapnik came up with Trans Duction an you know people are still using transductive SVM now.",
                    "label": 0
                },
                {
                    "sent": "And is it you can see a lot of relationships with the self learning algorithm, but somehow it's just a little bit more principled way of doing it and I'll get to that.",
                    "label": 0
                },
                {
                    "sent": "And then the generative model approaches, which I'm not going to talk about and Co training.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Which yeah, I don't know if I should mention what what it does, but 'cause these these three are actually the ones I'm going to talk about, so.",
                    "label": 0
                },
                {
                    "sent": "People started seeing the you could see the semi supervised learning problem is regularization based.",
                    "label": 0
                },
                {
                    "sent": "So you take your supervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Your add on an extra regularizer and that tries to take into account one of these assumptions that was talking about like the cluster assumption and then you can make an algorithm from that and transductive transduction.",
                    "label": 0
                },
                {
                    "sent": "So T SVM is kind of in that framework, but then there's other algorithms in that framework too.",
                    "label": 0
                },
                {
                    "sent": "And then there is also these graphed based approaches.",
                    "label": 0
                },
                {
                    "sent": "So you you well, I'm going to get to them too.",
                    "label": 0
                },
                {
                    "sent": "You build a graph out of the actual data and then try to do some kind of diffusion normally along the graph to to spread the supervised labels to the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And use them that way.",
                    "label": 0
                },
                {
                    "sent": "And then there's also kind of change of representation methods as well, which are kind of a way of doing the regularization based, but in a simpler two stage approach.",
                    "label": 1
                },
                {
                    "sent": "So first you re represent your data in a new metric, so you have a original input space you go through and you space and you try and use the unlabeled data to do that change and then you just run a supervised classifier in that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "Typically.",
                    "label": 0
                },
                {
                    "sent": "Pin.",
                    "label": 0
                },
                {
                    "sent": "But yeah, a general approach for discriminative semi supervised learning.",
                    "label": 1
                },
                {
                    "sent": "Is just to write this.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the regularization approach I guess.",
                    "label": 0
                },
                {
                    "sent": "So in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "This is a typical way of of optimizing.",
                    "label": 0
                },
                {
                    "sent": "You minimize the empirical loss, and then you have some regularizer Sue too.",
                    "label": 0
                },
                {
                    "sent": "To reduce the capacity.",
                    "label": 0
                },
                {
                    "sent": "And find a smoother function.",
                    "label": 0
                },
                {
                    "sent": "And then you you kind of implement your SRM using this two terms and in semi supervised learning you simply stick on a third term basically which.",
                    "label": 0
                },
                {
                    "sent": "I will capture the function will change the function that you're going to find out if your set of functions according to the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So then the question is, how do you?",
                    "label": 0
                },
                {
                    "sent": "What do you choose there?",
                    "label": 0
                },
                {
                    "sent": "And there's lots of different choices and I think people haven't found maybe the best ones yet, but you know, typically you could encode, say, the cluster assumption with that, and you could make it a kind of pointwise loss, so it decomposes to a sum over unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So my unlabeled data here is.",
                    "label": 0
                },
                {
                    "sent": "You unlabeled points and have L labeled points.",
                    "label": 0
                },
                {
                    "sent": "So so T SVM looks like that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, I'm going to talk about these last three.",
                    "label": 0
                },
                {
                    "sent": "They happened to be the last three in my history slide.",
                    "label": 0
                },
                {
                    "sent": "And you can kind of fit them all.",
                    "label": 0
                },
                {
                    "sent": "A special case cases of that slide before really, so they're all kind of regularization approaches, but they just look a little bit different when you describe them.",
                    "label": 0
                },
                {
                    "sent": "And a good reference book for this.",
                    "label": 1
                },
                {
                    "sent": "For this, for semi supervised learning, is this book.",
                    "label": 0
                },
                {
                    "sent": "Here I mean really this whole field is is kind of in its infancy and that's just a collection of contributed chapters, but I don't think there is.",
                    "label": 1
                },
                {
                    "sent": "A better book right now.",
                    "label": 0
                },
                {
                    "sent": "So first I'm going to talk about this.",
                    "label": 1
                },
                {
                    "sent": "Is the graph based.",
                    "label": 0
                },
                {
                    "sent": "This is a particular instance of a graph.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Base method called label propagation.",
                    "label": 0
                },
                {
                    "sent": "So if you have if you have some training data, so these circles two different, there will be two different classes were actually you don't know that in the beginning these are all just unlabeled apart from the ones that I've put with little crosses.",
                    "label": 0
                },
                {
                    "sent": "Those are the three labeled points.",
                    "label": 0
                },
                {
                    "sent": "So then this algorithm works like this.",
                    "label": 0
                },
                {
                    "sent": "You find the nearest neighbors of each of the points, the unlabeled and labeled points.",
                    "label": 0
                },
                {
                    "sent": "And you build some sort of graph like this.",
                    "label": 0
                },
                {
                    "sent": "I mean here I was just I drew it so that it was lucky that there wasn't any edges between the two manifolds actually, but that could happen.",
                    "label": 0
                },
                {
                    "sent": "But still the algorithm could work because there's very few edges between and.",
                    "label": 0
                },
                {
                    "sent": "Then the way this algorithm works is that these three points are going to kind of send messages to all the other points using these edges as their paths of sending message.",
                    "label": 0
                },
                {
                    "sent": "So they kind of, I mean this called like diffusion.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I mean you people call it like.",
                    "label": 0
                },
                {
                    "sent": "Pumping out a signal or energy or something like that.",
                    "label": 0
                },
                {
                    "sent": "So like this, this labeled point says to all his neighbors I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm label.",
                    "label": 0
                },
                {
                    "sent": "You know I'm class one and this this point says to his neighbors.",
                    "label": 0
                },
                {
                    "sent": "I'm class minus one, and then the algorithm works that all of the nodes are actually sending those messages.",
                    "label": 0
                },
                {
                    "sent": "So these guys are also going to say to their neighbors, so I'm mostly class one and if they receive messages from both, then they'll whichever.",
                    "label": 0
                },
                {
                    "sent": "This signal is loudest if you like from the two messages.",
                    "label": 0
                },
                {
                    "sent": "The hearing will be the one that they.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take they believe they belong to that class.",
                    "label": 0
                },
                {
                    "sent": "So this thing kind of spreads an you can see this kind of this label class one will spread along all these neighbors and go along this manifold.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and then that can.",
                    "label": 0
                },
                {
                    "sent": "Correctly, well it will take it will take into account this kind of assumption that these examples should be the same class, whereas you know again if you train.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something like a support vector machine on just these labeled points.",
                    "label": 0
                },
                {
                    "sent": "Three labeled points.",
                    "label": 0
                },
                {
                    "sent": "You would get a hyperplane cutting across here that would label this part of this manifold belonging to this class, so you get a quite different.",
                    "label": 0
                },
                {
                    "sent": "Testera if we now easier cluster assumption or manifold assumption is true.",
                    "label": 0
                },
                {
                    "sent": "This house.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's going to be much better.",
                    "label": 0
                },
                {
                    "sent": "And the idea of just showing actually.",
                    "label": 0
                },
                {
                    "sent": "What that looks like, yeah.",
                    "label": 0
                },
                {
                    "sent": "So like an SVM will give you labeling like this in this label propagation algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you're labeling like this and K nearest neighbor doesn't work.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just in actual the actual algorithm, I mean, I just explained it intuitively.",
                    "label": 0
                },
                {
                    "sent": "It looks like this.",
                    "label": 0
                },
                {
                    "sent": "You construct this matrix matrix K where KIJ will tell you about the edge between example I&J.",
                    "label": 0
                },
                {
                    "sent": "And then each example has a vector YYYJI mean why I was the example why I and then there will be one dimension for each class in that vector and you'll set.",
                    "label": 1
                },
                {
                    "sent": "The the that dimension to one if it if it's labeled with that class, and 0 otherwise, so an unlabeled point will just have a vector of all zeros and then you just do these updates iteratively, which is basically sending those messages on the graph they were showing.",
                    "label": 1
                },
                {
                    "sent": "So the label of a point becomes the weighted combination of the labels of the points that are sending messages to it.",
                    "label": 0
                },
                {
                    "sent": "If you like and then at the end you can take the argmax over the vector to say which is the label prediction for an unlabeled point.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You can show this algorithm converges and you can.",
                    "label": 0
                },
                {
                    "sent": "Actually you don't have to write it as an iterative algorithm.",
                    "label": 0
                },
                {
                    "sent": "You can write it in closed form.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "No, yeah you.",
                    "label": 0
                },
                {
                    "sent": "I mean you can, but generally people don't.",
                    "label": 0
                },
                {
                    "sent": "The only way?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the only way using the labels is that they're kind of pumping this information across the graph if you like so.",
                    "label": 1
                },
                {
                    "sent": "Yeah, you can compute this in closed form, but that can be slower actually than doing it in the iterative way, because you could do some kind of stopping after only a few iterations.",
                    "label": 1
                },
                {
                    "sent": "And also you can take account of sparsity in this matrix.",
                    "label": 0
                },
                {
                    "sent": "Try and make it faster so you just set a lot of the kij to zero when you think they're too far away from each other.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of simple, but some about it can can be slow as I mean here you have to invert this matrix.",
                    "label": 0
                },
                {
                    "sent": "And if you actually analyze this algorithm, what it's really doing is it's kind of a K nearest neighbor or powers on Windows, with an extra regularizer that uses the unlabeled data regularizer that looks kind of like this.",
                    "label": 0
                },
                {
                    "sent": "There's some extra normalizations that it says that if two points are closer than, they should have a similar label basically, so you kind of added this regularizer to to a supervised learning algorithm, and so that's kind of a con because I mean KNN isn't seen as the best.",
                    "label": 0
                },
                {
                    "sent": "Algorithm to use.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In supervised classification anyway?",
                    "label": 0
                },
                {
                    "sent": "So here's here's another method.",
                    "label": 0
                },
                {
                    "sent": "This is general set of methods, change of representation methods where you basically do an unsupervised re representation of your data and then you do supervised learning in the new space so.",
                    "label": 1
                },
                {
                    "sent": "I imagine you had data like this an you you might try to re represent it.",
                    "label": 0
                },
                {
                    "sent": "I mean if you could to something simpler where you've captured this manifold structure in a simpler way, and then you can run a classifier and you know people use things like spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "So to do that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so spectral clustering on the unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and then you could run something like an SVM.",
                    "label": 0
                },
                {
                    "sent": "As in the second stage, I won't talk about SVM's.",
                    "label": 0
                },
                {
                    "sent": "We just to say, yeah, part of what they do is they use kernel functions.",
                    "label": 0
                },
                {
                    "sent": "When you're optimizing the jewel, so you get this K matrix again.",
                    "label": 0
                },
                {
                    "sent": "It looks like the K matrix of the graph.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithm I just talked about.",
                    "label": 0
                },
                {
                    "sent": "So because a lot of people have done these things called cluster kernels, which is basically just this change of representation techniques that I was talking about.",
                    "label": 1
                },
                {
                    "sent": "So because, yeah, they're just define the kernel matrix, which is just the way of defining distances to give to a support vector machine that tried to use the unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So two versions of one is a random walk kernel where you try to make kij, so the dot product between point Iron Point J to be something like the probability of walking.",
                    "label": 0
                },
                {
                    "sent": "Along a graph and from I and ending up at J.",
                    "label": 0
                },
                {
                    "sent": "So taking into account all the paths so this could be a good idea, because if you're in a manifold or a cluster there will be lots of paths to get from.",
                    "label": 0
                },
                {
                    "sent": "I to J and you'll have a higher probability so.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a way of doing that.",
                    "label": 0
                },
                {
                    "sent": "You basically do like it and I'll be F Kern or something like that and normalize it so they look kind of like probabilities and take it to the power of T and then that will give you walking T steps so it will be the probability of walking T steps.",
                    "label": 0
                },
                {
                    "sent": "There's some other bits you have to do, but it's kind of like that an and that kind of works and then another way is just using spectral clustering.",
                    "label": 1
                },
                {
                    "sent": "Which I won't really talk about, but.",
                    "label": 0
                },
                {
                    "sent": "That gives you spectral clustering doesn't actually give you a clustering per say.",
                    "label": 0
                },
                {
                    "sent": "I mean, it doesn't say this point belongs to class one or two, it gives you.",
                    "label": 0
                },
                {
                    "sent": "I mean the whole steps of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "People normally use K means at the end or something like that.",
                    "label": 1
                },
                {
                    "sent": "Just just before that, what you get is a new representation of your data.",
                    "label": 0
                },
                {
                    "sent": "Where, where K means or something works better than in the beginning.",
                    "label": 0
                },
                {
                    "sent": "So you could just stop there and then give that to your support vector machine.",
                    "label": 0
                },
                {
                    "sent": "That's what people have tried and that also can work.",
                    "label": 1
                },
                {
                    "sent": "So these methods have shown could performance, but they can be slow because they involve diagonalizing a matrix.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They have a lot of parameters.",
                    "label": 0
                },
                {
                    "sent": "And then there's tears, themes, transductor there seems.",
                    "label": 0
                },
                {
                    "sent": "So again, they want to do something.",
                    "label": 0
                },
                {
                    "sent": "Like that, they all want to do this kind of thing, so where where you find a classification in region of low density and the way they do that is well efms they try to maximize the margin on labeled data, so you find a large margin here between these two points.",
                    "label": 0
                },
                {
                    "sent": "But transductive SVM is find a large margin on both the labeled an unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So here this decision rule, which is a nonlinear one doesn't pass close to the unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "So you have a large margin on them as well, so if you can implement this principle, you're effectively implementing the cluster assumption and it you know it looks.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like it's going to do something like the graph based methods, but it doesn't use KNN or K nearest neighbor type algorithm as a base algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's using SVM, so maybe it's a little better.",
                    "label": 0
                },
                {
                    "sent": "The trouble is, this optimization problem is non convex, so this is the normal SVM optimization problem in the primal.",
                    "label": 0
                },
                {
                    "sent": "So you just minimizing the training error and then you you minimize W squared which gives you a large margin.",
                    "label": 0
                },
                {
                    "sent": "But then you add on this extra term.",
                    "label": 0
                },
                {
                    "sent": "Which basically is for the unlabeled points only an it's a loss function on the.",
                    "label": 0
                },
                {
                    "sent": "The prediction of an unlabeled point.",
                    "label": 0
                },
                {
                    "sent": "So if here along this graph I've got the prediction of an unlabeled point, so F of XI star, if it's zero, it means it passes.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly through my hyper hyper power or decision rule, so it's like here so you don't want that right?",
                    "label": 0
                },
                {
                    "sent": "You're trying to push it.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why so?",
                    "label": 0
                },
                {
                    "sent": "You have a large margin between these points, so if you have a high penalty when it's zero, and so that's basically it.",
                    "label": 0
                },
                {
                    "sent": "So if your point.",
                    "label": 0
                },
                {
                    "sent": "And unlike what point is in the middle of the margin, it sort of here taking a high cost and as soon as it chooses one side or the other, and you don't know which side, you don't know which label is you get less cost, so it pushes it one way or the other.",
                    "label": 0
                },
                {
                    "sent": "And that's why it's nonconvex.",
                    "label": 0
                },
                {
                    "sent": "Because you don't know which side if you like so.",
                    "label": 0
                },
                {
                    "sent": "And then there's several implementations of that.",
                    "label": 0
                },
                {
                    "sent": "One way to optimize it is to do this kind of simple switch.",
                    "label": 0
                },
                {
                    "sent": "You switch labels so you train on your unlabeled data as if it's labeled, and then you switch the labels to try and find the right ones, so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you try to find that this one you know belongs to the same class as these guys and this.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Online same classes, these ones and you switch them to opt to find the minimum of this function, which is a bit.",
                    "label": 0
                },
                {
                    "sent": "Heuristic and kind of slow, but you could also.",
                    "label": 0
                },
                {
                    "sent": "Just try to.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's what Chappelle and Xena have done.",
                    "label": 0
                },
                {
                    "sent": "Just optimize this primal thing directly with gradient descent, so they use this function so it's smooth and just did just compute the gradient of this thing and try to minimize it.",
                    "label": 0
                },
                {
                    "sent": "And that works a little bit better, but.",
                    "label": 0
                },
                {
                    "sent": "It still has this.",
                    "label": 0
                },
                {
                    "sent": "Cubic speed in the number of labeled and unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "And they have to do some tricks like use kernel PCA for nonlinearity as well.",
                    "label": 0
                },
                {
                    "sent": "'cause this is only in the for the linear case.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so far I've just talked about.",
                    "label": 0
                },
                {
                    "sent": "These three sort of typical methods that people are using right now for a semi supervised learning and here's a little comparison of them on a couple of problems.",
                    "label": 0
                },
                {
                    "sent": "Small scale problems.",
                    "label": 0
                },
                {
                    "sent": "So, uh, on USPS, this is digit recognition problem with only 32 labeled points and 4000 unlabeled points.",
                    "label": 1
                },
                {
                    "sent": "And for document classification problem, these are both just two class problems where you have two 2000 unlabeled points and 16 labeled points.",
                    "label": 0
                },
                {
                    "sent": "So you can see that compared to a support vector machine you can get an improvement in accuracy, but it's kind of not that impressive for the SVM here and actually the label propagation does well.",
                    "label": 0
                },
                {
                    "sent": "Everything is better than nearest neighbor though, and for the text classification problem the SVM is doing well and the label propagation doesn't work.",
                    "label": 0
                },
                {
                    "sent": "So I mean this one people argue is because label propagation doesn't really work in high dimensions 'cause it's kind of based on density estimation which kind of breaks breaks down in high dimension.",
                    "label": 0
                },
                {
                    "sent": "But this one why it's so good.",
                    "label": 0
                },
                {
                    "sent": "I'm not so sure.",
                    "label": 0
                },
                {
                    "sent": "But essentially there will.",
                    "label": 0
                },
                {
                    "sent": "They are all helping to improve.",
                    "label": 0
                },
                {
                    "sent": "Performance on these problems, but it looks like you really need.",
                    "label": 0
                },
                {
                    "sent": "Have more more unlabeled data to make your bigger impact.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you're going to need some large scale semi supervised learning, so speeding up these algorithms I think I mentioned briefly label propagation.",
                    "label": 1
                },
                {
                    "sent": "You can do a few things like sparsifying the graph and some people have tried approximating the K matrix by making some examples expressed as linear combinations of others trying to try to make this key matrix easier to deal with, so it's much faster, but I'm still.",
                    "label": 0
                },
                {
                    "sent": "Not so impressed that that's going to go anywhere to be honest, but still.",
                    "label": 0
                },
                {
                    "sent": "You can try and.",
                    "label": 1
                },
                {
                    "sent": "Further cluster kernel so the re representation approach, I mean one obvious thing is basically you split the problem into 2 problems.",
                    "label": 0
                },
                {
                    "sent": "You're running dimensionally dimensionality reduction algorithm or clustering algorithm.",
                    "label": 1
                },
                {
                    "sent": "Then you're running a supervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "So to make it fast you just make both things really fast right?",
                    "label": 0
                },
                {
                    "sent": "So I mean why not use something fast like K means in the first stage and then try and use a fast classifier in the second stage?",
                    "label": 0
                },
                {
                    "sent": "So that's kind of obvious.",
                    "label": 0
                },
                {
                    "sent": "But then a little bit more tricky.",
                    "label": 0
                },
                {
                    "sent": "If you want to speed up the the regularization based methods like T SVM which have this extra.",
                    "label": 0
                },
                {
                    "sent": "Regularizer to deal with the unlabeled data because for things like T SVM is not convex anymore so.",
                    "label": 0
                },
                {
                    "sent": "A lot of the tricks that people are using to speed up SVN's and things like that aren't aren't going to work.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So 11 obvious way that people have tried is simply to take this switching heuristic and trying and fix it by switching lots of things at once and showed that there's some speed up there.",
                    "label": 0
                },
                {
                    "sent": "But what I'm going to talk about is their concave convex minimization procedure that's been applied to T SVM and seems to be.",
                    "label": 0
                },
                {
                    "sent": "Really, a nice clean way of optimizing that.",
                    "label": 0
                },
                {
                    "sent": "That's that's quite fast.",
                    "label": 1
                },
                {
                    "sent": "And just to say about scaling, because all these things here I'm talking about support vector machines.",
                    "label": 0
                },
                {
                    "sent": "If you have a linear classifier, so no kernels, then you can get linear scaling with a number of examples.",
                    "label": 0
                },
                {
                    "sent": "So and there's things like SVM, Lin like you can download that do things like that.",
                    "label": 0
                },
                {
                    "sent": "But if you have a nonlinear classifier using kernels, then it's not so great.",
                    "label": 0
                },
                {
                    "sent": "Then you have more like.",
                    "label": 0
                },
                {
                    "sent": "A quadratic tendency.",
                    "label": 0
                },
                {
                    "sent": "So that's just to say well, but a lot of the things I'm talking about, or in this nonlinear case.",
                    "label": 0
                },
                {
                    "sent": "But if you want to go faster, you can just go back to the linear case at least, or else you something like.",
                    "label": 0
                },
                {
                    "sent": "Your networks.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Austin an so.",
                    "label": 0
                },
                {
                    "sent": "For fast cluster kernels, one.",
                    "label": 1
                },
                {
                    "sent": "One easy trick is to take her fast clustering algorithm, for example, like K means.",
                    "label": 0
                },
                {
                    "sent": "So that can be much faster in spectral clustering where you have to, you know dialogue, diagonalize a big matrix.",
                    "label": 0
                },
                {
                    "sent": "So I won't talk about K means.",
                    "label": 0
                },
                {
                    "sent": "But just to say what you do with it afterwards.",
                    "label": 0
                },
                {
                    "sent": "So here's some quick thing that you can try.",
                    "label": 1
                },
                {
                    "sent": "You can run K means end times and then you construct a kernel matrix for a support vector machine where you say the kernel between I&J is the number of times inj or in the same cluster over the number of.",
                    "label": 0
                },
                {
                    "sent": "Runs you tried the end over end so that kind of gives you like a probability that they rely on the same cluster, because K means every time you run it you get different result and then you can do something like take the product between two kernels, which is still a valid kernel.",
                    "label": 0
                },
                {
                    "sent": "So your original kernel and this new.",
                    "label": 0
                },
                {
                    "sent": "Bags K means kernel will effectively scale your original kernel, taking into account of the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of the original kernels rescaled if you like by the probability that two points are in the same cluster.",
                    "label": 1
                },
                {
                    "sent": "So you're if they're not in the same cluster, you're going to lower that down to zero.",
                    "label": 0
                },
                {
                    "sent": "That dot product and and it's not going to try to relate them to each other, so it's going to take account of this clustering.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For May shun.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the original space, yeah.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 1
                },
                {
                    "sent": "Yes, your assumption that you can do a good clustering with K means has to be true, otherwise K means.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Profile so and I tried this in the context of protein super family classification, so this is averaged over 54 target families.",
                    "label": 1
                },
                {
                    "sent": "So you try to predict whether a protein belongs to super family or not.",
                    "label": 0
                },
                {
                    "sent": "So I know if you know anything about proteins, but they are organizing to families, super families and folds.",
                    "label": 0
                },
                {
                    "sent": "It's kind of hierarchy of labels and well, you don't really need to know that to look at this.",
                    "label": 0
                },
                {
                    "sent": "These results, but the error rate is measured using either RAC or RAC 50 where you look up to the first 50 false positives in your rank list so you care more about the top of the ranking.",
                    "label": 0
                },
                {
                    "sent": "So this kind of more relevant this rock 50 result and I compared efms so you get Rs so a large score is good here.",
                    "label": 0
                },
                {
                    "sent": "If .4 and then I was trying.",
                    "label": 1
                },
                {
                    "sent": "Spectral clustering cone or the random walk kernel and you get like .5 eight .69.",
                    "label": 0
                },
                {
                    "sent": "And with this camins kernel, you don't actually do that back you're doing.",
                    "label": 0
                },
                {
                    "sent": "Pretty much like these other slower cluster kernels amazingly, but you can be much faster and they also happen to be better than than T SVM.",
                    "label": 0
                },
                {
                    "sent": "But that was with the.",
                    "label": 0
                },
                {
                    "sent": "3000 labeled points and 3000 unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "But then if I go up to 30,000 unlabeled points, which I can't run, the spectral clustering algorithm on, I get.",
                    "label": 0
                },
                {
                    "sent": "Better results again.",
                    "label": 0
                },
                {
                    "sent": "So now this is really quite significantly.",
                    "label": 0
                },
                {
                    "sent": "Better than the axiom, sorry.",
                    "label": 0
                },
                {
                    "sent": "Starting to do something useful.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I guess I'm way overtime.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so and then, here's a way of speeding up transductive SVM's.",
                    "label": 0
                },
                {
                    "sent": "So as I said, the problem is is non convex and actually happens that you can split that problem into a convex and concave part.",
                    "label": 1
                },
                {
                    "sent": "So the whole SVM part is convex, but then the other part.",
                    "label": 0
                },
                {
                    "sent": "That's this function.",
                    "label": 0
                },
                {
                    "sent": "You can split that into a convex and concave park.",
                    "label": 0
                },
                {
                    "sent": "And then there exists this algorithm that if you can do that for optimizing such problems, so you can optimize some some objective J, where if you can split it into these two parts and the way it works is iteratively, you optimize basically the convex part fully plus an approximation of the concave part is just by linearizing it using its tangent.",
                    "label": 0
                },
                {
                    "sent": "So this whole thing is still convex.",
                    "label": 0
                },
                {
                    "sent": "And you can prove the running that iteratively.",
                    "label": 0
                },
                {
                    "sent": "You will converge to to the.",
                    "label": 0
                },
                {
                    "sent": "A local minimum of the whole non convex problem.",
                    "label": 1
                },
                {
                    "sent": "And also decreases each iteration.",
                    "label": 0
                },
                {
                    "sent": "So armed with that, we can simply apply it to the T SVM problem and we avoid these kind of switching label heuristic.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Books and things like that, and it turns out that that's actually much faster.",
                    "label": 0
                },
                {
                    "sent": "The algorithm looks like this, but I don't think I'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "Is just to say though, in each iteration it kind of looks like an SVM, so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The speed of this whole algorithm is basically.",
                    "label": 0
                },
                {
                    "sent": "If you, I mean it's a constant times the speed of the SVM 'cause it is typically 5 to 10 iterations of running in SVM, so an you can use in each iteration.",
                    "label": 0
                },
                {
                    "sent": "You're kind of like the last one, so you can use some restarts as well, so maybe it's not just five times slower like that.",
                    "label": 0
                },
                {
                    "sent": "And that also.",
                    "label": 0
                },
                {
                    "sent": "Works.",
                    "label": 0
                },
                {
                    "sent": "Very well compared to SVM like see SVM and the nabla TSV M of Chappelle and Zoom on these are just a few small scale datasets.",
                    "label": 0
                },
                {
                    "sent": "We're just taken from from the paper of Chappelle.",
                    "label": 0
                },
                {
                    "sent": "You just replied to the same problems so.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It works.",
                    "label": 0
                },
                {
                    "sent": "And then if you look at the training time there as a function of the number of unlabeled examples, this is actually.",
                    "label": 1
                },
                {
                    "sent": "So this is the time so.",
                    "label": 0
                },
                {
                    "sent": "SVM Lite, transductive SVM, is getting much lower as the number of training examples increases.",
                    "label": 1
                },
                {
                    "sent": "Well, we're also getting slower, but we're down here.",
                    "label": 0
                },
                {
                    "sent": "This is the CCC PTSM an same behavior on two different datasets.",
                    "label": 0
                },
                {
                    "sent": "And like for example here with 2000 unlabeled examples, it's 133 times faster than SVM light, and that gap is going to get bigger as well, with more unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "So it's really.",
                    "label": 0
                },
                {
                    "sent": "Working much better than.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The heuristic.",
                    "label": 0
                },
                {
                    "sent": "And this is just a plot to show that that's the iterations of the CCP.",
                    "label": 1
                },
                {
                    "sent": "We're plotting the objective function and the test error, and you show this converges very quickly actually, because the algorithm for CCP only said that there is a local minima and it decreases on each iteration.",
                    "label": 1
                },
                {
                    "sent": "You didn't say how many iterations, so this is just empirically to show.",
                    "label": 0
                },
                {
                    "sent": "Actually is quite only few iterations.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "So you can try that now on some some bigger datasets.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The result here is not not so great, I would say actually because so here with 100 labeled points, an increasing number of unlabeled points, you can see a drop in error is quite nice, but if you have only if you have 1000 labeled training points, so more more training points that are labeled, then this decrease in error is not very big anymore so.",
                    "label": 0
                },
                {
                    "sent": "Snow.",
                    "label": 0
                },
                {
                    "sent": "That's overall.",
                    "label": 0
                },
                {
                    "sent": "That's just a two class problem.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I didn't say.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which one is?",
                    "label": 0
                },
                {
                    "sent": "An hour MNIST you get something something similar.",
                    "label": 0
                },
                {
                    "sent": "With with 1000 labeled points, you can get a decrease.",
                    "label": 0
                },
                {
                    "sent": "In error rate, if you go up to 40,000 unlabeled examples.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And it's just to show their training time is basically has a quadratic tendency with their number of unlabeled examples on these problems, which is as good as you can get with.",
                    "label": 1
                },
                {
                    "sent": "Support that nonlinear support vector machines because they already have that tendency.",
                    "label": 0
                },
                {
                    "sent": "So basically we have the same scaling as them.",
                    "label": 0
                },
                {
                    "sent": "If we did this in the linear case though, we would have a linear.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scaling.",
                    "label": 0
                },
                {
                    "sent": "So, but what you might be able to do is to avoid this kind of quadratic tendency is to do something like neural networks instead.",
                    "label": 0
                },
                {
                    "sent": "'cause there the problem with support vector machines is that F of X.",
                    "label": 0
                },
                {
                    "sent": "Uses, it grows as the number of unlabeled as a number of examples grow right?",
                    "label": 0
                },
                {
                    "sent": "Because there are a number of support vectors grow, so somehow if you were doing online learning, your model is always getting slower and slower, which isn't a very good property for large scale so.",
                    "label": 1
                },
                {
                    "sent": "Things like neural networks, on the other hand, have a fixed capacity.",
                    "label": 1
                },
                {
                    "sent": "They have a fixed size network, so you know how fast you're going to be.",
                    "label": 0
                },
                {
                    "sent": "Seems the nicer property, and you could easily apply this same kind of semi supervised regularizers to that as well.",
                    "label": 0
                },
                {
                    "sent": "And then you'd have better scaling an that's just a quick plot to show.",
                    "label": 0
                },
                {
                    "sent": "We just started doing that and it's working, but I don't have full.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results of that.",
                    "label": 0
                },
                {
                    "sent": "So then the summary.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised learning WHI because labels are expensive or incomplete and unlabeled data is cheap.",
                    "label": 1
                },
                {
                    "sent": "When when, when can you use it when P of X is useful basically so one of these assumptions has to be true.",
                    "label": 0
                },
                {
                    "sent": "I don't know quite how many datasets that's true and how many.",
                    "label": 0
                },
                {
                    "sent": "It's not true.",
                    "label": 0
                },
                {
                    "sent": "So obviously semi supervised learning researchers tend to pick the datasets where it where it works so.",
                    "label": 0
                },
                {
                    "sent": "I asked actually alleviation power is the.",
                    "label": 0
                },
                {
                    "sent": "Author of the semi supervised learning book.",
                    "label": 0
                },
                {
                    "sent": "How many he thought and he said maybe half half the datasets have these assumptions but I don't know.",
                    "label": 0
                },
                {
                    "sent": "And then how can we do it?",
                    "label": 0
                },
                {
                    "sent": "Well, we can use a semi supervised regularizer which we add to our supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Or we can implement that in a number of ways and.",
                    "label": 0
                },
                {
                    "sent": "We're not sure which is which is the best regularizer yet.",
                    "label": 0
                },
                {
                    "sent": "For example, it's been shown that if you add some of these regularizers together like the one of the TSV M, which pushes the examples away from the margin plus the one of the graph diffusion, which says two labels should be 2, two outputs should be the same if their points are close.",
                    "label": 0
                },
                {
                    "sent": "If you add these two regularizers together then actually you're better than using one or the other.",
                    "label": 0
                },
                {
                    "sent": "So somehow people haven't found what the best best regularizer is yet.",
                    "label": 0
                },
                {
                    "sent": "And then why?",
                    "label": 0
                },
                {
                    "sent": "Large scale?",
                    "label": 1
                },
                {
                    "sent": "Because lots lots of you need lots of unlabeled data because each unlabeled point has very little impact.",
                    "label": 0
                },
                {
                    "sent": "And then just a couple of ways of doing that.",
                    "label": 1
                },
                {
                    "sent": "Large scale.",
                    "label": 0
                },
                {
                    "sent": "A two stage approach of doing fast unsupervised learning plus fast supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And then the problem of the large scaling is no longer the problem for semi supervised learning research is it's more problem for people researching supervised and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Or else if you do a direct regularizer then you might need to do something clever like this.",
                    "label": 0
                },
                {
                    "sent": "SCCP optimization, so non convex optimization techniques can apply then and things are more tricky.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Detect yeah, certainly if you have better methods for unsupervised learning then you might be able to slot them right in.",
                    "label": 0
                },
                {
                    "sent": "Well the reason well semi supervised learning researchers hope the reason is because.",
                    "label": 0
                },
                {
                    "sent": "They have this cluster assumption that you know the the decision boundary lies.",
                    "label": 0
                },
                {
                    "sent": "In a region of low density so that even though the classes are overlapping, it's still lower density.",
                    "label": 0
                },
                {
                    "sent": "There where the overlaps occurring than than where the you know the cluster is there you know the centers of the clusters if you don't have that.",
                    "label": 0
                },
                {
                    "sent": "If the clusters overlap that there isn't a region of low density, then the cluster assumption isn't true, and those things won't help I guess, but because that is hopefully true, those those kernels are giving better results.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can easily make them, I mean free.",
                    "label": 0
                },
                {
                    "sent": "You can easily construct them.",
                    "label": 0
                },
                {
                    "sent": "Real world yeah, definitely.",
                    "label": 0
                },
                {
                    "sent": "I mean, I've tried it on some things and.",
                    "label": 0
                },
                {
                    "sent": "It didn't help, I mean.",
                    "label": 0
                },
                {
                    "sent": "Well, I was doing.",
                    "label": 0
                },
                {
                    "sent": "I was I was like trying to classify preferences of like movie preferences an I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't get an improvement there.",
                    "label": 0
                },
                {
                    "sent": "I thought I mean no, I didn't.",
                    "label": 0
                },
                {
                    "sent": "I mean The thing is, it might not be that problem per se.",
                    "label": 0
                },
                {
                    "sent": "It might have been my representation of the problem as well.",
                    "label": 0
                },
                {
                    "sent": "You know, like the feature representation I had was the people I guess along that's the normal 1, isn't it?",
                    "label": 0
                },
                {
                    "sent": "Which people like this movie kind of thing and you have that vector.",
                    "label": 0
                },
                {
                    "sent": "For each movie.",
                    "label": 0
                },
                {
                    "sent": "Oh, why didn't I don't know, but I was.",
                    "label": 0
                },
                {
                    "sent": "I was just saying it might not be the problem per say, but just my representation.",
                    "label": 0
                },
                {
                    "sent": "You know the features that I took for that problem, which were basically for each movie I'd have like which person liked it, which user liked it.",
                    "label": 0
                },
                {
                    "sent": "So then you know, if you take a dot product between 2 movies, it's high.",
                    "label": 0
                },
                {
                    "sent": "If the same users like them, right?",
                    "label": 0
                },
                {
                    "sent": "But whi that representation wasn't good.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Or maybe there's there's no representation for that problem, although I believe there is because I think you know their Netflix competition.",
                    "label": 0
                },
                {
                    "sent": "Mintons results, which are one of the best or the best.",
                    "label": 0
                },
                {
                    "sent": "He used to kind of transduction where he used, like the unlabeled set the test set.",
                    "label": 0
                },
                {
                    "sent": "To improve his results.",
                    "label": 0
                },
                {
                    "sent": "But he didn't say exactly so I don't think he said you.",
                    "label": 0
                },
                {
                    "sent": "The improvement from either using or not using it, but just extrapolating that you beat everybody else was maybe that was one of the reasons.",
                    "label": 0
                }
            ]
        }
    }
}