{
    "id": "i2lncuafc2uls5goywaubkv6em5kw4mb",
    "title": "Kernel Methods and Support Vector Machines",
    "info": {
        "author": [
            "John Shawe-Taylor, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/mlss09us_shawe-taylor_kmsvm/",
    "segmentation": [
        [
            "OK, at this point I would like to introduce our first speaker, tutorial Speaker John Shaw, Taylor University College of London.",
            "Speaking on kernel methods and support vector machines, thank you."
        ],
        [
            "OK, thank you very much and.",
            "It's a pleasure, great pleasure to be here.",
            "Thank you to the organizers for inviting me.",
            "So the.",
            "I was asked to do a sort of tutorial on kernel methods and support vector machines.",
            "It's quite a tricky one in that I think you know probably the majority of you will know a lot about kernel methods already and and and support vector machines probably have used them.",
            "So in a way, it's a.",
            "It's a difficult one to pitch, so my aim is to try and give you the big picture.",
            "So to actually sort of take you through the basics of what, yes?",
            "OK, is that better?",
            "Thanks, please do Introducton to interrupt if you have questions or problems with what the microphone or whatever.",
            "So I'm going to take you through sort of the try and put it in perspective.",
            "What the kernel methods are trying to do.",
            "Perhaps also some of their limitations, but but also, of course you know their strengths, and I think the thing I'll really try and emphasis emphasize is the plug and play in nature that you've got a sort of group of methods that can be used to analyze different data, different algorithms, different pre processing, and so on.",
            "And it's sort of a package which you can actually draw tools and algorithms out of for your particular application.",
            "So that's my that's my aim."
        ],
        [
            "So here it is.",
            "I the tutorial will sort of start at the top level.",
            "You know why linear pattern functions wire kernel approach.",
            "But as I said the emphasis will be on trying to give you this flavor of the plug and play.",
            "So please bear with me if some of the stuff I say is very familiar with to you.",
            "I apologize ahead for that, but I've I've sort of deliberately tried to make it accessible to it.",
            "People who maybe haven't seen kernel methods before."
        ],
        [
            "Um?",
            "I won't be looking at sort of other approaches to pattern analysis.",
            "I won't be attempting a complete history of who did what, or you know who thinks they you know did something first or whatever.",
            "More importantly, I won't be actually talking about the Bayesian method.",
            "I'll be method or view of of kernel methods.",
            "Will allude to it a couple of times and will mention more about it in my talk this afternoon, but as far as this tutorial is concerned I'm taking the mainly the traditional kernel view and I won't be talking about more recent developments.",
            "So I."
        ],
        [
            "Divide it into four parts which doesn't sit too well because we're planning to have three sessions of an hour and then 1/4 of an hour break.",
            "So that's the aim anyway, but we probably won't strictly strictly hold too.",
            "You know, an hourly session, but I'll try, you know, fit in a couple of breaks at some points, so the four parts are basically the introduction sort of motivation and a sort of worked example, just so you get the flavor of the whole thing.",
            "Then I'll talk about the pre processing stage.",
            "Then the sort of group of algorithms with support vector machines as the prime example here and then finally look at some of the kernel design strategies.",
            "So in terms of an application, what one would typically be doing is actually doing it probably in the reverse order.",
            "You'd be thinking of OK.",
            "I've got this kind of data.",
            "I'll need to design kernel for it.",
            "Maybe there's one that I can pick off the shelf, or maybe I need to adapt it.",
            "Then I have to think about which algorithm I'm going to apply, and perhaps after.",
            "Looking at that, think about some pre processing, so it's probably completely the wrong order in terms of applications, but it sort of seemed a natural way to do it when I was putting it together.",
            "OK, so."
        ],
        [
            "Introduction then.",
            "So a very brief history of how things came about.",
            "I think that's worth mentioning just to kind of put in perspective why kernel methods came rather late in the day.",
            "Or at least there there's adoption came late in the day.",
            "Then go through this worked example, which will be Kernel Ridge regression and I'll mention briefly properties of kernels and and show exactly you know the benefit that you can gain from using a kernel approach.",
            "So."
        ],
        [
            "What is pattern analysis about?",
            "Of course, I'm sure you all have a good idea of this.",
            "You're in this area, but perhaps it's sometimes worth stepping back and thinking about it more generally.",
            "We're looking for patterns and you know, in historical terms.",
            "People have looked at various types.",
            "You know exact patterns were traditionally studied in in physics.",
            "You know the motion of the planets and so on.",
            "There are complex patterns which are just very sort of deterministic, but very complex functions.",
            "And then there are probabilistic funk patterns which are really just coming out of the uncertainty or the randomness in the world.",
            "And for instance trying to predict.",
            "The attitude of buyers in a market is is a typical example of that.",
            "However, I should mention that often complex patterns there may be underlying deterministic function, but it's better to try and learn it as a probabilistic one because you're never going to be able to capture all of the details of the actual complex phenomenon.",
            "So actually using a probabilistic approach can be a good tactic.",
            "Um?",
            "What is the aim game is that we would like to understand the regularity's in the data.",
            "The data may look very complex or random, but there are possibly underlying regularity's and the machine learning or the pattern analysis approach is attempting to extract those irregularities from the data in order to make benefit in terms of predictions of future activity or analyzing the best way to actually use or manipulate that data that situation.",
            "So pattern analysis is the study of automatic detection of patterns in data is probably a nice way of viewing it."
        ],
        [
            "You can think of patterns of sort of exact patterns as defining some function which for all data is equal to 0, some sort of.",
            "Deterministic function approximate patterns are ones where the function is approximately 0.",
            "An statistical patterns are ones where it's expectation is approximately 0, so this is the kind of thing where we're thinking of when we're doing the probabilistic pattern analysis or machine learning."
        ],
        [
            "And the properties we've like of our algorithms are.",
            "They should be computationally efficient.",
            "We would like them to run in polynomial time and hopefully have a fairly low degree in order to scale to large datasets.",
            "We'd like them to be able to handle noisy data app some of the data we've been given is not correctly classified or correctly labeled.",
            "Maybe noisy sensor data in in your inputs and the outputs.",
            "Maybe only measured to certain accuracy.",
            "You'd like your algorithms to be able to handle that type of data.",
            "And more importantly, perhaps the most important feature that I think is is.",
            "New and interesting in machine learning is the statistical stability.",
            "That is, we want to be able to distinguish between patterns that have arisen by chance in a particular data set.",
            "Just because it is a finite data set and ones which really characterize the underlying source of the data, and the hope is that we're learning those more stable or underlying patterns that will actually be visible on new data, because that's where we're hoping to make benefit by applying that what we learned on our.",
            "Finite sample of data to new data, and we're hoping that that features or or pattern that we extracted will actually be present in the new data and therefore give us the benefit that we're hoping for.",
            "So this is perhaps the key.",
            "I won't be actually speaking a lot about this in this.",
            "Tutorial I'll allude to it and give the example of support vector machines and come back to it in in in more detail in the afternoon lecture that I have this afternoon."
        ],
        [
            "OK, so brief historical perspective.",
            "So machine learning really grew out of a study of neural like structures and they were first considered seriously in the 1960s with such systems as the perceptron and the perceptron algorithm.",
            "It caused a lot of excitement.",
            "It seemed that there was an algorithm that could learn patterns in data.",
            "And mimic in some sense the way that biological systems learn.",
            "So this was caused a lot of excitement, but the patterns that were learned were just linear.",
            "But there was a nice convergence theorem that showed that if there was a a classifier that could separate the data nontrivially, then there was the algorithm would in fact find that pattern.",
            "But there was a lot of discussion about this and a book by Minskin peppered rather sort of laid into the approach as being limited in its complexity.",
            "The complexity of the patterns that it could analyze, and this led to a fall off of interest at the end of the 1960s.",
            "It was not until the 1980s that the ideas came back in force and this was with the resurrection of the multilayer perceptron idea.",
            "These were networks of perceptrons with continuous activation functions, so called neural networks, and again the same sort of level of excitement was generated.",
            "Everyone was, you know, getting very sort of attracted to the idea that we were finally capturing something about biological intelligence.",
            "Human intelligence perhaps.",
            "But the algorithms were very, very slow and there was also limited statistical analysis and so things actually weren't as good as they might have appeared, or certainly as they were.",
            "Hyped up to be at that point.",
            "It was."
        ],
        [
            "This time in the early 1990s that the.",
            "Kernel methods was was introduced or I should say, reintroduced.",
            "I'll mention later the first idea of using a kernel was actually back in 1964.",
            "Believe it or not, where somebody implemented the perceptron algorithm with the kernel in in the USSR.",
            "So the Eisaman ET al.",
            "I'll I'll refer to that, but it was reintroduced in the 1990s.",
            "So the the the problem that.",
            "The perceptrons face was their limited.",
            "Functionality just taking linear functions in in the input space was was restricting their power to express patterns, and so the tack that had been taken with the multilayer perceptron was to glue these things together to make a more complex network and hence up the computational power.",
            "Kernel methods take a different approach.",
            "What they do is they stick with the linear functions, but they apply them in a high dimensional space, so they project the input data into a more complex feature space and then learn with linear patterns or linear functions in that space so they retain the benefits of learning linear functions.",
            "Which are, you know, computationally tractable, and there's a unique optimal solution, and so on.",
            "But gain the power from this more complex feature space.",
            "Um?",
            "Now there's a danger there because using two complex a feature space can also cause overfitting.",
            "But then statistical analysis showed that this large margin idea, and again I'll come back to that actually overcomes this curse of dimensionality.",
            "So this was really the breakthrough and the final sort of piece of the jigsaw was that you don't actually have to work explicitly in that high dimensional space, so there's a sort of advantage of using high dimensional space.",
            "The disadvantage is overcome by somehow controlling the complexity and the complexity.",
            "Computational is overcome by this so called dual representation, so I'll take you through all of those, but I'm just trying to give you the the sort of how it fits into the overall picture.",
            "So after this first method of support vector machines, it was rapidly expanded to other other tasks other than classification.",
            "In fact, we're going to start with with the Ridge regression example, but I'll mention others as well.",
            "Novelty detection and of course you know kernel, PCA and some of these other methods that operate in a kernel defined feature space."
        ],
        [
            "OK, so.",
            "The kernel methods approach the data is embedded into a Euclidean feature space.",
            "Linear relations are sort among the images of the data.",
            "The algorithms implemented must only require inner products between vectors, and this is how we avoid the actual explicit projection into the feature space.",
            "And the final ingredient is that the inner product between images of two points in the input space.",
            "So you project two points from the input space into the feature space, compute the inner product that would involve.",
            "Of course, if you did it explicitly, computing in the feature space itself, but the so called kernel function is a shortcut that computes that directly on the input vectors, so it actually shortcuts the actual computation of the projection and the.",
            "In a product, so that's what a kernel function does for you, and I'll give examples again in a minute."
        ],
        [
            "So this is the idea, sort of top level idea.",
            "You've got an input space in which the data is not linearly separable, so there's not a straight line that separates this data.",
            "You projected into some high dimensional.",
            "Here it's the same dimension obviously, but you know this is the idea is this is some much higher dimensional space.",
            "Now there is a linear separation of the data for the positive and negative data, and so you actually learn this separation in the high dimensional space and it corresponds to a non linear separation.",
            "In the input space, because the effect is is the way that the five Maps things creates a nonlinearity in the in the mapping.",
            "So the FI embeds the data into a feature space where the nonlinear pattern Now appears linear.",
            "The kernel computes the inner product in the feature space directly from the input.",
            "So this is the shortcut computation.",
            "Do the projection of X do the projection of said compute the inner product and that is computed directly by this kernel function?"
        ],
        [
            "OK.",
            "So now I'll take this worked example of Ridge regression.",
            "Sorry, I must keep the microphone so this is just to show how things work out.",
            "In one practical example and hopefully it will cement the ideas that I've given you sort of.",
            "Add a brush stroke level.",
            "So far, so all of the ingredients come into this example.",
            "So we're interested in linear function.",
            "And we're just going to consider now a linear function in the input space initially will then think of doing it in a feature space.",
            "So at the moment this is just a linear combination of the input features and this is just notation I will use interchangeably these notations.",
            "This is a more traditional inner product notation.",
            "This is just thinking of these column vector with a dash indicating a transpose, so this is now a row vector times the column vector, which is just the same thing.",
            "It's the inner product between those two vectors.",
            "And this is writing it out explicitly, so this is just to get you familiar with the notation.",
            "I use these interchangeably.",
            "Again, you know if anything is unclear.",
            "Stick your hand up and I'll I'll probably ignore you, but I'll try and hopefully notice.",
            "OK, so the aim of Ridge regression is to find the best interpolate of some data where you have some inputs X one up to XMN corresponding outputs which are real values Y one up to YM.",
            "So we're thinking them as as labels, but their real valued outputs."
        ],
        [
            "And so if we want to think of it in terms of those pattern functions I described, we need to create a function which we're hoping is going to be small or close to 0 in expectation, and the way to do that is to take the output of our linear function, which I'm denoting by G of X and subtract the output.",
            "The correct output from it, and square the difference.",
            "So this is the squared error if you like in the output of the function on this particular example.",
            "And we're thinking of that as our pattern function, and we're hoping that that's going to be small, both on training and and test data.",
            "So FG is our function with.",
            "We're hoping to minimize.",
            "So OK, I'm going to now use some notation to make this sort of explicit mathematically.",
            "I'm going to use a matrix X to denote my examples from that."
        ],
        [
            "Training set here.",
            "This is X one up to XM."
        ],
        [
            "And they're going to be the rows of X, so that X is going to have em rose and the length of the Rose is the dimension of the input vectors, so we might call that D. The weight vector is also D dimensional because it's in the same space as the.",
            "You know it's awaiting over the features.",
            "So if we now create this vector PSI which is y -- X times W X * W just evaluates each of the examples through the linear function and subtracting from Y gives us the residual.",
            "So this is the difference between the correct output and the.",
            "Actual output generated by the current weight vector, the current function.",
            "So this is a vector of residuals, and now we're trying to actually minimum."
        ],
        [
            "Size a ^2 error so the square those residuals, the norm squared of that vector is the sum of the squares of those residuals.",
            "So this is our error term and what we do in Ridge regression is actually introduce a regularization term which is controlling the norm of the weight vector that we use.",
            "So we trade off, we minimize W over a term that involves the norm squared of W. Scaled by a parameter Lambda, which is referred to as the regularization parameter plus this error term, which is dependent on W through that funk."
        ],
        [
            "And G that's all through this.",
            "XW here so excited.",
            "Depends on W as well."
        ],
        [
            "So this is our error term, so the choice of W will attempt to make the error term small, but not if that involves making a very very large norm weight vector, so it's trading complexity of the weight vector with the error on the on the training data.",
            "So this is the optimization that is adopted for Ridge regression.",
            "So let's just run through it.",
            "See what happens.",
            "Well, we can explicitly right outside squared as y -- X, W, y -- X W in a product with itself.",
            "If we multiply out this inner product, we get the following.",
            "And then if we set the derivative of this quantity equal to zero, we get the following.",
            "The equation XX primed XW that comes from this term here, which with a factor 2.",
            "But I've cancelled the two and we take the X primed Y onto the other side and this Lambda W comes from the derivative of this term.",
            "So this is the derivative of this set equal to 0 and treated as an equation.",
            "OK, so this is what we're trying to solve and clearly we can solve it by just inverting this matrix and that in."
        ],
        [
            "Indeed, gives us what's known as the primal weight vector solution.",
            "So we use the term primal to mean the explicit representation in the of the weight vector in the feature space.",
            "Now in this case it's in the input space, but if we were doing it in a more complex feature space it would again be in that in that feature space.",
            "So this is the primal solution."
        ],
        [
            "Vector, all I've done is just invert this matrix and apply it to this side here and it's got by just setting the derivative of this equal to."
        ],
        [
            "0.",
            "So notice that because it's a linear function, we're optimizing, there's a unique solution well, and a quadratic loss, so it's a convex loss as a unique solution, and we can actually explicitly write it out, and we can find it by solving a set of linear equations.",
            "It's very straightforward.",
            "And if we want to progress on a new data point, we can just take the weight vector and take the inner product with that new data point.",
            "And here we have the explicit representation.",
            "So this is just regularised least squares regression so far.",
            "Referred to as Ridge regression.",
            "So now what I want to show you is that that can now be actually performed in a kernel defined feature space.",
            "So this is where we showing the power of a kernel method and how it can be used.",
            "Or this algorithm can be used in combination with the kernel defined feature space.",
            "So."
        ],
        [
            "OK.",
            "The key idea is that you need a so-called dual representation of that weight vector, so instead."
        ],
        [
            "Of that explicit vector, we're going."
        ],
        [
            "Express the weight vector W as a linear combination of the training data.",
            "So we're going to express the weight vector in terms of the training data.",
            "Here is the expression that we're looking at.",
            "W is just a sum with weighting coefficients.",
            "Alpha Rye of the training data.",
            "Now that's you might say, well, how.",
            "How do I know that's going to be the case?",
            "Well, in this case we can actually write that down that equation that we had for W. And we can actually express W by taking this onto the other side and dividing through by Lambda.",
            "We express W in terms of these other terms in the equation, and if we bring the factor X primed out front, we see in fact W is expressible as X primed times some vector which I'm demoting by here Alpha.",
            "So Alpha is just one over Lambda y -- X W. So we have OK.",
            "It involves W, but we've shown that the solution does in fact lie in the span of the training data by that manipulation.",
            "Well, in a sense that's almost a no brainer.",
            "I mean in it because what we're doing is.",
            "Computing inner products with training data.",
            "Putting into your solution vector some parts of the sum component which is orthogonal to the space spanned by the training data would appear very.",
            "You know it would play no role because it would have a zero inner product with all of the training data.",
            "So this extra component that will be orthogonal to the span of the training data would have no effect.",
            "And since we have that regularization term which is trying to minimize the norm, you would immediately throw that component away and get a lower optimization of your."
        ],
        [
            "Your objective here.",
            "Because throwing that component away would have no effect on this because it would have no effect on these inner products.",
            "The component is orthogonal to the span of the training data, so would have no effect on this, but it would reduce this because you've actually projected the data the the weight vector down into a into a smaller vector in this space spanned by the training data.",
            "So it's clear you know if you think about it that way, that the the weight vector will be in the span of the.",
            "Of the training data.",
            "Therefore, it will be expressible in this form."
        ],
        [
            "That I've put here, and indeed this just verifies that fact, but also explicitly computes the form of that Alpha.",
            "It will actually be of this form OK.",
            "So this is the so called dual representation.",
            "Now it's slightly at odds with the.",
            "Optimization theory, where you have primal and dual.",
            "Sometimes they coincide, sometimes they don't exactly support vector machines.",
            "There's a very nice coincidence between the primal and dual of representation that I've referred to here and the primal and dual in optimization theory, but I think if we're being honest we need to just say this is our terminology that we use in kernel methods, and it's loosely borrowed from optimization theory, but doesn't always exactly accord with optimization theory.",
            "So think of it as just a way of representing your your solution vector in terms of the training data.",
            "And now what we want to do is not try and learn W, but try and learn Alpha.",
            "So we've gotta re sort of jigging at the problem where now rather than try and learn W we try and learn Alpha.",
            "So we sort of implicitly learning W through learning it's dual representation.",
            "OK, well let's try and do that in this case.",
            "Here's our Alpha.",
            "Here's an equation for it, but unfortunately it involves W, so we're not going to get Alpha out of this equation because it's sort of a recursive.",
            "But what we can do is just substitute for this W what we know it can be expressed as in terms of Alpha, so we're just going to substitute X primed Alpha in here and will now have an equation involving only Alpha, so that's."
        ],
        [
            "What we do here?",
            "And this is the equation that you get Lambda Alpha equals y -- X prime X expr."
        ],
        [
            "And Alpha.",
            "OK, so that's just multiplied up.",
            "This Lambda y -- X X prime Alpha, so just sub."
        ],
        [
            "You did that in there.",
            "OK, so now rearranging terms we've now got the following equation.",
            "It looks very similar to what we had for the primal solution, but actually in some respects it's even simpler because now on the right hand side we just have Y."
        ],
        [
            "Whereas before we had ex prime."
        ],
        [
            "I on the right hand side here.",
            "And this term here was X prime X plus Lambda Rye in the equation for Alpha it's XX prime, so the order of these two is swapped and we've got rid of this X prime on the right hand side.",
            "In"
        ],
        [
            "In this equation here, right?",
            "So it's XX primed plus Lambda Rai.",
            "Of course the dimension now is different.",
            "The X primed X had dimension equal to the feature space dimension.",
            "Because of course the weight vector had has dimension equal to the feature space dimension.",
            "So the identity matrix also had the same dimension that we added, but in this case Alpha has dimension equal to the number of training points.",
            "Remember there was 1A for each training point we were learning the combination of training points that created the weight vector, so there's 1A for each training point, and so this is now.",
            "I MM I'm using to denote the size of the training set and XX primed is is also an M by N matrix.",
            "So now we can actually write the dual solution.",
            "It's it's a very similar problem to solve.",
            "It's a set of linear equations we need to solve.",
            "But as I said before, it's actually a different dimension to the set of equations we were solving for W. And now if we want to progress on a new point, what do we do?",
            "OK, we we take X prime times W but now W of course is X Capital X prime times Alpha.",
            "That's how it's expressed.",
            "So if we now.",
            "Write that out explicitly.",
            "Some Alpha XI by the linearity of the inner product, we can bring the sum outside and we actually express this G of X as the sum weighted sum of the inner products between the test point and the training data.",
            "OK.",
            "So now the key observation here is that we actually only need to compute inner product between test points and training data, and this matrix actually only involves inner product between training data, the entry the IG entry in this matrix is just the inner product between the ice and J training points.",
            "That's because remember the rows of X were the training data.",
            "Therefore the columns of X prime to the training data.",
            "So the IJ entry in this matrix is the I throw in a product with the Jason column, which is just the inner product between the I&J training points in the input space.",
            "The way I've done it, OK."
        ],
        [
            "So the key ingredients are you.",
            "Now I'm using this KA little bit, sort of suggestively here.",
            "Kernel to denote this because as I said, the kij entry is just the inner product between the- J for training points.",
            "The evaluation of a new point is again now the weighted weighting of these inner products with the.",
            "Knew test point and the training data weighted according to this Alpha Rye that we got by solving this set of equations here.",
            "So the key observation here is that both steps.",
            "This step here and this step here only involve inner products between input data points.",
            "OK, here we have X the test point and the training data.",
            "And here we just have inner products between training data.",
            "So remember what I said before the kernel provides a shortcut.",
            "To compute the inner product between projections of data points into a complex feature space and then take their inner products.",
            "OK, cool well.",
            "Here if we just substitute in some feature vectors fire Vex I5 XJ Ann.",
            "Similarly, here 5X5 XI.",
            "Some feature vector, then all we're needing here is the kernel function to compute that inner product.",
            "It's an inner product of projections into a feature space of input data.",
            "And so we can substitute a kernel function here as suggested by this K and Similarly here we have a kernel function between the test point and the training data."
        ],
        [
            "So this is the key idea.",
            "So it only involves occurrences of the inner product.",
            "A kernel function computes that inner product in the kernel find feature space, and so we can apply the Ridge regression algorithm in that high dimensional potentially high dimensional feature space by just."
        ],
        [
            "Substituting the kernel function for the computation of this matrix here, the IJ entry of this matrix here and the kernel function for the evaluation of this new point.",
            "So it's sort of a little bit of magic, you know.",
            "You kind of apparently operating in this very high dimensional space without actually explicitly computing in it.",
            "By using this this trick."
        ],
        [
            "OK, well you may say.",
            "Do such functions exist?",
            "I mean give me an example of.",
            "I mean maybe this kernel function is going to be just as difficult to compute as actually computing these explicit vectors and taking it in a product.",
            "So what I want to do now is show you an example very simple example just to convince you that yes, you know there are very natural ways of using defining kernel functions that do correspond to very interesting high dimension."
        ],
        [
            "Feature spaces.",
            "So here's the first sort of nontrivial example of a kernel function.",
            "It's the so-called quadratic kernel.",
            "So what you do is you simply take the inner product between the feature vectors in the input space.",
            "And then you just square that number.",
            "Time.",
            "So it's one extra computation, one extra multiplication that's required over and above what you would have done if you just done the computation anyway in the feature space, so it's certainly not an overload of computation here.",
            "You know this might be a D dimensional vector, so you'd have to do D multiplications and D -- 1 additions anyway.",
            "And all we're saying is do one more multiplication OK.",
            "So it's a very minor.",
            "Additional load Now what I can claim is this actually corresponds to an inner product in a in a complex feature space.",
            "Well, why OK?",
            "This is what I've written.",
            "I'm now just using a different notation here.",
            "X prime zed squared is just another way of expressing this, but now I can actually swap one of these two round the order.",
            "And change the bracketing and I get zed XX prime ZOXX prime does a matrix.",
            "And so this is a matrix with a row vector times a column vector.",
            "And in fact, this is now the Frobenius inner product between two matrices or the inner product between two vectors where you vectorize the matrix.",
            "Zed, zed primed and the Matrix XX primed if you're.",
            "You know, if you're not sure that you're happy with that.",
            "Derivation here.",
            "It's very easy just to write it out explicitly.",
            "Just write the thumbs down on a piece of paper and you'll see that it works out.",
            "It's very easy.",
            "This is just a you know, a slightly maybe slicker way of showing it, but if you're not happy, just write out the sons and you'll see that it actually works that this actually corresponds to an inner product between these this feature vector for X.",
            "Sorry, this work on this the wrong way around, but anyway, whichever this feature vector for X and this feature vector for zed.",
            "So what it what it corresponds to is taking instead of just say, D components.",
            "There are now D squared components where the components are the products of the features of the original feature vector.",
            "So there's a feature for XIXJ, and there's actually another feature of XJ, XI, so you sort of double counting actually get repeated features, But there's certainly N choose two different or N + 1.",
            "Choose two different sorry entries, do different features here so the feature space dimension has been significantly increased compared to the original input space, and I'll make that it."
        ],
        [
            "List it with an example of, say.",
            "Doing a regression over a set of images.",
            "So think of pixel vectors, which might be 32 by 32 pixel images.",
            "So the input space dimension is 1024 dimensions, so D in this case is 1024.",
            "If we use the quadratic kernel, we're implementing regression in a million dimensional space, right?",
            "Roughly 1000 squared or well 500,000 dimension.",
            "If you take into account some of them are the same, but certainly a very very high dimensional space.",
            "And the cost of doing it is actually lower than doing it in the original space because we're having to invert 1000 by 1000 matrix.",
            "That's the dimension of the training set.",
            "As opposed to 1024 by 1024 Dimensional Matrix, which we would have had if we'd done explicit computation in the input space.",
            "So we're running rich regression in a hugely more powerful space at no extra cost, or in fact reduced cost compared to the doing a primal sort of normal Ridge regression without the kernel trick.",
            "OK, there's a slight caveat to that that when you're evaluating on an you test point, it is going to be more complex because you have to run over in order to compute the.",
            "Let me go back to the evaluation on a test point here."
        ],
        [
            "In order to evaluate on test point, all of these alphas are going to be non zero and so you're going to have to compute the inner products between the test point and each of the OR the kernel functions are between the test point in each of the training data.",
            "So there's going to be 1000 of these to compute, so it's 1000 * 1000 'cause there are 1000 dimensional.",
            "So that's a million computations to do this, Whereas in if we had an explicit vector in the input in, in, the in the primal space it would just be 1000 computation, so that.",
            "That is a slight caveat here, but certainly in solving the."
        ],
        [
            "The problem actually learning the Alpha vector is less expensive than learning the primal vector in the original space.",
            "OK, so."
        ],
        [
            "What are the implications?",
            "So what we've actually done?",
            "We've performed linear regression in a very high dimensional, and in fact even can be done in infinite dimensional spaces efficiently.",
            "Through this kernel trick.",
            "It's equivalent to performing non linear regression in the original input space.",
            "So we've effectively moved to doing nonlinear regression in some sense.",
            "In the sense that the final function that you're actually learning has a form.",
            "In this case, where we use the quadratic kernel.",
            "This is the form it's a sum of Alpha right inner product XIX squared.",
            "So it's actually a nonlinear function is a quadratic polynomial function of the of the components of the input vector.",
            "So we're actually doing nonlinear regression, but in a way that retains the attractive features of linear regression that there's a unique global solution.",
            "We can find it fast by simple.",
            "Inversion of a of a matrix, so it seems to be, you know, magic the best of all possible worlds in a sense, but you know there must be some warnings.",
            "There must be some problems here and clearly there is the curse of dimensionality.",
            "Working in high dimensional spaces is dangerous and you are liable to overfit your data.",
            "I kind of alluded to how that's overcome by controlling that weight vector norm is effectively controlling the complexity, so it's sort of.",
            "Giving with one hand and taking back with the other, you're giving lots and lots of dimensions with the kernel trick, but you're actually trying to.",
            "You know, keep a tight control on their use through the regularization of the weight vector norm.",
            "So it's a kind of.",
            "Let's the data decide innocence.",
            "How much complexity is actually needed, and that seems to be a very effective way of operating you.",
            "You kind of.",
            "Don't try and apriori say how much complexity you need.",
            "You give a lot of complexity, but you try and keep tight control on its use.",
            "Hopefully that actually ends you up with the right level of usage of complexity for the problem that you're actually trying to solve, so that's one of the ways in which the curse of dimensionality is handled within kernel methods generally, and the same applies to support vector machines and an most of the other techniques are very similar approaches adopted in terms of controlling the complexity.",
            "So I'm doing a handwaving argument here.",
            "I'll give a little bit more of a concrete example of.",
            "Statistical analysis in the support vector case and as said, I'll come back to that this afternoon and talk about some of the pounds for analyzing that.",
            "OK."
        ],
        [
            "So.",
            "That completes my my my simple example.",
            "Hopefully it's giving you the flavor of what's what's involved, the components, so you need the kernel function.",
            "You need an algorithm that you can express in terms of inner products, both the training and the testing.",
            "And and then it's basically a plug and play, and all you need to do in order to to actually apply the algorithm is compute this kernel matrix from using your kernel.",
            "Your chosen kernel passed that to your algorithm.",
            "It turns away and comes back with a dual vector and then you use that to evaluate you test points.",
            "So it's a very kind of.",
            "You know?",
            "Ready to use approach which you can you know.",
            "Use the same algorithm with different kernels or different algorithms with the same kernel and so on.",
            "So there's a plug and play aspect here that I think is is very useful.",
            "And just to convince you that there are other kernels, I will at the end, you know, in the in the final section, spend a bit more time talking about different kernel strategies for defining kernels, but just here I wanted to mention this kernel, the Gaussian kernel.",
            "Which is a popular very popular kernel.",
            "And it actually sorry there's a missing square here.",
            "There should be that should be squared.",
            "The norm squared divided by two Sigma squared.",
            "This actually corresponds to using an infinite dimensional feature space.",
            "If you've got a continuous space in the input, so actually performing, say Ridge regression.",
            "Using this kernel will correspond to doing it in an infinite dimensional space, which is sort of an interesting property of these approaches.",
            "So what is it that makes a kernel function a kernel function?",
            "I mean, you might say, well, look, I'm not too worried about all this projection.",
            "Let me just define my similarity measure.",
            "Maybe it's a good projection and then I can just use it.",
            "All I need is to be able to compute the function the kernel function.",
            "And so you might ask, what are the properties of a function that makes it a kernel function makes it correspond to some projection into a feature space and some inner product.",
            "Well, clearly it needs to be symmetric, and it needs to satisfy this property, because this corresponds to the norm squared of the vector X in the feature space.",
            "I'll show you that in a minute, but those properties are not enough.",
            "So caveat here, just writing down any old function, the chances are it won't be a kernel function.",
            "And I'll describe what are the properties that are needed for a function to be a kernel function in the in the last part of the of the tutorial.",
            "OK.",
            "So.",
            "Hopefully that's set."
        ],
        [
            "The scene.",
            "So what I want to do now is just say, OK, we've got this technique of representing vectors implicitly through this kernel function.",
            "Let's try and see what we can do with them.",
            "What kind of things can we actually compute in this feature space without actually explicitly computing the vectors?",
            "So it's it's like sort of dealing with some very complex object, but through looking at it through a mirror or sort of through some sort of simple way of manipulating.",
            "But of course this will reduce what we can do.",
            "In some sense.",
            "We've got a limited flexibility of what we can do, and this is really just to explore some of the things that we can do, and this will lead to some new algorithms that we can apply in these kernel defined feature spaces.",
            "So.",
            "Let's just see where we are with time.",
            "OK, so I think I'll do a little bit more and then we'll take a 15 minute break.",
            "OK, so.",
            "So here's our general.",
            "You know vector that we might express in the feature space.",
            "It's a linear combination of the training data, so we're always imagining there's some set of training data we're dealing with.",
            "And we've got this kernel that we've fixed on and we were happy with.",
            "That somehow corresponds to the data we're interested in.",
            "So what let's see can we compute the norm squared of that?",
            "We've already sort of.",
            "Implicitly done that because we were optimizing it in the in the Ridge regression.",
            "But can we actually write down what it is?",
            "Well, it's very simple.",
            "We just write down this inner product explicitly, use again the linearity of the inner product, move the sums outside, and the Alpha rise, and we end up with an inner product between 5X.",
            "I'm Flying J Bingo we know to compute that.",
            "That's just the kernel function, and so here is the norm squared of that weight vector.",
            "So it's just Alpha as a vector.",
            "Transpose times the kernel matrix.",
            "Times Alpha, so it's the outer product with the kernel matrix of of the jewel.",
            "Vector that we're out have to represent our weight vector."
        ],
        [
            "Um?",
            "Can we normalize data?",
            "OK, so we would like to move from some mapping 5X which is given by a Colonel Kappa to a, uh, a mapping Phi hat, which is just the re normalized data from 5X.",
            "We assume 5X is not equal to 0 for any of our inputs.",
            "Well let's again look at what that would be.",
            "Here's the new kernel.",
            "Cape Kappa Hat is just the normalized data in a product with its with the normal ized.",
            "Projection.",
            "Well, again moving the one on five normal 5X out of the inner product and the one on five zed would end up with an inner product between 5X and five ZB which is just here and the norm of 5X where we've already.",
            "We know how to compute that.",
            "That's just the square root of the inner product between 5X and itself which is just Kappa XX square root and this one's Kappa zed, zed square root.",
            "So here we are, we can we can compute in that feature space so we can actually re normalize in the feature space.",
            "Without again having to explicitly compute anything there.",
            "So it's just a again, you know you.",
            "You can play with all sorts of little manipulations and see whether you can act."
        ],
        [
            "Perform them so if you have two vectors WAWB, you can compute the with dual representations, Alpha and beta.",
            "You can compute the difference between those two vectors.",
            "Alpha has dual representation, Alpha minus beta.",
            "And so you can compute the distance between those two vectors in the feature space, because it's just the norm of the difference between these two vectors.",
            "Sorry, the norm of this vector, which we now have a dual representation."
        ],
        [
            "And we know how to compute norms."
        ],
        [
            "Because we were able to do it here.",
            "So it's just the square root of.",
            "You know Alpha minus beta.",
            "Transpose times K times Alpha minus beta."
        ],
        [
            "So we can compute distances in in the feature space."
        ],
        [
            "OK, what about the norm of the mean of a sample?",
            "So here's the sample mean.",
            "Just take the sum of the training data divided by its number.",
            "And we're going to call that Vector Phi sub S just the mean of the sample.",
            "Well, again, it's it's very simple to see that the vector that expresses the jewel of this vector is just the all ones vector J / M. And so again, using that same expression as before the norm of this is just the square root of J of primed over M * K * J primed over M. Take the the M outside and you have one on M. Square root of J prime KJ.",
            "So this is just actually the some of the entries in the kernel matrix, so the sum of the entries in the kernel matrix is is the norm.",
            "Norm squared, or sorry, that number divided by M ^2 is the OR the average of the entries in the kernel matrix is the norm squared of the.",
            "Mean vector of that training set, so that's sort of an easy way to compute that.",
            "So what's the expected squared distance to the mean of a sample?",
            "So this is the expectation over the sample I'm denoting by hat or sometimes Y sub M, so just an expectation average over the sample of the difference between 5X.",
            "So this is like the spread of the data in the feature space.",
            "Well, it's if you just multiply these things out, you get this quantity, which is the inner product of this with itself.",
            "Remember, this is a sum over the excise 1 /, 1 M and then you get the average of this in a productive with this, which of course is just 5S with itself minus twice that and then plus 5S with itself.",
            "So you end up with 1 -- 5 S with itself.",
            "And so this we know what it is.",
            "It's just one on M ^2, J prime KJ.",
            "That's just this quantity.",
            "And this is just the trace.",
            "Notice the sum of the diagonal entries of the kernel matrix.",
            "So the one on M, the trace of the kernel matrix minus the average entry of the kernel matrix is the spread of the data in the feature space."
        ],
        [
            "Um?",
            "And if we consider centering, the sample will move the origin to the sample mean.",
            "So in the new feature space than you kernel in that centered of that centered data will actually have average Entry 0, because the norm of the average of the data now is at the origin, so it has zero norm, so this will become zero in the new coordinates."
        ],
        [
            "But if we go back here, this spread will not change if you just shift than or the the data in the feature space, it doesn't actually change it spread, so this will actually be the same we've set.",
            "This goes to a minimum because this is going to be positive, so it goes to a minimum.",
            "So this also must go to a minimum when we sent to the data.",
            "So centering the data."
        ],
        [
            "Actually makes the trace of the matrix minimal.",
            "And we achieve that by this transformation where we move from 5X25 hat of X, which is just subtracting off the mean of the sample."
        ],
        [
            "And we can do that, compute the kernel for that transformation.",
            "It has this quantity.",
            "This sorry, this form.",
            "Where we have to sum over the rows and columns or effectively, you know these are these are the same and then we can actually express that in an explicit manipulation of the kernel matrix as follows.",
            "So K -- 1 on MJJ prime K plus KJJ, primed and then this is this.",
            "Some of the entries times JJ Prime, so this is the expression that centers the data in the feature space.",
            "And as I said, that minimizes the trace of the of the representation.",
            "OK, so.",
            "Hopefully that's just giving you a little bit of a flavor of the kind of things we can do with the.",
            "With the kernel representation, certainly not exhaustive in any sense, and you know you can think of other.",
            "Algorithms you might want to try and see if they can be represented in in a kernel defined feature space, just as an."
        ],
        [
            "Sample you know he is a kind of thing one might want to do.",
            "You might want to say look, I'll put a ball.",
            "This is a very simple novelty detector.",
            "I'm going to put a ball around my my mean of my data and I'm going to say that anything else so that all of the training data is inside it, just containing the training data.",
            "And I'm going to say new data that falls outside that ball is is weird, is novel in some sense.",
            "You know it's a sort of.",
            "Abnormality detector if you like trying to sort of monitor, say some process and if something falls out of the norm, you raise an alarm.",
            "You know something the engine might be making strange sounds or you know some fault might be some onset of fault or or so on, so this would be a very simple way of expressing novelty and we can give a kernel expression for that quantity quite simply because we can compute this difference between the.",
            "I mean and the and the.",
            "Training data, and similarly between this test data and the mean, and so we can write this down in a kernel defined feature space.",
            "So these are all methods that you can just rapidly translate into a kernel framework, and obviously these are relatively simple ones, but many, many algorithms have been transformed into that into that kernel framework, and I think when we come back, I'll probably take a break now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, at this point I would like to introduce our first speaker, tutorial Speaker John Shaw, Taylor University College of London.",
                    "label": 0
                },
                {
                    "sent": "Speaking on kernel methods and support vector machines, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thank you very much and.",
                    "label": 0
                },
                {
                    "sent": "It's a pleasure, great pleasure to be here.",
                    "label": 0
                },
                {
                    "sent": "Thank you to the organizers for inviting me.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "I was asked to do a sort of tutorial on kernel methods and support vector machines.",
                    "label": 1
                },
                {
                    "sent": "It's quite a tricky one in that I think you know probably the majority of you will know a lot about kernel methods already and and and support vector machines probably have used them.",
                    "label": 0
                },
                {
                    "sent": "So in a way, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a difficult one to pitch, so my aim is to try and give you the big picture.",
                    "label": 0
                },
                {
                    "sent": "So to actually sort of take you through the basics of what, yes?",
                    "label": 0
                },
                {
                    "sent": "OK, is that better?",
                    "label": 0
                },
                {
                    "sent": "Thanks, please do Introducton to interrupt if you have questions or problems with what the microphone or whatever.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to take you through sort of the try and put it in perspective.",
                    "label": 0
                },
                {
                    "sent": "What the kernel methods are trying to do.",
                    "label": 0
                },
                {
                    "sent": "Perhaps also some of their limitations, but but also, of course you know their strengths, and I think the thing I'll really try and emphasis emphasize is the plug and play in nature that you've got a sort of group of methods that can be used to analyze different data, different algorithms, different pre processing, and so on.",
                    "label": 0
                },
                {
                    "sent": "And it's sort of a package which you can actually draw tools and algorithms out of for your particular application.",
                    "label": 0
                },
                {
                    "sent": "So that's my that's my aim.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here it is.",
                    "label": 0
                },
                {
                    "sent": "I the tutorial will sort of start at the top level.",
                    "label": 0
                },
                {
                    "sent": "You know why linear pattern functions wire kernel approach.",
                    "label": 1
                },
                {
                    "sent": "But as I said the emphasis will be on trying to give you this flavor of the plug and play.",
                    "label": 0
                },
                {
                    "sent": "So please bear with me if some of the stuff I say is very familiar with to you.",
                    "label": 0
                },
                {
                    "sent": "I apologize ahead for that, but I've I've sort of deliberately tried to make it accessible to it.",
                    "label": 0
                },
                {
                    "sent": "People who maybe haven't seen kernel methods before.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I won't be looking at sort of other approaches to pattern analysis.",
                    "label": 1
                },
                {
                    "sent": "I won't be attempting a complete history of who did what, or you know who thinks they you know did something first or whatever.",
                    "label": 0
                },
                {
                    "sent": "More importantly, I won't be actually talking about the Bayesian method.",
                    "label": 1
                },
                {
                    "sent": "I'll be method or view of of kernel methods.",
                    "label": 0
                },
                {
                    "sent": "Will allude to it a couple of times and will mention more about it in my talk this afternoon, but as far as this tutorial is concerned I'm taking the mainly the traditional kernel view and I won't be talking about more recent developments.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Divide it into four parts which doesn't sit too well because we're planning to have three sessions of an hour and then 1/4 of an hour break.",
                    "label": 0
                },
                {
                    "sent": "So that's the aim anyway, but we probably won't strictly strictly hold too.",
                    "label": 0
                },
                {
                    "sent": "You know, an hourly session, but I'll try, you know, fit in a couple of breaks at some points, so the four parts are basically the introduction sort of motivation and a sort of worked example, just so you get the flavor of the whole thing.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk about the pre processing stage.",
                    "label": 0
                },
                {
                    "sent": "Then the sort of group of algorithms with support vector machines as the prime example here and then finally look at some of the kernel design strategies.",
                    "label": 1
                },
                {
                    "sent": "So in terms of an application, what one would typically be doing is actually doing it probably in the reverse order.",
                    "label": 0
                },
                {
                    "sent": "You'd be thinking of OK.",
                    "label": 0
                },
                {
                    "sent": "I've got this kind of data.",
                    "label": 0
                },
                {
                    "sent": "I'll need to design kernel for it.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's one that I can pick off the shelf, or maybe I need to adapt it.",
                    "label": 0
                },
                {
                    "sent": "Then I have to think about which algorithm I'm going to apply, and perhaps after.",
                    "label": 0
                },
                {
                    "sent": "Looking at that, think about some pre processing, so it's probably completely the wrong order in terms of applications, but it sort of seemed a natural way to do it when I was putting it together.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduction then.",
                    "label": 0
                },
                {
                    "sent": "So a very brief history of how things came about.",
                    "label": 0
                },
                {
                    "sent": "I think that's worth mentioning just to kind of put in perspective why kernel methods came rather late in the day.",
                    "label": 0
                },
                {
                    "sent": "Or at least there there's adoption came late in the day.",
                    "label": 0
                },
                {
                    "sent": "Then go through this worked example, which will be Kernel Ridge regression and I'll mention briefly properties of kernels and and show exactly you know the benefit that you can gain from using a kernel approach.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is pattern analysis about?",
                    "label": 1
                },
                {
                    "sent": "Of course, I'm sure you all have a good idea of this.",
                    "label": 0
                },
                {
                    "sent": "You're in this area, but perhaps it's sometimes worth stepping back and thinking about it more generally.",
                    "label": 0
                },
                {
                    "sent": "We're looking for patterns and you know, in historical terms.",
                    "label": 0
                },
                {
                    "sent": "People have looked at various types.",
                    "label": 0
                },
                {
                    "sent": "You know exact patterns were traditionally studied in in physics.",
                    "label": 0
                },
                {
                    "sent": "You know the motion of the planets and so on.",
                    "label": 0
                },
                {
                    "sent": "There are complex patterns which are just very sort of deterministic, but very complex functions.",
                    "label": 0
                },
                {
                    "sent": "And then there are probabilistic funk patterns which are really just coming out of the uncertainty or the randomness in the world.",
                    "label": 0
                },
                {
                    "sent": "And for instance trying to predict.",
                    "label": 0
                },
                {
                    "sent": "The attitude of buyers in a market is is a typical example of that.",
                    "label": 0
                },
                {
                    "sent": "However, I should mention that often complex patterns there may be underlying deterministic function, but it's better to try and learn it as a probabilistic one because you're never going to be able to capture all of the details of the actual complex phenomenon.",
                    "label": 0
                },
                {
                    "sent": "So actually using a probabilistic approach can be a good tactic.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What is the aim game is that we would like to understand the regularity's in the data.",
                    "label": 0
                },
                {
                    "sent": "The data may look very complex or random, but there are possibly underlying regularity's and the machine learning or the pattern analysis approach is attempting to extract those irregularities from the data in order to make benefit in terms of predictions of future activity or analyzing the best way to actually use or manipulate that data that situation.",
                    "label": 0
                },
                {
                    "sent": "So pattern analysis is the study of automatic detection of patterns in data is probably a nice way of viewing it.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can think of patterns of sort of exact patterns as defining some function which for all data is equal to 0, some sort of.",
                    "label": 1
                },
                {
                    "sent": "Deterministic function approximate patterns are ones where the function is approximately 0.",
                    "label": 1
                },
                {
                    "sent": "An statistical patterns are ones where it's expectation is approximately 0, so this is the kind of thing where we're thinking of when we're doing the probabilistic pattern analysis or machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the properties we've like of our algorithms are.",
                    "label": 0
                },
                {
                    "sent": "They should be computationally efficient.",
                    "label": 1
                },
                {
                    "sent": "We would like them to run in polynomial time and hopefully have a fairly low degree in order to scale to large datasets.",
                    "label": 0
                },
                {
                    "sent": "We'd like them to be able to handle noisy data app some of the data we've been given is not correctly classified or correctly labeled.",
                    "label": 1
                },
                {
                    "sent": "Maybe noisy sensor data in in your inputs and the outputs.",
                    "label": 0
                },
                {
                    "sent": "Maybe only measured to certain accuracy.",
                    "label": 0
                },
                {
                    "sent": "You'd like your algorithms to be able to handle that type of data.",
                    "label": 0
                },
                {
                    "sent": "And more importantly, perhaps the most important feature that I think is is.",
                    "label": 0
                },
                {
                    "sent": "New and interesting in machine learning is the statistical stability.",
                    "label": 1
                },
                {
                    "sent": "That is, we want to be able to distinguish between patterns that have arisen by chance in a particular data set.",
                    "label": 0
                },
                {
                    "sent": "Just because it is a finite data set and ones which really characterize the underlying source of the data, and the hope is that we're learning those more stable or underlying patterns that will actually be visible on new data, because that's where we're hoping to make benefit by applying that what we learned on our.",
                    "label": 0
                },
                {
                    "sent": "Finite sample of data to new data, and we're hoping that that features or or pattern that we extracted will actually be present in the new data and therefore give us the benefit that we're hoping for.",
                    "label": 0
                },
                {
                    "sent": "So this is perhaps the key.",
                    "label": 0
                },
                {
                    "sent": "I won't be actually speaking a lot about this in this.",
                    "label": 0
                },
                {
                    "sent": "Tutorial I'll allude to it and give the example of support vector machines and come back to it in in in more detail in the afternoon lecture that I have this afternoon.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so brief historical perspective.",
                    "label": 1
                },
                {
                    "sent": "So machine learning really grew out of a study of neural like structures and they were first considered seriously in the 1960s with such systems as the perceptron and the perceptron algorithm.",
                    "label": 1
                },
                {
                    "sent": "It caused a lot of excitement.",
                    "label": 0
                },
                {
                    "sent": "It seemed that there was an algorithm that could learn patterns in data.",
                    "label": 0
                },
                {
                    "sent": "And mimic in some sense the way that biological systems learn.",
                    "label": 0
                },
                {
                    "sent": "So this was caused a lot of excitement, but the patterns that were learned were just linear.",
                    "label": 0
                },
                {
                    "sent": "But there was a nice convergence theorem that showed that if there was a a classifier that could separate the data nontrivially, then there was the algorithm would in fact find that pattern.",
                    "label": 0
                },
                {
                    "sent": "But there was a lot of discussion about this and a book by Minskin peppered rather sort of laid into the approach as being limited in its complexity.",
                    "label": 0
                },
                {
                    "sent": "The complexity of the patterns that it could analyze, and this led to a fall off of interest at the end of the 1960s.",
                    "label": 0
                },
                {
                    "sent": "It was not until the 1980s that the ideas came back in force and this was with the resurrection of the multilayer perceptron idea.",
                    "label": 1
                },
                {
                    "sent": "These were networks of perceptrons with continuous activation functions, so called neural networks, and again the same sort of level of excitement was generated.",
                    "label": 0
                },
                {
                    "sent": "Everyone was, you know, getting very sort of attracted to the idea that we were finally capturing something about biological intelligence.",
                    "label": 0
                },
                {
                    "sent": "Human intelligence perhaps.",
                    "label": 0
                },
                {
                    "sent": "But the algorithms were very, very slow and there was also limited statistical analysis and so things actually weren't as good as they might have appeared, or certainly as they were.",
                    "label": 0
                },
                {
                    "sent": "Hyped up to be at that point.",
                    "label": 0
                },
                {
                    "sent": "It was.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This time in the early 1990s that the.",
                    "label": 0
                },
                {
                    "sent": "Kernel methods was was introduced or I should say, reintroduced.",
                    "label": 0
                },
                {
                    "sent": "I'll mention later the first idea of using a kernel was actually back in 1964.",
                    "label": 0
                },
                {
                    "sent": "Believe it or not, where somebody implemented the perceptron algorithm with the kernel in in the USSR.",
                    "label": 0
                },
                {
                    "sent": "So the Eisaman ET al.",
                    "label": 0
                },
                {
                    "sent": "I'll I'll refer to that, but it was reintroduced in the 1990s.",
                    "label": 1
                },
                {
                    "sent": "So the the the problem that.",
                    "label": 0
                },
                {
                    "sent": "The perceptrons face was their limited.",
                    "label": 0
                },
                {
                    "sent": "Functionality just taking linear functions in in the input space was was restricting their power to express patterns, and so the tack that had been taken with the multilayer perceptron was to glue these things together to make a more complex network and hence up the computational power.",
                    "label": 1
                },
                {
                    "sent": "Kernel methods take a different approach.",
                    "label": 0
                },
                {
                    "sent": "What they do is they stick with the linear functions, but they apply them in a high dimensional space, so they project the input data into a more complex feature space and then learn with linear patterns or linear functions in that space so they retain the benefits of learning linear functions.",
                    "label": 0
                },
                {
                    "sent": "Which are, you know, computationally tractable, and there's a unique optimal solution, and so on.",
                    "label": 0
                },
                {
                    "sent": "But gain the power from this more complex feature space.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now there's a danger there because using two complex a feature space can also cause overfitting.",
                    "label": 0
                },
                {
                    "sent": "But then statistical analysis showed that this large margin idea, and again I'll come back to that actually overcomes this curse of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "So this was really the breakthrough and the final sort of piece of the jigsaw was that you don't actually have to work explicitly in that high dimensional space, so there's a sort of advantage of using high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The disadvantage is overcome by somehow controlling the complexity and the complexity.",
                    "label": 0
                },
                {
                    "sent": "Computational is overcome by this so called dual representation, so I'll take you through all of those, but I'm just trying to give you the the sort of how it fits into the overall picture.",
                    "label": 0
                },
                {
                    "sent": "So after this first method of support vector machines, it was rapidly expanded to other other tasks other than classification.",
                    "label": 1
                },
                {
                    "sent": "In fact, we're going to start with with the Ridge regression example, but I'll mention others as well.",
                    "label": 0
                },
                {
                    "sent": "Novelty detection and of course you know kernel, PCA and some of these other methods that operate in a kernel defined feature space.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The kernel methods approach the data is embedded into a Euclidean feature space.",
                    "label": 1
                },
                {
                    "sent": "Linear relations are sort among the images of the data.",
                    "label": 0
                },
                {
                    "sent": "The algorithms implemented must only require inner products between vectors, and this is how we avoid the actual explicit projection into the feature space.",
                    "label": 0
                },
                {
                    "sent": "And the final ingredient is that the inner product between images of two points in the input space.",
                    "label": 0
                },
                {
                    "sent": "So you project two points from the input space into the feature space, compute the inner product that would involve.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you did it explicitly, computing in the feature space itself, but the so called kernel function is a shortcut that computes that directly on the input vectors, so it actually shortcuts the actual computation of the projection and the.",
                    "label": 0
                },
                {
                    "sent": "In a product, so that's what a kernel function does for you, and I'll give examples again in a minute.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the idea, sort of top level idea.",
                    "label": 0
                },
                {
                    "sent": "You've got an input space in which the data is not linearly separable, so there's not a straight line that separates this data.",
                    "label": 0
                },
                {
                    "sent": "You projected into some high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Here it's the same dimension obviously, but you know this is the idea is this is some much higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Now there is a linear separation of the data for the positive and negative data, and so you actually learn this separation in the high dimensional space and it corresponds to a non linear separation.",
                    "label": 0
                },
                {
                    "sent": "In the input space, because the effect is is the way that the five Maps things creates a nonlinearity in the in the mapping.",
                    "label": 0
                },
                {
                    "sent": "So the FI embeds the data into a feature space where the nonlinear pattern Now appears linear.",
                    "label": 1
                },
                {
                    "sent": "The kernel computes the inner product in the feature space directly from the input.",
                    "label": 0
                },
                {
                    "sent": "So this is the shortcut computation.",
                    "label": 0
                },
                {
                    "sent": "Do the projection of X do the projection of said compute the inner product and that is computed directly by this kernel function?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now I'll take this worked example of Ridge regression.",
                    "label": 1
                },
                {
                    "sent": "Sorry, I must keep the microphone so this is just to show how things work out.",
                    "label": 0
                },
                {
                    "sent": "In one practical example and hopefully it will cement the ideas that I've given you sort of.",
                    "label": 0
                },
                {
                    "sent": "Add a brush stroke level.",
                    "label": 0
                },
                {
                    "sent": "So far, so all of the ingredients come into this example.",
                    "label": 0
                },
                {
                    "sent": "So we're interested in linear function.",
                    "label": 0
                },
                {
                    "sent": "And we're just going to consider now a linear function in the input space initially will then think of doing it in a feature space.",
                    "label": 0
                },
                {
                    "sent": "So at the moment this is just a linear combination of the input features and this is just notation I will use interchangeably these notations.",
                    "label": 0
                },
                {
                    "sent": "This is a more traditional inner product notation.",
                    "label": 0
                },
                {
                    "sent": "This is just thinking of these column vector with a dash indicating a transpose, so this is now a row vector times the column vector, which is just the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's the inner product between those two vectors.",
                    "label": 0
                },
                {
                    "sent": "And this is writing it out explicitly, so this is just to get you familiar with the notation.",
                    "label": 0
                },
                {
                    "sent": "I use these interchangeably.",
                    "label": 0
                },
                {
                    "sent": "Again, you know if anything is unclear.",
                    "label": 0
                },
                {
                    "sent": "Stick your hand up and I'll I'll probably ignore you, but I'll try and hopefully notice.",
                    "label": 0
                },
                {
                    "sent": "OK, so the aim of Ridge regression is to find the best interpolate of some data where you have some inputs X one up to XMN corresponding outputs which are real values Y one up to YM.",
                    "label": 0
                },
                {
                    "sent": "So we're thinking them as as labels, but their real valued outputs.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if we want to think of it in terms of those pattern functions I described, we need to create a function which we're hoping is going to be small or close to 0 in expectation, and the way to do that is to take the output of our linear function, which I'm denoting by G of X and subtract the output.",
                    "label": 0
                },
                {
                    "sent": "The correct output from it, and square the difference.",
                    "label": 0
                },
                {
                    "sent": "So this is the squared error if you like in the output of the function on this particular example.",
                    "label": 0
                },
                {
                    "sent": "And we're thinking of that as our pattern function, and we're hoping that that's going to be small, both on training and and test data.",
                    "label": 0
                },
                {
                    "sent": "So FG is our function with.",
                    "label": 0
                },
                {
                    "sent": "We're hoping to minimize.",
                    "label": 0
                },
                {
                    "sent": "So OK, I'm going to now use some notation to make this sort of explicit mathematically.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use a matrix X to denote my examples from that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training set here.",
                    "label": 0
                },
                {
                    "sent": "This is X one up to XM.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And they're going to be the rows of X, so that X is going to have em rose and the length of the Rose is the dimension of the input vectors, so we might call that D. The weight vector is also D dimensional because it's in the same space as the.",
                    "label": 0
                },
                {
                    "sent": "You know it's awaiting over the features.",
                    "label": 0
                },
                {
                    "sent": "So if we now create this vector PSI which is y -- X times W X * W just evaluates each of the examples through the linear function and subtracting from Y gives us the residual.",
                    "label": 1
                },
                {
                    "sent": "So this is the difference between the correct output and the.",
                    "label": 1
                },
                {
                    "sent": "Actual output generated by the current weight vector, the current function.",
                    "label": 0
                },
                {
                    "sent": "So this is a vector of residuals, and now we're trying to actually minimum.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Size a ^2 error so the square those residuals, the norm squared of that vector is the sum of the squares of those residuals.",
                    "label": 0
                },
                {
                    "sent": "So this is our error term and what we do in Ridge regression is actually introduce a regularization term which is controlling the norm of the weight vector that we use.",
                    "label": 1
                },
                {
                    "sent": "So we trade off, we minimize W over a term that involves the norm squared of W. Scaled by a parameter Lambda, which is referred to as the regularization parameter plus this error term, which is dependent on W through that funk.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And G that's all through this.",
                    "label": 0
                },
                {
                    "sent": "XW here so excited.",
                    "label": 0
                },
                {
                    "sent": "Depends on W as well.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is our error term, so the choice of W will attempt to make the error term small, but not if that involves making a very very large norm weight vector, so it's trading complexity of the weight vector with the error on the on the training data.",
                    "label": 1
                },
                {
                    "sent": "So this is the optimization that is adopted for Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "So let's just run through it.",
                    "label": 0
                },
                {
                    "sent": "See what happens.",
                    "label": 0
                },
                {
                    "sent": "Well, we can explicitly right outside squared as y -- X, W, y -- X W in a product with itself.",
                    "label": 0
                },
                {
                    "sent": "If we multiply out this inner product, we get the following.",
                    "label": 0
                },
                {
                    "sent": "And then if we set the derivative of this quantity equal to zero, we get the following.",
                    "label": 0
                },
                {
                    "sent": "The equation XX primed XW that comes from this term here, which with a factor 2.",
                    "label": 0
                },
                {
                    "sent": "But I've cancelled the two and we take the X primed Y onto the other side and this Lambda W comes from the derivative of this term.",
                    "label": 0
                },
                {
                    "sent": "So this is the derivative of this set equal to 0 and treated as an equation.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is what we're trying to solve and clearly we can solve it by just inverting this matrix and that in.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Indeed, gives us what's known as the primal weight vector solution.",
                    "label": 0
                },
                {
                    "sent": "So we use the term primal to mean the explicit representation in the of the weight vector in the feature space.",
                    "label": 1
                },
                {
                    "sent": "Now in this case it's in the input space, but if we were doing it in a more complex feature space it would again be in that in that feature space.",
                    "label": 1
                },
                {
                    "sent": "So this is the primal solution.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vector, all I've done is just invert this matrix and apply it to this side here and it's got by just setting the derivative of this equal to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "So notice that because it's a linear function, we're optimizing, there's a unique solution well, and a quadratic loss, so it's a convex loss as a unique solution, and we can actually explicitly write it out, and we can find it by solving a set of linear equations.",
                    "label": 0
                },
                {
                    "sent": "It's very straightforward.",
                    "label": 0
                },
                {
                    "sent": "And if we want to progress on a new data point, we can just take the weight vector and take the inner product with that new data point.",
                    "label": 0
                },
                {
                    "sent": "And here we have the explicit representation.",
                    "label": 1
                },
                {
                    "sent": "So this is just regularised least squares regression so far.",
                    "label": 0
                },
                {
                    "sent": "Referred to as Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "So now what I want to show you is that that can now be actually performed in a kernel defined feature space.",
                    "label": 0
                },
                {
                    "sent": "So this is where we showing the power of a kernel method and how it can be used.",
                    "label": 1
                },
                {
                    "sent": "Or this algorithm can be used in combination with the kernel defined feature space.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The key idea is that you need a so-called dual representation of that weight vector, so instead.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of that explicit vector, we're going.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Express the weight vector W as a linear combination of the training data.",
                    "label": 1
                },
                {
                    "sent": "So we're going to express the weight vector in terms of the training data.",
                    "label": 1
                },
                {
                    "sent": "Here is the expression that we're looking at.",
                    "label": 0
                },
                {
                    "sent": "W is just a sum with weighting coefficients.",
                    "label": 0
                },
                {
                    "sent": "Alpha Rye of the training data.",
                    "label": 0
                },
                {
                    "sent": "Now that's you might say, well, how.",
                    "label": 0
                },
                {
                    "sent": "How do I know that's going to be the case?",
                    "label": 0
                },
                {
                    "sent": "Well, in this case we can actually write that down that equation that we had for W. And we can actually express W by taking this onto the other side and dividing through by Lambda.",
                    "label": 0
                },
                {
                    "sent": "We express W in terms of these other terms in the equation, and if we bring the factor X primed out front, we see in fact W is expressible as X primed times some vector which I'm demoting by here Alpha.",
                    "label": 0
                },
                {
                    "sent": "So Alpha is just one over Lambda y -- X W. So we have OK.",
                    "label": 0
                },
                {
                    "sent": "It involves W, but we've shown that the solution does in fact lie in the span of the training data by that manipulation.",
                    "label": 0
                },
                {
                    "sent": "Well, in a sense that's almost a no brainer.",
                    "label": 0
                },
                {
                    "sent": "I mean in it because what we're doing is.",
                    "label": 0
                },
                {
                    "sent": "Computing inner products with training data.",
                    "label": 0
                },
                {
                    "sent": "Putting into your solution vector some parts of the sum component which is orthogonal to the space spanned by the training data would appear very.",
                    "label": 0
                },
                {
                    "sent": "You know it would play no role because it would have a zero inner product with all of the training data.",
                    "label": 0
                },
                {
                    "sent": "So this extra component that will be orthogonal to the span of the training data would have no effect.",
                    "label": 0
                },
                {
                    "sent": "And since we have that regularization term which is trying to minimize the norm, you would immediately throw that component away and get a lower optimization of your.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your objective here.",
                    "label": 0
                },
                {
                    "sent": "Because throwing that component away would have no effect on this because it would have no effect on these inner products.",
                    "label": 0
                },
                {
                    "sent": "The component is orthogonal to the span of the training data, so would have no effect on this, but it would reduce this because you've actually projected the data the the weight vector down into a into a smaller vector in this space spanned by the training data.",
                    "label": 0
                },
                {
                    "sent": "So it's clear you know if you think about it that way, that the the weight vector will be in the span of the.",
                    "label": 0
                },
                {
                    "sent": "Of the training data.",
                    "label": 0
                },
                {
                    "sent": "Therefore, it will be expressible in this form.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That I've put here, and indeed this just verifies that fact, but also explicitly computes the form of that Alpha.",
                    "label": 0
                },
                {
                    "sent": "It will actually be of this form OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the so called dual representation.",
                    "label": 1
                },
                {
                    "sent": "Now it's slightly at odds with the.",
                    "label": 0
                },
                {
                    "sent": "Optimization theory, where you have primal and dual.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they coincide, sometimes they don't exactly support vector machines.",
                    "label": 0
                },
                {
                    "sent": "There's a very nice coincidence between the primal and dual of representation that I've referred to here and the primal and dual in optimization theory, but I think if we're being honest we need to just say this is our terminology that we use in kernel methods, and it's loosely borrowed from optimization theory, but doesn't always exactly accord with optimization theory.",
                    "label": 0
                },
                {
                    "sent": "So think of it as just a way of representing your your solution vector in terms of the training data.",
                    "label": 1
                },
                {
                    "sent": "And now what we want to do is not try and learn W, but try and learn Alpha.",
                    "label": 0
                },
                {
                    "sent": "So we've gotta re sort of jigging at the problem where now rather than try and learn W we try and learn Alpha.",
                    "label": 0
                },
                {
                    "sent": "So we sort of implicitly learning W through learning it's dual representation.",
                    "label": 0
                },
                {
                    "sent": "OK, well let's try and do that in this case.",
                    "label": 0
                },
                {
                    "sent": "Here's our Alpha.",
                    "label": 0
                },
                {
                    "sent": "Here's an equation for it, but unfortunately it involves W, so we're not going to get Alpha out of this equation because it's sort of a recursive.",
                    "label": 0
                },
                {
                    "sent": "But what we can do is just substitute for this W what we know it can be expressed as in terms of Alpha, so we're just going to substitute X primed Alpha in here and will now have an equation involving only Alpha, so that's.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do here?",
                    "label": 0
                },
                {
                    "sent": "And this is the equation that you get Lambda Alpha equals y -- X prime X expr.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just multiplied up.",
                    "label": 0
                },
                {
                    "sent": "This Lambda y -- X X prime Alpha, so just sub.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You did that in there.",
                    "label": 0
                },
                {
                    "sent": "OK, so now rearranging terms we've now got the following equation.",
                    "label": 0
                },
                {
                    "sent": "It looks very similar to what we had for the primal solution, but actually in some respects it's even simpler because now on the right hand side we just have Y.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whereas before we had ex prime.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I on the right hand side here.",
                    "label": 0
                },
                {
                    "sent": "And this term here was X prime X plus Lambda Rye in the equation for Alpha it's XX prime, so the order of these two is swapped and we've got rid of this X prime on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this equation here, right?",
                    "label": 0
                },
                {
                    "sent": "So it's XX primed plus Lambda Rai.",
                    "label": 0
                },
                {
                    "sent": "Of course the dimension now is different.",
                    "label": 0
                },
                {
                    "sent": "The X primed X had dimension equal to the feature space dimension.",
                    "label": 0
                },
                {
                    "sent": "Because of course the weight vector had has dimension equal to the feature space dimension.",
                    "label": 0
                },
                {
                    "sent": "So the identity matrix also had the same dimension that we added, but in this case Alpha has dimension equal to the number of training points.",
                    "label": 0
                },
                {
                    "sent": "Remember there was 1A for each training point we were learning the combination of training points that created the weight vector, so there's 1A for each training point, and so this is now.",
                    "label": 0
                },
                {
                    "sent": "I MM I'm using to denote the size of the training set and XX primed is is also an M by N matrix.",
                    "label": 0
                },
                {
                    "sent": "So now we can actually write the dual solution.",
                    "label": 1
                },
                {
                    "sent": "It's it's a very similar problem to solve.",
                    "label": 0
                },
                {
                    "sent": "It's a set of linear equations we need to solve.",
                    "label": 0
                },
                {
                    "sent": "But as I said before, it's actually a different dimension to the set of equations we were solving for W. And now if we want to progress on a new point, what do we do?",
                    "label": 0
                },
                {
                    "sent": "OK, we we take X prime times W but now W of course is X Capital X prime times Alpha.",
                    "label": 0
                },
                {
                    "sent": "That's how it's expressed.",
                    "label": 0
                },
                {
                    "sent": "So if we now.",
                    "label": 0
                },
                {
                    "sent": "Write that out explicitly.",
                    "label": 0
                },
                {
                    "sent": "Some Alpha XI by the linearity of the inner product, we can bring the sum outside and we actually express this G of X as the sum weighted sum of the inner products between the test point and the training data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now the key observation here is that we actually only need to compute inner product between test points and training data, and this matrix actually only involves inner product between training data, the entry the IG entry in this matrix is just the inner product between the ice and J training points.",
                    "label": 0
                },
                {
                    "sent": "That's because remember the rows of X were the training data.",
                    "label": 0
                },
                {
                    "sent": "Therefore the columns of X prime to the training data.",
                    "label": 0
                },
                {
                    "sent": "So the IJ entry in this matrix is the I throw in a product with the Jason column, which is just the inner product between the I&J training points in the input space.",
                    "label": 0
                },
                {
                    "sent": "The way I've done it, OK.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key ingredients are you.",
                    "label": 1
                },
                {
                    "sent": "Now I'm using this KA little bit, sort of suggestively here.",
                    "label": 0
                },
                {
                    "sent": "Kernel to denote this because as I said, the kij entry is just the inner product between the- J for training points.",
                    "label": 0
                },
                {
                    "sent": "The evaluation of a new point is again now the weighted weighting of these inner products with the.",
                    "label": 0
                },
                {
                    "sent": "Knew test point and the training data weighted according to this Alpha Rye that we got by solving this set of equations here.",
                    "label": 0
                },
                {
                    "sent": "So the key observation here is that both steps.",
                    "label": 0
                },
                {
                    "sent": "This step here and this step here only involve inner products between input data points.",
                    "label": 1
                },
                {
                    "sent": "OK, here we have X the test point and the training data.",
                    "label": 0
                },
                {
                    "sent": "And here we just have inner products between training data.",
                    "label": 0
                },
                {
                    "sent": "So remember what I said before the kernel provides a shortcut.",
                    "label": 0
                },
                {
                    "sent": "To compute the inner product between projections of data points into a complex feature space and then take their inner products.",
                    "label": 0
                },
                {
                    "sent": "OK, cool well.",
                    "label": 0
                },
                {
                    "sent": "Here if we just substitute in some feature vectors fire Vex I5 XJ Ann.",
                    "label": 0
                },
                {
                    "sent": "Similarly, here 5X5 XI.",
                    "label": 0
                },
                {
                    "sent": "Some feature vector, then all we're needing here is the kernel function to compute that inner product.",
                    "label": 0
                },
                {
                    "sent": "It's an inner product of projections into a feature space of input data.",
                    "label": 0
                },
                {
                    "sent": "And so we can substitute a kernel function here as suggested by this K and Similarly here we have a kernel function between the test point and the training data.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the key idea.",
                    "label": 0
                },
                {
                    "sent": "So it only involves occurrences of the inner product.",
                    "label": 0
                },
                {
                    "sent": "A kernel function computes that inner product in the kernel find feature space, and so we can apply the Ridge regression algorithm in that high dimensional potentially high dimensional feature space by just.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Substituting the kernel function for the computation of this matrix here, the IJ entry of this matrix here and the kernel function for the evaluation of this new point.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of a little bit of magic, you know.",
                    "label": 0
                },
                {
                    "sent": "You kind of apparently operating in this very high dimensional space without actually explicitly computing in it.",
                    "label": 0
                },
                {
                    "sent": "By using this this trick.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, well you may say.",
                    "label": 0
                },
                {
                    "sent": "Do such functions exist?",
                    "label": 0
                },
                {
                    "sent": "I mean give me an example of.",
                    "label": 0
                },
                {
                    "sent": "I mean maybe this kernel function is going to be just as difficult to compute as actually computing these explicit vectors and taking it in a product.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do now is show you an example very simple example just to convince you that yes, you know there are very natural ways of using defining kernel functions that do correspond to very interesting high dimension.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feature spaces.",
                    "label": 0
                },
                {
                    "sent": "So here's the first sort of nontrivial example of a kernel function.",
                    "label": 0
                },
                {
                    "sent": "It's the so-called quadratic kernel.",
                    "label": 1
                },
                {
                    "sent": "So what you do is you simply take the inner product between the feature vectors in the input space.",
                    "label": 0
                },
                {
                    "sent": "And then you just square that number.",
                    "label": 0
                },
                {
                    "sent": "Time.",
                    "label": 0
                },
                {
                    "sent": "So it's one extra computation, one extra multiplication that's required over and above what you would have done if you just done the computation anyway in the feature space, so it's certainly not an overload of computation here.",
                    "label": 0
                },
                {
                    "sent": "You know this might be a D dimensional vector, so you'd have to do D multiplications and D -- 1 additions anyway.",
                    "label": 0
                },
                {
                    "sent": "And all we're saying is do one more multiplication OK.",
                    "label": 0
                },
                {
                    "sent": "So it's a very minor.",
                    "label": 0
                },
                {
                    "sent": "Additional load Now what I can claim is this actually corresponds to an inner product in a in a complex feature space.",
                    "label": 1
                },
                {
                    "sent": "Well, why OK?",
                    "label": 0
                },
                {
                    "sent": "This is what I've written.",
                    "label": 0
                },
                {
                    "sent": "I'm now just using a different notation here.",
                    "label": 0
                },
                {
                    "sent": "X prime zed squared is just another way of expressing this, but now I can actually swap one of these two round the order.",
                    "label": 0
                },
                {
                    "sent": "And change the bracketing and I get zed XX prime ZOXX prime does a matrix.",
                    "label": 0
                },
                {
                    "sent": "And so this is a matrix with a row vector times a column vector.",
                    "label": 1
                },
                {
                    "sent": "And in fact, this is now the Frobenius inner product between two matrices or the inner product between two vectors where you vectorize the matrix.",
                    "label": 0
                },
                {
                    "sent": "Zed, zed primed and the Matrix XX primed if you're.",
                    "label": 0
                },
                {
                    "sent": "You know, if you're not sure that you're happy with that.",
                    "label": 0
                },
                {
                    "sent": "Derivation here.",
                    "label": 0
                },
                {
                    "sent": "It's very easy just to write it out explicitly.",
                    "label": 0
                },
                {
                    "sent": "Just write the thumbs down on a piece of paper and you'll see that it works out.",
                    "label": 0
                },
                {
                    "sent": "It's very easy.",
                    "label": 0
                },
                {
                    "sent": "This is just a you know, a slightly maybe slicker way of showing it, but if you're not happy, just write out the sons and you'll see that it actually works that this actually corresponds to an inner product between these this feature vector for X.",
                    "label": 0
                },
                {
                    "sent": "Sorry, this work on this the wrong way around, but anyway, whichever this feature vector for X and this feature vector for zed.",
                    "label": 0
                },
                {
                    "sent": "So what it what it corresponds to is taking instead of just say, D components.",
                    "label": 0
                },
                {
                    "sent": "There are now D squared components where the components are the products of the features of the original feature vector.",
                    "label": 1
                },
                {
                    "sent": "So there's a feature for XIXJ, and there's actually another feature of XJ, XI, so you sort of double counting actually get repeated features, But there's certainly N choose two different or N + 1.",
                    "label": 0
                },
                {
                    "sent": "Choose two different sorry entries, do different features here so the feature space dimension has been significantly increased compared to the original input space, and I'll make that it.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "List it with an example of, say.",
                    "label": 0
                },
                {
                    "sent": "Doing a regression over a set of images.",
                    "label": 1
                },
                {
                    "sent": "So think of pixel vectors, which might be 32 by 32 pixel images.",
                    "label": 0
                },
                {
                    "sent": "So the input space dimension is 1024 dimensions, so D in this case is 1024.",
                    "label": 0
                },
                {
                    "sent": "If we use the quadratic kernel, we're implementing regression in a million dimensional space, right?",
                    "label": 1
                },
                {
                    "sent": "Roughly 1000 squared or well 500,000 dimension.",
                    "label": 0
                },
                {
                    "sent": "If you take into account some of them are the same, but certainly a very very high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "And the cost of doing it is actually lower than doing it in the original space because we're having to invert 1000 by 1000 matrix.",
                    "label": 0
                },
                {
                    "sent": "That's the dimension of the training set.",
                    "label": 0
                },
                {
                    "sent": "As opposed to 1024 by 1024 Dimensional Matrix, which we would have had if we'd done explicit computation in the input space.",
                    "label": 0
                },
                {
                    "sent": "So we're running rich regression in a hugely more powerful space at no extra cost, or in fact reduced cost compared to the doing a primal sort of normal Ridge regression without the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a slight caveat to that that when you're evaluating on an you test point, it is going to be more complex because you have to run over in order to compute the.",
                    "label": 0
                },
                {
                    "sent": "Let me go back to the evaluation on a test point here.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to evaluate on test point, all of these alphas are going to be non zero and so you're going to have to compute the inner products between the test point and each of the OR the kernel functions are between the test point in each of the training data.",
                    "label": 0
                },
                {
                    "sent": "So there's going to be 1000 of these to compute, so it's 1000 * 1000 'cause there are 1000 dimensional.",
                    "label": 0
                },
                {
                    "sent": "So that's a million computations to do this, Whereas in if we had an explicit vector in the input in, in, the in the primal space it would just be 1000 computation, so that.",
                    "label": 0
                },
                {
                    "sent": "That is a slight caveat here, but certainly in solving the.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem actually learning the Alpha vector is less expensive than learning the primal vector in the original space.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are the implications?",
                    "label": 0
                },
                {
                    "sent": "So what we've actually done?",
                    "label": 0
                },
                {
                    "sent": "We've performed linear regression in a very high dimensional, and in fact even can be done in infinite dimensional spaces efficiently.",
                    "label": 1
                },
                {
                    "sent": "Through this kernel trick.",
                    "label": 0
                },
                {
                    "sent": "It's equivalent to performing non linear regression in the original input space.",
                    "label": 1
                },
                {
                    "sent": "So we've effectively moved to doing nonlinear regression in some sense.",
                    "label": 0
                },
                {
                    "sent": "In the sense that the final function that you're actually learning has a form.",
                    "label": 1
                },
                {
                    "sent": "In this case, where we use the quadratic kernel.",
                    "label": 0
                },
                {
                    "sent": "This is the form it's a sum of Alpha right inner product XIX squared.",
                    "label": 0
                },
                {
                    "sent": "So it's actually a nonlinear function is a quadratic polynomial function of the of the components of the input vector.",
                    "label": 1
                },
                {
                    "sent": "So we're actually doing nonlinear regression, but in a way that retains the attractive features of linear regression that there's a unique global solution.",
                    "label": 0
                },
                {
                    "sent": "We can find it fast by simple.",
                    "label": 0
                },
                {
                    "sent": "Inversion of a of a matrix, so it seems to be, you know, magic the best of all possible worlds in a sense, but you know there must be some warnings.",
                    "label": 0
                },
                {
                    "sent": "There must be some problems here and clearly there is the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Working in high dimensional spaces is dangerous and you are liable to overfit your data.",
                    "label": 0
                },
                {
                    "sent": "I kind of alluded to how that's overcome by controlling that weight vector norm is effectively controlling the complexity, so it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Giving with one hand and taking back with the other, you're giving lots and lots of dimensions with the kernel trick, but you're actually trying to.",
                    "label": 0
                },
                {
                    "sent": "You know, keep a tight control on their use through the regularization of the weight vector norm.",
                    "label": 0
                },
                {
                    "sent": "So it's a kind of.",
                    "label": 0
                },
                {
                    "sent": "Let's the data decide innocence.",
                    "label": 0
                },
                {
                    "sent": "How much complexity is actually needed, and that seems to be a very effective way of operating you.",
                    "label": 0
                },
                {
                    "sent": "You kind of.",
                    "label": 0
                },
                {
                    "sent": "Don't try and apriori say how much complexity you need.",
                    "label": 0
                },
                {
                    "sent": "You give a lot of complexity, but you try and keep tight control on its use.",
                    "label": 0
                },
                {
                    "sent": "Hopefully that actually ends you up with the right level of usage of complexity for the problem that you're actually trying to solve, so that's one of the ways in which the curse of dimensionality is handled within kernel methods generally, and the same applies to support vector machines and an most of the other techniques are very similar approaches adopted in terms of controlling the complexity.",
                    "label": 0
                },
                {
                    "sent": "So I'm doing a handwaving argument here.",
                    "label": 0
                },
                {
                    "sent": "I'll give a little bit more of a concrete example of.",
                    "label": 0
                },
                {
                    "sent": "Statistical analysis in the support vector case and as said, I'll come back to that this afternoon and talk about some of the pounds for analyzing that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That completes my my my simple example.",
                    "label": 0
                },
                {
                    "sent": "Hopefully it's giving you the flavor of what's what's involved, the components, so you need the kernel function.",
                    "label": 0
                },
                {
                    "sent": "You need an algorithm that you can express in terms of inner products, both the training and the testing.",
                    "label": 0
                },
                {
                    "sent": "And and then it's basically a plug and play, and all you need to do in order to to actually apply the algorithm is compute this kernel matrix from using your kernel.",
                    "label": 0
                },
                {
                    "sent": "Your chosen kernel passed that to your algorithm.",
                    "label": 0
                },
                {
                    "sent": "It turns away and comes back with a dual vector and then you use that to evaluate you test points.",
                    "label": 0
                },
                {
                    "sent": "So it's a very kind of.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Ready to use approach which you can you know.",
                    "label": 0
                },
                {
                    "sent": "Use the same algorithm with different kernels or different algorithms with the same kernel and so on.",
                    "label": 0
                },
                {
                    "sent": "So there's a plug and play aspect here that I think is is very useful.",
                    "label": 0
                },
                {
                    "sent": "And just to convince you that there are other kernels, I will at the end, you know, in the in the final section, spend a bit more time talking about different kernel strategies for defining kernels, but just here I wanted to mention this kernel, the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Which is a popular very popular kernel.",
                    "label": 1
                },
                {
                    "sent": "And it actually sorry there's a missing square here.",
                    "label": 0
                },
                {
                    "sent": "There should be that should be squared.",
                    "label": 0
                },
                {
                    "sent": "The norm squared divided by two Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "This actually corresponds to using an infinite dimensional feature space.",
                    "label": 1
                },
                {
                    "sent": "If you've got a continuous space in the input, so actually performing, say Ridge regression.",
                    "label": 1
                },
                {
                    "sent": "Using this kernel will correspond to doing it in an infinite dimensional space, which is sort of an interesting property of these approaches.",
                    "label": 0
                },
                {
                    "sent": "So what is it that makes a kernel function a kernel function?",
                    "label": 0
                },
                {
                    "sent": "I mean, you might say, well, look, I'm not too worried about all this projection.",
                    "label": 0
                },
                {
                    "sent": "Let me just define my similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a good projection and then I can just use it.",
                    "label": 0
                },
                {
                    "sent": "All I need is to be able to compute the function the kernel function.",
                    "label": 0
                },
                {
                    "sent": "And so you might ask, what are the properties of a function that makes it a kernel function makes it correspond to some projection into a feature space and some inner product.",
                    "label": 0
                },
                {
                    "sent": "Well, clearly it needs to be symmetric, and it needs to satisfy this property, because this corresponds to the norm squared of the vector X in the feature space.",
                    "label": 0
                },
                {
                    "sent": "I'll show you that in a minute, but those properties are not enough.",
                    "label": 0
                },
                {
                    "sent": "So caveat here, just writing down any old function, the chances are it won't be a kernel function.",
                    "label": 0
                },
                {
                    "sent": "And I'll describe what are the properties that are needed for a function to be a kernel function in the in the last part of the of the tutorial.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Hopefully that's set.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The scene.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do now is just say, OK, we've got this technique of representing vectors implicitly through this kernel function.",
                    "label": 0
                },
                {
                    "sent": "Let's try and see what we can do with them.",
                    "label": 0
                },
                {
                    "sent": "What kind of things can we actually compute in this feature space without actually explicitly computing the vectors?",
                    "label": 0
                },
                {
                    "sent": "So it's it's like sort of dealing with some very complex object, but through looking at it through a mirror or sort of through some sort of simple way of manipulating.",
                    "label": 0
                },
                {
                    "sent": "But of course this will reduce what we can do.",
                    "label": 0
                },
                {
                    "sent": "In some sense.",
                    "label": 0
                },
                {
                    "sent": "We've got a limited flexibility of what we can do, and this is really just to explore some of the things that we can do, and this will lead to some new algorithms that we can apply in these kernel defined feature spaces.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's just see where we are with time.",
                    "label": 1
                },
                {
                    "sent": "OK, so I think I'll do a little bit more and then we'll take a 15 minute break.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So here's our general.",
                    "label": 0
                },
                {
                    "sent": "You know vector that we might express in the feature space.",
                    "label": 0
                },
                {
                    "sent": "It's a linear combination of the training data, so we're always imagining there's some set of training data we're dealing with.",
                    "label": 0
                },
                {
                    "sent": "And we've got this kernel that we've fixed on and we were happy with.",
                    "label": 0
                },
                {
                    "sent": "That somehow corresponds to the data we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So what let's see can we compute the norm squared of that?",
                    "label": 0
                },
                {
                    "sent": "We've already sort of.",
                    "label": 0
                },
                {
                    "sent": "Implicitly done that because we were optimizing it in the in the Ridge regression.",
                    "label": 1
                },
                {
                    "sent": "But can we actually write down what it is?",
                    "label": 0
                },
                {
                    "sent": "Well, it's very simple.",
                    "label": 0
                },
                {
                    "sent": "We just write down this inner product explicitly, use again the linearity of the inner product, move the sums outside, and the Alpha rise, and we end up with an inner product between 5X.",
                    "label": 1
                },
                {
                    "sent": "I'm Flying J Bingo we know to compute that.",
                    "label": 0
                },
                {
                    "sent": "That's just the kernel function, and so here is the norm squared of that weight vector.",
                    "label": 0
                },
                {
                    "sent": "So it's just Alpha as a vector.",
                    "label": 0
                },
                {
                    "sent": "Transpose times the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Times Alpha, so it's the outer product with the kernel matrix of of the jewel.",
                    "label": 0
                },
                {
                    "sent": "Vector that we're out have to represent our weight vector.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Can we normalize data?",
                    "label": 0
                },
                {
                    "sent": "OK, so we would like to move from some mapping 5X which is given by a Colonel Kappa to a, uh, a mapping Phi hat, which is just the re normalized data from 5X.",
                    "label": 0
                },
                {
                    "sent": "We assume 5X is not equal to 0 for any of our inputs.",
                    "label": 0
                },
                {
                    "sent": "Well let's again look at what that would be.",
                    "label": 0
                },
                {
                    "sent": "Here's the new kernel.",
                    "label": 0
                },
                {
                    "sent": "Cape Kappa Hat is just the normalized data in a product with its with the normal ized.",
                    "label": 0
                },
                {
                    "sent": "Projection.",
                    "label": 0
                },
                {
                    "sent": "Well, again moving the one on five normal 5X out of the inner product and the one on five zed would end up with an inner product between 5X and five ZB which is just here and the norm of 5X where we've already.",
                    "label": 0
                },
                {
                    "sent": "We know how to compute that.",
                    "label": 0
                },
                {
                    "sent": "That's just the square root of the inner product between 5X and itself which is just Kappa XX square root and this one's Kappa zed, zed square root.",
                    "label": 0
                },
                {
                    "sent": "So here we are, we can we can compute in that feature space so we can actually re normalize in the feature space.",
                    "label": 1
                },
                {
                    "sent": "Without again having to explicitly compute anything there.",
                    "label": 0
                },
                {
                    "sent": "So it's just a again, you know you.",
                    "label": 0
                },
                {
                    "sent": "You can play with all sorts of little manipulations and see whether you can act.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Perform them so if you have two vectors WAWB, you can compute the with dual representations, Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "You can compute the difference between those two vectors.",
                    "label": 0
                },
                {
                    "sent": "Alpha has dual representation, Alpha minus beta.",
                    "label": 0
                },
                {
                    "sent": "And so you can compute the distance between those two vectors in the feature space, because it's just the norm of the difference between these two vectors.",
                    "label": 1
                },
                {
                    "sent": "Sorry, the norm of this vector, which we now have a dual representation.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we know how to compute norms.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because we were able to do it here.",
                    "label": 0
                },
                {
                    "sent": "So it's just the square root of.",
                    "label": 0
                },
                {
                    "sent": "You know Alpha minus beta.",
                    "label": 0
                },
                {
                    "sent": "Transpose times K times Alpha minus beta.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can compute distances in in the feature space.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what about the norm of the mean of a sample?",
                    "label": 1
                },
                {
                    "sent": "So here's the sample mean.",
                    "label": 0
                },
                {
                    "sent": "Just take the sum of the training data divided by its number.",
                    "label": 0
                },
                {
                    "sent": "And we're going to call that Vector Phi sub S just the mean of the sample.",
                    "label": 0
                },
                {
                    "sent": "Well, again, it's it's very simple to see that the vector that expresses the jewel of this vector is just the all ones vector J / M. And so again, using that same expression as before the norm of this is just the square root of J of primed over M * K * J primed over M. Take the the M outside and you have one on M. Square root of J prime KJ.",
                    "label": 0
                },
                {
                    "sent": "So this is just actually the some of the entries in the kernel matrix, so the sum of the entries in the kernel matrix is is the norm.",
                    "label": 0
                },
                {
                    "sent": "Norm squared, or sorry, that number divided by M ^2 is the OR the average of the entries in the kernel matrix is the norm squared of the.",
                    "label": 0
                },
                {
                    "sent": "Mean vector of that training set, so that's sort of an easy way to compute that.",
                    "label": 0
                },
                {
                    "sent": "So what's the expected squared distance to the mean of a sample?",
                    "label": 1
                },
                {
                    "sent": "So this is the expectation over the sample I'm denoting by hat or sometimes Y sub M, so just an expectation average over the sample of the difference between 5X.",
                    "label": 0
                },
                {
                    "sent": "So this is like the spread of the data in the feature space.",
                    "label": 0
                },
                {
                    "sent": "Well, it's if you just multiply these things out, you get this quantity, which is the inner product of this with itself.",
                    "label": 0
                },
                {
                    "sent": "Remember, this is a sum over the excise 1 /, 1 M and then you get the average of this in a productive with this, which of course is just 5S with itself minus twice that and then plus 5S with itself.",
                    "label": 0
                },
                {
                    "sent": "So you end up with 1 -- 5 S with itself.",
                    "label": 0
                },
                {
                    "sent": "And so this we know what it is.",
                    "label": 0
                },
                {
                    "sent": "It's just one on M ^2, J prime KJ.",
                    "label": 0
                },
                {
                    "sent": "That's just this quantity.",
                    "label": 0
                },
                {
                    "sent": "And this is just the trace.",
                    "label": 0
                },
                {
                    "sent": "Notice the sum of the diagonal entries of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So the one on M, the trace of the kernel matrix minus the average entry of the kernel matrix is the spread of the data in the feature space.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And if we consider centering, the sample will move the origin to the sample mean.",
                    "label": 1
                },
                {
                    "sent": "So in the new feature space than you kernel in that centered of that centered data will actually have average Entry 0, because the norm of the average of the data now is at the origin, so it has zero norm, so this will become zero in the new coordinates.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we go back here, this spread will not change if you just shift than or the the data in the feature space, it doesn't actually change it spread, so this will actually be the same we've set.",
                    "label": 0
                },
                {
                    "sent": "This goes to a minimum because this is going to be positive, so it goes to a minimum.",
                    "label": 0
                },
                {
                    "sent": "So this also must go to a minimum when we sent to the data.",
                    "label": 0
                },
                {
                    "sent": "So centering the data.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually makes the trace of the matrix minimal.",
                    "label": 0
                },
                {
                    "sent": "And we achieve that by this transformation where we move from 5X25 hat of X, which is just subtracting off the mean of the sample.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can do that, compute the kernel for that transformation.",
                    "label": 0
                },
                {
                    "sent": "It has this quantity.",
                    "label": 0
                },
                {
                    "sent": "This sorry, this form.",
                    "label": 0
                },
                {
                    "sent": "Where we have to sum over the rows and columns or effectively, you know these are these are the same and then we can actually express that in an explicit manipulation of the kernel matrix as follows.",
                    "label": 0
                },
                {
                    "sent": "So K -- 1 on MJJ prime K plus KJJ, primed and then this is this.",
                    "label": 1
                },
                {
                    "sent": "Some of the entries times JJ Prime, so this is the expression that centers the data in the feature space.",
                    "label": 0
                },
                {
                    "sent": "And as I said, that minimizes the trace of the of the representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Hopefully that's just giving you a little bit of a flavor of the kind of things we can do with the.",
                    "label": 1
                },
                {
                    "sent": "With the kernel representation, certainly not exhaustive in any sense, and you know you can think of other.",
                    "label": 0
                },
                {
                    "sent": "Algorithms you might want to try and see if they can be represented in in a kernel defined feature space, just as an.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sample you know he is a kind of thing one might want to do.",
                    "label": 0
                },
                {
                    "sent": "You might want to say look, I'll put a ball.",
                    "label": 1
                },
                {
                    "sent": "This is a very simple novelty detector.",
                    "label": 1
                },
                {
                    "sent": "I'm going to put a ball around my my mean of my data and I'm going to say that anything else so that all of the training data is inside it, just containing the training data.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to say new data that falls outside that ball is is weird, is novel in some sense.",
                    "label": 0
                },
                {
                    "sent": "You know it's a sort of.",
                    "label": 0
                },
                {
                    "sent": "Abnormality detector if you like trying to sort of monitor, say some process and if something falls out of the norm, you raise an alarm.",
                    "label": 0
                },
                {
                    "sent": "You know something the engine might be making strange sounds or you know some fault might be some onset of fault or or so on, so this would be a very simple way of expressing novelty and we can give a kernel expression for that quantity quite simply because we can compute this difference between the.",
                    "label": 1
                },
                {
                    "sent": "I mean and the and the.",
                    "label": 0
                },
                {
                    "sent": "Training data, and similarly between this test data and the mean, and so we can write this down in a kernel defined feature space.",
                    "label": 0
                },
                {
                    "sent": "So these are all methods that you can just rapidly translate into a kernel framework, and obviously these are relatively simple ones, but many, many algorithms have been transformed into that into that kernel framework, and I think when we come back, I'll probably take a break now.",
                    "label": 0
                }
            ]
        }
    }
}