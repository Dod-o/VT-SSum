{
    "id": "f4qk4tbkjiy3lmbfcualzth2ff5nqqw4",
    "title": "Universal Learning over Related Distributions and Adaptive Graph Transduction",
    "info": {
        "author": [
            "Wei Fan, Baidu, Inc."
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_fan_ulrdagt/",
    "segmentation": [
        [
            "Graph transaction I saw it's me again.",
            "Yeah, sorry it's only one don't have any visa issue, so the other people they have a traveling with constraints so OK this the title is universal.",
            "Learning over related distribution and adaptive graph transduction.",
            "So and so the same way from IBM TJ Watson.",
            "The other authors are from.",
            "Or are hung Dong and Jintao gentleman from Sensible staying Guangzhou China and Jinpeng from Monkey University in New Jersey in the USOK?",
            "So this paper is different from the other one presented.",
            "There are several important differences.",
            "First of all, I think the most important thing is we go beyond transfer learning to also consider sample selection bias and uncertainty mining the same.",
            "Framework and we want to propose.",
            "We propose one unified solution because as you will see in my slides, these three problems transfer learning, sample selection bias and uncertainty.",
            "Meaning they are quite related.",
            "And then so we provide a unified framework and so that is the main motivation in the paper and the solution we use is an improved version of."
        ],
        [
            "Graph transfer instruction.",
            "OK.",
            "So this is standard super learning as we."
        ],
        [
            "Talk already, so let's.",
            "Done quickly to sample selection bias.",
            "So for central selection, bias is the data, the label data you have are from the same domain fee interest like New York Times, but the label data you have, they have a different distribution.",
            "Do too many reality like practical reasons.",
            "For example, in August there are multiple articles in your time talking about the typhoon in Taiwan.",
            "And because the fact and body in September, because New York City will have US Open, there are lots of articles about the US Open.",
            "So obviously there are no more typhoon and more anymore in September.",
            "So which is good?",
            "You guys open?",
            "We have every day but not iPhone.",
            "OK, but problem is, if you learn a model using the the iPhone articles, talking about iPhone to label the data value is open, they won't be accurate.",
            "So these are the sample called sample selection bias.",
            "Or in other words, missing not at random.",
            "Other scenario like for bank data, you know people applying for mortgage and usually quite different from people they have already in the bank.",
            "Also people they join when they decide with school to study is that people self select.",
            "This is not the random sample, so these are examples of some both selection bias."
        ],
        [
            "So fun, uncertainty data mining.",
            "So the both feature vectors and class label contain noise that usually assumed to be Gaussian.",
            "And these are very common for data collected over sensor network becausw you know the reality the sensor diploid in forest due to the.",
            "Yeah, the weather and the battery problem transmission errors, so the data collected usually have errors in there and testing data.",
            "The feature vector also have some noise in there as well, so these are certainly mining so traditionally in the last few years there have been a lot of studying each topic, but these are studied separately and."
        ],
        [
            "By actually all these problems they are related, so the traditional super learning, the training and testing data follow the identical distribution.",
            "That's our basic assumption.",
            "So in transfer learning they are from different domains and sample selection bias there on the same domain.",
            "But distribution is different such as missing, not at random.",
            "In on certainty, data mining data contains noise.",
            "The noise happens in both feature vector and class labels.",
            "So in other words, you can forget all the details.",
            "So in all three cases, the training and testing data are from different distributions, but they are related, not completely different.",
            "Otherwise we cannot do much.",
            "So so traditionally problems handled separately and."
        ],
        [
            "Uh, so ideas?",
            "Can we propose a unified solution?",
            "So main challenges.",
            "So we have a domain transfer or have a sample selection bias?",
            "Or have a noisy training data so we know word data from different but similar distributions.",
            "And if you apply traditional approach like you have a drop in performance, so could we propose crease of this different under unified framework and have one single solution?",
            "So the concept we propose is called."
        ],
        [
            "Universal learning, so here is what we mean by universal learning.",
            "So here is a definition we borrow from from previous work.",
            "So H is the subset of examples.",
            "Under the support of some hypothesis in the fixed about this space, it's a little bit hard to read, so here is 1 illustration OK.",
            "So we have a hypothesis space and a subset of abundances.",
            "Here less H1H 2H3.",
            "An X is a.",
            "The marginal the examples we consider.",
            "So the.",
            "Support here are B&C becausw.",
            "BNC had same prediction by the three.",
            "Hypothesis, so basically you consider the set of examples which can Group A set of about this together.",
            "And these are the support for this hypothesis.",
            "So based on this definition, the difference between two distributions is the following, which is again hard to parse.",
            "So let's see one example to understand it.",
            "So we had support.",
            "Which is a B&C and using the previous OK animation will have a two distribution here.",
            "So there are differences in the distribution.",
            "Is the maximum OK you consider the probability for B to appear in the test distribution and in the trading distribution, and these are the differences in the distribution.",
            "So what are the chances you appear?",
            "For example, if B only appears in the training never in testing, you have one here.",
            "Then the differences between these two distributions for those support SATA support will be true, because when you find 1 example always appears that doesn't appear at all.",
            "So using this previous definition of the difference between distributions and when your distributions are different and so we have this definition of universal learning so.",
            "We will learn over 2 distributions, the training and testing when they are different or the difference is small enough."
        ],
        [
            "So how to handle universal learning?",
            "OK so.",
            "Many traditional classifier will not guarantee performance and becausw they rely on the assumption that the training and testing are from the same distribution.",
            "So when they are violated is very hard to prove anything.",
            "You may run it, get some results, but you do not know how bad to perform.",
            "So we want an approach.",
            "We can prove something.",
            "Some bound OK.",
            "So can we find one classifier under this weaker assumption or for universal learning?",
            "So the answer is a great transaction, so might be one approach, probably not, not."
        ],
        [
            "Only approach OK.",
            "So the good thing of great transaction is it makes a weaker assumption about decision boundary.",
            "It basically says the district boundary lines on the low density regions of the unlabeled data.",
            "So here are two examples towards datasets.",
            "One is the two Gaussian.",
            "And one one is 2001 is too large problem and if you if you apply this data, obviously they look for quite different right?",
            "If you use traditional ground transaction you will be able to learn a classifier which can learn.",
            "Perfectly across these two distributions.",
            "Even though they look quite different, so that's the advantage why we consider ground transduction for this universal learning problem.",
            "But like."
        ],
        [
            "Life is not easy.",
            "So, but is it only transaction can solve the problem?",
            "The answer is no, because there are two problems in a traditional graph transduction.",
            "One is called Unsmooth label problem.",
            "There's a classic imbalance problem, so they both are discussed in last year's paper believing in ICM.",
            "Well, so unsmooth problem means that more examples in low density region can really skew your decision boundary and classic.",
            "The problem is you have a lot of data belong to one class and not the other one.",
            "So here is OK. Can see that we have a two Gaussian.",
            "You have a four red example here and one blue here.",
            "So if you run the traditional graph transduction an all this party here all this circles.",
            "And all these will be predicted to be red, state or blue because they are closer.",
            "Two to the red ones.",
            "That's how the label propagates in the graph.",
            "So then the simple intuitions.",
            "Can we have a user sample selection to solve the problem, and so if so, then which samples are the good ones?"
        ],
        [
            "And which are not OK.",
            "So here are two samples in here, the blue and the red.",
            "And if you run the graph transduction.",
            "Half examples will be wrong and these are the red circles will be actually predicted as blue.",
            "I put them as red circle because they are wrong.",
            "They are real.",
            "Label is red but they are predicted as blue by traditional graph transduction and these are examples would be wrong as well because they are closer to this example.",
            "So the observation is if you draw.",
            "The decision boundary, which is the line.",
            "The middle if you draw the margin.",
            "On the unlabeled example, the margin question more is only here.",
            "For those above on those below.",
            "That's not good.",
            "So then the intuitions we need to pick those examples to solve the class imbalance as wells on smooth problem we discussed in the earlier slide by picking those examples which will have a larger unlabeled example margin.",
            "So these are the examples we pick this blue one and this red one.",
            "And that's the decision boundary.",
            "And this line here and this line are the margin.",
            "Of the unlabeled data.",
            "Obviously, this margin is much larger than this one."
        ],
        [
            "So here is the main flow based on the previous slide and the intuition.",
            "These are the main flow of the approach.",
            "So we have unlabeled target domain data and we have labeled source domain data OK and we use a sample selection which chooses examples with a high with a large unlabeled data margin.",
            "And we do the update on the graph and this propagates and through the graph and this iterates.",
            "A few times every time we obtain a new graph and then we basically have an ensemble.",
            "Then we use the averaging ensemble to be the final classifier.",
            "So the intuition is so sample selection.",
            "We want to maximize the unlabeled data margin.",
            "And then we predict the labels on the unlabeled data.",
            "We use the average ensemble, which will increase the margin even more.",
            "And for each case we have approved analysis in the paper."
        ],
        [
            "So actually happens OK.",
            "So.",
            "As we said, you know why one reason we want to pick your transaction is we can prove something to show that even the distribution are different and performance are bounded.",
            "So here is the bound and the details are in the paper.",
            "I just want to explain what are the terms are so the first round the training error in terms of approximating the ideal hypothesis.",
            "And because you have the training.",
            "Data that the limited you may you may screw up, you know, pick up a bottle which is not ideal and these are the errors.",
            "The ideal hypothesis.",
            "And this is the difference in distribution between your training and testing data.",
            "So these are in the inside the inside the bound, because obviously you cannot overcome it completely and this the rest of the terms are the VC dimension and the number of examples."
        ],
        [
            "In the training set.",
            "And we also have a formal approving, the data that the classifier has a larger, unlabeled data margin.",
            "It will have a smaller training error in the unlabeled data.",
            "So which is good?",
            "We want to prefer a classifier with a smaller training error, but also we reduce the bond in the previous slide.",
            "And we also saw in the paper by using average and sample we can reduce the expected margin.",
            "We can increase the expected margin even more.",
            "As a result, we can reduce."
        ],
        [
            "Sierra so let's see some results.",
            "So we for since we Havasu scenario transfer learning, sample selection bias and uncertainty mining, so we have a three sets of experiments for transfer learning.",
            "We use two set of text manager, set once, Reuters one is Cisco, Albert and Cisco.",
            "Robert have the different web pages plus users rating on this web pages.",
            "And for writers you know they have a different topic, and to be the higher level high level class label and to a compose a French learning problem, we pick two different subtopics.",
            "So Topic 8 topic tapiet from the organ place become the target domain and the different sub topic before the source domain.",
            "And in the Cisco Robert so we have a the web pages from completely different topics.",
            "Sheep, biomedical goats."
        ],
        [
            "Versus bands recording.",
            "And for sample selection bias we use some data from UCI data set.",
            "I know sphere, diabetes, blah blah.",
            "So to generate the sample selection bias we randomly choose 50% of the features, then sort the data according to each selected feature.",
            "Then we only keep the top this instances from our list or the training set.",
            "Obviously the unlikely to have the same distribution as original training set, because they are sorted.",
            "So these are some examples to show the how substation buyers will affect us.",
            "So these are the two classes and the Gray and green.",
            "And if you use a linear boundary, that's the boundary on the pic.",
            "If you have only two examples, that's one possible boundary you can pick in there, which is obviously quite different on the ideal boundary around.",
            "And for the answer to the mining, we use account rich biomedical repository and these are high dimensional.",
            "An low sample size and to generate those noises we generate Gaussian noises and insert them into the bowl."
        ],
        [
            "The training and testing set.",
            "OK, so these are some baseline methods we compared, so one is the traditional original.",
            "Well transaction algorithm and there are two variations we experimented.",
            "One is using the entire training set so no change there.",
            "Second one variations because we do the sample selection.",
            "So we want to show that the sample we choose by using the maximum unlabeled margin principle is better.",
            "So we have to compare with using the same sample the same size but not using the unlabeled margin principle as the criteria.",
            "And then the for transfer learning we use a CDSC.",
            "That's one transfer learning approach which appeared in last year's quality.",
            "And therefore bias some more selection.",
            "We use the two methods based on clustering, finding, using clustering, finding the structure mapping between the training testing data which appeared in last years."
        ],
        [
            "I am data mining conference and these are."
        ],
        [
            "Labels, which is hard to read, so these are the charts.",
            "So for transfer learning, we compared with graph random growth graphs, original graph.",
            "And random graph is a one I mentioned using the smaller sample size same size as our sample selection, but without the unlabeled margin criteria.",
            "And CDC is the transfer learning and margin graph is the proposed methods using ensemble plus unlabeled example margin.",
            "Obviously for internal accuracy and the proposed approach have a higher accuracy than any proposed methods.",
            "And these are good.",
            "Sometimes it's not."
        ],
        [
            "Fair criteria, so we also computed AUC, so we are better in the five out of 6 cases.",
            "And these are the results on sample selection bias again table."
        ],
        [
            "Is not easy to read, so these are in charts so we have accuracy and AUC and so we have a graph or original graph random graph by using a smaller sample without using the unlabeled label margin and margin graph.",
            "Also they to baseline methods on sample selection bias.",
            "So obviously you see we are the best of all four accuracy where the best all four and you see we are the best on two out of four.",
            "These are."
        ],
        [
            "Incidentally, mining and these are the results, so accuracy and AUC.",
            "And.",
            "So actually in this case and the proposed approach approach are better than both the original graph mining and the random graph using the.",
            "Not even the."
        ],
        [
            "Use the unlabeled example margin.",
            "So just want to show you how the margin plays a big role in all this.",
            "So modern base, the base classifier of margin graph each iteration so these are using sample selection based on the maximize the unlabeled margin criteria and low base is the other way we choose examples which had the minimum margin OK. Then low graph is basically we have the ensemble, but each base model using the low model."
        ],
        [
            "Margin criteria.",
            "And so these are the results and the margin graph is on the top.",
            "Obviously that was the best.",
            "Then you compare the low margin with the margin base.",
            "Obviously almost none of them, the low low bass, low margin is better than the higher margin, so obviously the shows a high margin is a good thing to have.",
            "And also ensembles are better than than the single classifier.",
            "This is true for both using large margin."
        ],
        [
            "Small Martin.",
            "So these are some quick conclusions, so we cover the reason we like this approach is covers the different formulations, transfer learning, sample selection bias, and certainly mining.",
            "And we propose approach using graph mining and by doing sample selection using the maximum margin principle using assembles and we have a performance analysis on the generalization bound.",
            "Again the data and code available."
        ],
        [
            "And from our research website so thank you.",
            "I have a question on your previous book, OK?",
            "Just wondering when you wake this hyperplane separating the samples, why wouldn't you use support vector machines instead of?",
            "This approach is spectral class right?",
            "Yeah, yeah, because the assumptions on the spectral clustering in our understanding is more suitable for transfer learning.",
            "But I see that recently there were coming out by improving SVM.",
            "Also for that.",
            "Yeah, because it doesn't have the problem with scalability so much as you mentioned with you.",
            "ICM do have.",
            "ICM doesn't have a ICM doesn't have a scalability issue with high dimension but have a scheduling issue on the number of examples.",
            "OK, and what are the assumptions because of which you prefer your approach?",
            "We think that the spectral clustering and are more suitable for transfer learning, but right now I think it's open question to discuss which one is better an under with scenario.",
            "Any other question?",
            "If not, then I would to thank the speaker and invited."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graph transaction I saw it's me again.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry it's only one don't have any visa issue, so the other people they have a traveling with constraints so OK this the title is universal.",
                    "label": 0
                },
                {
                    "sent": "Learning over related distribution and adaptive graph transduction.",
                    "label": 1
                },
                {
                    "sent": "So and so the same way from IBM TJ Watson.",
                    "label": 0
                },
                {
                    "sent": "The other authors are from.",
                    "label": 0
                },
                {
                    "sent": "Or are hung Dong and Jintao gentleman from Sensible staying Guangzhou China and Jinpeng from Monkey University in New Jersey in the USOK?",
                    "label": 0
                },
                {
                    "sent": "So this paper is different from the other one presented.",
                    "label": 0
                },
                {
                    "sent": "There are several important differences.",
                    "label": 0
                },
                {
                    "sent": "First of all, I think the most important thing is we go beyond transfer learning to also consider sample selection bias and uncertainty mining the same.",
                    "label": 1
                },
                {
                    "sent": "Framework and we want to propose.",
                    "label": 0
                },
                {
                    "sent": "We propose one unified solution because as you will see in my slides, these three problems transfer learning, sample selection bias and uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Meaning they are quite related.",
                    "label": 0
                },
                {
                    "sent": "And then so we provide a unified framework and so that is the main motivation in the paper and the solution we use is an improved version of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph transfer instruction.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is standard super learning as we.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk already, so let's.",
                    "label": 0
                },
                {
                    "sent": "Done quickly to sample selection bias.",
                    "label": 0
                },
                {
                    "sent": "So for central selection, bias is the data, the label data you have are from the same domain fee interest like New York Times, but the label data you have, they have a different distribution.",
                    "label": 1
                },
                {
                    "sent": "Do too many reality like practical reasons.",
                    "label": 0
                },
                {
                    "sent": "For example, in August there are multiple articles in your time talking about the typhoon in Taiwan.",
                    "label": 0
                },
                {
                    "sent": "And because the fact and body in September, because New York City will have US Open, there are lots of articles about the US Open.",
                    "label": 0
                },
                {
                    "sent": "So obviously there are no more typhoon and more anymore in September.",
                    "label": 0
                },
                {
                    "sent": "So which is good?",
                    "label": 0
                },
                {
                    "sent": "You guys open?",
                    "label": 0
                },
                {
                    "sent": "We have every day but not iPhone.",
                    "label": 0
                },
                {
                    "sent": "OK, but problem is, if you learn a model using the the iPhone articles, talking about iPhone to label the data value is open, they won't be accurate.",
                    "label": 0
                },
                {
                    "sent": "So these are the sample called sample selection bias.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, missing not at random.",
                    "label": 0
                },
                {
                    "sent": "Other scenario like for bank data, you know people applying for mortgage and usually quite different from people they have already in the bank.",
                    "label": 0
                },
                {
                    "sent": "Also people they join when they decide with school to study is that people self select.",
                    "label": 0
                },
                {
                    "sent": "This is not the random sample, so these are examples of some both selection bias.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So fun, uncertainty data mining.",
                    "label": 1
                },
                {
                    "sent": "So the both feature vectors and class label contain noise that usually assumed to be Gaussian.",
                    "label": 1
                },
                {
                    "sent": "And these are very common for data collected over sensor network becausw you know the reality the sensor diploid in forest due to the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the weather and the battery problem transmission errors, so the data collected usually have errors in there and testing data.",
                    "label": 0
                },
                {
                    "sent": "The feature vector also have some noise in there as well, so these are certainly mining so traditionally in the last few years there have been a lot of studying each topic, but these are studied separately and.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By actually all these problems they are related, so the traditional super learning, the training and testing data follow the identical distribution.",
                    "label": 1
                },
                {
                    "sent": "That's our basic assumption.",
                    "label": 1
                },
                {
                    "sent": "So in transfer learning they are from different domains and sample selection bias there on the same domain.",
                    "label": 0
                },
                {
                    "sent": "But distribution is different such as missing, not at random.",
                    "label": 1
                },
                {
                    "sent": "In on certainty, data mining data contains noise.",
                    "label": 0
                },
                {
                    "sent": "The noise happens in both feature vector and class labels.",
                    "label": 0
                },
                {
                    "sent": "So in other words, you can forget all the details.",
                    "label": 1
                },
                {
                    "sent": "So in all three cases, the training and testing data are from different distributions, but they are related, not completely different.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we cannot do much.",
                    "label": 0
                },
                {
                    "sent": "So so traditionally problems handled separately and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uh, so ideas?",
                    "label": 0
                },
                {
                    "sent": "Can we propose a unified solution?",
                    "label": 0
                },
                {
                    "sent": "So main challenges.",
                    "label": 0
                },
                {
                    "sent": "So we have a domain transfer or have a sample selection bias?",
                    "label": 0
                },
                {
                    "sent": "Or have a noisy training data so we know word data from different but similar distributions.",
                    "label": 1
                },
                {
                    "sent": "And if you apply traditional approach like you have a drop in performance, so could we propose crease of this different under unified framework and have one single solution?",
                    "label": 0
                },
                {
                    "sent": "So the concept we propose is called.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Universal learning, so here is what we mean by universal learning.",
                    "label": 0
                },
                {
                    "sent": "So here is a definition we borrow from from previous work.",
                    "label": 0
                },
                {
                    "sent": "So H is the subset of examples.",
                    "label": 0
                },
                {
                    "sent": "Under the support of some hypothesis in the fixed about this space, it's a little bit hard to read, so here is 1 illustration OK.",
                    "label": 1
                },
                {
                    "sent": "So we have a hypothesis space and a subset of abundances.",
                    "label": 0
                },
                {
                    "sent": "Here less H1H 2H3.",
                    "label": 0
                },
                {
                    "sent": "An X is a.",
                    "label": 0
                },
                {
                    "sent": "The marginal the examples we consider.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Support here are B&C becausw.",
                    "label": 0
                },
                {
                    "sent": "BNC had same prediction by the three.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis, so basically you consider the set of examples which can Group A set of about this together.",
                    "label": 0
                },
                {
                    "sent": "And these are the support for this hypothesis.",
                    "label": 1
                },
                {
                    "sent": "So based on this definition, the difference between two distributions is the following, which is again hard to parse.",
                    "label": 0
                },
                {
                    "sent": "So let's see one example to understand it.",
                    "label": 0
                },
                {
                    "sent": "So we had support.",
                    "label": 0
                },
                {
                    "sent": "Which is a B&C and using the previous OK animation will have a two distribution here.",
                    "label": 0
                },
                {
                    "sent": "So there are differences in the distribution.",
                    "label": 1
                },
                {
                    "sent": "Is the maximum OK you consider the probability for B to appear in the test distribution and in the trading distribution, and these are the differences in the distribution.",
                    "label": 0
                },
                {
                    "sent": "So what are the chances you appear?",
                    "label": 0
                },
                {
                    "sent": "For example, if B only appears in the training never in testing, you have one here.",
                    "label": 0
                },
                {
                    "sent": "Then the differences between these two distributions for those support SATA support will be true, because when you find 1 example always appears that doesn't appear at all.",
                    "label": 0
                },
                {
                    "sent": "So using this previous definition of the difference between distributions and when your distributions are different and so we have this definition of universal learning so.",
                    "label": 0
                },
                {
                    "sent": "We will learn over 2 distributions, the training and testing when they are different or the difference is small enough.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how to handle universal learning?",
                    "label": 1
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Many traditional classifier will not guarantee performance and becausw they rely on the assumption that the training and testing are from the same distribution.",
                    "label": 0
                },
                {
                    "sent": "So when they are violated is very hard to prove anything.",
                    "label": 0
                },
                {
                    "sent": "You may run it, get some results, but you do not know how bad to perform.",
                    "label": 0
                },
                {
                    "sent": "So we want an approach.",
                    "label": 0
                },
                {
                    "sent": "We can prove something.",
                    "label": 0
                },
                {
                    "sent": "Some bound OK.",
                    "label": 0
                },
                {
                    "sent": "So can we find one classifier under this weaker assumption or for universal learning?",
                    "label": 1
                },
                {
                    "sent": "So the answer is a great transaction, so might be one approach, probably not, not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only approach OK.",
                    "label": 0
                },
                {
                    "sent": "So the good thing of great transaction is it makes a weaker assumption about decision boundary.",
                    "label": 0
                },
                {
                    "sent": "It basically says the district boundary lines on the low density regions of the unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So here are two examples towards datasets.",
                    "label": 0
                },
                {
                    "sent": "One is the two Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And one one is 2001 is too large problem and if you if you apply this data, obviously they look for quite different right?",
                    "label": 0
                },
                {
                    "sent": "If you use traditional ground transaction you will be able to learn a classifier which can learn.",
                    "label": 0
                },
                {
                    "sent": "Perfectly across these two distributions.",
                    "label": 0
                },
                {
                    "sent": "Even though they look quite different, so that's the advantage why we consider ground transduction for this universal learning problem.",
                    "label": 0
                },
                {
                    "sent": "But like.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Life is not easy.",
                    "label": 0
                },
                {
                    "sent": "So, but is it only transaction can solve the problem?",
                    "label": 0
                },
                {
                    "sent": "The answer is no, because there are two problems in a traditional graph transduction.",
                    "label": 0
                },
                {
                    "sent": "One is called Unsmooth label problem.",
                    "label": 0
                },
                {
                    "sent": "There's a classic imbalance problem, so they both are discussed in last year's paper believing in ICM.",
                    "label": 0
                },
                {
                    "sent": "Well, so unsmooth problem means that more examples in low density region can really skew your decision boundary and classic.",
                    "label": 1
                },
                {
                    "sent": "The problem is you have a lot of data belong to one class and not the other one.",
                    "label": 0
                },
                {
                    "sent": "So here is OK. Can see that we have a two Gaussian.",
                    "label": 0
                },
                {
                    "sent": "You have a four red example here and one blue here.",
                    "label": 0
                },
                {
                    "sent": "So if you run the traditional graph transduction an all this party here all this circles.",
                    "label": 0
                },
                {
                    "sent": "And all these will be predicted to be red, state or blue because they are closer.",
                    "label": 0
                },
                {
                    "sent": "Two to the red ones.",
                    "label": 0
                },
                {
                    "sent": "That's how the label propagates in the graph.",
                    "label": 0
                },
                {
                    "sent": "So then the simple intuitions.",
                    "label": 1
                },
                {
                    "sent": "Can we have a user sample selection to solve the problem, and so if so, then which samples are the good ones?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And which are not OK.",
                    "label": 0
                },
                {
                    "sent": "So here are two samples in here, the blue and the red.",
                    "label": 0
                },
                {
                    "sent": "And if you run the graph transduction.",
                    "label": 1
                },
                {
                    "sent": "Half examples will be wrong and these are the red circles will be actually predicted as blue.",
                    "label": 0
                },
                {
                    "sent": "I put them as red circle because they are wrong.",
                    "label": 0
                },
                {
                    "sent": "They are real.",
                    "label": 0
                },
                {
                    "sent": "Label is red but they are predicted as blue by traditional graph transduction and these are examples would be wrong as well because they are closer to this example.",
                    "label": 0
                },
                {
                    "sent": "So the observation is if you draw.",
                    "label": 1
                },
                {
                    "sent": "The decision boundary, which is the line.",
                    "label": 0
                },
                {
                    "sent": "The middle if you draw the margin.",
                    "label": 0
                },
                {
                    "sent": "On the unlabeled example, the margin question more is only here.",
                    "label": 0
                },
                {
                    "sent": "For those above on those below.",
                    "label": 0
                },
                {
                    "sent": "That's not good.",
                    "label": 0
                },
                {
                    "sent": "So then the intuitions we need to pick those examples to solve the class imbalance as wells on smooth problem we discussed in the earlier slide by picking those examples which will have a larger unlabeled example margin.",
                    "label": 0
                },
                {
                    "sent": "So these are the examples we pick this blue one and this red one.",
                    "label": 0
                },
                {
                    "sent": "And that's the decision boundary.",
                    "label": 1
                },
                {
                    "sent": "And this line here and this line are the margin.",
                    "label": 0
                },
                {
                    "sent": "Of the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Obviously, this margin is much larger than this one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the main flow based on the previous slide and the intuition.",
                    "label": 0
                },
                {
                    "sent": "These are the main flow of the approach.",
                    "label": 0
                },
                {
                    "sent": "So we have unlabeled target domain data and we have labeled source domain data OK and we use a sample selection which chooses examples with a high with a large unlabeled data margin.",
                    "label": 1
                },
                {
                    "sent": "And we do the update on the graph and this propagates and through the graph and this iterates.",
                    "label": 0
                },
                {
                    "sent": "A few times every time we obtain a new graph and then we basically have an ensemble.",
                    "label": 0
                },
                {
                    "sent": "Then we use the averaging ensemble to be the final classifier.",
                    "label": 0
                },
                {
                    "sent": "So the intuition is so sample selection.",
                    "label": 1
                },
                {
                    "sent": "We want to maximize the unlabeled data margin.",
                    "label": 0
                },
                {
                    "sent": "And then we predict the labels on the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "We use the average ensemble, which will increase the margin even more.",
                    "label": 0
                },
                {
                    "sent": "And for each case we have approved analysis in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually happens OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As we said, you know why one reason we want to pick your transaction is we can prove something to show that even the distribution are different and performance are bounded.",
                    "label": 0
                },
                {
                    "sent": "So here is the bound and the details are in the paper.",
                    "label": 0
                },
                {
                    "sent": "I just want to explain what are the terms are so the first round the training error in terms of approximating the ideal hypothesis.",
                    "label": 1
                },
                {
                    "sent": "And because you have the training.",
                    "label": 0
                },
                {
                    "sent": "Data that the limited you may you may screw up, you know, pick up a bottle which is not ideal and these are the errors.",
                    "label": 0
                },
                {
                    "sent": "The ideal hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And this is the difference in distribution between your training and testing data.",
                    "label": 0
                },
                {
                    "sent": "So these are in the inside the inside the bound, because obviously you cannot overcome it completely and this the rest of the terms are the VC dimension and the number of examples.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the training set.",
                    "label": 0
                },
                {
                    "sent": "And we also have a formal approving, the data that the classifier has a larger, unlabeled data margin.",
                    "label": 1
                },
                {
                    "sent": "It will have a smaller training error in the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So which is good?",
                    "label": 0
                },
                {
                    "sent": "We want to prefer a classifier with a smaller training error, but also we reduce the bond in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "And we also saw in the paper by using average and sample we can reduce the expected margin.",
                    "label": 0
                },
                {
                    "sent": "We can increase the expected margin even more.",
                    "label": 0
                },
                {
                    "sent": "As a result, we can reduce.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sierra so let's see some results.",
                    "label": 0
                },
                {
                    "sent": "So we for since we Havasu scenario transfer learning, sample selection bias and uncertainty mining, so we have a three sets of experiments for transfer learning.",
                    "label": 0
                },
                {
                    "sent": "We use two set of text manager, set once, Reuters one is Cisco, Albert and Cisco.",
                    "label": 0
                },
                {
                    "sent": "Robert have the different web pages plus users rating on this web pages.",
                    "label": 1
                },
                {
                    "sent": "And for writers you know they have a different topic, and to be the higher level high level class label and to a compose a French learning problem, we pick two different subtopics.",
                    "label": 0
                },
                {
                    "sent": "So Topic 8 topic tapiet from the organ place become the target domain and the different sub topic before the source domain.",
                    "label": 0
                },
                {
                    "sent": "And in the Cisco Robert so we have a the web pages from completely different topics.",
                    "label": 0
                },
                {
                    "sent": "Sheep, biomedical goats.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Versus bands recording.",
                    "label": 0
                },
                {
                    "sent": "And for sample selection bias we use some data from UCI data set.",
                    "label": 0
                },
                {
                    "sent": "I know sphere, diabetes, blah blah.",
                    "label": 0
                },
                {
                    "sent": "So to generate the sample selection bias we randomly choose 50% of the features, then sort the data according to each selected feature.",
                    "label": 1
                },
                {
                    "sent": "Then we only keep the top this instances from our list or the training set.",
                    "label": 0
                },
                {
                    "sent": "Obviously the unlikely to have the same distribution as original training set, because they are sorted.",
                    "label": 0
                },
                {
                    "sent": "So these are some examples to show the how substation buyers will affect us.",
                    "label": 0
                },
                {
                    "sent": "So these are the two classes and the Gray and green.",
                    "label": 0
                },
                {
                    "sent": "And if you use a linear boundary, that's the boundary on the pic.",
                    "label": 0
                },
                {
                    "sent": "If you have only two examples, that's one possible boundary you can pick in there, which is obviously quite different on the ideal boundary around.",
                    "label": 0
                },
                {
                    "sent": "And for the answer to the mining, we use account rich biomedical repository and these are high dimensional.",
                    "label": 1
                },
                {
                    "sent": "An low sample size and to generate those noises we generate Gaussian noises and insert them into the bowl.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The training and testing set.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are some baseline methods we compared, so one is the traditional original.",
                    "label": 1
                },
                {
                    "sent": "Well transaction algorithm and there are two variations we experimented.",
                    "label": 0
                },
                {
                    "sent": "One is using the entire training set so no change there.",
                    "label": 1
                },
                {
                    "sent": "Second one variations because we do the sample selection.",
                    "label": 0
                },
                {
                    "sent": "So we want to show that the sample we choose by using the maximum unlabeled margin principle is better.",
                    "label": 0
                },
                {
                    "sent": "So we have to compare with using the same sample the same size but not using the unlabeled margin principle as the criteria.",
                    "label": 0
                },
                {
                    "sent": "And then the for transfer learning we use a CDSC.",
                    "label": 1
                },
                {
                    "sent": "That's one transfer learning approach which appeared in last year's quality.",
                    "label": 0
                },
                {
                    "sent": "And therefore bias some more selection.",
                    "label": 0
                },
                {
                    "sent": "We use the two methods based on clustering, finding, using clustering, finding the structure mapping between the training testing data which appeared in last years.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I am data mining conference and these are.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Labels, which is hard to read, so these are the charts.",
                    "label": 0
                },
                {
                    "sent": "So for transfer learning, we compared with graph random growth graphs, original graph.",
                    "label": 0
                },
                {
                    "sent": "And random graph is a one I mentioned using the smaller sample size same size as our sample selection, but without the unlabeled margin criteria.",
                    "label": 0
                },
                {
                    "sent": "And CDC is the transfer learning and margin graph is the proposed methods using ensemble plus unlabeled example margin.",
                    "label": 0
                },
                {
                    "sent": "Obviously for internal accuracy and the proposed approach have a higher accuracy than any proposed methods.",
                    "label": 0
                },
                {
                    "sent": "And these are good.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's not.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fair criteria, so we also computed AUC, so we are better in the five out of 6 cases.",
                    "label": 0
                },
                {
                    "sent": "And these are the results on sample selection bias again table.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is not easy to read, so these are in charts so we have accuracy and AUC and so we have a graph or original graph random graph by using a smaller sample without using the unlabeled label margin and margin graph.",
                    "label": 0
                },
                {
                    "sent": "Also they to baseline methods on sample selection bias.",
                    "label": 1
                },
                {
                    "sent": "So obviously you see we are the best of all four accuracy where the best all four and you see we are the best on two out of four.",
                    "label": 0
                },
                {
                    "sent": "These are.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Incidentally, mining and these are the results, so accuracy and AUC.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So actually in this case and the proposed approach approach are better than both the original graph mining and the random graph using the.",
                    "label": 0
                },
                {
                    "sent": "Not even the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use the unlabeled example margin.",
                    "label": 0
                },
                {
                    "sent": "So just want to show you how the margin plays a big role in all this.",
                    "label": 0
                },
                {
                    "sent": "So modern base, the base classifier of margin graph each iteration so these are using sample selection based on the maximize the unlabeled margin criteria and low base is the other way we choose examples which had the minimum margin OK. Then low graph is basically we have the ensemble, but each base model using the low model.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Margin criteria.",
                    "label": 0
                },
                {
                    "sent": "And so these are the results and the margin graph is on the top.",
                    "label": 0
                },
                {
                    "sent": "Obviously that was the best.",
                    "label": 0
                },
                {
                    "sent": "Then you compare the low margin with the margin base.",
                    "label": 0
                },
                {
                    "sent": "Obviously almost none of them, the low low bass, low margin is better than the higher margin, so obviously the shows a high margin is a good thing to have.",
                    "label": 1
                },
                {
                    "sent": "And also ensembles are better than than the single classifier.",
                    "label": 0
                },
                {
                    "sent": "This is true for both using large margin.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Small Martin.",
                    "label": 0
                },
                {
                    "sent": "So these are some quick conclusions, so we cover the reason we like this approach is covers the different formulations, transfer learning, sample selection bias, and certainly mining.",
                    "label": 0
                },
                {
                    "sent": "And we propose approach using graph mining and by doing sample selection using the maximum margin principle using assembles and we have a performance analysis on the generalization bound.",
                    "label": 0
                },
                {
                    "sent": "Again the data and code available.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And from our research website so thank you.",
                    "label": 0
                },
                {
                    "sent": "I have a question on your previous book, OK?",
                    "label": 0
                },
                {
                    "sent": "Just wondering when you wake this hyperplane separating the samples, why wouldn't you use support vector machines instead of?",
                    "label": 0
                },
                {
                    "sent": "This approach is spectral class right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, because the assumptions on the spectral clustering in our understanding is more suitable for transfer learning.",
                    "label": 0
                },
                {
                    "sent": "But I see that recently there were coming out by improving SVM.",
                    "label": 0
                },
                {
                    "sent": "Also for that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because it doesn't have the problem with scalability so much as you mentioned with you.",
                    "label": 0
                },
                {
                    "sent": "ICM do have.",
                    "label": 0
                },
                {
                    "sent": "ICM doesn't have a ICM doesn't have a scalability issue with high dimension but have a scheduling issue on the number of examples.",
                    "label": 0
                },
                {
                    "sent": "OK, and what are the assumptions because of which you prefer your approach?",
                    "label": 0
                },
                {
                    "sent": "We think that the spectral clustering and are more suitable for transfer learning, but right now I think it's open question to discuss which one is better an under with scenario.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "If not, then I would to thank the speaker and invited.",
                    "label": 0
                }
            ]
        }
    }
}