{
    "id": "plkq26d6ffeyqmdn5d7r5ubvv4tbzeg3",
    "title": "A note of caution regarding distances on graphs",
    "info": {
        "author": [
            "Ulrike von Luxburg, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "July 20, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/icml2010_von_luxburg_ancr/",
    "segmentation": [
        [
            "We've already seen many talks in this workshop where the basic object in the talk was always to graph.",
            "And depending on what information you're given."
        ],
        [
            "Essentially, you build your graph based on local relationships in your data.",
            "For example, you're given.",
            "I don't have a social network in the links in your graph, or whether people are friends to each other or you have neighborhood graph and you connect points which are sort of similar to each other so that the edges in the graph describe the local structure of your data.",
            "Well then the goal of machine learning is in most cases is to instruct to infer global information based on this graph.",
            "So you might want to infer what is the manifold structure in your data.",
            "You might want to know about the cluster structure in your data, and so on.",
            "So the crucial question is always given this local information based on this similarity or link structure in the graph.",
            "How can I get a global characterization of my graph?"
        ],
        [
            "One such way is to try to attempt to build a similar similarity function or a distance function on top of your graph which tries to incorporate the structure of your graph.",
            "So what do I mean by this?",
            "So assume we're given a graph like the one here, and we want to define a distance function between the vertex between the vertices of the graph.",
            "So the standard way computer science classes in many algorithms is.",
            "We just look at the shortest path distance between each vertex and this is our new distance function.",
            "The problem with this distance function is that it throws away most information about the graph because it only looks at the shortest path between each two vertices and ignores completely what is what.",
            "The other parts of the graph say.",
            "The opposite is then to try to define a distance which does incorporate this structure of the graph and the way to achieve this is not to look only at one path between the two vertices, but to look at all paths between two vertices and then try to aggregate this information somehow to one global distance function.",
            "Most prominent example is the commute distance, and I want just want to illustrate what the differences between these two things.",
            "So here I so this is the graph and what I plot here is the shortest path distance to one fixed data point.",
            "So I fix this black data point here and then I color code what the distances of all other vertices in the graph and we can see here that in the shortest path distance.",
            "Essentially this point in this point, for example, have the same distance to this one, so the shortest path distance does not look at the cluster structure, whereas the commute distance.",
            "We see that clearly.",
            "All points which are more or less in the same cluster of the vertex we're looking at have a small distance and points in the other cluster have a large distance.",
            "And this of course seems to be a very attractive proper."
        ],
        [
            "She and you.",
            "The assumption is that once you have that you can do machine learning based on this distance function.",
            "What is the formal way of defining the commute distance?",
            "Essentially, there are two ways, and I will keep track of both of them during this talk because both of them have different intuitions and I find both of them very helpful.",
            "The first definition is based on a random walk, so assume you're giving your graph.",
            "You want to compute the distance between two vertices.",
            "What you do is you start a random walk at the first vertex and you run the random walk until it hits the hits, the second vertex, and you record how long does it take to achieve this?",
            "And then you look at the expected time.",
            "So this is the hitting time hiji the expected time to travel from vertex I to vertex J.",
            "And then the commute distance is simply the symmetrized version of this, where you travel from I to J and then back from data.",
            "This is called the commute distance, and it's already a well known distance it has been in the mathematics literature for many years already.",
            "The alternative definition goes by electrical networks.",
            "Here what you do is you interpret your graph in as an electrical network, where each edge is a sort of, has a resistance and what you want to do is you want to.",
            "What you do is you connect the battery to each of the two vertices you want to look at.",
            "So you want to compute the distance between I&J, say.",
            "So I connect the battery one and two I and the other into J and then I measure what is the effective resistance that the electrical circuit has.",
            "If I do this.",
            "And this is called the resistance distance, and this has already been also been around for quite some time already.",
            "And one can show with that these two definitions are essentially equivalent, so the commute distance is just a constant time the resistance distance.",
            "So essentially we can go back and forth between both definitions, and it doesn't really make a difference."
        ],
        [
            "The resistance distance is used very very in very many areas in machine learning.",
            "Because of this property that it tests that points in the same cluster are supposed to have the same disk."
        ],
        [
            "To each other.",
            "And there are also some other properties which make it quite attractive.",
            "So the first of properties that you can compute it easily.",
            "So and this is not obvious at all, so often if you have like hitting times are defined by recursive equations in random box.",
            "So if you want to compute it often, things get very complicated, but for the resistance distance it's easy to compute it, and it turns out.",
            "And that's actually the formula.",
            "Nicola Nicola also had is you need to look at the pseudo inverse of the graph Laplacian.",
            "And multiply these are the unit vectors license, chase unit vector.",
            "So you have a closed form to compute it.",
            "What is also very attractive that it's a Euclidean distance and this is also not obvious.",
            "So you can prove all these things, but they are already well known and the reason why this is attractive is what you can do now.",
            "You can embed your graph into a Euclidean space and the distance in this Euclidean space corresponds to the commute distance between the vertices in your graph, and So what you can do is you can perform.",
            "You can perform this embedding and then you're in a Euclidean space.",
            "You have all the tools you have in Euclidean spaces, you can run a PCA, or you can run K means or whatever you want.",
            "So this makes it very attractive.",
            "OK. Just go ahead."
        ],
        [
            "And the question I want to answer now is it's statistically question about this distance function.",
            "Now assume we are given in.",
            "We're in a standard statistical setting.",
            "There are data points come from some probability distribution.",
            "What I do is I fix two data points and I build some neighborhood graph on top of these points.",
            "SEO K nearest neighbor graph.",
            "So that's a graph where I connect each point to its K nearest neighbors.",
            "And now I can fix these two points and then I keep on drawing you sample points, new sample points, building a larger and larger graph, and the question I want to answer is Now what happens to the commute distance as the sample size in the size of the graph increases."
        ],
        [
            "And I already want to present the answer on this slide, and I'm going to be a bit more formal in the next slide, but this is sort of the high level message, but we're going to show is.",
            "That if the graph becomes larger and larger, the resistance distance is approximated by a very simple quantity, namely by the inverse degree at the two vertices were looking at.",
            "So the is a degree in the graph.",
            "This is just the the weight of the edge weights of the edges attached to this vertex.",
            "And this seems very surprising, because the degree is a.",
            "It's a very local property of the graph, it just depends on the local neighborhood of the point.",
            "The degree has nothing to do with the topology of the graph, nor does it include anything about the cluster structure.",
            "So it seems that if the graph becomes very large, the resistance distance completely loses this attractive feature of having small distances in the same class and large distances in different cluster.",
            "To the opposite.",
            "So if you have like, just as an example here, we have say two clusters, and here we have a red point and Anna Yellow Point.",
            "When both of them are sort of have high high density.",
            "Then these two points of about the same distance to each other as these two points here.",
            "So the cluster structure is completely irrelevant.",
            "And of course for machine learning this kind of distance function on the right hand side is completely meaningless.",
            "It doesn't in covered structure in the graph.",
            "It also doesn't incorporate any of the original distance.",
            "If we came from an original Indian space, it is simply meaningless.",
            "What is very nice also said this is this result can explain several findings we have in the literature.",
            "So many people observed already that the resistance distance seems to be quite sensitive to the degree of vertices and they observed that if the degree is very large, the resistance distance becomes very small.",
            "And then they tried to sort of by heuristically they tried to sort of counteract this effect by I don't know.",
            "Normalizing by the degree degree or something like that and now sort of this limit result perfectly explains why this is the case.",
            "Just to show that this is as effective.",
            "Exactly how you think that, yeah, right?",
            "Now you're thinking something rather than if you think about it, but the difference is that this holds for a very large graph, and the intuition I presented on in the beginning holds for very small graphs, and we'll see later I have examples later where it's more obvious.",
            "So if the graph is small, this property, this attractive property still holds that you have this cluster Ness in the in the resistance distance, but it gets lost if the graph gets too long, get gets too large.",
            "Without anything, if you add vertices without adding edges, you don't change the graph, but OK, and let's OK, so let me."
        ],
        [
            "Go through the next theorem because here stated formally.",
            "Maybe then it's easier.",
            "So what we're going to look at is this quantity.",
            "Here.",
            "This is the resistance distance minus limit expression.",
            "We want to show convergence to OK and what we're going to do is we're going to bound this by the right hand side and then later on we will see that this right inside goes to 0.",
            "First of all, what we have to take care of is that OK and what I forgot to get to say here is, this is now for a random geometric graph model, so the model is a have this underlying sample of points with endpoints.",
            "I draw these endpoints.",
            "Then I built say the epsilon graph.",
            "So I connect each point to each other point which is in the neighborhood of epsilon.",
            "And then I compute the resistance distance in this graph.",
            "So, and of course the parameter epsilon has to decay in a certain way.",
            "So this is a random graph model.",
            "Now we look at this expression and what is going to happen is that trivially, the degree of the vertices will increase because I had more and more points and more edges to the graph, so the degree will converge to zero trivially, so it doesn't make sense to look at this quantity without rescaling it.",
            "So what we do is we re scale by a factor of end to the end times epsilon to the D, because this is sort of the order of magnitude degree normally has.",
            "Bounded support OK, we have some assumptions, so the distribution has a bounded support and it's bounded away from zero on the support.",
            "Right?",
            "So how does the name works with structure change with respect to the gap between you two galaxies?",
            "Well, the OK the gap goes to 0 but.",
            "OK, so that's maybe a bit.",
            "How should I say it?",
            "When you, when you expect that you have a big distance going from the right to the left Gaussian, then somehow you scaling and the way you construct your graph should respect that gap in between.",
            "Otherwise you don't see the dentist right, and this is sort of.",
            "This is exactly the scaling which is the correct scaling.",
            "So you see the density.",
            "So what we will see?",
            "OK, So what one can show it with this scaling?",
            "Here this risk degree converges to the density of the data point, so this is this degree in the.",
            "We scale exactly in the way that the essentially the degrees in the epsilon graph converge to the to the density in our data space, so this is really the way to rescale us this result.",
            "And what we can show then?",
            "And maybe I can explain it later against in order to make sure that everybody but baby let me just continue to finish this slide and then I go back.",
            "So what we can show we rescaled by this quantity and then we look at the right hand side.",
            "And now if we let tend to Infinity and epsilon to 0 but not too fast and I come back to this later, then one can prove that this high probability converges to one this right inside your converges to zero.",
            "And what you then sees that the rescaled resistance distance converges to.",
            "One over the density at the first point plus one over the density at the second point.",
            "Of the notes.",
            "No becausw.",
            "This is the degree in the actual graph, so I'm giving the epsilon graph so it's not really scared in any way.",
            "It's right, it's a random variable, right, right?",
            "And essentially this rescaled random variable converges to the density.",
            "If you look at your two brothers.",
            "Now we take a note just in the game.",
            "Then the distance would be very large.",
            "As importantly now we take a note from the right or right to the cloud and one from the left cloud.",
            "Now this is very short.",
            "Yeah, yeah.",
            "So maybe what I suggest now is I have two slides now which explain the intuition about this and maybe we can come back to it later in the in the questions section.",
            "Just one more, one more comment and maybe this is sort of goes into the direction of what you're wondering about, so essentially similar results can be approved for all kinds of graphs, not also only for random geometric graphs, but also for graphs with given expected degree distributions.",
            "Or you can also prove it for fixed given graph.",
            "Then you have something like the spectral gap on the right hand side.",
            "The only the only requirement we have if we want to get convergence in the end is that the minimal degree in the graph has to increase with a sample size, otherwise it doesn't hold.",
            "This is just so your example of having many points here.",
            "One point in the middle and many points here doesn't work, so sort of and this is also so we assume we're we're on a on a compact, for example, we're on a compact support, but identity is slightly bounded away from zero, so we have points everywhere.",
            "It's not just because we're in a statistical setting, so we can just have one isolated points number and nothing else around it."
        ],
        [
            "And maybe I start with this intuition because it may be shows what is going on.",
            "Just look at the figure.",
            "So my intuition is here.",
            "I now look at this random walk explanation of the resistance distance.",
            "So say this, is this a part of our identity and we want to compute the resistance distance between this vertex in this vertex now in the graph is very large, so it has very many edges and so on.",
            "So what is going to happen?",
            "Is the random walk starts at this point and then it runs around forever.",
            "It runs around and runs around in the graph is really huge so it.",
            "Takes ages before it comes close to this vertex, J or maybe even even even passes goes back and so on.",
            "It at some point it straight and the point what happens now is that the random one can say by the time the random walk hits this point, it has already forgotten where it came from, because just because essentially it has been running around so long that it it doesn't know whether it started here or here or here somewhere on the path.",
            "Yeah, essentially you can.",
            "You can what we?",
            "I mean we have different proofs for this result so we can bound this sort of this deviation in terms of the spectral gap in the spectral gap is also bound for the hitting time, so this is a way you can state it right?",
            "Right, so essentially the point is a random walk is mixed before it hits J and This is why the resistance.",
            "So the hitting time hij then doesn't depend on anymore.",
            "It only depends on J and the way it does depend on JS by the inverse degree becausw.",
            "Intuitively if there are many edges which go into J, then you're more likely to hit Jay, so it takes faster.",
            "So this is this is very intuitive and but you can prove this by by spectral techniques and looking at the spectral gap of your graph."
        ],
        [
            "A slightly more formal way is in terms of electrical networks.",
            "And here we just look at the figure again.",
            "So what we're going to do now is we so we want to.",
            "So I plotted the graph.",
            "Now in the way that this is our vertex I, and this is the vertex J.",
            "And we now want to exploit Lawson electrical networks, so you might remember from school that in electrical networks, if you have resistances in series, the resistances add and if you have resistances in parallel, they inverse resistances add by by this kind of law here.",
            "If you know it, just look at these three edges here.",
            "These are three edges in parallel.",
            "And if each of the edges has resistance one, then the overall resistance of a parallel circuit with such the edges is 1 / 3.",
            "But by this log here and in general, is this one over the degree of the vertex of S?",
            "And the same happens at this end here.",
            "So these couple of edges here contributes to the overall resistance by as one over the degree of T. And now what is going to happen?",
            "This is we have a random geometric graph.",
            "You have very many edges in between, because it's sort of it splits off here and what you can prove then is that the edges in between.",
            "Essentially, if the graph is large enough, they don't provide any resistance anymore.",
            "So essentially the electricity can in this part of the graphic and more or less flow freely.",
            "It's just the bounding factors.",
            "Are the this vertex in this vertex.",
            "And this is another way of providing an intuition why this why this result holds.",
            "If I want to formally prove it, there are many ways so so by by this argument.",
            "One can one construct a flow technique and approve it by a flow technique, and this I mean all these techniques.",
            "At some point they get very very technical because essentially in this technique what you have to do is you cover your space with a small grid.",
            "Then you need to have a flow between different cells in the grid and you have to control the number of edges which go between all these different grids, grid cells and then sort of upper and lower bound.",
            "What is going to happen to the resistance?",
            "So this is very technical but essentially this intuition."
        ],
        [
            "OK, now one question is of course this is a limit result and the question is how like does it really affect practice?",
            "Or does it only occur if I have 5 billion data points and the answer is 1 really has to take care of it in practice.",
            "What I have here is.",
            "And this is just data from a mixture of Gaussians in the first row I have 100 sample points in here I have 1000 sample points.",
            "What you can see here is this is just the adjacency matrix of the graph.",
            "So you see, it's very nicely clustered.",
            "It's a Gaussian mixture model and you also see the adjacency matrix of the commute distance is also very nicely clustered.",
            "It says it is essentially a test block structure.",
            "Now if we look at the same for 1000 vertices, I guess you can't see it anymore here, so the adjacency matrix still is very nicely clustered, but the dots are so tiny you can see them anymore.",
            "But essentially it is more or less clustered.",
            "Where is the commute distance?",
            "You don't see any class structure at all.",
            "It is already so close to this limit expression that the class structure is essentially lost.",
            "And this already happens for a very small number of data points."
        ],
        [
            "One can also make this a bit more formally.",
            "We ran a couple of experiments or what we look at here is so we look at different random graph or different graphs, an both artificial and actual ones, and what we look at is the resists, minuses, limit expression, but we normalize.",
            "We look at the relative deviation of this quantity because it otherwise it.",
            "Can happen that we divide by zero here and then it sort of isn't meaningful anymore.",
            "So what you can do is you can show for example if you just have uniform data without any cluster structure is expected as the size of the graph increases.",
            "This deviation decreases and just so this is a log scale.",
            "So essentially the deviation here for example is 10 to the minus two, so this is really tiny.",
            "I mean, this is surprisingly time in fact, and this is for.",
            "I guess it's all know here.",
            "It's with, so it increases with and.",
            "So here we are, say 500 data points and this is 3 times of random graph.",
            "So we already have a debate deviation much smaller than 10 to the minus one.",
            "What we can also show is that here we plot the same, but we against the dimension, so this effect gets stronger if the dimension of the data increases.",
            "And this is also clearer because then the graph that has less cluster structure because there are more shortcuts between the clusters.",
            "One can also show the opposite effect if one increases separation between different clusters.",
            "So here's a mixture of Gaussians.",
            "If you increase the separation, then this approximation gets gets worse.",
            "So if the separation between the clusters is larger, I mean it's still on a good scale.",
            "So here we have 10 to the minus one for example.",
            "But if the separation get larger, sort of this approximation formula gets worse.",
            "So these three plots essentially tell you if your data is very, very extremely clustered.",
            "This property of the commute distance still holds, but as soon as this class structure is not as clear anymore, it sort of breaks down.",
            "Sorry.",
            "No.",
            "No, they are.",
            "You can give bounds which just if they depend on the spectral gap and on the minimal maximal degrees in the graph and the minimal maximal weights in your graph essentially.",
            "And here is finally just one one real data set.",
            "This is the USPS data set and we compute what we did is we took all data points.",
            "Is there about 9000 and build AK nearest neighbor graph.",
            "On top of that?",
            "That is what is often done for just classification on this data set.",
            "And here we did it for different parameters K and what you can also see here.",
            "I mean this is for different types of graphs but the deviation is also for like this is K = 10 which is very small if you.",
            "If you think of a graph, is 9000 points.",
            "Usually you would choose K slightly larger then you're already in a deviation of a very very small scale.",
            "OK, and this essentially this slide shows that the commute distance you really, really have to be careful if you use it in practice."
        ],
        [
            "OK, so I think the the main message was and everybody should have got this by now that this is this approximation of the resistance distance and it takes place in large graphs.",
            "There is largest relative, so in practice it can already happen for graphs."
        ],
        [
            "Not so large.",
            "Um, I think this website is also meaningful in a larger context, so.",
            "OK, the first the first context is the context of distances on graphs, so I think many, many there are many different ways where people try to build a global distance function by looking at all paths between different points.",
            "The commute distance is just one of them.",
            "There exist different ways of correcting the commute distance, but there also exists other distances and I think that most of them have exactly the same problem because the path at some point are essentially too long and you run around forever and you sort of you lose, you lose focus.",
            "Essentially, you don't look at the correct things anymore.",
            "The second message is even on our general scale that if you if you apply random walk methods to very large graphs, this is very problematic cause and intuitively this is clear.",
            "If I have a huge graph and if I run a random walk it takes forever before it comes from one vertex to the other vertex.",
            "So it might not be a good idea to look at hitting times or such global measures for random walk because they.",
            "If the random walk is so slow that it already mixed before before the hitting time takes place, the hitting event takes place, then this is completely meaningless to use the setting time, and I think many other algorithms implicitly hitting times are often used in semi supervised learning and there are also like in the social network community people.",
            "For example there are lots of scores where you look at the between the score of order, the centrality score of a certain vertex in these cars are often based on random walks and I think for all of them one has to be very very careful and I'm not sure.",
            "And this is future work to look at all these things and workout which ones still work and which ones have the same problem.",
            "But I think it can happen more generally.",
            "OK, thank you.",
            "So you said large graphs, but you actually mean.",
            "Grasp that bit by something.",
            "Space.",
            "Would you know of spectral crucial spectral properties that causes affected in stochastic grass, for instance, preferential attachment right?",
            "So OK so OK.",
            "So there's the results hold much more.",
            "Generally adjust doesn't present them in this talk so.",
            "You don't even.",
            "I mean you can present you can prove such a deviation formula for just one fixed graph and then so it's not for stochastic model.",
            "But you can prove a deviation formula formula which just says the resistance distance minus this limit form is bounded by something and something just depends on the fixed graph and essentially it's a spectral gap and sort of statistics like minimal and maximal degree and so on.",
            "So in principle you can provide such a formula for any fixed graph.",
            "But then the question is if you want to have conversions in some statistical model.",
            "Of course you have to have some.",
            "Like then then you sort of embedded in a more general context, but essentially you can prove it for a fixed graph and I don't have it in this talk, But essentially if the spectral gap is very.",
            "The smaller the spectral gap, the versus approximation is, so it's something like inverse in this.",
            "In this in the second eigenvalue.",
            "Yep.",
            "Yeah, right?",
            "Increases.",
            "Times.",
            "In applications, what you would do would be somehow through the ground.",
            "So for instance I mean Queen idea.",
            "So experiment spending quite large graphs affecting pixel matches, but there may be restricting the neighborhood structures to be local.",
            "Game results right?",
            "If you take your your analysis and then consider what would happen, for instance if you go for minimum spanning tree all the time.",
            "You do.",
            "OK, so I think so.",
            "In general what you could do is, and we've already considered this.",
            "You could try to subsample your graph to small size.",
            "And then sort of compute compute the commute distance on the small size.",
            "The problem is just how the subsampling should be done, because if you have a huge graph, it's not so clear how to transform into a graph of 100 vertices, say and still keep the cluster cluster structure.",
            "Yeah you, I'm not sure what happens, so the problem is if you subsample the edges at some point, your graph gets disconnected and so on.",
            "So I think it's it's not.",
            "We thought about this and this might be a way of doing it, but it's not straightforward.",
            "That's my first answer.",
            "The second one is if you use the minimal spanning tree.",
            "I think this wouldn't work because the minimal spanning tree does not contain the cluster information anymore.",
            "So and the crucial point is that somewhere in this structure you look at, you need to have this information of whether there is a bottleneck or not.",
            "So I think the back in the definition of the distance comes from the fact that you let you go into the asymptotics, but you still require that you hit the note exactly.",
            "So if you would re scale the hiding property by basically saying what is the first time you reach the neighborhood of this node and scaled it appropriately, then I think at least the.",
            "The no.",
            "OK, so it's not.",
            "We also thought about this and it's not so straightforward.",
            "So essentially what we for example what we tried this you can first of all, not to make sure to make sure that we make steps of a reasonably large size, so not to make these tiny diffusion like steps, but sort of have a minimum step size.",
            "You take something like that.",
            "This doesn't solve the problem.",
            "I think what might work is, and this is something we are working on is sort of cover your space or whatever with reasonably large balls and then just jump from one ball to the other but with but on a reasonably large scale.",
            "This might work, but this is this is has a similar problem as a subsampling problem.",
            "Somehow you need somehow you're giving you a big graph.",
            "You need to transform it to a very small graph, and then you can.",
            "You might be able to solve it, but it's not straightforward to do that.",
            "Do you have any intuition?",
            "Density.",
            "The density the underlying density.",
            "The edge density essentially I only know a dependence on the minimal degree, and this needs to increase with N and then of course the density of the graph also increases.",
            "I'm not sure.",
            "I think.",
            "OK, if you have two classes with a very dense and you have just one bottleneck edge between them, then sort of you have a very small spectral gap in this approximation property will be bad, so in this sense it depends indirectly on the density, but there's not.",
            "It's nothing like it's not a direct dependency, it's sort of more indirect.",
            "I found this analogy with you electric circuit very intuitive because it's quite clear from there, but if you prevalence resistance.",
            "Right, I mean that's a completely different way of designing it.",
            "So there you say we just look at, we look at the path, but then we sort of look at the weakest link in this path.",
            "Something like that.",
            "That's that's a different way of.",
            "Yeah, of course that exists, and I think you've already done something like that long time ago.",
            "It, yeah, yeah, yeah yeah, I mean, yeah, yeah yeah, I mean one other way.",
            "We think we can fix the problem.",
            "This problem is in fact, and it's not so easy to explain.",
            "But if we look at the residual, so we take the resistance distance minus.",
            "Limit expression one can show that the thing that is left.",
            "This is the thing which actually does incorporate the graph structure.",
            "Because what happens is you can write the inverse graph Laplacian as a sum of two things.",
            "One is the inverse degree matrix and this leads to essentially this term and the other thing is something like a kernel matrix.",
            "But this is 1 order of magnitude smaller and if you sort of if you get rid of this first thing here which dominates everything, then you get your left with the thing which is actually encoding the graph structure and this is sort of and we think that this is a way of terms and we also have experiments, but it's sort of not done yet, so I didn't want to present anything.",
            "But I think that is a way of proceeding."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've already seen many talks in this workshop where the basic object in the talk was always to graph.",
                    "label": 0
                },
                {
                    "sent": "And depending on what information you're given.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Essentially, you build your graph based on local relationships in your data.",
                    "label": 1
                },
                {
                    "sent": "For example, you're given.",
                    "label": 0
                },
                {
                    "sent": "I don't have a social network in the links in your graph, or whether people are friends to each other or you have neighborhood graph and you connect points which are sort of similar to each other so that the edges in the graph describe the local structure of your data.",
                    "label": 0
                },
                {
                    "sent": "Well then the goal of machine learning is in most cases is to instruct to infer global information based on this graph.",
                    "label": 1
                },
                {
                    "sent": "So you might want to infer what is the manifold structure in your data.",
                    "label": 0
                },
                {
                    "sent": "You might want to know about the cluster structure in your data, and so on.",
                    "label": 0
                },
                {
                    "sent": "So the crucial question is always given this local information based on this similarity or link structure in the graph.",
                    "label": 0
                },
                {
                    "sent": "How can I get a global characterization of my graph?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One such way is to try to attempt to build a similar similarity function or a distance function on top of your graph which tries to incorporate the structure of your graph.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by this?",
                    "label": 0
                },
                {
                    "sent": "So assume we're given a graph like the one here, and we want to define a distance function between the vertex between the vertices of the graph.",
                    "label": 0
                },
                {
                    "sent": "So the standard way computer science classes in many algorithms is.",
                    "label": 0
                },
                {
                    "sent": "We just look at the shortest path distance between each vertex and this is our new distance function.",
                    "label": 1
                },
                {
                    "sent": "The problem with this distance function is that it throws away most information about the graph because it only looks at the shortest path between each two vertices and ignores completely what is what.",
                    "label": 0
                },
                {
                    "sent": "The other parts of the graph say.",
                    "label": 0
                },
                {
                    "sent": "The opposite is then to try to define a distance which does incorporate this structure of the graph and the way to achieve this is not to look only at one path between the two vertices, but to look at all paths between two vertices and then try to aggregate this information somehow to one global distance function.",
                    "label": 1
                },
                {
                    "sent": "Most prominent example is the commute distance, and I want just want to illustrate what the differences between these two things.",
                    "label": 0
                },
                {
                    "sent": "So here I so this is the graph and what I plot here is the shortest path distance to one fixed data point.",
                    "label": 0
                },
                {
                    "sent": "So I fix this black data point here and then I color code what the distances of all other vertices in the graph and we can see here that in the shortest path distance.",
                    "label": 0
                },
                {
                    "sent": "Essentially this point in this point, for example, have the same distance to this one, so the shortest path distance does not look at the cluster structure, whereas the commute distance.",
                    "label": 0
                },
                {
                    "sent": "We see that clearly.",
                    "label": 0
                },
                {
                    "sent": "All points which are more or less in the same cluster of the vertex we're looking at have a small distance and points in the other cluster have a large distance.",
                    "label": 0
                },
                {
                    "sent": "And this of course seems to be a very attractive proper.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "She and you.",
                    "label": 0
                },
                {
                    "sent": "The assumption is that once you have that you can do machine learning based on this distance function.",
                    "label": 0
                },
                {
                    "sent": "What is the formal way of defining the commute distance?",
                    "label": 0
                },
                {
                    "sent": "Essentially, there are two ways, and I will keep track of both of them during this talk because both of them have different intuitions and I find both of them very helpful.",
                    "label": 0
                },
                {
                    "sent": "The first definition is based on a random walk, so assume you're giving your graph.",
                    "label": 0
                },
                {
                    "sent": "You want to compute the distance between two vertices.",
                    "label": 0
                },
                {
                    "sent": "What you do is you start a random walk at the first vertex and you run the random walk until it hits the hits, the second vertex, and you record how long does it take to achieve this?",
                    "label": 0
                },
                {
                    "sent": "And then you look at the expected time.",
                    "label": 0
                },
                {
                    "sent": "So this is the hitting time hiji the expected time to travel from vertex I to vertex J.",
                    "label": 1
                },
                {
                    "sent": "And then the commute distance is simply the symmetrized version of this, where you travel from I to J and then back from data.",
                    "label": 0
                },
                {
                    "sent": "This is called the commute distance, and it's already a well known distance it has been in the mathematics literature for many years already.",
                    "label": 0
                },
                {
                    "sent": "The alternative definition goes by electrical networks.",
                    "label": 0
                },
                {
                    "sent": "Here what you do is you interpret your graph in as an electrical network, where each edge is a sort of, has a resistance and what you want to do is you want to.",
                    "label": 1
                },
                {
                    "sent": "What you do is you connect the battery to each of the two vertices you want to look at.",
                    "label": 0
                },
                {
                    "sent": "So you want to compute the distance between I&J, say.",
                    "label": 0
                },
                {
                    "sent": "So I connect the battery one and two I and the other into J and then I measure what is the effective resistance that the electrical circuit has.",
                    "label": 0
                },
                {
                    "sent": "If I do this.",
                    "label": 0
                },
                {
                    "sent": "And this is called the resistance distance, and this has already been also been around for quite some time already.",
                    "label": 1
                },
                {
                    "sent": "And one can show with that these two definitions are essentially equivalent, so the commute distance is just a constant time the resistance distance.",
                    "label": 0
                },
                {
                    "sent": "So essentially we can go back and forth between both definitions, and it doesn't really make a difference.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The resistance distance is used very very in very many areas in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Because of this property that it tests that points in the same cluster are supposed to have the same disk.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To each other.",
                    "label": 0
                },
                {
                    "sent": "And there are also some other properties which make it quite attractive.",
                    "label": 0
                },
                {
                    "sent": "So the first of properties that you can compute it easily.",
                    "label": 0
                },
                {
                    "sent": "So and this is not obvious at all, so often if you have like hitting times are defined by recursive equations in random box.",
                    "label": 0
                },
                {
                    "sent": "So if you want to compute it often, things get very complicated, but for the resistance distance it's easy to compute it, and it turns out.",
                    "label": 1
                },
                {
                    "sent": "And that's actually the formula.",
                    "label": 0
                },
                {
                    "sent": "Nicola Nicola also had is you need to look at the pseudo inverse of the graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "And multiply these are the unit vectors license, chase unit vector.",
                    "label": 0
                },
                {
                    "sent": "So you have a closed form to compute it.",
                    "label": 1
                },
                {
                    "sent": "What is also very attractive that it's a Euclidean distance and this is also not obvious.",
                    "label": 1
                },
                {
                    "sent": "So you can prove all these things, but they are already well known and the reason why this is attractive is what you can do now.",
                    "label": 0
                },
                {
                    "sent": "You can embed your graph into a Euclidean space and the distance in this Euclidean space corresponds to the commute distance between the vertices in your graph, and So what you can do is you can perform.",
                    "label": 0
                },
                {
                    "sent": "You can perform this embedding and then you're in a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "You have all the tools you have in Euclidean spaces, you can run a PCA, or you can run K means or whatever you want.",
                    "label": 0
                },
                {
                    "sent": "So this makes it very attractive.",
                    "label": 0
                },
                {
                    "sent": "OK. Just go ahead.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the question I want to answer now is it's statistically question about this distance function.",
                    "label": 0
                },
                {
                    "sent": "Now assume we are given in.",
                    "label": 0
                },
                {
                    "sent": "We're in a standard statistical setting.",
                    "label": 0
                },
                {
                    "sent": "There are data points come from some probability distribution.",
                    "label": 1
                },
                {
                    "sent": "What I do is I fix two data points and I build some neighborhood graph on top of these points.",
                    "label": 1
                },
                {
                    "sent": "SEO K nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "So that's a graph where I connect each point to its K nearest neighbors.",
                    "label": 1
                },
                {
                    "sent": "And now I can fix these two points and then I keep on drawing you sample points, new sample points, building a larger and larger graph, and the question I want to answer is Now what happens to the commute distance as the sample size in the size of the graph increases.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I already want to present the answer on this slide, and I'm going to be a bit more formal in the next slide, but this is sort of the high level message, but we're going to show is.",
                    "label": 0
                },
                {
                    "sent": "That if the graph becomes larger and larger, the resistance distance is approximated by a very simple quantity, namely by the inverse degree at the two vertices were looking at.",
                    "label": 0
                },
                {
                    "sent": "So the is a degree in the graph.",
                    "label": 1
                },
                {
                    "sent": "This is just the the weight of the edge weights of the edges attached to this vertex.",
                    "label": 0
                },
                {
                    "sent": "And this seems very surprising, because the degree is a.",
                    "label": 0
                },
                {
                    "sent": "It's a very local property of the graph, it just depends on the local neighborhood of the point.",
                    "label": 0
                },
                {
                    "sent": "The degree has nothing to do with the topology of the graph, nor does it include anything about the cluster structure.",
                    "label": 0
                },
                {
                    "sent": "So it seems that if the graph becomes very large, the resistance distance completely loses this attractive feature of having small distances in the same class and large distances in different cluster.",
                    "label": 0
                },
                {
                    "sent": "To the opposite.",
                    "label": 0
                },
                {
                    "sent": "So if you have like, just as an example here, we have say two clusters, and here we have a red point and Anna Yellow Point.",
                    "label": 0
                },
                {
                    "sent": "When both of them are sort of have high high density.",
                    "label": 0
                },
                {
                    "sent": "Then these two points of about the same distance to each other as these two points here.",
                    "label": 0
                },
                {
                    "sent": "So the cluster structure is completely irrelevant.",
                    "label": 0
                },
                {
                    "sent": "And of course for machine learning this kind of distance function on the right hand side is completely meaningless.",
                    "label": 1
                },
                {
                    "sent": "It doesn't in covered structure in the graph.",
                    "label": 1
                },
                {
                    "sent": "It also doesn't incorporate any of the original distance.",
                    "label": 0
                },
                {
                    "sent": "If we came from an original Indian space, it is simply meaningless.",
                    "label": 1
                },
                {
                    "sent": "What is very nice also said this is this result can explain several findings we have in the literature.",
                    "label": 1
                },
                {
                    "sent": "So many people observed already that the resistance distance seems to be quite sensitive to the degree of vertices and they observed that if the degree is very large, the resistance distance becomes very small.",
                    "label": 0
                },
                {
                    "sent": "And then they tried to sort of by heuristically they tried to sort of counteract this effect by I don't know.",
                    "label": 0
                },
                {
                    "sent": "Normalizing by the degree degree or something like that and now sort of this limit result perfectly explains why this is the case.",
                    "label": 0
                },
                {
                    "sent": "Just to show that this is as effective.",
                    "label": 0
                },
                {
                    "sent": "Exactly how you think that, yeah, right?",
                    "label": 0
                },
                {
                    "sent": "Now you're thinking something rather than if you think about it, but the difference is that this holds for a very large graph, and the intuition I presented on in the beginning holds for very small graphs, and we'll see later I have examples later where it's more obvious.",
                    "label": 0
                },
                {
                    "sent": "So if the graph is small, this property, this attractive property still holds that you have this cluster Ness in the in the resistance distance, but it gets lost if the graph gets too long, get gets too large.",
                    "label": 0
                },
                {
                    "sent": "Without anything, if you add vertices without adding edges, you don't change the graph, but OK, and let's OK, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go through the next theorem because here stated formally.",
                    "label": 0
                },
                {
                    "sent": "Maybe then it's easier.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to look at is this quantity.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "This is the resistance distance minus limit expression.",
                    "label": 0
                },
                {
                    "sent": "We want to show convergence to OK and what we're going to do is we're going to bound this by the right hand side and then later on we will see that this right inside goes to 0.",
                    "label": 0
                },
                {
                    "sent": "First of all, what we have to take care of is that OK and what I forgot to get to say here is, this is now for a random geometric graph model, so the model is a have this underlying sample of points with endpoints.",
                    "label": 0
                },
                {
                    "sent": "I draw these endpoints.",
                    "label": 0
                },
                {
                    "sent": "Then I built say the epsilon graph.",
                    "label": 1
                },
                {
                    "sent": "So I connect each point to each other point which is in the neighborhood of epsilon.",
                    "label": 0
                },
                {
                    "sent": "And then I compute the resistance distance in this graph.",
                    "label": 0
                },
                {
                    "sent": "So, and of course the parameter epsilon has to decay in a certain way.",
                    "label": 0
                },
                {
                    "sent": "So this is a random graph model.",
                    "label": 0
                },
                {
                    "sent": "Now we look at this expression and what is going to happen is that trivially, the degree of the vertices will increase because I had more and more points and more edges to the graph, so the degree will converge to zero trivially, so it doesn't make sense to look at this quantity without rescaling it.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we re scale by a factor of end to the end times epsilon to the D, because this is sort of the order of magnitude degree normally has.",
                    "label": 0
                },
                {
                    "sent": "Bounded support OK, we have some assumptions, so the distribution has a bounded support and it's bounded away from zero on the support.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So how does the name works with structure change with respect to the gap between you two galaxies?",
                    "label": 0
                },
                {
                    "sent": "Well, the OK the gap goes to 0 but.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's maybe a bit.",
                    "label": 0
                },
                {
                    "sent": "How should I say it?",
                    "label": 0
                },
                {
                    "sent": "When you, when you expect that you have a big distance going from the right to the left Gaussian, then somehow you scaling and the way you construct your graph should respect that gap in between.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you don't see the dentist right, and this is sort of.",
                    "label": 0
                },
                {
                    "sent": "This is exactly the scaling which is the correct scaling.",
                    "label": 1
                },
                {
                    "sent": "So you see the density.",
                    "label": 0
                },
                {
                    "sent": "So what we will see?",
                    "label": 0
                },
                {
                    "sent": "OK, So what one can show it with this scaling?",
                    "label": 0
                },
                {
                    "sent": "Here this risk degree converges to the density of the data point, so this is this degree in the.",
                    "label": 0
                },
                {
                    "sent": "We scale exactly in the way that the essentially the degrees in the epsilon graph converge to the to the density in our data space, so this is really the way to rescale us this result.",
                    "label": 0
                },
                {
                    "sent": "And what we can show then?",
                    "label": 0
                },
                {
                    "sent": "And maybe I can explain it later against in order to make sure that everybody but baby let me just continue to finish this slide and then I go back.",
                    "label": 0
                },
                {
                    "sent": "So what we can show we rescaled by this quantity and then we look at the right hand side.",
                    "label": 0
                },
                {
                    "sent": "And now if we let tend to Infinity and epsilon to 0 but not too fast and I come back to this later, then one can prove that this high probability converges to one this right inside your converges to zero.",
                    "label": 0
                },
                {
                    "sent": "And what you then sees that the rescaled resistance distance converges to.",
                    "label": 0
                },
                {
                    "sent": "One over the density at the first point plus one over the density at the second point.",
                    "label": 0
                },
                {
                    "sent": "Of the notes.",
                    "label": 0
                },
                {
                    "sent": "No becausw.",
                    "label": 0
                },
                {
                    "sent": "This is the degree in the actual graph, so I'm giving the epsilon graph so it's not really scared in any way.",
                    "label": 0
                },
                {
                    "sent": "It's right, it's a random variable, right, right?",
                    "label": 0
                },
                {
                    "sent": "And essentially this rescaled random variable converges to the density.",
                    "label": 0
                },
                {
                    "sent": "If you look at your two brothers.",
                    "label": 0
                },
                {
                    "sent": "Now we take a note just in the game.",
                    "label": 0
                },
                {
                    "sent": "Then the distance would be very large.",
                    "label": 0
                },
                {
                    "sent": "As importantly now we take a note from the right or right to the cloud and one from the left cloud.",
                    "label": 0
                },
                {
                    "sent": "Now this is very short.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So maybe what I suggest now is I have two slides now which explain the intuition about this and maybe we can come back to it later in the in the questions section.",
                    "label": 0
                },
                {
                    "sent": "Just one more, one more comment and maybe this is sort of goes into the direction of what you're wondering about, so essentially similar results can be approved for all kinds of graphs, not also only for random geometric graphs, but also for graphs with given expected degree distributions.",
                    "label": 0
                },
                {
                    "sent": "Or you can also prove it for fixed given graph.",
                    "label": 0
                },
                {
                    "sent": "Then you have something like the spectral gap on the right hand side.",
                    "label": 1
                },
                {
                    "sent": "The only the only requirement we have if we want to get convergence in the end is that the minimal degree in the graph has to increase with a sample size, otherwise it doesn't hold.",
                    "label": 0
                },
                {
                    "sent": "This is just so your example of having many points here.",
                    "label": 0
                },
                {
                    "sent": "One point in the middle and many points here doesn't work, so sort of and this is also so we assume we're we're on a on a compact, for example, we're on a compact support, but identity is slightly bounded away from zero, so we have points everywhere.",
                    "label": 0
                },
                {
                    "sent": "It's not just because we're in a statistical setting, so we can just have one isolated points number and nothing else around it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And maybe I start with this intuition because it may be shows what is going on.",
                    "label": 0
                },
                {
                    "sent": "Just look at the figure.",
                    "label": 0
                },
                {
                    "sent": "So my intuition is here.",
                    "label": 0
                },
                {
                    "sent": "I now look at this random walk explanation of the resistance distance.",
                    "label": 1
                },
                {
                    "sent": "So say this, is this a part of our identity and we want to compute the resistance distance between this vertex in this vertex now in the graph is very large, so it has very many edges and so on.",
                    "label": 1
                },
                {
                    "sent": "So what is going to happen?",
                    "label": 0
                },
                {
                    "sent": "Is the random walk starts at this point and then it runs around forever.",
                    "label": 0
                },
                {
                    "sent": "It runs around and runs around in the graph is really huge so it.",
                    "label": 0
                },
                {
                    "sent": "Takes ages before it comes close to this vertex, J or maybe even even even passes goes back and so on.",
                    "label": 0
                },
                {
                    "sent": "It at some point it straight and the point what happens now is that the random one can say by the time the random walk hits this point, it has already forgotten where it came from, because just because essentially it has been running around so long that it it doesn't know whether it started here or here or here somewhere on the path.",
                    "label": 0
                },
                {
                    "sent": "Yeah, essentially you can.",
                    "label": 0
                },
                {
                    "sent": "You can what we?",
                    "label": 1
                },
                {
                    "sent": "I mean we have different proofs for this result so we can bound this sort of this deviation in terms of the spectral gap in the spectral gap is also bound for the hitting time, so this is a way you can state it right?",
                    "label": 0
                },
                {
                    "sent": "Right, so essentially the point is a random walk is mixed before it hits J and This is why the resistance.",
                    "label": 0
                },
                {
                    "sent": "So the hitting time hij then doesn't depend on anymore.",
                    "label": 0
                },
                {
                    "sent": "It only depends on J and the way it does depend on JS by the inverse degree becausw.",
                    "label": 0
                },
                {
                    "sent": "Intuitively if there are many edges which go into J, then you're more likely to hit Jay, so it takes faster.",
                    "label": 0
                },
                {
                    "sent": "So this is this is very intuitive and but you can prove this by by spectral techniques and looking at the spectral gap of your graph.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A slightly more formal way is in terms of electrical networks.",
                    "label": 0
                },
                {
                    "sent": "And here we just look at the figure again.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do now is we so we want to.",
                    "label": 0
                },
                {
                    "sent": "So I plotted the graph.",
                    "label": 0
                },
                {
                    "sent": "Now in the way that this is our vertex I, and this is the vertex J.",
                    "label": 1
                },
                {
                    "sent": "And we now want to exploit Lawson electrical networks, so you might remember from school that in electrical networks, if you have resistances in series, the resistances add and if you have resistances in parallel, they inverse resistances add by by this kind of law here.",
                    "label": 0
                },
                {
                    "sent": "If you know it, just look at these three edges here.",
                    "label": 0
                },
                {
                    "sent": "These are three edges in parallel.",
                    "label": 1
                },
                {
                    "sent": "And if each of the edges has resistance one, then the overall resistance of a parallel circuit with such the edges is 1 / 3.",
                    "label": 0
                },
                {
                    "sent": "But by this log here and in general, is this one over the degree of the vertex of S?",
                    "label": 1
                },
                {
                    "sent": "And the same happens at this end here.",
                    "label": 0
                },
                {
                    "sent": "So these couple of edges here contributes to the overall resistance by as one over the degree of T. And now what is going to happen?",
                    "label": 0
                },
                {
                    "sent": "This is we have a random geometric graph.",
                    "label": 1
                },
                {
                    "sent": "You have very many edges in between, because it's sort of it splits off here and what you can prove then is that the edges in between.",
                    "label": 1
                },
                {
                    "sent": "Essentially, if the graph is large enough, they don't provide any resistance anymore.",
                    "label": 0
                },
                {
                    "sent": "So essentially the electricity can in this part of the graphic and more or less flow freely.",
                    "label": 0
                },
                {
                    "sent": "It's just the bounding factors.",
                    "label": 0
                },
                {
                    "sent": "Are the this vertex in this vertex.",
                    "label": 0
                },
                {
                    "sent": "And this is another way of providing an intuition why this why this result holds.",
                    "label": 0
                },
                {
                    "sent": "If I want to formally prove it, there are many ways so so by by this argument.",
                    "label": 0
                },
                {
                    "sent": "One can one construct a flow technique and approve it by a flow technique, and this I mean all these techniques.",
                    "label": 0
                },
                {
                    "sent": "At some point they get very very technical because essentially in this technique what you have to do is you cover your space with a small grid.",
                    "label": 0
                },
                {
                    "sent": "Then you need to have a flow between different cells in the grid and you have to control the number of edges which go between all these different grids, grid cells and then sort of upper and lower bound.",
                    "label": 0
                },
                {
                    "sent": "What is going to happen to the resistance?",
                    "label": 0
                },
                {
                    "sent": "So this is very technical but essentially this intuition.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now one question is of course this is a limit result and the question is how like does it really affect practice?",
                    "label": 0
                },
                {
                    "sent": "Or does it only occur if I have 5 billion data points and the answer is 1 really has to take care of it in practice.",
                    "label": 0
                },
                {
                    "sent": "What I have here is.",
                    "label": 0
                },
                {
                    "sent": "And this is just data from a mixture of Gaussians in the first row I have 100 sample points in here I have 1000 sample points.",
                    "label": 1
                },
                {
                    "sent": "What you can see here is this is just the adjacency matrix of the graph.",
                    "label": 0
                },
                {
                    "sent": "So you see, it's very nicely clustered.",
                    "label": 0
                },
                {
                    "sent": "It's a Gaussian mixture model and you also see the adjacency matrix of the commute distance is also very nicely clustered.",
                    "label": 0
                },
                {
                    "sent": "It says it is essentially a test block structure.",
                    "label": 0
                },
                {
                    "sent": "Now if we look at the same for 1000 vertices, I guess you can't see it anymore here, so the adjacency matrix still is very nicely clustered, but the dots are so tiny you can see them anymore.",
                    "label": 0
                },
                {
                    "sent": "But essentially it is more or less clustered.",
                    "label": 0
                },
                {
                    "sent": "Where is the commute distance?",
                    "label": 0
                },
                {
                    "sent": "You don't see any class structure at all.",
                    "label": 0
                },
                {
                    "sent": "It is already so close to this limit expression that the class structure is essentially lost.",
                    "label": 0
                },
                {
                    "sent": "And this already happens for a very small number of data points.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One can also make this a bit more formally.",
                    "label": 0
                },
                {
                    "sent": "We ran a couple of experiments or what we look at here is so we look at different random graph or different graphs, an both artificial and actual ones, and what we look at is the resists, minuses, limit expression, but we normalize.",
                    "label": 0
                },
                {
                    "sent": "We look at the relative deviation of this quantity because it otherwise it.",
                    "label": 0
                },
                {
                    "sent": "Can happen that we divide by zero here and then it sort of isn't meaningful anymore.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is you can show for example if you just have uniform data without any cluster structure is expected as the size of the graph increases.",
                    "label": 0
                },
                {
                    "sent": "This deviation decreases and just so this is a log scale.",
                    "label": 0
                },
                {
                    "sent": "So essentially the deviation here for example is 10 to the minus two, so this is really tiny.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is surprisingly time in fact, and this is for.",
                    "label": 0
                },
                {
                    "sent": "I guess it's all know here.",
                    "label": 0
                },
                {
                    "sent": "It's with, so it increases with and.",
                    "label": 0
                },
                {
                    "sent": "So here we are, say 500 data points and this is 3 times of random graph.",
                    "label": 0
                },
                {
                    "sent": "So we already have a debate deviation much smaller than 10 to the minus one.",
                    "label": 0
                },
                {
                    "sent": "What we can also show is that here we plot the same, but we against the dimension, so this effect gets stronger if the dimension of the data increases.",
                    "label": 0
                },
                {
                    "sent": "And this is also clearer because then the graph that has less cluster structure because there are more shortcuts between the clusters.",
                    "label": 0
                },
                {
                    "sent": "One can also show the opposite effect if one increases separation between different clusters.",
                    "label": 0
                },
                {
                    "sent": "So here's a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "If you increase the separation, then this approximation gets gets worse.",
                    "label": 0
                },
                {
                    "sent": "So if the separation between the clusters is larger, I mean it's still on a good scale.",
                    "label": 0
                },
                {
                    "sent": "So here we have 10 to the minus one for example.",
                    "label": 0
                },
                {
                    "sent": "But if the separation get larger, sort of this approximation formula gets worse.",
                    "label": 0
                },
                {
                    "sent": "So these three plots essentially tell you if your data is very, very extremely clustered.",
                    "label": 0
                },
                {
                    "sent": "This property of the commute distance still holds, but as soon as this class structure is not as clear anymore, it sort of breaks down.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "No, they are.",
                    "label": 0
                },
                {
                    "sent": "You can give bounds which just if they depend on the spectral gap and on the minimal maximal degrees in the graph and the minimal maximal weights in your graph essentially.",
                    "label": 0
                },
                {
                    "sent": "And here is finally just one one real data set.",
                    "label": 0
                },
                {
                    "sent": "This is the USPS data set and we compute what we did is we took all data points.",
                    "label": 0
                },
                {
                    "sent": "Is there about 9000 and build AK nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "On top of that?",
                    "label": 0
                },
                {
                    "sent": "That is what is often done for just classification on this data set.",
                    "label": 0
                },
                {
                    "sent": "And here we did it for different parameters K and what you can also see here.",
                    "label": 0
                },
                {
                    "sent": "I mean this is for different types of graphs but the deviation is also for like this is K = 10 which is very small if you.",
                    "label": 0
                },
                {
                    "sent": "If you think of a graph, is 9000 points.",
                    "label": 0
                },
                {
                    "sent": "Usually you would choose K slightly larger then you're already in a deviation of a very very small scale.",
                    "label": 0
                },
                {
                    "sent": "OK, and this essentially this slide shows that the commute distance you really, really have to be careful if you use it in practice.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I think the the main message was and everybody should have got this by now that this is this approximation of the resistance distance and it takes place in large graphs.",
                    "label": 0
                },
                {
                    "sent": "There is largest relative, so in practice it can already happen for graphs.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not so large.",
                    "label": 0
                },
                {
                    "sent": "Um, I think this website is also meaningful in a larger context, so.",
                    "label": 0
                },
                {
                    "sent": "OK, the first the first context is the context of distances on graphs, so I think many, many there are many different ways where people try to build a global distance function by looking at all paths between different points.",
                    "label": 0
                },
                {
                    "sent": "The commute distance is just one of them.",
                    "label": 1
                },
                {
                    "sent": "There exist different ways of correcting the commute distance, but there also exists other distances and I think that most of them have exactly the same problem because the path at some point are essentially too long and you run around forever and you sort of you lose, you lose focus.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you don't look at the correct things anymore.",
                    "label": 0
                },
                {
                    "sent": "The second message is even on our general scale that if you if you apply random walk methods to very large graphs, this is very problematic cause and intuitively this is clear.",
                    "label": 1
                },
                {
                    "sent": "If I have a huge graph and if I run a random walk it takes forever before it comes from one vertex to the other vertex.",
                    "label": 0
                },
                {
                    "sent": "So it might not be a good idea to look at hitting times or such global measures for random walk because they.",
                    "label": 0
                },
                {
                    "sent": "If the random walk is so slow that it already mixed before before the hitting time takes place, the hitting event takes place, then this is completely meaningless to use the setting time, and I think many other algorithms implicitly hitting times are often used in semi supervised learning and there are also like in the social network community people.",
                    "label": 0
                },
                {
                    "sent": "For example there are lots of scores where you look at the between the score of order, the centrality score of a certain vertex in these cars are often based on random walks and I think for all of them one has to be very very careful and I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "And this is future work to look at all these things and workout which ones still work and which ones have the same problem.",
                    "label": 0
                },
                {
                    "sent": "But I think it can happen more generally.",
                    "label": 1
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So you said large graphs, but you actually mean.",
                    "label": 0
                },
                {
                    "sent": "Grasp that bit by something.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "Would you know of spectral crucial spectral properties that causes affected in stochastic grass, for instance, preferential attachment right?",
                    "label": 0
                },
                {
                    "sent": "So OK so OK.",
                    "label": 0
                },
                {
                    "sent": "So there's the results hold much more.",
                    "label": 0
                },
                {
                    "sent": "Generally adjust doesn't present them in this talk so.",
                    "label": 0
                },
                {
                    "sent": "You don't even.",
                    "label": 0
                },
                {
                    "sent": "I mean you can present you can prove such a deviation formula for just one fixed graph and then so it's not for stochastic model.",
                    "label": 0
                },
                {
                    "sent": "But you can prove a deviation formula formula which just says the resistance distance minus this limit form is bounded by something and something just depends on the fixed graph and essentially it's a spectral gap and sort of statistics like minimal and maximal degree and so on.",
                    "label": 0
                },
                {
                    "sent": "So in principle you can provide such a formula for any fixed graph.",
                    "label": 0
                },
                {
                    "sent": "But then the question is if you want to have conversions in some statistical model.",
                    "label": 0
                },
                {
                    "sent": "Of course you have to have some.",
                    "label": 0
                },
                {
                    "sent": "Like then then you sort of embedded in a more general context, but essentially you can prove it for a fixed graph and I don't have it in this talk, But essentially if the spectral gap is very.",
                    "label": 0
                },
                {
                    "sent": "The smaller the spectral gap, the versus approximation is, so it's something like inverse in this.",
                    "label": 1
                },
                {
                    "sent": "In this in the second eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right?",
                    "label": 0
                },
                {
                    "sent": "Increases.",
                    "label": 0
                },
                {
                    "sent": "Times.",
                    "label": 0
                },
                {
                    "sent": "In applications, what you would do would be somehow through the ground.",
                    "label": 0
                },
                {
                    "sent": "So for instance I mean Queen idea.",
                    "label": 0
                },
                {
                    "sent": "So experiment spending quite large graphs affecting pixel matches, but there may be restricting the neighborhood structures to be local.",
                    "label": 0
                },
                {
                    "sent": "Game results right?",
                    "label": 0
                },
                {
                    "sent": "If you take your your analysis and then consider what would happen, for instance if you go for minimum spanning tree all the time.",
                    "label": 0
                },
                {
                    "sent": "You do.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think so.",
                    "label": 0
                },
                {
                    "sent": "In general what you could do is, and we've already considered this.",
                    "label": 0
                },
                {
                    "sent": "You could try to subsample your graph to small size.",
                    "label": 0
                },
                {
                    "sent": "And then sort of compute compute the commute distance on the small size.",
                    "label": 0
                },
                {
                    "sent": "The problem is just how the subsampling should be done, because if you have a huge graph, it's not so clear how to transform into a graph of 100 vertices, say and still keep the cluster cluster structure.",
                    "label": 0
                },
                {
                    "sent": "Yeah you, I'm not sure what happens, so the problem is if you subsample the edges at some point, your graph gets disconnected and so on.",
                    "label": 0
                },
                {
                    "sent": "So I think it's it's not.",
                    "label": 0
                },
                {
                    "sent": "We thought about this and this might be a way of doing it, but it's not straightforward.",
                    "label": 0
                },
                {
                    "sent": "That's my first answer.",
                    "label": 0
                },
                {
                    "sent": "The second one is if you use the minimal spanning tree.",
                    "label": 0
                },
                {
                    "sent": "I think this wouldn't work because the minimal spanning tree does not contain the cluster information anymore.",
                    "label": 0
                },
                {
                    "sent": "So and the crucial point is that somewhere in this structure you look at, you need to have this information of whether there is a bottleneck or not.",
                    "label": 0
                },
                {
                    "sent": "So I think the back in the definition of the distance comes from the fact that you let you go into the asymptotics, but you still require that you hit the note exactly.",
                    "label": 0
                },
                {
                    "sent": "So if you would re scale the hiding property by basically saying what is the first time you reach the neighborhood of this node and scaled it appropriately, then I think at least the.",
                    "label": 0
                },
                {
                    "sent": "The no.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not.",
                    "label": 0
                },
                {
                    "sent": "We also thought about this and it's not so straightforward.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we for example what we tried this you can first of all, not to make sure to make sure that we make steps of a reasonably large size, so not to make these tiny diffusion like steps, but sort of have a minimum step size.",
                    "label": 0
                },
                {
                    "sent": "You take something like that.",
                    "label": 1
                },
                {
                    "sent": "This doesn't solve the problem.",
                    "label": 0
                },
                {
                    "sent": "I think what might work is, and this is something we are working on is sort of cover your space or whatever with reasonably large balls and then just jump from one ball to the other but with but on a reasonably large scale.",
                    "label": 0
                },
                {
                    "sent": "This might work, but this is this is has a similar problem as a subsampling problem.",
                    "label": 0
                },
                {
                    "sent": "Somehow you need somehow you're giving you a big graph.",
                    "label": 0
                },
                {
                    "sent": "You need to transform it to a very small graph, and then you can.",
                    "label": 0
                },
                {
                    "sent": "You might be able to solve it, but it's not straightforward to do that.",
                    "label": 0
                },
                {
                    "sent": "Do you have any intuition?",
                    "label": 0
                },
                {
                    "sent": "Density.",
                    "label": 0
                },
                {
                    "sent": "The density the underlying density.",
                    "label": 0
                },
                {
                    "sent": "The edge density essentially I only know a dependence on the minimal degree, and this needs to increase with N and then of course the density of the graph also increases.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "OK, if you have two classes with a very dense and you have just one bottleneck edge between them, then sort of you have a very small spectral gap in this approximation property will be bad, so in this sense it depends indirectly on the density, but there's not.",
                    "label": 0
                },
                {
                    "sent": "It's nothing like it's not a direct dependency, it's sort of more indirect.",
                    "label": 0
                },
                {
                    "sent": "I found this analogy with you electric circuit very intuitive because it's quite clear from there, but if you prevalence resistance.",
                    "label": 0
                },
                {
                    "sent": "Right, I mean that's a completely different way of designing it.",
                    "label": 0
                },
                {
                    "sent": "So there you say we just look at, we look at the path, but then we sort of look at the weakest link in this path.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "That's that's a different way of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course that exists, and I think you've already done something like that long time ago.",
                    "label": 0
                },
                {
                    "sent": "It, yeah, yeah, yeah yeah, I mean, yeah, yeah yeah, I mean one other way.",
                    "label": 0
                },
                {
                    "sent": "We think we can fix the problem.",
                    "label": 0
                },
                {
                    "sent": "This problem is in fact, and it's not so easy to explain.",
                    "label": 1
                },
                {
                    "sent": "But if we look at the residual, so we take the resistance distance minus.",
                    "label": 0
                },
                {
                    "sent": "Limit expression one can show that the thing that is left.",
                    "label": 0
                },
                {
                    "sent": "This is the thing which actually does incorporate the graph structure.",
                    "label": 0
                },
                {
                    "sent": "Because what happens is you can write the inverse graph Laplacian as a sum of two things.",
                    "label": 0
                },
                {
                    "sent": "One is the inverse degree matrix and this leads to essentially this term and the other thing is something like a kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "But this is 1 order of magnitude smaller and if you sort of if you get rid of this first thing here which dominates everything, then you get your left with the thing which is actually encoding the graph structure and this is sort of and we think that this is a way of terms and we also have experiments, but it's sort of not done yet, so I didn't want to present anything.",
                    "label": 0
                },
                {
                    "sent": "But I think that is a way of proceeding.",
                    "label": 0
                }
            ]
        }
    }
}