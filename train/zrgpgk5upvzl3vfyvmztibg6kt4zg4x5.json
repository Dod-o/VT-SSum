{
    "id": "zrgpgk5upvzl3vfyvmztibg6kt4zg4x5",
    "title": "Learning the structure of deep sparse graphical models",
    "info": {
        "author": [
            "Hanna M. Wallach, Department of Computer Science, University of Massachusetts Amherst"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/aistats2010_wallach_ltsods/",
    "segmentation": [
        [
            "Hi everyone, I'm Hannah and I'm going to be talking about learning the structure of deep sparse graphical models and this is joint work with Ryan Adams and Zubin Ghahramani, neither of whom could be here today.",
            "But I'm sure they would."
        ],
        [
            "I'd like to.",
            "So as I'm sure you all know, deep belief networks have been extremely popular in the past couple of years, and there's been a lot of talk about them.",
            "What you might not know is what they are, so I thought I'd start with a little quote from Geoff Hinton from Scholarpedia, and I guess I'll just read it to you.",
            "Deep belief Nets are probabilistic generative models that are composed of multiple layers of stochastic latent variables.",
            "The latent variables typically have binary values and are often called hidden units or feature detectors.",
            "The lower layers received top down directed connections from the layer above the states of the units in the lowest layer represented data vector.",
            "So we've got these probabilistic generative models and their layered.",
            "There's multiple different layers.",
            "The layers are in all but the bottommost layer hidden their latent variables.",
            "And in the bottommost layer, you've actually got the visible variables, so like images and that kind of thing."
        ],
        [
            "So.",
            "Here's a picture of the kind of thing that I'm talking about.",
            "This is what I mean by that, OK?",
            "Possibly.",
            "This is what I mean by a deep belief network, and I don't think my laser pointer works very well.",
            "So we've got multiple different layers and we've got connections from each layer to the layer below.",
            "All the way down to the bottom most layer where we've got visible units, and I've denoted the visible units by these kind of double circles, and so when we're looking at these kinds of models, there are a bunch of different structural questions that we can ask for a start we can."
        ],
        [
            "Ask how many.",
            "How many units should there be in each hidden layer.",
            "So how wide should each hidden layer be?"
        ],
        [
            "We can also ask how many hidden layers should there be?",
            "How deep should the network structure be?"
        ],
        [
            "We can ask what should the network connectivity be?",
            "So how should the units from one layer be connected up?"
        ],
        [
            "The layer below and also how should each individual hidden unit behave?",
            "Should it have a continuous representation of binary representation?",
            "That kind of thing?",
            "And so one of the biggest questions when looking."
        ],
        [
            "At these kinds of models is what should the network structure be, and can we actually learn this network structure rather than specifying it in advance?",
            "So."
        ],
        [
            "So that's what I'm going to be talking about in this talk, so I apologize for the little stock images.",
            "I tried to get online to get people's photographs, but as I'm sure you're all aware, the Internet is not working so well, so you have to make do with the standard stock images of people that were on my computer.",
            "So this talk is about a nonparametric Bayesian approach for learning the structure of a layered, directed deep belief network.",
            "And one of the nice things about this piece of work is that it really unites two areas of research that haven't particularly been united in the past.",
            "Deep belief networks that's represented by Jeff Hinton over there on the left and nonparametric Bayesian inference represented by those three people there on the right."
        ],
        [
            "So this is what I'm going to be talking about.",
            "I'm going to tell you a little bit about finite single layer networks to start off with to make sure we're all kind of on the same page.",
            "Then I'm going to talk about making these networks infinite, so learning the number of hidden units in each layer, learning the number of hidden layers and learning the types of unit behavior, and then finally I'll present some experimental results."
        ],
        [
            "Alright, so finite."
        ],
        [
            "Single layer networks.",
            "Well, finally single layer networks.",
            "We've simply got two layers.",
            "We've got the bottommost layer, which is the visible layer.",
            "It's actually, you know the observations and then we've got a single layer of hidden units.",
            "And one thing about this kind of model is that it's very simple.",
            "We just got two layers.",
            "We've just got connections between these two layers and we can use a binary matrix to represent the edge structure or the network connectivity of one of these networks.",
            "And so over here we've got.",
            "At the bottom I've got a little example of a really simple finite single layer network, and then at the top I've got a picture of a binary matrix that represents the edge edge structure and the number of hidden invisible variables in this network.",
            "So you see, if you look at the so each of the rows corresponds to a single visible unit and shows which hidden units that visible unit is connected to.",
            "Similarly, each column represents a hidden unit and shows which of the visible units that hidden unit is connected to.",
            "So the first row has two entries that are non 02 entries that are one and that simply represents the fact that the first visible unit is connected to the 1st and 2nd hidden unit, and so when we're thinking about these kinds of matrices representing the structure of one of these models, one thing that's important to notice is that a prior distribution on binary matrices therefore implies a prior distribution on single layered belief networks.",
            "If we can come up with some kind of prior distribution on binary matrices.",
            "We've got a prior distribution on single layer belief networks."
        ],
        [
            "So that's fine.",
            "We're talking about, you know these kind of finite finite models where we've got a finite number of hidden units.",
            "But suppose we actually don't know how many hidden units there should be.",
            "Suppose we actually maybe want to have it potentially infinite number of hidden units.",
            "Well, that would correspond to having a potentially infinite number of columns.",
            "So is there some way that we can actually come up with a model, maybe a prior distribution on binary matrices that would allow binary matrices with a potentially infinite number of columns?",
            "And the answer is yes, the Indian buffet process actually gives us exactly this kind of prior so."
        ],
        [
            "I'm going to talk about learning the number of hidden units in each layer, and we're going to do this using the Indian buffet process so."
        ],
        [
            "So infinitely wide layers.",
            "It's possible to use an infinite an Indian buffet process as a prior and binary matrices as I said and the kind of matrices that this is a prior over have countably infinite columns.",
            "In other words, an unbounded number of hidden units in the networks of these matrices represent all, though there's a countably infinite number of columns, an unbounded number of hidden units for any given data set, posterior inference simply determines the subset of the hidden units that are actually responsible from the observations, so that we could have potentially.",
            "Arbitrarily many hidden units we're going to use posterior inference to determine the subset of these units that are responsible for the observations, and the ICP ensures that matrices these binary matrices that this is a prior over are extremely sparse.",
            "There's only a small number of nonzero columns.",
            "In other words, there's only a small number of hidden units that are active in the model.",
            "Um?",
            "And So what I'm going to do now is run you through a little illustration."
        ],
        [
            "All of the Indian buffet process.",
            "So the idea in the Indian buffet process is that it's a prior over binary matrices and these matrices can have any number of columns.",
            "So I've tried to represent that by the fact that I've got, you know, a large number of columns here.",
            "That is sort of fading out to unshaded.",
            "And the metaphor is as follows.",
            "So we have a bunch of customers who arrive in an Indian buffet and they each sample some number of dishes and the number of dishes they sample is given by some particular probabilities.",
            "There are, so there's a couple of different variants of the Indian buffet process.",
            "One has a single parameter Alpha, the other one has two parameters, Alpha and beta.",
            "Here, I'm going to be talking about the variant with two parameters, Alpha and beta.",
            "So the first customer, the first observation who walks into this restaurant, tries.",
            "A number of dishes that is given by plus an Alpha, so that's what we've got represented in this first row.",
            "Here this customer has tried three dishes, the next customer."
        ],
        [
            "Who walks into the restaurant?",
            "Tries previously tasted Dish K with probability given by the number of that time that that DISH has been tasted by a previous customer divided by this quantity there beta plus the total number of customers who've entered the restaurant so far, minus one.",
            "This customer, in addition to trying existing dishes so the second customer is tried OK, they haven't tried to fire station.",
            "They have tried the second dish and haven't tried the third one.",
            "They're also going to try some number of completely new dishes and the number of completely new dishes that they're going to try is given by this quantity here, plus an Alpha beta divided by beta plus and minus Alpha and so in this case they've chosen to try one completely new dish, the 4th dish, the next customer."
        ],
        [
            "Does the same thing they try previously tasted dishes with probability proportional to the number of times that those dishes have been tasted, and new dishes.",
            "Again, with this, with the probability on the slide next customer does the same."
        ],
        [
            "Thing, and if you run through doing this, what you end up with is something that looks like this.",
            "You end up with a binary matrix, zeros and ones that basically show which customers or which visible units have tasted which dishes.",
            "In other words, which hidden units these visible units are connected to."
        ],
        [
            "So some properties of the Indian buffet process for a finite number of customers.",
            "There will always be a finite number of dishes tasted the rows and the columns are infinitely exchangeable.",
            "There is a related stick breaking construction and the Indian buffet process has been used previously for shared latent feature or hidden cause models in order to basically infer how many latent features there should be in such models and which ones are related to which observations.",
            "And one of the nice things about this.",
            "These kinds of models is that although there are potentially infinitely many dishes or hidden hidden variables, these latent features can be added and removed from the model without using any kind of dimensionality altering MCMC methods.",
            "Any units that are not connected to visible units simply factor out and you don't need to worry about them."
        ],
        [
            "So I was talking about customers and dishes before and now I'm just going to run through quickly and show how this relates to a single layer belief network, just to make that try and make that clear.",
            "So we've got customers which correspond to visible units and dishes which correspond to hidden units.",
            "So I've got a network down here on the bottom with four visible you."
        ],
        [
            "It's in a whole bunch of hidden units and the first customer, the first visible unit, is connected to 3 hidden units.",
            "In other words, that customer has tasted three dishes.",
            "The next."
        ],
        [
            "Every unit is connected to 2 hidden units that customer has tasted two dishes and the next one, and so on."
        ],
        [
            "So in this way we can actually sample the connectivity of a single layer of finite belief."
        ],
        [
            "Sorry, single Labony belief network with a potentially infinite number of hidden units.",
            "By actually sampling from the Indian buffet process.",
            "And that's fine, but single layer belief networks have pretty limited utility apriori.",
            "All the hidden units are independent, So what can actually be represented with one of these models is pretty simple.",
            "In contrast, deep networks have multiple hidden layers, so the hidden layers in any level apart from the top level or actually related via the hidden hidden units at the next level up, and so the hidden units are actually dependent a priority, and as a result you can actually represent a lot more complicated things using such networks.",
            "So we wondered whether it would be possible to extend the Indian buffet process in order to construct deep belief networks with an unbounded width and an unbounded depth as well."
        ],
        [
            "So now I'm going to talk about learning the number of hidden layers in one of these network."
        ],
        [
            "So I'm just going to run through and show you kind of how a multilayer belief network is constructed, constructed or put."
        ],
        [
            "Together at the bottom again, you've got visible units.",
            "Then you've got a layer of hidden units with some connections between them that are represented."
        ],
        [
            "By binary matrix.",
            "Oops.",
            "Next level up you have another layer of hidden units which are connected to the hidden units on the layer below via binary."
        ],
        [
            "Matrix of connections.",
            "Another layer of hidden units, again connected to the layer below.",
            "We can represent the connections unit using a binary matrix.",
            "And so on."
        ],
        [
            "And so one way of representing this kind of thing, if we knew the number of layers we wanted to use, we could use a finite number of Indian buffet process is.",
            "But what about actually using an infinite recursion of these Indian buffet processes in which every dish is a customer in another restaurant?",
            "And if we actually do this, then we can define a model where we have an unbounded number of layers each of unbounded width.",
            "So each Indian buffet process gives us an unbounded number of hidden units in that layer.",
            "And the fact that each customer in that restaurants are each dish in that restaurant becomes a customer in another restaurant means that we can have an unbounded number of layers.",
            "And the interesting thing about this is that you might think that this process was kind of recurse forever and you continue to sample from one of these things.",
            "You just continue to sample new layers continuously until you know, well, Infinity, but in fact we will actually always hit a layer with 0 units.",
            "We will always reach the point at which no customer will try.",
            "Any dish in a restaurant and this is great because it means we will always sample networks that have a finite but unbounded depth.",
            "And this is exactly the property we want.",
            "We don't want to make an ad hoc choice of the network depth we want to actually be able to kind of learn this kind of thing.",
            "The proof for actually proving that we will always hit a layer with 0 units.",
            "I'm not going to go into here, but it's in the paper if you want to look at the details.",
            "So I'm going to call this can."
        ],
        [
            "Action the Cascading Indian buffet process and it's a stochastic process that results in an infinite sequence of infinite binary matrices and each matrix is exchangeable in both rows and columns.",
            "So if I permute the variables at one layer and propagate this all the way down, the probability of the entire model is unchanged.",
            "And so one thing that we might want to ask is how do we know that the Cascading Indian buffet converges?",
            "How do we know that it's actually going to, you know, stop at some point and we're not going to have any more layers and the way we can prove this is.",
            "We can note that the number of dishes in any given layer depends only on the number of customers in the previous layer, and that sort of defines a Markov chain.",
            "And then if we can prove that this Markov chain reaches an absorbing state and finite time with probability one, we've proved that these things actually converge.",
            "So as I was saying before, this is in the paper.",
            "If you want to look at the details."
        ],
        [
            "So some properties of the sea ICP, the expected number of parents for any hidden unit is Alpha.",
            "The expected number of children for any hidden unit varies between the number of units in the layer that this unit is connected to an one and where the exact number of children that any hidden unit has between those two extremes is given by the parameter beta, so varying beta various expected number of children and just one minor point.",
            "We don't really want network properties to be constant across all depths.",
            "We want some levels to be sparser than some other levels.",
            "And so we can have layer specific Indian buffet process parameters."
        ],
        [
            "What are the samples from the Cascading Indian buffet process look like?",
            "So here are some samples from the prior over.",
            "On the left we have samples where we were using an Alpha of one and beta values of 1/2 and oh sorry Alpha of 1 1/2.",
            "One for the first 2 1/2 of the second one we've got beta values of also 1 1/2 over here we've got Alpha and beta values of 1, two and one and a half and one and so you can see the kinds of network structures that you get if you sample from the prior using different.",
            "Different parameter values."
        ],
        [
            "Uh."
        ],
        [
            "Learning the types of unit behavior.",
            "So if we assume that unit activations are as is typical in these models, weighted linear sums with biases, then we actually decided to use the nonlinear Gaussian belief network approach, which says that the probability of any given unit is given by a sigmoid function which takes as the arguments to that function the activation some plus Gaussian noise with some precision new and you see as we vary that parameter, we get three different types of unit behavior.",
            "With a small value of that precision parameter, we get binary behavior with an intermediate value.",
            "We get Gaussian behavior and with a really large value we get more deterministic behavior, so learning that parameter is going to enable us to learn the behavior of the hidden units."
        ],
        [
            "Prizing inference, I'm actually not going to talk about the priors inference for all of these things can be done with Markov chain Monte Carlo.",
            "Parameter inference is really easy given the network structure, and we can add and remove edge."
        ],
        [
            "Is using metropolis?"
        ],
        [
            "These things I'm now going to show you some pretty pictures, so we run this stuff on three different data sets of different sizes.",
            "The first one is the Olivetti data set, which is faces than the endless digit data set, which is digits, and finally the fry face data set, which again is faces but have only a single person and for all of these datasets and model info to round 3 hidden layers.",
            "When we draw samples from this we're looking at around 3 hidden layers.",
            "We've got different numbers of hidden units in each layer depending on which data set we're looking at.",
            "Um?"
        ],
        [
            "The stuff on the left.",
            "What we've got is in the leftmost column.",
            "We've got true images in the column next to it, so again, you know the grayscale column over there on the left, but the second one over.",
            "We're doing image reconstruction, so we're giving the model the top half of these faces, and saying, can you reconstruct the bottom half?",
            "And they don't look bad, they look alright.",
            "Then over next to that, these are the feature weights from each of the hidden units in the first hidden layer to the visible units, and you can see from that the kind of features we're getting.",
            "Our face specific features, so we getting mouths were getting noses.",
            "We're getting eyebrow detectors, things like that, so these features actually at that level are actually really nicely interpretable."
        ],
        [
            "I'm not going to talk about that, although the ones right on the left are fantasies from the model.",
            "So we just sampled faces from the model to give you an idea of the kinds of things that you can generate using this.",
            "'cause this is a generative model, digits, again, we've given."
        ],
        [
            "Model the top half of the digits and are set to predict the bottom half.",
            "One thing to note is the reason why we're predicting the bottom half given the top half, rather than say, the left half given the right half is because with faces and actually with many digits, you have this left right symmetry and so you don't want to be doing something as trivial as that.",
            "Again, we've got some feature weights.",
            "What we're seeing here is that these things the network is actually much sparser for the digit data than it was for the face data not going to."
        ],
        [
            "By the third thing, so over here again, we've got true.",
            "Faith is an.",
            "Then what happens when we try and predict the bottom half of each face?",
            "The weights for the phrase face data are actually much, much denser, and one reason why is that all of these faces are of the same person, so we're not learning things.",
            "We're learning things.",
            "Sorry on a much kind of finer grained kind of granularity.",
            "We're looking at these much more kind of like global across the entire face, kind of features as opposed to just detecting eyebrows.",
            "OK."
        ],
        [
            "So to summarize, this piece of work kind of unites two areas of research, deep belief networks and Bayesian nonparametrics, and it introduces a new Bayesian nonparametric process.",
            "The Cascading Indian buffet process.",
            "We prove convergence, convergence properties of it in the paper, so if you want to look those up, you can do that, and in doing this we address 3 issues with deep belief networks.",
            "The number of learning the number of units in each hidden layer, learning the number of hidden layers and also learning the type of unit behavior."
        ],
        [
            "So thanks everyone, and that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone, I'm Hannah and I'm going to be talking about learning the structure of deep sparse graphical models and this is joint work with Ryan Adams and Zubin Ghahramani, neither of whom could be here today.",
                    "label": 0
                },
                {
                    "sent": "But I'm sure they would.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'd like to.",
                    "label": 0
                },
                {
                    "sent": "So as I'm sure you all know, deep belief networks have been extremely popular in the past couple of years, and there's been a lot of talk about them.",
                    "label": 0
                },
                {
                    "sent": "What you might not know is what they are, so I thought I'd start with a little quote from Geoff Hinton from Scholarpedia, and I guess I'll just read it to you.",
                    "label": 0
                },
                {
                    "sent": "Deep belief Nets are probabilistic generative models that are composed of multiple layers of stochastic latent variables.",
                    "label": 1
                },
                {
                    "sent": "The latent variables typically have binary values and are often called hidden units or feature detectors.",
                    "label": 0
                },
                {
                    "sent": "The lower layers received top down directed connections from the layer above the states of the units in the lowest layer represented data vector.",
                    "label": 0
                },
                {
                    "sent": "So we've got these probabilistic generative models and their layered.",
                    "label": 0
                },
                {
                    "sent": "There's multiple different layers.",
                    "label": 0
                },
                {
                    "sent": "The layers are in all but the bottommost layer hidden their latent variables.",
                    "label": 0
                },
                {
                    "sent": "And in the bottommost layer, you've actually got the visible variables, so like images and that kind of thing.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's a picture of the kind of thing that I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "This is what I mean by that, OK?",
                    "label": 0
                },
                {
                    "sent": "Possibly.",
                    "label": 0
                },
                {
                    "sent": "This is what I mean by a deep belief network, and I don't think my laser pointer works very well.",
                    "label": 0
                },
                {
                    "sent": "So we've got multiple different layers and we've got connections from each layer to the layer below.",
                    "label": 0
                },
                {
                    "sent": "All the way down to the bottom most layer where we've got visible units, and I've denoted the visible units by these kind of double circles, and so when we're looking at these kinds of models, there are a bunch of different structural questions that we can ask for a start we can.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ask how many.",
                    "label": 0
                },
                {
                    "sent": "How many units should there be in each hidden layer.",
                    "label": 1
                },
                {
                    "sent": "So how wide should each hidden layer be?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also ask how many hidden layers should there be?",
                    "label": 0
                },
                {
                    "sent": "How deep should the network structure be?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can ask what should the network connectivity be?",
                    "label": 0
                },
                {
                    "sent": "So how should the units from one layer be connected up?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The layer below and also how should each individual hidden unit behave?",
                    "label": 0
                },
                {
                    "sent": "Should it have a continuous representation of binary representation?",
                    "label": 0
                },
                {
                    "sent": "That kind of thing?",
                    "label": 0
                },
                {
                    "sent": "And so one of the biggest questions when looking.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At these kinds of models is what should the network structure be, and can we actually learn this network structure rather than specifying it in advance?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's what I'm going to be talking about in this talk, so I apologize for the little stock images.",
                    "label": 0
                },
                {
                    "sent": "I tried to get online to get people's photographs, but as I'm sure you're all aware, the Internet is not working so well, so you have to make do with the standard stock images of people that were on my computer.",
                    "label": 0
                },
                {
                    "sent": "So this talk is about a nonparametric Bayesian approach for learning the structure of a layered, directed deep belief network.",
                    "label": 1
                },
                {
                    "sent": "And one of the nice things about this piece of work is that it really unites two areas of research that haven't particularly been united in the past.",
                    "label": 0
                },
                {
                    "sent": "Deep belief networks that's represented by Jeff Hinton over there on the left and nonparametric Bayesian inference represented by those three people there on the right.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is what I'm going to be talking about.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you a little bit about finite single layer networks to start off with to make sure we're all kind of on the same page.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about making these networks infinite, so learning the number of hidden units in each layer, learning the number of hidden layers and learning the types of unit behavior, and then finally I'll present some experimental results.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so finite.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Single layer networks.",
                    "label": 0
                },
                {
                    "sent": "Well, finally single layer networks.",
                    "label": 0
                },
                {
                    "sent": "We've simply got two layers.",
                    "label": 0
                },
                {
                    "sent": "We've got the bottommost layer, which is the visible layer.",
                    "label": 0
                },
                {
                    "sent": "It's actually, you know the observations and then we've got a single layer of hidden units.",
                    "label": 0
                },
                {
                    "sent": "And one thing about this kind of model is that it's very simple.",
                    "label": 0
                },
                {
                    "sent": "We just got two layers.",
                    "label": 0
                },
                {
                    "sent": "We've just got connections between these two layers and we can use a binary matrix to represent the edge structure or the network connectivity of one of these networks.",
                    "label": 1
                },
                {
                    "sent": "And so over here we've got.",
                    "label": 0
                },
                {
                    "sent": "At the bottom I've got a little example of a really simple finite single layer network, and then at the top I've got a picture of a binary matrix that represents the edge edge structure and the number of hidden invisible variables in this network.",
                    "label": 0
                },
                {
                    "sent": "So you see, if you look at the so each of the rows corresponds to a single visible unit and shows which hidden units that visible unit is connected to.",
                    "label": 0
                },
                {
                    "sent": "Similarly, each column represents a hidden unit and shows which of the visible units that hidden unit is connected to.",
                    "label": 0
                },
                {
                    "sent": "So the first row has two entries that are non 02 entries that are one and that simply represents the fact that the first visible unit is connected to the 1st and 2nd hidden unit, and so when we're thinking about these kinds of matrices representing the structure of one of these models, one thing that's important to notice is that a prior distribution on binary matrices therefore implies a prior distribution on single layered belief networks.",
                    "label": 1
                },
                {
                    "sent": "If we can come up with some kind of prior distribution on binary matrices.",
                    "label": 1
                },
                {
                    "sent": "We've got a prior distribution on single layer belief networks.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's fine.",
                    "label": 0
                },
                {
                    "sent": "We're talking about, you know these kind of finite finite models where we've got a finite number of hidden units.",
                    "label": 0
                },
                {
                    "sent": "But suppose we actually don't know how many hidden units there should be.",
                    "label": 0
                },
                {
                    "sent": "Suppose we actually maybe want to have it potentially infinite number of hidden units.",
                    "label": 1
                },
                {
                    "sent": "Well, that would correspond to having a potentially infinite number of columns.",
                    "label": 0
                },
                {
                    "sent": "So is there some way that we can actually come up with a model, maybe a prior distribution on binary matrices that would allow binary matrices with a potentially infinite number of columns?",
                    "label": 0
                },
                {
                    "sent": "And the answer is yes, the Indian buffet process actually gives us exactly this kind of prior so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about learning the number of hidden units in each layer, and we're going to do this using the Indian buffet process so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So infinitely wide layers.",
                    "label": 0
                },
                {
                    "sent": "It's possible to use an infinite an Indian buffet process as a prior and binary matrices as I said and the kind of matrices that this is a prior over have countably infinite columns.",
                    "label": 1
                },
                {
                    "sent": "In other words, an unbounded number of hidden units in the networks of these matrices represent all, though there's a countably infinite number of columns, an unbounded number of hidden units for any given data set, posterior inference simply determines the subset of the hidden units that are actually responsible from the observations, so that we could have potentially.",
                    "label": 1
                },
                {
                    "sent": "Arbitrarily many hidden units we're going to use posterior inference to determine the subset of these units that are responsible for the observations, and the ICP ensures that matrices these binary matrices that this is a prior over are extremely sparse.",
                    "label": 0
                },
                {
                    "sent": "There's only a small number of nonzero columns.",
                    "label": 0
                },
                {
                    "sent": "In other words, there's only a small number of hidden units that are active in the model.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And So what I'm going to do now is run you through a little illustration.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All of the Indian buffet process.",
                    "label": 1
                },
                {
                    "sent": "So the idea in the Indian buffet process is that it's a prior over binary matrices and these matrices can have any number of columns.",
                    "label": 0
                },
                {
                    "sent": "So I've tried to represent that by the fact that I've got, you know, a large number of columns here.",
                    "label": 0
                },
                {
                    "sent": "That is sort of fading out to unshaded.",
                    "label": 0
                },
                {
                    "sent": "And the metaphor is as follows.",
                    "label": 0
                },
                {
                    "sent": "So we have a bunch of customers who arrive in an Indian buffet and they each sample some number of dishes and the number of dishes they sample is given by some particular probabilities.",
                    "label": 0
                },
                {
                    "sent": "There are, so there's a couple of different variants of the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "One has a single parameter Alpha, the other one has two parameters, Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "Here, I'm going to be talking about the variant with two parameters, Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "So the first customer, the first observation who walks into this restaurant, tries.",
                    "label": 0
                },
                {
                    "sent": "A number of dishes that is given by plus an Alpha, so that's what we've got represented in this first row.",
                    "label": 0
                },
                {
                    "sent": "Here this customer has tried three dishes, the next customer.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who walks into the restaurant?",
                    "label": 0
                },
                {
                    "sent": "Tries previously tasted Dish K with probability given by the number of that time that that DISH has been tasted by a previous customer divided by this quantity there beta plus the total number of customers who've entered the restaurant so far, minus one.",
                    "label": 1
                },
                {
                    "sent": "This customer, in addition to trying existing dishes so the second customer is tried OK, they haven't tried to fire station.",
                    "label": 0
                },
                {
                    "sent": "They have tried the second dish and haven't tried the third one.",
                    "label": 0
                },
                {
                    "sent": "They're also going to try some number of completely new dishes and the number of completely new dishes that they're going to try is given by this quantity here, plus an Alpha beta divided by beta plus and minus Alpha and so in this case they've chosen to try one completely new dish, the 4th dish, the next customer.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does the same thing they try previously tasted dishes with probability proportional to the number of times that those dishes have been tasted, and new dishes.",
                    "label": 0
                },
                {
                    "sent": "Again, with this, with the probability on the slide next customer does the same.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing, and if you run through doing this, what you end up with is something that looks like this.",
                    "label": 0
                },
                {
                    "sent": "You end up with a binary matrix, zeros and ones that basically show which customers or which visible units have tasted which dishes.",
                    "label": 0
                },
                {
                    "sent": "In other words, which hidden units these visible units are connected to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some properties of the Indian buffet process for a finite number of customers.",
                    "label": 1
                },
                {
                    "sent": "There will always be a finite number of dishes tasted the rows and the columns are infinitely exchangeable.",
                    "label": 1
                },
                {
                    "sent": "There is a related stick breaking construction and the Indian buffet process has been used previously for shared latent feature or hidden cause models in order to basically infer how many latent features there should be in such models and which ones are related to which observations.",
                    "label": 0
                },
                {
                    "sent": "And one of the nice things about this.",
                    "label": 0
                },
                {
                    "sent": "These kinds of models is that although there are potentially infinitely many dishes or hidden hidden variables, these latent features can be added and removed from the model without using any kind of dimensionality altering MCMC methods.",
                    "label": 0
                },
                {
                    "sent": "Any units that are not connected to visible units simply factor out and you don't need to worry about them.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I was talking about customers and dishes before and now I'm just going to run through quickly and show how this relates to a single layer belief network, just to make that try and make that clear.",
                    "label": 0
                },
                {
                    "sent": "So we've got customers which correspond to visible units and dishes which correspond to hidden units.",
                    "label": 1
                },
                {
                    "sent": "So I've got a network down here on the bottom with four visible you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's in a whole bunch of hidden units and the first customer, the first visible unit, is connected to 3 hidden units.",
                    "label": 0
                },
                {
                    "sent": "In other words, that customer has tasted three dishes.",
                    "label": 0
                },
                {
                    "sent": "The next.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Every unit is connected to 2 hidden units that customer has tasted two dishes and the next one, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this way we can actually sample the connectivity of a single layer of finite belief.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry, single Labony belief network with a potentially infinite number of hidden units.",
                    "label": 0
                },
                {
                    "sent": "By actually sampling from the Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "And that's fine, but single layer belief networks have pretty limited utility apriori.",
                    "label": 1
                },
                {
                    "sent": "All the hidden units are independent, So what can actually be represented with one of these models is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "In contrast, deep networks have multiple hidden layers, so the hidden layers in any level apart from the top level or actually related via the hidden hidden units at the next level up, and so the hidden units are actually dependent a priority, and as a result you can actually represent a lot more complicated things using such networks.",
                    "label": 0
                },
                {
                    "sent": "So we wondered whether it would be possible to extend the Indian buffet process in order to construct deep belief networks with an unbounded width and an unbounded depth as well.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to talk about learning the number of hidden layers in one of these network.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm just going to run through and show you kind of how a multilayer belief network is constructed, constructed or put.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Together at the bottom again, you've got visible units.",
                    "label": 0
                },
                {
                    "sent": "Then you've got a layer of hidden units with some connections between them that are represented.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By binary matrix.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "Next level up you have another layer of hidden units which are connected to the hidden units on the layer below via binary.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix of connections.",
                    "label": 0
                },
                {
                    "sent": "Another layer of hidden units, again connected to the layer below.",
                    "label": 0
                },
                {
                    "sent": "We can represent the connections unit using a binary matrix.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so one way of representing this kind of thing, if we knew the number of layers we wanted to use, we could use a finite number of Indian buffet process is.",
                    "label": 0
                },
                {
                    "sent": "But what about actually using an infinite recursion of these Indian buffet processes in which every dish is a customer in another restaurant?",
                    "label": 1
                },
                {
                    "sent": "And if we actually do this, then we can define a model where we have an unbounded number of layers each of unbounded width.",
                    "label": 0
                },
                {
                    "sent": "So each Indian buffet process gives us an unbounded number of hidden units in that layer.",
                    "label": 0
                },
                {
                    "sent": "And the fact that each customer in that restaurants are each dish in that restaurant becomes a customer in another restaurant means that we can have an unbounded number of layers.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing about this is that you might think that this process was kind of recurse forever and you continue to sample from one of these things.",
                    "label": 0
                },
                {
                    "sent": "You just continue to sample new layers continuously until you know, well, Infinity, but in fact we will actually always hit a layer with 0 units.",
                    "label": 0
                },
                {
                    "sent": "We will always reach the point at which no customer will try.",
                    "label": 0
                },
                {
                    "sent": "Any dish in a restaurant and this is great because it means we will always sample networks that have a finite but unbounded depth.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly the property we want.",
                    "label": 1
                },
                {
                    "sent": "We don't want to make an ad hoc choice of the network depth we want to actually be able to kind of learn this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "The proof for actually proving that we will always hit a layer with 0 units.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into here, but it's in the paper if you want to look at the details.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to call this can.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action the Cascading Indian buffet process and it's a stochastic process that results in an infinite sequence of infinite binary matrices and each matrix is exchangeable in both rows and columns.",
                    "label": 1
                },
                {
                    "sent": "So if I permute the variables at one layer and propagate this all the way down, the probability of the entire model is unchanged.",
                    "label": 0
                },
                {
                    "sent": "And so one thing that we might want to ask is how do we know that the Cascading Indian buffet converges?",
                    "label": 0
                },
                {
                    "sent": "How do we know that it's actually going to, you know, stop at some point and we're not going to have any more layers and the way we can prove this is.",
                    "label": 1
                },
                {
                    "sent": "We can note that the number of dishes in any given layer depends only on the number of customers in the previous layer, and that sort of defines a Markov chain.",
                    "label": 0
                },
                {
                    "sent": "And then if we can prove that this Markov chain reaches an absorbing state and finite time with probability one, we've proved that these things actually converge.",
                    "label": 0
                },
                {
                    "sent": "So as I was saying before, this is in the paper.",
                    "label": 0
                },
                {
                    "sent": "If you want to look at the details.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some properties of the sea ICP, the expected number of parents for any hidden unit is Alpha.",
                    "label": 0
                },
                {
                    "sent": "The expected number of children for any hidden unit varies between the number of units in the layer that this unit is connected to an one and where the exact number of children that any hidden unit has between those two extremes is given by the parameter beta, so varying beta various expected number of children and just one minor point.",
                    "label": 0
                },
                {
                    "sent": "We don't really want network properties to be constant across all depths.",
                    "label": 1
                },
                {
                    "sent": "We want some levels to be sparser than some other levels.",
                    "label": 0
                },
                {
                    "sent": "And so we can have layer specific Indian buffet process parameters.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are the samples from the Cascading Indian buffet process look like?",
                    "label": 0
                },
                {
                    "sent": "So here are some samples from the prior over.",
                    "label": 1
                },
                {
                    "sent": "On the left we have samples where we were using an Alpha of one and beta values of 1/2 and oh sorry Alpha of 1 1/2.",
                    "label": 0
                },
                {
                    "sent": "One for the first 2 1/2 of the second one we've got beta values of also 1 1/2 over here we've got Alpha and beta values of 1, two and one and a half and one and so you can see the kinds of network structures that you get if you sample from the prior using different.",
                    "label": 0
                },
                {
                    "sent": "Different parameter values.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning the types of unit behavior.",
                    "label": 0
                },
                {
                    "sent": "So if we assume that unit activations are as is typical in these models, weighted linear sums with biases, then we actually decided to use the nonlinear Gaussian belief network approach, which says that the probability of any given unit is given by a sigmoid function which takes as the arguments to that function the activation some plus Gaussian noise with some precision new and you see as we vary that parameter, we get three different types of unit behavior.",
                    "label": 1
                },
                {
                    "sent": "With a small value of that precision parameter, we get binary behavior with an intermediate value.",
                    "label": 0
                },
                {
                    "sent": "We get Gaussian behavior and with a really large value we get more deterministic behavior, so learning that parameter is going to enable us to learn the behavior of the hidden units.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prizing inference, I'm actually not going to talk about the priors inference for all of these things can be done with Markov chain Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Parameter inference is really easy given the network structure, and we can add and remove edge.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is using metropolis?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These things I'm now going to show you some pretty pictures, so we run this stuff on three different data sets of different sizes.",
                    "label": 0
                },
                {
                    "sent": "The first one is the Olivetti data set, which is faces than the endless digit data set, which is digits, and finally the fry face data set, which again is faces but have only a single person and for all of these datasets and model info to round 3 hidden layers.",
                    "label": 1
                },
                {
                    "sent": "When we draw samples from this we're looking at around 3 hidden layers.",
                    "label": 1
                },
                {
                    "sent": "We've got different numbers of hidden units in each layer depending on which data set we're looking at.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The stuff on the left.",
                    "label": 0
                },
                {
                    "sent": "What we've got is in the leftmost column.",
                    "label": 0
                },
                {
                    "sent": "We've got true images in the column next to it, so again, you know the grayscale column over there on the left, but the second one over.",
                    "label": 0
                },
                {
                    "sent": "We're doing image reconstruction, so we're giving the model the top half of these faces, and saying, can you reconstruct the bottom half?",
                    "label": 0
                },
                {
                    "sent": "And they don't look bad, they look alright.",
                    "label": 0
                },
                {
                    "sent": "Then over next to that, these are the feature weights from each of the hidden units in the first hidden layer to the visible units, and you can see from that the kind of features we're getting.",
                    "label": 0
                },
                {
                    "sent": "Our face specific features, so we getting mouths were getting noses.",
                    "label": 0
                },
                {
                    "sent": "We're getting eyebrow detectors, things like that, so these features actually at that level are actually really nicely interpretable.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not going to talk about that, although the ones right on the left are fantasies from the model.",
                    "label": 0
                },
                {
                    "sent": "So we just sampled faces from the model to give you an idea of the kinds of things that you can generate using this.",
                    "label": 0
                },
                {
                    "sent": "'cause this is a generative model, digits, again, we've given.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model the top half of the digits and are set to predict the bottom half.",
                    "label": 0
                },
                {
                    "sent": "One thing to note is the reason why we're predicting the bottom half given the top half, rather than say, the left half given the right half is because with faces and actually with many digits, you have this left right symmetry and so you don't want to be doing something as trivial as that.",
                    "label": 0
                },
                {
                    "sent": "Again, we've got some feature weights.",
                    "label": 0
                },
                {
                    "sent": "What we're seeing here is that these things the network is actually much sparser for the digit data than it was for the face data not going to.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the third thing, so over here again, we've got true.",
                    "label": 0
                },
                {
                    "sent": "Faith is an.",
                    "label": 0
                },
                {
                    "sent": "Then what happens when we try and predict the bottom half of each face?",
                    "label": 0
                },
                {
                    "sent": "The weights for the phrase face data are actually much, much denser, and one reason why is that all of these faces are of the same person, so we're not learning things.",
                    "label": 0
                },
                {
                    "sent": "We're learning things.",
                    "label": 0
                },
                {
                    "sent": "Sorry on a much kind of finer grained kind of granularity.",
                    "label": 0
                },
                {
                    "sent": "We're looking at these much more kind of like global across the entire face, kind of features as opposed to just detecting eyebrows.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, this piece of work kind of unites two areas of research, deep belief networks and Bayesian nonparametrics, and it introduces a new Bayesian nonparametric process.",
                    "label": 0
                },
                {
                    "sent": "The Cascading Indian buffet process.",
                    "label": 0
                },
                {
                    "sent": "We prove convergence, convergence properties of it in the paper, so if you want to look those up, you can do that, and in doing this we address 3 issues with deep belief networks.",
                    "label": 1
                },
                {
                    "sent": "The number of learning the number of units in each hidden layer, learning the number of hidden layers and also learning the type of unit behavior.",
                    "label": 1
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks everyone, and that's it.",
                    "label": 0
                }
            ]
        }
    }
}