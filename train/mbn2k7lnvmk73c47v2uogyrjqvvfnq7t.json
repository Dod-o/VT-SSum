{
    "id": "mbn2k7lnvmk73c47v2uogyrjqvvfnq7t",
    "title": "Supervised Clustering with Support Vector Machines",
    "info": {
        "author": [
            "Thomas Finley, Cornell University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml05_finley_scsvm/",
    "segmentation": [
        [
            "I was introduced on Tom Finley.",
            "I'm here to talk to you today about supervised clustering with support vector machines.",
            "And this is work that I have done with Torsten Yokums at Cornell University.",
            "Alright, first, let's talk about what we're going to talk about.",
            "First, we're going to talk about what I mean."
        ],
        [
            "By supervised clustering and why we might want to do it, then we're going to discuss why it's an interesting problem.",
            "Then we're going to talk about our way of doing supervised clustering, and then we're going to show some results.",
            "OK, so suppose that we want to cluster a collection of more."
        ],
        [
            "Now, it might be that the user wants has some criteria in mind for how they want to cluster these models.",
            "They might want to cluster my color or by size."
        ],
        [
            "Or by whether they're transparent or not, for example.",
            "So if we have a goal like this in mind, how do we make the clustering algorithm come up with the desired partition?"
        ],
        [
            "Partitioning thank you.",
            "Well.",
            "One way that we could do it is we could simply manually adjust the clustering algorithm or the similarity measure.",
            "Um, but we might want to do sort of an automatic.",
            "That have an automatic way of adjusting the clustering algorithm, and for this we have fields like semi supervised clustering.",
            "The most common instance of this is some form of constrained clustering, where the user specifies that they want, say, two items in the same cluster or tour items not in the same cluster, and the algorithm works to satisfy those constraints.",
            "But we're not talking about that.",
            "What we're talking about is fully supervised clustering, and in this case.",
            "You have multiple sets of items.",
            "And you are and the learning algorithm is provided with complete partition partitionings of those sets and it learns how to partition future sets of items.",
            "Based on these examples just to give you an idea of what I mean.",
            "One example of this is noun phrase."
        ],
        [
            "Correct for reference, where sets of items.",
            "Are the are the collection of noun phrases in a particular document and the clustering of those noun phrases are are the partition of the non phrases that refer to the same entity.",
            "For example, the two he's in the policeman that would form one cluster, the cat and Peter would form another cluster and hopefully it would learn to win given a new document.",
            "Find out which noun phrases?",
            "Cover for another example is."
        ],
        [
            "News, story clustering.",
            "Sort of like what Google News does, for example.",
            "Where you're given, say, the collection of all news articles from a particular day.",
            "And you want to cluster them according to which refer to the same story.",
            "OK, and there are other examples like image partitioning and image segmentation which we don't cover in this talk.",
            "OK, so let's."
        ],
        [
            "Formalize it a bit.",
            "Well, as I said, were provided with sets of items and example clusterings.",
            "And we learn how to cluster future sets of items in a similar way.",
            "So OK, just to specify what we mean."
        ],
        [
            "By clustering we suppose that we have a set of items, let's say an items, in this case 10.",
            "We suppose that we have some sort of similarity measure, but for all item pairs."
        ],
        [
            "And what we want to do is we want to produce a partitioning of the item set with with respect to some objective function.",
            "And in this work what we do is we hold the the clustering algorithm constant and we just change the similarity measure so that all of the so that it will tend to produce the types of clusterings that we want.",
            "So in this particular work we looked at."
        ],
        [
            "Correlation clustering.",
            "You could also use another clustering algorithm if you wanted, but in this case we just use correlation clustering.",
            "Correlation clustering is very simple.",
            "Basically, if we suppose that we have all these items similar in impaired similarities, it finds the partitioning of the set.",
            "So that the sum of Inter cluster similarities is maximized.",
            "And of course, this this requires having some negative similarities.",
            "Annual note that it will allow certain discrepancies if the net effect of including that pair in the same cluster or not is beneficial.",
            "OK so.",
            "Now that we've defined correlation clustering, let's talk about the types of similarity that we here consider."
        ],
        [
            "Well, basically all that we have, we have the similarity between all item pairs and in order to get this similarity, we suppose that we have a feature vector for all item pairs.",
            "This Phi here and what we do is we learn.",
            "We learn a vector W that we take the inner product with of these.",
            "Well, these pairwise feature vectors to get the similarity of two items.",
            "For example, in the case of clustering marbles, we might have say the difference in Hue, angle, difference in radius, difference in transparency and we also have say a constant term.",
            "You know, for thresholding a bias term.",
            "OK.",
            "So."
        ],
        [
            "Once I mean why is this hard at all?",
            "You know, suppose that we're given an example clustering.",
            "Couldn't we just use a regular binary classifier, say, take all the pairs of items which are in the same cluster as a positive examples in all the on all the pairs of items which are not in the same cluster as negative examples and come up with our similarity matrix.",
            "That way you know we have ones for the things that are on the same cluster and minus one for not.",
            "OK, and we say if we trained a linear SVM on this, you know we learn this weight vector and then we'd be done.",
            "Well, that's fine, but."
        ],
        [
            "Sometimes it wouldn't be possible to learn this concept exactly, and.",
            "In the event that we can't learn it.",
            "It would try to maximize accuracy over over this training set, but we might have some other clustering performance measure in mind.",
            "For example the mitre measure.",
            "For non fresko reference would not be up, would not be able to be optimized directly through this.",
            "Method.",
            "Also, in these sorts of problems you would have many more negative examples on positive examples, so there would be the classical class imbalance problem which would also cause difficulty.",
            "Another problem and more subtle problem."
        ],
        [
            "Is that there are sometimes complex interactions between item pairs.",
            "And for suppose that we have these two noun phrases in the noun phrase coreference problem.",
            "It's not obvious that by viewing this pair in isolation, you would actually be able to learn the concept that beats are in the same cluster, but the pairwise classifier will try to do it.",
            "Whereas if you could classify them all collectively, it could perhaps implicitly determine that it can say.",
            "Match these as being Co referent and figure out that it doesn't actually need to match to classify this as Co referent in the context of a clustering algorithm.",
            "OK, so.",
            "Alright, so how can we do that?"
        ],
        [
            "Well, what we use is."
        ],
        [
            "Then an SVM for structured optimization program programs.",
            "Yeah, so you've heard this on Sunday from Thomas Hoffman.",
            "You heard this on Monday from tornado comes here this on Tuesday from.",
            "Fantastic car and I guess today you're going to hear from me, but.",
            "Basically.",
            "What we can do in these espions for structured outputs?",
            "Is learn functions from.",
            "X2 a structured output Y of that can be phrased in this form.",
            "That is, as the linear product between some learned weight vector W. And some function that produces a compliant combined feature representation of the input and output.",
            "And so.",
            "How can we do this for clustering?"
        ],
        [
            "Well, we need this PSI function for the combined input and output vector and well.",
            "If we just set this.",
            "See function 2.",
            "I'm sorry fi function to the sum of all the of all the intercluster.",
            "Intercluster feature vectors.",
            "Then and then if we try to maximize this, then that is just the same as maximizing the correlation clustering objective.",
            "When we find the Y that maximizes this objective function OK. Also, in the event that we can't."
        ],
        [
            "Find the exact concept we need to find some way of judging the fitness of our imperfect solutions.",
            "And for this reason we also need a loss function which specifies how unrelated the clusterings that we find our to the true clustering.",
            "This supposes the true clustering and event that they're identical.",
            "The loss is zero in the event that there are little off it's low, and in the event that it's totally off the wall, the loss would be high.",
            "The two loss functions that we consider in this talk are pairwise loss.",
            "Which is basically over all pairs of items.",
            "How many pairwise cluster relationships did we get wrong?",
            "And the other is this might or loss, which is specific to noun phrase coreference.",
            "It's a little complicated to explain.",
            "So, given that we have these two functions."
        ],
        [
            "Now how can we?",
            "How can we learn to?",
            "How can we learn to find this W well?",
            "Basically.",
            "What we want is we want the objective function.",
            "At the correct output to be greater than the objective function for the incorrect.",
            "Output for every incorrect output in fact, and also to have that.",
            "As a sort of margin term, we have the loss between this correct and incorrect output to make sure that the objective function is clear by a certain margin.",
            "And this is a common.",
            "Common formulation used in SPMS for structured outputs.",
            "OK, so great."
        ],
        [
            "So suppose that we insert this into into our regular SVM.",
            "Well, this looks fine and this will perhaps do the correct thing, But the problem is that we need to introduce such constraint for every wrong.",
            "Output and the number of wrong outputs for clustering is, well, it's more than exponential less than factorial, so we can't really do that.",
            "So what we do is we have an iterative algorithm to select."
        ],
        [
            "The constraints that we will introduce.",
            "So basically what we do is we have this.",
            "We set up this cost function here.",
            "And we find the Y that is the incorrect output, which would require the greatest slack under this current W here.",
            "And if that can the constraint associated with that output is.",
            "In fact violates the current slack by more than well by more than an epsilon.",
            "Then we introduce this constraint and re optimize.",
            "And eventually this converges, right?",
            "So all we have to do is be able to compute this argmax.",
            "Well, we can't exactly compute this argmax because for correlation clustering."
        ],
        [
            "Finding just the Y that maximizes the subject.",
            "This objective function just this one term here.",
            "That's known to be NP complete or NP hard, and the sum.",
            "And just.",
            "To have to maximize the sum of this plus this loss would be.",
            "Would be very hard indeed, so we can't compute this argmax exactly.",
            "So what we can do, however, is use approximations to correlation clustering, and we consider 22 approximations here.",
            "The first is reading clustering.",
            "Where we have.",
            "Where we just merge clusters until no merge is beneficial to the maximizing the objective function.",
            "It's very simple.",
            "And but the problem with this is that this will tend to find constraints that lower bound this objective function.",
            "That is, it won't find the highest clustering objective function, so it will tend to miss constraints that otherwise should be introduced.",
            "So we also look at a different approximation, which is real relaxation to correlation clustering, which can be phrased as a linear program, and in this case, because it's a real relaxation and it's convex, we get an upper bounded argmax approximation to this objective function.",
            "But the problem with that is that we might.",
            "If we introduce constraints based on this relaxation.",
            "We might introduce constraints that.",
            "That over constraint this solution so you know neither of these is perfect and will go over these in the results right now, OK."
        ],
        [
            "So let's just review the two types of problems that we're looking at.",
            "Now the first is the noun phrase coreference.",
            "The."
        ],
        [
            "Data that we took was data from the Mark 6 task, it's.",
            "We actually use the features which were used by Vincent Ingane Claire Cardi.",
            "And.",
            "Well, we had have two approaches.",
            "Our baseline approach is something we call pairwise classifier clustering, in which we take all the pairs of noun phrases.",
            "And we.",
            "An we learn a pairwise classifier over this.",
            "Use this pairwise classifier to come up with the similarity matrix and then cluster over the similarity matrix.",
            "And then we also have our SVM structure structured output learner with correlation clustering.",
            "And we're going to compare those and the other tasks."
        ],
        [
            "We're going to do is new story clustering.",
            "This is a data set that we built ourselves.",
            "And basically we have data 3030 sets of items that is 30 days worth of news stories each day, with about 900 stories and.",
            "Yeah.",
            "And with so many clusters and so forth.",
            "In this case, the features that we used are simple."
        ],
        [
            "Unigrams, bigrams and trigrams of, say the title article text, quoted article, text, and so forth.",
            "So very simple features.",
            "And now let's take a look at."
        ],
        [
            "Performance.",
            "In this case, this Delta M is the mitre loss that is the F1 measure of the mitre precision and recall.",
            "And this is just the pairwise loss now.",
            "And in the in the.",
            "Headings we see the training.",
            "Method that was used to come up with our model and this is how and this is the type of clustering method that we used to to evaluate our model.",
            "So we see here.",
            "That there's well.",
            "Neither of these performances is very good, but this is quite a bit lower than this.",
            "Um?",
            "These are errors, the minor error.",
            "And.",
            "So OK, so it seems like something is going on there and this is the default classifier, which is the number of.",
            "Which is the model that we would use if we just inserted?",
            "If we just kept everything in a separate cluster?",
            "Um?",
            "Actually, I'm sorry that's default is for pairwise.",
            "If we kept everything in a separate cluster for minor measure, it's if we put everything in one cluster, whichever will give the highest performance.",
            "So as we see, the pairwise classifying cluster actually does seems to do worse than default cluster.",
            "Actually, these results are not statistically significantly different, but this is statistically significantly different.",
            "Now, if we train on pairwise loss, which is sort of closer to being accuracy, overall item pairs.",
            "We see that PC seems to be doing something a little more sensible, but these two results are still statistically significantly different.",
            "OK. Anne.",
            "An for news story.",
            "News story, clustering.",
            "We have a bit of a different story this time because we see that the pairwise classification clustering and the SVM cluster are actually pretty similar, and in fact none of these results either in this row nor in this row are significantly different.",
            "Incidentally, this is Grady clustering.",
            "And this is relaxed clustering, which is later forced into a discrete solution.",
            "Alright, both both evaluated on pairwise loss OK. And."
        ],
        [
            "The way that I tend to interpret this.",
            "Is that for news story clustering?",
            "It's very easy to tell given 2 news stories in isolation, whether they refer to the same.",
            "Same subject or not?",
            "Whereas for Nonphrasal reference it's sometimes a little more difficult to tell based on two noun phrases in isolation, whether they actually Co refer.",
            "OK, so.",
            "Now we also have this ability to optimize too.",
            "An arbitrary loss function, more or less arbitrary loss function, so how helpful is this?",
            "Well, if we train SVM struct, if we train the SVM clustering algorithm on mitre loss for the noun phrase coreference problem.",
            "Or if we trained it on pairwise loss, we get these two performance figures.",
            "These two performance figures were actually not statistically significantly different, so you know, it seems like they both do OK when evaluated on the minor loss, but for the pairwise loss, we see that when optimizing on minor things, completely fall down.",
            "So an in fact this is worse than if we had simply done absolutely no learning at all and just kept everything in a separate cluster.",
            "So this indicates that indeed it is doing something to optimize for the correct loss and."
        ],
        [
            "So.",
            "For our third experiment, we looked at whether it was important to include the loss function in this argmax approximation.",
            "Sometimes we can't actually do this.",
            "For example, the mitre loss is rather complex and could not really be phrased put into a linear program, whereas the pairwise loss can.",
            "You can introduce that into a linear program so.",
            "What we looked at was whether if it was, whether it actually made a difference.",
            "If we actually drop the loss from this argmax approximation, and if we could do that, then we could perhaps justify omitting other losses if we can't introduce them into into this argmax effectively.",
            "So, as we see here.",
            "Um?",
            "Well, these numbers are pretty similar.",
            "And.",
            "Well, the nice thing about this is that in no case are the results statistically significantly different.",
            "So at least in this case we got away with with not including the loss in the arc Maps.",
            "OK, so and for our last."
        ],
        [
            "We look at greedy versus correlation clustering.",
            "Now.",
            "The greedy clustering is bad because it.",
            "Very often finds a local minimum that is a local maximum and so doesn't find the absolute highest.",
            "Objective by doesn't find the Y for the absolute highest objective function.",
            "So.",
            "This could lead to an underconstrained problem.",
            "And so lead to a bad solution.",
            "But the good news is that.",
            "As we see here.",
            "If we train using the correlation correlation clustering, we don't get results that are statistically significantly different whether we are evaluating angree or evaluating on the discretized relaxation.",
            "Indeed, you can see the difference here, that the discretized relaxation produces error rates which are significantly lower, which.",
            "Which really points to this.",
            "Finding local Maxima.",
            "OK, so to close."
        ],
        [
            "What we did here is we adapted SPM structure to supervised clustering.",
            "For problems where the where there are not complex interactions, we didn't see any real benefit, but for problems like noun phrase, for reference we could see quite a sizable jump in performance.",
            "Now as far as what we could do in the future, we could perhaps try this on other problems.",
            "But another thing that we might want to do is we might want to try this.",
            "For semi supervised clustering.",
            "That is not require complete labels, but that's in the future, so thank you and any questions.",
            "Any questions?",
            "Any questions?",
            "Use for your testing.",
            "The real question is you figured out how you might encode.",
            "Nothing.",
            "Well, in the case of what we use the two tailed.",
            "Paired T test for significance testing and the results that I presented are the average across all of the testing results.",
            "As far as how to include?",
            "Must not link or muscle in constraints.",
            "I suppose that when you're actually actually running the clustering algorithm, you could instead of just using straight correlation clustering, have a variant of correlation clustering, which actually does.",
            "Does include these constraints?",
            "There's no real reason to use correlation clustering exactly, except for the fact that it allowed me to run some some comparisons in this paper, so you could have some sort of constrained clustering if you wanted just that, answer your question.",
            "OK. Any other?",
            "Strange.",
            "The same number of restroom.",
            "The tests that compare to the cream.",
            "Court.",
            "Correlation clustering in particular does not require that you set a set number of clusters, and in fact it does find widely different numbers of numbers of clusters.",
            "In particular, in the nonphrasal reference problem where many many documents have different numbers of coreference clusters.",
            "So now we wouldn't even want to do that.",
            "Yeah, anything else.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was introduced on Tom Finley.",
                    "label": 0
                },
                {
                    "sent": "I'm here to talk to you today about supervised clustering with support vector machines.",
                    "label": 1
                },
                {
                    "sent": "And this is work that I have done with Torsten Yokums at Cornell University.",
                    "label": 0
                },
                {
                    "sent": "Alright, first, let's talk about what we're going to talk about.",
                    "label": 0
                },
                {
                    "sent": "First, we're going to talk about what I mean.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By supervised clustering and why we might want to do it, then we're going to discuss why it's an interesting problem.",
                    "label": 1
                },
                {
                    "sent": "Then we're going to talk about our way of doing supervised clustering, and then we're going to show some results.",
                    "label": 1
                },
                {
                    "sent": "OK, so suppose that we want to cluster a collection of more.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, it might be that the user wants has some criteria in mind for how they want to cluster these models.",
                    "label": 0
                },
                {
                    "sent": "They might want to cluster my color or by size.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or by whether they're transparent or not, for example.",
                    "label": 0
                },
                {
                    "sent": "So if we have a goal like this in mind, how do we make the clustering algorithm come up with the desired partition?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Partitioning thank you.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "One way that we could do it is we could simply manually adjust the clustering algorithm or the similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Um, but we might want to do sort of an automatic.",
                    "label": 0
                },
                {
                    "sent": "That have an automatic way of adjusting the clustering algorithm, and for this we have fields like semi supervised clustering.",
                    "label": 1
                },
                {
                    "sent": "The most common instance of this is some form of constrained clustering, where the user specifies that they want, say, two items in the same cluster or tour items not in the same cluster, and the algorithm works to satisfy those constraints.",
                    "label": 1
                },
                {
                    "sent": "But we're not talking about that.",
                    "label": 0
                },
                {
                    "sent": "What we're talking about is fully supervised clustering, and in this case.",
                    "label": 0
                },
                {
                    "sent": "You have multiple sets of items.",
                    "label": 0
                },
                {
                    "sent": "And you are and the learning algorithm is provided with complete partition partitionings of those sets and it learns how to partition future sets of items.",
                    "label": 0
                },
                {
                    "sent": "Based on these examples just to give you an idea of what I mean.",
                    "label": 0
                },
                {
                    "sent": "One example of this is noun phrase.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correct for reference, where sets of items.",
                    "label": 0
                },
                {
                    "sent": "Are the are the collection of noun phrases in a particular document and the clustering of those noun phrases are are the partition of the non phrases that refer to the same entity.",
                    "label": 1
                },
                {
                    "sent": "For example, the two he's in the policeman that would form one cluster, the cat and Peter would form another cluster and hopefully it would learn to win given a new document.",
                    "label": 0
                },
                {
                    "sent": "Find out which noun phrases?",
                    "label": 0
                },
                {
                    "sent": "Cover for another example is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "News, story clustering.",
                    "label": 0
                },
                {
                    "sent": "Sort of like what Google News does, for example.",
                    "label": 1
                },
                {
                    "sent": "Where you're given, say, the collection of all news articles from a particular day.",
                    "label": 1
                },
                {
                    "sent": "And you want to cluster them according to which refer to the same story.",
                    "label": 0
                },
                {
                    "sent": "OK, and there are other examples like image partitioning and image segmentation which we don't cover in this talk.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formalize it a bit.",
                    "label": 0
                },
                {
                    "sent": "Well, as I said, were provided with sets of items and example clusterings.",
                    "label": 0
                },
                {
                    "sent": "And we learn how to cluster future sets of items in a similar way.",
                    "label": 1
                },
                {
                    "sent": "So OK, just to specify what we mean.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By clustering we suppose that we have a set of items, let's say an items, in this case 10.",
                    "label": 0
                },
                {
                    "sent": "We suppose that we have some sort of similarity measure, but for all item pairs.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we want to do is we want to produce a partitioning of the item set with with respect to some objective function.",
                    "label": 1
                },
                {
                    "sent": "And in this work what we do is we hold the the clustering algorithm constant and we just change the similarity measure so that all of the so that it will tend to produce the types of clusterings that we want.",
                    "label": 0
                },
                {
                    "sent": "So in this particular work we looked at.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correlation clustering.",
                    "label": 0
                },
                {
                    "sent": "You could also use another clustering algorithm if you wanted, but in this case we just use correlation clustering.",
                    "label": 0
                },
                {
                    "sent": "Correlation clustering is very simple.",
                    "label": 0
                },
                {
                    "sent": "Basically, if we suppose that we have all these items similar in impaired similarities, it finds the partitioning of the set.",
                    "label": 0
                },
                {
                    "sent": "So that the sum of Inter cluster similarities is maximized.",
                    "label": 1
                },
                {
                    "sent": "And of course, this this requires having some negative similarities.",
                    "label": 0
                },
                {
                    "sent": "Annual note that it will allow certain discrepancies if the net effect of including that pair in the same cluster or not is beneficial.",
                    "label": 1
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Now that we've defined correlation clustering, let's talk about the types of similarity that we here consider.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, basically all that we have, we have the similarity between all item pairs and in order to get this similarity, we suppose that we have a feature vector for all item pairs.",
                    "label": 0
                },
                {
                    "sent": "This Phi here and what we do is we learn.",
                    "label": 0
                },
                {
                    "sent": "We learn a vector W that we take the inner product with of these.",
                    "label": 1
                },
                {
                    "sent": "Well, these pairwise feature vectors to get the similarity of two items.",
                    "label": 1
                },
                {
                    "sent": "For example, in the case of clustering marbles, we might have say the difference in Hue, angle, difference in radius, difference in transparency and we also have say a constant term.",
                    "label": 0
                },
                {
                    "sent": "You know, for thresholding a bias term.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once I mean why is this hard at all?",
                    "label": 0
                },
                {
                    "sent": "You know, suppose that we're given an example clustering.",
                    "label": 0
                },
                {
                    "sent": "Couldn't we just use a regular binary classifier, say, take all the pairs of items which are in the same cluster as a positive examples in all the on all the pairs of items which are not in the same cluster as negative examples and come up with our similarity matrix.",
                    "label": 0
                },
                {
                    "sent": "That way you know we have ones for the things that are on the same cluster and minus one for not.",
                    "label": 0
                },
                {
                    "sent": "OK, and we say if we trained a linear SVM on this, you know we learn this weight vector and then we'd be done.",
                    "label": 0
                },
                {
                    "sent": "Well, that's fine, but.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sometimes it wouldn't be possible to learn this concept exactly, and.",
                    "label": 0
                },
                {
                    "sent": "In the event that we can't learn it.",
                    "label": 0
                },
                {
                    "sent": "It would try to maximize accuracy over over this training set, but we might have some other clustering performance measure in mind.",
                    "label": 0
                },
                {
                    "sent": "For example the mitre measure.",
                    "label": 0
                },
                {
                    "sent": "For non fresko reference would not be up, would not be able to be optimized directly through this.",
                    "label": 0
                },
                {
                    "sent": "Method.",
                    "label": 0
                },
                {
                    "sent": "Also, in these sorts of problems you would have many more negative examples on positive examples, so there would be the classical class imbalance problem which would also cause difficulty.",
                    "label": 0
                },
                {
                    "sent": "Another problem and more subtle problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that there are sometimes complex interactions between item pairs.",
                    "label": 0
                },
                {
                    "sent": "And for suppose that we have these two noun phrases in the noun phrase coreference problem.",
                    "label": 1
                },
                {
                    "sent": "It's not obvious that by viewing this pair in isolation, you would actually be able to learn the concept that beats are in the same cluster, but the pairwise classifier will try to do it.",
                    "label": 1
                },
                {
                    "sent": "Whereas if you could classify them all collectively, it could perhaps implicitly determine that it can say.",
                    "label": 0
                },
                {
                    "sent": "Match these as being Co referent and figure out that it doesn't actually need to match to classify this as Co referent in the context of a clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Alright, so how can we do that?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, what we use is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then an SVM for structured optimization program programs.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you've heard this on Sunday from Thomas Hoffman.",
                    "label": 0
                },
                {
                    "sent": "You heard this on Monday from tornado comes here this on Tuesday from.",
                    "label": 0
                },
                {
                    "sent": "Fantastic car and I guess today you're going to hear from me, but.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "What we can do in these espions for structured outputs?",
                    "label": 0
                },
                {
                    "sent": "Is learn functions from.",
                    "label": 0
                },
                {
                    "sent": "X2 a structured output Y of that can be phrased in this form.",
                    "label": 1
                },
                {
                    "sent": "That is, as the linear product between some learned weight vector W. And some function that produces a compliant combined feature representation of the input and output.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "How can we do this for clustering?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we need this PSI function for the combined input and output vector and well.",
                    "label": 0
                },
                {
                    "sent": "If we just set this.",
                    "label": 0
                },
                {
                    "sent": "See function 2.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry fi function to the sum of all the of all the intercluster.",
                    "label": 0
                },
                {
                    "sent": "Intercluster feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Then and then if we try to maximize this, then that is just the same as maximizing the correlation clustering objective.",
                    "label": 0
                },
                {
                    "sent": "When we find the Y that maximizes this objective function OK. Also, in the event that we can't.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find the exact concept we need to find some way of judging the fitness of our imperfect solutions.",
                    "label": 0
                },
                {
                    "sent": "And for this reason we also need a loss function which specifies how unrelated the clusterings that we find our to the true clustering.",
                    "label": 0
                },
                {
                    "sent": "This supposes the true clustering and event that they're identical.",
                    "label": 0
                },
                {
                    "sent": "The loss is zero in the event that there are little off it's low, and in the event that it's totally off the wall, the loss would be high.",
                    "label": 0
                },
                {
                    "sent": "The two loss functions that we consider in this talk are pairwise loss.",
                    "label": 0
                },
                {
                    "sent": "Which is basically over all pairs of items.",
                    "label": 0
                },
                {
                    "sent": "How many pairwise cluster relationships did we get wrong?",
                    "label": 0
                },
                {
                    "sent": "And the other is this might or loss, which is specific to noun phrase coreference.",
                    "label": 0
                },
                {
                    "sent": "It's a little complicated to explain.",
                    "label": 0
                },
                {
                    "sent": "So, given that we have these two functions.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now how can we?",
                    "label": 0
                },
                {
                    "sent": "How can we learn to?",
                    "label": 0
                },
                {
                    "sent": "How can we learn to find this W well?",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "What we want is we want the objective function.",
                    "label": 0
                },
                {
                    "sent": "At the correct output to be greater than the objective function for the incorrect.",
                    "label": 1
                },
                {
                    "sent": "Output for every incorrect output in fact, and also to have that.",
                    "label": 1
                },
                {
                    "sent": "As a sort of margin term, we have the loss between this correct and incorrect output to make sure that the objective function is clear by a certain margin.",
                    "label": 0
                },
                {
                    "sent": "And this is a common.",
                    "label": 0
                },
                {
                    "sent": "Common formulation used in SPMS for structured outputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so great.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So suppose that we insert this into into our regular SVM.",
                    "label": 0
                },
                {
                    "sent": "Well, this looks fine and this will perhaps do the correct thing, But the problem is that we need to introduce such constraint for every wrong.",
                    "label": 1
                },
                {
                    "sent": "Output and the number of wrong outputs for clustering is, well, it's more than exponential less than factorial, so we can't really do that.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we have an iterative algorithm to select.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The constraints that we will introduce.",
                    "label": 0
                },
                {
                    "sent": "So basically what we do is we have this.",
                    "label": 0
                },
                {
                    "sent": "We set up this cost function here.",
                    "label": 0
                },
                {
                    "sent": "And we find the Y that is the incorrect output, which would require the greatest slack under this current W here.",
                    "label": 1
                },
                {
                    "sent": "And if that can the constraint associated with that output is.",
                    "label": 1
                },
                {
                    "sent": "In fact violates the current slack by more than well by more than an epsilon.",
                    "label": 1
                },
                {
                    "sent": "Then we introduce this constraint and re optimize.",
                    "label": 0
                },
                {
                    "sent": "And eventually this converges, right?",
                    "label": 0
                },
                {
                    "sent": "So all we have to do is be able to compute this argmax.",
                    "label": 0
                },
                {
                    "sent": "Well, we can't exactly compute this argmax because for correlation clustering.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finding just the Y that maximizes the subject.",
                    "label": 0
                },
                {
                    "sent": "This objective function just this one term here.",
                    "label": 0
                },
                {
                    "sent": "That's known to be NP complete or NP hard, and the sum.",
                    "label": 0
                },
                {
                    "sent": "And just.",
                    "label": 0
                },
                {
                    "sent": "To have to maximize the sum of this plus this loss would be.",
                    "label": 0
                },
                {
                    "sent": "Would be very hard indeed, so we can't compute this argmax exactly.",
                    "label": 0
                },
                {
                    "sent": "So what we can do, however, is use approximations to correlation clustering, and we consider 22 approximations here.",
                    "label": 0
                },
                {
                    "sent": "The first is reading clustering.",
                    "label": 0
                },
                {
                    "sent": "Where we have.",
                    "label": 0
                },
                {
                    "sent": "Where we just merge clusters until no merge is beneficial to the maximizing the objective function.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "And but the problem with this is that this will tend to find constraints that lower bound this objective function.",
                    "label": 0
                },
                {
                    "sent": "That is, it won't find the highest clustering objective function, so it will tend to miss constraints that otherwise should be introduced.",
                    "label": 0
                },
                {
                    "sent": "So we also look at a different approximation, which is real relaxation to correlation clustering, which can be phrased as a linear program, and in this case, because it's a real relaxation and it's convex, we get an upper bounded argmax approximation to this objective function.",
                    "label": 1
                },
                {
                    "sent": "But the problem with that is that we might.",
                    "label": 0
                },
                {
                    "sent": "If we introduce constraints based on this relaxation.",
                    "label": 0
                },
                {
                    "sent": "We might introduce constraints that.",
                    "label": 0
                },
                {
                    "sent": "That over constraint this solution so you know neither of these is perfect and will go over these in the results right now, OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just review the two types of problems that we're looking at.",
                    "label": 0
                },
                {
                    "sent": "Now the first is the noun phrase coreference.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data that we took was data from the Mark 6 task, it's.",
                    "label": 0
                },
                {
                    "sent": "We actually use the features which were used by Vincent Ingane Claire Cardi.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, we had have two approaches.",
                    "label": 0
                },
                {
                    "sent": "Our baseline approach is something we call pairwise classifier clustering, in which we take all the pairs of noun phrases.",
                    "label": 1
                },
                {
                    "sent": "And we.",
                    "label": 0
                },
                {
                    "sent": "An we learn a pairwise classifier over this.",
                    "label": 0
                },
                {
                    "sent": "Use this pairwise classifier to come up with the similarity matrix and then cluster over the similarity matrix.",
                    "label": 1
                },
                {
                    "sent": "And then we also have our SVM structure structured output learner with correlation clustering.",
                    "label": 0
                },
                {
                    "sent": "And we're going to compare those and the other tasks.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to do is new story clustering.",
                    "label": 1
                },
                {
                    "sent": "This is a data set that we built ourselves.",
                    "label": 0
                },
                {
                    "sent": "And basically we have data 3030 sets of items that is 30 days worth of news stories each day, with about 900 stories and.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And with so many clusters and so forth.",
                    "label": 0
                },
                {
                    "sent": "In this case, the features that we used are simple.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unigrams, bigrams and trigrams of, say the title article text, quoted article, text, and so forth.",
                    "label": 0
                },
                {
                    "sent": "So very simple features.",
                    "label": 0
                },
                {
                    "sent": "And now let's take a look at.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Performance.",
                    "label": 0
                },
                {
                    "sent": "In this case, this Delta M is the mitre loss that is the F1 measure of the mitre precision and recall.",
                    "label": 1
                },
                {
                    "sent": "And this is just the pairwise loss now.",
                    "label": 1
                },
                {
                    "sent": "And in the in the.",
                    "label": 0
                },
                {
                    "sent": "Headings we see the training.",
                    "label": 0
                },
                {
                    "sent": "Method that was used to come up with our model and this is how and this is the type of clustering method that we used to to evaluate our model.",
                    "label": 0
                },
                {
                    "sent": "So we see here.",
                    "label": 0
                },
                {
                    "sent": "That there's well.",
                    "label": 0
                },
                {
                    "sent": "Neither of these performances is very good, but this is quite a bit lower than this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "These are errors, the minor error.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So OK, so it seems like something is going on there and this is the default classifier, which is the number of.",
                    "label": 0
                },
                {
                    "sent": "Which is the model that we would use if we just inserted?",
                    "label": 0
                },
                {
                    "sent": "If we just kept everything in a separate cluster?",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm sorry that's default is for pairwise.",
                    "label": 0
                },
                {
                    "sent": "If we kept everything in a separate cluster for minor measure, it's if we put everything in one cluster, whichever will give the highest performance.",
                    "label": 0
                },
                {
                    "sent": "So as we see, the pairwise classifying cluster actually does seems to do worse than default cluster.",
                    "label": 0
                },
                {
                    "sent": "Actually, these results are not statistically significantly different, but this is statistically significantly different.",
                    "label": 0
                },
                {
                    "sent": "Now, if we train on pairwise loss, which is sort of closer to being accuracy, overall item pairs.",
                    "label": 0
                },
                {
                    "sent": "We see that PC seems to be doing something a little more sensible, but these two results are still statistically significantly different.",
                    "label": 0
                },
                {
                    "sent": "OK. Anne.",
                    "label": 0
                },
                {
                    "sent": "An for news story.",
                    "label": 0
                },
                {
                    "sent": "News story, clustering.",
                    "label": 0
                },
                {
                    "sent": "We have a bit of a different story this time because we see that the pairwise classification clustering and the SVM cluster are actually pretty similar, and in fact none of these results either in this row nor in this row are significantly different.",
                    "label": 1
                },
                {
                    "sent": "Incidentally, this is Grady clustering.",
                    "label": 0
                },
                {
                    "sent": "And this is relaxed clustering, which is later forced into a discrete solution.",
                    "label": 0
                },
                {
                    "sent": "Alright, both both evaluated on pairwise loss OK. And.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way that I tend to interpret this.",
                    "label": 0
                },
                {
                    "sent": "Is that for news story clustering?",
                    "label": 0
                },
                {
                    "sent": "It's very easy to tell given 2 news stories in isolation, whether they refer to the same.",
                    "label": 0
                },
                {
                    "sent": "Same subject or not?",
                    "label": 0
                },
                {
                    "sent": "Whereas for Nonphrasal reference it's sometimes a little more difficult to tell based on two noun phrases in isolation, whether they actually Co refer.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now we also have this ability to optimize too.",
                    "label": 0
                },
                {
                    "sent": "An arbitrary loss function, more or less arbitrary loss function, so how helpful is this?",
                    "label": 0
                },
                {
                    "sent": "Well, if we train SVM struct, if we train the SVM clustering algorithm on mitre loss for the noun phrase coreference problem.",
                    "label": 0
                },
                {
                    "sent": "Or if we trained it on pairwise loss, we get these two performance figures.",
                    "label": 0
                },
                {
                    "sent": "These two performance figures were actually not statistically significantly different, so you know, it seems like they both do OK when evaluated on the minor loss, but for the pairwise loss, we see that when optimizing on minor things, completely fall down.",
                    "label": 0
                },
                {
                    "sent": "So an in fact this is worse than if we had simply done absolutely no learning at all and just kept everything in a separate cluster.",
                    "label": 0
                },
                {
                    "sent": "So this indicates that indeed it is doing something to optimize for the correct loss and.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For our third experiment, we looked at whether it was important to include the loss function in this argmax approximation.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we can't actually do this.",
                    "label": 0
                },
                {
                    "sent": "For example, the mitre loss is rather complex and could not really be phrased put into a linear program, whereas the pairwise loss can.",
                    "label": 0
                },
                {
                    "sent": "You can introduce that into a linear program so.",
                    "label": 0
                },
                {
                    "sent": "What we looked at was whether if it was, whether it actually made a difference.",
                    "label": 0
                },
                {
                    "sent": "If we actually drop the loss from this argmax approximation, and if we could do that, then we could perhaps justify omitting other losses if we can't introduce them into into this argmax effectively.",
                    "label": 0
                },
                {
                    "sent": "So, as we see here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well, these numbers are pretty similar.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, the nice thing about this is that in no case are the results statistically significantly different.",
                    "label": 0
                },
                {
                    "sent": "So at least in this case we got away with with not including the loss in the arc Maps.",
                    "label": 0
                },
                {
                    "sent": "OK, so and for our last.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We look at greedy versus correlation clustering.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The greedy clustering is bad because it.",
                    "label": 1
                },
                {
                    "sent": "Very often finds a local minimum that is a local maximum and so doesn't find the absolute highest.",
                    "label": 0
                },
                {
                    "sent": "Objective by doesn't find the Y for the absolute highest objective function.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This could lead to an underconstrained problem.",
                    "label": 0
                },
                {
                    "sent": "And so lead to a bad solution.",
                    "label": 0
                },
                {
                    "sent": "But the good news is that.",
                    "label": 0
                },
                {
                    "sent": "As we see here.",
                    "label": 1
                },
                {
                    "sent": "If we train using the correlation correlation clustering, we don't get results that are statistically significantly different whether we are evaluating angree or evaluating on the discretized relaxation.",
                    "label": 0
                },
                {
                    "sent": "Indeed, you can see the difference here, that the discretized relaxation produces error rates which are significantly lower, which.",
                    "label": 0
                },
                {
                    "sent": "Which really points to this.",
                    "label": 0
                },
                {
                    "sent": "Finding local Maxima.",
                    "label": 0
                },
                {
                    "sent": "OK, so to close.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we did here is we adapted SPM structure to supervised clustering.",
                    "label": 0
                },
                {
                    "sent": "For problems where the where there are not complex interactions, we didn't see any real benefit, but for problems like noun phrase, for reference we could see quite a sizable jump in performance.",
                    "label": 0
                },
                {
                    "sent": "Now as far as what we could do in the future, we could perhaps try this on other problems.",
                    "label": 0
                },
                {
                    "sent": "But another thing that we might want to do is we might want to try this.",
                    "label": 0
                },
                {
                    "sent": "For semi supervised clustering.",
                    "label": 0
                },
                {
                    "sent": "That is not require complete labels, but that's in the future, so thank you and any questions.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Use for your testing.",
                    "label": 0
                },
                {
                    "sent": "The real question is you figured out how you might encode.",
                    "label": 0
                },
                {
                    "sent": "Nothing.",
                    "label": 0
                },
                {
                    "sent": "Well, in the case of what we use the two tailed.",
                    "label": 0
                },
                {
                    "sent": "Paired T test for significance testing and the results that I presented are the average across all of the testing results.",
                    "label": 0
                },
                {
                    "sent": "As far as how to include?",
                    "label": 0
                },
                {
                    "sent": "Must not link or muscle in constraints.",
                    "label": 0
                },
                {
                    "sent": "I suppose that when you're actually actually running the clustering algorithm, you could instead of just using straight correlation clustering, have a variant of correlation clustering, which actually does.",
                    "label": 0
                },
                {
                    "sent": "Does include these constraints?",
                    "label": 0
                },
                {
                    "sent": "There's no real reason to use correlation clustering exactly, except for the fact that it allowed me to run some some comparisons in this paper, so you could have some sort of constrained clustering if you wanted just that, answer your question.",
                    "label": 0
                },
                {
                    "sent": "OK. Any other?",
                    "label": 0
                },
                {
                    "sent": "Strange.",
                    "label": 0
                },
                {
                    "sent": "The same number of restroom.",
                    "label": 0
                },
                {
                    "sent": "The tests that compare to the cream.",
                    "label": 0
                },
                {
                    "sent": "Court.",
                    "label": 0
                },
                {
                    "sent": "Correlation clustering in particular does not require that you set a set number of clusters, and in fact it does find widely different numbers of numbers of clusters.",
                    "label": 0
                },
                {
                    "sent": "In particular, in the nonphrasal reference problem where many many documents have different numbers of coreference clusters.",
                    "label": 0
                },
                {
                    "sent": "So now we wouldn't even want to do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, anything else.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}