{
    "id": "loeyzvdhoa5dirbcni67ukukwvufodqa",
    "title": "Gaussian Process Temporal Difference",
    "info": {
        "author": [
            "Yaakov Engel, University of Alberta"
        ],
        "published": "June 22, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml07_engel_gptd/",
    "segmentation": [
        [
            "And request is that although we might have a bit of a problem with the screen, I still would like you to sit close to me.",
            "So, like in the first few lines in this area, and to encourage you to do that, I'm going to forgo the use of the microphone.",
            "So if you sit in the back, I'm sorry.",
            "OK.",
            "When is the next?",
            "OK questions.",
            "Interested in how.",
            "Are there empirical studies looking at how performance degrades when the prior is chosen incorrectly?",
            "Is that there is this slide about needle where you increase K and get better performance.",
            "I was surprised it wasn't also a slide looking at OK, and now if our priors wrong with this much, here's how much performance degrades.",
            "It's wrong, but this much.",
            "Here's how much it degrades.",
            "Is that a popular topic?",
            "Actually, I think first of all, Pascal is more qualified than me to answer this, but I think he had a comparison there where he had one uninformative prior, an one informative prior right in your.",
            "Yeah, so I have several columns.",
            "Fire.",
            "Performance.",
            "But that was just increasing.",
            "You increase if you're increasing accounts, but with the wrong file, not just less informative but incorrect.",
            "So.",
            "But the easy answer is that, like in any other machine learning technique, you can do arbitrarily bad.",
            "So it's up to you as the designer and the user to come up with the best price you can think of.",
            "Can say OK, this is our primary and define it very precisely, so it seems like you want to know.",
            "How closely that prior needs to match the actual price?",
            "I guess that's more of a imparable.",
            "Well, that's also sort of algorithm algorithm dependent, so it.",
            "Slide.",
            "Fire.",
            "Update.",
            "5.",
            "So.",
            "So.",
            "Questions like how bad is required?",
            "OK."
        ],
        [
            "So I will start so my name is I have to use this laser.",
            "My name is Yaacov Angle and the work that I'm going to describe here, namely Gaussian process, temporal difference learning is my collaboration with Shimon Owen on my ear from Israel, or at least one of them is from Israel and the other is now in Canada.",
            "And those are my sponsoring agencies and institutions and etc."
        ],
        [
            "OK.",
            "So Pascal already, yeah.",
            "OK, I'll try to speak forward.",
            "OK.",
            "So Pascal already said mentioned the benefits of using the Bayesian approach in general and for reinforcement learning.",
            "Here we use a specific.",
            "Basian tool known as Gaussian processes and I'm going to sort of mention the benefits of using that.",
            "So first the Bayesian approach.",
            "So Gaussian process processes for reinforcement learning is a Bayesian approach to value estimation with all the benefits and the difficulties.",
            "Or maybe not all the difficulties it forces to make our assumptions explicit as Pascal said.",
            "In this case, in the Gaussian process case, it's a nonparametric approach to reinforcement learning, meaning that priors are placed.",
            "An inference is performed directly in function space, and this is through the use of kernels.",
            "The cones that you know from kernel machines.",
            "However, Gaussian processes can also be defined parametrically.",
            "Domain knowledge is intuitively coded in priors.",
            "It provides a full posterior over value functions, not just point estimates like frequentist methods do.",
            "And as I will show you near the end, hopefully there exist sufficient online implementations for the algorithms resulting from this framework.",
            "So for at least for the first part of the talk, I'm going to concentrate on the problem of policy evaluation.",
            "So we have a fixed policy and we want to evaluate the value function for that policy.",
            "So what are Gauss?"
        ],
        [
            "And processes so the formal definition of Gaussian processes goes as follows.",
            "A Gaussian processes an index set of jointly Gaussian random variables.",
            "Now the word index might be a bit misleading, because in the context of random processes and Gaussian random processes in general in particular.",
            "The index set can be just about any set, and here in this example I of the batteries down on this.",
            "So.",
            "Can you see anything?",
            "Anyway, I'll try to juggle these.",
            "So in this case I chose.",
            "The index set to be the N dimensional unit cube, so indexes are just points in an dimensional space.",
            "And.",
            "Oh thanks.",
            "It's red.",
            "Oh, that's a bit better.",
            "OK so.",
            "Since since are this Gaussian process F?",
            "Is a Gaussian process.",
            "We only need to specify its mean and its covariance.",
            "Now, it's mean is going to be a function over those indexes over this X and its covariance is also going to be a function over pairs of X is like that.",
            "And there the conditions on K in order to to make a legitimate covariance function is that it is of course symmetric and positive definite.",
            "And this means that this function K can be legitimate, legitimately be named a Mercer kernel.",
            "The kernels that we know from kernel machines, any questions so far?",
            "OK.",
            "So this is basically a generalization of a Gaussian of a multivariate.",
            "Gaussian random variable to the case where the number of components is infinite or even uncountably infinite.",
            "So that's sort of a use."
        ],
        [
            "Cool way to think about it.",
            "So here's an example of a Gaussian process.",
            "We have a set of basis functions.",
            "Set of basis functions there.",
            "If you want to feed N, we combine them linearly using the weights W 1 to WN.",
            "And some of these up and the result is RF of X. OK, so that's a shorthand notation for this linear combination and what we do is to specify a normal distribution for the mean and covariance of that weight vector, so that weight vector is no longer just a vector, it's a.",
            "It's a random vector, and it's a Gaussian random vector.",
            "So what happens when we do that?",
            "Let's look how how that influences the distribution of F, because F is a function of a random variable, so F in itself.",
            "Is now a random variable, or more precisely, random vector, so the expected value of F of X for any X?",
            "In our domain is the inner product between these this vector of features fee of X and the mean vector of of the weight vector?",
            "And the covariance is given by this expression here.",
            "And this is just basic algebra.",
            "You can do it yourself very easily, so this is 1 sort of easy parametric way to specify a Gaussian random process.",
            "But as we shall see soon, we don't have to define it parametrically.",
            "Maybe I should say something more about that so so when I say that we don't have to define it parametrically, what I mean is that instead of defining MNC, the vector M in the matrix C, We just need to define this function one function of X here, which doesn't necessarily have to be written in this form and one function of X&X prime here which has to satisfy this symmetry and positive definiteness of the curve of the kernel function.",
            "OK, so now we."
        ],
        [
            "It happens.",
            "When we want to condition on such.",
            "In this space of Gaussian processes, so this is a theorem which is, at least according to one book, it's called the Gauss Markov Theorem, although I think there's no agreement about the what people actually mean when they say Gauss Markov theorem.",
            "But anyway, this is just basic use of Bayes rule, so we have Z&Y which are random vectors that are jointly distributed according to the multivariate normal distribution.",
            "So that's Z and that's why they have a joint mean given by this vector and the joint covariance given by this matrix.",
            "Here, each each element here is in itself a matrix, because both then why can be vectors?",
            "And then what the theorem says is that in order to compute the value of Z conditioned on the value of Y or the distribution of the condition on the value of Y.",
            "This is.",
            "The that that distribution that conditional distribution is also Gaussian and the mean is given by this expression here, which is linear in Y. OK, then we have all these matrices here that appear in the joint covariance.",
            "And the and the conditional covariance is given by this term here, which is just the prior covariance minus some term.",
            "So measurements basically always reduce our uncertainty about Z measurements of why reducer uncertainty about.",
            "See they never increase it.",
            "OK, so that's that's actually basically applying Bayes rule too.",
            "Multivariate gaussians."
        ],
        [
            "OK, so how?",
            "Al Gaussian process is used in machine learning.",
            "Well, the simplest example is Gaussian process regression and in Gaussian process regression, what you basically get is a set of pairs of samples.",
            "Pairs of XX is an wise, and the goal is to regress to find the function of X, which sort of approximate those Y values in a sort of a good way.",
            "So the way this is done using Gaussian process regression is by specifying this generative model.",
            "So the idea here is that Y is generated by taking a sample from F. And then adding some white noise to that sample.",
            "And.",
            "What we need to do the basean.",
            "Trick here is to place a prior on F. So we use a Gaussian process prior with a 0 mean and covariance given by a covariance function K. And we assume that each of these samples are generated is generated independently of all the others.",
            "So we have this graphical model where for each measurement of Y.",
            "Each measurement of Y only depends on the value of the function at that point at the corresponding X and the noise.",
            "For that for that measurement.",
            "And we as I think I said, we assume that the noise is IID is identically and independently distributed.",
            "Zero mean Gaussian noise and becausw every all the variables in this generative model are normal variables we can actually apply Bayes rule and get closed form expressions like we saw in the previous slide.",
            "So these are the closed form expressions that we're going to get through the posterior."
        ],
        [
            "For the posterior.",
            "OK, so we define we define a vector of these wise we define a kernel vector, which is just the the composed of its components are the kernel valuations of the points in the training set with some arbitrary query point X we have a kernel matrix whose columns are just those kernel vectors.",
            "We write the joint distribution of F in our measurements Y as multivariate normal distribution an we apply the conditioning formula from the previous slide of the slide before that to compute.",
            "The distribution the posterior distribution of F at some X conditioned on the Y.",
            "So we use this formula here.",
            "To compute this posterior distribution conditioned on the observed data.",
            "Right, so the point here is that."
        ],
        [
            "Maybe the best way to demonstrate it is by using this example.",
            "So what we have here is that the black line is the sync function so that sine of X divided by the absolute value of X.",
            "And.",
            "We generated 10 samples from this function with some noise, so these are noisy samples.",
            "The black dots of the noisy samples.",
            "And the.",
            "Perform Gaussian process regression.",
            "The blue line, the blue dashed line is the posterior mean and I think here we used a Gaussian kernel.",
            "And the red dotted lines are one standard deviation above and below that posterior mean.",
            "So, compared to other frequentist approaches to regression here, we not only get our estimate for the function, we are also getting.",
            "Confidence intervals on on this estimate, and you see that in places where we didn't have enough samples, our confidence is in our confidence is low, so we have high margins of error an it's the other way around in this area here where we do have a lot of."
        ],
        [
            "Apples.",
            "OK.",
            "So what does this have anything to do with with Markov decision processes and reinforcement learning?",
            "So let me just make a few notational definitions that I'm going to need in the sequel.",
            "So.",
            "In reinforcement learning, we have a mark of decision process.",
            "It generates samples of states an rewards.",
            "It is controlled by some controller that given the state, outputs an action to the MDP.",
            "So the States belong to state SpaceX the actions belong to an action space A.",
            "We also, for notational convenience we will introduce a new notation which is just.",
            "Which is Z, which is just a pair of states of a state and an action.",
            "We have our transition probability density an we have a reward probability density P&Q.",
            "And of course, the rewards, which are now our random variables.",
            "Of the state and the action, the mean reward will be denoted by R with the bar."
        ],
        [
            "Over it.",
            "OK.",
            "So when we go about talking about how to control an MDP, we have our policy which we do not buy mu.",
            "We have a trajectory which is just a sequence of state action pairs.",
            "We have a random variable for each trajectory for each path, which is just the discounted sum of rewards for that path.",
            "The value function is the expected value.",
            "Of of this, D, given an initial state and Q is the same expectation with given an initial state and an initial action.",
            "The goal is to Max to find a policy that maximizes our value function.",
            "And we know that if we have the optimal state action value function available, we can easily find.",
            "The optimal policy an optimal policy, it's now."
        ],
        [
            "Necessarily unique.",
            "So that's the general architecture of value based reinforcement learning algorithms.",
            "So Mohammed, in his talk had a sort of a diagram that showed the different families of reinforcement learning algorithms.",
            "So here we're going to concentrate on those reinforcement learning architectures that are based on a value estimator that learn an user value estimator.",
            "So we have this MDP here.",
            "With a policy controlling it some fixed policy.",
            "And then the these triplets of state rewards and actions are used to learn.",
            "A value estimator and estimate for the true value function of the process.",
            "And this estimate in turn is used to adapt the policy to learn to improve basically the policy, because what we are interested in eventually is a good policy."
        ],
        [
            "So we have Bellman's equation.",
            "And Mohammed already talked about it.",
            "But let me reiterate Bellman's equation basically.",
            "Provides a recursive definition for the value functions of the value function at one state is given in terms of the rewards in that state and the value function of the states to which you can transition from that state.",
            "And the optimal value in policy are just the ones that are attained by by the policy that that provides the maximum.",
            "Over all policies of the value function.",
            "Ends as I said before, their methods that are based on.",
            "Well, I actually didn't say before their methods for solving this based on value iteration and Q learning is 1 good example of that.",
            "But we're not going to discuss that and there are methods that are based on the policy iteration idea.",
            "Such as certain salsa, optimistic policy iteration actor critic algorithms where the idea is basically that you start with some random policy or so, or not necessarily random, but you start with some policy.",
            "You evaluate it and then you do a policy improvement step which requires the value function for the that policy and following that policy improvement step.",
            "Hopefully you have a better policy.",
            "You evaluate that policy.",
            "And after a few steps, hopefully you have a much better policy than the one used."
        ],
        [
            "That'll do it.",
            "So that's sort of the the taxonomy of reinforcement learning algorithms, and we're going to focus on policy iteration, type of algorithms, and as I said before, we need some subroutine that will perform for us the task of policy evaluation, fixed policy.",
            "We want to find its value function.",
            "This is what we're interested in, at least for the next."
        ],
        [
            "Two slides.",
            "So what is missing in current ring for policy evaluation techniques?",
            "I mean there are plenty of algorithms for for evaluating Policy's TD Lambda, for instance, LS, TD, Lambda, all sorts of nice nice algorithms with some even some theoretical nice theoretical properties.",
            "But there's still a few things missing.",
            "And here's a sort of a list of what's missing, and it's not necessarily that every algorithm has all of these things missing to it, but there isn't one algorithm that has that doesn't have at least one of these shortcomings, so some methods can only be applied to small problems, and at least in the early days of reinforcement learning, people tended to use always.",
            "Tabular representation look up tables and look up.",
            "Tables are inherently limited to small problems with small state space and small action space.",
            "There is no probabilistic interpretation about the quality of the estimate, so algorithms tend to return an estimate for the value function, but they don't tell you how good they think this estimate is.",
            "Parametric methods are capable of operating online, so algorithms like TD, Lambda with function approximation or STD.",
            "Lambda.",
            "They can actually update the their estimate of the value function based on data that is accumulated progressively without having to recompute everything from scratch.",
            "So this is what I mean by online.",
            "It's not necessarily what Pascal meant.",
            "There are some nonparametric methods which are more flexible than the parametric ones, but they only they are only capable of working offline.",
            "Because of computational restrictions.",
            "Small stepsize methods based on stochastic approximation algorithms, such as TD Lambda to the Lambda is a good example for that.",
            "They use data inefficiently because inherently what happens is that in the online setting you are provided with a stream of data you see a sample state action reward, some sample, you do some update based on that sample, and that updates.",
            "Because of this stochastic approximation property of these algorithms has to be a small 1, otherwise convergence is not assured.",
            "And then you discard that sample and you'll never see it again.",
            "And this this is an inherent shortcoming of stochastic approximation algorithms.",
            "Finite time solutions lack interpretability, So what I mean by that is beyond the problem that we are not told how good is the estimate.",
            "We are also not told well what is the meaning of that estimate, what the theoretical results that we usually get with these algorithms are symptomatic ones.",
            "So we're told that having watched an infinite amount of data, we converge to something that is close to the true value function.",
            "But what happens if we?",
            "Watched only finite amount of data and we always watching just a finite amount of data.",
            "And then there are convergence issues, which I'm not going to talk to, but some of these algorithms suffer quite heavily from either divergent or convergence too."
        ],
        [
            "Wrong thing.",
            "How much time do I have?",
            "OK, any questions so far.",
            "Please even silly questions.",
            "Yes.",
            "How?",
            "Your quality.",
            "Probably.",
            "If you're not.",
            "So.",
            "I'm not going to talk a lot about actually the the part.",
            "The step of the policy improvement.",
            "Most of this talk is is going to be concerned with the policy evaluation, but imagine that you have.",
            "A distribution over value functions.",
            "Then the naive thing to do would probably be to choose a policy which is greedy with respect to the expected value.",
            "The expectation of the value function.",
            "So there is 1.",
            "A line or one function which corresponds to the blue line.",
            "In the example that I showed you for Gaussian process regression, which is the expected mean and you could.",
            "The nice thing would be to behave greedy greedy with respect to that, but another thing, for example, which would correspond to what is known as interval estimation, would be to act greedily with respect to the posterior mean plus some constant times the posterior standard deviation, and that would encourage exploration of states where you have a high uncertainty about their value.",
            "So this is one thing you could do, but there.",
            "Many other things you can you can do there and, but I'm I don't have the time to discuss it in this talk.",
            "OK, so.",
            "Now I'm going to talk about the Gaussian process, temporal difference learning model, and just like in Gaussian process regression, we need a generative model that relates the observed quantity in our model and the unobserved quantity.",
            "And in our case, the UN observed quantity is going to be the value function.",
            "And the observed quantity is going to be the observed rewards.",
            "The rewards that we observed along a trajectory.",
            "And we're going to use this model.",
            "So the reward of a state or a state action pair depends on what we want to evaluate.",
            "Is the value of that state action of that state minus the discount factor times the value of the next state.",
            "Plus some noise term that should take into account the discrepancy between this term here and this term here.",
            "And why we choose that specific form?",
            "This is something I will show you.",
            "The next slide, so be patient.",
            "In a compact form, this can be written in this way with with the Matrix H being defined as this matrix.",
            "Here an ARB Asian goal, as always, is to compute the posterior distribution of over value functions conditioned on the sequence of observed states.",
            "An rewards OK, and so we will basically follow the recipe that we used in the Gaussian process case.",
            "We will write the joint distribution of rewards and values and then use the conditioning formula to compute the conditional mean and covariance of the value given the observed rule."
        ],
        [
            "Words.",
            "So.",
            "There is 1.",
            "Case where this is quite simple, where it's simple to motivate this this form of our model this form here.",
            "So this is the case where the dynamics of the MDP are deterministic.",
            "So given a state, the next state is deterministically given by the current state.",
            "Or if you like to work using state action pairs, the same is true for state action pairs.",
            "So everything is deterministic except for the rewards which we assume have a mean.",
            "Given by our boss, so the rewards are random variables, but the dynamics is deterministic and now and here if we write the.",
            "The bellman equation.",
            "We get this.",
            "We have the value of the state X is the mean reward of that state plus.",
            "The discount factor times the expected value of the next state, but since the next state is deterministically given, there's no expectation here.",
            "So everything is deterministic, and if you defined the noise variable N as the noise in the rewards, so the difference between the reward and its mean.",
            "Then what you get from this equation here is exactly this.",
            "Equation here this generative model which is.",
            "The same of the same form as the model that I proposed here, or maybe.",
            "Specialization of this of this form, right?",
            "So here the noise is, we assume that the noise is IID and Gaussian, which is a reasonable assumption."
        ],
        [
            "So what happens in the case of stochastic transition?",
            "That's a bit more complex, but I'm going to hopefully convince you that we're getting the same form model of the exact same form.",
            "With the only difference being in the behavior of the noise variable in the covariance of the noise variable.",
            "So the idea here is to take the discounted return from some state from the state's eye and decompose it into its mean, which is just a value function.",
            "And a residual.",
            "OK, so this is basically just adding and subtracting the expectation of the.",
            "And this is what we get.",
            "We have the value function here and this residual which I define as Delta V. And then, assuming the MDP is stationary.",
            "And that we're using a stationary policy.",
            "The.",
            "Where was that?",
            "Yeah, the discounted return satisfies this formula, so the discounted return of XI of the state's eye is the reward plus the discount factor times the discounted return of a state.",
            "Of the next state where the next state is generated according to this to the transition probability distribution of the MDP when it is controlled by the current policy and then by substituting and rearranging.",
            "We can write.",
            "This thing here.",
            "This formula here.",
            "This way, using the value function and the residuals with the.",
            "Where the noise term here in this case is the residual for.",
            "For XI minus the discount factor times the residual for XI plus one.",
            "And what we're assuming here, which is.",
            "As Pascal said, it being basean means that use that.",
            "You specify your assumption explicitly.",
            "So what we assume here is that those residuals are normal.",
            "IID and with some fixed variance, or actually the vents, doesn't have to be fixed, but some variance it can be even state dependent variable variance.",
            "And this, written in a compact form, gives us.",
            "Gives us.",
            "Again, this form of a model, which is the exact same form as the model that we use in the deterministic case, with the only difference being in the covariance of this is not working with the covariance of.",
            "Of the noise.",
            "So instead of having a covariance which is proportional to the identity matrix, here we have a slightly more complex covariance, but it's not all that complex, it's only tridiagonal, so it's the covariance is all zeros except for the main diagonal and the two diagonals above and below that.",
            "Yes.",
            "Seems like you could often get.",
            "Things that get more value.",
            "So that kind of behavior.",
            "That's true, that's true.",
            "So so the this assumption in many cases is pretty far from the truth.",
            "But what I'm I'm not going to show it in this talk, but you can actually you can take a look at the 2nd paper the paper from 2005 or in my thesis and see that this is the assumption that algorithms such as TD, Lambda and LSD Lambda.",
            "Or actually TD one and let us see one implicitly make.",
            "So there is.",
            "So there are cases where this assumption would fail miserably.",
            "Still, the results returned by this algorithm would be not too bad.",
            "Yes.",
            "Assumption which is also can be violated in?",
            "Yeah, that's another assumption, which is generally generally that doesn't hold, so both assumptions generally don't hold.",
            "But the nice thing about it that even if they don't hold.",
            "These algorithms should perform pretty well, because I I can in certain cases I can show how they reduce to well known algorithms that people have been using for maybe 10 or 20 years, and there is a correspondence between these algorithms, but you're right, so these are quite restrictive assumptions.",
            "The idea assumption and the normality assumption yes.",
            "Clear.",
            "Yes, it's the expectation of overall possible trajectories in all possible rewards given the the initial state is XI.",
            "And that depends on the on the policy, which is the reason why denoted by email.",
            "So.",
            "I averaged I don't average over policy's I average over the trajectory trajectory is generated by the policy.",
            "Any more questions, yes.",
            "So Sigma basically serves as a as a regularising has a regularising influence, so the higher the Sigma, the slower you will converge, but the better chances are that you're going to converge to the correct answer.",
            "If Sigma is too small, you will converge very quickly, but not necessarily to the same to the correct answer."
        ],
        [
            "OK, so once we define generative model and the prior a Gaussian process prior, all we have to do is to apply the condition.",
            "The conditioning formula that I showed you in the beginning of the talk.",
            "To this joint distribution of rewards and the value function at some query point X.",
            "That's that's the joint distribution.",
            "This is the prior mean, and this is the prior covariance.",
            "And then use condition on V of X on the sequence of rewards and the resulting expressions have this general kernel form as the.",
            "So basically the posterior mean is just a linear combination of Colonel terms and the posterior covariance is sort of be linear or quadratic form of kernel terms.",
            "With with the Vector Alpha given by this expression here, which is again as we saw in the case of Gaussian process regression is linear in the observed rewards, with the covariance being completely independent of the observed rewards.",
            "So this is just a sort of a generalization of Gaussian process regression.",
            "If the discount factor is 1, this H reduces to the identity matrix and then you our return to the classic Gaussian process.",
            "Regression result, yes.",
            "This car"
        ],
        [
            "So what about learning state action values?",
            "It's basically the same thing.",
            "You just instead of of defining a state based value function, you define a state action based value function and you do the whole exercise again and again.",
            "You have the same sort of expressions for the posterior mean and the posterior covariance.",
            "No, no big deal here, very simple."
        ],
        [
            "Now what happens happens if we want to improve policy's right.",
            "We are interested in in in policy's in obtaining good policy's, not in approximating value functions.",
            "So one family of algorithms for policy improvement is called optimistic policy iteration, and the idea there is that you start with some policy.",
            "Or or some value function and you follow that policy.",
            "You evaluate that policy, but you don't finish evaluating it.",
            "You don't wait until you converge.",
            "Your evaluation algorithm converges to something close to the true value function of that policy, but you immediately update your policy again based on greedy criterion such as the epsilon greedy criterion, for instance, based on your current value function.",
            "So for example, Sarsa.",
            "Is essentially.",
            "Takes online TD, Lambda Saturns TD algorithm and applies this idea of optimistic policy iteration to it, and this this ends up as this is the South algorithm by Romary and Nirajan, and then Sutton popularized it.",
            "And if if what we do, if what if the basic algorithm for policy evaluation that we use is our Gaussian process temporal difference algorithm?",
            "What we end up is with an algorithm that we called GP salsa, and in the case of TD Lambda the parameters of of the policy evaluator is the weight vector that the linear combination coefficients of the basis function of the basis functions for the value function.",
            "And in our case.",
            "It's this vector Alpha for the posterior mean and the matrix C for the posterior covariance."
        ],
        [
            "And that's the algorithm I don't want to.",
            "I don't expect you to sort of understand, go out and implement it using this slide, but the basic idea is that you.",
            "You observe states, actions and rewards.",
            "You choose your action, sort of semi greedily with respect to your current value estimates.",
            "And then you make an update and maybe one thing that I didn't mention is that GPT can be implemented recursively.",
            "So given your current posterior for the value function and and you observed triplet of state, action and rewards.",
            "A simple update, relatively simple update allows you to compute the new posterior without having to compute everything from scratch, so that's a nice property of GPT.",
            "So this is what is done here.",
            "And the reason I call it that there is a temporal difference in the name of the algorithm is that it does use temporal differences to update the posterior here.",
            "And then this whole thing is reiterated.",
            "Hopefully leading to better and better policies, so there's no convergence guarantees for these optimistic policy iteration algorithms, but in practice they tend to.",
            "They seem to work in many practical."
        ],
        [
            "Applications.",
            "Oh OK, never mind.",
            "So here's here's a maze world.",
            "Now.",
            "It's not your conventional maze world or gridworld because it's not agreed.",
            "The state space here is continuous.",
            "It's a 2 dimensional continuous state space.",
            "The actions are also continuous.",
            "So basically we have an agent that is capable of making a move in any directions and those moves are corrupted by noise.",
            "So I think the noise is is like 30 degrees distributed uniformly 30 degrees above or below the intended.",
            "Direction of Motion of the agent.",
            "We have a goal region up here in the top left corner we have obstacles, which are those blue lines.",
            "You can see them better here.",
            "These are the obstacles.",
            "And we have an agent that wants to reach the goal as quickly as possible, so the rewards are negative one for every time step before reaching the goal, and then that's it basically, and then the task terminates once the goal is reached.",
            "And that's the value function.",
            "Attained by GPS salsa, I think after about 200 episodes, so that's that's really not too much in reinforcement learning standards and.",
            "And here is the greedy policy with respect to this value function and you can see that in most cases it does the right thing.",
            "There are a few places where it doesn't do that here for instance here, but in most most places it if you follow the errors you reach the goal.",
            "And the kernel functions function that I used here well for the actions it was a linear kernel.",
            "So basically the kernel is a product of two kernels.",
            "One is a kernel for action which is linear.",
            "It's basically a dot product kernel and the kernel for states is the Gaussian kernel, and these artifacts here are due to the fact that the Gaussian kernel basically assumes that the functions in the hypothesis space are smooth and continuous.",
            "So in borders like this.",
            "It naturally has some trouble in modeling the value function.",
            "Because here the value function is highly discontinuous."
        ],
        [
            "OK. That's my last slide should be, so there are a few challenges left to be tackled.",
            "One of them is how are we?",
            "How are we going to use the value on certainty?",
            "So one suggestion goes back to my answer to your question, which was to use interval estimation to update the policy.",
            "But there there there, there could be many other different ideas and in what I described here, I didn't use any of these.",
            "What's a disciplined way to select actions so Pascal talked quite in length about reconciling exploration and exploitation?",
            "And this GPT architecture sort of provides a mechanism to do that for large reinforcement learning problems.",
            "Reinforcement learning problems with large state and action spaces.",
            "What is the best noise covariance in what I showed you here, I propose two noise covariances, one of them for deterministic transitions and one of them for stochastic transitions, But the best one might be a third different noise covariance, and that's still an open question.",
            "There are issues of bias, variance learning curves.",
            "It would be interesting to see if we could use GPT to solve partially observable Markov decision processes.",
            "And of course, it would be nice to show how this Gaussian process architecture works on more complex tasks and at the end of this tutorial I'm going to show you demo on how we used GPT or other GP salsa to learn to control high dimensional.",
            "Simulated octopus arm.",
            "And the yeah, that's basically questions.",
            "Yes.",
            "You first you didn't ask.",
            "Can you can you talk more loudly?",
            "I can't hear.",
            "Shape.",
            "Are you talking the the Allman Oil paper?",
            "Oh Taiwan's paper.",
            "Yeah that that's.",
            "Well, first of all, this is not a Gaussian mixture model.",
            "This is a Gaussian process model.",
            "These are two different things.",
            "The the basis functions corresponding to the kernel that you choose.",
            "I'm not necessarily Gaussians.",
            "They can be just about anything.",
            "Right there the basically the egg and vectors of the kernel in of the linear operator corresponding to the kernel to your choice of kernel.",
            "So this is not limited to using RBF's.",
            "For example, you can use just about any kernel that you can think of using GPT, and that also includes kernels that are used for things other than points in Euclidean space.",
            "So people have constructed kernels for trees for strings for all sorts of.",
            "Wild things and you can use all of those within this framework.",
            "Yeah, there is a basic so I I showed the the sort of generative model that I assume holds for the value function and I.",
            "In both cases, I hope I motivated it and made my assumptions explicit.",
            "Can I come back?",
            "Yes.",
            "Very simple case at the North Pole.",
            "Well, in the bandit case.",
            "It's the noise model will be exact right?",
            "So you have only one state.",
            "And your rewards are generated from a normal distribution.",
            "Then the noise modeler is exact.",
            "But that's a sort of extremely degenerate case, but The thing is that.",
            "That they are well known results on on estimation results, for example, pertaining to the Kalman filter that basically you re derive the Kalman filter and not by making assumptions on the gaussianity of the variables but but trying to find the optimal linear estimator for your hidden variable.",
            "And there you don't make any any Goshen Attia Sumption.",
            "You just make assumptions about the meaning, the covariance of the variables in question.",
            "And you can show that models very similar to that basically, linear Gaussian models can be derived as optimal minimum variance unbiased estimators, so that might answer any sort of issues concerning the gaussianity of the variables.",
            "It's not.",
            "It's not a critical component.",
            "So noise comes from the randomness of the trajectories, and within each given trajectory the randomness of the rewards collected along their trajectory.",
            "So for instance.",
            "If you know that you have a sort of shortest path kind of problem.",
            "And you have a good guess that it might take you between.",
            "I don't know 10 and 50.",
            "Time steps to reach your goal.",
            "Then that should give you a pretty good indication or in the noise or in the range of the discounted returns that are that you might get while following some policy, right?",
            "You know it's going to be somewhere between what did I say 50 and I don't remember what then what were the numbers that I chose?",
            "But let's say between 10 and 50, right?",
            "So that would be something that you could use as a sort of guiding.",
            "A rule of thumb to tune your your parameters, but you could do what people are doing all the time in machine learning and supervised learning.",
            "And that is just basically tuning parameters using cross validation, right?",
            "This is what people do all the time in supervised learning.",
            "You could do the same here.",
            "While question is not that critical.",
            "The linear estimate only our minimum variance unbiased estimate noise.",
            "Non Russian especially for common filter and so the derivation of the common producer.",
            "This is.",
            "Uncentered countering that actually not the translation.",
            "But nothing seems to be working so well.",
            "No, but if the noise is not Gaussian and you're restricting yourself a priority to linear estimators, then the best you can do, assuming you know the meaning, the covariance of all the variables in question.",
            "The best you can do.",
            "Among this class of linear estimators is the Kalman filter.",
            "I'm not this is not a statement about general estimators.",
            "This is a statement about linear estimators.",
            "OK. OK, I'll pass the podium to Mohammed."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And request is that although we might have a bit of a problem with the screen, I still would like you to sit close to me.",
                    "label": 0
                },
                {
                    "sent": "So, like in the first few lines in this area, and to encourage you to do that, I'm going to forgo the use of the microphone.",
                    "label": 0
                },
                {
                    "sent": "So if you sit in the back, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "When is the next?",
                    "label": 0
                },
                {
                    "sent": "OK questions.",
                    "label": 0
                },
                {
                    "sent": "Interested in how.",
                    "label": 0
                },
                {
                    "sent": "Are there empirical studies looking at how performance degrades when the prior is chosen incorrectly?",
                    "label": 0
                },
                {
                    "sent": "Is that there is this slide about needle where you increase K and get better performance.",
                    "label": 0
                },
                {
                    "sent": "I was surprised it wasn't also a slide looking at OK, and now if our priors wrong with this much, here's how much performance degrades.",
                    "label": 0
                },
                {
                    "sent": "It's wrong, but this much.",
                    "label": 0
                },
                {
                    "sent": "Here's how much it degrades.",
                    "label": 0
                },
                {
                    "sent": "Is that a popular topic?",
                    "label": 0
                },
                {
                    "sent": "Actually, I think first of all, Pascal is more qualified than me to answer this, but I think he had a comparison there where he had one uninformative prior, an one informative prior right in your.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I have several columns.",
                    "label": 0
                },
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "Performance.",
                    "label": 0
                },
                {
                    "sent": "But that was just increasing.",
                    "label": 0
                },
                {
                    "sent": "You increase if you're increasing accounts, but with the wrong file, not just less informative but incorrect.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But the easy answer is that, like in any other machine learning technique, you can do arbitrarily bad.",
                    "label": 0
                },
                {
                    "sent": "So it's up to you as the designer and the user to come up with the best price you can think of.",
                    "label": 0
                },
                {
                    "sent": "Can say OK, this is our primary and define it very precisely, so it seems like you want to know.",
                    "label": 0
                },
                {
                    "sent": "How closely that prior needs to match the actual price?",
                    "label": 0
                },
                {
                    "sent": "I guess that's more of a imparable.",
                    "label": 0
                },
                {
                    "sent": "Well, that's also sort of algorithm algorithm dependent, so it.",
                    "label": 0
                },
                {
                    "sent": "Slide.",
                    "label": 0
                },
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "Update.",
                    "label": 0
                },
                {
                    "sent": "5.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Questions like how bad is required?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will start so my name is I have to use this laser.",
                    "label": 0
                },
                {
                    "sent": "My name is Yaacov Angle and the work that I'm going to describe here, namely Gaussian process, temporal difference learning is my collaboration with Shimon Owen on my ear from Israel, or at least one of them is from Israel and the other is now in Canada.",
                    "label": 1
                },
                {
                    "sent": "And those are my sponsoring agencies and institutions and etc.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So Pascal already, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll try to speak forward.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So Pascal already said mentioned the benefits of using the Bayesian approach in general and for reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Here we use a specific.",
                    "label": 0
                },
                {
                    "sent": "Basian tool known as Gaussian processes and I'm going to sort of mention the benefits of using that.",
                    "label": 0
                },
                {
                    "sent": "So first the Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "So Gaussian process processes for reinforcement learning is a Bayesian approach to value estimation with all the benefits and the difficulties.",
                    "label": 1
                },
                {
                    "sent": "Or maybe not all the difficulties it forces to make our assumptions explicit as Pascal said.",
                    "label": 0
                },
                {
                    "sent": "In this case, in the Gaussian process case, it's a nonparametric approach to reinforcement learning, meaning that priors are placed.",
                    "label": 0
                },
                {
                    "sent": "An inference is performed directly in function space, and this is through the use of kernels.",
                    "label": 1
                },
                {
                    "sent": "The cones that you know from kernel machines.",
                    "label": 1
                },
                {
                    "sent": "However, Gaussian processes can also be defined parametrically.",
                    "label": 1
                },
                {
                    "sent": "Domain knowledge is intuitively coded in priors.",
                    "label": 0
                },
                {
                    "sent": "It provides a full posterior over value functions, not just point estimates like frequentist methods do.",
                    "label": 0
                },
                {
                    "sent": "And as I will show you near the end, hopefully there exist sufficient online implementations for the algorithms resulting from this framework.",
                    "label": 0
                },
                {
                    "sent": "So for at least for the first part of the talk, I'm going to concentrate on the problem of policy evaluation.",
                    "label": 0
                },
                {
                    "sent": "So we have a fixed policy and we want to evaluate the value function for that policy.",
                    "label": 0
                },
                {
                    "sent": "So what are Gauss?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And processes so the formal definition of Gaussian processes goes as follows.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian processes an index set of jointly Gaussian random variables.",
                    "label": 1
                },
                {
                    "sent": "Now the word index might be a bit misleading, because in the context of random processes and Gaussian random processes in general in particular.",
                    "label": 0
                },
                {
                    "sent": "The index set can be just about any set, and here in this example I of the batteries down on this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Can you see anything?",
                    "label": 0
                },
                {
                    "sent": "Anyway, I'll try to juggle these.",
                    "label": 0
                },
                {
                    "sent": "So in this case I chose.",
                    "label": 0
                },
                {
                    "sent": "The index set to be the N dimensional unit cube, so indexes are just points in an dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Oh thanks.",
                    "label": 0
                },
                {
                    "sent": "It's red.",
                    "label": 0
                },
                {
                    "sent": "Oh, that's a bit better.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Since since are this Gaussian process F?",
                    "label": 0
                },
                {
                    "sent": "Is a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "We only need to specify its mean and its covariance.",
                    "label": 0
                },
                {
                    "sent": "Now, it's mean is going to be a function over those indexes over this X and its covariance is also going to be a function over pairs of X is like that.",
                    "label": 0
                },
                {
                    "sent": "And there the conditions on K in order to to make a legitimate covariance function is that it is of course symmetric and positive definite.",
                    "label": 0
                },
                {
                    "sent": "And this means that this function K can be legitimate, legitimately be named a Mercer kernel.",
                    "label": 0
                },
                {
                    "sent": "The kernels that we know from kernel machines, any questions so far?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a generalization of a Gaussian of a multivariate.",
                    "label": 0
                },
                {
                    "sent": "Gaussian random variable to the case where the number of components is infinite or even uncountably infinite.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of a use.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cool way to think about it.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "We have a set of basis functions.",
                    "label": 0
                },
                {
                    "sent": "Set of basis functions there.",
                    "label": 1
                },
                {
                    "sent": "If you want to feed N, we combine them linearly using the weights W 1 to WN.",
                    "label": 0
                },
                {
                    "sent": "And some of these up and the result is RF of X. OK, so that's a shorthand notation for this linear combination and what we do is to specify a normal distribution for the mean and covariance of that weight vector, so that weight vector is no longer just a vector, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a random vector, and it's a Gaussian random vector.",
                    "label": 0
                },
                {
                    "sent": "So what happens when we do that?",
                    "label": 1
                },
                {
                    "sent": "Let's look how how that influences the distribution of F, because F is a function of a random variable, so F in itself.",
                    "label": 0
                },
                {
                    "sent": "Is now a random variable, or more precisely, random vector, so the expected value of F of X for any X?",
                    "label": 0
                },
                {
                    "sent": "In our domain is the inner product between these this vector of features fee of X and the mean vector of of the weight vector?",
                    "label": 0
                },
                {
                    "sent": "And the covariance is given by this expression here.",
                    "label": 0
                },
                {
                    "sent": "And this is just basic algebra.",
                    "label": 0
                },
                {
                    "sent": "You can do it yourself very easily, so this is 1 sort of easy parametric way to specify a Gaussian random process.",
                    "label": 0
                },
                {
                    "sent": "But as we shall see soon, we don't have to define it parametrically.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should say something more about that so so when I say that we don't have to define it parametrically, what I mean is that instead of defining MNC, the vector M in the matrix C, We just need to define this function one function of X here, which doesn't necessarily have to be written in this form and one function of X&X prime here which has to satisfy this symmetry and positive definiteness of the curve of the kernel function.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It happens.",
                    "label": 0
                },
                {
                    "sent": "When we want to condition on such.",
                    "label": 0
                },
                {
                    "sent": "In this space of Gaussian processes, so this is a theorem which is, at least according to one book, it's called the Gauss Markov Theorem, although I think there's no agreement about the what people actually mean when they say Gauss Markov theorem.",
                    "label": 0
                },
                {
                    "sent": "But anyway, this is just basic use of Bayes rule, so we have Z&Y which are random vectors that are jointly distributed according to the multivariate normal distribution.",
                    "label": 1
                },
                {
                    "sent": "So that's Z and that's why they have a joint mean given by this vector and the joint covariance given by this matrix.",
                    "label": 0
                },
                {
                    "sent": "Here, each each element here is in itself a matrix, because both then why can be vectors?",
                    "label": 0
                },
                {
                    "sent": "And then what the theorem says is that in order to compute the value of Z conditioned on the value of Y or the distribution of the condition on the value of Y.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "The that that distribution that conditional distribution is also Gaussian and the mean is given by this expression here, which is linear in Y. OK, then we have all these matrices here that appear in the joint covariance.",
                    "label": 0
                },
                {
                    "sent": "And the and the conditional covariance is given by this term here, which is just the prior covariance minus some term.",
                    "label": 0
                },
                {
                    "sent": "So measurements basically always reduce our uncertainty about Z measurements of why reducer uncertainty about.",
                    "label": 0
                },
                {
                    "sent": "See they never increase it.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's actually basically applying Bayes rule too.",
                    "label": 0
                },
                {
                    "sent": "Multivariate gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how?",
                    "label": 0
                },
                {
                    "sent": "Al Gaussian process is used in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Well, the simplest example is Gaussian process regression and in Gaussian process regression, what you basically get is a set of pairs of samples.",
                    "label": 0
                },
                {
                    "sent": "Pairs of XX is an wise, and the goal is to regress to find the function of X, which sort of approximate those Y values in a sort of a good way.",
                    "label": 0
                },
                {
                    "sent": "So the way this is done using Gaussian process regression is by specifying this generative model.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that Y is generated by taking a sample from F. And then adding some white noise to that sample.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What we need to do the basean.",
                    "label": 0
                },
                {
                    "sent": "Trick here is to place a prior on F. So we use a Gaussian process prior with a 0 mean and covariance given by a covariance function K. And we assume that each of these samples are generated is generated independently of all the others.",
                    "label": 0
                },
                {
                    "sent": "So we have this graphical model where for each measurement of Y.",
                    "label": 0
                },
                {
                    "sent": "Each measurement of Y only depends on the value of the function at that point at the corresponding X and the noise.",
                    "label": 0
                },
                {
                    "sent": "For that for that measurement.",
                    "label": 0
                },
                {
                    "sent": "And we as I think I said, we assume that the noise is IID is identically and independently distributed.",
                    "label": 0
                },
                {
                    "sent": "Zero mean Gaussian noise and becausw every all the variables in this generative model are normal variables we can actually apply Bayes rule and get closed form expressions like we saw in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So these are the closed form expressions that we're going to get through the posterior.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the posterior.",
                    "label": 0
                },
                {
                    "sent": "OK, so we define we define a vector of these wise we define a kernel vector, which is just the the composed of its components are the kernel valuations of the points in the training set with some arbitrary query point X we have a kernel matrix whose columns are just those kernel vectors.",
                    "label": 0
                },
                {
                    "sent": "We write the joint distribution of F in our measurements Y as multivariate normal distribution an we apply the conditioning formula from the previous slide of the slide before that to compute.",
                    "label": 1
                },
                {
                    "sent": "The distribution the posterior distribution of F at some X conditioned on the Y.",
                    "label": 0
                },
                {
                    "sent": "So we use this formula here.",
                    "label": 0
                },
                {
                    "sent": "To compute this posterior distribution conditioned on the observed data.",
                    "label": 0
                },
                {
                    "sent": "Right, so the point here is that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe the best way to demonstrate it is by using this example.",
                    "label": 0
                },
                {
                    "sent": "So what we have here is that the black line is the sync function so that sine of X divided by the absolute value of X.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We generated 10 samples from this function with some noise, so these are noisy samples.",
                    "label": 0
                },
                {
                    "sent": "The black dots of the noisy samples.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "Perform Gaussian process regression.",
                    "label": 0
                },
                {
                    "sent": "The blue line, the blue dashed line is the posterior mean and I think here we used a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "And the red dotted lines are one standard deviation above and below that posterior mean.",
                    "label": 0
                },
                {
                    "sent": "So, compared to other frequentist approaches to regression here, we not only get our estimate for the function, we are also getting.",
                    "label": 0
                },
                {
                    "sent": "Confidence intervals on on this estimate, and you see that in places where we didn't have enough samples, our confidence is in our confidence is low, so we have high margins of error an it's the other way around in this area here where we do have a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apples.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what does this have anything to do with with Markov decision processes and reinforcement learning?",
                    "label": 1
                },
                {
                    "sent": "So let me just make a few notational definitions that I'm going to need in the sequel.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In reinforcement learning, we have a mark of decision process.",
                    "label": 0
                },
                {
                    "sent": "It generates samples of states an rewards.",
                    "label": 0
                },
                {
                    "sent": "It is controlled by some controller that given the state, outputs an action to the MDP.",
                    "label": 0
                },
                {
                    "sent": "So the States belong to state SpaceX the actions belong to an action space A.",
                    "label": 1
                },
                {
                    "sent": "We also, for notational convenience we will introduce a new notation which is just.",
                    "label": 0
                },
                {
                    "sent": "Which is Z, which is just a pair of states of a state and an action.",
                    "label": 0
                },
                {
                    "sent": "We have our transition probability density an we have a reward probability density P&Q.",
                    "label": 0
                },
                {
                    "sent": "And of course, the rewards, which are now our random variables.",
                    "label": 0
                },
                {
                    "sent": "Of the state and the action, the mean reward will be denoted by R with the bar.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Over it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So when we go about talking about how to control an MDP, we have our policy which we do not buy mu.",
                    "label": 0
                },
                {
                    "sent": "We have a trajectory which is just a sequence of state action pairs.",
                    "label": 0
                },
                {
                    "sent": "We have a random variable for each trajectory for each path, which is just the discounted sum of rewards for that path.",
                    "label": 0
                },
                {
                    "sent": "The value function is the expected value.",
                    "label": 0
                },
                {
                    "sent": "Of of this, D, given an initial state and Q is the same expectation with given an initial state and an initial action.",
                    "label": 0
                },
                {
                    "sent": "The goal is to Max to find a policy that maximizes our value function.",
                    "label": 1
                },
                {
                    "sent": "And we know that if we have the optimal state action value function available, we can easily find.",
                    "label": 1
                },
                {
                    "sent": "The optimal policy an optimal policy, it's now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Necessarily unique.",
                    "label": 0
                },
                {
                    "sent": "So that's the general architecture of value based reinforcement learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So Mohammed, in his talk had a sort of a diagram that showed the different families of reinforcement learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So here we're going to concentrate on those reinforcement learning architectures that are based on a value estimator that learn an user value estimator.",
                    "label": 0
                },
                {
                    "sent": "So we have this MDP here.",
                    "label": 0
                },
                {
                    "sent": "With a policy controlling it some fixed policy.",
                    "label": 0
                },
                {
                    "sent": "And then the these triplets of state rewards and actions are used to learn.",
                    "label": 0
                },
                {
                    "sent": "A value estimator and estimate for the true value function of the process.",
                    "label": 0
                },
                {
                    "sent": "And this estimate in turn is used to adapt the policy to learn to improve basically the policy, because what we are interested in eventually is a good policy.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have Bellman's equation.",
                    "label": 1
                },
                {
                    "sent": "And Mohammed already talked about it.",
                    "label": 0
                },
                {
                    "sent": "But let me reiterate Bellman's equation basically.",
                    "label": 0
                },
                {
                    "sent": "Provides a recursive definition for the value functions of the value function at one state is given in terms of the rewards in that state and the value function of the states to which you can transition from that state.",
                    "label": 0
                },
                {
                    "sent": "And the optimal value in policy are just the ones that are attained by by the policy that that provides the maximum.",
                    "label": 0
                },
                {
                    "sent": "Over all policies of the value function.",
                    "label": 0
                },
                {
                    "sent": "Ends as I said before, their methods that are based on.",
                    "label": 0
                },
                {
                    "sent": "Well, I actually didn't say before their methods for solving this based on value iteration and Q learning is 1 good example of that.",
                    "label": 1
                },
                {
                    "sent": "But we're not going to discuss that and there are methods that are based on the policy iteration idea.",
                    "label": 0
                },
                {
                    "sent": "Such as certain salsa, optimistic policy iteration actor critic algorithms where the idea is basically that you start with some random policy or so, or not necessarily random, but you start with some policy.",
                    "label": 0
                },
                {
                    "sent": "You evaluate it and then you do a policy improvement step which requires the value function for the that policy and following that policy improvement step.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you have a better policy.",
                    "label": 0
                },
                {
                    "sent": "You evaluate that policy.",
                    "label": 0
                },
                {
                    "sent": "And after a few steps, hopefully you have a much better policy than the one used.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That'll do it.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the the taxonomy of reinforcement learning algorithms, and we're going to focus on policy iteration, type of algorithms, and as I said before, we need some subroutine that will perform for us the task of policy evaluation, fixed policy.",
                    "label": 1
                },
                {
                    "sent": "We want to find its value function.",
                    "label": 0
                },
                {
                    "sent": "This is what we're interested in, at least for the next.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two slides.",
                    "label": 0
                },
                {
                    "sent": "So what is missing in current ring for policy evaluation techniques?",
                    "label": 0
                },
                {
                    "sent": "I mean there are plenty of algorithms for for evaluating Policy's TD Lambda, for instance, LS, TD, Lambda, all sorts of nice nice algorithms with some even some theoretical nice theoretical properties.",
                    "label": 0
                },
                {
                    "sent": "But there's still a few things missing.",
                    "label": 0
                },
                {
                    "sent": "And here's a sort of a list of what's missing, and it's not necessarily that every algorithm has all of these things missing to it, but there isn't one algorithm that has that doesn't have at least one of these shortcomings, so some methods can only be applied to small problems, and at least in the early days of reinforcement learning, people tended to use always.",
                    "label": 1
                },
                {
                    "sent": "Tabular representation look up tables and look up.",
                    "label": 0
                },
                {
                    "sent": "Tables are inherently limited to small problems with small state space and small action space.",
                    "label": 0
                },
                {
                    "sent": "There is no probabilistic interpretation about the quality of the estimate, so algorithms tend to return an estimate for the value function, but they don't tell you how good they think this estimate is.",
                    "label": 0
                },
                {
                    "sent": "Parametric methods are capable of operating online, so algorithms like TD, Lambda with function approximation or STD.",
                    "label": 1
                },
                {
                    "sent": "Lambda.",
                    "label": 0
                },
                {
                    "sent": "They can actually update the their estimate of the value function based on data that is accumulated progressively without having to recompute everything from scratch.",
                    "label": 0
                },
                {
                    "sent": "So this is what I mean by online.",
                    "label": 1
                },
                {
                    "sent": "It's not necessarily what Pascal meant.",
                    "label": 0
                },
                {
                    "sent": "There are some nonparametric methods which are more flexible than the parametric ones, but they only they are only capable of working offline.",
                    "label": 0
                },
                {
                    "sent": "Because of computational restrictions.",
                    "label": 0
                },
                {
                    "sent": "Small stepsize methods based on stochastic approximation algorithms, such as TD Lambda to the Lambda is a good example for that.",
                    "label": 0
                },
                {
                    "sent": "They use data inefficiently because inherently what happens is that in the online setting you are provided with a stream of data you see a sample state action reward, some sample, you do some update based on that sample, and that updates.",
                    "label": 0
                },
                {
                    "sent": "Because of this stochastic approximation property of these algorithms has to be a small 1, otherwise convergence is not assured.",
                    "label": 0
                },
                {
                    "sent": "And then you discard that sample and you'll never see it again.",
                    "label": 0
                },
                {
                    "sent": "And this this is an inherent shortcoming of stochastic approximation algorithms.",
                    "label": 1
                },
                {
                    "sent": "Finite time solutions lack interpretability, So what I mean by that is beyond the problem that we are not told how good is the estimate.",
                    "label": 0
                },
                {
                    "sent": "We are also not told well what is the meaning of that estimate, what the theoretical results that we usually get with these algorithms are symptomatic ones.",
                    "label": 0
                },
                {
                    "sent": "So we're told that having watched an infinite amount of data, we converge to something that is close to the true value function.",
                    "label": 0
                },
                {
                    "sent": "But what happens if we?",
                    "label": 0
                },
                {
                    "sent": "Watched only finite amount of data and we always watching just a finite amount of data.",
                    "label": 0
                },
                {
                    "sent": "And then there are convergence issues, which I'm not going to talk to, but some of these algorithms suffer quite heavily from either divergent or convergence too.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wrong thing.",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "OK, any questions so far.",
                    "label": 0
                },
                {
                    "sent": "Please even silly questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Your quality.",
                    "label": 0
                },
                {
                    "sent": "Probably.",
                    "label": 0
                },
                {
                    "sent": "If you're not.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk a lot about actually the the part.",
                    "label": 0
                },
                {
                    "sent": "The step of the policy improvement.",
                    "label": 0
                },
                {
                    "sent": "Most of this talk is is going to be concerned with the policy evaluation, but imagine that you have.",
                    "label": 0
                },
                {
                    "sent": "A distribution over value functions.",
                    "label": 0
                },
                {
                    "sent": "Then the naive thing to do would probably be to choose a policy which is greedy with respect to the expected value.",
                    "label": 0
                },
                {
                    "sent": "The expectation of the value function.",
                    "label": 0
                },
                {
                    "sent": "So there is 1.",
                    "label": 0
                },
                {
                    "sent": "A line or one function which corresponds to the blue line.",
                    "label": 0
                },
                {
                    "sent": "In the example that I showed you for Gaussian process regression, which is the expected mean and you could.",
                    "label": 0
                },
                {
                    "sent": "The nice thing would be to behave greedy greedy with respect to that, but another thing, for example, which would correspond to what is known as interval estimation, would be to act greedily with respect to the posterior mean plus some constant times the posterior standard deviation, and that would encourage exploration of states where you have a high uncertainty about their value.",
                    "label": 0
                },
                {
                    "sent": "So this is one thing you could do, but there.",
                    "label": 0
                },
                {
                    "sent": "Many other things you can you can do there and, but I'm I don't have the time to discuss it in this talk.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to talk about the Gaussian process, temporal difference learning model, and just like in Gaussian process regression, we need a generative model that relates the observed quantity in our model and the unobserved quantity.",
                    "label": 0
                },
                {
                    "sent": "And in our case, the UN observed quantity is going to be the value function.",
                    "label": 0
                },
                {
                    "sent": "And the observed quantity is going to be the observed rewards.",
                    "label": 0
                },
                {
                    "sent": "The rewards that we observed along a trajectory.",
                    "label": 0
                },
                {
                    "sent": "And we're going to use this model.",
                    "label": 1
                },
                {
                    "sent": "So the reward of a state or a state action pair depends on what we want to evaluate.",
                    "label": 0
                },
                {
                    "sent": "Is the value of that state action of that state minus the discount factor times the value of the next state.",
                    "label": 0
                },
                {
                    "sent": "Plus some noise term that should take into account the discrepancy between this term here and this term here.",
                    "label": 0
                },
                {
                    "sent": "And why we choose that specific form?",
                    "label": 0
                },
                {
                    "sent": "This is something I will show you.",
                    "label": 0
                },
                {
                    "sent": "The next slide, so be patient.",
                    "label": 0
                },
                {
                    "sent": "In a compact form, this can be written in this way with with the Matrix H being defined as this matrix.",
                    "label": 0
                },
                {
                    "sent": "Here an ARB Asian goal, as always, is to compute the posterior distribution of over value functions conditioned on the sequence of observed states.",
                    "label": 1
                },
                {
                    "sent": "An rewards OK, and so we will basically follow the recipe that we used in the Gaussian process case.",
                    "label": 0
                },
                {
                    "sent": "We will write the joint distribution of rewards and values and then use the conditioning formula to compute the conditional mean and covariance of the value given the observed rule.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There is 1.",
                    "label": 0
                },
                {
                    "sent": "Case where this is quite simple, where it's simple to motivate this this form of our model this form here.",
                    "label": 0
                },
                {
                    "sent": "So this is the case where the dynamics of the MDP are deterministic.",
                    "label": 0
                },
                {
                    "sent": "So given a state, the next state is deterministically given by the current state.",
                    "label": 0
                },
                {
                    "sent": "Or if you like to work using state action pairs, the same is true for state action pairs.",
                    "label": 0
                },
                {
                    "sent": "So everything is deterministic except for the rewards which we assume have a mean.",
                    "label": 0
                },
                {
                    "sent": "Given by our boss, so the rewards are random variables, but the dynamics is deterministic and now and here if we write the.",
                    "label": 0
                },
                {
                    "sent": "The bellman equation.",
                    "label": 0
                },
                {
                    "sent": "We get this.",
                    "label": 0
                },
                {
                    "sent": "We have the value of the state X is the mean reward of that state plus.",
                    "label": 0
                },
                {
                    "sent": "The discount factor times the expected value of the next state, but since the next state is deterministically given, there's no expectation here.",
                    "label": 0
                },
                {
                    "sent": "So everything is deterministic, and if you defined the noise variable N as the noise in the rewards, so the difference between the reward and its mean.",
                    "label": 0
                },
                {
                    "sent": "Then what you get from this equation here is exactly this.",
                    "label": 0
                },
                {
                    "sent": "Equation here this generative model which is.",
                    "label": 0
                },
                {
                    "sent": "The same of the same form as the model that I proposed here, or maybe.",
                    "label": 0
                },
                {
                    "sent": "Specialization of this of this form, right?",
                    "label": 0
                },
                {
                    "sent": "So here the noise is, we assume that the noise is IID and Gaussian, which is a reasonable assumption.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what happens in the case of stochastic transition?",
                    "label": 0
                },
                {
                    "sent": "That's a bit more complex, but I'm going to hopefully convince you that we're getting the same form model of the exact same form.",
                    "label": 0
                },
                {
                    "sent": "With the only difference being in the behavior of the noise variable in the covariance of the noise variable.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is to take the discounted return from some state from the state's eye and decompose it into its mean, which is just a value function.",
                    "label": 0
                },
                {
                    "sent": "And a residual.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is basically just adding and subtracting the expectation of the.",
                    "label": 0
                },
                {
                    "sent": "And this is what we get.",
                    "label": 0
                },
                {
                    "sent": "We have the value function here and this residual which I define as Delta V. And then, assuming the MDP is stationary.",
                    "label": 0
                },
                {
                    "sent": "And that we're using a stationary policy.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Where was that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the discounted return satisfies this formula, so the discounted return of XI of the state's eye is the reward plus the discount factor times the discounted return of a state.",
                    "label": 0
                },
                {
                    "sent": "Of the next state where the next state is generated according to this to the transition probability distribution of the MDP when it is controlled by the current policy and then by substituting and rearranging.",
                    "label": 0
                },
                {
                    "sent": "We can write.",
                    "label": 0
                },
                {
                    "sent": "This thing here.",
                    "label": 0
                },
                {
                    "sent": "This formula here.",
                    "label": 0
                },
                {
                    "sent": "This way, using the value function and the residuals with the.",
                    "label": 0
                },
                {
                    "sent": "Where the noise term here in this case is the residual for.",
                    "label": 0
                },
                {
                    "sent": "For XI minus the discount factor times the residual for XI plus one.",
                    "label": 0
                },
                {
                    "sent": "And what we're assuming here, which is.",
                    "label": 0
                },
                {
                    "sent": "As Pascal said, it being basean means that use that.",
                    "label": 0
                },
                {
                    "sent": "You specify your assumption explicitly.",
                    "label": 0
                },
                {
                    "sent": "So what we assume here is that those residuals are normal.",
                    "label": 0
                },
                {
                    "sent": "IID and with some fixed variance, or actually the vents, doesn't have to be fixed, but some variance it can be even state dependent variable variance.",
                    "label": 0
                },
                {
                    "sent": "And this, written in a compact form, gives us.",
                    "label": 0
                },
                {
                    "sent": "Gives us.",
                    "label": 0
                },
                {
                    "sent": "Again, this form of a model, which is the exact same form as the model that we use in the deterministic case, with the only difference being in the covariance of this is not working with the covariance of.",
                    "label": 0
                },
                {
                    "sent": "Of the noise.",
                    "label": 0
                },
                {
                    "sent": "So instead of having a covariance which is proportional to the identity matrix, here we have a slightly more complex covariance, but it's not all that complex, it's only tridiagonal, so it's the covariance is all zeros except for the main diagonal and the two diagonals above and below that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Seems like you could often get.",
                    "label": 0
                },
                {
                    "sent": "Things that get more value.",
                    "label": 0
                },
                {
                    "sent": "So that kind of behavior.",
                    "label": 0
                },
                {
                    "sent": "That's true, that's true.",
                    "label": 0
                },
                {
                    "sent": "So so the this assumption in many cases is pretty far from the truth.",
                    "label": 0
                },
                {
                    "sent": "But what I'm I'm not going to show it in this talk, but you can actually you can take a look at the 2nd paper the paper from 2005 or in my thesis and see that this is the assumption that algorithms such as TD, Lambda and LSD Lambda.",
                    "label": 0
                },
                {
                    "sent": "Or actually TD one and let us see one implicitly make.",
                    "label": 0
                },
                {
                    "sent": "So there is.",
                    "label": 0
                },
                {
                    "sent": "So there are cases where this assumption would fail miserably.",
                    "label": 0
                },
                {
                    "sent": "Still, the results returned by this algorithm would be not too bad.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Assumption which is also can be violated in?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's another assumption, which is generally generally that doesn't hold, so both assumptions generally don't hold.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing about it that even if they don't hold.",
                    "label": 0
                },
                {
                    "sent": "These algorithms should perform pretty well, because I I can in certain cases I can show how they reduce to well known algorithms that people have been using for maybe 10 or 20 years, and there is a correspondence between these algorithms, but you're right, so these are quite restrictive assumptions.",
                    "label": 0
                },
                {
                    "sent": "The idea assumption and the normality assumption yes.",
                    "label": 0
                },
                {
                    "sent": "Clear.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's the expectation of overall possible trajectories in all possible rewards given the the initial state is XI.",
                    "label": 0
                },
                {
                    "sent": "And that depends on the on the policy, which is the reason why denoted by email.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I averaged I don't average over policy's I average over the trajectory trajectory is generated by the policy.",
                    "label": 0
                },
                {
                    "sent": "Any more questions, yes.",
                    "label": 0
                },
                {
                    "sent": "So Sigma basically serves as a as a regularising has a regularising influence, so the higher the Sigma, the slower you will converge, but the better chances are that you're going to converge to the correct answer.",
                    "label": 0
                },
                {
                    "sent": "If Sigma is too small, you will converge very quickly, but not necessarily to the same to the correct answer.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so once we define generative model and the prior a Gaussian process prior, all we have to do is to apply the condition.",
                    "label": 0
                },
                {
                    "sent": "The conditioning formula that I showed you in the beginning of the talk.",
                    "label": 0
                },
                {
                    "sent": "To this joint distribution of rewards and the value function at some query point X.",
                    "label": 0
                },
                {
                    "sent": "That's that's the joint distribution.",
                    "label": 1
                },
                {
                    "sent": "This is the prior mean, and this is the prior covariance.",
                    "label": 1
                },
                {
                    "sent": "And then use condition on V of X on the sequence of rewards and the resulting expressions have this general kernel form as the.",
                    "label": 0
                },
                {
                    "sent": "So basically the posterior mean is just a linear combination of Colonel terms and the posterior covariance is sort of be linear or quadratic form of kernel terms.",
                    "label": 0
                },
                {
                    "sent": "With with the Vector Alpha given by this expression here, which is again as we saw in the case of Gaussian process regression is linear in the observed rewards, with the covariance being completely independent of the observed rewards.",
                    "label": 0
                },
                {
                    "sent": "So this is just a sort of a generalization of Gaussian process regression.",
                    "label": 0
                },
                {
                    "sent": "If the discount factor is 1, this H reduces to the identity matrix and then you our return to the classic Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Regression result, yes.",
                    "label": 0
                },
                {
                    "sent": "This car",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what about learning state action values?",
                    "label": 0
                },
                {
                    "sent": "It's basically the same thing.",
                    "label": 0
                },
                {
                    "sent": "You just instead of of defining a state based value function, you define a state action based value function and you do the whole exercise again and again.",
                    "label": 0
                },
                {
                    "sent": "You have the same sort of expressions for the posterior mean and the posterior covariance.",
                    "label": 0
                },
                {
                    "sent": "No, no big deal here, very simple.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what happens happens if we want to improve policy's right.",
                    "label": 0
                },
                {
                    "sent": "We are interested in in in policy's in obtaining good policy's, not in approximating value functions.",
                    "label": 0
                },
                {
                    "sent": "So one family of algorithms for policy improvement is called optimistic policy iteration, and the idea there is that you start with some policy.",
                    "label": 1
                },
                {
                    "sent": "Or or some value function and you follow that policy.",
                    "label": 0
                },
                {
                    "sent": "You evaluate that policy, but you don't finish evaluating it.",
                    "label": 0
                },
                {
                    "sent": "You don't wait until you converge.",
                    "label": 0
                },
                {
                    "sent": "Your evaluation algorithm converges to something close to the true value function of that policy, but you immediately update your policy again based on greedy criterion such as the epsilon greedy criterion, for instance, based on your current value function.",
                    "label": 0
                },
                {
                    "sent": "So for example, Sarsa.",
                    "label": 0
                },
                {
                    "sent": "Is essentially.",
                    "label": 0
                },
                {
                    "sent": "Takes online TD, Lambda Saturns TD algorithm and applies this idea of optimistic policy iteration to it, and this this ends up as this is the South algorithm by Romary and Nirajan, and then Sutton popularized it.",
                    "label": 0
                },
                {
                    "sent": "And if if what we do, if what if the basic algorithm for policy evaluation that we use is our Gaussian process temporal difference algorithm?",
                    "label": 0
                },
                {
                    "sent": "What we end up is with an algorithm that we called GP salsa, and in the case of TD Lambda the parameters of of the policy evaluator is the weight vector that the linear combination coefficients of the basis function of the basis functions for the value function.",
                    "label": 0
                },
                {
                    "sent": "And in our case.",
                    "label": 0
                },
                {
                    "sent": "It's this vector Alpha for the posterior mean and the matrix C for the posterior covariance.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's the algorithm I don't want to.",
                    "label": 0
                },
                {
                    "sent": "I don't expect you to sort of understand, go out and implement it using this slide, but the basic idea is that you.",
                    "label": 0
                },
                {
                    "sent": "You observe states, actions and rewards.",
                    "label": 0
                },
                {
                    "sent": "You choose your action, sort of semi greedily with respect to your current value estimates.",
                    "label": 0
                },
                {
                    "sent": "And then you make an update and maybe one thing that I didn't mention is that GPT can be implemented recursively.",
                    "label": 0
                },
                {
                    "sent": "So given your current posterior for the value function and and you observed triplet of state, action and rewards.",
                    "label": 0
                },
                {
                    "sent": "A simple update, relatively simple update allows you to compute the new posterior without having to compute everything from scratch, so that's a nice property of GPT.",
                    "label": 0
                },
                {
                    "sent": "So this is what is done here.",
                    "label": 0
                },
                {
                    "sent": "And the reason I call it that there is a temporal difference in the name of the algorithm is that it does use temporal differences to update the posterior here.",
                    "label": 0
                },
                {
                    "sent": "And then this whole thing is reiterated.",
                    "label": 0
                },
                {
                    "sent": "Hopefully leading to better and better policies, so there's no convergence guarantees for these optimistic policy iteration algorithms, but in practice they tend to.",
                    "label": 0
                },
                {
                    "sent": "They seem to work in many practical.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applications.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, never mind.",
                    "label": 0
                },
                {
                    "sent": "So here's here's a maze world.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "It's not your conventional maze world or gridworld because it's not agreed.",
                    "label": 0
                },
                {
                    "sent": "The state space here is continuous.",
                    "label": 0
                },
                {
                    "sent": "It's a 2 dimensional continuous state space.",
                    "label": 0
                },
                {
                    "sent": "The actions are also continuous.",
                    "label": 0
                },
                {
                    "sent": "So basically we have an agent that is capable of making a move in any directions and those moves are corrupted by noise.",
                    "label": 0
                },
                {
                    "sent": "So I think the noise is is like 30 degrees distributed uniformly 30 degrees above or below the intended.",
                    "label": 0
                },
                {
                    "sent": "Direction of Motion of the agent.",
                    "label": 0
                },
                {
                    "sent": "We have a goal region up here in the top left corner we have obstacles, which are those blue lines.",
                    "label": 0
                },
                {
                    "sent": "You can see them better here.",
                    "label": 0
                },
                {
                    "sent": "These are the obstacles.",
                    "label": 0
                },
                {
                    "sent": "And we have an agent that wants to reach the goal as quickly as possible, so the rewards are negative one for every time step before reaching the goal, and then that's it basically, and then the task terminates once the goal is reached.",
                    "label": 0
                },
                {
                    "sent": "And that's the value function.",
                    "label": 0
                },
                {
                    "sent": "Attained by GPS salsa, I think after about 200 episodes, so that's that's really not too much in reinforcement learning standards and.",
                    "label": 0
                },
                {
                    "sent": "And here is the greedy policy with respect to this value function and you can see that in most cases it does the right thing.",
                    "label": 0
                },
                {
                    "sent": "There are a few places where it doesn't do that here for instance here, but in most most places it if you follow the errors you reach the goal.",
                    "label": 0
                },
                {
                    "sent": "And the kernel functions function that I used here well for the actions it was a linear kernel.",
                    "label": 0
                },
                {
                    "sent": "So basically the kernel is a product of two kernels.",
                    "label": 0
                },
                {
                    "sent": "One is a kernel for action which is linear.",
                    "label": 0
                },
                {
                    "sent": "It's basically a dot product kernel and the kernel for states is the Gaussian kernel, and these artifacts here are due to the fact that the Gaussian kernel basically assumes that the functions in the hypothesis space are smooth and continuous.",
                    "label": 0
                },
                {
                    "sent": "So in borders like this.",
                    "label": 0
                },
                {
                    "sent": "It naturally has some trouble in modeling the value function.",
                    "label": 0
                },
                {
                    "sent": "Because here the value function is highly discontinuous.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. That's my last slide should be, so there are a few challenges left to be tackled.",
                    "label": 0
                },
                {
                    "sent": "One of them is how are we?",
                    "label": 0
                },
                {
                    "sent": "How are we going to use the value on certainty?",
                    "label": 0
                },
                {
                    "sent": "So one suggestion goes back to my answer to your question, which was to use interval estimation to update the policy.",
                    "label": 0
                },
                {
                    "sent": "But there there there, there could be many other different ideas and in what I described here, I didn't use any of these.",
                    "label": 0
                },
                {
                    "sent": "What's a disciplined way to select actions so Pascal talked quite in length about reconciling exploration and exploitation?",
                    "label": 1
                },
                {
                    "sent": "And this GPT architecture sort of provides a mechanism to do that for large reinforcement learning problems.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning problems with large state and action spaces.",
                    "label": 0
                },
                {
                    "sent": "What is the best noise covariance in what I showed you here, I propose two noise covariances, one of them for deterministic transitions and one of them for stochastic transitions, But the best one might be a third different noise covariance, and that's still an open question.",
                    "label": 1
                },
                {
                    "sent": "There are issues of bias, variance learning curves.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting to see if we could use GPT to solve partially observable Markov decision processes.",
                    "label": 0
                },
                {
                    "sent": "And of course, it would be nice to show how this Gaussian process architecture works on more complex tasks and at the end of this tutorial I'm going to show you demo on how we used GPT or other GP salsa to learn to control high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Simulated octopus arm.",
                    "label": 0
                },
                {
                    "sent": "And the yeah, that's basically questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You first you didn't ask.",
                    "label": 0
                },
                {
                    "sent": "Can you can you talk more loudly?",
                    "label": 0
                },
                {
                    "sent": "I can't hear.",
                    "label": 0
                },
                {
                    "sent": "Shape.",
                    "label": 0
                },
                {
                    "sent": "Are you talking the the Allman Oil paper?",
                    "label": 0
                },
                {
                    "sent": "Oh Taiwan's paper.",
                    "label": 0
                },
                {
                    "sent": "Yeah that that's.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, this is not a Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "This is a Gaussian process model.",
                    "label": 0
                },
                {
                    "sent": "These are two different things.",
                    "label": 0
                },
                {
                    "sent": "The the basis functions corresponding to the kernel that you choose.",
                    "label": 0
                },
                {
                    "sent": "I'm not necessarily Gaussians.",
                    "label": 0
                },
                {
                    "sent": "They can be just about anything.",
                    "label": 0
                },
                {
                    "sent": "Right there the basically the egg and vectors of the kernel in of the linear operator corresponding to the kernel to your choice of kernel.",
                    "label": 0
                },
                {
                    "sent": "So this is not limited to using RBF's.",
                    "label": 0
                },
                {
                    "sent": "For example, you can use just about any kernel that you can think of using GPT, and that also includes kernels that are used for things other than points in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So people have constructed kernels for trees for strings for all sorts of.",
                    "label": 0
                },
                {
                    "sent": "Wild things and you can use all of those within this framework.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there is a basic so I I showed the the sort of generative model that I assume holds for the value function and I.",
                    "label": 0
                },
                {
                    "sent": "In both cases, I hope I motivated it and made my assumptions explicit.",
                    "label": 0
                },
                {
                    "sent": "Can I come back?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Very simple case at the North Pole.",
                    "label": 0
                },
                {
                    "sent": "Well, in the bandit case.",
                    "label": 0
                },
                {
                    "sent": "It's the noise model will be exact right?",
                    "label": 0
                },
                {
                    "sent": "So you have only one state.",
                    "label": 0
                },
                {
                    "sent": "And your rewards are generated from a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Then the noise modeler is exact.",
                    "label": 0
                },
                {
                    "sent": "But that's a sort of extremely degenerate case, but The thing is that.",
                    "label": 0
                },
                {
                    "sent": "That they are well known results on on estimation results, for example, pertaining to the Kalman filter that basically you re derive the Kalman filter and not by making assumptions on the gaussianity of the variables but but trying to find the optimal linear estimator for your hidden variable.",
                    "label": 0
                },
                {
                    "sent": "And there you don't make any any Goshen Attia Sumption.",
                    "label": 0
                },
                {
                    "sent": "You just make assumptions about the meaning, the covariance of the variables in question.",
                    "label": 0
                },
                {
                    "sent": "And you can show that models very similar to that basically, linear Gaussian models can be derived as optimal minimum variance unbiased estimators, so that might answer any sort of issues concerning the gaussianity of the variables.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a critical component.",
                    "label": 0
                },
                {
                    "sent": "So noise comes from the randomness of the trajectories, and within each given trajectory the randomness of the rewards collected along their trajectory.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "If you know that you have a sort of shortest path kind of problem.",
                    "label": 0
                },
                {
                    "sent": "And you have a good guess that it might take you between.",
                    "label": 0
                },
                {
                    "sent": "I don't know 10 and 50.",
                    "label": 0
                },
                {
                    "sent": "Time steps to reach your goal.",
                    "label": 0
                },
                {
                    "sent": "Then that should give you a pretty good indication or in the noise or in the range of the discounted returns that are that you might get while following some policy, right?",
                    "label": 0
                },
                {
                    "sent": "You know it's going to be somewhere between what did I say 50 and I don't remember what then what were the numbers that I chose?",
                    "label": 0
                },
                {
                    "sent": "But let's say between 10 and 50, right?",
                    "label": 0
                },
                {
                    "sent": "So that would be something that you could use as a sort of guiding.",
                    "label": 0
                },
                {
                    "sent": "A rule of thumb to tune your your parameters, but you could do what people are doing all the time in machine learning and supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And that is just basically tuning parameters using cross validation, right?",
                    "label": 0
                },
                {
                    "sent": "This is what people do all the time in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "You could do the same here.",
                    "label": 0
                },
                {
                    "sent": "While question is not that critical.",
                    "label": 0
                },
                {
                    "sent": "The linear estimate only our minimum variance unbiased estimate noise.",
                    "label": 0
                },
                {
                    "sent": "Non Russian especially for common filter and so the derivation of the common producer.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Uncentered countering that actually not the translation.",
                    "label": 0
                },
                {
                    "sent": "But nothing seems to be working so well.",
                    "label": 1
                },
                {
                    "sent": "No, but if the noise is not Gaussian and you're restricting yourself a priority to linear estimators, then the best you can do, assuming you know the meaning, the covariance of all the variables in question.",
                    "label": 0
                },
                {
                    "sent": "The best you can do.",
                    "label": 0
                },
                {
                    "sent": "Among this class of linear estimators is the Kalman filter.",
                    "label": 0
                },
                {
                    "sent": "I'm not this is not a statement about general estimators.",
                    "label": 0
                },
                {
                    "sent": "This is a statement about linear estimators.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, I'll pass the podium to Mohammed.",
                    "label": 0
                }
            ]
        }
    }
}