{
    "id": "zz5q5ebgwe66fzqapkxncapifemhoeej",
    "title": "Graphical modelling and Bayesian structural learning",
    "info": {
        "author": [
            "Peter Green, Department of Mathematics, University of Bristol"
        ],
        "published": "Sept. 1, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/isba2016_green_graphical_modelling/",
    "segmentation": [
        [
            "My my first Isba experience was a rather negative one.",
            "I was in a cab in Turkey I think with Steve Feinberg and Alicia Cara query.",
            "And they were telling me about this new thing called isba.",
            "And I was very negative.",
            "I thought, well, we really don't need that.",
            "Bayesian statistics is already becoming completely mainstream.",
            "It's fully embedded in all sorts of general meetings.",
            "We really don't need to succeed.",
            "Ourselves away into special private meetings.",
            "I still think that to some extent, but obviously I couldn't have been more wrong about how valuable is Bruce been that growing this community and the huge number of people here at this conference is is testimony to that so well done to those early pioneers of the society for having the vision to to do this.",
            "Anne.",
            "Thanks to the organizers for giving me the honor of speaking in one of these lectures after they invited me, and after I accepted they then changed the time limit so I've got less time than I thought I would have originally, and you'll probably be glad of that, but.",
            "Inconsequence, I'm going to concentrate particularly on methods and how they work rather than show you applications.",
            "The contents is somewhat crowded, but the general structure don't bother to study."
        ],
        [
            "This in detail the general structure is I'm going to talk a little bit about the fundamental underpinnings of conditional independence and Markov properties.",
            "To introduce the subject of structural learning, which is the main point of this.",
            "And then talk about various classes of graphs and how we do structural learning in them.",
            "There's going to be quite an emphasis on decomposable models, but not exclusively, and I'll spend as much time on these last sections as I as I have when we when we get there."
        ],
        [
            "So thanks to pioneers of of statistics who are here.",
            "The conditional independence has become one of the absolutely central ideas of our discipline is key to understanding the structure of multivariate distributions and it's key to understanding the structure of samples of data.",
            "And in the early days this was something that was study, particularly in in relatively narrow areas of the discipline.",
            "But since become very very widespread, so to remind you the idea of conditional independence we have, we're talking bout 3 random quantities.",
            "XY and Z&X&Y are conditionally independent given zed if.",
            "If written in this form in Phil David's famous invented symbol.",
            "Exclusively independent of Y given said Mr.",
            "If you already know the value of zed.",
            "If that's background environmental information.",
            "If you like learning the value of Y tells you nothing more about X, and this means that any dependence between X&Y is indirect.",
            "It's mediated through the values of zip."
        ],
        [
            "And that's that's an idea that can be studied more abstractly.",
            "And Stephen Larison's book is wonderful representation of that of that formulation.",
            "But in probability terms, it's something quite concrete.",
            "It says that the joint distribution of X&Y gives their factorises into the conditional marginals.",
            "And what we've learned is that it becomes really useful to represent conditional independence in a graphical form.",
            "And I'm because I began in this within spatial statistics.",
            "My my pictures are somewhat spatial in character here.",
            "So one of the early."
        ],
        [
            "Origins of this idea was were in the attempts to extend to spatial processes, the ideas of Markov modeling in time series.",
            "So we have the idea of perhaps some spatially distributed collection of of entities, here linked by pathways.",
            "We're trying to think about how to model the yellow node given all the others, and by analogy with time series modeling in Markov chain modeling.",
            "In time series, we might think that we want to model that distribution of of the yellow given all the others as depending only on the red ones only on the neighbors in the graph.",
            "And that's something we now call the local Markov property.",
            "If a graph has that property, if whichever node you choose is the yellow node, you have that property, then the local Markov property holds.",
            "In"
        ],
        [
            "In physics sessions became aware that these ideas were around already an the idea of Gibbs distributions was was was present and the Gibbs distribution for our terms is a distribution of factorizes as products over cliques.",
            "So my talk, a clique is going to be a maximally connected sub graph of this blue graph.",
            "Subgraph is an example, here's another."
        ],
        [
            "A maximally completed subgraph.",
            "And if the distribution factorizes as a product of functions of the variables in those cliques, then we call that a Gibbs distribution.",
            "So there we have two 2 ideas, graphical property and a factorization property, and."
        ],
        [
            "One of the one of the first early results in this in this subject was the discovery that these were.",
            "Under conditions equivalent.",
            "And that's something that special statisticians called the Hammersley Clifford theorem."
        ],
        [
            "And with a little more time, a little more understanding, other kinds of Markov property were identified.",
            "These are all properties that are obviously true in a linear chain.",
            "Thinking of a Markov chain model.",
            "But that one idea is that if two variables are not adjacent in the graph.",
            "Then they are conditionally independent given all the others.",
            "So the graph in that picture now is a graph of those that are not conditionally independent given all others.",
            "And they're fine."
        ],
        [
            "The the global prop Markov property is a separation property.",
            "I need four colors now, and if you look at this picture, there's no way of going from blue to yellow except through red.",
            "And it say if we can state that whenever that happens we have yellow conditionally independent of blue.",
            "Given Red, then this global Markov property applies.",
            "So there's several ideas.",
            "There are three different graphical ideas and an algebraic idea.",
            "Anne."
        ],
        [
            "And the beautiful thing is, these are all deeply connected.",
            "So.",
            "We now have a more complete understanding of the connection between these properties in undirected graphs.",
            "Now graphs without errors on the edges.",
            "And very conveniently, in English and in some other languages, these are in alphabetical order.",
            "F implies G implies L implies P. And under additional conditions these are sufficient, but not necessary.",
            "Under additional conditions.",
            "These are actually all equivalent.",
            "So this is telling us something quite quite profound is telling us that the idea that conditional independence has a structure.",
            "It tells us that we can manipulate it graphically.",
            "We can perhaps build algorithms that are themselves graphical.",
            "And that we can link these properties to underlining factorization properties so that we can prove results in the algebraic domain as well as in the graphical one."
        ],
        [
            "And also I hope it's clear that the idea of conditional independence is absolutely key in in Bayesian statistics, 'cause it precisely describes what variables tell us about other variables.",
            "What data tells us about parameters conditional on other knowledge?",
            "So when I just talk about the conditional independence graph I'm talking about this graph defined by the pairwise Markov property.",
            "We draw edges between two vertices, vertices representing variables.",
            "We draw edges between two variables unless they are conditionally independent given all other variables."
        ],
        [
            "Now goes these undirected graphs are not the only kinds of graphs that are important in statistics, and the reasons of time.",
            "I'll only mention one other class and at another really important class.",
            "The class of directed acyclic graphs.",
            "Directed graphs are those where the edges are all arrows pointing in One Direction only.",
            "A directed graph in which there are no directed loops.",
            "We call a DAG.",
            "Equivalently, it's the.",
            "It's a graph where the edges can be numbered so that edges can only go from a low #2 to a high number."
        ],
        [
            "And there's a whole system of Markov properties, and conditional independence is that hold for these graphs as well.",
            "And again these can all be discovered fully in in Stephens book the Markov property in its local form for a directed graph."
        ],
        [
            "Says the following rather more complicated statement variables are independent of their non decendants given their parents.",
            "So my picture here.",
            "If we look at variable, the red one is non descendants of the two yellow nodes and if we and it is true if the directed local Markov property holes that red is conditionally independent of yellow given green.",
            "The factorization property, of course, is the property we always use when we're building models that the joint distribution of all variables is the product of the full of the conditional distributions or variables given their parents in the graph."
        ],
        [
            "So graphs happen, graphs are Kurth.",
            "All around statistics.",
            "I mean there's there's.",
            "650 people here.",
            "There are many many posters and talks and graphical ideas are going to be explicit or or sometimes implicit in a very large fraction of the presentations and posters that will see.",
            "Obviously they are important for visualization that obviously important for modeling.",
            "I'm today particularly talking about use of grass as outputs of inference, discovery or structure, and also of course they."
        ],
        [
            "Find a big role in in in building algorithms and.",
            "Many of the computations that we will be doing this week and talking about this week explicitly exploit the graphical structures and the conditional independence is that these graphs hold."
        ],
        [
            "Another key idea, and I hope we will see quite a lot about this in this conference as well.",
            "Of course we know that Association and causality are different, but.",
            "But drawing graphs can really be a key idea in helping us understand the connections and the and the possibilities for causal inference in statistics, and indeed during a graph immediately tells us why this is complicated.",
            "The possibility that we're trying to study the way a variable affects another risk factor for disease, for example, and the idea that there may be other variables, possibly unmeasured.",
            "This influence them both.",
            "As soon as you draw the picture you want.",
            "You understand the problem."
        ],
        [
            "But my talk is not about these things, it's about learning.",
            "It's about structural learning.",
            "And I hope I've sold sold the idea to you that the graph is a really key or the conditional independence is a really key idea in understanding what a what a set of data is telling us.",
            "And that drawing pictures could help us understand that.",
            "So we have the idea that given data, perhaps to keep it simple a an ID sample of data from some continuous story from motivate distribution.",
            "We may be interested in inferring this conditional independence graph, and that's the problem of structural learning in machine learning language.",
            "And it's the subject of the talk today.",
            "So why do we want to do that?",
            "Well, first of all, the graph may be of direct interest.",
            "It may be the real subject of inference if we try to construct pedigrees, for example.",
            "Of Of of human families.",
            "It's because we want to understand the pedigree.",
            "That's the genuine output.",
            "You could say the same thing about gene Networks, for example.",
            "We're really trying to understand what's going on.",
            "The graph really is a representation of the science.",
            "But it's not often that it's not always the objective we may be looking for parsimony.",
            "We all know that estimating something as simple as estimating a covariance matrix becomes really difficult as the dimension increases.",
            "Therapy variables.",
            "Then there are of order P squared covariances and the larger P gets, and these days it does get very large.",
            "It becomes extremely difficult to estimate something as simple as that.",
            "And of course, if we, if we know the dependencies, have a simple structure, we can, we can build that in and solve the problem, remove degrees of freedom so that we have a chance of of of estimating things stable.",
            "So that's an indirect use of a graph.",
            "And then also, of course, informally, it's an aid to understanding is an aid to appreciating what the data is telling us.",
            "A sort of exploratory data analysis.",
            "So this is a kind of model selection problem.",
            "But it can be a section problem on huge scale.",
            "Because if we have if we have vertices or variables.",
            "Then there are V. Choose two V * V -- 1 /, 2 possible edges that may or may not be present.",
            "Each of them may or may not be present in a graph.",
            "So we have two to the power.",
            "Viccis two possible models is a discrete space, but one that becomes huge as V increases."
        ],
        [
            "But people have been drawing graphs as outputs, results of inferences for over 30 years now.",
            "This is the one I often used as a opening example for 31 years ago.",
            "Well, this was the summary of a set of.",
            "Analysis On a 2 to the 6th Contingency Table.",
            "So six binary variables that happen to be prognostic factors for heart disease.",
            "And it was in havranek display this graph as a summary of the results of their conditional independence tests.",
            "Six variables there were quite.",
            "There were two to the.",
            "15 possible graphs, and this is the one they choose to to display."
        ],
        [
            "Much more recently stolen this graph from one of Stephen's presentations.",
            "This is a a.",
            "A graph in Ferd.",
            "From data representing snips.",
            "So the nodes are all snips here.",
            "And this is a forest collection of trees fitted to that data, using a minimum BICS criterion."
        ],
        [
            "The steel a slide from another person present at the conference.",
            "These were two analysis from financial data so S&P equity equity data.",
            "I've shown them to because it's in two examples here because it emphasizes that the graph.",
            "Which of course is an important.",
            "Structural, logical, structural, discrete structural representation of the dependencies is not independent of inference about parameters, and so under different distributional assumptions and different criteria.",
            "Different graphs may be inferred."
        ],
        [
            "And someday I've worked quite a lot with on these models is on these methods is Alan Thomas who's interested in genetic Epidemiology and this was a graph he produced some years ago.",
            "As a result of analyzing contingency table data relating to disease status to snips modelled dependently of course, covariates and some quantitative traits.",
            "So these are all visual representations of the results of analysis of data and are intended to convey to the human eye something about the pattern of dependence.",
            "So for which we can learn and start perhaps.",
            "At least to generate hypothesis for future future study."
        ],
        [
            "I think it's worth pausing just a moment to think in the light of those examples, and others think about what structural learning is really supposed to deliver.",
            "Very often we can't truthfully say there really is a graph.",
            "There's not a true graph that is sparse and simple, and if we only knew that, we'd really understand something.",
            "The absence of an edge in practice perhaps means more.",
            "Not conditional independence, but insignificant dependence, so I'd like to think of these.",
            "Sometimes there's more likes modeling spot, using sparsity assumptions in variable in variable selection.",
            "It's not that we truly believe there are any zero regression coefficients like life is often quite a bit more complicated than that, but that we choose to treat some of the beta zero will find out which ones are zero, and we'll build models accordingly."
        ],
        [
            "So this is a Bayesian meeting and I'm only going to talk about Bayesian methods for structural learning.",
            "They're not the only methods there are.",
            "There are other classes and methods, many, and indeed some ways that other other methods.",
            "Non Bayesian methods are ahead of us in terms of being able to deal with very big datasets.",
            "But I'm particularly going to talk about Bayesian methods, particularly among those those that really try to deliver posterior probabilities.",
            "So these are not necessarily methods that simply.",
            "Set up a objective function based on some.",
            "Model that might be interpreted as Bayesian and then optimize it.",
            "And that's not delivering the power of the Bayesian paradigm, and it's not allowing us to do various other things we would like to do from a fully Bayesian analysis.",
            "And although of course we'd like to do everything we like to the world.",
            "But typically going to restrict the spacecraft to to subset of possible graphs and those that have been particularly studied be things like trees and forests, Dags of course, and decomposable graphs.",
            "I'm going to spend a bit of time talking about the problems of structural learning in these different in these different situations.",
            "So."
        ],
        [
            "Let's talk about Decomposability first.",
            "This is being very much studied.",
            "It's it's a subject in graph theory.",
            "It's a.",
            "It's a pure mathematical idea.",
            "If you like this one that has profound statistical and computational implications.",
            "It's a funny word.",
            "I will see in a minute why it's got that name, why we talk about decomposability?",
            "But if this is new to you, it's probably a simpler to think of the definition Victoria Lee in terms of one of these two words triangulated or chordal graph is decomposable if and only if it has no cordless K cycles for K bigger than four so.",
            "A case cycle is what will be.",
            "It's a loop like this one here 1234.",
            "It's a cordless cycle is a loop like that one which has no shortcuts.",
            "There are no cords connecting intermediate vertices on that path, so the graph on the left is not decomposable because there is such a circuit or on the right is decomposable 'cause there is no such circuit.",
            "As you can see, it looks like a collection of triangles hits those two hits.",
            "Those two words.",
            "Such as."
        ],
        [
            "Cool idea.",
            "But one with very profound implications.",
            "Well, first of all, the graph is decomposable if and only if it has this rather.",
            "In directed representation in terms of what we call a junction tree.",
            "So junction trees are graphs, new graphs whose vertices are cliques.",
            "I've already told you cliques are maximal complete subgraphs.",
            "With the property that the edges, the cliques containing any prescribed set of vertices former connected subtree.",
            "Look at this picture here.",
            "Let's look look for some cliques 267.",
            "There's a clique.",
            "OK, they're all connected.",
            "267 There is a click.",
            "And so there are four clicks in all these things here.",
            "And I've connected them up in a tree.",
            "The tree is called a junction tree.",
            "And if you just mentally choose one of these seven vertices.",
            "Three for example, and look at where three lies in this picture.",
            "And you'll find that the threes are connected.",
            "And that doesn't just work for three.",
            "We label the links of the tree with with column separators there the intersections of the adjacent cliques 0267 and 236.",
            "The intersection is 26.",
            "And you can spot the two six as the boundary between two of the cliques in this picture here.",
            "And these objects.",
            "These junction trees forgiven.",
            "Decomposable graphs are not unique.",
            "There may be many of them."
        ],
        [
            "And the case that the real thing that particularly us makes the idea of.",
            "Decomposability so important is this rather beautiful factorization.",
            "So.",
            "We're now linking together distribution theory and graph, so if the distribution of a random vector has to decompose, decomposable conditional independence graph, then it's joint distribution P of all the variables, the joint full multivariate distribution factorizes as a product of low dimensional things.",
            "Here, low dimensional marginals, a product of all them.",
            "Click marginals divided by the predictable separator marginals.",
            "Right, an extraordinary extraordinary thing.",
            "It's the.",
            "If you've ever done first year probability, you've met it because you have met the idea of a Markov chain and.",
            "This is the defining property of a distribution of a Markov chain, and of course writing these conditional probabilities as as joints divided by marginals.",
            "We see that a factorization rather like that, and that's exactly what what we're seeing in this more general definition.",
            "Essentially, Decomposability allows you to take this Markov chain, like factorization, and spread it out through the branches of a junction tree.",
            "So decomposable graphs, joint distributions.",
            "Have very simple structure and that is the key to many many many many things."
        ],
        [
            "Mutational significance is that we can exploit that in algorithms for fitting models.",
            "For performing inference on parameters and so on.",
            "Giving dramatic speeds up, especially when the clicks are small."
        ],
        [
            "There's lots of important statistical significance is from this.",
            "I mean it's it's classical that.",
            "That you can do maximum likelihood estimation explicitly on contingency table models when the when the graphs are decomposable that you can do exact tests.",
            "We're going to see today that this gives dramatic speedups in structural learning, and for Bayesian something particularly beautiful.",
            "The idea that.",
            "It gives us an indecomposable graph.",
            "We have a very clear idea of what what are the natural priors on parameters to have in these in these graphs.",
            "Anne and.",
            "Giving us prizes are both philosophically attractive but also computationally tractable.",
            "So it gives us a framework for the construction of consistent prior distributions.",
            "Of course."
        ],
        [
            "If all graphs are decomposable, this would be wonderful.",
            "Unfortunately not in fact, very few graphs are decomposable.",
            "All small graphs are decomposable as soon as you have four vertices.",
            "You start to lose some Ann as the number of vertices increases, the proportion of of them a portion of the graph that are decomposable becomes.",
            "Tiny.",
            "Here the three smallest non compete non decomposable graphs.",
            "The 3 three ways of connecting four vertices to give you a a cordless cycle.",
            "Now."
        ],
        [
            "If we knew nature was decomposable, life would be beautiful, but it's not unlikely.",
            "The nature is kind enough to do that for us.",
            "Well, does this matter well?",
            "We can always given any.",
            "Non decomposable graph.",
            "We can always fill in edges so that it becomes decomposable.",
            "So imagine you're analyzing data and this was the real structural graph, and that's what you'd like to find, and you're going to be forced to find this one instead.",
            "Well, how much would that really matter?",
            "Providing our model allows arbitrarily small interactions, we may lose little by assuming decomposability.",
            "Well, that's hand WAVY and wishful thinking, but."
        ],
        [
            "It's been.",
            "Firmed up with a real theorem.",
            "Thanks to Fitch and Jones and Massam.",
            "Who showed that in?",
            "In the in the Gaussian case with particular priors.",
            "Something about what happens asymptotically.",
            "If you fit a decomposable model to a non decomposable distribution.",
            "The nicest first of all.",
            "The main statement is that the posterior converges to graphical structures that are minimal triangulations of the true graph.",
            "There was my wishful thinking picture just back here.",
            "Was the right one.",
            "If you if you perform Bayesian structural learning.",
            "Under those assumptions, when that's the true model, what you'll get is things like this.",
            "There may be several minimal triangulations amenable.",
            "Triangulation is is a triangulation which has the smallest number of edges needed to make it decomposable.",
            "There may be many such triangulations.",
            "They will typically all have positive posterior probability, even asymptotically.",
            "But if you're interested in.",
            "The graph simply is ensuring stability for covariance estimation.",
            "That doesn't matter because the resulting fitted covariance matrices are all a sensually the same."
        ],
        [
            "And there really are tremendous advantages in assuming decomposability, not just for computation and fitting it in the first place.",
            "The decomposable structure turns out to be key to all sorts of things you might do with the model, evaluate its fit, predicting from its sampling from it.",
            "The fitted decomposable model is something you can really use.",
            "A fitted non decomposable model is sometimes something that is hard to use as it was defined in the first place."
        ],
        [
            "So what's the setup for Bayesian graphical model determination then we have.",
            "We just talk about IID data here, but multi data distribution.",
            "Most of the unknowns here.",
            "There's an unknown distribution and which is governed by a graph describing its condition in dependencies and parameters.",
            "So the natural hierarchical model for this setup is that we're going to have a prior autographs aprior on parameters for each graph and a likelihood describing the data under that assumption.",
            "And the the the Bayesian problem of joint structural quantitative learning is about inferring that joint posterior distribution joint posterior distribution of the graph and the parameters.",
            "And of course we have then issues about.",
            "Our choice of prior an for the graph and our choice of parameter price is very commonly going to be the case just just to emphasize this, it's very commonly going to be the case that when you change the graph completely, different parameters will be needed.",
            "Typically, the denser the graph, the more parameters are involved."
        ],
        [
            "The subject of priors on graphs is A is a longstanding one.",
            "People have chosen made different choices.",
            "Many people simply ignore the issue and assume it to be uniform.",
            "More recent suggestion was to control.",
            "The prior on the size of the graph and then distribute that that probability equally among all the graphs with that size.",
            "That's that's number of edges.",
            "We might ask is there a?",
            "A more Canonical or conjugate approach than just these rather informal ideas.",
            "And."
        ],
        [
            "What might we be looking for?",
            "Reminding ourselves of this?",
            "Beautiful factorization of the likelihood essentially into a product of clique marginals over separated marginals.",
            "The kind of prior on the graph.",
            "That would make life simple, would be 1 where.",
            "The prior for the probability of G itself factorizes in the same kind of way.",
            "Anne."
        ],
        [
            "Well, one of the lovely example of the capacity of this area to generate beautiful results.",
            "Thanks to Simon Byrne and Phil David a few years ago.",
            "Who considered this idea?",
            "This idea of having a Markov property on on structure?",
            "And their assumption looks a bit abstract, but let's let's go through it.",
            "G is a graph, G. Sobeys.",
            "The graph restricted to the subset of vertices in A and similarly for B.",
            "And the graph is an unknown thing, so we're talking about distributions on that graph.",
            "And if G restricted way in JIRA 60 to be are conditionally independent given this.",
            "Then we say this structural Markov property holds.",
            "Now what is this condition here?",
            "This funny shaped you very embodies the set of decomposable graphs.",
            "We know what they are for, which into a B is a decomposition.",
            "So there's two new ideas there.",
            "There's the idea of a covering pair, and the idea of a decomposition a covering pair is just the ordinary math pure math idea of covering that is a baby's covering pair.",
            "If it's union is the whole set of vertices.",
            "This is a decomposition.",
            "If the intersection is complete and it's and separates.",
            "The rest of a from the rest of be.",
            "This picture tells the story.",
            "So in this particular picture, the A vertices of those in this rectangle, the bees are these.",
            "The intersection is that triangle drawn in green, and that's a decomposition because it's complete all the edges in that intersection are connected and it's separates the yellows and Blues."
        ],
        [
            "Because that's an idea about Markov Mr.",
            "The idea that.",
            "Given this.",
            "Is separates the remaining vertices.",
            "It says that we would like.",
            "Philosophically, we'd like our uncertainty about the edges over here and our uncertainty about the edges over here to be independent, and that's what this property demands.",
            "That will be all very well if.",
            "But you useless if he didn't really lead somewhere in the point is it does it leads to.",
            "Exactly the kind of structure for factorization structure we would like.",
            "They show that the graph toys structurally Markov if and only if it can be factorized in this way.",
            "And what these fires are, these fires are arbitrary positive set index parameters, so any prior which has that.",
            "Structural Markov property can be written in this way.",
            "And that's really quite flat."
        ],
        [
            "That's really quite powerful and all such.",
            "All such graph models have that conjugacy property I described.",
            "Compared to that, this is a very minor little extra bit, but if you slightly change the condition there, restrict to decomposable graphs for which.",
            "That intersection is a clique in.",
            "One side, let's say a remains a clique in GA. Then we're going to call that the weak structural Markov property.",
            "That places fewer conditional independence constraints on pie.",
            "OK, that's a.",
            "That's a smaller set of GS, therefore this is fewer condition independent statements.",
            "This this determines for us.",
            "So potentially it might lead to a richer class of graph priors, but.",
            "We will see in a moment that it's still a usable thing.",
            "So just to clarify what that is."
        ],
        [
            "Here are some pictures.",
            "The vertex set of these five red dots A is this the four on the left of square on the left, B is the triangle on the right.",
            "Missioning throughout on the AM, those two being connected 'cause we want it to be a decomposition and I'm also conditioning on the graph that the A intersection B remains a clique in GTA and that leaves us with 16 possibilities for GA and four possibilities for GB.",
            "Please dotted lines.",
            "And.",
            "So the weak structural Markov property would say that your choice among those near choice among those have to be independent."
        ],
        [
            "For every such AMB.",
            "And the remarkable thing is, this can still lead to a factorization property with slightly more general, because we can now have different functions on the denominator in the numerator.",
            "And that that extra flexibility have already been used in.",
            "In computational work, bye bye bye Born and Karen."
        ],
        [
            "So this this idea underlines what we saw at the beginning, where the graphical properties and the factorization properties coexist and have implications between them.",
            "A beautiful further episode in that story.",
            "And then you see the kind of conjugacy we would then get."
        ],
        [
            "Right which we want to talk about?",
            "Computing for trees.",
            "There are explicit.",
            "Explicit algorithms we can do perfect simulation for models on trees, and it will be a beautiful thing if you could use these strong, powerful properties of decomposability here to find perfect samplers for.",
            "For Bayesian decomposable graph models, but there's really no been no progress on that."
        ],
        [
            "So we're looking at MSMC again.",
            "And we do the usual thing we'll we'll construct a Markov chain.",
            "Top detail balance with respect to the posterior we want and we will will sample from it.",
            "Again, DK."
        ],
        [
            "Possibility helps us because we can.",
            "We can tell mathematically what kind of perturbations we can make to graphs and still keep them decomposable.",
            "So there's a quite an old result by by Steven Erikson and colleague, and even well, not quite so old result.",
            "That I'm partly responsible for.",
            "That tells us what edge moves can be made that maintain decomposability, 'cause if you want to conduct a Markov chain and running the space of decomposable graphs, you don't want to be jumping outside that space.",
            "And these these kind of moves, and that includes some multiple later moves, more certain kinds of multiple Asian moves that completely connect.",
            "Vertices that are not currently connected or completely disconnect those that are certain multiple Asian moves can also be characterized in this way.",
            "And you can efficiently represent those moves using a junction tree representation of the graph.",
            "These moves make any local changes to the junction tree, but they can change its topology locally as they do so."
        ],
        [
            "Now it turns out that that it turns out that that.",
            "That topology change is actually a bit of a performance quite damaging in performance terms, and we recently found a simple way to speed up the sampling quite dramatically by cutting out the need to change the topology, and we do that by a trick treating the junction tree itself as part of the model parameterisation.",
            "And that means augmenting the model so that we distribute the prior probability on a particular graph equally among the junction trees that represent that graph.",
            "And that is a useful idea, because we can, we can count the number of such graphs so we know what probabilities we're assigning.",
            "In the interest of time, I shall."
        ],
        [
            "Be forward a little bit, but this is sort of what we're getting here is.",
            "It's a perfect alignment of several nice things that give us computational performance.",
            "We got this pretesting for maintaining decomposability.",
            "We have simplification of likelihood ratios because of that clique.",
            "Separate separated factorization.",
            "We're going to use the junction tree as the state, and we could do some certain multiple edge moves and this allows effectively sampling models of of a moderate size.",
            "And they're not.",
            "It's not the only way to sample to sample decomposable graphs by by local changes, but it's one of the most explored.",
            "Let me just.",
            "Just a.",
            "Quick change of gear to show you that working.",
            "So here you're seeing a sampler running in real time, slowed down a bit by the need to to render the graphs.",
            "This is a model with 50 vertices.",
            "It's simulated Gaussian data.",
            "And.",
            "Think you'll begin to practice to see what the true model was.",
            "The the jittering you're seeing is partly just the randomization.",
            "Sorry, partly just the rendering but you can see quite a lot about the way this is sampling.",
            "From this picture you can see edges that come and go.",
            "Of course, the frequency with which they are present is estimating a positive posterior probability, and so on and so on, and we can of course evaluate joint distributions of all sorts of coexistence of edges.",
            "That the true graph was the was that sort of lattice structure that you can see there.",
            "So that's 50 vertices, and we'd be reasonably confident.",
            "Running"
        ],
        [
            "That kind of thing on somewhat larger situation.",
            "Let me say.",
            "I haven't thought about parameters here.",
            "In early work, people tended to use reversible jump type or other transformation or sampling to to jointly sample from this variable dimension posterior distribution.",
            "But in order to try and scale up to larger problems very early on, people who started to use conjugate hyper Markov priors for the parameters so that those could be without and what you get then is that metropolis ratio for updates to the graphs can be simplified into a locally computable form.",
            "So these so called hyper inverse wish ArtPrize have been used for various matrices and Gaussian models and Hyper Doris Day for the multinomial case, and there's been a lot of work on choice of hyperparameters in the in those in those priors.",
            "But they only go so far and it's still the true that we can only deal with quite moderate so moderate graphs here."
        ],
        [
            "So there's accumulated evidence that.",
            "MCMC using local moves is good at recovering low dimensional features such as edge inclusion, priors, we's inclusion probabilities.",
            "We can be fairly confident.",
            "In the estimated values of the posterior probability of a particular age being present, for example, or small probabilities of small combinations of particular edges.",
            "But it's these kind of models in larger graph support recovering global properties 'cause they're really severe mixing problems.",
            "And that motivates a powerful idea.",
            "Really, really beautiful idea of using.",
            "Using the edge inclusion probabilities estimated online to drive a stochastic search algorithm for, which is now an optimization method for trying to find high posterior probability graph.",
            "So we move beyond sampling now, but using sampling based methods in order to conduct our optimization.",
            "And the this thinks method feature inclusion stochastic search as proved very powerful at.",
            "Hitting high posterior probability graphs and it's good for prediction tasks.",
            "I think it's still true to say we don't really know enough about how well different approach approaches cover regions of high total probability.",
            "One of the key ideas in in in sampling these high dimensional graph spaces is of course that the map."
        ],
        [
            "Estimate the the highest for posterior probability graph.",
            "May or may not be representative of a region of graphs that are all of high probability.",
            "Let me move on to non decomposable graphs.",
            "Even if the graph is not decomposable, we still have a similar factorization.",
            "But the sets on the in the numerator now they called prime components.",
            "Their maximal subgraphs that can't be decomposed in the sense I've talked about.",
            "But they won't necessarily be complete.",
            "And."
        ],
        [
            "This was explored in the Cisco Science Paper about 10 years ago.",
            "And.",
            "The additional difficulties are that the normalizing constants in those non complete prime components don't have closed form, so you have to do Markov chain sampling within each step of the main chain in order to estimate them and those that those calculated values are very high variance.",
            "So when you make.",
            "Oh yeah, further when you make single edge perturbations to the graph, there is no guarantee of significant cancellation in the likelihood ratio.",
            "So these these difficulties really add on computer time.",
            "In that paper you can see them the time scalings are enormous.",
            "Still quite modest graphs."
        ],
        [
            "Their conclusion at the time was that.",
            "That posterior sampling was not practical for problems with more than about 15 nodes, and they resort to heuristics like stochastic shotgun search.",
            "Some of those difficulties have been ironed out later, and they need for MCMC and those normalizing constants has now been limited, but it's still the case that MCMC sampling for decomposable graphs non decomposable graphs is very very much harder.",
            "The sick."
        ],
        [
            "Styx shotgun search idea.",
            "Proceeds by starting with the graph, selecting graphs that differ by 1 edge from it computing there, not unnormalized posterior probabilities, retaining the top ones and then.",
            "Presuming proceeding from that among those neighbors take the I TH graph that you starting point, running little MCMC's and so on."
        ],
        [
            "So it's a heuristic.",
            "It works rather well and it's been improved and further explored in in other papers."
        ],
        [
            "But it is a method optimization, not a method of sampling.",
            "I think I'll skip the."
        ],
        [
            "Stration stolen from that from that paper.",
            "But another way to approach the non decomposable cases to remind ourselves, at least in the this, is in the Gaussian case.",
            "That old old conditional distributions in the multivariate Gaussian are of course Gaussian an linear.",
            "The mean has the mean as a linear structure.",
            "So that the problem of fitting a graphical Gaussian model is also the problem of fitting a lot of regressions.",
            "And you could simply do that job, do the fit multiple regressions for example by Bayesian methods, Bayesian sparse regression methods.",
            "You end up with a lot of conditional distributions of variables given others, and you need some way of.",
            "Piercing them together to deliver.",
            "A single inference.",
            "And there be various approaches to doing that.",
            "Combining separately fitted sparse regression models to form a graphical estimate."
        ],
        [
            "And.",
            "These have been applied to quite quite large datasets and.",
            "And appear interpretable, but the is a real.",
            "Yeah, that we've currently moved some way from the pure Bayesian paradigm of fitting the model.",
            "We originally thought of in this case."
        ],
        [
            "There's been a lot of work on non decomposable models.",
            "And there isn't time to go going to now.",
            "So we know a lot."
        ],
        [
            "More about fitting non decomposable models, but it's still the case that they are very much harder to fit the indecomposable ones.",
            "The.",
            "I'll say a little bit about treason, then I must wrap up.",
            "A tree is a connected, undirected graph with no loops."
        ],
        [
            "And it is not necessarily correct if we call it a forest.",
            "These"
        ],
        [
            "Themselves decomposable graphs.",
            "For a tree, the junction trees essentially isomorphic to the graph itself."
        ],
        [
            "Except for the.",
            "Ends the leaves at the end of the branches.",
            "So of course there's an intimate connection between junction tree algorithms and algorithms on trees.",
            "The in a tree."
        ],
        [
            "The likelihood will always factorize in this way.",
            "That's the decomposability condition, and that's essentially a product of edge probabilities divided by product of vertex probabilities."
        ],
        [
            "And so when we come to do posterior inference.",
            "The the the the posterior divided by the prior is essentially a product of Bayes factors for dependence along the edges.",
            "And that structure is something that's amenable to perfect simulation I mentioned earlier.",
            "Because we have powerful algorithms for random spanning trees that can exploit that kind of of.",
            "Of structure, and that's been extended to tree prize themselves decomposable unfortunate choice of words, but the fact rise again in this way is the product of weights of edges, and that works for treason.",
            "It works, but if you do a little bit more work, it works also for forests."
        ],
        [
            "I think I should have to skip the small section on Daggs.",
            "I'll just briefly mention what the difficulty is with tags.",
            "This is a situation where we wish to fit a DAG model to data that doesn't know anything about the directions."
        ],
        [
            "The pure form of this problem.",
            "Fitting tags is particularly important because of real or possibly imagined causal interpretations.",
            "And of course, there's a huge literature on Bayesian networks which.",
            "Which exploits these models.",
            "The complicating factor in.",
            "In fitting tags, is this problem of Markov?"
        ],
        [
            "Equivalence.",
            "Two different tags can imply the exactly the same set of conditional independence.",
            "Is is not true in a undirected graph, and it is true in a directed graph.",
            "So these two tags.",
            "So this this tag is, this is equivalent to this one in the sense that the set of conditional dependencies that are true in this one and this one are the same.",
            "This here again I've taken from one bank to another by changing the order a few of a few arrows.",
            "These are not equivalent in terms of the conditional independence is now.",
            "If you're analyzing data which doesn't know anything about the directions, you can only discover the conditional independencies.",
            "So so that.",
            "So the inference cannot distinguish those two.",
            "So kind of distinguish those two, but can distinguish these.",
            "Now, Fortunately there is a graphical simple.",
            "Easily understood test for Markov equivalence.",
            "Two Dag Samokov equivalent if they had the same skeleton, that is the same graph ignoring directions and the same sets of unmarried parents.",
            "The same sets of Immoralities as they sometimes called David's term.",
            "There is had a lot of life."
        ],
        [
            "And what this means is, if you want to conduct Bayesian inference using sampling methods on daggs, what you have to do is operate on on equivalence classes of the.",
            "That off.",
            "Equivalence classes under Markov equivalence."
        ],
        [
            "And this is very recent work and I'd love to see if this is going to be exploited in Bayesian computation.",
            "Very recent work on Markov chains.",
            "For completely tags.",
            "That are guaranteed to be reversible and therefore potentially usable."
        ],
        [
            "Right, just to wrap up quickly.",
            "Bayesian structural learning is one of these beautifully.",
            "Simply stated problems.",
            "What could be an easier problem to state?",
            "But it show is proven amazingly hard to deliver except on a modest scale.",
            "And that's true even with special choices of priors.",
            "When we first learned about MCMC, we thought, oh, now we're relieved we can.",
            "We can fit any model we like, but in order to make these methods work for even moderate sized graphs, we start having to make special assumptions.",
            "And there's been a huge amount of effort on on on on these problems.",
            "Some very creative people, many of whom are here.",
            "I mean, this is not.",
            "This is not a neglected area.",
            "As a result, we have a lot of.",
            "Now we have heuristics work a lot of relying on optimization rather than sampling.",
            "And to be honest, we don't really know what they're delivering.",
            "I mean, if a probability surface is hard to sample from is probably it's probably also true that finding an optimum find us something that's very unrepresentative.",
            "I'm wondering whether it's time now to give up if you like, but that's to refocus effort a bit.",
            "What we can estimate stable areas, inclusion probabilities and low dimensional marginals press the time is now come to really focus sampling methods on those, and on theoretical work that tries to give us some kind of guarantees.",
            "And finally I wondered whether.",
            "The problems are hard.",
            "While they obviously are hard because they're discrete, but where there's opportunities for relaxing the hard constraints and looking at weaker models."
        ],
        [
            "Thanks for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My my first Isba experience was a rather negative one.",
                    "label": 0
                },
                {
                    "sent": "I was in a cab in Turkey I think with Steve Feinberg and Alicia Cara query.",
                    "label": 0
                },
                {
                    "sent": "And they were telling me about this new thing called isba.",
                    "label": 0
                },
                {
                    "sent": "And I was very negative.",
                    "label": 0
                },
                {
                    "sent": "I thought, well, we really don't need that.",
                    "label": 0
                },
                {
                    "sent": "Bayesian statistics is already becoming completely mainstream.",
                    "label": 0
                },
                {
                    "sent": "It's fully embedded in all sorts of general meetings.",
                    "label": 0
                },
                {
                    "sent": "We really don't need to succeed.",
                    "label": 0
                },
                {
                    "sent": "Ourselves away into special private meetings.",
                    "label": 0
                },
                {
                    "sent": "I still think that to some extent, but obviously I couldn't have been more wrong about how valuable is Bruce been that growing this community and the huge number of people here at this conference is is testimony to that so well done to those early pioneers of the society for having the vision to to do this.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Thanks to the organizers for giving me the honor of speaking in one of these lectures after they invited me, and after I accepted they then changed the time limit so I've got less time than I thought I would have originally, and you'll probably be glad of that, but.",
                    "label": 0
                },
                {
                    "sent": "Inconsequence, I'm going to concentrate particularly on methods and how they work rather than show you applications.",
                    "label": 0
                },
                {
                    "sent": "The contents is somewhat crowded, but the general structure don't bother to study.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This in detail the general structure is I'm going to talk a little bit about the fundamental underpinnings of conditional independence and Markov properties.",
                    "label": 1
                },
                {
                    "sent": "To introduce the subject of structural learning, which is the main point of this.",
                    "label": 1
                },
                {
                    "sent": "And then talk about various classes of graphs and how we do structural learning in them.",
                    "label": 0
                },
                {
                    "sent": "There's going to be quite an emphasis on decomposable models, but not exclusively, and I'll spend as much time on these last sections as I as I have when we when we get there.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thanks to pioneers of of statistics who are here.",
                    "label": 0
                },
                {
                    "sent": "The conditional independence has become one of the absolutely central ideas of our discipline is key to understanding the structure of multivariate distributions and it's key to understanding the structure of samples of data.",
                    "label": 0
                },
                {
                    "sent": "And in the early days this was something that was study, particularly in in relatively narrow areas of the discipline.",
                    "label": 0
                },
                {
                    "sent": "But since become very very widespread, so to remind you the idea of conditional independence we have, we're talking bout 3 random quantities.",
                    "label": 0
                },
                {
                    "sent": "XY and Z&X&Y are conditionally independent given zed if.",
                    "label": 0
                },
                {
                    "sent": "If written in this form in Phil David's famous invented symbol.",
                    "label": 0
                },
                {
                    "sent": "Exclusively independent of Y given said Mr.",
                    "label": 0
                },
                {
                    "sent": "If you already know the value of zed.",
                    "label": 0
                },
                {
                    "sent": "If that's background environmental information.",
                    "label": 0
                },
                {
                    "sent": "If you like learning the value of Y tells you nothing more about X, and this means that any dependence between X&Y is indirect.",
                    "label": 1
                },
                {
                    "sent": "It's mediated through the values of zip.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's that's an idea that can be studied more abstractly.",
                    "label": 0
                },
                {
                    "sent": "And Stephen Larison's book is wonderful representation of that of that formulation.",
                    "label": 1
                },
                {
                    "sent": "But in probability terms, it's something quite concrete.",
                    "label": 0
                },
                {
                    "sent": "It says that the joint distribution of X&Y gives their factorises into the conditional marginals.",
                    "label": 0
                },
                {
                    "sent": "And what we've learned is that it becomes really useful to represent conditional independence in a graphical form.",
                    "label": 1
                },
                {
                    "sent": "And I'm because I began in this within spatial statistics.",
                    "label": 0
                },
                {
                    "sent": "My my pictures are somewhat spatial in character here.",
                    "label": 0
                },
                {
                    "sent": "So one of the early.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Origins of this idea was were in the attempts to extend to spatial processes, the ideas of Markov modeling in time series.",
                    "label": 0
                },
                {
                    "sent": "So we have the idea of perhaps some spatially distributed collection of of entities, here linked by pathways.",
                    "label": 0
                },
                {
                    "sent": "We're trying to think about how to model the yellow node given all the others, and by analogy with time series modeling in Markov chain modeling.",
                    "label": 0
                },
                {
                    "sent": "In time series, we might think that we want to model that distribution of of the yellow given all the others as depending only on the red ones only on the neighbors in the graph.",
                    "label": 0
                },
                {
                    "sent": "And that's something we now call the local Markov property.",
                    "label": 1
                },
                {
                    "sent": "If a graph has that property, if whichever node you choose is the yellow node, you have that property, then the local Markov property holds.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In physics sessions became aware that these ideas were around already an the idea of Gibbs distributions was was was present and the Gibbs distribution for our terms is a distribution of factorizes as products over cliques.",
                    "label": 0
                },
                {
                    "sent": "So my talk, a clique is going to be a maximally connected sub graph of this blue graph.",
                    "label": 0
                },
                {
                    "sent": "Subgraph is an example, here's another.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A maximally completed subgraph.",
                    "label": 0
                },
                {
                    "sent": "And if the distribution factorizes as a product of functions of the variables in those cliques, then we call that a Gibbs distribution.",
                    "label": 0
                },
                {
                    "sent": "So there we have two 2 ideas, graphical property and a factorization property, and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the one of the first early results in this in this subject was the discovery that these were.",
                    "label": 0
                },
                {
                    "sent": "Under conditions equivalent.",
                    "label": 0
                },
                {
                    "sent": "And that's something that special statisticians called the Hammersley Clifford theorem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And with a little more time, a little more understanding, other kinds of Markov property were identified.",
                    "label": 1
                },
                {
                    "sent": "These are all properties that are obviously true in a linear chain.",
                    "label": 0
                },
                {
                    "sent": "Thinking of a Markov chain model.",
                    "label": 0
                },
                {
                    "sent": "But that one idea is that if two variables are not adjacent in the graph.",
                    "label": 0
                },
                {
                    "sent": "Then they are conditionally independent given all the others.",
                    "label": 0
                },
                {
                    "sent": "So the graph in that picture now is a graph of those that are not conditionally independent given all others.",
                    "label": 0
                },
                {
                    "sent": "And they're fine.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The the global prop Markov property is a separation property.",
                    "label": 0
                },
                {
                    "sent": "I need four colors now, and if you look at this picture, there's no way of going from blue to yellow except through red.",
                    "label": 0
                },
                {
                    "sent": "And it say if we can state that whenever that happens we have yellow conditionally independent of blue.",
                    "label": 0
                },
                {
                    "sent": "Given Red, then this global Markov property applies.",
                    "label": 1
                },
                {
                    "sent": "So there's several ideas.",
                    "label": 0
                },
                {
                    "sent": "There are three different graphical ideas and an algebraic idea.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the beautiful thing is, these are all deeply connected.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We now have a more complete understanding of the connection between these properties in undirected graphs.",
                    "label": 1
                },
                {
                    "sent": "Now graphs without errors on the edges.",
                    "label": 0
                },
                {
                    "sent": "And very conveniently, in English and in some other languages, these are in alphabetical order.",
                    "label": 0
                },
                {
                    "sent": "F implies G implies L implies P. And under additional conditions these are sufficient, but not necessary.",
                    "label": 0
                },
                {
                    "sent": "Under additional conditions.",
                    "label": 1
                },
                {
                    "sent": "These are actually all equivalent.",
                    "label": 0
                },
                {
                    "sent": "So this is telling us something quite quite profound is telling us that the idea that conditional independence has a structure.",
                    "label": 0
                },
                {
                    "sent": "It tells us that we can manipulate it graphically.",
                    "label": 0
                },
                {
                    "sent": "We can perhaps build algorithms that are themselves graphical.",
                    "label": 0
                },
                {
                    "sent": "And that we can link these properties to underlining factorization properties so that we can prove results in the algebraic domain as well as in the graphical one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also I hope it's clear that the idea of conditional independence is absolutely key in in Bayesian statistics, 'cause it precisely describes what variables tell us about other variables.",
                    "label": 0
                },
                {
                    "sent": "What data tells us about parameters conditional on other knowledge?",
                    "label": 1
                },
                {
                    "sent": "So when I just talk about the conditional independence graph I'm talking about this graph defined by the pairwise Markov property.",
                    "label": 1
                },
                {
                    "sent": "We draw edges between two vertices, vertices representing variables.",
                    "label": 0
                },
                {
                    "sent": "We draw edges between two variables unless they are conditionally independent given all other variables.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now goes these undirected graphs are not the only kinds of graphs that are important in statistics, and the reasons of time.",
                    "label": 0
                },
                {
                    "sent": "I'll only mention one other class and at another really important class.",
                    "label": 0
                },
                {
                    "sent": "The class of directed acyclic graphs.",
                    "label": 1
                },
                {
                    "sent": "Directed graphs are those where the edges are all arrows pointing in One Direction only.",
                    "label": 0
                },
                {
                    "sent": "A directed graph in which there are no directed loops.",
                    "label": 1
                },
                {
                    "sent": "We call a DAG.",
                    "label": 1
                },
                {
                    "sent": "Equivalently, it's the.",
                    "label": 0
                },
                {
                    "sent": "It's a graph where the edges can be numbered so that edges can only go from a low #2 to a high number.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's a whole system of Markov properties, and conditional independence is that hold for these graphs as well.",
                    "label": 0
                },
                {
                    "sent": "And again these can all be discovered fully in in Stephens book the Markov property in its local form for a directed graph.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Says the following rather more complicated statement variables are independent of their non decendants given their parents.",
                    "label": 1
                },
                {
                    "sent": "So my picture here.",
                    "label": 1
                },
                {
                    "sent": "If we look at variable, the red one is non descendants of the two yellow nodes and if we and it is true if the directed local Markov property holes that red is conditionally independent of yellow given green.",
                    "label": 0
                },
                {
                    "sent": "The factorization property, of course, is the property we always use when we're building models that the joint distribution of all variables is the product of the full of the conditional distributions or variables given their parents in the graph.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So graphs happen, graphs are Kurth.",
                    "label": 0
                },
                {
                    "sent": "All around statistics.",
                    "label": 0
                },
                {
                    "sent": "I mean there's there's.",
                    "label": 0
                },
                {
                    "sent": "650 people here.",
                    "label": 0
                },
                {
                    "sent": "There are many many posters and talks and graphical ideas are going to be explicit or or sometimes implicit in a very large fraction of the presentations and posters that will see.",
                    "label": 0
                },
                {
                    "sent": "Obviously they are important for visualization that obviously important for modeling.",
                    "label": 0
                },
                {
                    "sent": "I'm today particularly talking about use of grass as outputs of inference, discovery or structure, and also of course they.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find a big role in in in building algorithms and.",
                    "label": 0
                },
                {
                    "sent": "Many of the computations that we will be doing this week and talking about this week explicitly exploit the graphical structures and the conditional independence is that these graphs hold.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another key idea, and I hope we will see quite a lot about this in this conference as well.",
                    "label": 0
                },
                {
                    "sent": "Of course we know that Association and causality are different, but.",
                    "label": 1
                },
                {
                    "sent": "But drawing graphs can really be a key idea in helping us understand the connections and the and the possibilities for causal inference in statistics, and indeed during a graph immediately tells us why this is complicated.",
                    "label": 0
                },
                {
                    "sent": "The possibility that we're trying to study the way a variable affects another risk factor for disease, for example, and the idea that there may be other variables, possibly unmeasured.",
                    "label": 0
                },
                {
                    "sent": "This influence them both.",
                    "label": 0
                },
                {
                    "sent": "As soon as you draw the picture you want.",
                    "label": 0
                },
                {
                    "sent": "You understand the problem.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But my talk is not about these things, it's about learning.",
                    "label": 0
                },
                {
                    "sent": "It's about structural learning.",
                    "label": 0
                },
                {
                    "sent": "And I hope I've sold sold the idea to you that the graph is a really key or the conditional independence is a really key idea in understanding what a what a set of data is telling us.",
                    "label": 0
                },
                {
                    "sent": "And that drawing pictures could help us understand that.",
                    "label": 0
                },
                {
                    "sent": "So we have the idea that given data, perhaps to keep it simple a an ID sample of data from some continuous story from motivate distribution.",
                    "label": 0
                },
                {
                    "sent": "We may be interested in inferring this conditional independence graph, and that's the problem of structural learning in machine learning language.",
                    "label": 1
                },
                {
                    "sent": "And it's the subject of the talk today.",
                    "label": 1
                },
                {
                    "sent": "So why do we want to do that?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, the graph may be of direct interest.",
                    "label": 0
                },
                {
                    "sent": "It may be the real subject of inference if we try to construct pedigrees, for example.",
                    "label": 0
                },
                {
                    "sent": "Of Of of human families.",
                    "label": 0
                },
                {
                    "sent": "It's because we want to understand the pedigree.",
                    "label": 0
                },
                {
                    "sent": "That's the genuine output.",
                    "label": 0
                },
                {
                    "sent": "You could say the same thing about gene Networks, for example.",
                    "label": 0
                },
                {
                    "sent": "We're really trying to understand what's going on.",
                    "label": 0
                },
                {
                    "sent": "The graph really is a representation of the science.",
                    "label": 0
                },
                {
                    "sent": "But it's not often that it's not always the objective we may be looking for parsimony.",
                    "label": 0
                },
                {
                    "sent": "We all know that estimating something as simple as estimating a covariance matrix becomes really difficult as the dimension increases.",
                    "label": 0
                },
                {
                    "sent": "Therapy variables.",
                    "label": 0
                },
                {
                    "sent": "Then there are of order P squared covariances and the larger P gets, and these days it does get very large.",
                    "label": 0
                },
                {
                    "sent": "It becomes extremely difficult to estimate something as simple as that.",
                    "label": 0
                },
                {
                    "sent": "And of course, if we, if we know the dependencies, have a simple structure, we can, we can build that in and solve the problem, remove degrees of freedom so that we have a chance of of of estimating things stable.",
                    "label": 0
                },
                {
                    "sent": "So that's an indirect use of a graph.",
                    "label": 0
                },
                {
                    "sent": "And then also, of course, informally, it's an aid to understanding is an aid to appreciating what the data is telling us.",
                    "label": 1
                },
                {
                    "sent": "A sort of exploratory data analysis.",
                    "label": 0
                },
                {
                    "sent": "So this is a kind of model selection problem.",
                    "label": 0
                },
                {
                    "sent": "But it can be a section problem on huge scale.",
                    "label": 0
                },
                {
                    "sent": "Because if we have if we have vertices or variables.",
                    "label": 0
                },
                {
                    "sent": "Then there are V. Choose two V * V -- 1 /, 2 possible edges that may or may not be present.",
                    "label": 0
                },
                {
                    "sent": "Each of them may or may not be present in a graph.",
                    "label": 0
                },
                {
                    "sent": "So we have two to the power.",
                    "label": 0
                },
                {
                    "sent": "Viccis two possible models is a discrete space, but one that becomes huge as V increases.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But people have been drawing graphs as outputs, results of inferences for over 30 years now.",
                    "label": 0
                },
                {
                    "sent": "This is the one I often used as a opening example for 31 years ago.",
                    "label": 0
                },
                {
                    "sent": "Well, this was the summary of a set of.",
                    "label": 0
                },
                {
                    "sent": "Analysis On a 2 to the 6th Contingency Table.",
                    "label": 1
                },
                {
                    "sent": "So six binary variables that happen to be prognostic factors for heart disease.",
                    "label": 1
                },
                {
                    "sent": "And it was in havranek display this graph as a summary of the results of their conditional independence tests.",
                    "label": 0
                },
                {
                    "sent": "Six variables there were quite.",
                    "label": 0
                },
                {
                    "sent": "There were two to the.",
                    "label": 0
                },
                {
                    "sent": "15 possible graphs, and this is the one they choose to to display.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much more recently stolen this graph from one of Stephen's presentations.",
                    "label": 0
                },
                {
                    "sent": "This is a a.",
                    "label": 0
                },
                {
                    "sent": "A graph in Ferd.",
                    "label": 0
                },
                {
                    "sent": "From data representing snips.",
                    "label": 0
                },
                {
                    "sent": "So the nodes are all snips here.",
                    "label": 0
                },
                {
                    "sent": "And this is a forest collection of trees fitted to that data, using a minimum BICS criterion.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The steel a slide from another person present at the conference.",
                    "label": 0
                },
                {
                    "sent": "These were two analysis from financial data so S&P equity equity data.",
                    "label": 1
                },
                {
                    "sent": "I've shown them to because it's in two examples here because it emphasizes that the graph.",
                    "label": 0
                },
                {
                    "sent": "Which of course is an important.",
                    "label": 0
                },
                {
                    "sent": "Structural, logical, structural, discrete structural representation of the dependencies is not independent of inference about parameters, and so under different distributional assumptions and different criteria.",
                    "label": 0
                },
                {
                    "sent": "Different graphs may be inferred.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And someday I've worked quite a lot with on these models is on these methods is Alan Thomas who's interested in genetic Epidemiology and this was a graph he produced some years ago.",
                    "label": 0
                },
                {
                    "sent": "As a result of analyzing contingency table data relating to disease status to snips modelled dependently of course, covariates and some quantitative traits.",
                    "label": 1
                },
                {
                    "sent": "So these are all visual representations of the results of analysis of data and are intended to convey to the human eye something about the pattern of dependence.",
                    "label": 0
                },
                {
                    "sent": "So for which we can learn and start perhaps.",
                    "label": 0
                },
                {
                    "sent": "At least to generate hypothesis for future future study.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think it's worth pausing just a moment to think in the light of those examples, and others think about what structural learning is really supposed to deliver.",
                    "label": 1
                },
                {
                    "sent": "Very often we can't truthfully say there really is a graph.",
                    "label": 0
                },
                {
                    "sent": "There's not a true graph that is sparse and simple, and if we only knew that, we'd really understand something.",
                    "label": 1
                },
                {
                    "sent": "The absence of an edge in practice perhaps means more.",
                    "label": 1
                },
                {
                    "sent": "Not conditional independence, but insignificant dependence, so I'd like to think of these.",
                    "label": 0
                },
                {
                    "sent": "Sometimes there's more likes modeling spot, using sparsity assumptions in variable in variable selection.",
                    "label": 0
                },
                {
                    "sent": "It's not that we truly believe there are any zero regression coefficients like life is often quite a bit more complicated than that, but that we choose to treat some of the beta zero will find out which ones are zero, and we'll build models accordingly.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a Bayesian meeting and I'm only going to talk about Bayesian methods for structural learning.",
                    "label": 1
                },
                {
                    "sent": "They're not the only methods there are.",
                    "label": 0
                },
                {
                    "sent": "There are other classes and methods, many, and indeed some ways that other other methods.",
                    "label": 0
                },
                {
                    "sent": "Non Bayesian methods are ahead of us in terms of being able to deal with very big datasets.",
                    "label": 1
                },
                {
                    "sent": "But I'm particularly going to talk about Bayesian methods, particularly among those those that really try to deliver posterior probabilities.",
                    "label": 0
                },
                {
                    "sent": "So these are not necessarily methods that simply.",
                    "label": 0
                },
                {
                    "sent": "Set up a objective function based on some.",
                    "label": 0
                },
                {
                    "sent": "Model that might be interpreted as Bayesian and then optimize it.",
                    "label": 0
                },
                {
                    "sent": "And that's not delivering the power of the Bayesian paradigm, and it's not allowing us to do various other things we would like to do from a fully Bayesian analysis.",
                    "label": 0
                },
                {
                    "sent": "And although of course we'd like to do everything we like to the world.",
                    "label": 0
                },
                {
                    "sent": "But typically going to restrict the spacecraft to to subset of possible graphs and those that have been particularly studied be things like trees and forests, Dags of course, and decomposable graphs.",
                    "label": 1
                },
                {
                    "sent": "I'm going to spend a bit of time talking about the problems of structural learning in these different in these different situations.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's talk about Decomposability first.",
                    "label": 0
                },
                {
                    "sent": "This is being very much studied.",
                    "label": 1
                },
                {
                    "sent": "It's it's a subject in graph theory.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's a pure mathematical idea.",
                    "label": 0
                },
                {
                    "sent": "If you like this one that has profound statistical and computational implications.",
                    "label": 1
                },
                {
                    "sent": "It's a funny word.",
                    "label": 0
                },
                {
                    "sent": "I will see in a minute why it's got that name, why we talk about decomposability?",
                    "label": 0
                },
                {
                    "sent": "But if this is new to you, it's probably a simpler to think of the definition Victoria Lee in terms of one of these two words triangulated or chordal graph is decomposable if and only if it has no cordless K cycles for K bigger than four so.",
                    "label": 1
                },
                {
                    "sent": "A case cycle is what will be.",
                    "label": 0
                },
                {
                    "sent": "It's a loop like this one here 1234.",
                    "label": 0
                },
                {
                    "sent": "It's a cordless cycle is a loop like that one which has no shortcuts.",
                    "label": 0
                },
                {
                    "sent": "There are no cords connecting intermediate vertices on that path, so the graph on the left is not decomposable because there is such a circuit or on the right is decomposable 'cause there is no such circuit.",
                    "label": 0
                },
                {
                    "sent": "As you can see, it looks like a collection of triangles hits those two hits.",
                    "label": 0
                },
                {
                    "sent": "Those two words.",
                    "label": 0
                },
                {
                    "sent": "Such as.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cool idea.",
                    "label": 0
                },
                {
                    "sent": "But one with very profound implications.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, the graph is decomposable if and only if it has this rather.",
                    "label": 1
                },
                {
                    "sent": "In directed representation in terms of what we call a junction tree.",
                    "label": 1
                },
                {
                    "sent": "So junction trees are graphs, new graphs whose vertices are cliques.",
                    "label": 0
                },
                {
                    "sent": "I've already told you cliques are maximal complete subgraphs.",
                    "label": 0
                },
                {
                    "sent": "With the property that the edges, the cliques containing any prescribed set of vertices former connected subtree.",
                    "label": 1
                },
                {
                    "sent": "Look at this picture here.",
                    "label": 0
                },
                {
                    "sent": "Let's look look for some cliques 267.",
                    "label": 0
                },
                {
                    "sent": "There's a clique.",
                    "label": 0
                },
                {
                    "sent": "OK, they're all connected.",
                    "label": 0
                },
                {
                    "sent": "267 There is a click.",
                    "label": 0
                },
                {
                    "sent": "And so there are four clicks in all these things here.",
                    "label": 0
                },
                {
                    "sent": "And I've connected them up in a tree.",
                    "label": 0
                },
                {
                    "sent": "The tree is called a junction tree.",
                    "label": 0
                },
                {
                    "sent": "And if you just mentally choose one of these seven vertices.",
                    "label": 0
                },
                {
                    "sent": "Three for example, and look at where three lies in this picture.",
                    "label": 0
                },
                {
                    "sent": "And you'll find that the threes are connected.",
                    "label": 0
                },
                {
                    "sent": "And that doesn't just work for three.",
                    "label": 1
                },
                {
                    "sent": "We label the links of the tree with with column separators there the intersections of the adjacent cliques 0267 and 236.",
                    "label": 0
                },
                {
                    "sent": "The intersection is 26.",
                    "label": 0
                },
                {
                    "sent": "And you can spot the two six as the boundary between two of the cliques in this picture here.",
                    "label": 1
                },
                {
                    "sent": "And these objects.",
                    "label": 0
                },
                {
                    "sent": "These junction trees forgiven.",
                    "label": 0
                },
                {
                    "sent": "Decomposable graphs are not unique.",
                    "label": 0
                },
                {
                    "sent": "There may be many of them.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the case that the real thing that particularly us makes the idea of.",
                    "label": 0
                },
                {
                    "sent": "Decomposability so important is this rather beautiful factorization.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're now linking together distribution theory and graph, so if the distribution of a random vector has to decompose, decomposable conditional independence graph, then it's joint distribution P of all the variables, the joint full multivariate distribution factorizes as a product of low dimensional things.",
                    "label": 1
                },
                {
                    "sent": "Here, low dimensional marginals, a product of all them.",
                    "label": 0
                },
                {
                    "sent": "Click marginals divided by the predictable separator marginals.",
                    "label": 0
                },
                {
                    "sent": "Right, an extraordinary extraordinary thing.",
                    "label": 0
                },
                {
                    "sent": "It's the.",
                    "label": 0
                },
                {
                    "sent": "If you've ever done first year probability, you've met it because you have met the idea of a Markov chain and.",
                    "label": 0
                },
                {
                    "sent": "This is the defining property of a distribution of a Markov chain, and of course writing these conditional probabilities as as joints divided by marginals.",
                    "label": 0
                },
                {
                    "sent": "We see that a factorization rather like that, and that's exactly what what we're seeing in this more general definition.",
                    "label": 1
                },
                {
                    "sent": "Essentially, Decomposability allows you to take this Markov chain, like factorization, and spread it out through the branches of a junction tree.",
                    "label": 0
                },
                {
                    "sent": "So decomposable graphs, joint distributions.",
                    "label": 0
                },
                {
                    "sent": "Have very simple structure and that is the key to many many many many things.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mutational significance is that we can exploit that in algorithms for fitting models.",
                    "label": 0
                },
                {
                    "sent": "For performing inference on parameters and so on.",
                    "label": 0
                },
                {
                    "sent": "Giving dramatic speeds up, especially when the clicks are small.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's lots of important statistical significance is from this.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's classical that.",
                    "label": 0
                },
                {
                    "sent": "That you can do maximum likelihood estimation explicitly on contingency table models when the when the graphs are decomposable that you can do exact tests.",
                    "label": 0
                },
                {
                    "sent": "We're going to see today that this gives dramatic speedups in structural learning, and for Bayesian something particularly beautiful.",
                    "label": 1
                },
                {
                    "sent": "The idea that.",
                    "label": 0
                },
                {
                    "sent": "It gives us an indecomposable graph.",
                    "label": 0
                },
                {
                    "sent": "We have a very clear idea of what what are the natural priors on parameters to have in these in these graphs.",
                    "label": 0
                },
                {
                    "sent": "Anne and.",
                    "label": 0
                },
                {
                    "sent": "Giving us prizes are both philosophically attractive but also computationally tractable.",
                    "label": 0
                },
                {
                    "sent": "So it gives us a framework for the construction of consistent prior distributions.",
                    "label": 1
                },
                {
                    "sent": "Of course.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If all graphs are decomposable, this would be wonderful.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately not in fact, very few graphs are decomposable.",
                    "label": 1
                },
                {
                    "sent": "All small graphs are decomposable as soon as you have four vertices.",
                    "label": 0
                },
                {
                    "sent": "You start to lose some Ann as the number of vertices increases, the proportion of of them a portion of the graph that are decomposable becomes.",
                    "label": 1
                },
                {
                    "sent": "Tiny.",
                    "label": 0
                },
                {
                    "sent": "Here the three smallest non compete non decomposable graphs.",
                    "label": 0
                },
                {
                    "sent": "The 3 three ways of connecting four vertices to give you a a cordless cycle.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we knew nature was decomposable, life would be beautiful, but it's not unlikely.",
                    "label": 0
                },
                {
                    "sent": "The nature is kind enough to do that for us.",
                    "label": 0
                },
                {
                    "sent": "Well, does this matter well?",
                    "label": 0
                },
                {
                    "sent": "We can always given any.",
                    "label": 0
                },
                {
                    "sent": "Non decomposable graph.",
                    "label": 0
                },
                {
                    "sent": "We can always fill in edges so that it becomes decomposable.",
                    "label": 1
                },
                {
                    "sent": "So imagine you're analyzing data and this was the real structural graph, and that's what you'd like to find, and you're going to be forced to find this one instead.",
                    "label": 0
                },
                {
                    "sent": "Well, how much would that really matter?",
                    "label": 0
                },
                {
                    "sent": "Providing our model allows arbitrarily small interactions, we may lose little by assuming decomposability.",
                    "label": 1
                },
                {
                    "sent": "Well, that's hand WAVY and wishful thinking, but.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's been.",
                    "label": 0
                },
                {
                    "sent": "Firmed up with a real theorem.",
                    "label": 0
                },
                {
                    "sent": "Thanks to Fitch and Jones and Massam.",
                    "label": 1
                },
                {
                    "sent": "Who showed that in?",
                    "label": 0
                },
                {
                    "sent": "In the in the Gaussian case with particular priors.",
                    "label": 0
                },
                {
                    "sent": "Something about what happens asymptotically.",
                    "label": 0
                },
                {
                    "sent": "If you fit a decomposable model to a non decomposable distribution.",
                    "label": 0
                },
                {
                    "sent": "The nicest first of all.",
                    "label": 0
                },
                {
                    "sent": "The main statement is that the posterior converges to graphical structures that are minimal triangulations of the true graph.",
                    "label": 1
                },
                {
                    "sent": "There was my wishful thinking picture just back here.",
                    "label": 0
                },
                {
                    "sent": "Was the right one.",
                    "label": 0
                },
                {
                    "sent": "If you if you perform Bayesian structural learning.",
                    "label": 0
                },
                {
                    "sent": "Under those assumptions, when that's the true model, what you'll get is things like this.",
                    "label": 0
                },
                {
                    "sent": "There may be several minimal triangulations amenable.",
                    "label": 0
                },
                {
                    "sent": "Triangulation is is a triangulation which has the smallest number of edges needed to make it decomposable.",
                    "label": 0
                },
                {
                    "sent": "There may be many such triangulations.",
                    "label": 0
                },
                {
                    "sent": "They will typically all have positive posterior probability, even asymptotically.",
                    "label": 0
                },
                {
                    "sent": "But if you're interested in.",
                    "label": 0
                },
                {
                    "sent": "The graph simply is ensuring stability for covariance estimation.",
                    "label": 0
                },
                {
                    "sent": "That doesn't matter because the resulting fitted covariance matrices are all a sensually the same.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there really are tremendous advantages in assuming decomposability, not just for computation and fitting it in the first place.",
                    "label": 1
                },
                {
                    "sent": "The decomposable structure turns out to be key to all sorts of things you might do with the model, evaluate its fit, predicting from its sampling from it.",
                    "label": 0
                },
                {
                    "sent": "The fitted decomposable model is something you can really use.",
                    "label": 0
                },
                {
                    "sent": "A fitted non decomposable model is sometimes something that is hard to use as it was defined in the first place.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the setup for Bayesian graphical model determination then we have.",
                    "label": 1
                },
                {
                    "sent": "We just talk about IID data here, but multi data distribution.",
                    "label": 0
                },
                {
                    "sent": "Most of the unknowns here.",
                    "label": 0
                },
                {
                    "sent": "There's an unknown distribution and which is governed by a graph describing its condition in dependencies and parameters.",
                    "label": 0
                },
                {
                    "sent": "So the natural hierarchical model for this setup is that we're going to have a prior autographs aprior on parameters for each graph and a likelihood describing the data under that assumption.",
                    "label": 0
                },
                {
                    "sent": "And the the the Bayesian problem of joint structural quantitative learning is about inferring that joint posterior distribution joint posterior distribution of the graph and the parameters.",
                    "label": 0
                },
                {
                    "sent": "And of course we have then issues about.",
                    "label": 0
                },
                {
                    "sent": "Our choice of prior an for the graph and our choice of parameter price is very commonly going to be the case just just to emphasize this, it's very commonly going to be the case that when you change the graph completely, different parameters will be needed.",
                    "label": 0
                },
                {
                    "sent": "Typically, the denser the graph, the more parameters are involved.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The subject of priors on graphs is A is a longstanding one.",
                    "label": 1
                },
                {
                    "sent": "People have chosen made different choices.",
                    "label": 0
                },
                {
                    "sent": "Many people simply ignore the issue and assume it to be uniform.",
                    "label": 0
                },
                {
                    "sent": "More recent suggestion was to control.",
                    "label": 0
                },
                {
                    "sent": "The prior on the size of the graph and then distribute that that probability equally among all the graphs with that size.",
                    "label": 1
                },
                {
                    "sent": "That's that's number of edges.",
                    "label": 1
                },
                {
                    "sent": "We might ask is there a?",
                    "label": 0
                },
                {
                    "sent": "A more Canonical or conjugate approach than just these rather informal ideas.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What might we be looking for?",
                    "label": 0
                },
                {
                    "sent": "Reminding ourselves of this?",
                    "label": 0
                },
                {
                    "sent": "Beautiful factorization of the likelihood essentially into a product of clique marginals over separated marginals.",
                    "label": 1
                },
                {
                    "sent": "The kind of prior on the graph.",
                    "label": 1
                },
                {
                    "sent": "That would make life simple, would be 1 where.",
                    "label": 0
                },
                {
                    "sent": "The prior for the probability of G itself factorizes in the same kind of way.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, one of the lovely example of the capacity of this area to generate beautiful results.",
                    "label": 0
                },
                {
                    "sent": "Thanks to Simon Byrne and Phil David a few years ago.",
                    "label": 0
                },
                {
                    "sent": "Who considered this idea?",
                    "label": 0
                },
                {
                    "sent": "This idea of having a Markov property on on structure?",
                    "label": 0
                },
                {
                    "sent": "And their assumption looks a bit abstract, but let's let's go through it.",
                    "label": 0
                },
                {
                    "sent": "G is a graph, G. Sobeys.",
                    "label": 1
                },
                {
                    "sent": "The graph restricted to the subset of vertices in A and similarly for B.",
                    "label": 0
                },
                {
                    "sent": "And the graph is an unknown thing, so we're talking about distributions on that graph.",
                    "label": 0
                },
                {
                    "sent": "And if G restricted way in JIRA 60 to be are conditionally independent given this.",
                    "label": 1
                },
                {
                    "sent": "Then we say this structural Markov property holds.",
                    "label": 0
                },
                {
                    "sent": "Now what is this condition here?",
                    "label": 0
                },
                {
                    "sent": "This funny shaped you very embodies the set of decomposable graphs.",
                    "label": 1
                },
                {
                    "sent": "We know what they are for, which into a B is a decomposition.",
                    "label": 1
                },
                {
                    "sent": "So there's two new ideas there.",
                    "label": 0
                },
                {
                    "sent": "There's the idea of a covering pair, and the idea of a decomposition a covering pair is just the ordinary math pure math idea of covering that is a baby's covering pair.",
                    "label": 1
                },
                {
                    "sent": "If it's union is the whole set of vertices.",
                    "label": 1
                },
                {
                    "sent": "This is a decomposition.",
                    "label": 0
                },
                {
                    "sent": "If the intersection is complete and it's and separates.",
                    "label": 0
                },
                {
                    "sent": "The rest of a from the rest of be.",
                    "label": 0
                },
                {
                    "sent": "This picture tells the story.",
                    "label": 0
                },
                {
                    "sent": "So in this particular picture, the A vertices of those in this rectangle, the bees are these.",
                    "label": 0
                },
                {
                    "sent": "The intersection is that triangle drawn in green, and that's a decomposition because it's complete all the edges in that intersection are connected and it's separates the yellows and Blues.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because that's an idea about Markov Mr.",
                    "label": 0
                },
                {
                    "sent": "The idea that.",
                    "label": 0
                },
                {
                    "sent": "Given this.",
                    "label": 0
                },
                {
                    "sent": "Is separates the remaining vertices.",
                    "label": 0
                },
                {
                    "sent": "It says that we would like.",
                    "label": 0
                },
                {
                    "sent": "Philosophically, we'd like our uncertainty about the edges over here and our uncertainty about the edges over here to be independent, and that's what this property demands.",
                    "label": 0
                },
                {
                    "sent": "That will be all very well if.",
                    "label": 0
                },
                {
                    "sent": "But you useless if he didn't really lead somewhere in the point is it does it leads to.",
                    "label": 0
                },
                {
                    "sent": "Exactly the kind of structure for factorization structure we would like.",
                    "label": 0
                },
                {
                    "sent": "They show that the graph toys structurally Markov if and only if it can be factorized in this way.",
                    "label": 1
                },
                {
                    "sent": "And what these fires are, these fires are arbitrary positive set index parameters, so any prior which has that.",
                    "label": 0
                },
                {
                    "sent": "Structural Markov property can be written in this way.",
                    "label": 0
                },
                {
                    "sent": "And that's really quite flat.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's really quite powerful and all such.",
                    "label": 0
                },
                {
                    "sent": "All such graph models have that conjugacy property I described.",
                    "label": 0
                },
                {
                    "sent": "Compared to that, this is a very minor little extra bit, but if you slightly change the condition there, restrict to decomposable graphs for which.",
                    "label": 0
                },
                {
                    "sent": "That intersection is a clique in.",
                    "label": 0
                },
                {
                    "sent": "One side, let's say a remains a clique in GA. Then we're going to call that the weak structural Markov property.",
                    "label": 1
                },
                {
                    "sent": "That places fewer conditional independence constraints on pie.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a.",
                    "label": 0
                },
                {
                    "sent": "That's a smaller set of GS, therefore this is fewer condition independent statements.",
                    "label": 0
                },
                {
                    "sent": "This this determines for us.",
                    "label": 0
                },
                {
                    "sent": "So potentially it might lead to a richer class of graph priors, but.",
                    "label": 1
                },
                {
                    "sent": "We will see in a moment that it's still a usable thing.",
                    "label": 0
                },
                {
                    "sent": "So just to clarify what that is.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are some pictures.",
                    "label": 0
                },
                {
                    "sent": "The vertex set of these five red dots A is this the four on the left of square on the left, B is the triangle on the right.",
                    "label": 0
                },
                {
                    "sent": "Missioning throughout on the AM, those two being connected 'cause we want it to be a decomposition and I'm also conditioning on the graph that the A intersection B remains a clique in GTA and that leaves us with 16 possibilities for GA and four possibilities for GB.",
                    "label": 1
                },
                {
                    "sent": "Please dotted lines.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the weak structural Markov property would say that your choice among those near choice among those have to be independent.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For every such AMB.",
                    "label": 0
                },
                {
                    "sent": "And the remarkable thing is, this can still lead to a factorization property with slightly more general, because we can now have different functions on the denominator in the numerator.",
                    "label": 1
                },
                {
                    "sent": "And that that extra flexibility have already been used in.",
                    "label": 0
                },
                {
                    "sent": "In computational work, bye bye bye Born and Karen.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this this idea underlines what we saw at the beginning, where the graphical properties and the factorization properties coexist and have implications between them.",
                    "label": 0
                },
                {
                    "sent": "A beautiful further episode in that story.",
                    "label": 0
                },
                {
                    "sent": "And then you see the kind of conjugacy we would then get.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right which we want to talk about?",
                    "label": 0
                },
                {
                    "sent": "Computing for trees.",
                    "label": 0
                },
                {
                    "sent": "There are explicit.",
                    "label": 0
                },
                {
                    "sent": "Explicit algorithms we can do perfect simulation for models on trees, and it will be a beautiful thing if you could use these strong, powerful properties of decomposability here to find perfect samplers for.",
                    "label": 0
                },
                {
                    "sent": "For Bayesian decomposable graph models, but there's really no been no progress on that.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're looking at MSMC again.",
                    "label": 0
                },
                {
                    "sent": "And we do the usual thing we'll we'll construct a Markov chain.",
                    "label": 1
                },
                {
                    "sent": "Top detail balance with respect to the posterior we want and we will will sample from it.",
                    "label": 1
                },
                {
                    "sent": "Again, DK.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Possibility helps us because we can.",
                    "label": 0
                },
                {
                    "sent": "We can tell mathematically what kind of perturbations we can make to graphs and still keep them decomposable.",
                    "label": 0
                },
                {
                    "sent": "So there's a quite an old result by by Steven Erikson and colleague, and even well, not quite so old result.",
                    "label": 0
                },
                {
                    "sent": "That I'm partly responsible for.",
                    "label": 0
                },
                {
                    "sent": "That tells us what edge moves can be made that maintain decomposability, 'cause if you want to conduct a Markov chain and running the space of decomposable graphs, you don't want to be jumping outside that space.",
                    "label": 0
                },
                {
                    "sent": "And these these kind of moves, and that includes some multiple later moves, more certain kinds of multiple Asian moves that completely connect.",
                    "label": 0
                },
                {
                    "sent": "Vertices that are not currently connected or completely disconnect those that are certain multiple Asian moves can also be characterized in this way.",
                    "label": 0
                },
                {
                    "sent": "And you can efficiently represent those moves using a junction tree representation of the graph.",
                    "label": 1
                },
                {
                    "sent": "These moves make any local changes to the junction tree, but they can change its topology locally as they do so.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now it turns out that that it turns out that that.",
                    "label": 0
                },
                {
                    "sent": "That topology change is actually a bit of a performance quite damaging in performance terms, and we recently found a simple way to speed up the sampling quite dramatically by cutting out the need to change the topology, and we do that by a trick treating the junction tree itself as part of the model parameterisation.",
                    "label": 1
                },
                {
                    "sent": "And that means augmenting the model so that we distribute the prior probability on a particular graph equally among the junction trees that represent that graph.",
                    "label": 0
                },
                {
                    "sent": "And that is a useful idea, because we can, we can count the number of such graphs so we know what probabilities we're assigning.",
                    "label": 0
                },
                {
                    "sent": "In the interest of time, I shall.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be forward a little bit, but this is sort of what we're getting here is.",
                    "label": 0
                },
                {
                    "sent": "It's a perfect alignment of several nice things that give us computational performance.",
                    "label": 1
                },
                {
                    "sent": "We got this pretesting for maintaining decomposability.",
                    "label": 1
                },
                {
                    "sent": "We have simplification of likelihood ratios because of that clique.",
                    "label": 1
                },
                {
                    "sent": "Separate separated factorization.",
                    "label": 1
                },
                {
                    "sent": "We're going to use the junction tree as the state, and we could do some certain multiple edge moves and this allows effectively sampling models of of a moderate size.",
                    "label": 1
                },
                {
                    "sent": "And they're not.",
                    "label": 0
                },
                {
                    "sent": "It's not the only way to sample to sample decomposable graphs by by local changes, but it's one of the most explored.",
                    "label": 0
                },
                {
                    "sent": "Let me just.",
                    "label": 0
                },
                {
                    "sent": "Just a.",
                    "label": 0
                },
                {
                    "sent": "Quick change of gear to show you that working.",
                    "label": 0
                },
                {
                    "sent": "So here you're seeing a sampler running in real time, slowed down a bit by the need to to render the graphs.",
                    "label": 0
                },
                {
                    "sent": "This is a model with 50 vertices.",
                    "label": 0
                },
                {
                    "sent": "It's simulated Gaussian data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Think you'll begin to practice to see what the true model was.",
                    "label": 0
                },
                {
                    "sent": "The the jittering you're seeing is partly just the randomization.",
                    "label": 0
                },
                {
                    "sent": "Sorry, partly just the rendering but you can see quite a lot about the way this is sampling.",
                    "label": 0
                },
                {
                    "sent": "From this picture you can see edges that come and go.",
                    "label": 0
                },
                {
                    "sent": "Of course, the frequency with which they are present is estimating a positive posterior probability, and so on and so on, and we can of course evaluate joint distributions of all sorts of coexistence of edges.",
                    "label": 0
                },
                {
                    "sent": "That the true graph was the was that sort of lattice structure that you can see there.",
                    "label": 0
                },
                {
                    "sent": "So that's 50 vertices, and we'd be reasonably confident.",
                    "label": 0
                },
                {
                    "sent": "Running",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That kind of thing on somewhat larger situation.",
                    "label": 0
                },
                {
                    "sent": "Let me say.",
                    "label": 0
                },
                {
                    "sent": "I haven't thought about parameters here.",
                    "label": 0
                },
                {
                    "sent": "In early work, people tended to use reversible jump type or other transformation or sampling to to jointly sample from this variable dimension posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "But in order to try and scale up to larger problems very early on, people who started to use conjugate hyper Markov priors for the parameters so that those could be without and what you get then is that metropolis ratio for updates to the graphs can be simplified into a locally computable form.",
                    "label": 1
                },
                {
                    "sent": "So these so called hyper inverse wish ArtPrize have been used for various matrices and Gaussian models and Hyper Doris Day for the multinomial case, and there's been a lot of work on choice of hyperparameters in the in those in those priors.",
                    "label": 1
                },
                {
                    "sent": "But they only go so far and it's still the true that we can only deal with quite moderate so moderate graphs here.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's accumulated evidence that.",
                    "label": 1
                },
                {
                    "sent": "MCMC using local moves is good at recovering low dimensional features such as edge inclusion, priors, we's inclusion probabilities.",
                    "label": 1
                },
                {
                    "sent": "We can be fairly confident.",
                    "label": 1
                },
                {
                    "sent": "In the estimated values of the posterior probability of a particular age being present, for example, or small probabilities of small combinations of particular edges.",
                    "label": 0
                },
                {
                    "sent": "But it's these kind of models in larger graph support recovering global properties 'cause they're really severe mixing problems.",
                    "label": 1
                },
                {
                    "sent": "And that motivates a powerful idea.",
                    "label": 0
                },
                {
                    "sent": "Really, really beautiful idea of using.",
                    "label": 1
                },
                {
                    "sent": "Using the edge inclusion probabilities estimated online to drive a stochastic search algorithm for, which is now an optimization method for trying to find high posterior probability graph.",
                    "label": 0
                },
                {
                    "sent": "So we move beyond sampling now, but using sampling based methods in order to conduct our optimization.",
                    "label": 1
                },
                {
                    "sent": "And the this thinks method feature inclusion stochastic search as proved very powerful at.",
                    "label": 0
                },
                {
                    "sent": "Hitting high posterior probability graphs and it's good for prediction tasks.",
                    "label": 0
                },
                {
                    "sent": "I think it's still true to say we don't really know enough about how well different approach approaches cover regions of high total probability.",
                    "label": 1
                },
                {
                    "sent": "One of the key ideas in in in sampling these high dimensional graph spaces is of course that the map.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Estimate the the highest for posterior probability graph.",
                    "label": 0
                },
                {
                    "sent": "May or may not be representative of a region of graphs that are all of high probability.",
                    "label": 0
                },
                {
                    "sent": "Let me move on to non decomposable graphs.",
                    "label": 1
                },
                {
                    "sent": "Even if the graph is not decomposable, we still have a similar factorization.",
                    "label": 1
                },
                {
                    "sent": "But the sets on the in the numerator now they called prime components.",
                    "label": 1
                },
                {
                    "sent": "Their maximal subgraphs that can't be decomposed in the sense I've talked about.",
                    "label": 0
                },
                {
                    "sent": "But they won't necessarily be complete.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This was explored in the Cisco Science Paper about 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The additional difficulties are that the normalizing constants in those non complete prime components don't have closed form, so you have to do Markov chain sampling within each step of the main chain in order to estimate them and those that those calculated values are very high variance.",
                    "label": 1
                },
                {
                    "sent": "So when you make.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, further when you make single edge perturbations to the graph, there is no guarantee of significant cancellation in the likelihood ratio.",
                    "label": 1
                },
                {
                    "sent": "So these these difficulties really add on computer time.",
                    "label": 0
                },
                {
                    "sent": "In that paper you can see them the time scalings are enormous.",
                    "label": 0
                },
                {
                    "sent": "Still quite modest graphs.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Their conclusion at the time was that.",
                    "label": 0
                },
                {
                    "sent": "That posterior sampling was not practical for problems with more than about 15 nodes, and they resort to heuristics like stochastic shotgun search.",
                    "label": 1
                },
                {
                    "sent": "Some of those difficulties have been ironed out later, and they need for MCMC and those normalizing constants has now been limited, but it's still the case that MCMC sampling for decomposable graphs non decomposable graphs is very very much harder.",
                    "label": 0
                },
                {
                    "sent": "The sick.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Styx shotgun search idea.",
                    "label": 0
                },
                {
                    "sent": "Proceeds by starting with the graph, selecting graphs that differ by 1 edge from it computing there, not unnormalized posterior probabilities, retaining the top ones and then.",
                    "label": 1
                },
                {
                    "sent": "Presuming proceeding from that among those neighbors take the I TH graph that you starting point, running little MCMC's and so on.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's a heuristic.",
                    "label": 0
                },
                {
                    "sent": "It works rather well and it's been improved and further explored in in other papers.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it is a method optimization, not a method of sampling.",
                    "label": 0
                },
                {
                    "sent": "I think I'll skip the.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stration stolen from that from that paper.",
                    "label": 0
                },
                {
                    "sent": "But another way to approach the non decomposable cases to remind ourselves, at least in the this, is in the Gaussian case.",
                    "label": 0
                },
                {
                    "sent": "That old old conditional distributions in the multivariate Gaussian are of course Gaussian an linear.",
                    "label": 1
                },
                {
                    "sent": "The mean has the mean as a linear structure.",
                    "label": 0
                },
                {
                    "sent": "So that the problem of fitting a graphical Gaussian model is also the problem of fitting a lot of regressions.",
                    "label": 1
                },
                {
                    "sent": "And you could simply do that job, do the fit multiple regressions for example by Bayesian methods, Bayesian sparse regression methods.",
                    "label": 0
                },
                {
                    "sent": "You end up with a lot of conditional distributions of variables given others, and you need some way of.",
                    "label": 0
                },
                {
                    "sent": "Piercing them together to deliver.",
                    "label": 0
                },
                {
                    "sent": "A single inference.",
                    "label": 0
                },
                {
                    "sent": "And there be various approaches to doing that.",
                    "label": 0
                },
                {
                    "sent": "Combining separately fitted sparse regression models to form a graphical estimate.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "These have been applied to quite quite large datasets and.",
                    "label": 0
                },
                {
                    "sent": "And appear interpretable, but the is a real.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that we've currently moved some way from the pure Bayesian paradigm of fitting the model.",
                    "label": 0
                },
                {
                    "sent": "We originally thought of in this case.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's been a lot of work on non decomposable models.",
                    "label": 0
                },
                {
                    "sent": "And there isn't time to go going to now.",
                    "label": 0
                },
                {
                    "sent": "So we know a lot.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More about fitting non decomposable models, but it's still the case that they are very much harder to fit the indecomposable ones.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "I'll say a little bit about treason, then I must wrap up.",
                    "label": 0
                },
                {
                    "sent": "A tree is a connected, undirected graph with no loops.",
                    "label": 1
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it is not necessarily correct if we call it a forest.",
                    "label": 0
                },
                {
                    "sent": "These",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Themselves decomposable graphs.",
                    "label": 0
                },
                {
                    "sent": "For a tree, the junction trees essentially isomorphic to the graph itself.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Except for the.",
                    "label": 0
                },
                {
                    "sent": "Ends the leaves at the end of the branches.",
                    "label": 0
                },
                {
                    "sent": "So of course there's an intimate connection between junction tree algorithms and algorithms on trees.",
                    "label": 1
                },
                {
                    "sent": "The in a tree.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The likelihood will always factorize in this way.",
                    "label": 0
                },
                {
                    "sent": "That's the decomposability condition, and that's essentially a product of edge probabilities divided by product of vertex probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so when we come to do posterior inference.",
                    "label": 0
                },
                {
                    "sent": "The the the the posterior divided by the prior is essentially a product of Bayes factors for dependence along the edges.",
                    "label": 0
                },
                {
                    "sent": "And that structure is something that's amenable to perfect simulation I mentioned earlier.",
                    "label": 1
                },
                {
                    "sent": "Because we have powerful algorithms for random spanning trees that can exploit that kind of of.",
                    "label": 1
                },
                {
                    "sent": "Of structure, and that's been extended to tree prize themselves decomposable unfortunate choice of words, but the fact rise again in this way is the product of weights of edges, and that works for treason.",
                    "label": 0
                },
                {
                    "sent": "It works, but if you do a little bit more work, it works also for forests.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I should have to skip the small section on Daggs.",
                    "label": 0
                },
                {
                    "sent": "I'll just briefly mention what the difficulty is with tags.",
                    "label": 0
                },
                {
                    "sent": "This is a situation where we wish to fit a DAG model to data that doesn't know anything about the directions.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The pure form of this problem.",
                    "label": 0
                },
                {
                    "sent": "Fitting tags is particularly important because of real or possibly imagined causal interpretations.",
                    "label": 1
                },
                {
                    "sent": "And of course, there's a huge literature on Bayesian networks which.",
                    "label": 0
                },
                {
                    "sent": "Which exploits these models.",
                    "label": 0
                },
                {
                    "sent": "The complicating factor in.",
                    "label": 0
                },
                {
                    "sent": "In fitting tags, is this problem of Markov?",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Equivalence.",
                    "label": 0
                },
                {
                    "sent": "Two different tags can imply the exactly the same set of conditional independence.",
                    "label": 1
                },
                {
                    "sent": "Is is not true in a undirected graph, and it is true in a directed graph.",
                    "label": 0
                },
                {
                    "sent": "So these two tags.",
                    "label": 0
                },
                {
                    "sent": "So this this tag is, this is equivalent to this one in the sense that the set of conditional dependencies that are true in this one and this one are the same.",
                    "label": 0
                },
                {
                    "sent": "This here again I've taken from one bank to another by changing the order a few of a few arrows.",
                    "label": 0
                },
                {
                    "sent": "These are not equivalent in terms of the conditional independence is now.",
                    "label": 0
                },
                {
                    "sent": "If you're analyzing data which doesn't know anything about the directions, you can only discover the conditional independencies.",
                    "label": 0
                },
                {
                    "sent": "So so that.",
                    "label": 0
                },
                {
                    "sent": "So the inference cannot distinguish those two.",
                    "label": 0
                },
                {
                    "sent": "So kind of distinguish those two, but can distinguish these.",
                    "label": 0
                },
                {
                    "sent": "Now, Fortunately there is a graphical simple.",
                    "label": 1
                },
                {
                    "sent": "Easily understood test for Markov equivalence.",
                    "label": 0
                },
                {
                    "sent": "Two Dag Samokov equivalent if they had the same skeleton, that is the same graph ignoring directions and the same sets of unmarried parents.",
                    "label": 1
                },
                {
                    "sent": "The same sets of Immoralities as they sometimes called David's term.",
                    "label": 0
                },
                {
                    "sent": "There is had a lot of life.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what this means is, if you want to conduct Bayesian inference using sampling methods on daggs, what you have to do is operate on on equivalence classes of the.",
                    "label": 0
                },
                {
                    "sent": "That off.",
                    "label": 0
                },
                {
                    "sent": "Equivalence classes under Markov equivalence.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is very recent work and I'd love to see if this is going to be exploited in Bayesian computation.",
                    "label": 0
                },
                {
                    "sent": "Very recent work on Markov chains.",
                    "label": 0
                },
                {
                    "sent": "For completely tags.",
                    "label": 0
                },
                {
                    "sent": "That are guaranteed to be reversible and therefore potentially usable.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, just to wrap up quickly.",
                    "label": 0
                },
                {
                    "sent": "Bayesian structural learning is one of these beautifully.",
                    "label": 1
                },
                {
                    "sent": "Simply stated problems.",
                    "label": 0
                },
                {
                    "sent": "What could be an easier problem to state?",
                    "label": 0
                },
                {
                    "sent": "But it show is proven amazingly hard to deliver except on a modest scale.",
                    "label": 1
                },
                {
                    "sent": "And that's true even with special choices of priors.",
                    "label": 0
                },
                {
                    "sent": "When we first learned about MCMC, we thought, oh, now we're relieved we can.",
                    "label": 0
                },
                {
                    "sent": "We can fit any model we like, but in order to make these methods work for even moderate sized graphs, we start having to make special assumptions.",
                    "label": 0
                },
                {
                    "sent": "And there's been a huge amount of effort on on on on these problems.",
                    "label": 0
                },
                {
                    "sent": "Some very creative people, many of whom are here.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not a neglected area.",
                    "label": 0
                },
                {
                    "sent": "As a result, we have a lot of.",
                    "label": 1
                },
                {
                    "sent": "Now we have heuristics work a lot of relying on optimization rather than sampling.",
                    "label": 0
                },
                {
                    "sent": "And to be honest, we don't really know what they're delivering.",
                    "label": 0
                },
                {
                    "sent": "I mean, if a probability surface is hard to sample from is probably it's probably also true that finding an optimum find us something that's very unrepresentative.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering whether it's time now to give up if you like, but that's to refocus effort a bit.",
                    "label": 0
                },
                {
                    "sent": "What we can estimate stable areas, inclusion probabilities and low dimensional marginals press the time is now come to really focus sampling methods on those, and on theoretical work that tries to give us some kind of guarantees.",
                    "label": 0
                },
                {
                    "sent": "And finally I wondered whether.",
                    "label": 0
                },
                {
                    "sent": "The problems are hard.",
                    "label": 0
                },
                {
                    "sent": "While they obviously are hard because they're discrete, but where there's opportunities for relaxing the hard constraints and looking at weaker models.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for your attention.",
                    "label": 0
                }
            ]
        }
    }
}