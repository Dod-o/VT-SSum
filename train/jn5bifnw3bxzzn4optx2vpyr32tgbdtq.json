{
    "id": "jn5bifnw3bxzzn4optx2vpyr32tgbdtq",
    "title": "Automatic and Efficient Long Term Arm and Hand Tracking for Continuous Sign Language TV Broadcasts",
    "info": {
        "author": [
            "Tomas Pfister, Department of Engineering Science, University of Oxford"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_pfister_sign_language/",
    "segmentation": [
        [
            "OK, so Richard mentioned this is joint work with Oxford and leads with James Charles who is here from Leeds and me from Oxford and obviously Mark and AC."
        ],
        [
            "Alright, so the problem that I'm going to talk about this well.",
            "How to how to be?",
            "Well, first, the motivation of this project is automatic sign language recognition.",
            "So what we want is just like in any other classification task, is a large set of training data.",
            "We want to have a large corpus just like what Stan mentioned.",
            "But our approach to this is a bit different.",
            "We don't want to collect it, we want to get it get this corpus automatically.",
            "I'll show you how this is done.",
            "So our idea here is that we obtain these training examples from.",
            "Signed TV broadcasts such as this one here.",
            "So what you see here is the signer in the bottom right hand corner, which is translating what's being said in the in the broadcast into sign language.",
            "His superimposed on the background and what's being broadcasted, what's being said in this broadcast is also been shown in subtitles, so by using this correspondence between the signing and the subtitles, we can automatically learn the signs such as in this.",
            "This one round round about the flower and then we can use this sign sign text pairs to train a sign language classifier and in this way we can get.",
            "We can solve this corpus problem that we hope by getting a near infinite amount of supervised training data and this is provided that we can track the person in the video and I'll show you how we how we do this in real time.",
            "Automatically in this talk."
        ],
        [
            "So I mentioned a key problem to solve.",
            "Here is how do we find the position of the head and the arms and hands of designers so that we can exploit this infinite amount of data that we get by correlating the subtitles with the signs.",
            "If we only detect the hands, it would be difficult to distinguish between the left and the right hands.",
            "So to help us disambiguate where the hands are we instead of only finding the hands instead of only doing hand detection, we find the arms, and that's the problem.",
            "I'm going to talk about today.",
            "So how do we find the arms, the head, and the hands?",
            "And I'll be showing you I wait to find all these things automatically and quickly, and I'll demonstrate that we get very good results for hours of tracking.",
            "And I'll be just like what Stan did earlier.",
            "He was saying he was doing tracking by detection doing the same thing here.",
            "We detect both these independently in each frame."
        ],
        [
            "So there's certain difficulties with using this TV broadcast material, so as soon as you can see the sign are in the background frequently quite similar color, which makes it a bit more difficult to use color as a clue.",
            "And the hands frequently overlapping or blurred, which makes things more difficult and less frequently.",
            "Faces and hands in the background, which can be confused with that of the signer.",
            "And finally, perhaps the biggest challenge for this piece of data is that there's a moving background behind the signer.",
            "And since in every frame both designer and the background is changing, makes things much more difficult in terms of extracting designer.",
            "There's both things are moving all the time.",
            "You don't have a static, you don't have a permanent static background behind signer for the whole of the signer."
        ],
        [
            "So our approach to solving this problem is really having two main steps.",
            "In the first one or we do is we segment the signer and in the second one we detect the joints using this segmentation.",
            "So I'll show you a flow kind of flow chart here of our approach.",
            "Keep referring to this throughout the talk to show where I am.",
            "So the first step as I mentioned, is to segment designer.",
            "And the after that we determine a color posterior of the signer, meaning of the torso, the skin region in the background, and finally we then detect the locations of the joints using a random forest."
        ],
        [
            "So we're not the first people to do this.",
            "There's been some excellent work working on this problem.",
            "From our lab that we're going to benefit from this.",
            "To solve this problem.",
            "Also, I have to mention the subtitle.",
            "Sign alignment problem has also been attacked before from our group, so we have a solution to do that already.",
            "But what we didn't have a solution for before was how do we solve this arm tracking problem efficiently?",
            "So in BMC 2008 brilliant or proposed this pictorial structure model which explained the foreground and background in sign language videos on the goal, there was the same.",
            "It was tracking the arms and hands.",
            "In this TV broadcasts, but the difference was that there was a very high cost for annotation and there was a very high cost of fitting the model, so they were using pictorial structure models which are.",
            "Take a while to run.",
            "They needed in particular 75 annotated frames for each hour of video.",
            "It's manually annotated frames, and that took about 3 hours per one hour of video.",
            "So if you really want to do that for a large amount of data is going to be quite labor intensive, and Secondly, they also use this pictorial structure model, which gave very accurate results.",
            "But it took about 100 seconds per frame to run, so again, running this and.",
            "Thousands of hours of media is not very feasible.",
            "So what?"
        ],
        [
            "Elementor does this we exploit the results of Buehler doors work there?",
            "Tracking in boots in a way that enables us to detect the joint locations with no manual annotation and in real time, so that means that we can finally run this on a large amount of data and extract a large amount of sign text pairs and use that to help us solve the sign language recognition problem."
        ],
        [
            "So I'll start by talking about the first step, which is how do we segment designer out of the TV broadcasts?"
        ],
        [
            "So the problem that we're looking at here, how do we segment designer given all these complications that I mentioned earlier or the moving background hands faces in the background, and so on?"
        ],
        [
            "So our life would be much easier if we had depth data.",
            "So if we had output from connects in the from the TV broadcast, life would be much easier because we could just simply segment the person from the background by looking at the depth.",
            "But sadly we only had two D data.",
            "So what do we do?"
        ],
        [
            "Well, we can we notice by looking at these videos that there's a number of Constance is things that don't.",
            "Things that are the same throughout the video.",
            "So for example, you can see that this permanently static background for that which is behind the sign a part of this which is behind part of designer.",
            "And then there's a box with a changing background and you can also notice that the signer is the same throughout the video and he actually never crosses this line.",
            "On the video.",
            "So he's always in his little box in the right."
        ],
        [
            "So what we really want to do is we want to find a generative model of the video that exploits these constancies and in this way you can view this video as being composed of a signer in the foreground and background that's moving, and a background that static, which you can see on this video.",
            "So you can see every video as being composed of these strings three things, and if they knew each of these layers are problem would be solved because we find the foreground with designer.",
            "So what we'll do is we exploit the constancies that I mentioned to help us find this layered model."
        ],
        [
            "OK, so we do this by using Co segmentation, which means we consider a bunch of frames so all the frames together rather than just doing segmentation per frame.",
            "So in particular we exploit the information of a sample of frames.",
            "To help us obtain the static background behind the signer, so that's you can be serialized in the top right hand corner there.",
            "So we find the background that's actually behind the signer that's revealed throughout the video, and we also find a color histogram of the foreground.",
            "That means the color histogram of the signer.",
            "For the whole video, we don't use these two things to obtain per frame.",
            "Segmentations.",
            "Now, briefly discuss how we obtain each of these two layers."
        ],
        [
            "So if we knew the background behind the signer weaker segment perfectly by by simply comparing the pixel values of a certain frame to the known background.",
            "So if you know this background and the right here simply if there's a frame with the signer there, we can just compare the pixel values 'cause we know what's behind here, so it would be really easy to segment him, at least on this side, when there's a static background.",
            "So in order to do this, what we do is was shown in the bottom left video.",
            "So we roughly segment regions behind the signer that are revealed throughout the video.",
            "To obtain this clean plate as we call it, so turn from the movie industry and then we take a median of these segmentations.",
            "Do we give us the final static background?",
            "Use grab cut for the segmentations."
        ],
        [
            "So another thing that we noticed earlier that I pointed out earlier was that throughout the video designer is always located in this same bottom right corner in his litter box.",
            "So we expect this to extract the foreground color model of designer.",
            "In particular, we do this by looking for faces in this box, and then we extract a color model based on regions relative to the face.",
            "As you see over there in the right hand video.",
            "That's the regions that we're using.",
            "And we then combine the color models from a sample of frames into one global color model that we used for the final four grams segmentations with grabcut.",
            "At some point the author so finger being pointing at designer through there."
        ],
        [
            "OK, so now to some qualitative results of the segmentation method.",
            "So overall if you look at this video, you'll see that the results are pretty good.",
            "Large part of the silhouettes are pretty much always correct.",
            "It's been particularly on the right hand side where we do the comparison to the clean plate.",
            "It's perfect because we know the background behind it.",
            "We can just compare the pixel values.",
            "Sometimes it misses parts of the hands, but that's not really a big problem in practice, and it also occasionally catches parts of the background as I show in some later examples."
        ],
        [
            "So now, having obtained the segmentations, I'll focus on how do we combine the segmentations with color clues to give us a more helpful input to the joint detector."
        ],
        [
            "So as we see in the foreground, segmentation is help us remove some of the collector in the background.",
            "We can just filter away the background but and also these silhouettes give us some key curious about where they had the shoulders and the arms are.",
            "If you look at the right up corner you can see you can see you can see that in some cases the.",
            "You can find out where their hands are, but that's not always the case sometimes.",
            "Sometimes it's not very useful for finding where the hand positions are depends on the post.",
            "So intuitively.",
            "Skin regions give a quite strong clue about where the hands are likely located, so today's purpose we find a color model of the skin and the torso here in the moving video, the red is skin, greenest are so blue is background.",
            "And we do this by extracting skin color from a face detector again and then a torso color from rough segmentation, and then we remove skin colored face like colored regions.",
            "From this segmentation and that gives us the torso and skin color models.",
            "And in addition to helping us find where the hands are, this color mode also helps us to abstract away from the color of the clothes of the signer.",
            "So helps us, yeah helps generalize a little bit."
        ],
        [
            "So now having obtained the segmented color posteriors, we will focus on how we obtain this, how we how we use these to help us find the joint positions of the signer?"
        ],
        [
            "So the aim here is defend.",
            "Find the layout of the upper body.",
            "That means the head, the shoulder, the elbows and wrists.",
            "Of the signer do help us disambiguate where this where.",
            "The hands are.",
            "The hand side of things that we want to ultimately track, because that's where a lot of the information in sign languages.",
            "So we do is we train our system on hours of joint tracking output from a very accurate but slow and not automatic tracker presented by Buehler at all the work that I discussed earlier in these slides.",
            "And here we see a small subset of the training material that we use and see the key on the right for the different dots.",
            "What different dots are and some tracking output at the top.",
            "For their method.",
            "So what we do is we combine the segmentations from the previous step with this large amount of of training data he had training data for about.",
            "There are 3030 hours of media.",
            "We offer which we used part of this work, so we combine the segmentations from the previous step with this large amount of data to find a signorie independent, fast and accurate automatic joint position estimating estimator.",
            "And that's what we're going to get here.",
            "I'll show you now how we."
        ],
        [
            "Get this so we we solve this problem by treating the task as multiclass classification problem, in which we use it use the color posterior of the skin and torso as an input.",
            "So what we do is we classify each pixel in the color posterior image in a sliding window fashion into one of eight categories.",
            "Each of these eight categories describe one joint.",
            "Then in a similar way to connect with the random forest classifier to classify an input pixel and these are the node tests shown showing up there.",
            "So basically they what they do is they take one or two color posterior values from the window, either red, green or blue channel and then apply this following this functions shown and what we then do is after we've processed all these.",
            "Pixels through the forest.",
            "Each pixel has assigned to it a probability, a class probability which measure which we can visualize.",
            "The probability density function.",
            "So that's this thing on the left in that red red box, and then finally what we do is we take the maximum of that probability density function and that gives us the joint positions."
        ],
        [
            "So now to do some evaluation here.",
            "So this video visualizes method and compares it to be alert or tracking tracking output and it's being slowed down to make it a bit easier to easier to follow and.",
            "The field circles show our estimations.",
            "Open circles show ruler dolls.",
            "Estimations"
        ],
        [
            "Yes, another way of visualizing it so you can view our results on the left pillar.",
            "Dawson the right differences in performance are quite small, but I can see that you are toast without some more noisy because they use the discrete both space.",
            "I remember again I meant that is much faster and doesn't require any manual annotation.",
            "Will actually help us solve this sign language problem."
        ],
        [
            "So we can observe some quantitative results.",
            "You can see the cumulative distribution function function of joint estimates here top near to the top left corner.",
            "We are the better results.",
            "For example here.",
            "At that point we in 80% of cases that the joint predictions for the wrists are within five pixels of the ground truth maintained to notice is that all the joint results are better than blur tours."
        ],
        [
            "Some problems that we have hand side occasionally mixed, but these are and there's some segmentation problems sometimes, but these are not really a problem in practice because the random forest sees this when it will turn its learning as well.",
            "We also, jenn."
        ],
        [
            "Realized the new training material so you can see the same videos here.",
            "Upper One is trained and tested on the same signer.",
            "This, under one is under untested or unseen signers, so we can generalize quite well to new material."
        ],
        [
            "And then to conclude, we presented our new method defines the position of the House and the arms in signed videos automatically and in real time we get reliable results for hours of tracking and generalized new signers.",
            "And in future work will add a special model to deal with some of the hand problems.",
            "Thank you for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so Richard mentioned this is joint work with Oxford and leads with James Charles who is here from Leeds and me from Oxford and obviously Mark and AC.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so the problem that I'm going to talk about this well.",
                    "label": 0
                },
                {
                    "sent": "How to how to be?",
                    "label": 0
                },
                {
                    "sent": "Well, first, the motivation of this project is automatic sign language recognition.",
                    "label": 1
                },
                {
                    "sent": "So what we want is just like in any other classification task, is a large set of training data.",
                    "label": 1
                },
                {
                    "sent": "We want to have a large corpus just like what Stan mentioned.",
                    "label": 0
                },
                {
                    "sent": "But our approach to this is a bit different.",
                    "label": 0
                },
                {
                    "sent": "We don't want to collect it, we want to get it get this corpus automatically.",
                    "label": 0
                },
                {
                    "sent": "I'll show you how this is done.",
                    "label": 1
                },
                {
                    "sent": "So our idea here is that we obtain these training examples from.",
                    "label": 0
                },
                {
                    "sent": "Signed TV broadcasts such as this one here.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is the signer in the bottom right hand corner, which is translating what's being said in the in the broadcast into sign language.",
                    "label": 0
                },
                {
                    "sent": "His superimposed on the background and what's being broadcasted, what's being said in this broadcast is also been shown in subtitles, so by using this correspondence between the signing and the subtitles, we can automatically learn the signs such as in this.",
                    "label": 0
                },
                {
                    "sent": "This one round round about the flower and then we can use this sign sign text pairs to train a sign language classifier and in this way we can get.",
                    "label": 1
                },
                {
                    "sent": "We can solve this corpus problem that we hope by getting a near infinite amount of supervised training data and this is provided that we can track the person in the video and I'll show you how we how we do this in real time.",
                    "label": 0
                },
                {
                    "sent": "Automatically in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I mentioned a key problem to solve.",
                    "label": 0
                },
                {
                    "sent": "Here is how do we find the position of the head and the arms and hands of designers so that we can exploit this infinite amount of data that we get by correlating the subtitles with the signs.",
                    "label": 1
                },
                {
                    "sent": "If we only detect the hands, it would be difficult to distinguish between the left and the right hands.",
                    "label": 0
                },
                {
                    "sent": "So to help us disambiguate where the hands are we instead of only finding the hands instead of only doing hand detection, we find the arms, and that's the problem.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "So how do we find the arms, the head, and the hands?",
                    "label": 0
                },
                {
                    "sent": "And I'll be showing you I wait to find all these things automatically and quickly, and I'll demonstrate that we get very good results for hours of tracking.",
                    "label": 0
                },
                {
                    "sent": "And I'll be just like what Stan did earlier.",
                    "label": 0
                },
                {
                    "sent": "He was saying he was doing tracking by detection doing the same thing here.",
                    "label": 0
                },
                {
                    "sent": "We detect both these independently in each frame.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's certain difficulties with using this TV broadcast material, so as soon as you can see the sign are in the background frequently quite similar color, which makes it a bit more difficult to use color as a clue.",
                    "label": 0
                },
                {
                    "sent": "And the hands frequently overlapping or blurred, which makes things more difficult and less frequently.",
                    "label": 0
                },
                {
                    "sent": "Faces and hands in the background, which can be confused with that of the signer.",
                    "label": 1
                },
                {
                    "sent": "And finally, perhaps the biggest challenge for this piece of data is that there's a moving background behind the signer.",
                    "label": 0
                },
                {
                    "sent": "And since in every frame both designer and the background is changing, makes things much more difficult in terms of extracting designer.",
                    "label": 0
                },
                {
                    "sent": "There's both things are moving all the time.",
                    "label": 0
                },
                {
                    "sent": "You don't have a static, you don't have a permanent static background behind signer for the whole of the signer.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach to solving this problem is really having two main steps.",
                    "label": 1
                },
                {
                    "sent": "In the first one or we do is we segment the signer and in the second one we detect the joints using this segmentation.",
                    "label": 0
                },
                {
                    "sent": "So I'll show you a flow kind of flow chart here of our approach.",
                    "label": 0
                },
                {
                    "sent": "Keep referring to this throughout the talk to show where I am.",
                    "label": 0
                },
                {
                    "sent": "So the first step as I mentioned, is to segment designer.",
                    "label": 0
                },
                {
                    "sent": "And the after that we determine a color posterior of the signer, meaning of the torso, the skin region in the background, and finally we then detect the locations of the joints using a random forest.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're not the first people to do this.",
                    "label": 0
                },
                {
                    "sent": "There's been some excellent work working on this problem.",
                    "label": 0
                },
                {
                    "sent": "From our lab that we're going to benefit from this.",
                    "label": 0
                },
                {
                    "sent": "To solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Also, I have to mention the subtitle.",
                    "label": 0
                },
                {
                    "sent": "Sign alignment problem has also been attacked before from our group, so we have a solution to do that already.",
                    "label": 0
                },
                {
                    "sent": "But what we didn't have a solution for before was how do we solve this arm tracking problem efficiently?",
                    "label": 0
                },
                {
                    "sent": "So in BMC 2008 brilliant or proposed this pictorial structure model which explained the foreground and background in sign language videos on the goal, there was the same.",
                    "label": 1
                },
                {
                    "sent": "It was tracking the arms and hands.",
                    "label": 0
                },
                {
                    "sent": "In this TV broadcasts, but the difference was that there was a very high cost for annotation and there was a very high cost of fitting the model, so they were using pictorial structure models which are.",
                    "label": 0
                },
                {
                    "sent": "Take a while to run.",
                    "label": 1
                },
                {
                    "sent": "They needed in particular 75 annotated frames for each hour of video.",
                    "label": 1
                },
                {
                    "sent": "It's manually annotated frames, and that took about 3 hours per one hour of video.",
                    "label": 1
                },
                {
                    "sent": "So if you really want to do that for a large amount of data is going to be quite labor intensive, and Secondly, they also use this pictorial structure model, which gave very accurate results.",
                    "label": 0
                },
                {
                    "sent": "But it took about 100 seconds per frame to run, so again, running this and.",
                    "label": 0
                },
                {
                    "sent": "Thousands of hours of media is not very feasible.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Elementor does this we exploit the results of Buehler doors work there?",
                    "label": 0
                },
                {
                    "sent": "Tracking in boots in a way that enables us to detect the joint locations with no manual annotation and in real time, so that means that we can finally run this on a large amount of data and extract a large amount of sign text pairs and use that to help us solve the sign language recognition problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll start by talking about the first step, which is how do we segment designer out of the TV broadcasts?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem that we're looking at here, how do we segment designer given all these complications that I mentioned earlier or the moving background hands faces in the background, and so on?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our life would be much easier if we had depth data.",
                    "label": 1
                },
                {
                    "sent": "So if we had output from connects in the from the TV broadcast, life would be much easier because we could just simply segment the person from the background by looking at the depth.",
                    "label": 0
                },
                {
                    "sent": "But sadly we only had two D data.",
                    "label": 1
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we can we notice by looking at these videos that there's a number of Constance is things that don't.",
                    "label": 0
                },
                {
                    "sent": "Things that are the same throughout the video.",
                    "label": 0
                },
                {
                    "sent": "So for example, you can see that this permanently static background for that which is behind the sign a part of this which is behind part of designer.",
                    "label": 0
                },
                {
                    "sent": "And then there's a box with a changing background and you can also notice that the signer is the same throughout the video and he actually never crosses this line.",
                    "label": 1
                },
                {
                    "sent": "On the video.",
                    "label": 1
                },
                {
                    "sent": "So he's always in his little box in the right.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we really want to do is we want to find a generative model of the video that exploits these constancies and in this way you can view this video as being composed of a signer in the foreground and background that's moving, and a background that static, which you can see on this video.",
                    "label": 1
                },
                {
                    "sent": "So you can see every video as being composed of these strings three things, and if they knew each of these layers are problem would be solved because we find the foreground with designer.",
                    "label": 0
                },
                {
                    "sent": "So what we'll do is we exploit the constancies that I mentioned to help us find this layered model.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we do this by using Co segmentation, which means we consider a bunch of frames so all the frames together rather than just doing segmentation per frame.",
                    "label": 0
                },
                {
                    "sent": "So in particular we exploit the information of a sample of frames.",
                    "label": 1
                },
                {
                    "sent": "To help us obtain the static background behind the signer, so that's you can be serialized in the top right hand corner there.",
                    "label": 1
                },
                {
                    "sent": "So we find the background that's actually behind the signer that's revealed throughout the video, and we also find a color histogram of the foreground.",
                    "label": 1
                },
                {
                    "sent": "That means the color histogram of the signer.",
                    "label": 0
                },
                {
                    "sent": "For the whole video, we don't use these two things to obtain per frame.",
                    "label": 0
                },
                {
                    "sent": "Segmentations.",
                    "label": 0
                },
                {
                    "sent": "Now, briefly discuss how we obtain each of these two layers.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we knew the background behind the signer weaker segment perfectly by by simply comparing the pixel values of a certain frame to the known background.",
                    "label": 0
                },
                {
                    "sent": "So if you know this background and the right here simply if there's a frame with the signer there, we can just compare the pixel values 'cause we know what's behind here, so it would be really easy to segment him, at least on this side, when there's a static background.",
                    "label": 0
                },
                {
                    "sent": "So in order to do this, what we do is was shown in the bottom left video.",
                    "label": 0
                },
                {
                    "sent": "So we roughly segment regions behind the signer that are revealed throughout the video.",
                    "label": 1
                },
                {
                    "sent": "To obtain this clean plate as we call it, so turn from the movie industry and then we take a median of these segmentations.",
                    "label": 1
                },
                {
                    "sent": "Do we give us the final static background?",
                    "label": 1
                },
                {
                    "sent": "Use grab cut for the segmentations.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another thing that we noticed earlier that I pointed out earlier was that throughout the video designer is always located in this same bottom right corner in his litter box.",
                    "label": 0
                },
                {
                    "sent": "So we expect this to extract the foreground color model of designer.",
                    "label": 0
                },
                {
                    "sent": "In particular, we do this by looking for faces in this box, and then we extract a color model based on regions relative to the face.",
                    "label": 1
                },
                {
                    "sent": "As you see over there in the right hand video.",
                    "label": 0
                },
                {
                    "sent": "That's the regions that we're using.",
                    "label": 0
                },
                {
                    "sent": "And we then combine the color models from a sample of frames into one global color model that we used for the final four grams segmentations with grabcut.",
                    "label": 1
                },
                {
                    "sent": "At some point the author so finger being pointing at designer through there.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now to some qualitative results of the segmentation method.",
                    "label": 0
                },
                {
                    "sent": "So overall if you look at this video, you'll see that the results are pretty good.",
                    "label": 0
                },
                {
                    "sent": "Large part of the silhouettes are pretty much always correct.",
                    "label": 0
                },
                {
                    "sent": "It's been particularly on the right hand side where we do the comparison to the clean plate.",
                    "label": 0
                },
                {
                    "sent": "It's perfect because we know the background behind it.",
                    "label": 0
                },
                {
                    "sent": "We can just compare the pixel values.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it misses parts of the hands, but that's not really a big problem in practice, and it also occasionally catches parts of the background as I show in some later examples.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now, having obtained the segmentations, I'll focus on how do we combine the segmentations with color clues to give us a more helpful input to the joint detector.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we see in the foreground, segmentation is help us remove some of the collector in the background.",
                    "label": 0
                },
                {
                    "sent": "We can just filter away the background but and also these silhouettes give us some key curious about where they had the shoulders and the arms are.",
                    "label": 0
                },
                {
                    "sent": "If you look at the right up corner you can see you can see you can see that in some cases the.",
                    "label": 0
                },
                {
                    "sent": "You can find out where their hands are, but that's not always the case sometimes.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's not very useful for finding where the hand positions are depends on the post.",
                    "label": 0
                },
                {
                    "sent": "So intuitively.",
                    "label": 0
                },
                {
                    "sent": "Skin regions give a quite strong clue about where the hands are likely located, so today's purpose we find a color model of the skin and the torso here in the moving video, the red is skin, greenest are so blue is background.",
                    "label": 1
                },
                {
                    "sent": "And we do this by extracting skin color from a face detector again and then a torso color from rough segmentation, and then we remove skin colored face like colored regions.",
                    "label": 0
                },
                {
                    "sent": "From this segmentation and that gives us the torso and skin color models.",
                    "label": 0
                },
                {
                    "sent": "And in addition to helping us find where the hands are, this color mode also helps us to abstract away from the color of the clothes of the signer.",
                    "label": 0
                },
                {
                    "sent": "So helps us, yeah helps generalize a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now having obtained the segmented color posteriors, we will focus on how we obtain this, how we how we use these to help us find the joint positions of the signer?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the aim here is defend.",
                    "label": 0
                },
                {
                    "sent": "Find the layout of the upper body.",
                    "label": 0
                },
                {
                    "sent": "That means the head, the shoulder, the elbows and wrists.",
                    "label": 1
                },
                {
                    "sent": "Of the signer do help us disambiguate where this where.",
                    "label": 0
                },
                {
                    "sent": "The hands are.",
                    "label": 0
                },
                {
                    "sent": "The hand side of things that we want to ultimately track, because that's where a lot of the information in sign languages.",
                    "label": 0
                },
                {
                    "sent": "So we do is we train our system on hours of joint tracking output from a very accurate but slow and not automatic tracker presented by Buehler at all the work that I discussed earlier in these slides.",
                    "label": 0
                },
                {
                    "sent": "And here we see a small subset of the training material that we use and see the key on the right for the different dots.",
                    "label": 0
                },
                {
                    "sent": "What different dots are and some tracking output at the top.",
                    "label": 0
                },
                {
                    "sent": "For their method.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we combine the segmentations from the previous step with this large amount of of training data he had training data for about.",
                    "label": 0
                },
                {
                    "sent": "There are 3030 hours of media.",
                    "label": 0
                },
                {
                    "sent": "We offer which we used part of this work, so we combine the segmentations from the previous step with this large amount of data to find a signorie independent, fast and accurate automatic joint position estimating estimator.",
                    "label": 0
                },
                {
                    "sent": "And that's what we're going to get here.",
                    "label": 0
                },
                {
                    "sent": "I'll show you now how we.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get this so we we solve this problem by treating the task as multiclass classification problem, in which we use it use the color posterior of the skin and torso as an input.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we classify each pixel in the color posterior image in a sliding window fashion into one of eight categories.",
                    "label": 1
                },
                {
                    "sent": "Each of these eight categories describe one joint.",
                    "label": 1
                },
                {
                    "sent": "Then in a similar way to connect with the random forest classifier to classify an input pixel and these are the node tests shown showing up there.",
                    "label": 0
                },
                {
                    "sent": "So basically they what they do is they take one or two color posterior values from the window, either red, green or blue channel and then apply this following this functions shown and what we then do is after we've processed all these.",
                    "label": 0
                },
                {
                    "sent": "Pixels through the forest.",
                    "label": 0
                },
                {
                    "sent": "Each pixel has assigned to it a probability, a class probability which measure which we can visualize.",
                    "label": 0
                },
                {
                    "sent": "The probability density function.",
                    "label": 0
                },
                {
                    "sent": "So that's this thing on the left in that red red box, and then finally what we do is we take the maximum of that probability density function and that gives us the joint positions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now to do some evaluation here.",
                    "label": 0
                },
                {
                    "sent": "So this video visualizes method and compares it to be alert or tracking tracking output and it's being slowed down to make it a bit easier to easier to follow and.",
                    "label": 0
                },
                {
                    "sent": "The field circles show our estimations.",
                    "label": 0
                },
                {
                    "sent": "Open circles show ruler dolls.",
                    "label": 0
                },
                {
                    "sent": "Estimations",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, another way of visualizing it so you can view our results on the left pillar.",
                    "label": 0
                },
                {
                    "sent": "Dawson the right differences in performance are quite small, but I can see that you are toast without some more noisy because they use the discrete both space.",
                    "label": 0
                },
                {
                    "sent": "I remember again I meant that is much faster and doesn't require any manual annotation.",
                    "label": 0
                },
                {
                    "sent": "Will actually help us solve this sign language problem.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can observe some quantitative results.",
                    "label": 1
                },
                {
                    "sent": "You can see the cumulative distribution function function of joint estimates here top near to the top left corner.",
                    "label": 0
                },
                {
                    "sent": "We are the better results.",
                    "label": 0
                },
                {
                    "sent": "For example here.",
                    "label": 0
                },
                {
                    "sent": "At that point we in 80% of cases that the joint predictions for the wrists are within five pixels of the ground truth maintained to notice is that all the joint results are better than blur tours.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some problems that we have hand side occasionally mixed, but these are and there's some segmentation problems sometimes, but these are not really a problem in practice because the random forest sees this when it will turn its learning as well.",
                    "label": 0
                },
                {
                    "sent": "We also, jenn.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Realized the new training material so you can see the same videos here.",
                    "label": 0
                },
                {
                    "sent": "Upper One is trained and tested on the same signer.",
                    "label": 1
                },
                {
                    "sent": "This, under one is under untested or unseen signers, so we can generalize quite well to new material.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then to conclude, we presented our new method defines the position of the House and the arms in signed videos automatically and in real time we get reliable results for hours of tracking and generalized new signers.",
                    "label": 1
                },
                {
                    "sent": "And in future work will add a special model to deal with some of the hand problems.",
                    "label": 0
                },
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                }
            ]
        }
    }
}