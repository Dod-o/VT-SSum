{
    "id": "5xkx6nqubmt2mo4kzzp4ozmxjlqlvhb5",
    "title": "Semi-supervised Graph Clustering: A Kernel Approach",
    "info": {
        "author": [
            "Brian Kulis, University of Texas at Austin"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml05_kulis_aa/",
    "segmentation": [
        [
            "A Colonel approach.",
            "This is joint work with Sugato Basu, Inderjit Dhillon and Ray Mooney at the University of Texas at Austin.",
            "So the topic of this paper is semi supervised."
        ],
        [
            "Is clustering so First things first.",
            "What is semi supervised clustering?",
            "So I'll just give a very small example.",
            "For those of you not familiar with it.",
            "So suppose I have these points in space that I want to close."
        ],
        [
            "Sure, we might run a clustering algorithm on these points, and perhaps it will return these horozontal clusters as a result.",
            "But the points are somewhat ambiguous, so.",
            "It might be the case that another algorithm would return the vertical."
        ],
        [
            "Busters as a result.",
            "In semi supervised clustering we would like to be able to give the clustering algorithm some background information that will guide the clustering algorithm to some desired clustering so."
        ],
        [
            "That background information generally comes in the form of pairwise constraints, that is, pairs of points that should be in the same cluster an pairs of points that should be in different clusters.",
            "In this case, we have the two red and the blue points.",
            "We say those are.",
            "Those should be in different clusters that have a cannot link, whereas the two blue points on the left and the two red points on the right.",
            "They have must link constraints, so we use.",
            "We give that to the algorithm and then the semi supervised clustering algorithm uses that to come up with the vertical clusters here.",
            "So this isn't a new problem, it's been around for for awhile and it's been an active area of research.",
            "One particular algorithm that will look at is called the HMRFHMRFK means algorithm.",
            "It's based on a probabilistic framework for semi supervised clustering using hidden Markov random fields.",
            "It's a vector based approach, meaning that the input is assumed to be a set of vectors that we're clustering in addition to the constraint that we give it."
        ],
        [
            "One drawback with algorithms such as HMRFK means is that in some cases the algorithm has some trouble.",
            "For example, if the data itself is not linearly separable, then HMRFK means is unable to recover the true clusters unless we give it a very large number of constraints.",
            "So in this case, in this case we have two circles, one inside the other and the algorithm is unable to recover those clusters.",
            "Another problem with.",
            "Some vector base."
        ],
        [
            "Algorithms such as HMRFK means is that our data are input data may not be a set of vectors at all, but rather a graph where we hope to cluster the nodes of the graph into different clusters.",
            "In this case, we have a social network here and of course vector base."
        ],
        [
            "Algorithms are inappropriate.",
            "So what's generally done for the unsupervised case is we run some sort of graph clustering algorithm, such as minimizing the normalized cut or ratio cut in the graph.",
            "We might do that on on this graph, for example, but again, just as in the vector case, we might want to incorporate.",
            "Constraints into the into our graph clustering algorithm to guide the class."
        ],
        [
            "Bring algorithm to a better cut of the graph.",
            "So in this case we give it these must link and cannot link constraints and our algorithm would use those to come up with a different cut.",
            "So one algorithm that does this sort of graph based semi supervised clustering is an algorithm called spectral learning and we'll talk about that and how that fits in with our framework.",
            "For semi supervised clustering a little bit later."
        ],
        [
            "So that's sort of the brief introduction to semi supervised clustering.",
            "And now let's talk about what we do in this paper.",
            "So in this paper, we're going to look at something that we that's called the weighted kernel.",
            "K means objective function, and the first thing we'll do is introduce that and review a previous result that shows that the weighted kernel K means object."
        ],
        [
            "Function.",
            "Captures a number of common graph clustering objectives, such as normalized, cut and ratio cut as special cases, and So what we're going to do is we're going to take that objective function, and we're going to extend those results to show that a number of vector based and graph based semi supervised clustering objective functions also fall out as special cases.",
            "So in a sense, the weighted kernel K means objective function will unify several vector based and graph based approaches to semi supervised clustering.",
            "An one implication of this is."
        ],
        [
            "So that will just have a single algorithm for clustering.",
            "For semi supervised clustering given either vectors or a graph."
        ],
        [
            "And as I mentioned, several objective functions will fall out of our framework as special cases."
        ],
        [
            "Then after we present the algorithm and the analysis will.",
            "Provide some empirical results showing that this algorithm is able to outperform other methods other previously proposed methods on both vector based an graph based datasets."
        ],
        [
            "So let's talk about the weighted kernel K means objective function.",
            "So I've written the objective function up here.",
            "For those of you familiar with the K means objective function, standard squared Euclidean K means objective function.",
            "This is simply a generalization of that objective.",
            "So if we were to eliminate the gamma, I weights, and the function Phi, then it would be exactly equivalent to standard K means.",
            "That is, we would trying to minimize.",
            "The sum over all vectors X of the squared Euclidean distance between X and the centroid of the cluster that X is in.",
            "So in weighted kernel K means we generalize that in two ways.",
            "First, we introduce a weight gamma eye on each point.",
            "And therefore the centroid becomes a weighted centroid and Secondly.",
            "We introduce a mapping fee that Maps are original vectors XI to a generally higher dimensional space and that allows us, for example, to if we if we cluster in the higher dimensional space and K means finds linear separators.",
            "Those linear separators in the higher dimensional space will correspond to non linear separators in the input space.",
            "The algorithm for weighted kernel K means is analogous to the algorithm for K means.",
            "There's one fundamental difference.",
            "When we compute the distance between 5X I&M see what we do.",
            "If we expand that distance computation using the definition of squared Euclidean distance, we find that every occurrence of five XI is in terms of DOT products.",
            "5X I .5 XJ, so we assume that the input to the algorithm is not a set of vectors, but rather a kernel matrix K who's ijaaf element is 5X I .5 XJ.",
            "So instead of vectors as input, we assume the kernel matrix is input and the rest of the algorithm is the same.",
            "As long as K is a positive semi definite matrix then we can guarantee that every iteration of the weighted kernel K means algorithm will monotonically decrease this objective function."
        ],
        [
            "So why is weighted kernel K means interesting?",
            "Well, one result in previous work.",
            "Is that weighted?",
            "Kernel K means captures a number of previously proposed graph graph clustering objectives.",
            "For example, if we have a graph.",
            "And we want to cluster the nodes of the graph and we want to say minimize the normalized cut in the graph.",
            "What this result says is that if we construct a certain kernel matrix K and we set the node weights for weighted kernel K means in a certain way, then the objective function for weighted kernel K means is mathematically equivalent to the objective function for normalized cut.",
            "In other words, we construct this matrix.",
            "We set these weights and we're unweighted.",
            "Kernel K means every iteration of the weighted kernel K means algorithm will monotonically decrease the normalized cut.",
            "In the graph and the same thing holds for ratio cut.",
            "So what we want to do now is extend that to.",
            "Semi super."
        ],
        [
            "Rise clustering and show that the weighted kernel K means objective function also captures a number of semi supervised clustering objectives.",
            "Both vector based.",
            "An graph based.",
            "So a little earlier I mentioned the HMRFK means objective function.",
            "That's actually a broad.",
            "There's a broad class of objectives that that fall out of this HMRFK means framework.",
            "We're looking at one particular case.",
            "In this case, uses squared Euclidean distance and a certain class of penalty functions.",
            "But when we do that, we get a semi supervised clustering objective that I've written up here.",
            "So it's three terms.",
            "The first term is just the unsupervised K means term of the objective function, minimizing the distance from every point to the centroid of the cluster that it's in.",
            "The second term is based on the must link constraints.",
            "It says that for every must link Scion XJ.",
            "If XI and XJ are in the same cluster, that means they've satisfied that constraint.",
            "So we reward the objective function by subtracting.",
            "Some pre specified weight WIJ, that's inversely proportional to the size of the cluster that the points are in.",
            "Similarly, the third term in the objective function says for every.",
            "XI XJ.",
            "That's a cannot link.",
            "If they are in the same cluster, then that means we violated that that constraint, so we penalize the objective function and we penalize it.",
            "Some pre specified penalty waived WIJ, that's also inversely proportional to the size of the cluster that the points are in.",
            "So that's the objective function for HMRFK means, and now what we'd like to show or briefly outline, at least, is how that can be seen as a special case of the weighted kernel.",
            "K means objective function.",
            "Of course the details are."
        ],
        [
            "In the paper, but as it turns out, so we have to construct a certain kernel matrix and we have to set weights in a certain way for for this equivalence to hold, and the way that works is we build this matrix K that comes from that has two components, the sum of two matrices.",
            "The first is the similarity matrix S. That's just the matrix of dot product XYZ DOT XJ and that comes from the first term, this unsupervised term.",
            "The second part of the mate."
        ],
        [
            "It's K is a matrix W, which I'll call the constraint matrix.",
            "This matrix W it's IJ entry is this pre specified?",
            "WIJ wait for must links and negative WIJ for cannot link, and 0 otherwise.",
            "So if we build these two matrices we add them together.",
            "That's our kernel function.",
            "I mean our kernel matrix and we also have to guarantee that K is positive semi definite, which I'm sort of ignoring for now.",
            "And if we then set all the weights for weighted kernel K means to be one.",
            "Then this objective function is mathematically equivalent to the weighted kernel.",
            "K means objective function.",
            "So in other words, we can run weighted kernel K means to decrease this objective.",
            "Now."
        ],
        [
            "So we can do the same thing for graph clustering.",
            "So suppose we are interested in a semi supervised version of the normalized cut objective an using the same strategy as in HMRFK means we define a three term objective function based on the normalized cut.",
            "The first term is the standard normalized cut objective.",
            "The second term is the must link term and the third term is the cannot link term.",
            "So just as in HMRFK means we can show that this objective function.",
            "Also follows as a special case of the weighted kernel.",
            "K means objective function with the appropriate kernel matrix and weight."
        ],
        [
            "It's for weighted kernel K means Now the kernel matrix again is this sort of the sum of two components.",
            "The first comes from this unsupervised normalized cut term.",
            "And the second comes from the."
        ],
        [
            "The constraint matrix and the constraint terms.",
            "And in this case for semi supervised normalized cut we have we set the node weights for weighted kernel K means to be the degrees of the nodes in the graph.",
            "And when we do that then this objective function is mathematically equivalent to the weighted kernel K means objective."
        ],
        [
            "We can also generalize this to include other graph clustering objectives.",
            "For example, we could define a semi supervised ratio cut objective which looks very similar except instead of degree.",
            "Here we're using the size of the cluster.",
            "And there are other some other objectives that we look at in the paper as well.",
            "I mentioned earlier the spectral learning algorithm.",
            "It can be viewed in our framework as.",
            "As the natural spectral relaxation to the semisupervised ratio cut objective, I don't have time to go into details, but if we choose the constraint WJ, these penalty weights appropriately and we take a spectral relaxation, we obtain exactly the spectral learning algorithm.",
            "OK.",
            "So here's the."
        ],
        [
            "Here's just the algorithm spelled out.",
            "It assumes as input three matrices, a similarity matrix, a constraint matrix, and a diagonal weight matrix.",
            "So the similarity matrix for, say, HMRFK means is by default is just that dot product matrix XIXJ.",
            "Now we could think about extending HMR.",
            "FK means to use a kernel function, say Gaussian kernel, so our similarity matrix could be generated from that as well.",
            "And then for HMRFK means the node weight matrix is just the identity matrix and then 4 semi supervised normalized cut.",
            "That similarity matrix is just the input graph for semi supervised ratio, cut the.",
            "The similarity matrix is the negative of the Laplacian matrix, so that's our input and then the algorithm itself is actually quite simple.",
            "We form the kernel matrix.",
            "Here it is and its general form.",
            "We have some method of getting initial clusters using the constraints.",
            "Then we just run weighted kernel K means with the weights from the gamma matrix and we input K and the initial clustering and then we return the resulting clusters.",
            "So."
        ],
        [
            "In the time left, let's look at some experimental results on vector based in graph based data.",
            "For this we choose datasets that have some pre existing cluster labels that allows us to compute some level of accuracy.",
            "An we cluster the whole data set, but we break up the data into two sets, a training set and a test set.",
            "The training set is used to generate the random pairwise constraints and the test set is used to compute accuracy.",
            "And we plot."
        ],
        [
            "Learning curves using two fold cross validation and the X axis corresponds to the number of constraints that we have and the Y axis corresponds to our measure of accuracy, which is the mutual information that's normalized between zero and one.",
            "So one data set that we."
        ],
        [
            "Looked at is the is the pen digits data set, and for those of you not familiar with it, it's it takes handwritten digits and encodes them as 16 dimensional vectors, say vector based data set obviously, and so we looked at just a subset of three digits, 3, eight and nine and we ran three algorithms.",
            "We ran one in the middle.",
            "The blue line is the HMRFK means algorithm.",
            "As implemented by Basu at LO4 and then, the bottom line is our algorithm with just the default dot product matrix's similarity matrix and the top.",
            "The Red Line is our implementation of HMRFK means.",
            "If we use a Gaussian kernel to generate the similarity matrix, and because the data is not linearly separable, that Gaussian kernel similarity matrix with HMRFK means does significantly better than the other.",
            "Algorithms?"
        ],
        [
            "And then also we looked at a graph based data set.",
            "This is a yeast interaction network containing 216 used jeans and we ran three algorithms on this semi supervised normalized cut, semi supervised ratio.",
            "Cut an the spectral learning algorithm and in this case the norm semi supervised normalized cut outperforms the other methods.",
            "So."
        ],
        [
            "To conclude.",
            "We introduced a framework for.",
            "Semi supervised clustering that unifies both graph based approaches and vector based approaches.",
            "This framework captures a number of previously proposed objectives such as HMRFK means and spectral learning, as well as some new objectives that we introduced, such as such supervised normalized cut.",
            "And we showed on some real life datasets that this approach outperforms previous methods.",
            "On both vector based in graph based datasets.",
            "Thank you."
        ],
        [
            "Questions.",
            "Penalizing broken constraints, why don't we just incorporate these constraints to the optimization problem?",
            "You know the question was rather than penalized the constraints in the objective.",
            "I guess through this sum this three term.",
            "Why not incorporate it into the just constraints into an optimization problem?",
            "That is one way you could think about doing it.",
            "I haven't looked at that.",
            "I mean, the common approach that's done most most of the time in semi supervised clustering is to use one of these three term objective functions and that's that's it has this interpretation in terms of hidden Markov random fields.",
            "That's one of the main reasons why it's done, although you could certainly do it.",
            "You could try doing it in in terms of just adding constraints to an optimization problem, and I don't know how that would.",
            "How that would perform?",
            "Wait?",
            "The weights OK so.",
            "I can't remember offhand how it's sort of like.",
            "You have to sort of tune these weights.",
            "Oftentimes an I think in ours we we ended up just choosing some arbitrary weights to run on every single algorithm.",
            "Something like one or something.",
            "I don't remember offhand exactly, but I want to say it was like one or or some some constant that we just used universally throughout all of our tests.",
            "Just wondering how you set your bandwidth for the Gaussian filter.",
            "Then again, I don't remember off the top of my head.",
            "I think I think probably we we.",
            "We tried a bunch of different different values for Sigma and took whatever looks best.",
            "I mean, I don't, I don't, I don't think there was.",
            "No, no.",
            "I mean, I think I mean that's not really a crucial point of the of the algorithm.",
            "I mean we.",
            "The point is that this Gaussian kernel does significantly better and we didn't tune that measure at all.",
            "I think we just chose something, so we probably could get even better results if we if we tune that parameter.",
            "Question.",
            "Yep.",
            "Well.",
            "That's a good question.",
            "It's possible.",
            "I don't know for sure how you would do that off the top of my head, but it's definitely possible.",
            "We have to sort of talk about that.",
            "It's definitely not not something I've given a lot of thought.",
            "No further questions.",
            "Let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A Colonel approach.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Sugato Basu, Inderjit Dhillon and Ray Mooney at the University of Texas at Austin.",
                    "label": 1
                },
                {
                    "sent": "So the topic of this paper is semi supervised.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is clustering so First things first.",
                    "label": 0
                },
                {
                    "sent": "What is semi supervised clustering?",
                    "label": 0
                },
                {
                    "sent": "So I'll just give a very small example.",
                    "label": 0
                },
                {
                    "sent": "For those of you not familiar with it.",
                    "label": 0
                },
                {
                    "sent": "So suppose I have these points in space that I want to close.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, we might run a clustering algorithm on these points, and perhaps it will return these horozontal clusters as a result.",
                    "label": 0
                },
                {
                    "sent": "But the points are somewhat ambiguous, so.",
                    "label": 0
                },
                {
                    "sent": "It might be the case that another algorithm would return the vertical.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Busters as a result.",
                    "label": 0
                },
                {
                    "sent": "In semi supervised clustering we would like to be able to give the clustering algorithm some background information that will guide the clustering algorithm to some desired clustering so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That background information generally comes in the form of pairwise constraints, that is, pairs of points that should be in the same cluster an pairs of points that should be in different clusters.",
                    "label": 0
                },
                {
                    "sent": "In this case, we have the two red and the blue points.",
                    "label": 0
                },
                {
                    "sent": "We say those are.",
                    "label": 0
                },
                {
                    "sent": "Those should be in different clusters that have a cannot link, whereas the two blue points on the left and the two red points on the right.",
                    "label": 0
                },
                {
                    "sent": "They have must link constraints, so we use.",
                    "label": 0
                },
                {
                    "sent": "We give that to the algorithm and then the semi supervised clustering algorithm uses that to come up with the vertical clusters here.",
                    "label": 0
                },
                {
                    "sent": "So this isn't a new problem, it's been around for for awhile and it's been an active area of research.",
                    "label": 0
                },
                {
                    "sent": "One particular algorithm that will look at is called the HMRFHMRFK means algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's based on a probabilistic framework for semi supervised clustering using hidden Markov random fields.",
                    "label": 1
                },
                {
                    "sent": "It's a vector based approach, meaning that the input is assumed to be a set of vectors that we're clustering in addition to the constraint that we give it.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One drawback with algorithms such as HMRFK means is that in some cases the algorithm has some trouble.",
                    "label": 1
                },
                {
                    "sent": "For example, if the data itself is not linearly separable, then HMRFK means is unable to recover the true clusters unless we give it a very large number of constraints.",
                    "label": 1
                },
                {
                    "sent": "So in this case, in this case we have two circles, one inside the other and the algorithm is unable to recover those clusters.",
                    "label": 0
                },
                {
                    "sent": "Another problem with.",
                    "label": 0
                },
                {
                    "sent": "Some vector base.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms such as HMRFK means is that our data are input data may not be a set of vectors at all, but rather a graph where we hope to cluster the nodes of the graph into different clusters.",
                    "label": 0
                },
                {
                    "sent": "In this case, we have a social network here and of course vector base.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms are inappropriate.",
                    "label": 0
                },
                {
                    "sent": "So what's generally done for the unsupervised case is we run some sort of graph clustering algorithm, such as minimizing the normalized cut or ratio cut in the graph.",
                    "label": 0
                },
                {
                    "sent": "We might do that on on this graph, for example, but again, just as in the vector case, we might want to incorporate.",
                    "label": 0
                },
                {
                    "sent": "Constraints into the into our graph clustering algorithm to guide the class.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bring algorithm to a better cut of the graph.",
                    "label": 0
                },
                {
                    "sent": "So in this case we give it these must link and cannot link constraints and our algorithm would use those to come up with a different cut.",
                    "label": 0
                },
                {
                    "sent": "So one algorithm that does this sort of graph based semi supervised clustering is an algorithm called spectral learning and we'll talk about that and how that fits in with our framework.",
                    "label": 0
                },
                {
                    "sent": "For semi supervised clustering a little bit later.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's sort of the brief introduction to semi supervised clustering.",
                    "label": 0
                },
                {
                    "sent": "And now let's talk about what we do in this paper.",
                    "label": 0
                },
                {
                    "sent": "So in this paper, we're going to look at something that we that's called the weighted kernel.",
                    "label": 0
                },
                {
                    "sent": "K means objective function, and the first thing we'll do is introduce that and review a previous result that shows that the weighted kernel K means object.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Captures a number of common graph clustering objectives, such as normalized, cut and ratio cut as special cases, and So what we're going to do is we're going to take that objective function, and we're going to extend those results to show that a number of vector based and graph based semi supervised clustering objective functions also fall out as special cases.",
                    "label": 1
                },
                {
                    "sent": "So in a sense, the weighted kernel K means objective function will unify several vector based and graph based approaches to semi supervised clustering.",
                    "label": 0
                },
                {
                    "sent": "An one implication of this is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that will just have a single algorithm for clustering.",
                    "label": 0
                },
                {
                    "sent": "For semi supervised clustering given either vectors or a graph.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as I mentioned, several objective functions will fall out of our framework as special cases.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then after we present the algorithm and the analysis will.",
                    "label": 0
                },
                {
                    "sent": "Provide some empirical results showing that this algorithm is able to outperform other methods other previously proposed methods on both vector based an graph based datasets.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk about the weighted kernel K means objective function.",
                    "label": 0
                },
                {
                    "sent": "So I've written the objective function up here.",
                    "label": 0
                },
                {
                    "sent": "For those of you familiar with the K means objective function, standard squared Euclidean K means objective function.",
                    "label": 0
                },
                {
                    "sent": "This is simply a generalization of that objective.",
                    "label": 0
                },
                {
                    "sent": "So if we were to eliminate the gamma, I weights, and the function Phi, then it would be exactly equivalent to standard K means.",
                    "label": 0
                },
                {
                    "sent": "That is, we would trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "The sum over all vectors X of the squared Euclidean distance between X and the centroid of the cluster that X is in.",
                    "label": 0
                },
                {
                    "sent": "So in weighted kernel K means we generalize that in two ways.",
                    "label": 0
                },
                {
                    "sent": "First, we introduce a weight gamma eye on each point.",
                    "label": 0
                },
                {
                    "sent": "And therefore the centroid becomes a weighted centroid and Secondly.",
                    "label": 0
                },
                {
                    "sent": "We introduce a mapping fee that Maps are original vectors XI to a generally higher dimensional space and that allows us, for example, to if we if we cluster in the higher dimensional space and K means finds linear separators.",
                    "label": 0
                },
                {
                    "sent": "Those linear separators in the higher dimensional space will correspond to non linear separators in the input space.",
                    "label": 0
                },
                {
                    "sent": "The algorithm for weighted kernel K means is analogous to the algorithm for K means.",
                    "label": 1
                },
                {
                    "sent": "There's one fundamental difference.",
                    "label": 0
                },
                {
                    "sent": "When we compute the distance between 5X I&M see what we do.",
                    "label": 0
                },
                {
                    "sent": "If we expand that distance computation using the definition of squared Euclidean distance, we find that every occurrence of five XI is in terms of DOT products.",
                    "label": 1
                },
                {
                    "sent": "5X I .5 XJ, so we assume that the input to the algorithm is not a set of vectors, but rather a kernel matrix K who's ijaaf element is 5X I .5 XJ.",
                    "label": 1
                },
                {
                    "sent": "So instead of vectors as input, we assume the kernel matrix is input and the rest of the algorithm is the same.",
                    "label": 0
                },
                {
                    "sent": "As long as K is a positive semi definite matrix then we can guarantee that every iteration of the weighted kernel K means algorithm will monotonically decrease this objective function.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is weighted kernel K means interesting?",
                    "label": 0
                },
                {
                    "sent": "Well, one result in previous work.",
                    "label": 0
                },
                {
                    "sent": "Is that weighted?",
                    "label": 0
                },
                {
                    "sent": "Kernel K means captures a number of previously proposed graph graph clustering objectives.",
                    "label": 1
                },
                {
                    "sent": "For example, if we have a graph.",
                    "label": 0
                },
                {
                    "sent": "And we want to cluster the nodes of the graph and we want to say minimize the normalized cut in the graph.",
                    "label": 0
                },
                {
                    "sent": "What this result says is that if we construct a certain kernel matrix K and we set the node weights for weighted kernel K means in a certain way, then the objective function for weighted kernel K means is mathematically equivalent to the objective function for normalized cut.",
                    "label": 0
                },
                {
                    "sent": "In other words, we construct this matrix.",
                    "label": 1
                },
                {
                    "sent": "We set these weights and we're unweighted.",
                    "label": 0
                },
                {
                    "sent": "Kernel K means every iteration of the weighted kernel K means algorithm will monotonically decrease the normalized cut.",
                    "label": 1
                },
                {
                    "sent": "In the graph and the same thing holds for ratio cut.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do now is extend that to.",
                    "label": 0
                },
                {
                    "sent": "Semi super.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rise clustering and show that the weighted kernel K means objective function also captures a number of semi supervised clustering objectives.",
                    "label": 0
                },
                {
                    "sent": "Both vector based.",
                    "label": 0
                },
                {
                    "sent": "An graph based.",
                    "label": 0
                },
                {
                    "sent": "So a little earlier I mentioned the HMRFK means objective function.",
                    "label": 0
                },
                {
                    "sent": "That's actually a broad.",
                    "label": 0
                },
                {
                    "sent": "There's a broad class of objectives that that fall out of this HMRFK means framework.",
                    "label": 0
                },
                {
                    "sent": "We're looking at one particular case.",
                    "label": 0
                },
                {
                    "sent": "In this case, uses squared Euclidean distance and a certain class of penalty functions.",
                    "label": 0
                },
                {
                    "sent": "But when we do that, we get a semi supervised clustering objective that I've written up here.",
                    "label": 0
                },
                {
                    "sent": "So it's three terms.",
                    "label": 0
                },
                {
                    "sent": "The first term is just the unsupervised K means term of the objective function, minimizing the distance from every point to the centroid of the cluster that it's in.",
                    "label": 0
                },
                {
                    "sent": "The second term is based on the must link constraints.",
                    "label": 0
                },
                {
                    "sent": "It says that for every must link Scion XJ.",
                    "label": 0
                },
                {
                    "sent": "If XI and XJ are in the same cluster, that means they've satisfied that constraint.",
                    "label": 0
                },
                {
                    "sent": "So we reward the objective function by subtracting.",
                    "label": 0
                },
                {
                    "sent": "Some pre specified weight WIJ, that's inversely proportional to the size of the cluster that the points are in.",
                    "label": 0
                },
                {
                    "sent": "Similarly, the third term in the objective function says for every.",
                    "label": 0
                },
                {
                    "sent": "XI XJ.",
                    "label": 0
                },
                {
                    "sent": "That's a cannot link.",
                    "label": 0
                },
                {
                    "sent": "If they are in the same cluster, then that means we violated that that constraint, so we penalize the objective function and we penalize it.",
                    "label": 0
                },
                {
                    "sent": "Some pre specified penalty waived WIJ, that's also inversely proportional to the size of the cluster that the points are in.",
                    "label": 0
                },
                {
                    "sent": "So that's the objective function for HMRFK means, and now what we'd like to show or briefly outline, at least, is how that can be seen as a special case of the weighted kernel.",
                    "label": 0
                },
                {
                    "sent": "K means objective function.",
                    "label": 0
                },
                {
                    "sent": "Of course the details are.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the paper, but as it turns out, so we have to construct a certain kernel matrix and we have to set weights in a certain way for for this equivalence to hold, and the way that works is we build this matrix K that comes from that has two components, the sum of two matrices.",
                    "label": 0
                },
                {
                    "sent": "The first is the similarity matrix S. That's just the matrix of dot product XYZ DOT XJ and that comes from the first term, this unsupervised term.",
                    "label": 0
                },
                {
                    "sent": "The second part of the mate.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's K is a matrix W, which I'll call the constraint matrix.",
                    "label": 0
                },
                {
                    "sent": "This matrix W it's IJ entry is this pre specified?",
                    "label": 0
                },
                {
                    "sent": "WIJ wait for must links and negative WIJ for cannot link, and 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "So if we build these two matrices we add them together.",
                    "label": 0
                },
                {
                    "sent": "That's our kernel function.",
                    "label": 0
                },
                {
                    "sent": "I mean our kernel matrix and we also have to guarantee that K is positive semi definite, which I'm sort of ignoring for now.",
                    "label": 0
                },
                {
                    "sent": "And if we then set all the weights for weighted kernel K means to be one.",
                    "label": 0
                },
                {
                    "sent": "Then this objective function is mathematically equivalent to the weighted kernel.",
                    "label": 0
                },
                {
                    "sent": "K means objective function.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we can run weighted kernel K means to decrease this objective.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can do the same thing for graph clustering.",
                    "label": 0
                },
                {
                    "sent": "So suppose we are interested in a semi supervised version of the normalized cut objective an using the same strategy as in HMRFK means we define a three term objective function based on the normalized cut.",
                    "label": 0
                },
                {
                    "sent": "The first term is the standard normalized cut objective.",
                    "label": 0
                },
                {
                    "sent": "The second term is the must link term and the third term is the cannot link term.",
                    "label": 0
                },
                {
                    "sent": "So just as in HMRFK means we can show that this objective function.",
                    "label": 0
                },
                {
                    "sent": "Also follows as a special case of the weighted kernel.",
                    "label": 0
                },
                {
                    "sent": "K means objective function with the appropriate kernel matrix and weight.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's for weighted kernel K means Now the kernel matrix again is this sort of the sum of two components.",
                    "label": 0
                },
                {
                    "sent": "The first comes from this unsupervised normalized cut term.",
                    "label": 0
                },
                {
                    "sent": "And the second comes from the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The constraint matrix and the constraint terms.",
                    "label": 0
                },
                {
                    "sent": "And in this case for semi supervised normalized cut we have we set the node weights for weighted kernel K means to be the degrees of the nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "And when we do that then this objective function is mathematically equivalent to the weighted kernel K means objective.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can also generalize this to include other graph clustering objectives.",
                    "label": 1
                },
                {
                    "sent": "For example, we could define a semi supervised ratio cut objective which looks very similar except instead of degree.",
                    "label": 0
                },
                {
                    "sent": "Here we're using the size of the cluster.",
                    "label": 0
                },
                {
                    "sent": "And there are other some other objectives that we look at in the paper as well.",
                    "label": 0
                },
                {
                    "sent": "I mentioned earlier the spectral learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "It can be viewed in our framework as.",
                    "label": 0
                },
                {
                    "sent": "As the natural spectral relaxation to the semisupervised ratio cut objective, I don't have time to go into details, but if we choose the constraint WJ, these penalty weights appropriately and we take a spectral relaxation, we obtain exactly the spectral learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here's the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's just the algorithm spelled out.",
                    "label": 0
                },
                {
                    "sent": "It assumes as input three matrices, a similarity matrix, a constraint matrix, and a diagonal weight matrix.",
                    "label": 1
                },
                {
                    "sent": "So the similarity matrix for, say, HMRFK means is by default is just that dot product matrix XIXJ.",
                    "label": 0
                },
                {
                    "sent": "Now we could think about extending HMR.",
                    "label": 0
                },
                {
                    "sent": "FK means to use a kernel function, say Gaussian kernel, so our similarity matrix could be generated from that as well.",
                    "label": 0
                },
                {
                    "sent": "And then for HMRFK means the node weight matrix is just the identity matrix and then 4 semi supervised normalized cut.",
                    "label": 0
                },
                {
                    "sent": "That similarity matrix is just the input graph for semi supervised ratio, cut the.",
                    "label": 0
                },
                {
                    "sent": "The similarity matrix is the negative of the Laplacian matrix, so that's our input and then the algorithm itself is actually quite simple.",
                    "label": 0
                },
                {
                    "sent": "We form the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Here it is and its general form.",
                    "label": 0
                },
                {
                    "sent": "We have some method of getting initial clusters using the constraints.",
                    "label": 1
                },
                {
                    "sent": "Then we just run weighted kernel K means with the weights from the gamma matrix and we input K and the initial clustering and then we return the resulting clusters.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the time left, let's look at some experimental results on vector based in graph based data.",
                    "label": 0
                },
                {
                    "sent": "For this we choose datasets that have some pre existing cluster labels that allows us to compute some level of accuracy.",
                    "label": 0
                },
                {
                    "sent": "An we cluster the whole data set, but we break up the data into two sets, a training set and a test set.",
                    "label": 0
                },
                {
                    "sent": "The training set is used to generate the random pairwise constraints and the test set is used to compute accuracy.",
                    "label": 0
                },
                {
                    "sent": "And we plot.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning curves using two fold cross validation and the X axis corresponds to the number of constraints that we have and the Y axis corresponds to our measure of accuracy, which is the mutual information that's normalized between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So one data set that we.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looked at is the is the pen digits data set, and for those of you not familiar with it, it's it takes handwritten digits and encodes them as 16 dimensional vectors, say vector based data set obviously, and so we looked at just a subset of three digits, 3, eight and nine and we ran three algorithms.",
                    "label": 1
                },
                {
                    "sent": "We ran one in the middle.",
                    "label": 0
                },
                {
                    "sent": "The blue line is the HMRFK means algorithm.",
                    "label": 0
                },
                {
                    "sent": "As implemented by Basu at LO4 and then, the bottom line is our algorithm with just the default dot product matrix's similarity matrix and the top.",
                    "label": 0
                },
                {
                    "sent": "The Red Line is our implementation of HMRFK means.",
                    "label": 0
                },
                {
                    "sent": "If we use a Gaussian kernel to generate the similarity matrix, and because the data is not linearly separable, that Gaussian kernel similarity matrix with HMRFK means does significantly better than the other.",
                    "label": 0
                },
                {
                    "sent": "Algorithms?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then also we looked at a graph based data set.",
                    "label": 0
                },
                {
                    "sent": "This is a yeast interaction network containing 216 used jeans and we ran three algorithms on this semi supervised normalized cut, semi supervised ratio.",
                    "label": 0
                },
                {
                    "sent": "Cut an the spectral learning algorithm and in this case the norm semi supervised normalized cut outperforms the other methods.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "We introduced a framework for.",
                    "label": 1
                },
                {
                    "sent": "Semi supervised clustering that unifies both graph based approaches and vector based approaches.",
                    "label": 0
                },
                {
                    "sent": "This framework captures a number of previously proposed objectives such as HMRFK means and spectral learning, as well as some new objectives that we introduced, such as such supervised normalized cut.",
                    "label": 1
                },
                {
                    "sent": "And we showed on some real life datasets that this approach outperforms previous methods.",
                    "label": 0
                },
                {
                    "sent": "On both vector based in graph based datasets.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Penalizing broken constraints, why don't we just incorporate these constraints to the optimization problem?",
                    "label": 0
                },
                {
                    "sent": "You know the question was rather than penalized the constraints in the objective.",
                    "label": 0
                },
                {
                    "sent": "I guess through this sum this three term.",
                    "label": 0
                },
                {
                    "sent": "Why not incorporate it into the just constraints into an optimization problem?",
                    "label": 0
                },
                {
                    "sent": "That is one way you could think about doing it.",
                    "label": 0
                },
                {
                    "sent": "I haven't looked at that.",
                    "label": 0
                },
                {
                    "sent": "I mean, the common approach that's done most most of the time in semi supervised clustering is to use one of these three term objective functions and that's that's it has this interpretation in terms of hidden Markov random fields.",
                    "label": 0
                },
                {
                    "sent": "That's one of the main reasons why it's done, although you could certainly do it.",
                    "label": 0
                },
                {
                    "sent": "You could try doing it in in terms of just adding constraints to an optimization problem, and I don't know how that would.",
                    "label": 0
                },
                {
                    "sent": "How that would perform?",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "The weights OK so.",
                    "label": 0
                },
                {
                    "sent": "I can't remember offhand how it's sort of like.",
                    "label": 0
                },
                {
                    "sent": "You have to sort of tune these weights.",
                    "label": 0
                },
                {
                    "sent": "Oftentimes an I think in ours we we ended up just choosing some arbitrary weights to run on every single algorithm.",
                    "label": 0
                },
                {
                    "sent": "Something like one or something.",
                    "label": 0
                },
                {
                    "sent": "I don't remember offhand exactly, but I want to say it was like one or or some some constant that we just used universally throughout all of our tests.",
                    "label": 0
                },
                {
                    "sent": "Just wondering how you set your bandwidth for the Gaussian filter.",
                    "label": 0
                },
                {
                    "sent": "Then again, I don't remember off the top of my head.",
                    "label": 0
                },
                {
                    "sent": "I think I think probably we we.",
                    "label": 0
                },
                {
                    "sent": "We tried a bunch of different different values for Sigma and took whatever looks best.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't, I don't, I don't think there was.",
                    "label": 0
                },
                {
                    "sent": "No, no.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think I mean that's not really a crucial point of the of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "I mean we.",
                    "label": 0
                },
                {
                    "sent": "The point is that this Gaussian kernel does significantly better and we didn't tune that measure at all.",
                    "label": 0
                },
                {
                    "sent": "I think we just chose something, so we probably could get even better results if we if we tune that parameter.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "It's possible.",
                    "label": 0
                },
                {
                    "sent": "I don't know for sure how you would do that off the top of my head, but it's definitely possible.",
                    "label": 0
                },
                {
                    "sent": "We have to sort of talk about that.",
                    "label": 0
                },
                {
                    "sent": "It's definitely not not something I've given a lot of thought.",
                    "label": 0
                },
                {
                    "sent": "No further questions.",
                    "label": 0
                },
                {
                    "sent": "Let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}