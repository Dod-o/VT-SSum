{
    "id": "lytyew2m45nbc6hpqqv7iadmgujmetvp",
    "title": "Knowledge Guided Attention and Inference for Describing Images Containing Unseen Objects",
    "info": {
        "author": [
            "Achim Rettinger, Institute of Applied Informatics and Formal Description Methods (AIFB), Karlsruhe Institute of Technology (KIT)"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_rettinger_unseen_objects/",
    "segmentation": [
        [
            "Thank you for the introduction.",
            "This is joint work with mostly with Aditya who can't be here today, so I'm giving the talk but also with manga and lexing from the Australian National University.",
            "Just to give you a little bit of context first.",
            "I will explain how this fits into some of the stuff that we have done before, so you might have seen their apples and oranges from a previous talk at isw.",
            "See the whole talk is situated in the context that we are trying to learn.",
            "Knowledge representation that go across modality.",
            "So we try to."
        ],
        [
            "Include not only the information from the Knowledge Graph but also from textin from.",
            "From images and throughout my talk I will use this metaphor representing image information as an orange text information as in Apple and the information from a knowledge graph as a pair.",
            "Since it's for a computer, it's really hard to integrate this right for them.",
            "It's like comparing Apple and Orange, and so This is why I'm using this.",
            "OK, we started all this research with the with the."
        ],
        [
            "Question if we can aggregate complementing information in those different representation into beneficial representations or in the end we did some representation learning across the modalities.",
            "And we found out, and this was the talk I gave at ISWC the last isw see that yes, you this some of the some of the standard knowledge graph benchmarks can really benefit a lot from this additional information.",
            "And then we had another paper at K-Cup where we tried to extrapolate this information.",
            "So the problem so far has been so if we have a joint representation that is learned, we can only do this for entities that we have data for training data for in both modalities.",
            "So for example, like text and images, there's only 1500 entities that we can identify in both modalities and we try to kind of transfer this information.",
            "Extrapolate this information also to entities that have not been in all of the modalities.",
            "So in this example we have word embeddings that we enrich with cross modal information, but also for.",
            "Entities that are very don't have visual information for and we found out yes, this is also possible.",
            "You can do this and specifically more concrete entities can benefit from it.",
            "OK."
        ],
        [
            "Coming to the next step that I'm presenting today is there.",
            "We want to extrapolate knowledge about the translation from one modality into the other modality and so so The thing is, we get one modality want to translate the information into the other modality part for some of the entities we don't have training information about how this transformation works, and this is what this work is about.",
            "So the idea is you start with an image we want to describe it with text, but for some entities where we don't have seen training information, how this is done.",
            "And we want to use the Knowledge Graph to kind of support this.",
            "OK.",
            "So this is what I'm presenting now.",
            "But so far about."
        ],
        [
            "At the general set up now about the specific task that we're tackling here, so it's about Image caption Generation, a task that has been.",
            "We are quite successful specifically in this deep learning area, but I will."
        ],
        [
            "Introduce it a little bit more for this audience so.",
            "What you typically need for this is 2 components.",
            "The first component is an image classifier, so you basically take a standard convolutional neural network, which we have seen in the previous talk has quite some good success stories in recent years so.",
            "Well, since we have this image, net competitions and large scale datasets and nice convolutional neural networks that do this very well.",
            "So now we can do this for thousands of object categories were a couple of years ago.",
            "This was only possible for whatever ten 10s or something so.",
            "In our experiments we have something like 600 to 700 objects that we cover.",
            "In this.",
            "Image."
        ],
        [
            "Action generation task the number of entities that can be captured is much smaller, so the standard data set here is this Ms Coco and they only have 80 entities that they that they have training data for.",
            "Of course this is you need much more complex training data, right?",
            "You need image, you need the text, the caption that you want to generate four, and the entities that you want to describe in the images.",
            "You need a lot of examples for this, right?",
            "You cannot have like 1.",
            "Image in the sentence pair one entity appears, but you need a lot of those pairs.",
            "OK, so this is kind of the restriction.",
            "We can have image classifiers that classify a lot of those concepts, but for the training data that you have for this Image, caption generation generation data, you have much less entities.",
            "So what do you?"
        ],
        [
            "From a model that does this task, is your input here the visual information and the expected model should output something like this, right?",
            "The sentence that says a man is holding a pizza in his hands.",
            "So you get text in the end.",
            "What happens if you now do this with a model that hasn't seen any Image caption training pairs that contain the term pizza?",
            "You get something like this, so it's typically something that is.",
            "If you're lucky somehow related to the term, often it's.",
            "Some some more abstract term, but it has to be from from the vocabulary of entities that is in the training data."
        ],
        [
            "There is some previous work on tackling this task of missing entities in the Image caption training data, but they have basically two drawbacks.",
            "This is the one thing is, an attention mechanism is missing and the other one is that the the inference is very limited.",
            "So what I mean by that I will explain in a second so."
        ],
        [
            "Attention.",
            "The purpose of this attention mechanism is that we that we add in our in our approach that we want to kind of find the salient entities in the image.",
            "So we want to learn which are the salient parts of the image that needs to be described in which are not that important.",
            "This is what we.",
            "That we do with the attention mechanism, and we also contribute to the the inference problem.",
            "So the inference problem here means you have you had the train model knows how to generate text, but not for those entities.",
            "So you need to.",
            "Be able to infer that a certain entity needs to be replaced, and it's not the correct entity that is in the image, right?",
            "This is the inference task, and we do this before enduring inference.",
            "The other methods to either or."
        ],
        [
            "OK, so now details about our approach.",
            "We propose."
        ],
        [
            "Two components to solve this, the one the first one is called explicit semantic attention module and the second one is this constraint inference mode module.",
            "And both of them are basically relying on background knowledge in the form of knowledge graphs.",
            "Yeah so."
        ],
        [
            "Don't get afraid, it's a little bit a lot on one slide, but I will go slowly through the slide and explain all the components that are in here.",
            "So for everyone who knows about this image, captioning with deep learning approaches, it's not that unusual.",
            "Basically, this is the part that we added, plus this layer, and those elements are basically in in the baseline models in this.",
            "Area so, but let me explain what what is here.",
            "So let's start with this part.",
            "This is just the language model.",
            "So in this case it's a two layer L STM which encodes a sequence of words and outputs.",
            "The next best word.",
            "So you always input a bird and it predicts what's the next best word.",
            "Input the generated work in the next one and it kind of always predicts the next word.",
            "Now you need to condition this somehow on the image information right?",
            "Because this is what you want to generate and the standard model.",
            "Takes the image, takes some sort of classifier on this image and extract some sort of visual features that are then in some sense used for.",
            "Predicting the next word.",
            "This is the standard model.",
            "What we add on top.",
            "Is first of all multi entity label classifiers so.",
            "You get an image you classify, not by some visual features, but by entities that are in the Knowledge graph.",
            "So you just need to train a standard convolutional network to predict basically your eyes in a knowledge graph and not some words in the image.",
            "This is done basically here, so we get some out.",
            "We identify where they are in the knowledge graph, and then we just pick some.",
            "Knowledge Graph, entity embedding representation of the entities that we identified and feed them also into this model.",
            "What we need to make sense of this is this additional layer which basically mixes.",
            "The text, the semantic information and the visual information together and has like this child cross modal representation and then this is used to predict the outcome of so the next word that should be generated.",
            "So in fact it's not that different and not so difficult.",
            "We'll go a little bit."
        ],
        [
            "Into the math here.",
            "So the better is basically the attention weight that we want to calculate.",
            "It's based on a word here on a word that at a specific point in time.",
            "So it's kind of it weighs.",
            "It's a scalar that weighs.",
            "Each entity given the word in the word sequence and it's calculated basically by a weight matrix.",
            "This needs to be learned times the hidden cell state of the second layer, so this square meters a second layer of the STM.",
            "That's the output layoff the STM times the entity embedding right?",
            "Then we do this for all of the embeddings that we get identified through the image classifier.",
            "So we get what we get is a better and this better basically tells us the higher the better is, the more salient this entity is in this image.",
            "To capture all of the entities that we identified, we just build a weighted sum of the entity, embedding them by the better that we calculate, and we call this the context."
        ],
        [
            "Yeah, and then how?",
            "We?",
            "This is what we feed into the system and we need to basically then take care of this in this TSV layer, which stands for text semantics and.",
            "Visual information so and it basically has those three components, so this is the information from the language model, so it's not a square, it's just the second layer of the STM.",
            "This is the context vector, so the knowledge graph information and this is the visual features that we get from the standard approach.",
            "We get 3 weight matrices right?",
            "So just linear conversion matrices that we have to learn which map from the internal hidden state so they have all different dimensions here to the dimension of the vocabulary of entities that we can produce.",
            "Or birds that we not entities words that we can produce with our.",
            "Language model.",
            "OK, then we run a standard softmax on this, and this basically predicts the system next most likely words or the top whatever next most likely words.",
            "Optimized as a standard optimization that we do here so it's the learning signal is the lock loss.",
            "And yeah, we minimize the negative log loss here of basically the word that is predicted to the word that is in the training data, right?",
            "And here you immediately see it's only possible for the words that are in the training data."
        ],
        [
            "Yeah, so in the end we have those for weight matrices.",
            "We optimize this with standard tools and standard backpropagation and so.",
            "OK, just a quick.",
            "Look into this inference.",
            "So.",
            "Yeah, so I probably just explained it.",
            "It's much easier so.",
            "What you now have to do is you have learned the weights for all of the known entities, but now you need to also know the weights for the unknown entities and to do this.",
            "You basically just take the existing weight matrix extended with the unseen object and then just look by some kind of cosine embedding similarity for the most similar entities to an existing entities and you just use a kind of a copying mechanism too.",
            "Copy the.",
            "Copy the weights from the known entities to the most similar unknown entities.",
            "You can do why we call this what we call this constraint and inferences because you can put some sort of inferences or rules on top of that.",
            "For example, you can use something like hand coded rules like different animals of the same type shouldn't appear in the same image.",
            "That might not hold for like general Pictures, but if you have seen those kind of image captioning datasets this always holds right?",
            "So maybe a kind of a cheating, but this is something that you could add here.",
            "Yeah, so this is captured here, but you could also use more.",
            "More complex rules."
        ],
        [
            "OK.",
            "So this is about the model.",
            "I skipped the part about the inference during word generation.",
            "I just for time reasons.",
            "But let's go to the evaluation."
        ],
        [
            "So it's just the standard evaluation where you leave some of the concepts or entities out of the predictions during trading.",
            "So we got rid of I think, 8 different concepts.",
            "Yeah, the setup is basically had eight held or project.",
            "This is the kind of the size of the caption data that you have in Ms Coco, so it's a lot this kind of datasets are very hard to obtain.",
            "For training our entity as a image to entity classifier used standard CNN architecture.",
            "And for also for later experiments we not only used the the text that is in Ms Coco, right the captions, but also more data.",
            "I will go into this a little bit later.",
            "We used RDF to work as the vector embeddings.",
            "And we reported different evaluation metrics where the 1st two are basically taken from the translation machine translation community.",
            "They measure how good a generated sentence is similar to the gold standard sentence and the F1 score basically just measures how good are we in predicting the correct entities or generating a sentence that contains the correct entity."
        ],
        [
            "OK, some qualitative results.",
            "For this standard task, just an Ms Coco data.",
            "Yeah, so we predict.",
            "This is what the image our image classifier predicts or predict.",
            "Something separate enclosure, Sue and so on.",
            "The base method as I showed before is not able to do this because several is just not in the as we've seen.",
            "Just seen be left this out.",
            "It's not in their capture generation data.",
            "So the task is somehow to replace animal with zebra.",
            "So the other methods can some of the other methods can pick the right entity here.",
            "But yeah, somehow also did the language that we generate looks a little bit more precise than before.",
            "What may be interesting is that often what happens is that more specific term is replaced with a more general one.",
            "OK, so then the quantitative analysis."
        ],
        [
            "Here that's the F1 score.",
            "It's a little bit mixed.",
            "And I can see.",
            "Can you distinguish the bold ones from so the underlined ones are the second best and the bold ones are the best?",
            "From here I cannot see the difference, but those are all bold and this one is bold.",
            "This one is bold and.",
            "I think yeah, so in most of the cases our model performs best, but in some cases also the other models perform best.",
            "Sometimes we are second best.",
            "OK, so this basically predicts how often we pick the right entity.",
            "In the different categories.",
            "So you have to imagine this like 1000."
        ],
        [
            "Of examples here.",
            "So averaged over thousands of examples, this is the score about how good the language is that we generated, and again those are the bold ones.",
            "And those are boats, and this is a bold one.",
            "OK, so again, in most of the cases our model performs better than the ones without those knowledge graph information.",
            "Then we did another expense."
        ],
        [
            "Meant.",
            "On top of that, where we thought, oh maybe we can also scale to outside of this Ms Coco.",
            "So right now we are in this 80 -- 8 objects and we thought since there are classifiers out there, the image net classifiers that classify whatever around 1000.",
            "So we had the 642.",
            "Why can't we try to evaluate this on on larger datasets in tears now?",
            "Examples of basically when we scale this from from those 82 to 800 almost.",
            "Yeah, examples what could be generated there and again you see some.",
            "Some patterns where we basically replace some abstract concept with something more specific.",
            "Yeah, we also did.",
            "Kind of the F1 score here."
        ],
        [
            "This is only.",
            "Trained on this Ms Coco data set and for this we then basically replaced the language model and the image classifier by language model versus trained on a larger corpus.",
            "Written here and basically image this image net classifier and as you can see, if you then evaluate this on also data from image net where you just say I can we predict the correct can we generate a sentence with the correct entity in it?",
            "This performs much better and we haven't seen any compatible algorithm doing this."
        ],
        [
            "OK, so summing up.",
            "We had this task where we want to extrapolate this kind of translation between modalities and we can say yes using knowledge graph embeddings in this attention mechanism that this can work."
        ],
        [
            "At least to some extent.",
            "I mean, there are still a lot of open issues here.",
            "Those are the references.",
            "That's my last slide, just as a summary, so I had this four parts in my presentation.",
            "About the general set up, then the task, our model and the evaluation maybe.",
            "2 interesting lessons learned is that the model somehow learns.",
            "I mean, this is also for future investigations to pick a more specific entity, which probably comes from the in the embeddings.",
            "The hierarchy is somehow encoded.",
            "And what's also interesting is how far we can scale.",
            "So this looked like we can just scale by basically the whatever the image classifier can give us, which is not true.",
            "So we notice that it's really dependent on the domain.",
            "So if you have images like in Ms Coco, that's on a certain domain, as soon as you try to do image captioning generation for images that are really from a different domain, so the language to describe those type of.",
            "Images is different.",
            "That system completely fails so.",
            "In this case, you still need this kind of image captioning parallel data to really perform well.",
            "OK, so let me finish here and I'm happy to take questions.",
            "Thanks very nice follow up work.",
            "I just have questions regarding possible extension.",
            "I don't know if you look at this, there are more and more large copper offering emerging.",
            "World image captioning translations basically.",
            "Because you have those knowledge graph embedding's that potentially have labels of things in many many languages.",
            "Do you think that can also help to obtain those translated captions?",
            "'cause there are international changes about this.",
            "Now we have a lot of training data, yeah?",
            "So.",
            "What's funny is we have not yet published model for generating captions in different languages.",
            "But that doesn't have this knowledge graph attention mechanism yet.",
            "So.",
            "I I guess it.",
            "It could help.",
            "So in the.",
            "The problem is that languages are very different so that.",
            "So like the structure of languages is very different.",
            "So here it's all about identifying the correct entity to describe this right so?",
            "I'm not sure if the Knowledge Graph helps in the sense of really generating this kind of caption data that tells you how to describe an image in different languages.",
            "That's probably pretty hard.",
            "What's easiest, of course, maybe you can generate training data.",
            "Through the language links that you have in such a graph.",
            "But I'm not sure about if it really helps, or at least it's not obvious how it really helps for multilingual caption generation.",
            "More questions.",
            "Very nice dog thanks.",
            "Have you also thought about applying this to image classification tasks and or do you know of any papers or but, uh, why like why would you do this immediately for a more difficult task?",
            "Like image captioning?",
            "Yeah, so I mean we have in the paper we have also the evaluation and.",
            "About just the image to entity classification task, right?",
            "Oh I see.",
            "Yeah so.",
            "But this is this is really standard methodology.",
            "There's no attention mechanism in it, it's just a convolutional neural network that links from an image to your eyes in the in the graph, right?",
            "It's.",
            "OK, and you see improvement in that case as well.",
            "Um, we are not aware of any approaches that link from my image to Knowledge Graph entity.",
            "So maybe I don't get your first, so I see no.",
            "I mean just regular image classification benchmarks like Image, Net and then you use your approach with the using the Knowledge graph from the image and like you have this extract entities.",
            "Oh no, OK. No, I think it doesn't actually work in this case.",
            "With your approach.",
            "I guess it would be interesting to see if this kind of if you have captured data.",
            "If this helps in the in also identifying the correct end entity, which I guess would be possible.",
            "Yeah, I was thinking about seeing graphs.",
            "I think I maybe I make something up, but yeah, I mean this is the next step.",
            "Going to seeing graphs and relations and so on.",
            "OK. With time for another question maybe.",
            "So if there are no more questions, then let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for the introduction.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with mostly with Aditya who can't be here today, so I'm giving the talk but also with manga and lexing from the Australian National University.",
                    "label": 0
                },
                {
                    "sent": "Just to give you a little bit of context first.",
                    "label": 0
                },
                {
                    "sent": "I will explain how this fits into some of the stuff that we have done before, so you might have seen their apples and oranges from a previous talk at isw.",
                    "label": 0
                },
                {
                    "sent": "See the whole talk is situated in the context that we are trying to learn.",
                    "label": 0
                },
                {
                    "sent": "Knowledge representation that go across modality.",
                    "label": 0
                },
                {
                    "sent": "So we try to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Include not only the information from the Knowledge Graph but also from textin from.",
                    "label": 0
                },
                {
                    "sent": "From images and throughout my talk I will use this metaphor representing image information as an orange text information as in Apple and the information from a knowledge graph as a pair.",
                    "label": 0
                },
                {
                    "sent": "Since it's for a computer, it's really hard to integrate this right for them.",
                    "label": 0
                },
                {
                    "sent": "It's like comparing Apple and Orange, and so This is why I'm using this.",
                    "label": 0
                },
                {
                    "sent": "OK, we started all this research with the with the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question if we can aggregate complementing information in those different representation into beneficial representations or in the end we did some representation learning across the modalities.",
                    "label": 0
                },
                {
                    "sent": "And we found out, and this was the talk I gave at ISWC the last isw see that yes, you this some of the some of the standard knowledge graph benchmarks can really benefit a lot from this additional information.",
                    "label": 0
                },
                {
                    "sent": "And then we had another paper at K-Cup where we tried to extrapolate this information.",
                    "label": 0
                },
                {
                    "sent": "So the problem so far has been so if we have a joint representation that is learned, we can only do this for entities that we have data for training data for in both modalities.",
                    "label": 0
                },
                {
                    "sent": "So for example, like text and images, there's only 1500 entities that we can identify in both modalities and we try to kind of transfer this information.",
                    "label": 0
                },
                {
                    "sent": "Extrapolate this information also to entities that have not been in all of the modalities.",
                    "label": 0
                },
                {
                    "sent": "So in this example we have word embeddings that we enrich with cross modal information, but also for.",
                    "label": 0
                },
                {
                    "sent": "Entities that are very don't have visual information for and we found out yes, this is also possible.",
                    "label": 0
                },
                {
                    "sent": "You can do this and specifically more concrete entities can benefit from it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coming to the next step that I'm presenting today is there.",
                    "label": 0
                },
                {
                    "sent": "We want to extrapolate knowledge about the translation from one modality into the other modality and so so The thing is, we get one modality want to translate the information into the other modality part for some of the entities we don't have training information about how this transformation works, and this is what this work is about.",
                    "label": 0
                },
                {
                    "sent": "So the idea is you start with an image we want to describe it with text, but for some entities where we don't have seen training information, how this is done.",
                    "label": 0
                },
                {
                    "sent": "And we want to use the Knowledge Graph to kind of support this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is what I'm presenting now.",
                    "label": 0
                },
                {
                    "sent": "But so far about.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the general set up now about the specific task that we're tackling here, so it's about Image caption Generation, a task that has been.",
                    "label": 0
                },
                {
                    "sent": "We are quite successful specifically in this deep learning area, but I will.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduce it a little bit more for this audience so.",
                    "label": 0
                },
                {
                    "sent": "What you typically need for this is 2 components.",
                    "label": 0
                },
                {
                    "sent": "The first component is an image classifier, so you basically take a standard convolutional neural network, which we have seen in the previous talk has quite some good success stories in recent years so.",
                    "label": 0
                },
                {
                    "sent": "Well, since we have this image, net competitions and large scale datasets and nice convolutional neural networks that do this very well.",
                    "label": 0
                },
                {
                    "sent": "So now we can do this for thousands of object categories were a couple of years ago.",
                    "label": 0
                },
                {
                    "sent": "This was only possible for whatever ten 10s or something so.",
                    "label": 0
                },
                {
                    "sent": "In our experiments we have something like 600 to 700 objects that we cover.",
                    "label": 0
                },
                {
                    "sent": "In this.",
                    "label": 0
                },
                {
                    "sent": "Image.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Action generation task the number of entities that can be captured is much smaller, so the standard data set here is this Ms Coco and they only have 80 entities that they that they have training data for.",
                    "label": 1
                },
                {
                    "sent": "Of course this is you need much more complex training data, right?",
                    "label": 0
                },
                {
                    "sent": "You need image, you need the text, the caption that you want to generate four, and the entities that you want to describe in the images.",
                    "label": 0
                },
                {
                    "sent": "You need a lot of examples for this, right?",
                    "label": 0
                },
                {
                    "sent": "You cannot have like 1.",
                    "label": 0
                },
                {
                    "sent": "Image in the sentence pair one entity appears, but you need a lot of those pairs.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of the restriction.",
                    "label": 1
                },
                {
                    "sent": "We can have image classifiers that classify a lot of those concepts, but for the training data that you have for this Image, caption generation generation data, you have much less entities.",
                    "label": 0
                },
                {
                    "sent": "So what do you?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From a model that does this task, is your input here the visual information and the expected model should output something like this, right?",
                    "label": 0
                },
                {
                    "sent": "The sentence that says a man is holding a pizza in his hands.",
                    "label": 1
                },
                {
                    "sent": "So you get text in the end.",
                    "label": 0
                },
                {
                    "sent": "What happens if you now do this with a model that hasn't seen any Image caption training pairs that contain the term pizza?",
                    "label": 0
                },
                {
                    "sent": "You get something like this, so it's typically something that is.",
                    "label": 0
                },
                {
                    "sent": "If you're lucky somehow related to the term, often it's.",
                    "label": 0
                },
                {
                    "sent": "Some some more abstract term, but it has to be from from the vocabulary of entities that is in the training data.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is some previous work on tackling this task of missing entities in the Image caption training data, but they have basically two drawbacks.",
                    "label": 0
                },
                {
                    "sent": "This is the one thing is, an attention mechanism is missing and the other one is that the the inference is very limited.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by that I will explain in a second so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Attention.",
                    "label": 0
                },
                {
                    "sent": "The purpose of this attention mechanism is that we that we add in our in our approach that we want to kind of find the salient entities in the image.",
                    "label": 1
                },
                {
                    "sent": "So we want to learn which are the salient parts of the image that needs to be described in which are not that important.",
                    "label": 0
                },
                {
                    "sent": "This is what we.",
                    "label": 0
                },
                {
                    "sent": "That we do with the attention mechanism, and we also contribute to the the inference problem.",
                    "label": 0
                },
                {
                    "sent": "So the inference problem here means you have you had the train model knows how to generate text, but not for those entities.",
                    "label": 0
                },
                {
                    "sent": "So you need to.",
                    "label": 0
                },
                {
                    "sent": "Be able to infer that a certain entity needs to be replaced, and it's not the correct entity that is in the image, right?",
                    "label": 1
                },
                {
                    "sent": "This is the inference task, and we do this before enduring inference.",
                    "label": 0
                },
                {
                    "sent": "The other methods to either or.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now details about our approach.",
                    "label": 0
                },
                {
                    "sent": "We propose.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two components to solve this, the one the first one is called explicit semantic attention module and the second one is this constraint inference mode module.",
                    "label": 0
                },
                {
                    "sent": "And both of them are basically relying on background knowledge in the form of knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't get afraid, it's a little bit a lot on one slide, but I will go slowly through the slide and explain all the components that are in here.",
                    "label": 0
                },
                {
                    "sent": "So for everyone who knows about this image, captioning with deep learning approaches, it's not that unusual.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is the part that we added, plus this layer, and those elements are basically in in the baseline models in this.",
                    "label": 0
                },
                {
                    "sent": "Area so, but let me explain what what is here.",
                    "label": 0
                },
                {
                    "sent": "So let's start with this part.",
                    "label": 0
                },
                {
                    "sent": "This is just the language model.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's a two layer L STM which encodes a sequence of words and outputs.",
                    "label": 0
                },
                {
                    "sent": "The next best word.",
                    "label": 0
                },
                {
                    "sent": "So you always input a bird and it predicts what's the next best word.",
                    "label": 0
                },
                {
                    "sent": "Input the generated work in the next one and it kind of always predicts the next word.",
                    "label": 0
                },
                {
                    "sent": "Now you need to condition this somehow on the image information right?",
                    "label": 0
                },
                {
                    "sent": "Because this is what you want to generate and the standard model.",
                    "label": 0
                },
                {
                    "sent": "Takes the image, takes some sort of classifier on this image and extract some sort of visual features that are then in some sense used for.",
                    "label": 0
                },
                {
                    "sent": "Predicting the next word.",
                    "label": 0
                },
                {
                    "sent": "This is the standard model.",
                    "label": 0
                },
                {
                    "sent": "What we add on top.",
                    "label": 0
                },
                {
                    "sent": "Is first of all multi entity label classifiers so.",
                    "label": 0
                },
                {
                    "sent": "You get an image you classify, not by some visual features, but by entities that are in the Knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "So you just need to train a standard convolutional network to predict basically your eyes in a knowledge graph and not some words in the image.",
                    "label": 0
                },
                {
                    "sent": "This is done basically here, so we get some out.",
                    "label": 0
                },
                {
                    "sent": "We identify where they are in the knowledge graph, and then we just pick some.",
                    "label": 0
                },
                {
                    "sent": "Knowledge Graph, entity embedding representation of the entities that we identified and feed them also into this model.",
                    "label": 0
                },
                {
                    "sent": "What we need to make sense of this is this additional layer which basically mixes.",
                    "label": 0
                },
                {
                    "sent": "The text, the semantic information and the visual information together and has like this child cross modal representation and then this is used to predict the outcome of so the next word that should be generated.",
                    "label": 0
                },
                {
                    "sent": "So in fact it's not that different and not so difficult.",
                    "label": 0
                },
                {
                    "sent": "We'll go a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into the math here.",
                    "label": 0
                },
                {
                    "sent": "So the better is basically the attention weight that we want to calculate.",
                    "label": 0
                },
                {
                    "sent": "It's based on a word here on a word that at a specific point in time.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of it weighs.",
                    "label": 0
                },
                {
                    "sent": "It's a scalar that weighs.",
                    "label": 0
                },
                {
                    "sent": "Each entity given the word in the word sequence and it's calculated basically by a weight matrix.",
                    "label": 0
                },
                {
                    "sent": "This needs to be learned times the hidden cell state of the second layer, so this square meters a second layer of the STM.",
                    "label": 0
                },
                {
                    "sent": "That's the output layoff the STM times the entity embedding right?",
                    "label": 0
                },
                {
                    "sent": "Then we do this for all of the embeddings that we get identified through the image classifier.",
                    "label": 0
                },
                {
                    "sent": "So we get what we get is a better and this better basically tells us the higher the better is, the more salient this entity is in this image.",
                    "label": 0
                },
                {
                    "sent": "To capture all of the entities that we identified, we just build a weighted sum of the entity, embedding them by the better that we calculate, and we call this the context.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and then how?",
                    "label": 0
                },
                {
                    "sent": "We?",
                    "label": 0
                },
                {
                    "sent": "This is what we feed into the system and we need to basically then take care of this in this TSV layer, which stands for text semantics and.",
                    "label": 0
                },
                {
                    "sent": "Visual information so and it basically has those three components, so this is the information from the language model, so it's not a square, it's just the second layer of the STM.",
                    "label": 0
                },
                {
                    "sent": "This is the context vector, so the knowledge graph information and this is the visual features that we get from the standard approach.",
                    "label": 0
                },
                {
                    "sent": "We get 3 weight matrices right?",
                    "label": 0
                },
                {
                    "sent": "So just linear conversion matrices that we have to learn which map from the internal hidden state so they have all different dimensions here to the dimension of the vocabulary of entities that we can produce.",
                    "label": 0
                },
                {
                    "sent": "Or birds that we not entities words that we can produce with our.",
                    "label": 0
                },
                {
                    "sent": "Language model.",
                    "label": 0
                },
                {
                    "sent": "OK, then we run a standard softmax on this, and this basically predicts the system next most likely words or the top whatever next most likely words.",
                    "label": 0
                },
                {
                    "sent": "Optimized as a standard optimization that we do here so it's the learning signal is the lock loss.",
                    "label": 0
                },
                {
                    "sent": "And yeah, we minimize the negative log loss here of basically the word that is predicted to the word that is in the training data, right?",
                    "label": 0
                },
                {
                    "sent": "And here you immediately see it's only possible for the words that are in the training data.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so in the end we have those for weight matrices.",
                    "label": 0
                },
                {
                    "sent": "We optimize this with standard tools and standard backpropagation and so.",
                    "label": 0
                },
                {
                    "sent": "OK, just a quick.",
                    "label": 0
                },
                {
                    "sent": "Look into this inference.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I probably just explained it.",
                    "label": 0
                },
                {
                    "sent": "It's much easier so.",
                    "label": 0
                },
                {
                    "sent": "What you now have to do is you have learned the weights for all of the known entities, but now you need to also know the weights for the unknown entities and to do this.",
                    "label": 0
                },
                {
                    "sent": "You basically just take the existing weight matrix extended with the unseen object and then just look by some kind of cosine embedding similarity for the most similar entities to an existing entities and you just use a kind of a copying mechanism too.",
                    "label": 0
                },
                {
                    "sent": "Copy the.",
                    "label": 0
                },
                {
                    "sent": "Copy the weights from the known entities to the most similar unknown entities.",
                    "label": 0
                },
                {
                    "sent": "You can do why we call this what we call this constraint and inferences because you can put some sort of inferences or rules on top of that.",
                    "label": 0
                },
                {
                    "sent": "For example, you can use something like hand coded rules like different animals of the same type shouldn't appear in the same image.",
                    "label": 0
                },
                {
                    "sent": "That might not hold for like general Pictures, but if you have seen those kind of image captioning datasets this always holds right?",
                    "label": 0
                },
                {
                    "sent": "So maybe a kind of a cheating, but this is something that you could add here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is captured here, but you could also use more.",
                    "label": 0
                },
                {
                    "sent": "More complex rules.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is about the model.",
                    "label": 0
                },
                {
                    "sent": "I skipped the part about the inference during word generation.",
                    "label": 0
                },
                {
                    "sent": "I just for time reasons.",
                    "label": 0
                },
                {
                    "sent": "But let's go to the evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's just the standard evaluation where you leave some of the concepts or entities out of the predictions during trading.",
                    "label": 0
                },
                {
                    "sent": "So we got rid of I think, 8 different concepts.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the setup is basically had eight held or project.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of the size of the caption data that you have in Ms Coco, so it's a lot this kind of datasets are very hard to obtain.",
                    "label": 0
                },
                {
                    "sent": "For training our entity as a image to entity classifier used standard CNN architecture.",
                    "label": 0
                },
                {
                    "sent": "And for also for later experiments we not only used the the text that is in Ms Coco, right the captions, but also more data.",
                    "label": 0
                },
                {
                    "sent": "I will go into this a little bit later.",
                    "label": 0
                },
                {
                    "sent": "We used RDF to work as the vector embeddings.",
                    "label": 0
                },
                {
                    "sent": "And we reported different evaluation metrics where the 1st two are basically taken from the translation machine translation community.",
                    "label": 0
                },
                {
                    "sent": "They measure how good a generated sentence is similar to the gold standard sentence and the F1 score basically just measures how good are we in predicting the correct entities or generating a sentence that contains the correct entity.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, some qualitative results.",
                    "label": 0
                },
                {
                    "sent": "For this standard task, just an Ms Coco data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we predict.",
                    "label": 0
                },
                {
                    "sent": "This is what the image our image classifier predicts or predict.",
                    "label": 0
                },
                {
                    "sent": "Something separate enclosure, Sue and so on.",
                    "label": 0
                },
                {
                    "sent": "The base method as I showed before is not able to do this because several is just not in the as we've seen.",
                    "label": 0
                },
                {
                    "sent": "Just seen be left this out.",
                    "label": 0
                },
                {
                    "sent": "It's not in their capture generation data.",
                    "label": 0
                },
                {
                    "sent": "So the task is somehow to replace animal with zebra.",
                    "label": 0
                },
                {
                    "sent": "So the other methods can some of the other methods can pick the right entity here.",
                    "label": 0
                },
                {
                    "sent": "But yeah, somehow also did the language that we generate looks a little bit more precise than before.",
                    "label": 0
                },
                {
                    "sent": "What may be interesting is that often what happens is that more specific term is replaced with a more general one.",
                    "label": 0
                },
                {
                    "sent": "OK, so then the quantitative analysis.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here that's the F1 score.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit mixed.",
                    "label": 0
                },
                {
                    "sent": "And I can see.",
                    "label": 0
                },
                {
                    "sent": "Can you distinguish the bold ones from so the underlined ones are the second best and the bold ones are the best?",
                    "label": 0
                },
                {
                    "sent": "From here I cannot see the difference, but those are all bold and this one is bold.",
                    "label": 0
                },
                {
                    "sent": "This one is bold and.",
                    "label": 0
                },
                {
                    "sent": "I think yeah, so in most of the cases our model performs best, but in some cases also the other models perform best.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we are second best.",
                    "label": 1
                },
                {
                    "sent": "OK, so this basically predicts how often we pick the right entity.",
                    "label": 0
                },
                {
                    "sent": "In the different categories.",
                    "label": 0
                },
                {
                    "sent": "So you have to imagine this like 1000.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of examples here.",
                    "label": 0
                },
                {
                    "sent": "So averaged over thousands of examples, this is the score about how good the language is that we generated, and again those are the bold ones.",
                    "label": 0
                },
                {
                    "sent": "And those are boats, and this is a bold one.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, in most of the cases our model performs better than the ones without those knowledge graph information.",
                    "label": 0
                },
                {
                    "sent": "Then we did another expense.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meant.",
                    "label": 0
                },
                {
                    "sent": "On top of that, where we thought, oh maybe we can also scale to outside of this Ms Coco.",
                    "label": 0
                },
                {
                    "sent": "So right now we are in this 80 -- 8 objects and we thought since there are classifiers out there, the image net classifiers that classify whatever around 1000.",
                    "label": 0
                },
                {
                    "sent": "So we had the 642.",
                    "label": 0
                },
                {
                    "sent": "Why can't we try to evaluate this on on larger datasets in tears now?",
                    "label": 0
                },
                {
                    "sent": "Examples of basically when we scale this from from those 82 to 800 almost.",
                    "label": 0
                },
                {
                    "sent": "Yeah, examples what could be generated there and again you see some.",
                    "label": 0
                },
                {
                    "sent": "Some patterns where we basically replace some abstract concept with something more specific.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we also did.",
                    "label": 0
                },
                {
                    "sent": "Kind of the F1 score here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is only.",
                    "label": 0
                },
                {
                    "sent": "Trained on this Ms Coco data set and for this we then basically replaced the language model and the image classifier by language model versus trained on a larger corpus.",
                    "label": 0
                },
                {
                    "sent": "Written here and basically image this image net classifier and as you can see, if you then evaluate this on also data from image net where you just say I can we predict the correct can we generate a sentence with the correct entity in it?",
                    "label": 0
                },
                {
                    "sent": "This performs much better and we haven't seen any compatible algorithm doing this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so summing up.",
                    "label": 0
                },
                {
                    "sent": "We had this task where we want to extrapolate this kind of translation between modalities and we can say yes using knowledge graph embeddings in this attention mechanism that this can work.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least to some extent.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are still a lot of open issues here.",
                    "label": 0
                },
                {
                    "sent": "Those are the references.",
                    "label": 0
                },
                {
                    "sent": "That's my last slide, just as a summary, so I had this four parts in my presentation.",
                    "label": 0
                },
                {
                    "sent": "About the general set up, then the task, our model and the evaluation maybe.",
                    "label": 0
                },
                {
                    "sent": "2 interesting lessons learned is that the model somehow learns.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is also for future investigations to pick a more specific entity, which probably comes from the in the embeddings.",
                    "label": 0
                },
                {
                    "sent": "The hierarchy is somehow encoded.",
                    "label": 0
                },
                {
                    "sent": "And what's also interesting is how far we can scale.",
                    "label": 0
                },
                {
                    "sent": "So this looked like we can just scale by basically the whatever the image classifier can give us, which is not true.",
                    "label": 0
                },
                {
                    "sent": "So we notice that it's really dependent on the domain.",
                    "label": 0
                },
                {
                    "sent": "So if you have images like in Ms Coco, that's on a certain domain, as soon as you try to do image captioning generation for images that are really from a different domain, so the language to describe those type of.",
                    "label": 0
                },
                {
                    "sent": "Images is different.",
                    "label": 0
                },
                {
                    "sent": "That system completely fails so.",
                    "label": 0
                },
                {
                    "sent": "In this case, you still need this kind of image captioning parallel data to really perform well.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me finish here and I'm happy to take questions.",
                    "label": 0
                },
                {
                    "sent": "Thanks very nice follow up work.",
                    "label": 0
                },
                {
                    "sent": "I just have questions regarding possible extension.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you look at this, there are more and more large copper offering emerging.",
                    "label": 0
                },
                {
                    "sent": "World image captioning translations basically.",
                    "label": 0
                },
                {
                    "sent": "Because you have those knowledge graph embedding's that potentially have labels of things in many many languages.",
                    "label": 0
                },
                {
                    "sent": "Do you think that can also help to obtain those translated captions?",
                    "label": 0
                },
                {
                    "sent": "'cause there are international changes about this.",
                    "label": 0
                },
                {
                    "sent": "Now we have a lot of training data, yeah?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What's funny is we have not yet published model for generating captions in different languages.",
                    "label": 0
                },
                {
                    "sent": "But that doesn't have this knowledge graph attention mechanism yet.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I I guess it.",
                    "label": 0
                },
                {
                    "sent": "It could help.",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                },
                {
                    "sent": "The problem is that languages are very different so that.",
                    "label": 0
                },
                {
                    "sent": "So like the structure of languages is very different.",
                    "label": 0
                },
                {
                    "sent": "So here it's all about identifying the correct entity to describe this right so?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if the Knowledge Graph helps in the sense of really generating this kind of caption data that tells you how to describe an image in different languages.",
                    "label": 0
                },
                {
                    "sent": "That's probably pretty hard.",
                    "label": 0
                },
                {
                    "sent": "What's easiest, of course, maybe you can generate training data.",
                    "label": 0
                },
                {
                    "sent": "Through the language links that you have in such a graph.",
                    "label": 0
                },
                {
                    "sent": "But I'm not sure about if it really helps, or at least it's not obvious how it really helps for multilingual caption generation.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Very nice dog thanks.",
                    "label": 0
                },
                {
                    "sent": "Have you also thought about applying this to image classification tasks and or do you know of any papers or but, uh, why like why would you do this immediately for a more difficult task?",
                    "label": 0
                },
                {
                    "sent": "Like image captioning?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I mean we have in the paper we have also the evaluation and.",
                    "label": 0
                },
                {
                    "sent": "About just the image to entity classification task, right?",
                    "label": 0
                },
                {
                    "sent": "Oh I see.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "But this is this is really standard methodology.",
                    "label": 0
                },
                {
                    "sent": "There's no attention mechanism in it, it's just a convolutional neural network that links from an image to your eyes in the in the graph, right?",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "OK, and you see improvement in that case as well.",
                    "label": 0
                },
                {
                    "sent": "Um, we are not aware of any approaches that link from my image to Knowledge Graph entity.",
                    "label": 0
                },
                {
                    "sent": "So maybe I don't get your first, so I see no.",
                    "label": 0
                },
                {
                    "sent": "I mean just regular image classification benchmarks like Image, Net and then you use your approach with the using the Knowledge graph from the image and like you have this extract entities.",
                    "label": 0
                },
                {
                    "sent": "Oh no, OK. No, I think it doesn't actually work in this case.",
                    "label": 0
                },
                {
                    "sent": "With your approach.",
                    "label": 0
                },
                {
                    "sent": "I guess it would be interesting to see if this kind of if you have captured data.",
                    "label": 0
                },
                {
                    "sent": "If this helps in the in also identifying the correct end entity, which I guess would be possible.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I was thinking about seeing graphs.",
                    "label": 0
                },
                {
                    "sent": "I think I maybe I make something up, but yeah, I mean this is the next step.",
                    "label": 0
                },
                {
                    "sent": "Going to seeing graphs and relations and so on.",
                    "label": 0
                },
                {
                    "sent": "OK. With time for another question maybe.",
                    "label": 0
                },
                {
                    "sent": "So if there are no more questions, then let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}