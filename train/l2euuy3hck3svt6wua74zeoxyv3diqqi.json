{
    "id": "l2euuy3hck3svt6wua74zeoxyv3diqqi",
    "title": "Robustness and Regularization of Support Vector Machines",
    "info": {
        "author": [
            "Huan Xu, Department of Mechanical Engineering, National University of Singapore"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/opt08_xu_raros/",
    "segmentation": [
        [
            "This is a gentle work with constant incriminates from the University of Texas at Austin and Sherman.",
            "From adversity and the main idea of this talk is to show that the well known support vector machine is indeed the dissolution of a robust optimization problem.",
            "Why this optimization is only optimized or minimized training error with certain perturbations?"
        ],
        [
            "The token is divided into 3 parts.",
            "I will first introduce some, like the high level motivations and the main results provide the main results of the whole research, and then I'll go to detail to show that particular machine has a equivalence relationship with robust optimization or robust classification, and by this robustness interpretation we can indeed establish some celery results, which is well known but was proved using like entropy methods or VC dimension.",
            "So stability, which is quite complicated tech.",
            "Nix, but following this robust interpretations you can derive them in a very, very, very easy right."
        ],
        [
            "Does standardized static learning setup is picks it up?",
            "It's like you have a set of training data which is assumed to be generated from some unknown distribution and the goal is to find the rules to predict the label wipe from the observation X and so that's the expected prediction error is minimized.",
            "This expectation is taken over the.",
            "And distribution, which generator of mooch generator observations?",
            "However, since the distribution is unknown, we have no access to really evaluate this expectation.",
            "So one way to approximate it is to use the empirical distribution of the observations, which leads to them minimizing the training error.",
            "But as most of you will know that this will also leads to overfitting."
        ],
        [
            "All of it soon basically means that to find some solutions which is way too complicated to fit the data is like the blue lines.",
            "It's filled the training errors perfectly with no training error, but you cannot imagine that this one will really predict good if you have any observations."
        ],
        [
            "And this leads to the first method to control overfitting, which is regulation.",
            "I think most of you knows this quite well.",
            "Regulation means that we want to penalize the complexity of a solution by adding some panel panelization term role of L. Typically this panelization term is true.",
            "Then as normal function.",
            "When one problem here is the physical meaning of this submission is not very clear because in the first part.",
            "You have summation of L, which is the training error in the second part.",
            "You have some penalty functions and you are adjusting just summing up apples with oranges.",
            "You do not have a physical interpretation of this and this makes it quite difficult to tune the regulation terms.",
            "I mean how large you should use this.",
            "This sort of should be like the normal function or maybe 100 times the normal function or even 1 million times normal function.",
            "The standard way to overcome this is like using cross validations.",
            "You try different regulation values and select the one that performs empirically good."
        ],
        [
            "Unless of this fact, but it's also known as like overfitting solutions are also sensitive to disturbance."
        ],
        [
            "Which I will show from a textbook example.",
            "Suppose we have 10 samples and we want to fit in them."
        ],
        [
            "Now if we fit them, use the arbitrary degree polynomials, then this is the solution which is way too complicated and over 50."
        ],
        [
            "King if we do a small perturbations around the training samples.",
            "Now we have set of new training samples which is very close to the pre."
        ],
        [
            "Test 1 and do the learning again.",
            "The solution we get is dramatically different from the previous one, which means this learning algorithm is very sensitive to small probations of the samples."
        ],
        [
            "Netflix restrict us to only degree two polynomial and this is the solution of the learning."
        ],
        [
            "With the same perturbations, we get a solution which is almost identical to the previous one.",
            "This means if your solution or your learning algorithm is not overfitting one.",
            "There it is quite robust."
        ],
        [
            "So to perturbation.",
            "So this leads to the second approach to controller fitting.",
            "That is, we want to find a solution which is robust with respect to perturbations and how to find our robust solution when one idea is to use robust optimization."
        ],
        [
            "I will briefly explain here.",
            "Robust optimization is a general framework to find optimal solution where your para meters to define the problems is not know exactly.",
            "See we have generalization problem.",
            "We want to maximize some utility function U of X and can see and hear axis the decision variable and consists of perimetre which defence your problem.",
            "Now Ative consists not precisely known.",
            "This can be due to the rhythm.",
            "Say you have noisy observations or you are estimating the para meters from only finite samples.",
            "Or maybe a simplified the problems to make it easy to solve.",
            "The way to handle this is to use this Max Mini.",
            "Formulations which assumes that although we do not know precisely what is discussing, but we are sure that this parameter belongs to some close set.",
            "Now instead of maximizing the tree function, we maximizing the lower bound of virtual function or the worst case of digital function and then this will lead to some maximal functions, so that you're the solution gets will always perform good as long as the para meters belongs to the set that you know."
        ],
        [
            "The main result of this talk is saying that we have to approach approach when is regulation approach to his or bus optimization, and indeed that the same thing the standard regulation terms is indeed the solution of robust optimization and Moreover based on standard statistical learning theory, all focus on regulation and show that by regularising the problems correctly or appropriate.",
            "Appropriately you can get.",
            "Properties like generalization ability, statistical consistency, not from the regulation from the robust interpretation.",
            "We can also direct this, and probably more easily."
        ],
        [
            "Now we go through the technical."
        ],
        [
            "Tails.",
            "This is the.",
            "This is a support vector machine for the linear case.",
            "If we change the XI to the future mapping file factor XI then we get kernelized support vector machine.",
            "Equivalently we can rewrite it as unconstrained optimization problem with a bit more complicated objective function."
        ],
        [
            "Now we go from the second approach to write a minimax problem.",
            "We want to minimize the training error, but assuming that there's some contributions on the training samples we have, so we have minimizing the decisions over the superior of perturbations for different end uncertainty setting you will eventually get the different formulations and."
        ],
        [
            "We are more interested in the uncertainty set that we called as.",
            "Sub linear algorithms in this set.",
            "Briefly speaking, this is.",
            "This set is defined by.",
            "For each samples you have set which determines what is the perturbation allowed.",
            "Apart from that you also have aggregated constraints which defines how your perturbations among different samples will be is allowable and I have."
        ],
        [
            "Show telestrations so if we have a very simple problem, we have two samples, each of them is of one dimension, then the atomic and sunset is just the interval.",
            "Now we have an inner set, an auto set.",
            "What is that?",
            "The inner set is basically this thing.",
            "The outset is basically this thing.",
            "And for any set which contains the inner set, but it's contained by the outset, it isn't sub sablina aggregating sunsets."
        ],
        [
            "So that these are some simple examples.",
            "To me the most interesting is seeing is the first one.",
            "That is, you have a.",
            "You have allowed perturbation so that the summation of the magnitude of perturbations each sample is constrained is restricted by some cons."
        ],
        [
            "Sent.",
            "Now the main theorem says that if you have certain if you have such uncertainty set, then the minimax problem is equivalent to the to the problem which we have a regulation term of superior W transpose Delta."
        ],
        [
            "And if we use the set that I said is of the most interesting, the set that your total perturbation is limited, then you will recover the standard regulations standard support vector machine.",
            "So this provides us with a physical interpretation of what regulation is indeed doing, besides penalizing the complex solutions.",
            "Indeed it is solving a robust version of minimizing training error.",
            "And this now we do not have.",
            "We are not really adding apples with oranges, because we know that oranges, just you want to perturb your apples to make it more robust."
        ],
        [
            "And for the kernelized version the the result is basically the same.",
            "You have a kernelized SVM."
        ],
        [
            "And this can be written as.",
            "Robust optimization if you have perturbations inside the feature space.",
            "Up at the."
        ],
        [
            "Vision in the feature space is less intuitive appealing compared to perturbation the sample space, but we have a lemma saying that features under very mild technique conditions, which is stated here then feature space cente implies input space insanity.",
            "I need to I need to notify that this technique condition is really might in particular order.",
            "Continuous kernels satisfy this one."
        ],
        [
            "Now let's try this with a simple example.",
            "Suppose we have a degree.",
            "We have a kernel, which is the degree two polynomial.",
            "Since this is a continuous mapping, we know that if I have a small ball inch small by the input space, then the image of this mobile will be contained in a bar in the feature space.",
            "Now if I have algorithm which is robust to the B of F, which is the ball in the feature space, then this means it's for any point in this mobile it is a performance which is guaranteed now for a subset.",
            "Of this bar, which is this file by the performance is also guaranteed, which implies that this is robust to our perturbation inside this."
        ],
        [
            "Input space.",
            "Now I show how this robust interpretations can be used to prove consistent consistency of kernelized."
        ],
        [
            "Vector machine this is.",
            "This is indeed unknown results, but standard proof is very complicated.",
            "We use the standard, picks it up.",
            "There's a bounded set X belongs to Euclidean space, which generated training samples.",
            "I deley and the distribution that generator distribution, distribution agent generator samples is of course, and no.",
            "There's some kind of functions which satisfy the lemma.",
            "One we know that all continuous functions satisfy that.",
            "Now we have some constant which is which is 4.",
            "Take the condition."
        ],
        [
            "The main theorem here is it Cindy to give two inequalities which uniform over all the decision variables or order order classifier.",
            "Here gamma of MC is sequence of random numbers which will converge to zero with probability one."
        ],
        [
            "To show this to.",
            "To show this to equalities, the basic idea here is we know that we are interested in bonding the testing error.",
            "Now if we look at, we treat the testing error is a perturbative version of the training error and the bond.",
            "The difference between the set of training error and the set of testing error we all set.",
            "So the first step is to define if a testing sample in the training sample close enough.",
            "By close enough, it means that they have the same label.",
            "And Euclidean distance between this these two samples bounded.",
            "Now, if most if all my samples can be bounded can be sampled.",
            "If all my training samples and my testing samples can be paired, then we have.",
            "We know that suppose these samples are close enough by the robust interpretation, the performance of the perturbative thing is bounded.",
            "So we have this inequality.",
            "Which indeed assess sets for the paired samples, it's.",
            "Testing error is bounded by C * M of the norm of W, and if you shrink the, see appropriate will get the consistency.",
            "Now what's left here?"
        ],
        [
            "Is to say that to argue that most of the testing samples will be eventually be paired and this is given by this lemma and the idea here is repartitioned.",
            "Compact set X into small sets so that if a testing some point the training sample falls in the same subset then there will be there will be paired.",
            "Now this will leads us to indeed a multinomial random variable.",
            "So the testing samples that's for the all testing samples the number that belongs to each of the subsets will give the multinomial random variable and the same for the testing samples, and they are following the same distribution.",
            "So by standard statistical statistics argument we know that this will converge to zero with probability, the difference will converge."
        ],
        [
            "Zero with probability one.",
            "So this means in the linear case, in the linear case, the most of samples will be paired and pair samples has.",
            "Her son training testing error which is bounded, so with probably when you will get consistency now, what about the kernel version but the kernel versions we need to distinguish the two different set up when it's for good kernels, which is the kernels that satisfied the technique conditions that we said for this kind of kernels we know that they will eventually transfer the robustness in feature space back into sample space.",
            "So the whole argument will.",
            "Will be valid with some algebras.",
            "On the other hand, if a kernel does not satisfy the conditions, then it can be non consistent.",
            "So we have an example saying that if you have a very trivial kernel which only gives which is like the indicated function of X = X prime and then we know that the decision you get is always of this form, which will provide no meaningful predictions for any testing sample if it is not one of the training sample.",
            "This for all the standard results showing consistency the assumed beforehand that kernels to be considered as a continuous one, and never discuss why it is and this.",
            "I think this robust interpretation indeed provide us from.",
            "Give us some insight of why this is, because for non continuous or certain bad kernels it does not really transfer the robustness in feature space back into sample space, so the consistency will fail."
        ],
        [
            "Now to sum up, the main result is saying that regulation, and in particular support vector machine is a solution of a robust optimization.",
            "If you just want to minimize training errors and assuming the specifications on the observations.",
            "On the other hand, it also provided a way to solve robust optimization problems by writing the regulation terms and the 2nd result here is by using the robust interpretations we can establish consistency and probably other theoretical results which which of which will be of interest future works.",
            "Future works of one of the future works that is of interest is to find the new regulation schemes and the new learning algorithms using robust optimizations.",
            "And.",
            "From a theoretical perspective, we are also interested in finding a general framework which will lead some generous tester consistency with robustness of a general learning algorithm.",
            "If you want the preprint, you can find it here.",
            "That's all.",
            "Information about the shape of.",
            "Yes, that that's what.",
            "That would be, that would be very great if you have the approval information of.",
            "Say if you have, you know that the observations for one feature of 1 value is.",
            "More noisy than the other one, then Best Buy bystander regulation.",
            "You do not have a way to overcome this or to in to exploit this kind of prior information before using the robust optimization you all you need to do is to design a different instance, reset and solve this result immigration problem which will lead to a new learning algorithm and.",
            "Possibly to our performance better because you exploit the approval or prior information that you have.",
            "Then you will get some.",
            "I think if you have correlations then you will the way that you will eventually get is a regulation term, which is not a spherical but certain kind of ellipsoids.",
            "Something about the case.",
            "Why is perturbed instead of text?",
            "Compared to what?",
            "Compared to preservation?",
            "Why?",
            "Because why is only two labels you have minus 1 + 1.",
            "You do not have permission there.",
            "You want.",
            "And indeed, that's what people use to analyze in linear regression.",
            "Fitting.",
            "Yeah, but this is a classification problem.",
            "It's not a regression problem.",
            "You have two classifiers.",
            "If you perturb minus one to plus one.",
            "I don't really see the physical meaning here, but mathematically, if you perturb this, I guess you only the formulation get is only add some constant which does not really change with your classification rules, so this is.",
            "This does not really change the result you get.",
            "So you can assume that specifications on what but remaining the resulting classifier is the same with some constant added inside objective function."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a gentle work with constant incriminates from the University of Texas at Austin and Sherman.",
                    "label": 0
                },
                {
                    "sent": "From adversity and the main idea of this talk is to show that the well known support vector machine is indeed the dissolution of a robust optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Why this optimization is only optimized or minimized training error with certain perturbations?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The token is divided into 3 parts.",
                    "label": 0
                },
                {
                    "sent": "I will first introduce some, like the high level motivations and the main results provide the main results of the whole research, and then I'll go to detail to show that particular machine has a equivalence relationship with robust optimization or robust classification, and by this robustness interpretation we can indeed establish some celery results, which is well known but was proved using like entropy methods or VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So stability, which is quite complicated tech.",
                    "label": 0
                },
                {
                    "sent": "Nix, but following this robust interpretations you can derive them in a very, very, very easy right.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does standardized static learning setup is picks it up?",
                    "label": 0
                },
                {
                    "sent": "It's like you have a set of training data which is assumed to be generated from some unknown distribution and the goal is to find the rules to predict the label wipe from the observation X and so that's the expected prediction error is minimized.",
                    "label": 0
                },
                {
                    "sent": "This expectation is taken over the.",
                    "label": 0
                },
                {
                    "sent": "And distribution, which generator of mooch generator observations?",
                    "label": 0
                },
                {
                    "sent": "However, since the distribution is unknown, we have no access to really evaluate this expectation.",
                    "label": 0
                },
                {
                    "sent": "So one way to approximate it is to use the empirical distribution of the observations, which leads to them minimizing the training error.",
                    "label": 0
                },
                {
                    "sent": "But as most of you will know that this will also leads to overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of it soon basically means that to find some solutions which is way too complicated to fit the data is like the blue lines.",
                    "label": 0
                },
                {
                    "sent": "It's filled the training errors perfectly with no training error, but you cannot imagine that this one will really predict good if you have any observations.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this leads to the first method to control overfitting, which is regulation.",
                    "label": 0
                },
                {
                    "sent": "I think most of you knows this quite well.",
                    "label": 0
                },
                {
                    "sent": "Regulation means that we want to penalize the complexity of a solution by adding some panel panelization term role of L. Typically this panelization term is true.",
                    "label": 1
                },
                {
                    "sent": "Then as normal function.",
                    "label": 0
                },
                {
                    "sent": "When one problem here is the physical meaning of this submission is not very clear because in the first part.",
                    "label": 1
                },
                {
                    "sent": "You have summation of L, which is the training error in the second part.",
                    "label": 0
                },
                {
                    "sent": "You have some penalty functions and you are adjusting just summing up apples with oranges.",
                    "label": 1
                },
                {
                    "sent": "You do not have a physical interpretation of this and this makes it quite difficult to tune the regulation terms.",
                    "label": 0
                },
                {
                    "sent": "I mean how large you should use this.",
                    "label": 0
                },
                {
                    "sent": "This sort of should be like the normal function or maybe 100 times the normal function or even 1 million times normal function.",
                    "label": 0
                },
                {
                    "sent": "The standard way to overcome this is like using cross validations.",
                    "label": 0
                },
                {
                    "sent": "You try different regulation values and select the one that performs empirically good.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unless of this fact, but it's also known as like overfitting solutions are also sensitive to disturbance.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which I will show from a textbook example.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have 10 samples and we want to fit in them.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if we fit them, use the arbitrary degree polynomials, then this is the solution which is way too complicated and over 50.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "King if we do a small perturbations around the training samples.",
                    "label": 0
                },
                {
                    "sent": "Now we have set of new training samples which is very close to the pre.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Test 1 and do the learning again.",
                    "label": 0
                },
                {
                    "sent": "The solution we get is dramatically different from the previous one, which means this learning algorithm is very sensitive to small probations of the samples.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Netflix restrict us to only degree two polynomial and this is the solution of the learning.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the same perturbations, we get a solution which is almost identical to the previous one.",
                    "label": 0
                },
                {
                    "sent": "This means if your solution or your learning algorithm is not overfitting one.",
                    "label": 0
                },
                {
                    "sent": "There it is quite robust.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to perturbation.",
                    "label": 0
                },
                {
                    "sent": "So this leads to the second approach to controller fitting.",
                    "label": 0
                },
                {
                    "sent": "That is, we want to find a solution which is robust with respect to perturbations and how to find our robust solution when one idea is to use robust optimization.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will briefly explain here.",
                    "label": 0
                },
                {
                    "sent": "Robust optimization is a general framework to find optimal solution where your para meters to define the problems is not know exactly.",
                    "label": 0
                },
                {
                    "sent": "See we have generalization problem.",
                    "label": 0
                },
                {
                    "sent": "We want to maximize some utility function U of X and can see and hear axis the decision variable and consists of perimetre which defence your problem.",
                    "label": 0
                },
                {
                    "sent": "Now Ative consists not precisely known.",
                    "label": 0
                },
                {
                    "sent": "This can be due to the rhythm.",
                    "label": 0
                },
                {
                    "sent": "Say you have noisy observations or you are estimating the para meters from only finite samples.",
                    "label": 0
                },
                {
                    "sent": "Or maybe a simplified the problems to make it easy to solve.",
                    "label": 0
                },
                {
                    "sent": "The way to handle this is to use this Max Mini.",
                    "label": 0
                },
                {
                    "sent": "Formulations which assumes that although we do not know precisely what is discussing, but we are sure that this parameter belongs to some close set.",
                    "label": 0
                },
                {
                    "sent": "Now instead of maximizing the tree function, we maximizing the lower bound of virtual function or the worst case of digital function and then this will lead to some maximal functions, so that you're the solution gets will always perform good as long as the para meters belongs to the set that you know.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main result of this talk is saying that we have to approach approach when is regulation approach to his or bus optimization, and indeed that the same thing the standard regulation terms is indeed the solution of robust optimization and Moreover based on standard statistical learning theory, all focus on regulation and show that by regularising the problems correctly or appropriate.",
                    "label": 0
                },
                {
                    "sent": "Appropriately you can get.",
                    "label": 0
                },
                {
                    "sent": "Properties like generalization ability, statistical consistency, not from the regulation from the robust interpretation.",
                    "label": 0
                },
                {
                    "sent": "We can also direct this, and probably more easily.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we go through the technical.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tails.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is a support vector machine for the linear case.",
                    "label": 1
                },
                {
                    "sent": "If we change the XI to the future mapping file factor XI then we get kernelized support vector machine.",
                    "label": 0
                },
                {
                    "sent": "Equivalently we can rewrite it as unconstrained optimization problem with a bit more complicated objective function.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we go from the second approach to write a minimax problem.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize the training error, but assuming that there's some contributions on the training samples we have, so we have minimizing the decisions over the superior of perturbations for different end uncertainty setting you will eventually get the different formulations and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are more interested in the uncertainty set that we called as.",
                    "label": 0
                },
                {
                    "sent": "Sub linear algorithms in this set.",
                    "label": 0
                },
                {
                    "sent": "Briefly speaking, this is.",
                    "label": 0
                },
                {
                    "sent": "This set is defined by.",
                    "label": 0
                },
                {
                    "sent": "For each samples you have set which determines what is the perturbation allowed.",
                    "label": 0
                },
                {
                    "sent": "Apart from that you also have aggregated constraints which defines how your perturbations among different samples will be is allowable and I have.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show telestrations so if we have a very simple problem, we have two samples, each of them is of one dimension, then the atomic and sunset is just the interval.",
                    "label": 0
                },
                {
                    "sent": "Now we have an inner set, an auto set.",
                    "label": 0
                },
                {
                    "sent": "What is that?",
                    "label": 0
                },
                {
                    "sent": "The inner set is basically this thing.",
                    "label": 1
                },
                {
                    "sent": "The outset is basically this thing.",
                    "label": 0
                },
                {
                    "sent": "And for any set which contains the inner set, but it's contained by the outset, it isn't sub sablina aggregating sunsets.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that these are some simple examples.",
                    "label": 0
                },
                {
                    "sent": "To me the most interesting is seeing is the first one.",
                    "label": 0
                },
                {
                    "sent": "That is, you have a.",
                    "label": 0
                },
                {
                    "sent": "You have allowed perturbation so that the summation of the magnitude of perturbations each sample is constrained is restricted by some cons.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sent.",
                    "label": 0
                },
                {
                    "sent": "Now the main theorem says that if you have certain if you have such uncertainty set, then the minimax problem is equivalent to the to the problem which we have a regulation term of superior W transpose Delta.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we use the set that I said is of the most interesting, the set that your total perturbation is limited, then you will recover the standard regulations standard support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So this provides us with a physical interpretation of what regulation is indeed doing, besides penalizing the complex solutions.",
                    "label": 0
                },
                {
                    "sent": "Indeed it is solving a robust version of minimizing training error.",
                    "label": 0
                },
                {
                    "sent": "And this now we do not have.",
                    "label": 0
                },
                {
                    "sent": "We are not really adding apples with oranges, because we know that oranges, just you want to perturb your apples to make it more robust.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the kernelized version the the result is basically the same.",
                    "label": 0
                },
                {
                    "sent": "You have a kernelized SVM.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this can be written as.",
                    "label": 0
                },
                {
                    "sent": "Robust optimization if you have perturbations inside the feature space.",
                    "label": 0
                },
                {
                    "sent": "Up at the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vision in the feature space is less intuitive appealing compared to perturbation the sample space, but we have a lemma saying that features under very mild technique conditions, which is stated here then feature space cente implies input space insanity.",
                    "label": 0
                },
                {
                    "sent": "I need to I need to notify that this technique condition is really might in particular order.",
                    "label": 0
                },
                {
                    "sent": "Continuous kernels satisfy this one.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's try this with a simple example.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have a degree.",
                    "label": 0
                },
                {
                    "sent": "We have a kernel, which is the degree two polynomial.",
                    "label": 0
                },
                {
                    "sent": "Since this is a continuous mapping, we know that if I have a small ball inch small by the input space, then the image of this mobile will be contained in a bar in the feature space.",
                    "label": 1
                },
                {
                    "sent": "Now if I have algorithm which is robust to the B of F, which is the ball in the feature space, then this means it's for any point in this mobile it is a performance which is guaranteed now for a subset.",
                    "label": 0
                },
                {
                    "sent": "Of this bar, which is this file by the performance is also guaranteed, which implies that this is robust to our perturbation inside this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Input space.",
                    "label": 0
                },
                {
                    "sent": "Now I show how this robust interpretations can be used to prove consistent consistency of kernelized.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vector machine this is.",
                    "label": 0
                },
                {
                    "sent": "This is indeed unknown results, but standard proof is very complicated.",
                    "label": 0
                },
                {
                    "sent": "We use the standard, picks it up.",
                    "label": 0
                },
                {
                    "sent": "There's a bounded set X belongs to Euclidean space, which generated training samples.",
                    "label": 0
                },
                {
                    "sent": "I deley and the distribution that generator distribution, distribution agent generator samples is of course, and no.",
                    "label": 0
                },
                {
                    "sent": "There's some kind of functions which satisfy the lemma.",
                    "label": 0
                },
                {
                    "sent": "One we know that all continuous functions satisfy that.",
                    "label": 0
                },
                {
                    "sent": "Now we have some constant which is which is 4.",
                    "label": 0
                },
                {
                    "sent": "Take the condition.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main theorem here is it Cindy to give two inequalities which uniform over all the decision variables or order order classifier.",
                    "label": 0
                },
                {
                    "sent": "Here gamma of MC is sequence of random numbers which will converge to zero with probability one.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To show this to.",
                    "label": 0
                },
                {
                    "sent": "To show this to equalities, the basic idea here is we know that we are interested in bonding the testing error.",
                    "label": 0
                },
                {
                    "sent": "Now if we look at, we treat the testing error is a perturbative version of the training error and the bond.",
                    "label": 1
                },
                {
                    "sent": "The difference between the set of training error and the set of testing error we all set.",
                    "label": 0
                },
                {
                    "sent": "So the first step is to define if a testing sample in the training sample close enough.",
                    "label": 1
                },
                {
                    "sent": "By close enough, it means that they have the same label.",
                    "label": 0
                },
                {
                    "sent": "And Euclidean distance between this these two samples bounded.",
                    "label": 0
                },
                {
                    "sent": "Now, if most if all my samples can be bounded can be sampled.",
                    "label": 0
                },
                {
                    "sent": "If all my training samples and my testing samples can be paired, then we have.",
                    "label": 1
                },
                {
                    "sent": "We know that suppose these samples are close enough by the robust interpretation, the performance of the perturbative thing is bounded.",
                    "label": 0
                },
                {
                    "sent": "So we have this inequality.",
                    "label": 0
                },
                {
                    "sent": "Which indeed assess sets for the paired samples, it's.",
                    "label": 0
                },
                {
                    "sent": "Testing error is bounded by C * M of the norm of W, and if you shrink the, see appropriate will get the consistency.",
                    "label": 0
                },
                {
                    "sent": "Now what's left here?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is to say that to argue that most of the testing samples will be eventually be paired and this is given by this lemma and the idea here is repartitioned.",
                    "label": 0
                },
                {
                    "sent": "Compact set X into small sets so that if a testing some point the training sample falls in the same subset then there will be there will be paired.",
                    "label": 1
                },
                {
                    "sent": "Now this will leads us to indeed a multinomial random variable.",
                    "label": 0
                },
                {
                    "sent": "So the testing samples that's for the all testing samples the number that belongs to each of the subsets will give the multinomial random variable and the same for the testing samples, and they are following the same distribution.",
                    "label": 1
                },
                {
                    "sent": "So by standard statistical statistics argument we know that this will converge to zero with probability, the difference will converge.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Zero with probability one.",
                    "label": 0
                },
                {
                    "sent": "So this means in the linear case, in the linear case, the most of samples will be paired and pair samples has.",
                    "label": 1
                },
                {
                    "sent": "Her son training testing error which is bounded, so with probably when you will get consistency now, what about the kernel version but the kernel versions we need to distinguish the two different set up when it's for good kernels, which is the kernels that satisfied the technique conditions that we said for this kind of kernels we know that they will eventually transfer the robustness in feature space back into sample space.",
                    "label": 0
                },
                {
                    "sent": "So the whole argument will.",
                    "label": 0
                },
                {
                    "sent": "Will be valid with some algebras.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, if a kernel does not satisfy the conditions, then it can be non consistent.",
                    "label": 0
                },
                {
                    "sent": "So we have an example saying that if you have a very trivial kernel which only gives which is like the indicated function of X = X prime and then we know that the decision you get is always of this form, which will provide no meaningful predictions for any testing sample if it is not one of the training sample.",
                    "label": 1
                },
                {
                    "sent": "This for all the standard results showing consistency the assumed beforehand that kernels to be considered as a continuous one, and never discuss why it is and this.",
                    "label": 1
                },
                {
                    "sent": "I think this robust interpretation indeed provide us from.",
                    "label": 0
                },
                {
                    "sent": "Give us some insight of why this is, because for non continuous or certain bad kernels it does not really transfer the robustness in feature space back into sample space, so the consistency will fail.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now to sum up, the main result is saying that regulation, and in particular support vector machine is a solution of a robust optimization.",
                    "label": 1
                },
                {
                    "sent": "If you just want to minimize training errors and assuming the specifications on the observations.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it also provided a way to solve robust optimization problems by writing the regulation terms and the 2nd result here is by using the robust interpretations we can establish consistency and probably other theoretical results which which of which will be of interest future works.",
                    "label": 0
                },
                {
                    "sent": "Future works of one of the future works that is of interest is to find the new regulation schemes and the new learning algorithms using robust optimizations.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "From a theoretical perspective, we are also interested in finding a general framework which will lead some generous tester consistency with robustness of a general learning algorithm.",
                    "label": 1
                },
                {
                    "sent": "If you want the preprint, you can find it here.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                },
                {
                    "sent": "Information about the shape of.",
                    "label": 0
                },
                {
                    "sent": "Yes, that that's what.",
                    "label": 0
                },
                {
                    "sent": "That would be, that would be very great if you have the approval information of.",
                    "label": 0
                },
                {
                    "sent": "Say if you have, you know that the observations for one feature of 1 value is.",
                    "label": 0
                },
                {
                    "sent": "More noisy than the other one, then Best Buy bystander regulation.",
                    "label": 0
                },
                {
                    "sent": "You do not have a way to overcome this or to in to exploit this kind of prior information before using the robust optimization you all you need to do is to design a different instance, reset and solve this result immigration problem which will lead to a new learning algorithm and.",
                    "label": 0
                },
                {
                    "sent": "Possibly to our performance better because you exploit the approval or prior information that you have.",
                    "label": 0
                },
                {
                    "sent": "Then you will get some.",
                    "label": 0
                },
                {
                    "sent": "I think if you have correlations then you will the way that you will eventually get is a regulation term, which is not a spherical but certain kind of ellipsoids.",
                    "label": 0
                },
                {
                    "sent": "Something about the case.",
                    "label": 0
                },
                {
                    "sent": "Why is perturbed instead of text?",
                    "label": 0
                },
                {
                    "sent": "Compared to what?",
                    "label": 0
                },
                {
                    "sent": "Compared to preservation?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because why is only two labels you have minus 1 + 1.",
                    "label": 0
                },
                {
                    "sent": "You do not have permission there.",
                    "label": 0
                },
                {
                    "sent": "You want.",
                    "label": 0
                },
                {
                    "sent": "And indeed, that's what people use to analyze in linear regression.",
                    "label": 0
                },
                {
                    "sent": "Fitting.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but this is a classification problem.",
                    "label": 0
                },
                {
                    "sent": "It's not a regression problem.",
                    "label": 0
                },
                {
                    "sent": "You have two classifiers.",
                    "label": 0
                },
                {
                    "sent": "If you perturb minus one to plus one.",
                    "label": 0
                },
                {
                    "sent": "I don't really see the physical meaning here, but mathematically, if you perturb this, I guess you only the formulation get is only add some constant which does not really change with your classification rules, so this is.",
                    "label": 1
                },
                {
                    "sent": "This does not really change the result you get.",
                    "label": 0
                },
                {
                    "sent": "So you can assume that specifications on what but remaining the resulting classifier is the same with some constant added inside objective function.",
                    "label": 0
                }
            ]
        }
    }
}