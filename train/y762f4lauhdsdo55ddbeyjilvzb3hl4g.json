{
    "id": "y762f4lauhdsdo55ddbeyjilvzb3hl4g",
    "title": "Inducing Cross-Lingual Semantic Representations of Words, Phrases, Sentences and Events",
    "info": {
        "author": [
            "Ivan Titov, Cluster of Excellence Multimodal Computing and Interaction, Saarland University"
        ],
        "published": "Jan. 11, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_titov_semantic_representations/",
    "segmentation": [
        [
            "OK, and so this is joint work with my postdoc, Alex Klementieff and the Student be number I and also with collaborators at Johns Hopkins University, in part of which is related to me."
        ],
        [
            "Team OK, so first of all, why do we care about semantic representations in general?",
            "Probably because we want to abstract away from specifics of lexical and syntactic representations.",
            "So consider just a very simple example of question answering tasks.",
            "So we have a question and we want to find an answer in a text and this actually actual answers produced by our system.",
            "So we want in the first case we want to figure out that suppliers and block.",
            "I mean essentially the same thing and map arguments.",
            "And in the second case we also want to figure out that entities TNF Alpha and too many crosses have the same thing as well.",
            "So we hope to provide some form abstraction, which is basically tell you that these are the same things.",
            "And the second question is of course, why do we want to do it?",
            "Cross lingually so."
        ],
        [
            "Basically, 2 answers to these questions.",
            "So first of all we hope to get an improvement in each individual language by doing it cross lingually so it has been shown for syntax that have been shown for morphology and so why not?",
            "For semantics?",
            "Intuitively it seems it should be easier for semantics because if syntax.",
            "Is there some hidden correspondences between syntactic representations in two languages.",
            "For semantics we should model more lesson for the consistency, but in practice things are not not as simple.",
            "And of course another reason we don't need to emphasize this.",
            "Shop across lingo representations are useful by themselves for a number of cross lingo."
        ],
        [
            "So is this an outline of my talk?",
            "So I'll first talk about our model for unsupervised induction of shallow semantics, and then I'll talk about it's across into extension and the second part will be about semantic representations.",
            "More level of phrases on a level of."
        ],
        [
            "Words.",
            "So for the first full of about events, so one way to define the semantics of events is traditional to use the notion of semantic frame, right?",
            "So semantic frame is conceptual structure which describes an event and its participants an or an object in its properties.",
            "So for example, you can have very simple sentence Jack opens the lock with a paper clip and here we have what is called closure opening frame in the standard frame net repository.",
            "And there's three semantic roles realized in this case.",
            "So we have an agent, Jacquie.",
            "If a patient lower can instrument OK, but in this frame actually includes many more semantic roles which are not realized at all, so this is a hard problem to induce this type of representation.",
            "Sorry, because we you can see it's very tight to lexical information, its role sign, general predicates."
        ],
        [
            "If you cannot set it up, and of course the question is how much we can rely here on how predictive syntactic and lexical information we have to rely on syntactically information, because there is nothing else but how is this task is an so on the left?",
            "I have different types of so called databases alternations, so you can reverse this situation of Jean breaking the window and basically in a couple of ways so you can say John broke the window, window broken, etc.",
            "So the one thing to notice in this example since the first case.",
            "Agent corresponds to syntactic subject in the second case in septic subject is actually passions affected entities.",
            "So you see this correspondence is not not really as trivial.",
            "And then you also can express the same situation using different verbs, or maybe even a nominal constructions.",
            "OK, so natural ways to apply supervised learning to this setting about this.",
            "This task is really challenging because data sets providing general very low coverage.",
            "It's very tight to lexical domain and then we don't have this data set for many many languages in fact, so only for a handful of languages we have.",
            "This information so let's."
        ],
        [
            "Try to define our task little bit more formally.",
            "So what we want to predict.",
            "If you want to predict so-called semantic dependency graph, so in the semantic dependency graph we have edges in court predicate argument relation and labels on edges, semantic roles and labels on nodes are basically semantic frame.",
            "So types of situations you have.",
            "But this is a very very simple example.",
            "So in practice this can be much more complex, so arguments can work their own frame.",
            "So dress is characterized by style and by creator.",
            "Arguments can be shared between multiple predicates and etc, so this can be quite complex and then also some of these.",
            "Situations can be expressed, can be evoked by multiverse expressions, so things like a light light work constructions.",
            "Person known center cetera.",
            "So what we want we want to induce this type."
        ],
        [
            "Representations from unlabeled data and since we inducing these representations from unlabeled data, this is actually more like a clustering task.",
            "So, so this kind of cluster clustering graph induction task, but probably is that we don't induce names, but really do some kind of clustering IDs.",
            "This really doesn't matter that much."
        ],
        [
            "OK, so let's see what kind of steps is this induction process actually involves.",
            "So first of all we need to induce some kind of transform semantic syntactic dependency graph.",
            "So Ryan mentions that syntactic dependency graph actually mimics the large degree predicate argument structure, so there is not much work to do exist."
        ],
        [
            "Actually, and then we want to induce frames, which basically means to decompose this graph and induce labeling for the fragments in this graph.",
            "And then we also want to induce."
        ],
        [
            "Rolls, which means labeling edges in this graph.",
            "So we don't care that much about the first stage because it can be handled is not perfectly, but it is relatively high accuracy with simple risztics over with a simple and Alex Kurtz classifier, so it's not as hard problem.",
            "So what we care about is the 2nd and the third stage, and we model this two sub task within a joint nonparametric buyer and model.",
            "So we try to induce this representations jointly and this is different from a match of previous work where in general people tried to tackle issues with sub task in elation and using multivariate expression inducing rolls.",
            "Inducing adverb clusters in etc."
        ],
        [
            "OK, so let's just talk about inducing as is similar frames or clusters fragment.",
            "So birth we call semantic classes because even linguists can't really agree with semantic frame.",
            "What is not so?",
            "We don't want a machine algorithm to learn to distinguish the tools.",
            "So we just want to have some kind of clusters and not to confuse the things we call them semantic classes.",
            "And then induction of semantic cloud basically involves this.",
            "The following thing.",
            "So we want to figure out how to essentially cluster different lexemes.",
            "So break Buster destroying essentially the same thing, and then we also want to be able to compose expressions so to support idiomatic expressions to support terminology etc.",
            "And of course it's not disjoint us, so if you decided to compose something, then later on we may want to cluster.",
            "So hell directory we can post and then we might want to cluster is green."
        ],
        [
            "OK so I talked about induction of semantic classes.",
            "So what basically it involves?",
            "What is semantical induction?",
            "And for simplicity let's think that we actually know we already induce the frames since we will be induced in this jointly, we're actually is not actually the case.",
            "We will use some kind of iterative process, But let's think that for now we know where these frames are.",
            "So you basically in this situation you have.",
            "You see why teaching frame is you observe some arguments but you don't know their labeling."
        ],
        [
            "So what you want you want to get some type of clustering?",
            "Basically on this rose.",
            "So basically you can approach this in some direct way, so you can define some generative story for this and try to induce clustering.",
            "But the problem with this is the search space is in general quite huge and we want to repeat this process process during inference of the entire model multiple times.",
            "So we really want to make this clustering step."
        ],
        [
            "Very fast.",
            "So we used an idea which had previously been proposed by Mirella and Joel Long, so that's a couple of years ago and.",
            "So the idea is to identify arguments with some properties, functions of syntactic frame.",
            "So let's look just in a very simple example, so this is called elective located drop lucrative proposition drop.",
            "So you can say Merry climbed up the mountain or you can say Mary climbs the mountain.",
            "So you can.",
            "I need this position and then we can identify arguments for example with this syntactic signature.",
            "So for the second argument we will have information that we have active voice that is from the right of.",
            "A predicate and we have a prepositional modification is preposition app, and in the second case we have a direct object, so that's the only difference between the two and when we basically designed this argument key, the idea is to have a relatively high precision, because then we will cluster them instead of clustering argument accuracies.",
            "And we can get relatively sufficiently car equal.",
            "So that's the basic intuition.",
            "With this argument keys.",
            "So in this case we would just want to cluster this target keys together.",
            "But for some more complex alternations you will want to have a more complex clustering.",
            "So far going and keys, so it's not only players of working with kids put together."
        ],
        [
            "So I talked about segmentation.",
            "Basically defines the task.",
            "Now I will provide some high level description of the model views first of all."
        ],
        [
            "I'll talk about modeling assumptions.",
            "So first 2 assumptions we want to encode about the model.",
            "I basically have to do is sparsity.",
            "So first one we want rolls to have a sparse distribution of argument fillers.",
            "So what it actually means, it means that two predicates will be likely to cluster it to be clustered together if they have a similar set of arguments.",
            "Because this type of move will preserve sparsity.",
            "And then for each semantic class, we assume that it can be verbalised with sparse distribution over elections, actually always syntactic tree fragments.",
            "So this for example, winning situation, we can say envying, hold the victory over and a couple other things, but not really flared distribution over dependency tree fragments.",
            "Then another thing which I'm not going to talk about in detail is that we expect that predicates which have similar syntactic behavior have similar semantics.",
            "So for some of you familiar with this, it has to do with Beth Levin.",
            "Levin classes basically live in classes, and this is a useful signal as well if you include in the model.",
            "And then another signal which is very useful but is very trivial.",
            "So you in general expect that for every predicate occurrence, single semantic role appears mostly mostly once, so you have usually one agent and one patient, but at most one I would say but.",
            "We don't include conjunction as care, so it's conjunctions of different stuff.",
            "Is bio for mattix.",
            "Have a devaluation and you send them by informatics so there's different ones.",
            "And right now it holds.",
            "The conjunctions are different story, so it's it can be a single argument or conjunction of two arguments.",
            "So basically I mean that configuration like I blah blah blah XI index and unlikely to be best surgeon engine.",
            "So they probably have a different role.",
            "And then we want to encode that argument.",
            "Clusterings are related in subway between the predicates.",
            "So essentially what it means that to express different action we use the same language anyway, right?",
            "So the same types of argument clustering should be performed, so this kind of multitask argument clustering really.",
            "So now I I."
        ],
        [
            "This trade, the generative story for our model.",
            "So on the left I will have illustration of this simplified generative story.",
            "On the right I will show how data is assumed to be generated from the model.",
            "So say this structure is 3, so we start with the with the root of the tree."
        ],
        [
            "And we start by generating semantic class for the route.",
            "So say we decided that we want to generate the request.",
            "So then we go in generate request."
        ],
        [
            "Semantic class by shost and drawing syntactic tree fragments associated with this semantic class.",
            "So in this case we decided to generate gave an order, but we could say request and just order and then we go."
        ],
        [
            "We're also in the simplest case, we will just completely independently overalls and decide if you want generator or."
        ],
        [
            "Or not, and we generate A roll by first."
        ],
        [
            "Choosing syntactic signature and then choosing what semantic role fills this argument, so request I usually made by."
        ],
        [
            "People, so we would generate sales person here and then person then records basically and recursion here.",
            "There is no."
        ],
        [
            "Much recourse here.",
            "So we just generated Peters a great serialization, and then we continue with generates."
        ],
        [
            "Second, roll and we we Rico."
        ],
        [
            "Earth and we generate restriction structure under knees.",
            "OK, so this is basically an idea of how this model generates the data."
        ],
        [
            "A couple of technical details, so this slide might not be very clear for many of you, but you'll get back on track in a moment, so this is some standard machinery used to encode our assumptions.",
            "I'll just go over a couple of these things so.",
            "To encode sparsity flow distribution, we basically assume that we use Heroku directly, processes which include sparsity.",
            "So in cost parts distribution of a syntactic tree fragments this again is the same dealership process, but with the base distribution over over trees.",
            "So this machinery is very similar to what is used for example in the in parsing in the context of Byron tree substitution grammars, but you can basically think of these models, kind of entry substitution, grammar with the labeling on top of fragments, and labeling how the fragments relate to each other.",
            "And then I won't go into details about sizes, so you can look it up in our papers.",
            "So to do inference we use MCMC approach.",
            "So we use a form of Metropost Metropolis Hastings sampling is split and merge moves.",
            "And we use some basic proposals based on the distribution of similarity to drive learning curve this model."
        ],
        [
            "So this is all what I wanted to mention about the model.",
            "Now let's look into cross lingual extension and cross lingual extension is actually quite similar to what we have seen in many papers.",
            "Many presentations today, some kind of regularization on parallel data."
        ],
        [
            "So let's consider Huawei.",
            "Intuitively, we would want to do this cross lingual and so this is the blame alternation so you can English say in English, say Peter Blank Mary for planning it.",
            "After you can say Peter blame pollen in the test and Mary so both equally good.",
            "But if you want to learn how argument keys are clustered for this for this predicate, it's going to be tricky.",
            "First of all, because Selectional preferences are not very restricted, which means that it accepts basically almost any arguments.",
            "So anyone can blame anything on anyone, basically so this will be really hard, and then this alternation is very specific to this verb, so you want won't be able to make our multi task learning to work to help.",
            "In this situation."
        ],
        [
            "So.",
            "Yeah, and in practice, of course when I say it's hard to learn.",
            "I mean that in the data we don't have the same alternation and verbalize the same situation device multiple ways.",
            "So we learn from the situations where first it was about Peter and Mary and then we have something about King James and Guy Fawkes for example.",
            "And this is not enough."
        ],
        [
            "Trivial task to so induce.",
            "Luckily we don't have this alternation for German, So what we hope that's in German is very easy.",
            "We will be able to transfer this information from German to English and."
        ],
        [
            "How's this we expect to happen, so we use parallel data, so this blue blue lines correspond to avert avert alignments, which give rise to argument alignments.",
            "So based on standard MD techniques we get them and we want to enforce basically one to one correspondence between the rules so."
        ],
        [
            "So you would be happy if Rolag responds."
        ],
        [
            "One in this case."
        ],
        [
            "Is B22 in both cases Cetus three inverse cases, so we want to favor this type of configuration what?"
        ],
        [
            "We don't want to happen is inconsistent mapping, so that's something we want to penalize for."
        ],
        [
            "So this is this.",
            "Extension basically can be formulated as a type of.",
            "Penalty, which is very similar to his expectation agriterra.",
            "Or if you like posterior regularization criteria.",
            "Again, I won't go into technical details.",
            "You can look down look them up."
        ],
        [
            "In the paper.",
            "OK, so let's see how well it is this."
        ],
        [
            "Model actually works, but first let's look in a lingo setting corner on the biomedical data and 1st in a qualitative evaluation.",
            "So what kind of semantic classes our model is capable of inducing?",
            "So let's just look in the Class 6.",
            "So this class seems fairly nice if you're a little bit familiar with biomedical domain, so these are different type of blood cells, and this might seem like a very nice idea.",
            "But the problem is that for some applications this class can be.",
            "Might not have enough granularity, basically because for example it put together be lymphocyte and T lymphocytes.",
            "So this might be not not a perfect idea.",
            "Eight class eight really makes general linguistic semantics is very happy because it corresponds very well to the frame.",
            "Net frame course change position on a scale, but from application point of view it might be somewhat problematic because it put together things like increase and decrease.",
            "So basically antonyms are in the same semantic loss, which is not a issue.",
            "Actually, not particularly surprising since Sapien a similar context and we didn't do anything specific to file this.",
            "Behavior."
        ],
        [
            "So now let's look into evaluation on question answering.",
            "So this slide you already saw in the very beginning of my talk.",
            "So I want stop."
        ],
        [
            "Here and so we use the same evaluation as was used in, so we use the same baseline as they did, so we have.",
            "Keyboard much we could not match with syntax and a couple of information extraction.",
            "Baselines, like Textron your resolver adored and use be biases our system so green bark responds to the number of answers.",
            "A number of questions answered correctly.",
            "Read part of the market response to to the number of mistakes so you can see that both our system animal and perform quite.",
            "Much better than other systems.",
            "And probably this has to do with joint modeling used in the in the model, but also you can see that our system somewhat underperforms comparing to a Markov logic networks, so that probably 2 reasons for this.",
            "First of all, I'm not sure this difference is actually very significant, because 55% of mistakes are just due to two classes which we wish were two cores.",
            "Anyway, you should notice this data set is fairly small, so.",
            "You should be a really conservative about small changes, but another thing we didn't try to tune our model particularly well because we used only visual inspection.",
            "Since there is no real held at held out data for this data set, so we also did an attempt to use any kind of techniques to prevent antonymy, whereas mbse use some type of here's patterns essentially to get rid of Antonia.",
            "So yeah.",
            "Anyway, that's the story here is that this?",
            "Works better than syntax, and syntax is usually syntax and lexical machines.",
            "This is not not so easy to."
        ],
        [
            "Eat in general.",
            "So now let's look into Revelation.",
            "We become compare induction of roles to the results to the linguist annotation annotation provided by language.",
            "So the standard program data set.",
            "So here we relate only role induction component.",
            "So on the left we have three state of data models and on the right we have a syntactic baseline, which is basically the best possible mapping from syntactic function to semantic role, and you can see that our system again performs better comparing to the previous work, and in a sense it's nice to see that we actually beat in this setting syntactic baseline, which is not exactly the case with the previous work.",
            "So we also have results on a frame net data, but frame data is a little bit problematic for relation and we can take this offline."
        ],
        [
            "OK, so now let's look in cross lingual induction again.",
            "Here we have a late on the on the level of roles and we take a pair of language is German and English and on the left group the leftmost group of the bars monolingual results, and then we have crossing guards and we have a syntactic baseline, so we get.",
            "You can see we get improvement on German by inducing since cross Lingually, but unfortunately we almost don't get any improvement on English, so improvement in English is.",
            "Very tiny, but in both cases we essentially beat the syntactic baseline.",
            "So in a sense, we're doing something meaningful, so we.",
            "The best for German and for English.",
            "OK, so."
        ],
        [
            "So this was all about inducing frame net style semantic representation, so I want to say a couple of years now about inducing representations of words and phrases.",
            "So this is the second half, but this second half going to be considerably shorter.",
            "Crossing.",
            "Parallel.",
            "Put expected so it's only used to avoid these, right?",
            "We don't use any kind of lexical information or any other sufficient statistics from parallel data source.",
            "That is that because.",
            "No, this is to make this comparison air really fair, because OK, may I think that basically points we want you to have the same setup as previously or canal in goal setting and they used on the Pro Bank for this evaluation.",
            "And then we also wanted to complain in crossing going the same, exactly the same setup.",
            "So and I should of course mentions that the point of unsupervised learning is that we want to use a lot of a lot of data and not just.",
            "40,000 sentences of probank, so we are now capable of running our experiments on a much larger data set, so Nancy, and we're getting quite a bit of improvement.",
            "So a couple of percent improvement from running.",
            "Running on large datasets, but the problem is only only our results.",
            "We can't really compare with other people's work in this setting.",
            "OK, so."
        ],
        [
            "About induction of phrase invert representations so, but before this I mentioned some problems.",
            "In general it's inducing clustering representations because clusters are easy to use, but there's some problems with them because in general in clustering, so you would expect to have several income, incompatible clusterings and so.",
            "For example, a dog is a cat of appeared and Wolf.",
            "If you're looking at biological classification.",
            "So this is something which is not not trivial in Cortana clustering and so one way to approach this is to use some type of embedding.",
            "Some type of feature latent feature representations and then we can encode this a different levels of granularity.",
            "We can encode among multiple incompatible clustering and in a way it's easier to deal with Compositionality's which is important for modeling phrases so."
        ],
        [
            "In general, we what we start with, we start with words right and we can get sometimes of embedding individual language."
        ],
        [
            "So how to get embedding conditioned visual language?",
            "We know quite well, so there are many techniques for doing this."
        ],
        [
            "And but what we?"
        ],
        [
            "Use we use parallel data to regularize as clouds in a way.",
            "Make them close to each other.",
            "So what we?"
        ],
        [
            "We want in the end we want that the this representations for two languages I embedded in the same space.",
            "So you get like market an marked for German close to each other.",
            "And so how do we consider how we treat this problem?",
            "We treat this problem as a multi task learning, so each word is basically an individual task and relatedness is with this task is derived from conquering statistics on a bilingual data.",
            "And this is work in a way, the first address.",
            "But as we see there are many people now working on this, so I mean it's fairly obvious idea to try to generalize distributed representations to cross lingual learning, so it's not."
        ],
        [
            "Not a big surprise.",
            "So we can see the specific type of multi task learning.",
            "So in general multi task learning you have several task and you want to learn representation for each task.",
            "Trying to exploit some kind of relation between the task.",
            "But what is characterizes this specific type of multi task learning that we know we have some prior knowledge about the relation between tasks, so there is some matrix vision costs, how similar some some tasks are.",
            "So what you have you have objective.",
            "Individual task and then you have regularizer which ties representations for each task together.",
            "So it basically tells you that some vectors we want to learn to be close to each other for some of the actors we really don't care that much.",
            "So what we use, we use this regularizer to in our setting of inducing vert representations so."
        ],
        [
            "So what we have we have.",
            "First we consider them as task and we want to learn a vector for for each word and A is.",
            "Using.",
            "Yeah, OK, so I will talk about the Matrix, say, but basically since.",
            "I think it will be a little clearer in a two slice if it's one big clear look, I'll explain what's going on actually.",
            "So what we want we want.",
            "We have an objective on individual language.",
            "So basically some objective objective fish drives this embedding.",
            "So, for example, narrow probabilistic language model.",
            "So that's what we use and then we have a component which forces this representations for frequently translated words to be close to each other so.",
            "You should note that we in general use small parallel data, so many words will not appear in.",
            "The regularizers will appear only on the part corresponding to the learning.",
            "Learning embedding, so we hope that we will be able to train your eyes to them anyway."
        ],
        [
            "OK, so we mentioned this matrix A which is we play this role in a regularizer.",
            "So how do we define this matrix and what kind of roses mattix place so so it's natural to think in this case of words as nodes and we have a bipartite graph.",
            "So verse for one language works for another language.",
            "Yes, I mean we have a frequency.",
            "How frequently this triggers aligned in a parallel data.",
            "So we can define for this graph the graph operation in one of standard ways for the weighted graph, and then the interaction matrix is basically identity matrix 4 + a graph Laplacian.",
            "So I'm sure this is familiar to some of you because its component which plays a key role in like label it label propagation algorithm.",
            "So inverse of these metrics will tell you how much information you essentially be needs to be propagated from one word.",
            "To another, so it will tell you basically solution of this label propagation problem and if you use online learning, so what's going?",
            "What's going to happen if you observe some work we would update not only waits for this world, but he will also update weights to other related words and how much this is this update is given by the component of the inverse matrix.",
            "So inverse matrix is inverting.",
            "This matrix is a little tricky because it's very large, but we can use standard.",
            "Estimation techniques to do this.",
            "Is it a little more clear now, OK?"
        ],
        [
            "OK, so let's just look at how this embedded looks like.",
            "So we have verse and we have the closest word in English, and so in original language and in German.",
            "So let's just look in the set so it looks pretty good for English, right?",
            "So they basically all these words are synonyms.",
            "Presently, they voted the closest to itself, right?",
            "So then we have a similar words and then for German the closest word is a form of dragon, which is say and.",
            "Then we have a form of McLaren which means clarify.",
            "So this looks looks fairly nicely."
        ],
        [
            "So now let's look how well it works.",
            "So first of all we are related on our document classification problem and we can pay it with.",
            "So it's basically direct projection and classification.",
            "So we don't have as nice experiments on like insect parts in case Ryan show it.",
            "So we just looking at relatively simple problems.",
            "So for document classification.",
            "We don't have any label data in target language, so we just use label data in the source.",
            "And see that we have to baseline which one is machine translation baseline.",
            "So we just translate the document and running through the classifier another one is glossed baseline.",
            "So we translate word by word so we can see that our performance is much better is distributed representations.",
            "And anyway it's for us.",
            "It's more like a test set.",
            "This representation is informative because if you would really want to learn distributed representation for document classification, we probably would do it in a different way.",
            "So we'll add for example, discriminative component in the objective function.",
            "OK, I'm finishing up in."
        ],
        [
            "OK, so just a couple of fields about ongoing work, so we started to look into this because in collaboration with people Johns Hopkins we want to use a distributor representations of words and phrases in the context of machine translation, poor resource machine translation where you have relatively little of parallel data but a lot of modeling or data, and so it would have been shown with some type of distributed similarity features.",
            "Vector space models are useful for for this task, Even so in not completely realistic settings.",
            "So what do we do?",
            "For the moment, we just compare how well our representations campaign with this representation previously used in machine translation in Lexicon induction.",
            "So basically you rank translations in rank verse.",
            "We should translate, and this is on three on 3 languages.",
            "Relatively poor resource languages, I would say.",
            "So you can see that across all the setups, distributed representations perform considerably better than then bills.",
            "This feature used previously and the translation which is induced from a small parallel corpora corpus.",
            "But it's a bit over optimistic because it's averaged over word type, so it's not.",
            "It doesn't correspond to the frequency in the actual."
        ],
        [
            "I said so in a way it emphasizes in rare words more than."
        ],
        [
            "Frequent words so you can use a similar technique.",
            "Very similar techniques for inducing phrase and cross lingual phrase representations, and maybe inducing phrase representation so you can regularize in the same way alignment or phrases, but you can use some phrasing building models, for example work of records.",
            "My colleagues at Stanford so and this is."
        ],
        [
            "Basically my last slide.",
            "I talked about different types of semantic representations about inducing representations of event phrases a little bit words and.",
            "And then I talked about distributed and clustering representation.",
            "So basically these two dimensions to this this work and now I'm starting to look into ways how we can actually induce a distributional features for events and situations, because this will allow me to deal with.",
            "And Ularity and compositionality, and etc.",
            "But I think you, if you're interested in this.",
            "You should also looking at paper by Jonathan and colleagues, where they look and in more context of statistical relation only running for binary relations.",
            "How to embed this binary relations.",
            "I think it's pretty cool work, OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so this is joint work with my postdoc, Alex Klementieff and the Student be number I and also with collaborators at Johns Hopkins University, in part of which is related to me.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Team OK, so first of all, why do we care about semantic representations in general?",
                    "label": 1
                },
                {
                    "sent": "Probably because we want to abstract away from specifics of lexical and syntactic representations.",
                    "label": 1
                },
                {
                    "sent": "So consider just a very simple example of question answering tasks.",
                    "label": 1
                },
                {
                    "sent": "So we have a question and we want to find an answer in a text and this actually actual answers produced by our system.",
                    "label": 0
                },
                {
                    "sent": "So we want in the first case we want to figure out that suppliers and block.",
                    "label": 0
                },
                {
                    "sent": "I mean essentially the same thing and map arguments.",
                    "label": 0
                },
                {
                    "sent": "And in the second case we also want to figure out that entities TNF Alpha and too many crosses have the same thing as well.",
                    "label": 0
                },
                {
                    "sent": "So we hope to provide some form abstraction, which is basically tell you that these are the same things.",
                    "label": 0
                },
                {
                    "sent": "And the second question is of course, why do we want to do it?",
                    "label": 0
                },
                {
                    "sent": "Cross lingually so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, 2 answers to these questions.",
                    "label": 0
                },
                {
                    "sent": "So first of all we hope to get an improvement in each individual language by doing it cross lingually so it has been shown for syntax that have been shown for morphology and so why not?",
                    "label": 0
                },
                {
                    "sent": "For semantics?",
                    "label": 0
                },
                {
                    "sent": "Intuitively it seems it should be easier for semantics because if syntax.",
                    "label": 0
                },
                {
                    "sent": "Is there some hidden correspondences between syntactic representations in two languages.",
                    "label": 0
                },
                {
                    "sent": "For semantics we should model more lesson for the consistency, but in practice things are not not as simple.",
                    "label": 0
                },
                {
                    "sent": "And of course another reason we don't need to emphasize this.",
                    "label": 0
                },
                {
                    "sent": "Shop across lingo representations are useful by themselves for a number of cross lingo.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is this an outline of my talk?",
                    "label": 0
                },
                {
                    "sent": "So I'll first talk about our model for unsupervised induction of shallow semantics, and then I'll talk about it's across into extension and the second part will be about semantic representations.",
                    "label": 1
                },
                {
                    "sent": "More level of phrases on a level of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Words.",
                    "label": 0
                },
                {
                    "sent": "So for the first full of about events, so one way to define the semantics of events is traditional to use the notion of semantic frame, right?",
                    "label": 0
                },
                {
                    "sent": "So semantic frame is conceptual structure which describes an event and its participants an or an object in its properties.",
                    "label": 1
                },
                {
                    "sent": "So for example, you can have very simple sentence Jack opens the lock with a paper clip and here we have what is called closure opening frame in the standard frame net repository.",
                    "label": 1
                },
                {
                    "sent": "And there's three semantic roles realized in this case.",
                    "label": 0
                },
                {
                    "sent": "So we have an agent, Jacquie.",
                    "label": 0
                },
                {
                    "sent": "If a patient lower can instrument OK, but in this frame actually includes many more semantic roles which are not realized at all, so this is a hard problem to induce this type of representation.",
                    "label": 0
                },
                {
                    "sent": "Sorry, because we you can see it's very tight to lexical information, its role sign, general predicates.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you cannot set it up, and of course the question is how much we can rely here on how predictive syntactic and lexical information we have to rely on syntactically information, because there is nothing else but how is this task is an so on the left?",
                    "label": 1
                },
                {
                    "sent": "I have different types of so called databases alternations, so you can reverse this situation of Jean breaking the window and basically in a couple of ways so you can say John broke the window, window broken, etc.",
                    "label": 1
                },
                {
                    "sent": "So the one thing to notice in this example since the first case.",
                    "label": 1
                },
                {
                    "sent": "Agent corresponds to syntactic subject in the second case in septic subject is actually passions affected entities.",
                    "label": 1
                },
                {
                    "sent": "So you see this correspondence is not not really as trivial.",
                    "label": 0
                },
                {
                    "sent": "And then you also can express the same situation using different verbs, or maybe even a nominal constructions.",
                    "label": 1
                },
                {
                    "sent": "OK, so natural ways to apply supervised learning to this setting about this.",
                    "label": 0
                },
                {
                    "sent": "This task is really challenging because data sets providing general very low coverage.",
                    "label": 1
                },
                {
                    "sent": "It's very tight to lexical domain and then we don't have this data set for many many languages in fact, so only for a handful of languages we have.",
                    "label": 0
                },
                {
                    "sent": "This information so let's.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try to define our task little bit more formally.",
                    "label": 0
                },
                {
                    "sent": "So what we want to predict.",
                    "label": 0
                },
                {
                    "sent": "If you want to predict so-called semantic dependency graph, so in the semantic dependency graph we have edges in court predicate argument relation and labels on edges, semantic roles and labels on nodes are basically semantic frame.",
                    "label": 0
                },
                {
                    "sent": "So types of situations you have.",
                    "label": 0
                },
                {
                    "sent": "But this is a very very simple example.",
                    "label": 0
                },
                {
                    "sent": "So in practice this can be much more complex, so arguments can work their own frame.",
                    "label": 0
                },
                {
                    "sent": "So dress is characterized by style and by creator.",
                    "label": 0
                },
                {
                    "sent": "Arguments can be shared between multiple predicates and etc, so this can be quite complex and then also some of these.",
                    "label": 0
                },
                {
                    "sent": "Situations can be expressed, can be evoked by multiverse expressions, so things like a light light work constructions.",
                    "label": 0
                },
                {
                    "sent": "Person known center cetera.",
                    "label": 0
                },
                {
                    "sent": "So what we want we want to induce this type.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Representations from unlabeled data and since we inducing these representations from unlabeled data, this is actually more like a clustering task.",
                    "label": 0
                },
                {
                    "sent": "So, so this kind of cluster clustering graph induction task, but probably is that we don't induce names, but really do some kind of clustering IDs.",
                    "label": 0
                },
                {
                    "sent": "This really doesn't matter that much.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's see what kind of steps is this induction process actually involves.",
                    "label": 0
                },
                {
                    "sent": "So first of all we need to induce some kind of transform semantic syntactic dependency graph.",
                    "label": 1
                },
                {
                    "sent": "So Ryan mentions that syntactic dependency graph actually mimics the large degree predicate argument structure, so there is not much work to do exist.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, and then we want to induce frames, which basically means to decompose this graph and induce labeling for the fragments in this graph.",
                    "label": 0
                },
                {
                    "sent": "And then we also want to induce.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rolls, which means labeling edges in this graph.",
                    "label": 0
                },
                {
                    "sent": "So we don't care that much about the first stage because it can be handled is not perfectly, but it is relatively high accuracy with simple risztics over with a simple and Alex Kurtz classifier, so it's not as hard problem.",
                    "label": 1
                },
                {
                    "sent": "So what we care about is the 2nd and the third stage, and we model this two sub task within a joint nonparametric buyer and model.",
                    "label": 0
                },
                {
                    "sent": "So we try to induce this representations jointly and this is different from a match of previous work where in general people tried to tackle issues with sub task in elation and using multivariate expression inducing rolls.",
                    "label": 1
                },
                {
                    "sent": "Inducing adverb clusters in etc.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's just talk about inducing as is similar frames or clusters fragment.",
                    "label": 0
                },
                {
                    "sent": "So birth we call semantic classes because even linguists can't really agree with semantic frame.",
                    "label": 1
                },
                {
                    "sent": "What is not so?",
                    "label": 0
                },
                {
                    "sent": "We don't want a machine algorithm to learn to distinguish the tools.",
                    "label": 1
                },
                {
                    "sent": "So we just want to have some kind of clusters and not to confuse the things we call them semantic classes.",
                    "label": 0
                },
                {
                    "sent": "And then induction of semantic cloud basically involves this.",
                    "label": 1
                },
                {
                    "sent": "The following thing.",
                    "label": 0
                },
                {
                    "sent": "So we want to figure out how to essentially cluster different lexemes.",
                    "label": 1
                },
                {
                    "sent": "So break Buster destroying essentially the same thing, and then we also want to be able to compose expressions so to support idiomatic expressions to support terminology etc.",
                    "label": 0
                },
                {
                    "sent": "And of course it's not disjoint us, so if you decided to compose something, then later on we may want to cluster.",
                    "label": 0
                },
                {
                    "sent": "So hell directory we can post and then we might want to cluster is green.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I talked about induction of semantic classes.",
                    "label": 1
                },
                {
                    "sent": "So what basically it involves?",
                    "label": 0
                },
                {
                    "sent": "What is semantical induction?",
                    "label": 0
                },
                {
                    "sent": "And for simplicity let's think that we actually know we already induce the frames since we will be induced in this jointly, we're actually is not actually the case.",
                    "label": 1
                },
                {
                    "sent": "We will use some kind of iterative process, But let's think that for now we know where these frames are.",
                    "label": 1
                },
                {
                    "sent": "So you basically in this situation you have.",
                    "label": 0
                },
                {
                    "sent": "You see why teaching frame is you observe some arguments but you don't know their labeling.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what you want you want to get some type of clustering?",
                    "label": 0
                },
                {
                    "sent": "Basically on this rose.",
                    "label": 0
                },
                {
                    "sent": "So basically you can approach this in some direct way, so you can define some generative story for this and try to induce clustering.",
                    "label": 0
                },
                {
                    "sent": "But the problem with this is the search space is in general quite huge and we want to repeat this process process during inference of the entire model multiple times.",
                    "label": 1
                },
                {
                    "sent": "So we really want to make this clustering step.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very fast.",
                    "label": 0
                },
                {
                    "sent": "So we used an idea which had previously been proposed by Mirella and Joel Long, so that's a couple of years ago and.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to identify arguments with some properties, functions of syntactic frame.",
                    "label": 0
                },
                {
                    "sent": "So let's look just in a very simple example, so this is called elective located drop lucrative proposition drop.",
                    "label": 0
                },
                {
                    "sent": "So you can say Merry climbed up the mountain or you can say Mary climbs the mountain.",
                    "label": 1
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "I need this position and then we can identify arguments for example with this syntactic signature.",
                    "label": 0
                },
                {
                    "sent": "So for the second argument we will have information that we have active voice that is from the right of.",
                    "label": 1
                },
                {
                    "sent": "A predicate and we have a prepositional modification is preposition app, and in the second case we have a direct object, so that's the only difference between the two and when we basically designed this argument key, the idea is to have a relatively high precision, because then we will cluster them instead of clustering argument accuracies.",
                    "label": 0
                },
                {
                    "sent": "And we can get relatively sufficiently car equal.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic intuition.",
                    "label": 0
                },
                {
                    "sent": "With this argument keys.",
                    "label": 0
                },
                {
                    "sent": "So in this case we would just want to cluster this target keys together.",
                    "label": 1
                },
                {
                    "sent": "But for some more complex alternations you will want to have a more complex clustering.",
                    "label": 1
                },
                {
                    "sent": "So far going and keys, so it's not only players of working with kids put together.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I talked about segmentation.",
                    "label": 0
                },
                {
                    "sent": "Basically defines the task.",
                    "label": 0
                },
                {
                    "sent": "Now I will provide some high level description of the model views first of all.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll talk about modeling assumptions.",
                    "label": 1
                },
                {
                    "sent": "So first 2 assumptions we want to encode about the model.",
                    "label": 0
                },
                {
                    "sent": "I basically have to do is sparsity.",
                    "label": 0
                },
                {
                    "sent": "So first one we want rolls to have a sparse distribution of argument fillers.",
                    "label": 1
                },
                {
                    "sent": "So what it actually means, it means that two predicates will be likely to cluster it to be clustered together if they have a similar set of arguments.",
                    "label": 0
                },
                {
                    "sent": "Because this type of move will preserve sparsity.",
                    "label": 0
                },
                {
                    "sent": "And then for each semantic class, we assume that it can be verbalised with sparse distribution over elections, actually always syntactic tree fragments.",
                    "label": 1
                },
                {
                    "sent": "So this for example, winning situation, we can say envying, hold the victory over and a couple other things, but not really flared distribution over dependency tree fragments.",
                    "label": 0
                },
                {
                    "sent": "Then another thing which I'm not going to talk about in detail is that we expect that predicates which have similar syntactic behavior have similar semantics.",
                    "label": 0
                },
                {
                    "sent": "So for some of you familiar with this, it has to do with Beth Levin.",
                    "label": 0
                },
                {
                    "sent": "Levin classes basically live in classes, and this is a useful signal as well if you include in the model.",
                    "label": 0
                },
                {
                    "sent": "And then another signal which is very useful but is very trivial.",
                    "label": 0
                },
                {
                    "sent": "So you in general expect that for every predicate occurrence, single semantic role appears mostly mostly once, so you have usually one agent and one patient, but at most one I would say but.",
                    "label": 0
                },
                {
                    "sent": "We don't include conjunction as care, so it's conjunctions of different stuff.",
                    "label": 0
                },
                {
                    "sent": "Is bio for mattix.",
                    "label": 0
                },
                {
                    "sent": "Have a devaluation and you send them by informatics so there's different ones.",
                    "label": 0
                },
                {
                    "sent": "And right now it holds.",
                    "label": 0
                },
                {
                    "sent": "The conjunctions are different story, so it's it can be a single argument or conjunction of two arguments.",
                    "label": 0
                },
                {
                    "sent": "So basically I mean that configuration like I blah blah blah XI index and unlikely to be best surgeon engine.",
                    "label": 1
                },
                {
                    "sent": "So they probably have a different role.",
                    "label": 0
                },
                {
                    "sent": "And then we want to encode that argument.",
                    "label": 0
                },
                {
                    "sent": "Clusterings are related in subway between the predicates.",
                    "label": 0
                },
                {
                    "sent": "So essentially what it means that to express different action we use the same language anyway, right?",
                    "label": 0
                },
                {
                    "sent": "So the same types of argument clustering should be performed, so this kind of multitask argument clustering really.",
                    "label": 0
                },
                {
                    "sent": "So now I I.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This trade, the generative story for our model.",
                    "label": 0
                },
                {
                    "sent": "So on the left I will have illustration of this simplified generative story.",
                    "label": 0
                },
                {
                    "sent": "On the right I will show how data is assumed to be generated from the model.",
                    "label": 0
                },
                {
                    "sent": "So say this structure is 3, so we start with the with the root of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we start by generating semantic class for the route.",
                    "label": 1
                },
                {
                    "sent": "So say we decided that we want to generate the request.",
                    "label": 0
                },
                {
                    "sent": "So then we go in generate request.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Semantic class by shost and drawing syntactic tree fragments associated with this semantic class.",
                    "label": 0
                },
                {
                    "sent": "So in this case we decided to generate gave an order, but we could say request and just order and then we go.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're also in the simplest case, we will just completely independently overalls and decide if you want generator or.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or not, and we generate A roll by first.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Choosing syntactic signature and then choosing what semantic role fills this argument, so request I usually made by.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People, so we would generate sales person here and then person then records basically and recursion here.",
                    "label": 0
                },
                {
                    "sent": "There is no.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much recourse here.",
                    "label": 0
                },
                {
                    "sent": "So we just generated Peters a great serialization, and then we continue with generates.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second, roll and we we Rico.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Earth and we generate restriction structure under knees.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is basically an idea of how this model generates the data.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A couple of technical details, so this slide might not be very clear for many of you, but you'll get back on track in a moment, so this is some standard machinery used to encode our assumptions.",
                    "label": 0
                },
                {
                    "sent": "I'll just go over a couple of these things so.",
                    "label": 0
                },
                {
                    "sent": "To encode sparsity flow distribution, we basically assume that we use Heroku directly, processes which include sparsity.",
                    "label": 0
                },
                {
                    "sent": "So in cost parts distribution of a syntactic tree fragments this again is the same dealership process, but with the base distribution over over trees.",
                    "label": 0
                },
                {
                    "sent": "So this machinery is very similar to what is used for example in the in parsing in the context of Byron tree substitution grammars, but you can basically think of these models, kind of entry substitution, grammar with the labeling on top of fragments, and labeling how the fragments relate to each other.",
                    "label": 0
                },
                {
                    "sent": "And then I won't go into details about sizes, so you can look it up in our papers.",
                    "label": 0
                },
                {
                    "sent": "So to do inference we use MCMC approach.",
                    "label": 0
                },
                {
                    "sent": "So we use a form of Metropost Metropolis Hastings sampling is split and merge moves.",
                    "label": 0
                },
                {
                    "sent": "And we use some basic proposals based on the distribution of similarity to drive learning curve this model.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is all what I wanted to mention about the model.",
                    "label": 0
                },
                {
                    "sent": "Now let's look into cross lingual extension and cross lingual extension is actually quite similar to what we have seen in many papers.",
                    "label": 0
                },
                {
                    "sent": "Many presentations today, some kind of regularization on parallel data.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's consider Huawei.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, we would want to do this cross lingual and so this is the blame alternation so you can English say in English, say Peter Blank Mary for planning it.",
                    "label": 0
                },
                {
                    "sent": "After you can say Peter blame pollen in the test and Mary so both equally good.",
                    "label": 0
                },
                {
                    "sent": "But if you want to learn how argument keys are clustered for this for this predicate, it's going to be tricky.",
                    "label": 0
                },
                {
                    "sent": "First of all, because Selectional preferences are not very restricted, which means that it accepts basically almost any arguments.",
                    "label": 0
                },
                {
                    "sent": "So anyone can blame anything on anyone, basically so this will be really hard, and then this alternation is very specific to this verb, so you want won't be able to make our multi task learning to work to help.",
                    "label": 0
                },
                {
                    "sent": "In this situation.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and in practice, of course when I say it's hard to learn.",
                    "label": 0
                },
                {
                    "sent": "I mean that in the data we don't have the same alternation and verbalize the same situation device multiple ways.",
                    "label": 0
                },
                {
                    "sent": "So we learn from the situations where first it was about Peter and Mary and then we have something about King James and Guy Fawkes for example.",
                    "label": 0
                },
                {
                    "sent": "And this is not enough.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trivial task to so induce.",
                    "label": 0
                },
                {
                    "sent": "Luckily we don't have this alternation for German, So what we hope that's in German is very easy.",
                    "label": 0
                },
                {
                    "sent": "We will be able to transfer this information from German to English and.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How's this we expect to happen, so we use parallel data, so this blue blue lines correspond to avert avert alignments, which give rise to argument alignments.",
                    "label": 0
                },
                {
                    "sent": "So based on standard MD techniques we get them and we want to enforce basically one to one correspondence between the rules so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you would be happy if Rolag responds.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One in this case.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is B22 in both cases Cetus three inverse cases, so we want to favor this type of configuration what?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't want to happen is inconsistent mapping, so that's something we want to penalize for.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is this.",
                    "label": 0
                },
                {
                    "sent": "Extension basically can be formulated as a type of.",
                    "label": 0
                },
                {
                    "sent": "Penalty, which is very similar to his expectation agriterra.",
                    "label": 0
                },
                {
                    "sent": "Or if you like posterior regularization criteria.",
                    "label": 0
                },
                {
                    "sent": "Again, I won't go into technical details.",
                    "label": 0
                },
                {
                    "sent": "You can look down look them up.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the paper.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see how well it is this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model actually works, but first let's look in a lingo setting corner on the biomedical data and 1st in a qualitative evaluation.",
                    "label": 0
                },
                {
                    "sent": "So what kind of semantic classes our model is capable of inducing?",
                    "label": 0
                },
                {
                    "sent": "So let's just look in the Class 6.",
                    "label": 0
                },
                {
                    "sent": "So this class seems fairly nice if you're a little bit familiar with biomedical domain, so these are different type of blood cells, and this might seem like a very nice idea.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that for some applications this class can be.",
                    "label": 0
                },
                {
                    "sent": "Might not have enough granularity, basically because for example it put together be lymphocyte and T lymphocytes.",
                    "label": 0
                },
                {
                    "sent": "So this might be not not a perfect idea.",
                    "label": 0
                },
                {
                    "sent": "Eight class eight really makes general linguistic semantics is very happy because it corresponds very well to the frame.",
                    "label": 0
                },
                {
                    "sent": "Net frame course change position on a scale, but from application point of view it might be somewhat problematic because it put together things like increase and decrease.",
                    "label": 0
                },
                {
                    "sent": "So basically antonyms are in the same semantic loss, which is not a issue.",
                    "label": 0
                },
                {
                    "sent": "Actually, not particularly surprising since Sapien a similar context and we didn't do anything specific to file this.",
                    "label": 0
                },
                {
                    "sent": "Behavior.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's look into evaluation on question answering.",
                    "label": 0
                },
                {
                    "sent": "So this slide you already saw in the very beginning of my talk.",
                    "label": 0
                },
                {
                    "sent": "So I want stop.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here and so we use the same evaluation as was used in, so we use the same baseline as they did, so we have.",
                    "label": 0
                },
                {
                    "sent": "Keyboard much we could not match with syntax and a couple of information extraction.",
                    "label": 0
                },
                {
                    "sent": "Baselines, like Textron your resolver adored and use be biases our system so green bark responds to the number of answers.",
                    "label": 0
                },
                {
                    "sent": "A number of questions answered correctly.",
                    "label": 0
                },
                {
                    "sent": "Read part of the market response to to the number of mistakes so you can see that both our system animal and perform quite.",
                    "label": 0
                },
                {
                    "sent": "Much better than other systems.",
                    "label": 0
                },
                {
                    "sent": "And probably this has to do with joint modeling used in the in the model, but also you can see that our system somewhat underperforms comparing to a Markov logic networks, so that probably 2 reasons for this.",
                    "label": 0
                },
                {
                    "sent": "First of all, I'm not sure this difference is actually very significant, because 55% of mistakes are just due to two classes which we wish were two cores.",
                    "label": 0
                },
                {
                    "sent": "Anyway, you should notice this data set is fairly small, so.",
                    "label": 0
                },
                {
                    "sent": "You should be a really conservative about small changes, but another thing we didn't try to tune our model particularly well because we used only visual inspection.",
                    "label": 0
                },
                {
                    "sent": "Since there is no real held at held out data for this data set, so we also did an attempt to use any kind of techniques to prevent antonymy, whereas mbse use some type of here's patterns essentially to get rid of Antonia.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "Anyway, that's the story here is that this?",
                    "label": 0
                },
                {
                    "sent": "Works better than syntax, and syntax is usually syntax and lexical machines.",
                    "label": 0
                },
                {
                    "sent": "This is not not so easy to.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eat in general.",
                    "label": 0
                },
                {
                    "sent": "So now let's look into Revelation.",
                    "label": 0
                },
                {
                    "sent": "We become compare induction of roles to the results to the linguist annotation annotation provided by language.",
                    "label": 0
                },
                {
                    "sent": "So the standard program data set.",
                    "label": 0
                },
                {
                    "sent": "So here we relate only role induction component.",
                    "label": 0
                },
                {
                    "sent": "So on the left we have three state of data models and on the right we have a syntactic baseline, which is basically the best possible mapping from syntactic function to semantic role, and you can see that our system again performs better comparing to the previous work, and in a sense it's nice to see that we actually beat in this setting syntactic baseline, which is not exactly the case with the previous work.",
                    "label": 0
                },
                {
                    "sent": "So we also have results on a frame net data, but frame data is a little bit problematic for relation and we can take this offline.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now let's look in cross lingual induction again.",
                    "label": 0
                },
                {
                    "sent": "Here we have a late on the on the level of roles and we take a pair of language is German and English and on the left group the leftmost group of the bars monolingual results, and then we have crossing guards and we have a syntactic baseline, so we get.",
                    "label": 0
                },
                {
                    "sent": "You can see we get improvement on German by inducing since cross Lingually, but unfortunately we almost don't get any improvement on English, so improvement in English is.",
                    "label": 0
                },
                {
                    "sent": "Very tiny, but in both cases we essentially beat the syntactic baseline.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, we're doing something meaningful, so we.",
                    "label": 0
                },
                {
                    "sent": "The best for German and for English.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was all about inducing frame net style semantic representation, so I want to say a couple of years now about inducing representations of words and phrases.",
                    "label": 0
                },
                {
                    "sent": "So this is the second half, but this second half going to be considerably shorter.",
                    "label": 0
                },
                {
                    "sent": "Crossing.",
                    "label": 0
                },
                {
                    "sent": "Parallel.",
                    "label": 0
                },
                {
                    "sent": "Put expected so it's only used to avoid these, right?",
                    "label": 0
                },
                {
                    "sent": "We don't use any kind of lexical information or any other sufficient statistics from parallel data source.",
                    "label": 0
                },
                {
                    "sent": "That is that because.",
                    "label": 0
                },
                {
                    "sent": "No, this is to make this comparison air really fair, because OK, may I think that basically points we want you to have the same setup as previously or canal in goal setting and they used on the Pro Bank for this evaluation.",
                    "label": 0
                },
                {
                    "sent": "And then we also wanted to complain in crossing going the same, exactly the same setup.",
                    "label": 0
                },
                {
                    "sent": "So and I should of course mentions that the point of unsupervised learning is that we want to use a lot of a lot of data and not just.",
                    "label": 0
                },
                {
                    "sent": "40,000 sentences of probank, so we are now capable of running our experiments on a much larger data set, so Nancy, and we're getting quite a bit of improvement.",
                    "label": 0
                },
                {
                    "sent": "So a couple of percent improvement from running.",
                    "label": 0
                },
                {
                    "sent": "Running on large datasets, but the problem is only only our results.",
                    "label": 0
                },
                {
                    "sent": "We can't really compare with other people's work in this setting.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About induction of phrase invert representations so, but before this I mentioned some problems.",
                    "label": 0
                },
                {
                    "sent": "In general it's inducing clustering representations because clusters are easy to use, but there's some problems with them because in general in clustering, so you would expect to have several income, incompatible clusterings and so.",
                    "label": 0
                },
                {
                    "sent": "For example, a dog is a cat of appeared and Wolf.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at biological classification.",
                    "label": 0
                },
                {
                    "sent": "So this is something which is not not trivial in Cortana clustering and so one way to approach this is to use some type of embedding.",
                    "label": 0
                },
                {
                    "sent": "Some type of feature latent feature representations and then we can encode this a different levels of granularity.",
                    "label": 0
                },
                {
                    "sent": "We can encode among multiple incompatible clustering and in a way it's easier to deal with Compositionality's which is important for modeling phrases so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general, we what we start with, we start with words right and we can get sometimes of embedding individual language.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how to get embedding conditioned visual language?",
                    "label": 0
                },
                {
                    "sent": "We know quite well, so there are many techniques for doing this.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And but what we?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use we use parallel data to regularize as clouds in a way.",
                    "label": 0
                },
                {
                    "sent": "Make them close to each other.",
                    "label": 0
                },
                {
                    "sent": "So what we?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want in the end we want that the this representations for two languages I embedded in the same space.",
                    "label": 0
                },
                {
                    "sent": "So you get like market an marked for German close to each other.",
                    "label": 0
                },
                {
                    "sent": "And so how do we consider how we treat this problem?",
                    "label": 0
                },
                {
                    "sent": "We treat this problem as a multi task learning, so each word is basically an individual task and relatedness is with this task is derived from conquering statistics on a bilingual data.",
                    "label": 0
                },
                {
                    "sent": "And this is work in a way, the first address.",
                    "label": 0
                },
                {
                    "sent": "But as we see there are many people now working on this, so I mean it's fairly obvious idea to try to generalize distributed representations to cross lingual learning, so it's not.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not a big surprise.",
                    "label": 0
                },
                {
                    "sent": "So we can see the specific type of multi task learning.",
                    "label": 0
                },
                {
                    "sent": "So in general multi task learning you have several task and you want to learn representation for each task.",
                    "label": 0
                },
                {
                    "sent": "Trying to exploit some kind of relation between the task.",
                    "label": 0
                },
                {
                    "sent": "But what is characterizes this specific type of multi task learning that we know we have some prior knowledge about the relation between tasks, so there is some matrix vision costs, how similar some some tasks are.",
                    "label": 0
                },
                {
                    "sent": "So what you have you have objective.",
                    "label": 0
                },
                {
                    "sent": "Individual task and then you have regularizer which ties representations for each task together.",
                    "label": 0
                },
                {
                    "sent": "So it basically tells you that some vectors we want to learn to be close to each other for some of the actors we really don't care that much.",
                    "label": 0
                },
                {
                    "sent": "So what we use, we use this regularizer to in our setting of inducing vert representations so.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we have we have.",
                    "label": 0
                },
                {
                    "sent": "First we consider them as task and we want to learn a vector for for each word and A is.",
                    "label": 0
                },
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so I will talk about the Matrix, say, but basically since.",
                    "label": 0
                },
                {
                    "sent": "I think it will be a little clearer in a two slice if it's one big clear look, I'll explain what's going on actually.",
                    "label": 0
                },
                {
                    "sent": "So what we want we want.",
                    "label": 0
                },
                {
                    "sent": "We have an objective on individual language.",
                    "label": 0
                },
                {
                    "sent": "So basically some objective objective fish drives this embedding.",
                    "label": 0
                },
                {
                    "sent": "So, for example, narrow probabilistic language model.",
                    "label": 0
                },
                {
                    "sent": "So that's what we use and then we have a component which forces this representations for frequently translated words to be close to each other so.",
                    "label": 0
                },
                {
                    "sent": "You should note that we in general use small parallel data, so many words will not appear in.",
                    "label": 0
                },
                {
                    "sent": "The regularizers will appear only on the part corresponding to the learning.",
                    "label": 0
                },
                {
                    "sent": "Learning embedding, so we hope that we will be able to train your eyes to them anyway.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we mentioned this matrix A which is we play this role in a regularizer.",
                    "label": 0
                },
                {
                    "sent": "So how do we define this matrix and what kind of roses mattix place so so it's natural to think in this case of words as nodes and we have a bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "So verse for one language works for another language.",
                    "label": 0
                },
                {
                    "sent": "Yes, I mean we have a frequency.",
                    "label": 0
                },
                {
                    "sent": "How frequently this triggers aligned in a parallel data.",
                    "label": 0
                },
                {
                    "sent": "So we can define for this graph the graph operation in one of standard ways for the weighted graph, and then the interaction matrix is basically identity matrix 4 + a graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So I'm sure this is familiar to some of you because its component which plays a key role in like label it label propagation algorithm.",
                    "label": 0
                },
                {
                    "sent": "So inverse of these metrics will tell you how much information you essentially be needs to be propagated from one word.",
                    "label": 0
                },
                {
                    "sent": "To another, so it will tell you basically solution of this label propagation problem and if you use online learning, so what's going?",
                    "label": 0
                },
                {
                    "sent": "What's going to happen if you observe some work we would update not only waits for this world, but he will also update weights to other related words and how much this is this update is given by the component of the inverse matrix.",
                    "label": 0
                },
                {
                    "sent": "So inverse matrix is inverting.",
                    "label": 0
                },
                {
                    "sent": "This matrix is a little tricky because it's very large, but we can use standard.",
                    "label": 0
                },
                {
                    "sent": "Estimation techniques to do this.",
                    "label": 0
                },
                {
                    "sent": "Is it a little more clear now, OK?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's just look at how this embedded looks like.",
                    "label": 0
                },
                {
                    "sent": "So we have verse and we have the closest word in English, and so in original language and in German.",
                    "label": 0
                },
                {
                    "sent": "So let's just look in the set so it looks pretty good for English, right?",
                    "label": 0
                },
                {
                    "sent": "So they basically all these words are synonyms.",
                    "label": 0
                },
                {
                    "sent": "Presently, they voted the closest to itself, right?",
                    "label": 0
                },
                {
                    "sent": "So then we have a similar words and then for German the closest word is a form of dragon, which is say and.",
                    "label": 0
                },
                {
                    "sent": "Then we have a form of McLaren which means clarify.",
                    "label": 0
                },
                {
                    "sent": "So this looks looks fairly nicely.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's look how well it works.",
                    "label": 0
                },
                {
                    "sent": "So first of all we are related on our document classification problem and we can pay it with.",
                    "label": 0
                },
                {
                    "sent": "So it's basically direct projection and classification.",
                    "label": 0
                },
                {
                    "sent": "So we don't have as nice experiments on like insect parts in case Ryan show it.",
                    "label": 0
                },
                {
                    "sent": "So we just looking at relatively simple problems.",
                    "label": 0
                },
                {
                    "sent": "So for document classification.",
                    "label": 0
                },
                {
                    "sent": "We don't have any label data in target language, so we just use label data in the source.",
                    "label": 0
                },
                {
                    "sent": "And see that we have to baseline which one is machine translation baseline.",
                    "label": 0
                },
                {
                    "sent": "So we just translate the document and running through the classifier another one is glossed baseline.",
                    "label": 0
                },
                {
                    "sent": "So we translate word by word so we can see that our performance is much better is distributed representations.",
                    "label": 0
                },
                {
                    "sent": "And anyway it's for us.",
                    "label": 0
                },
                {
                    "sent": "It's more like a test set.",
                    "label": 0
                },
                {
                    "sent": "This representation is informative because if you would really want to learn distributed representation for document classification, we probably would do it in a different way.",
                    "label": 0
                },
                {
                    "sent": "So we'll add for example, discriminative component in the objective function.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm finishing up in.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just a couple of fields about ongoing work, so we started to look into this because in collaboration with people Johns Hopkins we want to use a distributor representations of words and phrases in the context of machine translation, poor resource machine translation where you have relatively little of parallel data but a lot of modeling or data, and so it would have been shown with some type of distributed similarity features.",
                    "label": 0
                },
                {
                    "sent": "Vector space models are useful for for this task, Even so in not completely realistic settings.",
                    "label": 0
                },
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "For the moment, we just compare how well our representations campaign with this representation previously used in machine translation in Lexicon induction.",
                    "label": 0
                },
                {
                    "sent": "So basically you rank translations in rank verse.",
                    "label": 0
                },
                {
                    "sent": "We should translate, and this is on three on 3 languages.",
                    "label": 0
                },
                {
                    "sent": "Relatively poor resource languages, I would say.",
                    "label": 0
                },
                {
                    "sent": "So you can see that across all the setups, distributed representations perform considerably better than then bills.",
                    "label": 0
                },
                {
                    "sent": "This feature used previously and the translation which is induced from a small parallel corpora corpus.",
                    "label": 0
                },
                {
                    "sent": "But it's a bit over optimistic because it's averaged over word type, so it's not.",
                    "label": 0
                },
                {
                    "sent": "It doesn't correspond to the frequency in the actual.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I said so in a way it emphasizes in rare words more than.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Frequent words so you can use a similar technique.",
                    "label": 0
                },
                {
                    "sent": "Very similar techniques for inducing phrase and cross lingual phrase representations, and maybe inducing phrase representation so you can regularize in the same way alignment or phrases, but you can use some phrasing building models, for example work of records.",
                    "label": 0
                },
                {
                    "sent": "My colleagues at Stanford so and this is.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically my last slide.",
                    "label": 0
                },
                {
                    "sent": "I talked about different types of semantic representations about inducing representations of event phrases a little bit words and.",
                    "label": 0
                },
                {
                    "sent": "And then I talked about distributed and clustering representation.",
                    "label": 0
                },
                {
                    "sent": "So basically these two dimensions to this this work and now I'm starting to look into ways how we can actually induce a distributional features for events and situations, because this will allow me to deal with.",
                    "label": 0
                },
                {
                    "sent": "And Ularity and compositionality, and etc.",
                    "label": 0
                },
                {
                    "sent": "But I think you, if you're interested in this.",
                    "label": 0
                },
                {
                    "sent": "You should also looking at paper by Jonathan and colleagues, where they look and in more context of statistical relation only running for binary relations.",
                    "label": 0
                },
                {
                    "sent": "How to embed this binary relations.",
                    "label": 0
                },
                {
                    "sent": "I think it's pretty cool work, OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}