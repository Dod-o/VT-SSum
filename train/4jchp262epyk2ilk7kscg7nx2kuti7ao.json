{
    "id": "4jchp262epyk2ilk7kscg7nx2kuti7ao",
    "title": "Semi-supervised Instance Matching Using Boosted Classifiers",
    "info": {
        "author": [
            "Mayank Kejriwal, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "July 15, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2015_kejriwal_boosted_classifiers/",
    "segmentation": [
        [
            "OK, so on my unk and I'll be presenting semisupervised instance matching using boosted classifiers.",
            "Hopefully that will all become clear as we the title will become clearer as we move through the talk.",
            "I'm from the University of Texas at Austin and we would like to acknowledge the USNSF for supporting this research.",
            "Solo."
        ],
        [
            "We get started very briefly with the instance matching.",
            "I'm sure we're all familiar with this problem, so I won't go into too much depth over the definition.",
            "But basically if we look at this graph, these two graphs, the problem is finding two instances that refer to the same underlying entity.",
            "It's a very difficult problem, although it's very easy for a human being, so you know, I'm sure almost all of us in the room can look at Mike Bates and Michael what Beats Junior and kind of figure out that.",
            "You know what these two people are the same thing.",
            "There's a lot of context.",
            "You know they have the same spouse.",
            "You know, just things like that, so we could make that out so it's easy for a human being."
        ],
        [
            "But it's a 50 year old problem, and that's often not appreciated.",
            "How old the problem is, but I actually have a reference over here from Newcomb in 59, and it's a very interesting paper.",
            "He uses punch cards and he actually describes a program using punch cards and things like that.",
            "So very, very interesting.",
            "People still undergoing research, so you know tough problem."
        ],
        [
            "And the primary application that is cited is linked open data, but I want to say that this will actually emphasize in the keynote today.",
            "So in the keynote today we actually had all these other applications graph mining.",
            "You know things like that and ER, entity resolution it was called in the keynote, but it goes by many different names because of the longevity of the problem and I'm actually cited the keynote speakers students.",
            "It's a it has a lot of creative users.",
            "If we solve this problem."
        ],
        [
            "And so now to just get into some technical depth, but still high level we found in the literature that there are usually two key aspects to most instance matches and the first key aspect is efficiency.",
            "Now every system has to be efficient, you know.",
            "Clearly as data continues to grow, but in the instance matching literature we have a very specific meaning for efficiency and This is why.",
            "So imagine that you have a functional specification, which is basically a Boolean function.",
            "It takes 2 entities as input.",
            "Returns true if they refer to the same thing, returns false otherwise.",
            "So very simple, but if even given that we would actually have to do MN comparison.",
            "So M cross N entities to figure out what the ones or what the owl same as links are, and this is usually very small compared to M and so in the efficiency phase usually are very small subset of the NM pairs are chosen that are good candidates, so this is usually done through heuristics through a phase called blocking or where you kind of cluster.",
            "Approximately similar instances using some heuristics into these blocks and then entities are shared a block or a cluster up a Dan become candidates for further comparison, so it so it removes a lot of these MN pairs that should not be compared.",
            "And I'll sort of show you the architecture for that briefly."
        ],
        [
            "And the second key aspect and the one that will be dealing with in this work is effectiveness.",
            "So you know, I just said that, you know, we assume that were given the functional specification, but in reality that's not the case.",
            "You won't be given that you will just have a lot of data on your hands.",
            "So how do you actually devise the rule or the function that says that these are the two things I should be linked together?",
            "These two things should not be linked together, and it has to be adaptive because it's going to be dependent on the data is going to be dependent on a lot of things, and for that the answer that has come to dominate.",
            "In the last decade for this problem or since 2003, pretty much is machine learning, so most of the state of the art systems are deal with this.",
            "Use machine learning to learn a good functional specification."
        ],
        [
            "So this is kind of what the pipeline looks like.",
            "A lot of systems can be placed in this pipeline using this as a reference, but it's not all theoretical constraint.",
            "I mean, you can combine both blocking and classification there.",
            "A lot of creative ways to do this, but this is kind of how it looks like that you have to RDF graphs.",
            "They go through this blocking phase.",
            "What comes out is a candidate set of pairs, so a lot less than MN.",
            "You know, the candidate said just as we said, blocking deals with efficiency.",
            "It's a lot less than MN.",
            "And then each pair in the candidate said.",
            "Valuated by the functional specification and what comes out are the ones that should be linked using all simars, so that's what it looks."
        ],
        [
            "Like and if you look at it from a machine learning perspective, then very few changes instead of a functional specification or training set of duplicates and non duplicates goes into the classification step.",
            "Modular, strained and the pairs in the candidate set are then converted into are labeled essentially by that by that model.",
            "So it's a binary classification problem.",
            "This is very high level.",
            "There's some other stuff will get into that more, but you know this is these are the basics.",
            "This is what it looks like from a very high."
        ],
        [
            "And the key problem that that I'm going to be dealing with is that you know it's expensive to get these training sets Becausw Jurassic datasets continue to grow.",
            "It's very difficult to make out what the duplicates are.",
            "The non duplicates are like you know, even in the keynote today as a keynote speaker said you know the data set had gone through extensive domain expertise and still had a lot of duplicates.",
            "So getting these training sets is actually not.",
            "Not easy."
        ],
        [
            "And so the question that I sort of are the research question behind this paper.",
            "If you will.",
            "Is that how can we actually minimize this labeling efforts?",
            "We want to try and minimize the training set collection without degrading quality.",
            "So the obvious way of minimizing is let's just collect a lot less training data.",
            "You instead of doing 20% training data, will just do 2% you know and and what that will result in is that you know your quality measures will drop.",
            "So how can we actually minimize?",
            "So get more for less, essentially without having too much degradation."
        ],
        [
            "On the quality measures and for minimizing labeling effort, there are various tactics that are already employed in machine learning.",
            "We're not going to invent a new one, but the one that we find will be particularly useful is what's called semi supervised learning, where we have very little label data, but we are leveraging a lot of the unlabeled data.",
            "I'll show how to actually improve performance and for this."
        ],
        [
            "Second case, now, let's imagine that we have a fixed training set.",
            "But say we have a weak classifier, or somehow the classifier is not capturing the training set appropriately, so our quality is not as high as we would like it to be.",
            "So there's another machine learning technique called boosting which can actually make a weak classifier stronger and can even increase the representation of the training set.",
            "So we we will kind of look at the marriage of these two techniques.",
            "So usually when we look at the literature.",
            "It's either one or diado.",
            "Both together are not actually considered and sort of show why we can actually consider both."
        ],
        [
            "Advantages of that?",
            "But just briefly, before I kind of get into the intuition behind how we would actually combine the two, and what the advantages are.",
            "Semi supervised learning.",
            "Is it kind of looks like this in our framework, so initially we have what's called a seed training set to see because it's very small.",
            "Let's say it's 2% only 2% of the ground truth goes in.",
            "Usually you know it has to be at least 20 percent, 50%.",
            "That's like the norm.",
            "20% is also considered very low in machine learning training, but we were just consider, say, 2%.",
            "Before screen some model, it could be anything.",
            "It could be an SVM, multilayer perceptron, random folders to some machine learning model.",
            "Then the crane classifier is, so the unlabeled set of instances the candidate set over here.",
            "So we're looking just at the classification phase and the train classifier would assign probabilities to each instance pair, and then the highest scoring pairs of pairs that are that are most confidently classified innocence by the classifier as a duplicate would then go into the model again and then the whole model would get retrained, and then you would get a retrain class when you keep doing this.",
            "Creatively and then.",
            "Finally, at some point at some convergence criteria, say for a fixed number of rounds, or some criteria that even you can define what you would do it 1 final time and then this set would get classified one final time and then that's that's the set that you would take for your answers.",
            "So that's kind of semi supervised learning over here.",
            "So we're actually leveraging unlabeled data because the stuff that goes in after the first iteration has been labeled by the classifier, not by us.",
            "So it's still unlabeled in a technical sense."
        ],
        [
            "And boosting, so I'll show the Adaboost algorithm, which is what we're using over here in the next slide.",
            "But boosting is also iterative, so you know just to kind of highlight those similarity between boosting and semi supervised.",
            "Learning boosting is also iterative, but the idea otherwise is very different.",
            "So initially it was proposed in response to a question in 1989 by two very famous researcher than what they asked was that if you have just to reclassify or your classifier that does slightly better than random.",
            "You have a classifier, so random would be 5050.",
            "If you have IID assumptions.",
            "But let's say you have a classifier that is slightly better than random, can you use that the classifier to build a stronger classifier that was your original question now and it turned out so in 9090 ninety the answer turned out to be yes and by very Seminole work by Freundin and Shapira to create a committee of classifiers.",
            "And the goal behind this is that, let's say we train the classifier.",
            "The classifier does badly.",
            "It's you know weekly.",
            "Let's see only 60% quality, you know, just slightly better than random and then the examples that are classified wrong.",
            "So there will be some training examples that could be classified wrong, but then get rebated.",
            "They would be assigned a higher weight so that in the next iteration the classifier would do better and then the next iteration of classifier would do better.",
            "So you train a sequence of classifiers and then you wait each classifier and build what's called an ensemble Layer Committee of class files that you would then."
        ],
        [
            "Use, this is what the algorithm looks like.",
            "It's very classic, very technical.",
            "This is kind of the updates that we can see that you know we increase like E to the Alpha.",
            "Each training samples weight would get updated on the final hypothesis is like Sigma each so its comedy of classifiers getting clean and this was found to.",
            "You can not only use this to convert a V classifier to a strong classifier, but you can also use it to increase the representation of the training set.",
            "But the corners that could also increase overfitting so you know it's notorious for overfitting and will get more into."
        ],
        [
            "How that works out?",
            "So before kind of giving you the intuition behind the combination of both techniques, I just want to show you the system so the red block is actually our contribution.",
            "And also we only use 2% of the ground truth or 50 training samples, whichever is smaller.",
            "So very very small said.",
            "So you are encouraged to keep that in mind as we as we will look at the results.",
            "So we're looking at a very small ground truth and the rest of this we took from the literature, so we implemented it.",
            "But for the blocking.",
            "Generate restriction sets essentially is or matching classes and properties very similar to ontology matching because you may have these graphs that have all these different classes and properties and you want to match between them before you actually match the instances.",
            "And also we need to extract features so these walls, standard text based numeric, lots of features we've we've given details in the paper and on the project website and then finally you know this is kind of what I was describing that you would first have the booster class file.",
            "It would go into the the classification step and then the high scoring samples.",
            "Would you know recycle and we would use only the boosted classifier in that in that first phase of the sentence supervised learning.",
            "So."
        ],
        [
            "This is what the algorithm looks like.",
            "You know, I just wanted to quickly just go over some of the parameters so you know as usually taking the Sikh raining sets, we're taking the candidate set which is unlabeled.",
            "We need to classify that we're taking a base classifier, so the Bayes classifier is what the committee will be formed from.",
            "So it could be like a random forest.",
            "It could be another boosted classifier could be any classification model with specifying the iteration rounds dumb.",
            "So in this work we specify the convergence criteria that we will do semisupervised learning for this NUM number of rounds.",
            "And we're also taking these factors, so this is important.",
            "So over here you know the boosting part is fairly standard.",
            "Use Adaboost and in the semi supervised learning.",
            "Or we are so we are actually increasing by the fact of the factor could be less than one.",
            "It should be positive, it could be less than one, it could be greater than one.",
            "We use a factor of two, so it's very aggressive.",
            "But what the factor means is that in each semi supervised iteration, how many more high scoring samples are you considering?",
            "So a lot more potential for noise.",
            "If your factory Ohio.",
            "But also, you could potentially also have a bonanza because you you would just be if your labels are correct.",
            "If the classification is getting a lot of the labels correct, then your representation of the training set would just increase by that much, which is why."
        ],
        [
            "Intuition comes in, so it turns out that boosting and semi supervised learning if we think about it, I don't need recently have these two separate issues when considered together, even in the general machine learning community is that these are two separate but mutually exclusive problems.",
            "So the semi supervised learning is addressing one problem that.",
            "But if your training set is too small, can we use the unlabeled data to leverage performance and boosting?",
            "Is that if you have a weak classifier?",
            "Then gotta make that stronger if I don't have a weak classifier, but I have a training set and my classifier somehow not.",
            "You know, being representative of the training set, so similar to a V classifier in some sense, then how do I represent my training set better?",
            "But there's there's a greater risk over there because if there's noise in your training set then you will overfit.",
            "Boosting would just over fit on the noise, so that's where we get and this.",
            "I hope that this is what you'll come away with.",
            "You know more than the actual work itself is that this?",
            "Marriage, so to speak, is giving you a different risk rate on trade off, so the initial classification error is actually what will prove to be the primary determinant of your overall performance than your training set size.",
            "You could start with a 2% training set, an still get very high performance and will show that as long as the initial classification errors are not high, so you know that's kind of the."
        ],
        [
            "Trade off and with that let's move on to the evaluations so we actually.",
            "So this was a very empirical work because, you know, we're not making any theoretical contributions really in this work.",
            "So we actually wanted to go and do a lot of comparative studies and a lot of evaluation, and I'll just show you a smattering of results.",
            "But on this project website you'll find a lot more.",
            "You know everything from runtime data to our dependence on semi supervised iterations and all kinds of things.",
            "So I just I just wanted to show you some of the more important results.",
            "We chose only public datasets.",
            "These are all available publicly from other places as well.",
            "Three of them quite the three of them, actually really simple data cases.",
            "In the OEI 2010, two Jolie difficult data cases, so this Amazon Google products about by it's not easy to get good performance even the best systems are getting less than 70% on them and is CMD.",
            "BIPS is kind of my favorite because it's real world but it does not have too much noise.",
            "It has noise but not too much noise.",
            "So it's actually most representative of the real world in my opinion and I will say a lot of the materials are released."
        ],
        [
            "So these are kind of the metrics we use.",
            "Standard question, Recall F score, which is the harmonic mean of these two.",
            "So you know, just if R is basically the set of true positives, T is to ground truth, then you know Christian is a fraction of correct examples.",
            "Retrieve or he calls the fraction that we have retrieved from the ground truth so."
        ],
        [
            "Very standard, we actually compare against.",
            "We do three separate evaluations in this work, so in the first evaluation we want to see that if we just use boosting, we use both boosting and semi supervised learning, and we don't use anything.",
            "We just use 2% training samples and we don't do boosting or semi supervised learning.",
            "What do we get?",
            "So we wanted to contrast those three things we wanted to contrast the effects of using different base class files so you know.",
            "So for example, you know we saw database classifiers input to the algorithm.",
            "And everything is going to depend not everything but a lot is going to depend on that.",
            "So we use random forest versus.",
            "We also checked against multilayer perceptrons.",
            "There's only one work to the best of our knowledge that actually has used for nothing accels work that is actually evaluated on multilayer perceptron.",
            "It's considered a deep network.",
            "Bottom is so multi or perception is very expressive classifier.",
            "It's classified as a deep network and we also wanted to do a comparative evaluation where we we chose those six datasets so that we could actually check the best F score results that we achieved.",
            "Versus a lot that had been reported in the literature from all these systems, so I'll kind of show you."
        ],
        [
            "Also, all of them, so these are six graphs for the six datasets will just go over it in some high level, so that records in every graph.",
            "You'll find 3 curves and Christian and recall on the access, so the blue curve is basically if you don't use anything, we just use 2%.",
            "We train we classify and we're done, or the red one is if we just use boosting, the green one is of use both semi supervised learning and boosting.",
            "And what we find with the random forest is that on the easy datasets to Green Cove clearly outperforms the iteration plus semi supervised learning.",
            "Outperforms all the rest of them on the ACM DLP.",
            "Both or boosting plaster boosting plus iteration do do well equally well.",
            "However on the Amazon, Google products in the about by George is also a lot more ambiguous, and that goes back to that risk return tradeoff.",
            "I was talking about.",
            "If you have high classification error only on the system could end up doing much worse, so that's where the green color you know it's very steep, it falls, you know quite rapidly, and at the end it has the worst performance of all.",
            "Very high levels of recall, so that so you know, that's kind of the risk return tradeoff."
        ],
        [
            "Will the multilayer perceptrons results kind of show that we can actually compensate for that because you know, again, we find that on the three easy data set so you know it's still.",
            "It's pretty much the same story.",
            "The ACM DLP shows now that all the curves are incidental.",
            "That's because the classifier is a lot better, a lot more expressive.",
            "It captures the representation of the training side even in that, even in that small fraction over here in Amazon Google products, we find that you know it's kind of stable.",
            "You know the initial classification.",
            "Edward is not that high so we are able to maintain stable performance for awhile and then it drops.",
            "But the most representative is actually about by over here because we see that if you have high classification, if you have high precision and low levels of recall then the drop is is a lot smoother and then we can just you know we have a much better step over there."
        ],
        [
            "I'm just very briefly, I'll go over the external baseline.",
            "So these all, I believe axle systems were sitting over there.",
            "You know it's I wouldn't say that we actually outperform any of these systems because on some datasets they do a lot better on other datasets.",
            "We do better and their results may have improved since, but these all minimally supervised system.",
            "So to me.",
            "Actually, this indicates that the Community has made a lot of progress as a whole because we're actually starting to see when we compare against supervised system state of the art systems that do a lot more feature engineering, lot more supervision.",
            "We catching up to them.",
            "We don't have, you know, the same 20 percent, 30% difference that we used to have like 2%, four percent, those kinds of differences.",
            "So we're catching up.",
            "You know, to do these."
        ],
        [
            "The system, so I'm ready to end now.",
            "So for key takeaways.",
            "You know whatever you wanted to show you is that the techniques that we proposed semi supervised learning, plus boosting is an effective technique.",
            "There are also others you know, sort of what I showed in the previous slide for minimally supervised instance matching.",
            "So cases where you have very little training data available.",
            "You can also I believe you over some of these findings to ontology matching and other similar problems, but we haven't evaluated that the best results seem to be achieved with an expressive classifier and finally, and this is what I really hope will be the takeaway.",
            "Is not this marriage between boosting and semi supervised learning?",
            "Has this nontraditional risk return tradeoff?",
            "Of that?",
            "I hope that you know other people who are using machine learning would at least think about investigating if they have very little training data."
        ],
        [
            "And with that I would like to conclude."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so on my unk and I'll be presenting semisupervised instance matching using boosted classifiers.",
                    "label": 1
                },
                {
                    "sent": "Hopefully that will all become clear as we the title will become clearer as we move through the talk.",
                    "label": 1
                },
                {
                    "sent": "I'm from the University of Texas at Austin and we would like to acknowledge the USNSF for supporting this research.",
                    "label": 0
                },
                {
                    "sent": "Solo.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We get started very briefly with the instance matching.",
                    "label": 1
                },
                {
                    "sent": "I'm sure we're all familiar with this problem, so I won't go into too much depth over the definition.",
                    "label": 0
                },
                {
                    "sent": "But basically if we look at this graph, these two graphs, the problem is finding two instances that refer to the same underlying entity.",
                    "label": 0
                },
                {
                    "sent": "It's a very difficult problem, although it's very easy for a human being, so you know, I'm sure almost all of us in the room can look at Mike Bates and Michael what Beats Junior and kind of figure out that.",
                    "label": 0
                },
                {
                    "sent": "You know what these two people are the same thing.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of context.",
                    "label": 0
                },
                {
                    "sent": "You know they have the same spouse.",
                    "label": 0
                },
                {
                    "sent": "You know, just things like that, so we could make that out so it's easy for a human being.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But it's a 50 year old problem, and that's often not appreciated.",
                    "label": 1
                },
                {
                    "sent": "How old the problem is, but I actually have a reference over here from Newcomb in 59, and it's a very interesting paper.",
                    "label": 0
                },
                {
                    "sent": "He uses punch cards and he actually describes a program using punch cards and things like that.",
                    "label": 0
                },
                {
                    "sent": "So very, very interesting.",
                    "label": 0
                },
                {
                    "sent": "People still undergoing research, so you know tough problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the primary application that is cited is linked open data, but I want to say that this will actually emphasize in the keynote today.",
                    "label": 1
                },
                {
                    "sent": "So in the keynote today we actually had all these other applications graph mining.",
                    "label": 0
                },
                {
                    "sent": "You know things like that and ER, entity resolution it was called in the keynote, but it goes by many different names because of the longevity of the problem and I'm actually cited the keynote speakers students.",
                    "label": 0
                },
                {
                    "sent": "It's a it has a lot of creative users.",
                    "label": 0
                },
                {
                    "sent": "If we solve this problem.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so now to just get into some technical depth, but still high level we found in the literature that there are usually two key aspects to most instance matches and the first key aspect is efficiency.",
                    "label": 0
                },
                {
                    "sent": "Now every system has to be efficient, you know.",
                    "label": 0
                },
                {
                    "sent": "Clearly as data continues to grow, but in the instance matching literature we have a very specific meaning for efficiency and This is why.",
                    "label": 0
                },
                {
                    "sent": "So imagine that you have a functional specification, which is basically a Boolean function.",
                    "label": 0
                },
                {
                    "sent": "It takes 2 entities as input.",
                    "label": 0
                },
                {
                    "sent": "Returns true if they refer to the same thing, returns false otherwise.",
                    "label": 0
                },
                {
                    "sent": "So very simple, but if even given that we would actually have to do MN comparison.",
                    "label": 0
                },
                {
                    "sent": "So M cross N entities to figure out what the ones or what the owl same as links are, and this is usually very small compared to M and so in the efficiency phase usually are very small subset of the NM pairs are chosen that are good candidates, so this is usually done through heuristics through a phase called blocking or where you kind of cluster.",
                    "label": 1
                },
                {
                    "sent": "Approximately similar instances using some heuristics into these blocks and then entities are shared a block or a cluster up a Dan become candidates for further comparison, so it so it removes a lot of these MN pairs that should not be compared.",
                    "label": 0
                },
                {
                    "sent": "And I'll sort of show you the architecture for that briefly.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second key aspect and the one that will be dealing with in this work is effectiveness.",
                    "label": 0
                },
                {
                    "sent": "So you know, I just said that, you know, we assume that were given the functional specification, but in reality that's not the case.",
                    "label": 0
                },
                {
                    "sent": "You won't be given that you will just have a lot of data on your hands.",
                    "label": 0
                },
                {
                    "sent": "So how do you actually devise the rule or the function that says that these are the two things I should be linked together?",
                    "label": 0
                },
                {
                    "sent": "These two things should not be linked together, and it has to be adaptive because it's going to be dependent on the data is going to be dependent on a lot of things, and for that the answer that has come to dominate.",
                    "label": 0
                },
                {
                    "sent": "In the last decade for this problem or since 2003, pretty much is machine learning, so most of the state of the art systems are deal with this.",
                    "label": 0
                },
                {
                    "sent": "Use machine learning to learn a good functional specification.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is kind of what the pipeline looks like.",
                    "label": 0
                },
                {
                    "sent": "A lot of systems can be placed in this pipeline using this as a reference, but it's not all theoretical constraint.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can combine both blocking and classification there.",
                    "label": 0
                },
                {
                    "sent": "A lot of creative ways to do this, but this is kind of how it looks like that you have to RDF graphs.",
                    "label": 0
                },
                {
                    "sent": "They go through this blocking phase.",
                    "label": 0
                },
                {
                    "sent": "What comes out is a candidate set of pairs, so a lot less than MN.",
                    "label": 1
                },
                {
                    "sent": "You know, the candidate said just as we said, blocking deals with efficiency.",
                    "label": 0
                },
                {
                    "sent": "It's a lot less than MN.",
                    "label": 0
                },
                {
                    "sent": "And then each pair in the candidate said.",
                    "label": 1
                },
                {
                    "sent": "Valuated by the functional specification and what comes out are the ones that should be linked using all simars, so that's what it looks.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like and if you look at it from a machine learning perspective, then very few changes instead of a functional specification or training set of duplicates and non duplicates goes into the classification step.",
                    "label": 1
                },
                {
                    "sent": "Modular, strained and the pairs in the candidate set are then converted into are labeled essentially by that by that model.",
                    "label": 0
                },
                {
                    "sent": "So it's a binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "This is very high level.",
                    "label": 0
                },
                {
                    "sent": "There's some other stuff will get into that more, but you know this is these are the basics.",
                    "label": 0
                },
                {
                    "sent": "This is what it looks like from a very high.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the key problem that that I'm going to be dealing with is that you know it's expensive to get these training sets Becausw Jurassic datasets continue to grow.",
                    "label": 0
                },
                {
                    "sent": "It's very difficult to make out what the duplicates are.",
                    "label": 0
                },
                {
                    "sent": "The non duplicates are like you know, even in the keynote today as a keynote speaker said you know the data set had gone through extensive domain expertise and still had a lot of duplicates.",
                    "label": 0
                },
                {
                    "sent": "So getting these training sets is actually not.",
                    "label": 0
                },
                {
                    "sent": "Not easy.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the question that I sort of are the research question behind this paper.",
                    "label": 0
                },
                {
                    "sent": "If you will.",
                    "label": 0
                },
                {
                    "sent": "Is that how can we actually minimize this labeling efforts?",
                    "label": 0
                },
                {
                    "sent": "We want to try and minimize the training set collection without degrading quality.",
                    "label": 1
                },
                {
                    "sent": "So the obvious way of minimizing is let's just collect a lot less training data.",
                    "label": 0
                },
                {
                    "sent": "You instead of doing 20% training data, will just do 2% you know and and what that will result in is that you know your quality measures will drop.",
                    "label": 0
                },
                {
                    "sent": "So how can we actually minimize?",
                    "label": 1
                },
                {
                    "sent": "So get more for less, essentially without having too much degradation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the quality measures and for minimizing labeling effort, there are various tactics that are already employed in machine learning.",
                    "label": 1
                },
                {
                    "sent": "We're not going to invent a new one, but the one that we find will be particularly useful is what's called semi supervised learning, where we have very little label data, but we are leveraging a lot of the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "I'll show how to actually improve performance and for this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second case, now, let's imagine that we have a fixed training set.",
                    "label": 0
                },
                {
                    "sent": "But say we have a weak classifier, or somehow the classifier is not capturing the training set appropriately, so our quality is not as high as we would like it to be.",
                    "label": 0
                },
                {
                    "sent": "So there's another machine learning technique called boosting which can actually make a weak classifier stronger and can even increase the representation of the training set.",
                    "label": 0
                },
                {
                    "sent": "So we we will kind of look at the marriage of these two techniques.",
                    "label": 0
                },
                {
                    "sent": "So usually when we look at the literature.",
                    "label": 0
                },
                {
                    "sent": "It's either one or diado.",
                    "label": 0
                },
                {
                    "sent": "Both together are not actually considered and sort of show why we can actually consider both.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Advantages of that?",
                    "label": 0
                },
                {
                    "sent": "But just briefly, before I kind of get into the intuition behind how we would actually combine the two, and what the advantages are.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Is it kind of looks like this in our framework, so initially we have what's called a seed training set to see because it's very small.",
                    "label": 1
                },
                {
                    "sent": "Let's say it's 2% only 2% of the ground truth goes in.",
                    "label": 0
                },
                {
                    "sent": "Usually you know it has to be at least 20 percent, 50%.",
                    "label": 0
                },
                {
                    "sent": "That's like the norm.",
                    "label": 0
                },
                {
                    "sent": "20% is also considered very low in machine learning training, but we were just consider, say, 2%.",
                    "label": 0
                },
                {
                    "sent": "Before screen some model, it could be anything.",
                    "label": 0
                },
                {
                    "sent": "It could be an SVM, multilayer perceptron, random folders to some machine learning model.",
                    "label": 1
                },
                {
                    "sent": "Then the crane classifier is, so the unlabeled set of instances the candidate set over here.",
                    "label": 0
                },
                {
                    "sent": "So we're looking just at the classification phase and the train classifier would assign probabilities to each instance pair, and then the highest scoring pairs of pairs that are that are most confidently classified innocence by the classifier as a duplicate would then go into the model again and then the whole model would get retrained, and then you would get a retrain class when you keep doing this.",
                    "label": 0
                },
                {
                    "sent": "Creatively and then.",
                    "label": 0
                },
                {
                    "sent": "Finally, at some point at some convergence criteria, say for a fixed number of rounds, or some criteria that even you can define what you would do it 1 final time and then this set would get classified one final time and then that's that's the set that you would take for your answers.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of semi supervised learning over here.",
                    "label": 0
                },
                {
                    "sent": "So we're actually leveraging unlabeled data because the stuff that goes in after the first iteration has been labeled by the classifier, not by us.",
                    "label": 0
                },
                {
                    "sent": "So it's still unlabeled in a technical sense.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And boosting, so I'll show the Adaboost algorithm, which is what we're using over here in the next slide.",
                    "label": 0
                },
                {
                    "sent": "But boosting is also iterative, so you know just to kind of highlight those similarity between boosting and semi supervised.",
                    "label": 0
                },
                {
                    "sent": "Learning boosting is also iterative, but the idea otherwise is very different.",
                    "label": 0
                },
                {
                    "sent": "So initially it was proposed in response to a question in 1989 by two very famous researcher than what they asked was that if you have just to reclassify or your classifier that does slightly better than random.",
                    "label": 0
                },
                {
                    "sent": "You have a classifier, so random would be 5050.",
                    "label": 0
                },
                {
                    "sent": "If you have IID assumptions.",
                    "label": 0
                },
                {
                    "sent": "But let's say you have a classifier that is slightly better than random, can you use that the classifier to build a stronger classifier that was your original question now and it turned out so in 9090 ninety the answer turned out to be yes and by very Seminole work by Freundin and Shapira to create a committee of classifiers.",
                    "label": 0
                },
                {
                    "sent": "And the goal behind this is that, let's say we train the classifier.",
                    "label": 0
                },
                {
                    "sent": "The classifier does badly.",
                    "label": 0
                },
                {
                    "sent": "It's you know weekly.",
                    "label": 0
                },
                {
                    "sent": "Let's see only 60% quality, you know, just slightly better than random and then the examples that are classified wrong.",
                    "label": 0
                },
                {
                    "sent": "So there will be some training examples that could be classified wrong, but then get rebated.",
                    "label": 0
                },
                {
                    "sent": "They would be assigned a higher weight so that in the next iteration the classifier would do better and then the next iteration of classifier would do better.",
                    "label": 0
                },
                {
                    "sent": "So you train a sequence of classifiers and then you wait each classifier and build what's called an ensemble Layer Committee of class files that you would then.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use, this is what the algorithm looks like.",
                    "label": 0
                },
                {
                    "sent": "It's very classic, very technical.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the updates that we can see that you know we increase like E to the Alpha.",
                    "label": 0
                },
                {
                    "sent": "Each training samples weight would get updated on the final hypothesis is like Sigma each so its comedy of classifiers getting clean and this was found to.",
                    "label": 0
                },
                {
                    "sent": "You can not only use this to convert a V classifier to a strong classifier, but you can also use it to increase the representation of the training set.",
                    "label": 0
                },
                {
                    "sent": "But the corners that could also increase overfitting so you know it's notorious for overfitting and will get more into.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How that works out?",
                    "label": 0
                },
                {
                    "sent": "So before kind of giving you the intuition behind the combination of both techniques, I just want to show you the system so the red block is actually our contribution.",
                    "label": 0
                },
                {
                    "sent": "And also we only use 2% of the ground truth or 50 training samples, whichever is smaller.",
                    "label": 1
                },
                {
                    "sent": "So very very small said.",
                    "label": 0
                },
                {
                    "sent": "So you are encouraged to keep that in mind as we as we will look at the results.",
                    "label": 0
                },
                {
                    "sent": "So we're looking at a very small ground truth and the rest of this we took from the literature, so we implemented it.",
                    "label": 0
                },
                {
                    "sent": "But for the blocking.",
                    "label": 0
                },
                {
                    "sent": "Generate restriction sets essentially is or matching classes and properties very similar to ontology matching because you may have these graphs that have all these different classes and properties and you want to match between them before you actually match the instances.",
                    "label": 0
                },
                {
                    "sent": "And also we need to extract features so these walls, standard text based numeric, lots of features we've we've given details in the paper and on the project website and then finally you know this is kind of what I was describing that you would first have the booster class file.",
                    "label": 0
                },
                {
                    "sent": "It would go into the the classification step and then the high scoring samples.",
                    "label": 0
                },
                {
                    "sent": "Would you know recycle and we would use only the boosted classifier in that in that first phase of the sentence supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is what the algorithm looks like.",
                    "label": 0
                },
                {
                    "sent": "You know, I just wanted to quickly just go over some of the parameters so you know as usually taking the Sikh raining sets, we're taking the candidate set which is unlabeled.",
                    "label": 0
                },
                {
                    "sent": "We need to classify that we're taking a base classifier, so the Bayes classifier is what the committee will be formed from.",
                    "label": 0
                },
                {
                    "sent": "So it could be like a random forest.",
                    "label": 0
                },
                {
                    "sent": "It could be another boosted classifier could be any classification model with specifying the iteration rounds dumb.",
                    "label": 0
                },
                {
                    "sent": "So in this work we specify the convergence criteria that we will do semisupervised learning for this NUM number of rounds.",
                    "label": 1
                },
                {
                    "sent": "And we're also taking these factors, so this is important.",
                    "label": 0
                },
                {
                    "sent": "So over here you know the boosting part is fairly standard.",
                    "label": 0
                },
                {
                    "sent": "Use Adaboost and in the semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Or we are so we are actually increasing by the fact of the factor could be less than one.",
                    "label": 0
                },
                {
                    "sent": "It should be positive, it could be less than one, it could be greater than one.",
                    "label": 0
                },
                {
                    "sent": "We use a factor of two, so it's very aggressive.",
                    "label": 0
                },
                {
                    "sent": "But what the factor means is that in each semi supervised iteration, how many more high scoring samples are you considering?",
                    "label": 0
                },
                {
                    "sent": "So a lot more potential for noise.",
                    "label": 0
                },
                {
                    "sent": "If your factory Ohio.",
                    "label": 0
                },
                {
                    "sent": "But also, you could potentially also have a bonanza because you you would just be if your labels are correct.",
                    "label": 0
                },
                {
                    "sent": "If the classification is getting a lot of the labels correct, then your representation of the training set would just increase by that much, which is why.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Intuition comes in, so it turns out that boosting and semi supervised learning if we think about it, I don't need recently have these two separate issues when considered together, even in the general machine learning community is that these are two separate but mutually exclusive problems.",
                    "label": 1
                },
                {
                    "sent": "So the semi supervised learning is addressing one problem that.",
                    "label": 0
                },
                {
                    "sent": "But if your training set is too small, can we use the unlabeled data to leverage performance and boosting?",
                    "label": 1
                },
                {
                    "sent": "Is that if you have a weak classifier?",
                    "label": 1
                },
                {
                    "sent": "Then gotta make that stronger if I don't have a weak classifier, but I have a training set and my classifier somehow not.",
                    "label": 0
                },
                {
                    "sent": "You know, being representative of the training set, so similar to a V classifier in some sense, then how do I represent my training set better?",
                    "label": 0
                },
                {
                    "sent": "But there's there's a greater risk over there because if there's noise in your training set then you will overfit.",
                    "label": 0
                },
                {
                    "sent": "Boosting would just over fit on the noise, so that's where we get and this.",
                    "label": 0
                },
                {
                    "sent": "I hope that this is what you'll come away with.",
                    "label": 0
                },
                {
                    "sent": "You know more than the actual work itself is that this?",
                    "label": 0
                },
                {
                    "sent": "Marriage, so to speak, is giving you a different risk rate on trade off, so the initial classification error is actually what will prove to be the primary determinant of your overall performance than your training set size.",
                    "label": 1
                },
                {
                    "sent": "You could start with a 2% training set, an still get very high performance and will show that as long as the initial classification errors are not high, so you know that's kind of the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trade off and with that let's move on to the evaluations so we actually.",
                    "label": 0
                },
                {
                    "sent": "So this was a very empirical work because, you know, we're not making any theoretical contributions really in this work.",
                    "label": 0
                },
                {
                    "sent": "So we actually wanted to go and do a lot of comparative studies and a lot of evaluation, and I'll just show you a smattering of results.",
                    "label": 0
                },
                {
                    "sent": "But on this project website you'll find a lot more.",
                    "label": 1
                },
                {
                    "sent": "You know everything from runtime data to our dependence on semi supervised iterations and all kinds of things.",
                    "label": 0
                },
                {
                    "sent": "So I just I just wanted to show you some of the more important results.",
                    "label": 0
                },
                {
                    "sent": "We chose only public datasets.",
                    "label": 1
                },
                {
                    "sent": "These are all available publicly from other places as well.",
                    "label": 0
                },
                {
                    "sent": "Three of them quite the three of them, actually really simple data cases.",
                    "label": 0
                },
                {
                    "sent": "In the OEI 2010, two Jolie difficult data cases, so this Amazon Google products about by it's not easy to get good performance even the best systems are getting less than 70% on them and is CMD.",
                    "label": 0
                },
                {
                    "sent": "BIPS is kind of my favorite because it's real world but it does not have too much noise.",
                    "label": 0
                },
                {
                    "sent": "It has noise but not too much noise.",
                    "label": 0
                },
                {
                    "sent": "So it's actually most representative of the real world in my opinion and I will say a lot of the materials are released.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are kind of the metrics we use.",
                    "label": 0
                },
                {
                    "sent": "Standard question, Recall F score, which is the harmonic mean of these two.",
                    "label": 0
                },
                {
                    "sent": "So you know, just if R is basically the set of true positives, T is to ground truth, then you know Christian is a fraction of correct examples.",
                    "label": 1
                },
                {
                    "sent": "Retrieve or he calls the fraction that we have retrieved from the ground truth so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very standard, we actually compare against.",
                    "label": 1
                },
                {
                    "sent": "We do three separate evaluations in this work, so in the first evaluation we want to see that if we just use boosting, we use both boosting and semi supervised learning, and we don't use anything.",
                    "label": 0
                },
                {
                    "sent": "We just use 2% training samples and we don't do boosting or semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "What do we get?",
                    "label": 1
                },
                {
                    "sent": "So we wanted to contrast those three things we wanted to contrast the effects of using different base class files so you know.",
                    "label": 0
                },
                {
                    "sent": "So for example, you know we saw database classifiers input to the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And everything is going to depend not everything but a lot is going to depend on that.",
                    "label": 1
                },
                {
                    "sent": "So we use random forest versus.",
                    "label": 0
                },
                {
                    "sent": "We also checked against multilayer perceptrons.",
                    "label": 0
                },
                {
                    "sent": "There's only one work to the best of our knowledge that actually has used for nothing accels work that is actually evaluated on multilayer perceptron.",
                    "label": 0
                },
                {
                    "sent": "It's considered a deep network.",
                    "label": 0
                },
                {
                    "sent": "Bottom is so multi or perception is very expressive classifier.",
                    "label": 0
                },
                {
                    "sent": "It's classified as a deep network and we also wanted to do a comparative evaluation where we we chose those six datasets so that we could actually check the best F score results that we achieved.",
                    "label": 0
                },
                {
                    "sent": "Versus a lot that had been reported in the literature from all these systems, so I'll kind of show you.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, all of them, so these are six graphs for the six datasets will just go over it in some high level, so that records in every graph.",
                    "label": 0
                },
                {
                    "sent": "You'll find 3 curves and Christian and recall on the access, so the blue curve is basically if you don't use anything, we just use 2%.",
                    "label": 0
                },
                {
                    "sent": "We train we classify and we're done, or the red one is if we just use boosting, the green one is of use both semi supervised learning and boosting.",
                    "label": 0
                },
                {
                    "sent": "And what we find with the random forest is that on the easy datasets to Green Cove clearly outperforms the iteration plus semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Outperforms all the rest of them on the ACM DLP.",
                    "label": 0
                },
                {
                    "sent": "Both or boosting plaster boosting plus iteration do do well equally well.",
                    "label": 0
                },
                {
                    "sent": "However on the Amazon, Google products in the about by George is also a lot more ambiguous, and that goes back to that risk return tradeoff.",
                    "label": 0
                },
                {
                    "sent": "I was talking about.",
                    "label": 0
                },
                {
                    "sent": "If you have high classification error only on the system could end up doing much worse, so that's where the green color you know it's very steep, it falls, you know quite rapidly, and at the end it has the worst performance of all.",
                    "label": 0
                },
                {
                    "sent": "Very high levels of recall, so that so you know, that's kind of the risk return tradeoff.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will the multilayer perceptrons results kind of show that we can actually compensate for that because you know, again, we find that on the three easy data set so you know it's still.",
                    "label": 0
                },
                {
                    "sent": "It's pretty much the same story.",
                    "label": 0
                },
                {
                    "sent": "The ACM DLP shows now that all the curves are incidental.",
                    "label": 0
                },
                {
                    "sent": "That's because the classifier is a lot better, a lot more expressive.",
                    "label": 0
                },
                {
                    "sent": "It captures the representation of the training side even in that, even in that small fraction over here in Amazon Google products, we find that you know it's kind of stable.",
                    "label": 0
                },
                {
                    "sent": "You know the initial classification.",
                    "label": 0
                },
                {
                    "sent": "Edward is not that high so we are able to maintain stable performance for awhile and then it drops.",
                    "label": 0
                },
                {
                    "sent": "But the most representative is actually about by over here because we see that if you have high classification, if you have high precision and low levels of recall then the drop is is a lot smoother and then we can just you know we have a much better step over there.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just very briefly, I'll go over the external baseline.",
                    "label": 0
                },
                {
                    "sent": "So these all, I believe axle systems were sitting over there.",
                    "label": 0
                },
                {
                    "sent": "You know it's I wouldn't say that we actually outperform any of these systems because on some datasets they do a lot better on other datasets.",
                    "label": 0
                },
                {
                    "sent": "We do better and their results may have improved since, but these all minimally supervised system.",
                    "label": 0
                },
                {
                    "sent": "So to me.",
                    "label": 0
                },
                {
                    "sent": "Actually, this indicates that the Community has made a lot of progress as a whole because we're actually starting to see when we compare against supervised system state of the art systems that do a lot more feature engineering, lot more supervision.",
                    "label": 0
                },
                {
                    "sent": "We catching up to them.",
                    "label": 0
                },
                {
                    "sent": "We don't have, you know, the same 20 percent, 30% difference that we used to have like 2%, four percent, those kinds of differences.",
                    "label": 0
                },
                {
                    "sent": "So we're catching up.",
                    "label": 0
                },
                {
                    "sent": "You know, to do these.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The system, so I'm ready to end now.",
                    "label": 0
                },
                {
                    "sent": "So for key takeaways.",
                    "label": 0
                },
                {
                    "sent": "You know whatever you wanted to show you is that the techniques that we proposed semi supervised learning, plus boosting is an effective technique.",
                    "label": 1
                },
                {
                    "sent": "There are also others you know, sort of what I showed in the previous slide for minimally supervised instance matching.",
                    "label": 1
                },
                {
                    "sent": "So cases where you have very little training data available.",
                    "label": 0
                },
                {
                    "sent": "You can also I believe you over some of these findings to ontology matching and other similar problems, but we haven't evaluated that the best results seem to be achieved with an expressive classifier and finally, and this is what I really hope will be the takeaway.",
                    "label": 0
                },
                {
                    "sent": "Is not this marriage between boosting and semi supervised learning?",
                    "label": 0
                },
                {
                    "sent": "Has this nontraditional risk return tradeoff?",
                    "label": 0
                },
                {
                    "sent": "Of that?",
                    "label": 0
                },
                {
                    "sent": "I hope that you know other people who are using machine learning would at least think about investigating if they have very little training data.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with that I would like to conclude.",
                    "label": 0
                }
            ]
        }
    }
}