{
    "id": "eqoqoqnc7mzgktw2475ofx4qtaolldnn",
    "title": "Detecting Hate Speech on Twitter Using a Convolution-GRU Based Deep Neural Network",
    "info": {
        "author": [
            "Ziqi Zhang, School of Science and Technology, Nottingham Trent University"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_zhang_neural_network/",
    "segmentation": [
        [
            "OK, thanks for him for the introduction so hi, I'm Zeke Young.",
            "I'm from the information School University of Sheffield.",
            "This is John work with one of my students and also previous colleague of mine from not even trends University.",
            "So we're going to look at identifying hate speech on social media.",
            "Twitter in this case using some kind of machine learning in SK Deep neural network methods."
        ],
        [
            "So as you are going to start giving you a bit of outline of this talk, so we'll we're looking to the problem of hate speech, and there's been a lot of work looking into this kind of problem recently, so I'll see you little bit reason to explain why that is the case, and obviously this is not the first work looking into this kind of problem, so I'll say something about what's the motivation.",
            "What is our unique contributions to this field of research?",
            "Then I want to explain how do we do it?",
            "Or the findings and what the lessons we have learned from this study."
        ],
        [
            "Now the problem of hate speech is really not a new one.",
            "If you think about it, probably started the moment we create the web so you know back then you could create a website so people can create website and then use it to propagate hateful content.",
            "But the problem probably got much, much worse and much more common when we gave everybody the possibility to create some content easily on web.",
            "So the birth of Web 2.0 early.",
            "And I mean, it's probably quite difficult to establish some kind of causal relationship between hate, speech, Anan crime, but there's a general consensus that the propagation and creation of such contents on the Internet on the web is creating some kind of environments that would encourage crime.",
            "So really, we have this kind of problem for a long time already.",
            "You know more than 20 years, but instead of having some kind of you know more.",
            "Effective methods that can deal with it.",
            "We are, I think having you know much, much, much, much more and more worse problems in this area and we kind of losing the control of it.",
            "A lot of questions we ask is, are there social media companies actually doing something and answer is definitely yes.",
            "They are doing something.",
            "They actually spending hundreds of millions of euros every year.",
            "Try to solve the problem, but very often they're looking at some kind of manual moderation process, and that's obviously is not going to scale to the web.",
            "And in in this area we also see the trends towards legislation.",
            "So back in January we have the first law against hate speech online introduced by the German government.",
            "But that was mostly targeting night social media companies try to force them to be more proactive to deal with that kind of problem.",
            "An introduction of the law was obviously with a lot of controversies and also has been very challenging to implement.",
            "In practice."
        ],
        [
            "So.",
            "If you want to deal with this kind of problem, the first question really is how do we find them?",
            "How do we identify this kind of content in a very effective and efficient way?",
            "So we sat there at moment.",
            "Social media companies spending hundreds of millions of you 100 million euros to deal with the problem.",
            "But typically they're doing some manual moderation process.",
            "This is never going to scale, so when what we need is really some kind of automated semantic analysis, we need to be able to understand the contents using natural language processing or some other kind of semantic content analysis methods to help us to automatically pinpoint is kind of.",
            "Content to support humans in the next part of decision-making, unsurprisingly, is a lot of work is being done is direction.",
            "And if you look at the literature, typically what you do with you, some machine learning based methods.",
            "So you create some training data and you use it to train the model.",
            "Do you apply the model to new data to make new predictions?",
            "And the general trend is being that people are really moving to some use of deep neural network based models.",
            "Which place to focus on creating some kind of interesting structures of network which would help us to automatically extract interesting features that would help with these kind of tasks and that is moving away from what we're doing.",
            "You know, fire six years ago using traditional machine learning methods, hugely popular SVM logistic regression.",
            "But with the 1st place on feature engineering.",
            "So you do corpus analysis.",
            "You do linguistic analysis to try to come up with some kind of.",
            "Lexical syntactic patterns in the data try to understand you know what are the features that can be really helpful to separate hate speech from non hate content.",
            "But when you look at the literature as one of the problem you find is that everybody has used their own sort of some data to test their model.",
            "So some cases those data are not even publicly available.",
            "So you can't really replicate what they have done.",
            "All they have used some public data, but they changed it slightly, so they merged some data or they took part of it for some reasons.",
            "Obviously what we want to do is we want to, you know, create an even better method.",
            "But how do we know that?",
            "Well, first of all, we want to fully test it.",
            "We want to test the method on all the publicly available datasets so we know how do we compare with previous method being used on the same data that we have used.",
            "But also that makes our method compatible for future research as well, so that's pretty much all motivation for this work and this."
        ],
        [
            "And this is what we done summarizing one sentence.",
            "So we create one new method.",
            "Which we test on all the publicly available Twitter corpus for hate speech.",
            "Now we show that I'm practically we can do better using this new model."
        ],
        [
            "So moving on to the methodology now.",
            "So what I mean the basic principle is really straightforward.",
            "We we take it as an LP problem.",
            "We use some kind of text classification method and in this case it's short text classification because you have tweet typically is one or two sentences.",
            "So as we said in the literature, in this area is really moving, so shifting towards use of deep neural network based method and I think the reasons are kind of two fold.",
            "On the one hand a serious huge amount of effort from Penn Philan laborers feature engineering.",
            "On the other hand, I'm pretty please being shown that it actually works better than those.",
            "Traditional methods that relies on feature engineering.",
            "So excuse me for shameless self promotional material here.",
            "So on the 7th of June we have a poster that will talk about exactly this kind of task about using traditional machine learning Masters and feature engineering, but I'm going to explain why you don't really need to do that.",
            "So if you're interested, please come along and we have chat about that.",
            "In the area of you look at the literature they use DNA, so some deep learning methods.",
            "Obviously they focus on the kind of structure of the network, so the topology of the network.",
            "But typically you have two flavors, so somebody use convolutional neural networks saying.",
            "And the other branch will be using some kind of recurrent neural networks.",
            "So the most popular being oh STM long short term memory.",
            "And obviously these two each type of these two networks have this own integration.",
            "But we were thinking, you know, would pop would be possible to combine them all together and."
        ],
        [
            "If you want to combine them, what what would be the right way to do it and what would be the intuition behind and other any you know advantage of combining the two networks altogether on these kind of space with task?",
            "So this is I'll take here.",
            "So what we do is we took CNN with Gru which is gated recurrent unit network.",
            "So if you're familiar with our STM or D and in general you know that it is could be considered as a simplified version of STM, so I don't think I will explain.",
            "Our networking will be more details in the next few slides, but will focus on the high level intuitions and principles behind or try to avoid technical details."
        ],
        [
            "So Network starts with the word embedding layer.",
            "So what it does is basically you take a tweet short sentence and you map that into a fixed lens fixed dimensional vector space.",
            "Representation of that tweet.",
            "And be cause the kind of data we do always comparat compared to the typical corporate you use for learning word embeddings, and much much smaller.",
            "So we don't actually learn what in bed in front of data, but we use some pre trends.",
            "What embeddings in this case we use the word two VEC model train.",
            "You can skip gram algorithm.",
            "If it is too long, we simply truncated and between these two short we simply padded with zeros."
        ],
        [
            "Immediately after the embedding, we have dropout layer.",
            "So the idea here is that during the training we want to force the network to randomly forget or ignore some words from the sentences.",
            "The idea is to.",
            "Forced network not to rely on individual keywords.",
            "Because that can cause overfitting.",
            "So empirically, dropout layers being shown to be quite useful for you know when you have small set of data where the training can be can be biased to some very strong features that might be caused causing overfitting.",
            "So this is the reason why we use."
        ],
        [
            "A dropout layer here.",
            "And then we have our CN plus Gru structure here, so.",
            "Convolutional layer convolutional layer.",
            "Here the intuition would be similar to extracting engrams because what it does is typically a has.",
            "It takes the input feature space and use a fixed window to scan the feature space each time it looks at a consecutive sequence coming from the input feature space and transform that into a new feature, and it does that.",
            "You know, sliding window over entire input feature space and that would give you a bunch of new features.",
            "So in analogy that would like that would be like a bunch of engrams if you like.",
            "And then the Max pooling layer would be selecting the most representative engrams."
        ],
        [
            "Situation and then the Gru layer.",
            "So the gates.",
            "Located in Recurrent Unit network, here is a societies you can you can think of it as a simplified version of our STM so our STM has three days here only has two and the reason why we choose it is kind of ad hoc cause.",
            "These data we deal with our show in a minute compared to other problems.",
            "Video is slightly smaller, so using a simpler structure you have fewer parameters to train so you avoid problem of overfitting.",
            "But the intuition here generally is we want to capture the kind of core currying engrams and also their ordering information.",
            "So to give an example, if you look at the sentence here so these refugees not welcome in my country, they should be all deported.",
            "So if our saying has captured in grams, like refugees, be deported and not welcome.",
            "Hopefully the Gru can capture the kind of dependencies between these phrases.",
            "And use those kind of dependencies or pairs of engrams too.",
            "To inform the classifier, so use that as as as the next part of layers.",
            "Next part of features in the network, so the Gio takes the feature space coming from the sea and transform data extracted features like that and represent the tweet in a final representation, which is just a vector of features."
        ],
        [
            "That is passed on to the final.",
            "Softmax layer, sorry so.",
            "National slide so softmax layer would be the one that does the actual classification and we add some kind of regular regularization tweaks.",
            "So we use the.",
            "Elastic net regularization and again, the reason is that Hawk we want to make sure that our model don't overfit.",
            "Too small data and when all these small tweaks put altogether associating the minute that they actually works like better."
        ],
        [
            "Now, if your machine and expertise will ask the question wasn't Noble T in your in your model, so this is the one slide that summarizes all kind of key points.",
            "I want to mention.",
            "So if you look at the area about identifying hate speech, as I said, that typically people you see in or some kind of recurrent neural networks and what we do is we try to combine them altogether and find a way to combine them so they're down in a meaningful way and give us some kind of interesting features.",
            "And there's also a lot of doubt about whether you can actually kept her capture useful ordering information in short sentences, and we actually could show that.",
            "Actually, yes, you can.",
            "And obviously, you know deep neural networks are not being used, not only being used for natural language processing, but a lot of other tasks like activity recognition, gesture recognition and their people has used saying plus RN.",
            "Are mostly the users ATM and our reason for using Gru is an prequel.",
            "We use Gru here and we also we introduce all these.",
            "Set of regularization regularization tweaks to make the the network work better."
        ],
        [
            "Now moving on to the evaluation.",
            "So we have collected all the datasets we know coming from Twitter and being used for the problem of identifying hate speech.",
            "And as you can see that the really varying in size and also nature so ranging from a couple of 1000 to over 24.",
            "Selden retreat"
        ],
        [
            "We need some comparative models and give you 4 here, so we use a simple SVM here, which is previously reported.",
            "But the authors mentioned that based on this, one plus is the SVM.",
            "But added with additional file types of features which we thought could be also useful for this kind of task.",
            "We have a baseline C and what it does is that we take our model.",
            "We remove all the Gru part so we have just saying we have seen plus Gru with an owners under.",
            "With a subscript B, which means that we are removing the old regularization tweaks.",
            "So we just have the simple model there.",
            "So if you look at the the the two 2 models that are introduced just now seeing the same plus G are you with with subscribe B you can think of it as some kind of ablation test of remote if you like.",
            "So we remove some components and see what's the difference.",
            "We also compare against state of Art so previous results reported.",
            "Only datasets that we test.",
            "And so the results are summarized all in here and you can see that you know compared to the state of art number patterns really strong apart from this sort of this kind of data where we only performed by by 1% and on the other cases we have obtained the best result.",
            "In some cases improvement can be quite significant.",
            "And if you compare the model we propose against the model with the alternative ones we propose to compare two models, you can see that there is some kind of incremental improvements when you add Gru.",
            "When you add those regularization regularization tweaks.",
            "So."
        ],
        [
            "So that's the bulk of the general result.",
            "We also done some kind of error analysis, try to understand what are the situations where it's just very difficult to get it right.",
            "So what we've done with two random sample of 200 tweets that auto model has failed and we try to understand what types of errors?",
            "Now there are other situations where the feature is very confusing, so country to a lot of us are probably thinking very often.",
            "You would imagine you know there might be some very strong features, or you know like use of abusive words.",
            "But actually these words are not very useful for prediction of hate speech.",
            "And because we use pre trained word embeddings models so there can be some out of out of vocabulary words.",
            "And then there are the situations where it's been very, very challenging for NLP in general for so many years.",
            "So cases like negations, questions matters, or even stereotypes, which is very subtle and implicit to understand.",
            "Which backs the quest."
        ],
        [
            "And you know.",
            "Can we actually solve the problem just by just looking at language?",
            "And here we done what we have done more analysis to show you something even more staggering numbers here so we can we calculate 2 numbers.",
            "One is that we take the number of trees for each class we divide the number by the unique words that are found in that class.",
            "So by by the work that we can see in that class.",
            "So basically it tells you know on average how many trees can share at least one word that give us some indication of.",
            "How to what extend the column twist we have for that class shared features?",
            "The next number is U2C, which stands for the unique worst 2 plus.",
            "I think you take the number of words.",
            "Fun in that class.",
            "And this number has to be the worst from owning that class, so it must not be funny.",
            "Other classes maybe 1 number by the worst found in the class, which can also include the ones from the other classes.",
            "So basically you're looking at what's the percentage of the words that are only found in that specific class, and again.",
            "If you think about if these numbers high basically says you can have a lot of possible features, so here the numbers I want you to look at the numbers comparatively, and especially compared to non hate against any other type of hate speech.",
            "Obviously you have a much higher number for both for both matrix for long hate speech.",
            "And the intuition is that you want these two figures to be high for that class to be considered in may be easier class for classification, right?",
            "But obviously what you can see here is that is a lot easier to classify to recognize non hate.",
            "But if you want to understand, you know what the type of hate is being expressed in the message really really challenging results have been shown in a couple of slides before was based on the micro average when you actually look at the precision, recall and F matter.",
            "Obtained by the by the per class basis.",
            "You notice that you get much higher results on this, but much lower on the other ones.",
            "So the question is, you know which one is more important, which I don't think I have a good answer to.",
            "I think it's probably running out of time for him so."
        ],
        [
            "I will probably skip the conclusion here and."
        ],
        [
            "I'll just stop here for questions, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thanks for him for the introduction so hi, I'm Zeke Young.",
                    "label": 0
                },
                {
                    "sent": "I'm from the information School University of Sheffield.",
                    "label": 1
                },
                {
                    "sent": "This is John work with one of my students and also previous colleague of mine from not even trends University.",
                    "label": 0
                },
                {
                    "sent": "So we're going to look at identifying hate speech on social media.",
                    "label": 0
                },
                {
                    "sent": "Twitter in this case using some kind of machine learning in SK Deep neural network methods.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as you are going to start giving you a bit of outline of this talk, so we'll we're looking to the problem of hate speech, and there's been a lot of work looking into this kind of problem recently, so I'll see you little bit reason to explain why that is the case, and obviously this is not the first work looking into this kind of problem, so I'll say something about what's the motivation.",
                    "label": 1
                },
                {
                    "sent": "What is our unique contributions to this field of research?",
                    "label": 0
                },
                {
                    "sent": "Then I want to explain how do we do it?",
                    "label": 0
                },
                {
                    "sent": "Or the findings and what the lessons we have learned from this study.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the problem of hate speech is really not a new one.",
                    "label": 1
                },
                {
                    "sent": "If you think about it, probably started the moment we create the web so you know back then you could create a website so people can create website and then use it to propagate hateful content.",
                    "label": 0
                },
                {
                    "sent": "But the problem probably got much, much worse and much more common when we gave everybody the possibility to create some content easily on web.",
                    "label": 0
                },
                {
                    "sent": "So the birth of Web 2.0 early.",
                    "label": 0
                },
                {
                    "sent": "And I mean, it's probably quite difficult to establish some kind of causal relationship between hate, speech, Anan crime, but there's a general consensus that the propagation and creation of such contents on the Internet on the web is creating some kind of environments that would encourage crime.",
                    "label": 0
                },
                {
                    "sent": "So really, we have this kind of problem for a long time already.",
                    "label": 0
                },
                {
                    "sent": "You know more than 20 years, but instead of having some kind of you know more.",
                    "label": 0
                },
                {
                    "sent": "Effective methods that can deal with it.",
                    "label": 0
                },
                {
                    "sent": "We are, I think having you know much, much, much, much more and more worse problems in this area and we kind of losing the control of it.",
                    "label": 0
                },
                {
                    "sent": "A lot of questions we ask is, are there social media companies actually doing something and answer is definitely yes.",
                    "label": 0
                },
                {
                    "sent": "They are doing something.",
                    "label": 0
                },
                {
                    "sent": "They actually spending hundreds of millions of euros every year.",
                    "label": 0
                },
                {
                    "sent": "Try to solve the problem, but very often they're looking at some kind of manual moderation process, and that's obviously is not going to scale to the web.",
                    "label": 0
                },
                {
                    "sent": "And in in this area we also see the trends towards legislation.",
                    "label": 1
                },
                {
                    "sent": "So back in January we have the first law against hate speech online introduced by the German government.",
                    "label": 0
                },
                {
                    "sent": "But that was mostly targeting night social media companies try to force them to be more proactive to deal with that kind of problem.",
                    "label": 0
                },
                {
                    "sent": "An introduction of the law was obviously with a lot of controversies and also has been very challenging to implement.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you want to deal with this kind of problem, the first question really is how do we find them?",
                    "label": 0
                },
                {
                    "sent": "How do we identify this kind of content in a very effective and efficient way?",
                    "label": 0
                },
                {
                    "sent": "So we sat there at moment.",
                    "label": 0
                },
                {
                    "sent": "Social media companies spending hundreds of millions of you 100 million euros to deal with the problem.",
                    "label": 1
                },
                {
                    "sent": "But typically they're doing some manual moderation process.",
                    "label": 0
                },
                {
                    "sent": "This is never going to scale, so when what we need is really some kind of automated semantic analysis, we need to be able to understand the contents using natural language processing or some other kind of semantic content analysis methods to help us to automatically pinpoint is kind of.",
                    "label": 0
                },
                {
                    "sent": "Content to support humans in the next part of decision-making, unsurprisingly, is a lot of work is being done is direction.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the literature, typically what you do with you, some machine learning based methods.",
                    "label": 0
                },
                {
                    "sent": "So you create some training data and you use it to train the model.",
                    "label": 0
                },
                {
                    "sent": "Do you apply the model to new data to make new predictions?",
                    "label": 1
                },
                {
                    "sent": "And the general trend is being that people are really moving to some use of deep neural network based models.",
                    "label": 1
                },
                {
                    "sent": "Which place to focus on creating some kind of interesting structures of network which would help us to automatically extract interesting features that would help with these kind of tasks and that is moving away from what we're doing.",
                    "label": 1
                },
                {
                    "sent": "You know, fire six years ago using traditional machine learning methods, hugely popular SVM logistic regression.",
                    "label": 0
                },
                {
                    "sent": "But with the 1st place on feature engineering.",
                    "label": 0
                },
                {
                    "sent": "So you do corpus analysis.",
                    "label": 0
                },
                {
                    "sent": "You do linguistic analysis to try to come up with some kind of.",
                    "label": 0
                },
                {
                    "sent": "Lexical syntactic patterns in the data try to understand you know what are the features that can be really helpful to separate hate speech from non hate content.",
                    "label": 0
                },
                {
                    "sent": "But when you look at the literature as one of the problem you find is that everybody has used their own sort of some data to test their model.",
                    "label": 0
                },
                {
                    "sent": "So some cases those data are not even publicly available.",
                    "label": 0
                },
                {
                    "sent": "So you can't really replicate what they have done.",
                    "label": 0
                },
                {
                    "sent": "All they have used some public data, but they changed it slightly, so they merged some data or they took part of it for some reasons.",
                    "label": 0
                },
                {
                    "sent": "Obviously what we want to do is we want to, you know, create an even better method.",
                    "label": 0
                },
                {
                    "sent": "But how do we know that?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, we want to fully test it.",
                    "label": 0
                },
                {
                    "sent": "We want to test the method on all the publicly available datasets so we know how do we compare with previous method being used on the same data that we have used.",
                    "label": 0
                },
                {
                    "sent": "But also that makes our method compatible for future research as well, so that's pretty much all motivation for this work and this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is what we done summarizing one sentence.",
                    "label": 0
                },
                {
                    "sent": "So we create one new method.",
                    "label": 0
                },
                {
                    "sent": "Which we test on all the publicly available Twitter corpus for hate speech.",
                    "label": 0
                },
                {
                    "sent": "Now we show that I'm practically we can do better using this new model.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So moving on to the methodology now.",
                    "label": 0
                },
                {
                    "sent": "So what I mean the basic principle is really straightforward.",
                    "label": 0
                },
                {
                    "sent": "We we take it as an LP problem.",
                    "label": 0
                },
                {
                    "sent": "We use some kind of text classification method and in this case it's short text classification because you have tweet typically is one or two sentences.",
                    "label": 1
                },
                {
                    "sent": "So as we said in the literature, in this area is really moving, so shifting towards use of deep neural network based method and I think the reasons are kind of two fold.",
                    "label": 1
                },
                {
                    "sent": "On the one hand a serious huge amount of effort from Penn Philan laborers feature engineering.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I'm pretty please being shown that it actually works better than those.",
                    "label": 1
                },
                {
                    "sent": "Traditional methods that relies on feature engineering.",
                    "label": 1
                },
                {
                    "sent": "So excuse me for shameless self promotional material here.",
                    "label": 0
                },
                {
                    "sent": "So on the 7th of June we have a poster that will talk about exactly this kind of task about using traditional machine learning Masters and feature engineering, but I'm going to explain why you don't really need to do that.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested, please come along and we have chat about that.",
                    "label": 1
                },
                {
                    "sent": "In the area of you look at the literature they use DNA, so some deep learning methods.",
                    "label": 0
                },
                {
                    "sent": "Obviously they focus on the kind of structure of the network, so the topology of the network.",
                    "label": 0
                },
                {
                    "sent": "But typically you have two flavors, so somebody use convolutional neural networks saying.",
                    "label": 0
                },
                {
                    "sent": "And the other branch will be using some kind of recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "So the most popular being oh STM long short term memory.",
                    "label": 0
                },
                {
                    "sent": "And obviously these two each type of these two networks have this own integration.",
                    "label": 0
                },
                {
                    "sent": "But we were thinking, you know, would pop would be possible to combine them all together and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you want to combine them, what what would be the right way to do it and what would be the intuition behind and other any you know advantage of combining the two networks altogether on these kind of space with task?",
                    "label": 0
                },
                {
                    "sent": "So this is I'll take here.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we took CNN with Gru which is gated recurrent unit network.",
                    "label": 1
                },
                {
                    "sent": "So if you're familiar with our STM or D and in general you know that it is could be considered as a simplified version of STM, so I don't think I will explain.",
                    "label": 0
                },
                {
                    "sent": "Our networking will be more details in the next few slides, but will focus on the high level intuitions and principles behind or try to avoid technical details.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Network starts with the word embedding layer.",
                    "label": 0
                },
                {
                    "sent": "So what it does is basically you take a tweet short sentence and you map that into a fixed lens fixed dimensional vector space.",
                    "label": 0
                },
                {
                    "sent": "Representation of that tweet.",
                    "label": 0
                },
                {
                    "sent": "And be cause the kind of data we do always comparat compared to the typical corporate you use for learning word embeddings, and much much smaller.",
                    "label": 0
                },
                {
                    "sent": "So we don't actually learn what in bed in front of data, but we use some pre trends.",
                    "label": 0
                },
                {
                    "sent": "What embeddings in this case we use the word two VEC model train.",
                    "label": 0
                },
                {
                    "sent": "You can skip gram algorithm.",
                    "label": 0
                },
                {
                    "sent": "If it is too long, we simply truncated and between these two short we simply padded with zeros.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Immediately after the embedding, we have dropout layer.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that during the training we want to force the network to randomly forget or ignore some words from the sentences.",
                    "label": 0
                },
                {
                    "sent": "The idea is to.",
                    "label": 0
                },
                {
                    "sent": "Forced network not to rely on individual keywords.",
                    "label": 1
                },
                {
                    "sent": "Because that can cause overfitting.",
                    "label": 0
                },
                {
                    "sent": "So empirically, dropout layers being shown to be quite useful for you know when you have small set of data where the training can be can be biased to some very strong features that might be caused causing overfitting.",
                    "label": 0
                },
                {
                    "sent": "So this is the reason why we use.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A dropout layer here.",
                    "label": 0
                },
                {
                    "sent": "And then we have our CN plus Gru structure here, so.",
                    "label": 0
                },
                {
                    "sent": "Convolutional layer convolutional layer.",
                    "label": 0
                },
                {
                    "sent": "Here the intuition would be similar to extracting engrams because what it does is typically a has.",
                    "label": 0
                },
                {
                    "sent": "It takes the input feature space and use a fixed window to scan the feature space each time it looks at a consecutive sequence coming from the input feature space and transform that into a new feature, and it does that.",
                    "label": 0
                },
                {
                    "sent": "You know, sliding window over entire input feature space and that would give you a bunch of new features.",
                    "label": 0
                },
                {
                    "sent": "So in analogy that would like that would be like a bunch of engrams if you like.",
                    "label": 0
                },
                {
                    "sent": "And then the Max pooling layer would be selecting the most representative engrams.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Situation and then the Gru layer.",
                    "label": 0
                },
                {
                    "sent": "So the gates.",
                    "label": 0
                },
                {
                    "sent": "Located in Recurrent Unit network, here is a societies you can you can think of it as a simplified version of our STM so our STM has three days here only has two and the reason why we choose it is kind of ad hoc cause.",
                    "label": 0
                },
                {
                    "sent": "These data we deal with our show in a minute compared to other problems.",
                    "label": 0
                },
                {
                    "sent": "Video is slightly smaller, so using a simpler structure you have fewer parameters to train so you avoid problem of overfitting.",
                    "label": 0
                },
                {
                    "sent": "But the intuition here generally is we want to capture the kind of core currying engrams and also their ordering information.",
                    "label": 0
                },
                {
                    "sent": "So to give an example, if you look at the sentence here so these refugees not welcome in my country, they should be all deported.",
                    "label": 1
                },
                {
                    "sent": "So if our saying has captured in grams, like refugees, be deported and not welcome.",
                    "label": 0
                },
                {
                    "sent": "Hopefully the Gru can capture the kind of dependencies between these phrases.",
                    "label": 0
                },
                {
                    "sent": "And use those kind of dependencies or pairs of engrams too.",
                    "label": 0
                },
                {
                    "sent": "To inform the classifier, so use that as as as the next part of layers.",
                    "label": 0
                },
                {
                    "sent": "Next part of features in the network, so the Gio takes the feature space coming from the sea and transform data extracted features like that and represent the tweet in a final representation, which is just a vector of features.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is passed on to the final.",
                    "label": 0
                },
                {
                    "sent": "Softmax layer, sorry so.",
                    "label": 0
                },
                {
                    "sent": "National slide so softmax layer would be the one that does the actual classification and we add some kind of regular regularization tweaks.",
                    "label": 0
                },
                {
                    "sent": "So we use the.",
                    "label": 0
                },
                {
                    "sent": "Elastic net regularization and again, the reason is that Hawk we want to make sure that our model don't overfit.",
                    "label": 0
                },
                {
                    "sent": "Too small data and when all these small tweaks put altogether associating the minute that they actually works like better.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, if your machine and expertise will ask the question wasn't Noble T in your in your model, so this is the one slide that summarizes all kind of key points.",
                    "label": 0
                },
                {
                    "sent": "I want to mention.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the area about identifying hate speech, as I said, that typically people you see in or some kind of recurrent neural networks and what we do is we try to combine them altogether and find a way to combine them so they're down in a meaningful way and give us some kind of interesting features.",
                    "label": 0
                },
                {
                    "sent": "And there's also a lot of doubt about whether you can actually kept her capture useful ordering information in short sentences, and we actually could show that.",
                    "label": 0
                },
                {
                    "sent": "Actually, yes, you can.",
                    "label": 0
                },
                {
                    "sent": "And obviously, you know deep neural networks are not being used, not only being used for natural language processing, but a lot of other tasks like activity recognition, gesture recognition and their people has used saying plus RN.",
                    "label": 0
                },
                {
                    "sent": "Are mostly the users ATM and our reason for using Gru is an prequel.",
                    "label": 0
                },
                {
                    "sent": "We use Gru here and we also we introduce all these.",
                    "label": 0
                },
                {
                    "sent": "Set of regularization regularization tweaks to make the the network work better.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now moving on to the evaluation.",
                    "label": 0
                },
                {
                    "sent": "So we have collected all the datasets we know coming from Twitter and being used for the problem of identifying hate speech.",
                    "label": 0
                },
                {
                    "sent": "And as you can see that the really varying in size and also nature so ranging from a couple of 1000 to over 24.",
                    "label": 0
                },
                {
                    "sent": "Selden retreat",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We need some comparative models and give you 4 here, so we use a simple SVM here, which is previously reported.",
                    "label": 1
                },
                {
                    "sent": "But the authors mentioned that based on this, one plus is the SVM.",
                    "label": 1
                },
                {
                    "sent": "But added with additional file types of features which we thought could be also useful for this kind of task.",
                    "label": 1
                },
                {
                    "sent": "We have a baseline C and what it does is that we take our model.",
                    "label": 0
                },
                {
                    "sent": "We remove all the Gru part so we have just saying we have seen plus Gru with an owners under.",
                    "label": 0
                },
                {
                    "sent": "With a subscript B, which means that we are removing the old regularization tweaks.",
                    "label": 0
                },
                {
                    "sent": "So we just have the simple model there.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the the the two 2 models that are introduced just now seeing the same plus G are you with with subscribe B you can think of it as some kind of ablation test of remote if you like.",
                    "label": 0
                },
                {
                    "sent": "So we remove some components and see what's the difference.",
                    "label": 0
                },
                {
                    "sent": "We also compare against state of Art so previous results reported.",
                    "label": 0
                },
                {
                    "sent": "Only datasets that we test.",
                    "label": 0
                },
                {
                    "sent": "And so the results are summarized all in here and you can see that you know compared to the state of art number patterns really strong apart from this sort of this kind of data where we only performed by by 1% and on the other cases we have obtained the best result.",
                    "label": 0
                },
                {
                    "sent": "In some cases improvement can be quite significant.",
                    "label": 0
                },
                {
                    "sent": "And if you compare the model we propose against the model with the alternative ones we propose to compare two models, you can see that there is some kind of incremental improvements when you add Gru.",
                    "label": 0
                },
                {
                    "sent": "When you add those regularization regularization tweaks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the bulk of the general result.",
                    "label": 0
                },
                {
                    "sent": "We also done some kind of error analysis, try to understand what are the situations where it's just very difficult to get it right.",
                    "label": 0
                },
                {
                    "sent": "So what we've done with two random sample of 200 tweets that auto model has failed and we try to understand what types of errors?",
                    "label": 0
                },
                {
                    "sent": "Now there are other situations where the feature is very confusing, so country to a lot of us are probably thinking very often.",
                    "label": 0
                },
                {
                    "sent": "You would imagine you know there might be some very strong features, or you know like use of abusive words.",
                    "label": 0
                },
                {
                    "sent": "But actually these words are not very useful for prediction of hate speech.",
                    "label": 0
                },
                {
                    "sent": "And because we use pre trained word embeddings models so there can be some out of out of vocabulary words.",
                    "label": 0
                },
                {
                    "sent": "And then there are the situations where it's been very, very challenging for NLP in general for so many years.",
                    "label": 0
                },
                {
                    "sent": "So cases like negations, questions matters, or even stereotypes, which is very subtle and implicit to understand.",
                    "label": 0
                },
                {
                    "sent": "Which backs the quest.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you know.",
                    "label": 0
                },
                {
                    "sent": "Can we actually solve the problem just by just looking at language?",
                    "label": 0
                },
                {
                    "sent": "And here we done what we have done more analysis to show you something even more staggering numbers here so we can we calculate 2 numbers.",
                    "label": 0
                },
                {
                    "sent": "One is that we take the number of trees for each class we divide the number by the unique words that are found in that class.",
                    "label": 1
                },
                {
                    "sent": "So by by the work that we can see in that class.",
                    "label": 1
                },
                {
                    "sent": "So basically it tells you know on average how many trees can share at least one word that give us some indication of.",
                    "label": 0
                },
                {
                    "sent": "How to what extend the column twist we have for that class shared features?",
                    "label": 1
                },
                {
                    "sent": "The next number is U2C, which stands for the unique worst 2 plus.",
                    "label": 0
                },
                {
                    "sent": "I think you take the number of words.",
                    "label": 0
                },
                {
                    "sent": "Fun in that class.",
                    "label": 0
                },
                {
                    "sent": "And this number has to be the worst from owning that class, so it must not be funny.",
                    "label": 0
                },
                {
                    "sent": "Other classes maybe 1 number by the worst found in the class, which can also include the ones from the other classes.",
                    "label": 0
                },
                {
                    "sent": "So basically you're looking at what's the percentage of the words that are only found in that specific class, and again.",
                    "label": 0
                },
                {
                    "sent": "If you think about if these numbers high basically says you can have a lot of possible features, so here the numbers I want you to look at the numbers comparatively, and especially compared to non hate against any other type of hate speech.",
                    "label": 0
                },
                {
                    "sent": "Obviously you have a much higher number for both for both matrix for long hate speech.",
                    "label": 0
                },
                {
                    "sent": "And the intuition is that you want these two figures to be high for that class to be considered in may be easier class for classification, right?",
                    "label": 0
                },
                {
                    "sent": "But obviously what you can see here is that is a lot easier to classify to recognize non hate.",
                    "label": 0
                },
                {
                    "sent": "But if you want to understand, you know what the type of hate is being expressed in the message really really challenging results have been shown in a couple of slides before was based on the micro average when you actually look at the precision, recall and F matter.",
                    "label": 0
                },
                {
                    "sent": "Obtained by the by the per class basis.",
                    "label": 0
                },
                {
                    "sent": "You notice that you get much higher results on this, but much lower on the other ones.",
                    "label": 0
                },
                {
                    "sent": "So the question is, you know which one is more important, which I don't think I have a good answer to.",
                    "label": 0
                },
                {
                    "sent": "I think it's probably running out of time for him so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will probably skip the conclusion here and.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll just stop here for questions, thank you.",
                    "label": 0
                }
            ]
        }
    }
}