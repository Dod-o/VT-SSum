{
    "id": "gv6ttdtod6fncb3uflo25rzu35dfyglx",
    "title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
    "info": {
        "author": [
            "Satyen Kale, Yahoo! Research"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Optimization Methods->Stochastic Optimization"
        ]
    },
    "url": "http://videolectures.net/colt2011_kale_optimization/",
    "segmentation": [
        [
            "So I."
        ],
        [
            "It started by giving some motivation for this work and this is probably well known to most people in the audience.",
            "But I'll just mention it anywhere just to bring this all on the same page.",
            "So everybody here probably knows what support vector machines are, so they are a very popular expressive algorithm for learning maximum margin classifiers.",
            "And the standard set up is that you have a data set of examples where exiled the features.",
            "And why are there labels the personal minus one and you want to try to find a linear classifier which basically has the widest margin around it, which separates the two into two parts.",
            "OK."
        ],
        [
            "Now."
        ],
        [
            "In usually for computational reasons, and also in practice when you have outliers, it makes more sense to have a smoother version of the 01 loss.",
            "Typically the hinge loss, which I've defined for you here.",
            "So for a weight vector W or an example, it's defined by this function.",
            "Now, while it is a convex loss function, it is.",
            "It is a nonsmooth function, because you have this Max appearing here, and this leads to the soft margin SVM formulation, which is typically written in the following way.",
            "So it's an optimization problem.",
            "You want to find a way to W which minimizes the average loss.",
            "Overall examples and some additional regularization term, which in this case is just some constant Lambda times the norm of W ^2.",
            "OK, so this is the optimization problem.",
            "They want to solve, so let us know if you features or subtraction problem.",
            "So the first feature is that it's a nonsmooth convex problem, because this term here is non soon convex.",
            "On the other hand, the second part of this optimization is actually not only convex, but it's strongly convex.",
            "OK, So what I mean by that is if you take the second derivative the Hessian, it is a positive definite matrix.",
            "And the third feature, which is probably the most important motivation for this talk, is that this is a huge scale problem.",
            "OK, so you are averaging over all examples, and in today's applications you have millions and millions of examples.",
            "Solving this optimization problem using a traditional solver becomes quite difficult."
        ],
        [
            "OK, so let's see what we mean by solving the SVM problem.",
            "So let me define capital F of W to be the objective function that we're trying to minimize.",
            "And our objective is to find an epsilon optimal solution which basically is a solution which is within an epsilon additive term of the optimal one.",
            "No, because this optimization this objective function has so many terms it if you try to use any traditional convex programming software is like interior point method or SM or assumed light.",
            "So all of their complexities are super linear in the number of examples and that makes completely infeasible for four deal Oscar datasets.",
            "What works in practice, or at least the method of choice, is a stochastic gradient descent.",
            "So so I will explain what."
        ],
        [
            "What I mean by that?",
            "So let me rewrite the objective function in the following way.",
            "So Capital F of W is now simply the expectation over examples drawn uniformly at random from the data set of the cost C of W for that example, where the cost is simply the hinge loss plus the regularization.",
            "Now viewing the objective function in this fashion, we can write down sort of gradient descent algorithm for this problem, which is the following one.",
            "So you choose starting starting weight vector.",
            "Arbitrarily.",
            "And in that iteration you sample data point from your data set and then you update your weight vector by moving in the negative gradient direction for the loss at the current example, the key insight here is that in expectation the gradient step that you take is basically the gradient of the function return to minimize.",
            "So in a sort of expected sense, you're trying to minimize the function that capital FW.",
            "So that is sort of a typical stochastic gradient descent algorithm."
        ],
        [
            "The question is how do you analyze it?",
            "There is a very nice and elegant way of analyzing stochastic gradient descent using regret minimization techniques.",
            "This is more commonly known as an online to batch conversion, so let's think of an online convex optimization problem where basically you're supposed to repeatedly choose points from some convex set, and you want to minimize regret with respect to some convex functions that are arriving online.",
            "And now suppose the convex functions that you get online are ID and let capital F be the expectation of the expectation of the of the cost functions.",
            "Now a very simple result is the following.",
            "So if there is an algorithm which has has average regret averaged over all iterations are of TT is the number of iterations, then if you run this algorithm for our inverse of one over epsilon iterations, then the average point that you will get in all of the iterations is an epsilon minimizer of F. So this is.",
            "This follows pretty much directly from the definition of regret."
        ],
        [
            "OK, so let's see what this gives us for stochastic gradient descent.",
            "The first thing is a theorem of Zinkevich from 2003.",
            "He proved that if you have arbitrary convex functions city, then the average grade can always be bounded by order one over square root T. This automatically leads to the fact that you can basically get an epsilon optimal solution to the problem in order one over epsilon squared iterations.",
            "A few years ago we in joint work with a lot as an Adam Killian with Wall.",
            "We showed that if the the cost functions are are actually strongly convex, then the stochastic gradient descent technique will actually have average regret of order log DVT.",
            "This gives you much faster convergence rates, so this was explored in this paper by.",
            "Schwartz they gave the Pegasos algorithm for solving SVM's in order one over epsilon times.",
            "Log one over epsilon iterations.",
            "So that's that's what you get from the upper bound side.",
            "On the lower bound side, there is a theorem of Agrawal, Bartlett, Ravi Kumar Wainwright, which says that if you are only allowed to access the functional trying to minimize using gradient green gradient or echo, then you must at least evaluated one over epsilon times.",
            "So this is pretty close to optimal.",
            "So within a log, one over epsilon factor, this upper bound is actually almost matches the lower bound."
        ],
        [
            "But there are some obvious natural questions, so the lower bound is order one Omega, one over epsilon.",
            "The question is, is there.",
            "Is it possible to get this?",
            "This kind of freight and order one over epsilon rate using a regret minimizing algorithm using the online to batch conversion?",
            "Now, one of the results of our paper is that the answer is no.",
            "So in particular we should we show that for any online algorithm which works for the stochastic strongly convex online optimization, there is some distribution for over which it always has regret Omega log TYT.",
            "So the log one over epsilon factor that you saw before it actually inherent an you cannot cannot really be reducing online to batch conversions.",
            "And this lower bound actually matches the similar lower bound of Abernethy, Agarwal, Bartlett and Rocklin, but they actually give a minimax lower bounds, so it actually works for adverse in the adversary setting.",
            "But we have obviously a more general lower bound because we actually only give stochastic cost functions.",
            "OK, so this is so we cannot do better of using regret minimizing or minimization or online."
        ],
        [
            "So there's obviously randomization barrier, so is it possible to overcome this barrier and the main result of this paper is that the answer is yes, we can actually get an algorithm which gets an epsilon optimal solution in order one over epsilon iterations of stochastic gradient descent, and this is obviously the optimal rate because it matches the lower bound.",
            "And this is actually a result that holds an expectation, but it's quite easy to get a high probability version of this of this result, except that we lose an extra factor of log log one over epsilon.",
            "So that's those are the two main results and I will start by describing the upper bound 1st and I'll quickly sketch the lower bound words.",
            "End of this talk."
        ],
        [
            "OK, so here this algorithm it is based on embarking idea, so we.",
            "So we basically divide times into geometrically increasing length.",
            "So we start with 2, ^2 2 squared and then two cubed and so on.",
            "And and in every epoch you do stochastic gradient descent, but with the learning rate at of value to the minus I. OK, so you basically increasing the epoch length geometrically and you are decreasing the learning rates geometrically as well.",
            "And the key point here, which makes this work, is that every epoch is started at the average point from the previous epoch.",
            "So this is some kind of bootstrapping idea.",
            "Basically we the average point in the park is already a pretty good solution.",
            "We start from there, pretty good solution and we actually get faster conversions.",
            "OK, so this is sort of a warm start or a bootstrapping approach, so that's the entire algorithm."
        ],
        [
            "So I'll quickly sketch that it's not.",
            "It's not very complicated actually.",
            "So for this analysis, I'm going to assume that all problem dependent constants are one.",
            "OK, that makes things very easy.",
            "So here is an invariant that I'll prove.",
            "So the invariant is the following.",
            "So in the epoch.",
            "The function value at the initial point minus the function value at the optimal point is at most two to the minus I -- 1.",
            "So we are two to the minus I -- 1 close to the optimal point in function value.",
            "Now the first thing to note is that using the strong convexity assumption on the function capital F, we immediately infer that this bound is sorry, the FW in it minus W star is actually bigger than the squared distance from the optimum point.",
            "OK, this is a simple consequence of strong convexity.",
            "So automatically we get an upper bound on the distance from the, I mean the actual Euclidean distance squared from the optimal point.",
            "OK, so now let me prove this invariant to you so it's very easy.",
            "So we just use the standard regret analysis for gradient descent that has been around from the paper of zinkevich, so standard regret bound says the following thing.",
            "So if you look at the average point.",
            "In your in your epoch and if you look at the difference of its function value from the optimal point, then it is at most wanted at most by at hour 2, which is the learning rate plus the distance squared of the initial point divided by 2:30.",
            "OK, now let's plug in the values that we chose which was at about 2:00 to the minus I and the number of iterations to be 2 to the."
        ],
        [
            "This one.",
            "So we get the following things, so this first term is to do the I -- I + 1.",
            "This distance squared is bounded by two to the minus I -- 1.",
            "And finally this Dominator is equal to 4.",
            "You put everything together and you get that average point minus the function value average point minus the optimal ones to the - and this is the point that you use for the next iteration.",
            "So the invariant is maintained.",
            "So that's that's pretty much the entire entire."
        ],
        [
            "Analysis.",
            "OK, so to complete analysis, what we showed is that basically the bootstrapping technique plus the strong convexity assumption implies that in the beginning of epoch I the initial point is true to the minus.",
            "Actually I minus one optimal.",
            "I mean just forget that.",
            "So after log to the base, two of one over epsilon iterations, we'll get an epsilon optimal point.",
            "And the number of gradient updates is simply the geometric sequence, so it's 2 ^2 + 2 ^3 plus something up to two to the log 2, one over epsilon plus one because this minus one.",
            "There is at most four epsilon.",
            "OK, so we take like one, so that's it basically.",
            "So we do one over epsilon written updates and we got to the point.",
            "OK, so that's the internal S of the upper bound.",
            "So let me move to the lower bound."
        ],
        [
            "So the lower Mount this first slide will only give you the intuition of the lower bound and the next.",
            "The next level formalize their intuition.",
            "So it's quite simple again, so we sorry I should anyway, so we use this distribution on on the strongly convex functions.",
            "So we define the next function city of W to be should be actually W -- X three squared, where XD is a Bernoulli random variable which takes one with probability P and zero probability 1 -- P. And P is an unknown parameter that we're trying to trying to figure out.",
            "Now if you look at the expectation of these cost functions, so the capital F of W, it is simply like W -- B squared plus some constant which depends on P. So forget about the constant.",
            "Essentially it's like W -- B ^2.",
            "So this automatically tells you that the best point to use in hindsight forward minimizing algorithm is equal to P. So you're basically trying to estimate the value of P as accurately as possible.",
            "So if you think about it intuitively, if you faced with this optimization, sorry this online optimization problem, there's sort of the best thing you can do is is to set the.",
            "Point to be the fraction of ones that you have seen so far.",
            "That's sort of like the best estimate you have of the probability P. And then so the regret at time T is going to be something like WT minus B squared and standard deviation bounds will tell you that WT is going to be around one over square root T away from the actual value.",
            "So you square that, you get 1 / T and you sum it up when you get locked.",
            "So that's sort of the intuition in one slide.",
            "Formalizing this intuition becomes much more tricky.",
            "It's a little bit messy, but I'll."
        ],
        [
            "Try to describe the formula, the formal analysis on this slide.",
            "So Interestingly, the lower bound is also based on any parking idea, so we again divide the time into epochs of geometrically increasing lengths.",
            "So we start from 2 squared to cubed, and so on.",
            "Now, given an online optimization algorithm, we use that algorithm to construct a nested sequence of intervals, one corresponding to every epoch.",
            "So they're nested in the sense that basically every subsequent interval is a subset of the previous one.",
            "And these intervals give you ranges of values for the unknown probability P. OK, so that is J1J 2J3, and so on.",
            "OK, So what is the property of these intervals?",
            "The key property is that using basically some information theoretic bounds, you can show that.",
            "For any probability P chosen inside your interval at time I so at any park I the algorithms output WT minus B squared must be at least two to the minus I so at least 2 minus I away from the true value.",
            "And now because the epoch has two to the I iterations.",
            "So we must incur like Omega one regret in every round.",
            "So we must incur at least a constant constant regret for any P in the interval in that epoch.",
            "And because we have log DPX overall because we don't take dramatically increasing lengths that you know.",
            "And we've got constant regret in every every bug.",
            "The total regret must be at least locked in.",
            "So that's the overall structure that most of the work was in in construct.",
            "Construction of these intervals, and I mean it's a little messy, but it's not.",
            "It's not too hard, but it's based on a standard sort of pinsker's inequality kind of stuff.",
            "So that's the sketch of the lower bound."
        ],
        [
            "And I'll end with my conclusion slides, right?",
            "Everybody can go to lunch.",
            "So I presented an optimal stochastic gradient descent algorithm for strongly convex optimization.",
            "And this and also presented a lower bound which basically shows that this kind of rate cannot be achieved by using online to batch conversion.",
            "So you really have to do something extra and as far as we know this is the first formal evidence that online optimization is actually strictly harder than the batch optimization.",
            "I mean, if I mean if somebody has other other examples, will be happy to hear them, but we couldn't find any other examples.",
            "And the main idea here is that basically even though regret minimizing algorithms by themselves will not work for this problem, using the bootstrapping technique in conjunction with minimizing algorithms can actually lead to very fast convergence rates, so that's pretty cool message from this stock and."
        ],
        [
            "It's all after 7.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It started by giving some motivation for this work and this is probably well known to most people in the audience.",
                    "label": 0
                },
                {
                    "sent": "But I'll just mention it anywhere just to bring this all on the same page.",
                    "label": 0
                },
                {
                    "sent": "So everybody here probably knows what support vector machines are, so they are a very popular expressive algorithm for learning maximum margin classifiers.",
                    "label": 1
                },
                {
                    "sent": "And the standard set up is that you have a data set of examples where exiled the features.",
                    "label": 0
                },
                {
                    "sent": "And why are there labels the personal minus one and you want to try to find a linear classifier which basically has the widest margin around it, which separates the two into two parts.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In usually for computational reasons, and also in practice when you have outliers, it makes more sense to have a smoother version of the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Typically the hinge loss, which I've defined for you here.",
                    "label": 0
                },
                {
                    "sent": "So for a weight vector W or an example, it's defined by this function.",
                    "label": 0
                },
                {
                    "sent": "Now, while it is a convex loss function, it is.",
                    "label": 0
                },
                {
                    "sent": "It is a nonsmooth function, because you have this Max appearing here, and this leads to the soft margin SVM formulation, which is typically written in the following way.",
                    "label": 0
                },
                {
                    "sent": "So it's an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You want to find a way to W which minimizes the average loss.",
                    "label": 0
                },
                {
                    "sent": "Overall examples and some additional regularization term, which in this case is just some constant Lambda times the norm of W ^2.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "They want to solve, so let us know if you features or subtraction problem.",
                    "label": 0
                },
                {
                    "sent": "So the first feature is that it's a nonsmooth convex problem, because this term here is non soon convex.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, the second part of this optimization is actually not only convex, but it's strongly convex.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I mean by that is if you take the second derivative the Hessian, it is a positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "And the third feature, which is probably the most important motivation for this talk, is that this is a huge scale problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so you are averaging over all examples, and in today's applications you have millions and millions of examples.",
                    "label": 1
                },
                {
                    "sent": "Solving this optimization problem using a traditional solver becomes quite difficult.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's see what we mean by solving the SVM problem.",
                    "label": 0
                },
                {
                    "sent": "So let me define capital F of W to be the objective function that we're trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "And our objective is to find an epsilon optimal solution which basically is a solution which is within an epsilon additive term of the optimal one.",
                    "label": 0
                },
                {
                    "sent": "No, because this optimization this objective function has so many terms it if you try to use any traditional convex programming software is like interior point method or SM or assumed light.",
                    "label": 0
                },
                {
                    "sent": "So all of their complexities are super linear in the number of examples and that makes completely infeasible for four deal Oscar datasets.",
                    "label": 0
                },
                {
                    "sent": "What works in practice, or at least the method of choice, is a stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So so I will explain what.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I mean by that?",
                    "label": 0
                },
                {
                    "sent": "So let me rewrite the objective function in the following way.",
                    "label": 0
                },
                {
                    "sent": "So Capital F of W is now simply the expectation over examples drawn uniformly at random from the data set of the cost C of W for that example, where the cost is simply the hinge loss plus the regularization.",
                    "label": 0
                },
                {
                    "sent": "Now viewing the objective function in this fashion, we can write down sort of gradient descent algorithm for this problem, which is the following one.",
                    "label": 0
                },
                {
                    "sent": "So you choose starting starting weight vector.",
                    "label": 0
                },
                {
                    "sent": "Arbitrarily.",
                    "label": 0
                },
                {
                    "sent": "And in that iteration you sample data point from your data set and then you update your weight vector by moving in the negative gradient direction for the loss at the current example, the key insight here is that in expectation the gradient step that you take is basically the gradient of the function return to minimize.",
                    "label": 0
                },
                {
                    "sent": "So in a sort of expected sense, you're trying to minimize the function that capital FW.",
                    "label": 0
                },
                {
                    "sent": "So that is sort of a typical stochastic gradient descent algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The question is how do you analyze it?",
                    "label": 0
                },
                {
                    "sent": "There is a very nice and elegant way of analyzing stochastic gradient descent using regret minimization techniques.",
                    "label": 0
                },
                {
                    "sent": "This is more commonly known as an online to batch conversion, so let's think of an online convex optimization problem where basically you're supposed to repeatedly choose points from some convex set, and you want to minimize regret with respect to some convex functions that are arriving online.",
                    "label": 1
                },
                {
                    "sent": "And now suppose the convex functions that you get online are ID and let capital F be the expectation of the expectation of the of the cost functions.",
                    "label": 0
                },
                {
                    "sent": "Now a very simple result is the following.",
                    "label": 0
                },
                {
                    "sent": "So if there is an algorithm which has has average regret averaged over all iterations are of TT is the number of iterations, then if you run this algorithm for our inverse of one over epsilon iterations, then the average point that you will get in all of the iterations is an epsilon minimizer of F. So this is.",
                    "label": 0
                },
                {
                    "sent": "This follows pretty much directly from the definition of regret.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's see what this gives us for stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "The first thing is a theorem of Zinkevich from 2003.",
                    "label": 0
                },
                {
                    "sent": "He proved that if you have arbitrary convex functions city, then the average grade can always be bounded by order one over square root T. This automatically leads to the fact that you can basically get an epsilon optimal solution to the problem in order one over epsilon squared iterations.",
                    "label": 0
                },
                {
                    "sent": "A few years ago we in joint work with a lot as an Adam Killian with Wall.",
                    "label": 0
                },
                {
                    "sent": "We showed that if the the cost functions are are actually strongly convex, then the stochastic gradient descent technique will actually have average regret of order log DVT.",
                    "label": 1
                },
                {
                    "sent": "This gives you much faster convergence rates, so this was explored in this paper by.",
                    "label": 0
                },
                {
                    "sent": "Schwartz they gave the Pegasos algorithm for solving SVM's in order one over epsilon times.",
                    "label": 0
                },
                {
                    "sent": "Log one over epsilon iterations.",
                    "label": 0
                },
                {
                    "sent": "So that's that's what you get from the upper bound side.",
                    "label": 0
                },
                {
                    "sent": "On the lower bound side, there is a theorem of Agrawal, Bartlett, Ravi Kumar Wainwright, which says that if you are only allowed to access the functional trying to minimize using gradient green gradient or echo, then you must at least evaluated one over epsilon times.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty close to optimal.",
                    "label": 0
                },
                {
                    "sent": "So within a log, one over epsilon factor, this upper bound is actually almost matches the lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there are some obvious natural questions, so the lower bound is order one Omega, one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "The question is, is there.",
                    "label": 0
                },
                {
                    "sent": "Is it possible to get this?",
                    "label": 0
                },
                {
                    "sent": "This kind of freight and order one over epsilon rate using a regret minimizing algorithm using the online to batch conversion?",
                    "label": 0
                },
                {
                    "sent": "Now, one of the results of our paper is that the answer is no.",
                    "label": 0
                },
                {
                    "sent": "So in particular we should we show that for any online algorithm which works for the stochastic strongly convex online optimization, there is some distribution for over which it always has regret Omega log TYT.",
                    "label": 0
                },
                {
                    "sent": "So the log one over epsilon factor that you saw before it actually inherent an you cannot cannot really be reducing online to batch conversions.",
                    "label": 0
                },
                {
                    "sent": "And this lower bound actually matches the similar lower bound of Abernethy, Agarwal, Bartlett and Rocklin, but they actually give a minimax lower bounds, so it actually works for adverse in the adversary setting.",
                    "label": 1
                },
                {
                    "sent": "But we have obviously a more general lower bound because we actually only give stochastic cost functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is so we cannot do better of using regret minimizing or minimization or online.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's obviously randomization barrier, so is it possible to overcome this barrier and the main result of this paper is that the answer is yes, we can actually get an algorithm which gets an epsilon optimal solution in order one over epsilon iterations of stochastic gradient descent, and this is obviously the optimal rate because it matches the lower bound.",
                    "label": 1
                },
                {
                    "sent": "And this is actually a result that holds an expectation, but it's quite easy to get a high probability version of this of this result, except that we lose an extra factor of log log one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So that's those are the two main results and I will start by describing the upper bound 1st and I'll quickly sketch the lower bound words.",
                    "label": 0
                },
                {
                    "sent": "End of this talk.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here this algorithm it is based on embarking idea, so we.",
                    "label": 0
                },
                {
                    "sent": "So we basically divide times into geometrically increasing length.",
                    "label": 0
                },
                {
                    "sent": "So we start with 2, ^2 2 squared and then two cubed and so on.",
                    "label": 0
                },
                {
                    "sent": "And and in every epoch you do stochastic gradient descent, but with the learning rate at of value to the minus I. OK, so you basically increasing the epoch length geometrically and you are decreasing the learning rates geometrically as well.",
                    "label": 0
                },
                {
                    "sent": "And the key point here, which makes this work, is that every epoch is started at the average point from the previous epoch.",
                    "label": 1
                },
                {
                    "sent": "So this is some kind of bootstrapping idea.",
                    "label": 0
                },
                {
                    "sent": "Basically we the average point in the park is already a pretty good solution.",
                    "label": 0
                },
                {
                    "sent": "We start from there, pretty good solution and we actually get faster conversions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is sort of a warm start or a bootstrapping approach, so that's the entire algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll quickly sketch that it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not very complicated actually.",
                    "label": 0
                },
                {
                    "sent": "So for this analysis, I'm going to assume that all problem dependent constants are one.",
                    "label": 0
                },
                {
                    "sent": "OK, that makes things very easy.",
                    "label": 0
                },
                {
                    "sent": "So here is an invariant that I'll prove.",
                    "label": 0
                },
                {
                    "sent": "So the invariant is the following.",
                    "label": 0
                },
                {
                    "sent": "So in the epoch.",
                    "label": 0
                },
                {
                    "sent": "The function value at the initial point minus the function value at the optimal point is at most two to the minus I -- 1.",
                    "label": 0
                },
                {
                    "sent": "So we are two to the minus I -- 1 close to the optimal point in function value.",
                    "label": 0
                },
                {
                    "sent": "Now the first thing to note is that using the strong convexity assumption on the function capital F, we immediately infer that this bound is sorry, the FW in it minus W star is actually bigger than the squared distance from the optimum point.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a simple consequence of strong convexity.",
                    "label": 0
                },
                {
                    "sent": "So automatically we get an upper bound on the distance from the, I mean the actual Euclidean distance squared from the optimal point.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let me prove this invariant to you so it's very easy.",
                    "label": 0
                },
                {
                    "sent": "So we just use the standard regret analysis for gradient descent that has been around from the paper of zinkevich, so standard regret bound says the following thing.",
                    "label": 1
                },
                {
                    "sent": "So if you look at the average point.",
                    "label": 0
                },
                {
                    "sent": "In your in your epoch and if you look at the difference of its function value from the optimal point, then it is at most wanted at most by at hour 2, which is the learning rate plus the distance squared of the initial point divided by 2:30.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's plug in the values that we chose which was at about 2:00 to the minus I and the number of iterations to be 2 to the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "So we get the following things, so this first term is to do the I -- I + 1.",
                    "label": 0
                },
                {
                    "sent": "This distance squared is bounded by two to the minus I -- 1.",
                    "label": 0
                },
                {
                    "sent": "And finally this Dominator is equal to 4.",
                    "label": 0
                },
                {
                    "sent": "You put everything together and you get that average point minus the function value average point minus the optimal ones to the - and this is the point that you use for the next iteration.",
                    "label": 0
                },
                {
                    "sent": "So the invariant is maintained.",
                    "label": 0
                },
                {
                    "sent": "So that's that's pretty much the entire entire.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so to complete analysis, what we showed is that basically the bootstrapping technique plus the strong convexity assumption implies that in the beginning of epoch I the initial point is true to the minus.",
                    "label": 1
                },
                {
                    "sent": "Actually I minus one optimal.",
                    "label": 0
                },
                {
                    "sent": "I mean just forget that.",
                    "label": 1
                },
                {
                    "sent": "So after log to the base, two of one over epsilon iterations, we'll get an epsilon optimal point.",
                    "label": 1
                },
                {
                    "sent": "And the number of gradient updates is simply the geometric sequence, so it's 2 ^2 + 2 ^3 plus something up to two to the log 2, one over epsilon plus one because this minus one.",
                    "label": 0
                },
                {
                    "sent": "There is at most four epsilon.",
                    "label": 0
                },
                {
                    "sent": "OK, so we take like one, so that's it basically.",
                    "label": 0
                },
                {
                    "sent": "So we do one over epsilon written updates and we got to the point.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the internal S of the upper bound.",
                    "label": 0
                },
                {
                    "sent": "So let me move to the lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the lower Mount this first slide will only give you the intuition of the lower bound and the next.",
                    "label": 0
                },
                {
                    "sent": "The next level formalize their intuition.",
                    "label": 0
                },
                {
                    "sent": "So it's quite simple again, so we sorry I should anyway, so we use this distribution on on the strongly convex functions.",
                    "label": 0
                },
                {
                    "sent": "So we define the next function city of W to be should be actually W -- X three squared, where XD is a Bernoulli random variable which takes one with probability P and zero probability 1 -- P. And P is an unknown parameter that we're trying to trying to figure out.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at the expectation of these cost functions, so the capital F of W, it is simply like W -- B squared plus some constant which depends on P. So forget about the constant.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's like W -- B ^2.",
                    "label": 0
                },
                {
                    "sent": "So this automatically tells you that the best point to use in hindsight forward minimizing algorithm is equal to P. So you're basically trying to estimate the value of P as accurately as possible.",
                    "label": 0
                },
                {
                    "sent": "So if you think about it intuitively, if you faced with this optimization, sorry this online optimization problem, there's sort of the best thing you can do is is to set the.",
                    "label": 0
                },
                {
                    "sent": "Point to be the fraction of ones that you have seen so far.",
                    "label": 1
                },
                {
                    "sent": "That's sort of like the best estimate you have of the probability P. And then so the regret at time T is going to be something like WT minus B squared and standard deviation bounds will tell you that WT is going to be around one over square root T away from the actual value.",
                    "label": 0
                },
                {
                    "sent": "So you square that, you get 1 / T and you sum it up when you get locked.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the intuition in one slide.",
                    "label": 0
                },
                {
                    "sent": "Formalizing this intuition becomes much more tricky.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit messy, but I'll.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Try to describe the formula, the formal analysis on this slide.",
                    "label": 0
                },
                {
                    "sent": "So Interestingly, the lower bound is also based on any parking idea, so we again divide the time into epochs of geometrically increasing lengths.",
                    "label": 0
                },
                {
                    "sent": "So we start from 2 squared to cubed, and so on.",
                    "label": 0
                },
                {
                    "sent": "Now, given an online optimization algorithm, we use that algorithm to construct a nested sequence of intervals, one corresponding to every epoch.",
                    "label": 0
                },
                {
                    "sent": "So they're nested in the sense that basically every subsequent interval is a subset of the previous one.",
                    "label": 0
                },
                {
                    "sent": "And these intervals give you ranges of values for the unknown probability P. OK, so that is J1J 2J3, and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the property of these intervals?",
                    "label": 0
                },
                {
                    "sent": "The key property is that using basically some information theoretic bounds, you can show that.",
                    "label": 0
                },
                {
                    "sent": "For any probability P chosen inside your interval at time I so at any park I the algorithms output WT minus B squared must be at least two to the minus I so at least 2 minus I away from the true value.",
                    "label": 0
                },
                {
                    "sent": "And now because the epoch has two to the I iterations.",
                    "label": 0
                },
                {
                    "sent": "So we must incur like Omega one regret in every round.",
                    "label": 0
                },
                {
                    "sent": "So we must incur at least a constant constant regret for any P in the interval in that epoch.",
                    "label": 1
                },
                {
                    "sent": "And because we have log DPX overall because we don't take dramatically increasing lengths that you know.",
                    "label": 0
                },
                {
                    "sent": "And we've got constant regret in every every bug.",
                    "label": 0
                },
                {
                    "sent": "The total regret must be at least locked in.",
                    "label": 0
                },
                {
                    "sent": "So that's the overall structure that most of the work was in in construct.",
                    "label": 0
                },
                {
                    "sent": "Construction of these intervals, and I mean it's a little messy, but it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not too hard, but it's based on a standard sort of pinsker's inequality kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "So that's the sketch of the lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll end with my conclusion slides, right?",
                    "label": 0
                },
                {
                    "sent": "Everybody can go to lunch.",
                    "label": 0
                },
                {
                    "sent": "So I presented an optimal stochastic gradient descent algorithm for strongly convex optimization.",
                    "label": 1
                },
                {
                    "sent": "And this and also presented a lower bound which basically shows that this kind of rate cannot be achieved by using online to batch conversion.",
                    "label": 0
                },
                {
                    "sent": "So you really have to do something extra and as far as we know this is the first formal evidence that online optimization is actually strictly harder than the batch optimization.",
                    "label": 1
                },
                {
                    "sent": "I mean, if I mean if somebody has other other examples, will be happy to hear them, but we couldn't find any other examples.",
                    "label": 0
                },
                {
                    "sent": "And the main idea here is that basically even though regret minimizing algorithms by themselves will not work for this problem, using the bootstrapping technique in conjunction with minimizing algorithms can actually lead to very fast convergence rates, so that's pretty cool message from this stock and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's all after 7.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}