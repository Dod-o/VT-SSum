{
    "id": "z72ipehvo5xfmidsaraghztq3dwpcnwz",
    "title": "Monte-Carlo Sampling for Regret Minimization in Extensive Games",
    "info": {
        "author": [
            "Marc Lanctot, Department of Computing Science, University of Alberta"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Monte Carlo Methods"
        ]
    },
    "url": "http://videolectures.net/icml09_lanctot_mccfr/",
    "segmentation": [
        [
            "Hi, my name is Mark and I'm going to be talking about Monte Carlo sampling for regret minimization in games."
        ],
        [
            "So here we go.",
            "Our goal here is to find Nash equilibrium in large 0 sum games.",
            "So it turns out that a lot of Multiagent decision problems could actually be modelled as games, and the complexity of games girls pretty quickly.",
            "So we want to know how to do this for large games.",
            "So here's the little talk overview.",
            "We're going to reduce this algorithm called counterfactual regret minimization.",
            "We're going to show how it doesn't scale to very large games, and we're going to propose a new way to scale it down.",
            "Bye."
        ],
        [
            "Using sampling.",
            "OK, first some background and extensive game is basically a game that could be expressed in tree form.",
            "So you have a collection of nodes which represent your state and you have a collection of actions that take you from note to note if you look at this example here you have a chance node at the top which is denoted with an* in there, that represents a chance event such as a roll of a dice or card that you get."
        ],
        [
            "From a deck.",
            "So in.",
            "Sorry in in these games you have histories which are basically sequences of actions.",
            "So in this particular case, C1A would be a history."
        ],
        [
            "And you have terminal histories, which I'm going to note by Zedd and in this case you have at C2 BE.",
            "That would be a terminal history.",
            "It takes you from."
        ],
        [
            "The route to the leaf you have.",
            "These things called information sets.",
            "Information sets are basically groupings of nodes that a player cannot distinguish between.",
            "So we have three here, one that belongs to player one, and two that belong to player 2 if I'm right.",
            "Yes, and the first one I won.",
            "Is basically basically holds two nodes that player 1 cannot distinguish between because he doesn't know what the outcome of the chance node was.",
            "So you can think about taking a card from a deck and player 1 does not find out what it is.",
            "So he knows that he's in one of those two nodes, but he."
        ],
        [
            "Don't know which one.",
            "OK, and we're just going to do a capital A of I to be the collection of actions that one can take from this information set high and small.",
            "I will always."
        ],
        [
            "Represent the actual player that's that's to go, or who's turn it is?",
            "OK, so we'll talk about strategies.",
            "Basically a strategy in an imperfect information game is a distribution from an information set to your actions.",
            "So here's an example of a of a strategy for player two, Player 2."
        ],
        [
            "Nation set in blue.",
            "I'm going to denote Sigma minus I to be the strategy of the opponents of I. Anna Profile is basically just a collection of strategies, so we're going to deal with two player 0 sum games.",
            "So a profile is just going to just mean Sigma one and Sigma 2."
        ],
        [
            "A grouping you is basically the payoff that a player would get if he plays until the end of the game.",
            "So here's an example of payoff negative one and plus three.",
            "Because this is a 0 sum game, I only use the payoff for player one, Player 2 pay offs are just the inverse, so the negative of these."
        ],
        [
            "OK, and Pi I'm going to use to refer to a product of probabilities.",
            "So Pi Sigma H is a product of probabilities along a history H IF players use their strategies this profile Sigma and you can separate this into each one of the players contributions.",
            "So Pi subscript I refers to that player's contribution.",
            "So here you can see .3 three point 4.2 that would be a product that would be this for that history.",
            "And if you just look at player one's contribution, it would be .4 that would be this pie."
        ],
        [
            "Bye, OK, so what's counterfactual?",
            "Regret counterfactual threat is an iterative strategy altering algorithm where we have a strategy for each player at each iteration."
        ],
        [
            "The strategies that we start off with or completely arbitrary and overtime we update these strategies in a party."
        ],
        [
            "Killer way.",
            "So we have iteration."
        ],
        [
            "Two and three, and what we can show.",
            "Well, if we can, if we can show that we can keep the average overall regret low.",
            "In particular, if we can keep it less than epsilon, we can show that the average profile of all these strategies overtime converges to a two epsilon Nash equilibrium.",
            "So what do I mean by Nash equilibrium?",
            "Nash equilibrium?",
            "Is the solution concept in game theory, where every player is playing with a strategy that they don't want to deviate from because they can't do any better than epsilon.",
            "Nash equilibrium means a player does have a small incentive to deviate because he can do better by epsilon for particular strategy.",
            "So what we want to do is we want to minimize this.",
            "Well sorry, what I should say is we can show that in CFR this this epsilon goes to zero overtime.",
            "So we're actually converging to a true Nash equilibrium, which is good."
        ],
        [
            "OK, this is the basic idea behind CFR.",
            "This is previous work, so these are sort of the details extracted from their paper to minimize overall regret.",
            "What we do is we define this term called immediate counterfactual regret.",
            "This is basically a function.",
            "It's a property of each information set and we can show that the overall average regret is bounded by the sum of the positive portion.",
            "Portions of this immediate counterfactual regret.",
            "So that's good.",
            "So if we can minimize that inner side of the sum, then we can we also minimize the total regret.",
            "And if we use in particular if we use regret matching blackwells approach ability, which is a way of changing our strategies based on our cumulative regrets, we can show that we get this particular bound here, so the overall average regret actually goes down as T goes to Infinity.",
            "That's the good thing.",
            "These terms in the numerator here are basically properties of the game, so this capital Delta is the payoff range.",
            "This is this is going to be a big quantity because it's the number of information sets which tends to be large in most games and a is just the average number of actions or maximum number of actions OK?"
        ],
        [
            "Now I'm going to define something very central to this entire talk.",
            "It's called counterfactual value.",
            "So here's an equation that looks a bit scary, but really.",
            "If you think about it for a second, it's pretty intuitive.",
            "If you look at the inside of some.",
            "So for every history in an information set.",
            "So this this value is defined given a particular profile at a particular information set, we want to define a value called counterfactual value for every history and then information set and for all terminal histories that you would reach through that information set we have a some of these product of terms.",
            "Now if you forget the first term Pi negative, I you look at the last two terms.",
            "\u03a0 of Hz and you I've said this is really the product of the probabilities.",
            "If the players were to play with their current strategies until each one of those terminal histories from that information set.",
            "So really you can think of these two terms here as just the expected value of playing with their strategies from that information set until the end.",
            "OK, so we you know this.",
            "That's quite familiar.",
            "We've seen that before.",
            "What we're going to do here is we're going to wait it by the opponents probability of getting there.",
            "OK, so that's where this counterfactual comes in.",
            "OK, now we can define a similar term.",
            "VI here with this special notation and really this is just the same thing, except that information set I were going to take action A so this is what this term.",
            "So what we want to do is we want to compare these two values.",
            "We've gotten to information set I.",
            "If we use our current strategies, we can expect to get a certain pay off, but what if we take action I?",
            "Sorry, what if we take action A with full probability?",
            "How does our expected value change?"
        ],
        [
            "And that's basically the idea.",
            "It's sort of an internal regret.",
            "Measure that we try to minimize and we show that we can minimize external grid by doing that.",
            "So here's the basic algorithm you want to walk the game tree.",
            "And here is a small example game tree.",
            "Now these values you at the bottom here are actually expected payoff if players play with the current strategies and we want to calculate the values for this information set which is represented by the data."
        ],
        [
            "Box here.",
            "OK, So what we do is we first if we would traverse the tree so we get to the node on the left and we calculate here.",
            "This V is basically the expected value or the counter to counter factual value.",
            "If we use our strategies until the end of the game.",
            "And the strategy at this particular information set is take a with 40%, take be with 60%."
        ],
        [
            "And then we can calculate the same value except we take a with one.",
            "Probability one and we take B with probability zero and we compare these two values.",
            "We just take the difference.",
            "This is the."
        ],
        [
            "Immediate regret, counterfactual regret, and we add this up for Action B."
        ],
        [
            "Now this this was for one node.",
            "We do this for all the nodes in the tree."
        ],
        [
            "And basic."
        ],
        [
            "What we're accumulating are these tables of accumulated cumulative regrets.",
            "OK."
        ],
        [
            "What do we do with those?",
            "Well, regret matching is a strategy that takes.",
            "The cumulative regret and turns them into new strategies.",
            "So what we do is we get a new strategy at the next information set, which is a function of these little regrets that were."
        ],
        [
            "Overtime and we update our average profile and this is the actual thing that's converting can."
        ],
        [
            "Urging to an equilibrium.",
            "OK, so that's vanilla so far.",
            "This was used in poker, so this is previous work.",
            "Our sampling variants are actually quite tide to these, so what's good about CFR?",
            "Well, the greatest thing I think is that counterfactual regret requires space linear in the number of information sets, that's.",
            "I consider that a breakthrough because we didn't see that before now or before two years ago.",
            "So we've solved games up to 10 to 12 states.",
            "We've used them in poker bots and we've we've defeated human."
        ],
        [
            "Experts, so that's good.",
            "What's not so good?",
            "Well, this is mostly an offline algorithm.",
            "It's not really suited for online use, it actually took, I think up to two weeks to converge to the poker strategy that we use against the humans."
        ],
        [
            "What's even less than good in that or not even less than not so good, is that the iterations require a full tree traversal an for some games that's just not possible."
        ],
        [
            "Or doing."
        ],
        [
            "Several of them is not possible.",
            "So how do we?"
        ],
        [
            "Overcome this pitfall.",
            "Well we define well.",
            "Basically we do sampling.",
            "So I'm going to.",
            "We're going to find here is script Q and that that's equal to a set of cues and what these cues are.",
            "These Q Jays.",
            "These are just blocks, so this is like a subset of the tree, but these are blocks of terminal history, so this little purple triangle that's represented in this triangle here would be one of those blocks, and we're going to sample those blocks.",
            "And we're going to say that the collection of all these cues spans Ed, so that we have a number of these blocks and the Union of all these blocks gives us all of the terminal histories in the entire."
        ],
        [
            "OK, now we're going to find something called sampled counterfactual value, which may or may not look scary to you, but it's basically the exact same thing that we had before, but it uses this little trick that Gabor mentioned this morning.",
            "I'm sorry."
        ],
        [
            "I gave it away, but we're going to use the trick that were mentioned this morning where we have where this sampled counterfactual value actually just is an unbiased estimator for the real counter factual value.",
            "So if we perform the analogous updates but on the sample part of the tree, we can show that we also converge to a Nash equilibrium."
        ],
        [
            "OK, so that's.",
            "Here that is.",
            "So the expected value of this sample counter factor value, which is really just divided.",
            "You divide by the probability of.",
            "Of sampling that one block.",
            "Well.",
            "Yes, yeah.",
            "That's why you have the terminal history is in the block with the intersection of Zed survive.",
            "Where is that?",
            "So by is the terminal histories that went through this this information set.",
            "So this will be 0 for everything that's outside of the block, yeah?",
            "OK, so here's this little trick."
        ],
        [
            "I'm mostly just going to skip this.",
            "The important part really is that this is easy to show.",
            "It's in the paper, but this is really the driving force of this of the entire sampling meth."
        ],
        [
            "So it's important.",
            "OK, so basically I'm going to propose the MC or Monte Carlo CFR algorithm, which is basically just sampling block performing the Monte Carlo version of the updates, and we're going to.",
            "We're going to minimize this sampled immediate counterfactual regret rather than the immediate counterfactual regret we're going to use regret matching blackwells to show that we can update the strategies and the average profile will."
        ],
        [
            "Converge to Nash equilibrium.",
            "OK few questions, what do we use as a sampling scheme?",
            "So I haven't told you how we sample these queues.",
            "I've just told you that we have a collection of them.",
            "What are these blocks and how do we sample them?",
            "Does it converge to epsilon Nash?",
            "If that's true, does it converge for any sampling scheme?",
            "An very important?",
            "What's the rate of convergence and how well does it?"
        ],
        [
            "Empirically, OK, here are some here some answers.",
            "We're going to talk about two sampling schemes.",
            "It does converge to epsilon Nash for what I'm going to call reasonable strategies, you can come up with reasonable sampling strategies.",
            "You can definitely come up with some sampling strategies that it won't converge on, because the sampled counterfactual value would not make sense, or it would be able to find.",
            "We're going to.",
            "We're going to give you some regret bounds for some of the convergence world will show you the convergence rate in both theoretical case."
        ],
        [
            "In the empirical case.",
            "OK, so.",
            "Two sampling schemes will start off with one.",
            "We call either outcome sampling or Singleton sampling, and the idea here is that these blocks that I told you about a second ago are just going to contain a single."
        ],
        [
            "Terminal history.",
            "OK, so we're going to take the full game tree.",
            "We're only going to sample one terminal history, so this is the most extreme form of sampling that you can think of.",
            "Now.",
            "What we're going to say?",
            "How are we going to sample this?",
            "There are several ways that you can sample these terminal histories.",
            "You can give each terminal history equal probability uniformly.",
            "We're not going to do that."
        ],
        [
            "What we're going to do is we're going to find a sampling strategy.",
            "Sigma Prime here and we're going to do this epsilon greedy approach, where with probability epsilon we're going to sample our action randomly and with probability 1 minus epsilon.",
            "We're going to sample our action according to the player's current strategy at time T. OK. Now, as long as we make sure that epsilon is not equal to zero, we have our definition of counterfactual value that still makes sense.",
            "'cause if epsilon is equal to 0 at anytime, it's possible that you that you never sample certain blocks, and that's not good.",
            "You want to make sure you cover your space with."
        ],
        [
            "Probability.",
            "Now we're going to find this new regret value are still here, which, which is essentially what we had before.",
            "Except we're going to do the updates with our sampled kept counterfactual values instead.",
            "So if you workout the details, this is what your regret equations turnover.",
            "Yes, your regret updates turn out to be.",
            "Now.",
            "Notice this WI here.",
            "This is the wait what we get on the bottom here is the probability of sampling that that particular terminal history and if we have this product, if we have this sampling profile, the way I've defined it, it's just the product of sampling each action at each node.",
            "OK."
        ],
        [
            "Now.",
            "Here's where this talk relates to this workshop so.",
            "Well, listen now if you haven't."
        ],
        [
            "OK, 'cause this is mostly used as an offline method."
        ],
        [
            "OK, but this is interesting because what we can do is we can define this sampling profile at the opponents nodes.",
            "We can define the sampling profile to be equal to their current strategy.",
            "Now what that does?"
        ],
        [
            "Is it turns this so this you can separate between your contribution and the opponent's contribution.",
            "If if the sampling strategy is equal to the opponent's current strategy at that iteration, this term and that term are equal, so they cancel out.",
            "That might not see."
        ],
        [
            "Seem significant, but what's actually good about that is that now your update equation does not rely on the opponent ever.",
            "You don't need to know what the opponents."
        ],
        [
            "That's very good, because what you can do with that."
        ],
        [
            "Is you can do general regret minimization.",
            "So sort of like what was done with Exp.",
            "Three in normal form games where you're playing against an unknown opponent you have.",
            "You don't know anything about his strategy.",
            "You could even be changing his strategy overtime and what you're doing is we can prove that these updates actually converge or they minimize overall regret, which is good.",
            "And Furthermore, what you can do is if you know the planes T. So if you know how many times you're going to play against this.",
            "Unknown opponent, you can actually find the optimal exploration epsilon for that given T. So this is future work where."
        ],
        [
            "We're hoping to do something cool there, OK?",
            "Second sampling scheme is called external sampling, so this one actually works better.",
            "We can show that the regret bound is actually tighter for this one."
        ],
        [
            "Though when I explain it to you, it might seem like it will do worse.",
            "So how are we going to sample?",
            "So I told you about an extreme case where we sample where a block is just contains one single terminal history now.",
            "In external sampling, what we're going to do, we're only going to sample chance and the opponents actions, so we're going to do this tree traversal over over the subtree that's defined by a particular pure strategy that's used by the opponent.",
            "So a pure strategy is just the deterministic mapping from information sets, the actions, and the way that we're going to sample that deterministic strategy is by using the opponent's current strategy so the opponent's current strategy at some in some iteration.",
            "Is going to be some mixed distribution.",
            "We're going to use that to sample a deterministic strategy, and whenever we encounter on an opponent node through our traversals, we're just going to choose the action that was sampled.",
            "OK, so we're adding some kind of bias here."
        ],
        [
            "So the block probabilities results.",
            "So these are.",
            "This is the term that we divide by in our counterfactual value definition.",
            "The regret update equations actually become simpler because terms cancel out.",
            "So if we if we apply these regret equations and are sampled nodes, we can show that this."
        ],
        [
            "Converges.",
            "OK theoretical result.",
            "So what we've done is we've actually tightened the bound.",
            "Of the vanilla, see if our algorithm, so the original counterfactual regret algorithm, by defining this term M and this term M What we had before was that the overall regret was less than a bunch of terms.",
            "But the number of information sets are the the number of information sets was in that term.",
            "Now we have a better bound.",
            "We can show that it's bounded by this value M, which we can show.",
            "It's always either between the root of the number of information sets or the number of information sets.",
            "And actually for most balanced games, it's going to be the root of the number of information sets, so that's good.",
            "Of course, in practice we notice that it converges a lot faster than this bound, so in practice that didn't really matter.",
            "But we have a good theoretical bound.",
            "Now we've showed a probability connection between sampling case and the non sampling case, and that's here if you notice the difference between these equations.",
            "So the second sampling scheme I talked about the external sampling scheme we're only a constant factor off where that constant depends on the probability that you want the bound to hold.",
            "So with bound one with probability 1 -- P. We have that this is true.",
            "So P is a root in the denominators, so that can still get big butt.",
            "At least it doesn't blow up some function in the actual game structure.",
            "Now for outcome sampling we have this extra little Delta.",
            "Here Delta was the the smallest probability of sampling a terminal history, which can be very small, so this will be large.",
            "So really, this is a sanity check.",
            "It does converge, but again, we're off from external sampling by some factor, which is one over the smallest probability of reaching of sampling a particular terminal history.",
            "So it does converge.",
            "So the question is really how does this work in practice?",
            "Because if you look at these regret bounds, you might not be convinced."
        ],
        [
            "So the answer is, it works really well in practice and the main idea of the main reason it works well in practice is because these iterations over these sampled blocks take a lot less long than the full iterations of counterfactual regret, because they had they have to go through the entire tree and the point is you're doing these regret updates on these as you go by it.",
            "So if you can do 10,000 iterations of the sample based counterfactual regret, you're gaining information over those.",
            "10,000 iterations where you can only do one counterfactual gret iteration.",
            "In that time, you're not getting as much information, or.",
            "The success of information successive iterations can take advantage of all the regret updates that you've done in the previous passes.",
            "OK, OK, So what does this mean?",
            "It turns out that there's a obvious optimization that you can apply in the counterfactual regret case.",
            "So think of this red line as being the old stuff.",
            "This we ran this on one card poker, which is a smallish game.",
            "We ran it on three other games and the convergence is much faster.",
            "We can we can get to a convergence point way before counterfactual great in most cases by using sampling."
        ],
        [
            "OK, larger games.",
            "This is a larger game.",
            "Clayton tic tac toe.",
            "Here we don't see as good results for the outcome sampling case, but we do for the external sampling.",
            "Again, the first iteration of counterfactual regret takes a lot a long time, but because we have this pruning optimization afterwards, successive iterations get go faster on the Y axis, sorry."
        ],
        [
            "Y axis here we have exploitability which is essentially distance to the Nash equilibrium, so we actually want to be closer to to zero and nodes touched is essentially the accumulative number of nodes."
        ],
        [
            "That we've visited.",
            "OK, so we see similar things for other games that we tried on like Princess and Monster which is pursuit evasion game an Gooch feel so.",
            "And in which field we actually see that outcome sampling does better than external sample, so that's good."
        ],
        [
            "Bridgeville is an interesting card game, but we especially where you have a bunch of cards.",
            "Let's say an.",
            "And you have a deck of point cards on N from one to N. And what you're doing is you're bidding.",
            "You only see one of the point cards at the top, and you're bidding on that point guard.",
            "And what you do is you.",
            "It's a simultaneous move game where you put down your card to bit, and if you get if your card is higher than the opponents, you get the point card and then you just add up all the point cards and whoever has the most points wins.",
            "We have a less informational version where you don't find out which bid cards were being used throughout the game.",
            "So one thing I didn't mention about all these four games that we chose.",
            "The imperfect information comes from different sources.",
            "So in one card poker, the imperfect information is all due to chance in Layton Tic Tac toe.",
            "It's due to the opponents moves.",
            "Well, yeah, studio opponents moves in Princess and Monster.",
            "It's a combination of chance and the opponents moves and push people.",
            "I think it's just the opponents moves because we actually choose a fixed deck."
        ],
        [
            "OK, so what's the bottom line?",
            "The bottom line is that because we take less time per iteration than we can, we can leverage this.",
            "The fact that the updates give us information and we're doing a lot more iterations so we can reuse that information to converge faster than than the vanilla CFR case and future work.",
            "We want to try and use this for general regret minimization.",
            "We just haven't yet.",
            "We want to look at this epsilon greedy profile sampling strategy that we that we defined a little more closely, 'cause we just in our experiments.",
            "We just chose the best value of epsilon that worked.",
            "We worked that worked well experimentally, so we don't know much more about that, and one very interesting future work is.",
            "Is this abstraction over through imperfect recall?",
            "Case where?",
            "All the information sets at the current time, like right now information sets are defined by their.",
            "The sequence that players took to get to that point in the game.",
            "But it turns out that often those sequences you don't need everything in those sequences.",
            "In fact, the Markov assumption is actually an extreme.",
            "Extreme example of that where you only care about the last last state.",
            "So we want to try and extend that to the.",
            "We can cut down the number of the memory requirements that we need for our algorithms by by looking at this and we have exciting pericle results on that so far.",
            "But the problem is that it becomes an imperfect recall game and we have no theory about it.",
            "We don't know if it will converge in the imperfect recall, we know very little about imperfect recall games, so that's it, thanks.",
            "Can you give me some information about the why you use this counterfactual?",
            "Just.",
            "What do you need to yeah?",
            "This because if OK, so the idea is that.",
            "You're never if you don't get to that information set because the opponent doesn't play to get there.",
            "The intuition is that you don't really care about it.",
            "It if your opponent doesn't play to get there, why would you care about the equilibrium?",
            "That's below that.",
            "Of course, the definition of a Nash equilibrium includes what you would do, but there at that point but.",
            "Yeah, so we should say that it's zero if you don't get there because we don't really need to know what goes on below.",
            "And you starting from the initial profile that maybe.",
            "Far away from the truth, yes, for sure.",
            "What's the purpose of?",
            "I mean this argument because at the beginning maybe OK, the quality is not the right one, but maybe the Nash actually put some property or in those.",
            "Yeah.",
            "You don't know, so you should explore everything so you always end up exploring anyway, because because what's actually converging to the Nash is the average profile, so the strategies that each at each iteration.",
            "They're not those strategies that those are not the ones that are converging.",
            "Converging to Nash, it's the average like sort of what happens in fictional play, so.",
            "So my point is, if?",
            "If so.",
            "So what you're doing is you're waiting it by the opponents probability at that particular time step, which might be meaningful at that time step, so it's not so much that that's going to be 0 eventually.",
            "What you could be doing is you could be flipping between two strategies, like, like you would see in a fictitious play.",
            "So I don't know if I'm answering your question properly.",
            "Using another different point in so I don't know actually, but I think you might get a regret bound if you don't use counter factual utility.",
            "I just don't know if it works as well if they are as tight.",
            "Yeah, I think like the mostly the intuition is that you're not going to reach there at that particular time, so.",
            "You're not going to have any regret by playing.",
            "You're not going to have a regret for that action that is at an information set under there, because you're not actually going to get there.",
            "Does that make sense?",
            "So the counter.",
            "So the idea behind the counter factual utility is to is to tell you the difference between at a particular information that the difference between playing action versus not playing in action.",
            "So for any given set of strategies.",
            "If there are some information sets that you would never get to.",
            "Then why you then?",
            "Then you don't have.",
            "Your regret is not really defined at those.",
            "It depends on the number of information, so it doesn't depend on the polity to reach.",
            "Reaches together plays together.",
            "So it's like you're using a very fancy.",
            "Right?",
            "Use this.",
            "It's like it seems that if you are using a more natural definition of the re read each permission set, you will have the same regret.",
            "It's possible, yeah, I don't other than the intuition.",
            "I don't know how the counterfactual comes into the actual regret bounds, but I think it might be apart of regret matching 'cause regret matching is how we update our strategies at each information shot.",
            "It might actually be, well, I don't know.",
            "I think you could you might.",
            "You might be able to do a regret matching without the counterfactual utility.",
            "Yeah, you can for sure, but I don't know how they come into the bounds.",
            "I just yeah, I just know that that's basically the intuition.",
            "So I guess if.",
            "If you if you can imagine a sequence where you never reach a certain information that ever again.",
            "It's possible that we like the opponent, never placed the reach there.",
            "It's possible that we've decided already that we don't ever want to go there again, like we are.",
            "Average profile might already be at an equilibrium in that information set.",
            "So from then on, we're never going to get there.",
            "You can think of dominated action.",
            "For example, if you have a dominated action at the first Information Center first.",
            "Note very obviously dominated action, it will get removed right away.",
            "Their player will never play to reach that any information sets under there because it's dominated.",
            "So you definitely so in every Nash equilibrium will have probability zero of taking an action that gets to any information sets below connection.",
            "So made that I guess that's still part of the intuition I I can't.",
            "I can't tell you how it enters the regret bounds, but that's the idea.",
            "Speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, my name is Mark and I'm going to be talking about Monte Carlo sampling for regret minimization in games.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we go.",
                    "label": 0
                },
                {
                    "sent": "Our goal here is to find Nash equilibrium in large 0 sum games.",
                    "label": 1
                },
                {
                    "sent": "So it turns out that a lot of Multiagent decision problems could actually be modelled as games, and the complexity of games girls pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "So we want to know how to do this for large games.",
                    "label": 1
                },
                {
                    "sent": "So here's the little talk overview.",
                    "label": 0
                },
                {
                    "sent": "We're going to reduce this algorithm called counterfactual regret minimization.",
                    "label": 0
                },
                {
                    "sent": "We're going to show how it doesn't scale to very large games, and we're going to propose a new way to scale it down.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using sampling.",
                    "label": 0
                },
                {
                    "sent": "OK, first some background and extensive game is basically a game that could be expressed in tree form.",
                    "label": 1
                },
                {
                    "sent": "So you have a collection of nodes which represent your state and you have a collection of actions that take you from note to note if you look at this example here you have a chance node at the top which is denoted with an* in there, that represents a chance event such as a roll of a dice or card that you get.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From a deck.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                },
                {
                    "sent": "Sorry in in these games you have histories which are basically sequences of actions.",
                    "label": 0
                },
                {
                    "sent": "So in this particular case, C1A would be a history.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you have terminal histories, which I'm going to note by Zedd and in this case you have at C2 BE.",
                    "label": 0
                },
                {
                    "sent": "That would be a terminal history.",
                    "label": 1
                },
                {
                    "sent": "It takes you from.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The route to the leaf you have.",
                    "label": 0
                },
                {
                    "sent": "These things called information sets.",
                    "label": 0
                },
                {
                    "sent": "Information sets are basically groupings of nodes that a player cannot distinguish between.",
                    "label": 0
                },
                {
                    "sent": "So we have three here, one that belongs to player one, and two that belong to player 2 if I'm right.",
                    "label": 0
                },
                {
                    "sent": "Yes, and the first one I won.",
                    "label": 0
                },
                {
                    "sent": "Is basically basically holds two nodes that player 1 cannot distinguish between because he doesn't know what the outcome of the chance node was.",
                    "label": 0
                },
                {
                    "sent": "So you can think about taking a card from a deck and player 1 does not find out what it is.",
                    "label": 0
                },
                {
                    "sent": "So he knows that he's in one of those two nodes, but he.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't know which one.",
                    "label": 0
                },
                {
                    "sent": "OK, and we're just going to do a capital A of I to be the collection of actions that one can take from this information set high and small.",
                    "label": 0
                },
                {
                    "sent": "I will always.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Represent the actual player that's that's to go, or who's turn it is?",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll talk about strategies.",
                    "label": 0
                },
                {
                    "sent": "Basically a strategy in an imperfect information game is a distribution from an information set to your actions.",
                    "label": 1
                },
                {
                    "sent": "So here's an example of a of a strategy for player two, Player 2.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation set in blue.",
                    "label": 0
                },
                {
                    "sent": "I'm going to denote Sigma minus I to be the strategy of the opponents of I. Anna Profile is basically just a collection of strategies, so we're going to deal with two player 0 sum games.",
                    "label": 1
                },
                {
                    "sent": "So a profile is just going to just mean Sigma one and Sigma 2.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A grouping you is basically the payoff that a player would get if he plays until the end of the game.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of payoff negative one and plus three.",
                    "label": 0
                },
                {
                    "sent": "Because this is a 0 sum game, I only use the payoff for player one, Player 2 pay offs are just the inverse, so the negative of these.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and Pi I'm going to use to refer to a product of probabilities.",
                    "label": 0
                },
                {
                    "sent": "So Pi Sigma H is a product of probabilities along a history H IF players use their strategies this profile Sigma and you can separate this into each one of the players contributions.",
                    "label": 1
                },
                {
                    "sent": "So Pi subscript I refers to that player's contribution.",
                    "label": 0
                },
                {
                    "sent": "So here you can see .3 three point 4.2 that would be a product that would be this for that history.",
                    "label": 0
                },
                {
                    "sent": "And if you just look at player one's contribution, it would be .4 that would be this pie.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bye, OK, so what's counterfactual?",
                    "label": 0
                },
                {
                    "sent": "Regret counterfactual threat is an iterative strategy altering algorithm where we have a strategy for each player at each iteration.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The strategies that we start off with or completely arbitrary and overtime we update these strategies in a party.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Killer way.",
                    "label": 0
                },
                {
                    "sent": "So we have iteration.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two and three, and what we can show.",
                    "label": 0
                },
                {
                    "sent": "Well, if we can, if we can show that we can keep the average overall regret low.",
                    "label": 1
                },
                {
                    "sent": "In particular, if we can keep it less than epsilon, we can show that the average profile of all these strategies overtime converges to a two epsilon Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by Nash equilibrium?",
                    "label": 0
                },
                {
                    "sent": "Nash equilibrium?",
                    "label": 0
                },
                {
                    "sent": "Is the solution concept in game theory, where every player is playing with a strategy that they don't want to deviate from because they can't do any better than epsilon.",
                    "label": 0
                },
                {
                    "sent": "Nash equilibrium means a player does have a small incentive to deviate because he can do better by epsilon for particular strategy.",
                    "label": 1
                },
                {
                    "sent": "So what we want to do is we want to minimize this.",
                    "label": 0
                },
                {
                    "sent": "Well sorry, what I should say is we can show that in CFR this this epsilon goes to zero overtime.",
                    "label": 0
                },
                {
                    "sent": "So we're actually converging to a true Nash equilibrium, which is good.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is the basic idea behind CFR.",
                    "label": 0
                },
                {
                    "sent": "This is previous work, so these are sort of the details extracted from their paper to minimize overall regret.",
                    "label": 0
                },
                {
                    "sent": "What we do is we define this term called immediate counterfactual regret.",
                    "label": 0
                },
                {
                    "sent": "This is basically a function.",
                    "label": 0
                },
                {
                    "sent": "It's a property of each information set and we can show that the overall average regret is bounded by the sum of the positive portion.",
                    "label": 0
                },
                {
                    "sent": "Portions of this immediate counterfactual regret.",
                    "label": 1
                },
                {
                    "sent": "So that's good.",
                    "label": 0
                },
                {
                    "sent": "So if we can minimize that inner side of the sum, then we can we also minimize the total regret.",
                    "label": 0
                },
                {
                    "sent": "And if we use in particular if we use regret matching blackwells approach ability, which is a way of changing our strategies based on our cumulative regrets, we can show that we get this particular bound here, so the overall average regret actually goes down as T goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "That's the good thing.",
                    "label": 0
                },
                {
                    "sent": "These terms in the numerator here are basically properties of the game, so this capital Delta is the payoff range.",
                    "label": 0
                },
                {
                    "sent": "This is this is going to be a big quantity because it's the number of information sets which tends to be large in most games and a is just the average number of actions or maximum number of actions OK?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to define something very central to this entire talk.",
                    "label": 0
                },
                {
                    "sent": "It's called counterfactual value.",
                    "label": 0
                },
                {
                    "sent": "So here's an equation that looks a bit scary, but really.",
                    "label": 0
                },
                {
                    "sent": "If you think about it for a second, it's pretty intuitive.",
                    "label": 0
                },
                {
                    "sent": "If you look at the inside of some.",
                    "label": 0
                },
                {
                    "sent": "So for every history in an information set.",
                    "label": 0
                },
                {
                    "sent": "So this this value is defined given a particular profile at a particular information set, we want to define a value called counterfactual value for every history and then information set and for all terminal histories that you would reach through that information set we have a some of these product of terms.",
                    "label": 0
                },
                {
                    "sent": "Now if you forget the first term Pi negative, I you look at the last two terms.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 of Hz and you I've said this is really the product of the probabilities.",
                    "label": 0
                },
                {
                    "sent": "If the players were to play with their current strategies until each one of those terminal histories from that information set.",
                    "label": 0
                },
                {
                    "sent": "So really you can think of these two terms here as just the expected value of playing with their strategies from that information set until the end.",
                    "label": 0
                },
                {
                    "sent": "OK, so we you know this.",
                    "label": 0
                },
                {
                    "sent": "That's quite familiar.",
                    "label": 0
                },
                {
                    "sent": "We've seen that before.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do here is we're going to wait it by the opponents probability of getting there.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's where this counterfactual comes in.",
                    "label": 0
                },
                {
                    "sent": "OK, now we can define a similar term.",
                    "label": 0
                },
                {
                    "sent": "VI here with this special notation and really this is just the same thing, except that information set I were going to take action A so this is what this term.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is we want to compare these two values.",
                    "label": 0
                },
                {
                    "sent": "We've gotten to information set I.",
                    "label": 0
                },
                {
                    "sent": "If we use our current strategies, we can expect to get a certain pay off, but what if we take action I?",
                    "label": 0
                },
                {
                    "sent": "Sorry, what if we take action A with full probability?",
                    "label": 0
                },
                {
                    "sent": "How does our expected value change?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's basically the idea.",
                    "label": 0
                },
                {
                    "sent": "It's sort of an internal regret.",
                    "label": 0
                },
                {
                    "sent": "Measure that we try to minimize and we show that we can minimize external grid by doing that.",
                    "label": 0
                },
                {
                    "sent": "So here's the basic algorithm you want to walk the game tree.",
                    "label": 1
                },
                {
                    "sent": "And here is a small example game tree.",
                    "label": 0
                },
                {
                    "sent": "Now these values you at the bottom here are actually expected payoff if players play with the current strategies and we want to calculate the values for this information set which is represented by the data.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Box here.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we do is we first if we would traverse the tree so we get to the node on the left and we calculate here.",
                    "label": 0
                },
                {
                    "sent": "This V is basically the expected value or the counter to counter factual value.",
                    "label": 0
                },
                {
                    "sent": "If we use our strategies until the end of the game.",
                    "label": 0
                },
                {
                    "sent": "And the strategy at this particular information set is take a with 40%, take be with 60%.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can calculate the same value except we take a with one.",
                    "label": 0
                },
                {
                    "sent": "Probability one and we take B with probability zero and we compare these two values.",
                    "label": 0
                },
                {
                    "sent": "We just take the difference.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Immediate regret, counterfactual regret, and we add this up for Action B.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this this was for one node.",
                    "label": 0
                },
                {
                    "sent": "We do this for all the nodes in the tree.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basic.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're accumulating are these tables of accumulated cumulative regrets.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do we do with those?",
                    "label": 0
                },
                {
                    "sent": "Well, regret matching is a strategy that takes.",
                    "label": 0
                },
                {
                    "sent": "The cumulative regret and turns them into new strategies.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we get a new strategy at the next information set, which is a function of these little regrets that were.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overtime and we update our average profile and this is the actual thing that's converting can.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Urging to an equilibrium.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's vanilla so far.",
                    "label": 0
                },
                {
                    "sent": "This was used in poker, so this is previous work.",
                    "label": 0
                },
                {
                    "sent": "Our sampling variants are actually quite tide to these, so what's good about CFR?",
                    "label": 0
                },
                {
                    "sent": "Well, the greatest thing I think is that counterfactual regret requires space linear in the number of information sets, that's.",
                    "label": 0
                },
                {
                    "sent": "I consider that a breakthrough because we didn't see that before now or before two years ago.",
                    "label": 0
                },
                {
                    "sent": "So we've solved games up to 10 to 12 states.",
                    "label": 1
                },
                {
                    "sent": "We've used them in poker bots and we've we've defeated human.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experts, so that's good.",
                    "label": 0
                },
                {
                    "sent": "What's not so good?",
                    "label": 0
                },
                {
                    "sent": "Well, this is mostly an offline algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's not really suited for online use, it actually took, I think up to two weeks to converge to the poker strategy that we use against the humans.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's even less than good in that or not even less than not so good, is that the iterations require a full tree traversal an for some games that's just not possible.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or doing.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Several of them is not possible.",
                    "label": 0
                },
                {
                    "sent": "So how do we?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overcome this pitfall.",
                    "label": 0
                },
                {
                    "sent": "Well we define well.",
                    "label": 0
                },
                {
                    "sent": "Basically we do sampling.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to find here is script Q and that that's equal to a set of cues and what these cues are.",
                    "label": 0
                },
                {
                    "sent": "These Q Jays.",
                    "label": 0
                },
                {
                    "sent": "These are just blocks, so this is like a subset of the tree, but these are blocks of terminal history, so this little purple triangle that's represented in this triangle here would be one of those blocks, and we're going to sample those blocks.",
                    "label": 0
                },
                {
                    "sent": "And we're going to say that the collection of all these cues spans Ed, so that we have a number of these blocks and the Union of all these blocks gives us all of the terminal histories in the entire.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we're going to find something called sampled counterfactual value, which may or may not look scary to you, but it's basically the exact same thing that we had before, but it uses this little trick that Gabor mentioned this morning.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I gave it away, but we're going to use the trick that were mentioned this morning where we have where this sampled counterfactual value actually just is an unbiased estimator for the real counter factual value.",
                    "label": 0
                },
                {
                    "sent": "So if we perform the analogous updates but on the sample part of the tree, we can show that we also converge to a Nash equilibrium.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's.",
                    "label": 0
                },
                {
                    "sent": "Here that is.",
                    "label": 0
                },
                {
                    "sent": "So the expected value of this sample counter factor value, which is really just divided.",
                    "label": 0
                },
                {
                    "sent": "You divide by the probability of.",
                    "label": 0
                },
                {
                    "sent": "Of sampling that one block.",
                    "label": 1
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "That's why you have the terminal history is in the block with the intersection of Zed survive.",
                    "label": 0
                },
                {
                    "sent": "Where is that?",
                    "label": 0
                },
                {
                    "sent": "So by is the terminal histories that went through this this information set.",
                    "label": 1
                },
                {
                    "sent": "So this will be 0 for everything that's outside of the block, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, so here's this little trick.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm mostly just going to skip this.",
                    "label": 0
                },
                {
                    "sent": "The important part really is that this is easy to show.",
                    "label": 0
                },
                {
                    "sent": "It's in the paper, but this is really the driving force of this of the entire sampling meth.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's important.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically I'm going to propose the MC or Monte Carlo CFR algorithm, which is basically just sampling block performing the Monte Carlo version of the updates, and we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to minimize this sampled immediate counterfactual regret rather than the immediate counterfactual regret we're going to use regret matching blackwells to show that we can update the strategies and the average profile will.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Converge to Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "OK few questions, what do we use as a sampling scheme?",
                    "label": 1
                },
                {
                    "sent": "So I haven't told you how we sample these queues.",
                    "label": 0
                },
                {
                    "sent": "I've just told you that we have a collection of them.",
                    "label": 0
                },
                {
                    "sent": "What are these blocks and how do we sample them?",
                    "label": 0
                },
                {
                    "sent": "Does it converge to epsilon Nash?",
                    "label": 1
                },
                {
                    "sent": "If that's true, does it converge for any sampling scheme?",
                    "label": 0
                },
                {
                    "sent": "An very important?",
                    "label": 0
                },
                {
                    "sent": "What's the rate of convergence and how well does it?",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Empirically, OK, here are some here some answers.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about two sampling schemes.",
                    "label": 1
                },
                {
                    "sent": "It does converge to epsilon Nash for what I'm going to call reasonable strategies, you can come up with reasonable sampling strategies.",
                    "label": 1
                },
                {
                    "sent": "You can definitely come up with some sampling strategies that it won't converge on, because the sampled counterfactual value would not make sense, or it would be able to find.",
                    "label": 0
                },
                {
                    "sent": "We're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to give you some regret bounds for some of the convergence world will show you the convergence rate in both theoretical case.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the empirical case.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Two sampling schemes will start off with one.",
                    "label": 0
                },
                {
                    "sent": "We call either outcome sampling or Singleton sampling, and the idea here is that these blocks that I told you about a second ago are just going to contain a single.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Terminal history.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to take the full game tree.",
                    "label": 0
                },
                {
                    "sent": "We're only going to sample one terminal history, so this is the most extreme form of sampling that you can think of.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What we're going to say?",
                    "label": 0
                },
                {
                    "sent": "How are we going to sample this?",
                    "label": 0
                },
                {
                    "sent": "There are several ways that you can sample these terminal histories.",
                    "label": 0
                },
                {
                    "sent": "You can give each terminal history equal probability uniformly.",
                    "label": 1
                },
                {
                    "sent": "We're not going to do that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're going to do is we're going to find a sampling strategy.",
                    "label": 0
                },
                {
                    "sent": "Sigma Prime here and we're going to do this epsilon greedy approach, where with probability epsilon we're going to sample our action randomly and with probability 1 minus epsilon.",
                    "label": 0
                },
                {
                    "sent": "We're going to sample our action according to the player's current strategy at time T. OK. Now, as long as we make sure that epsilon is not equal to zero, we have our definition of counterfactual value that still makes sense.",
                    "label": 0
                },
                {
                    "sent": "'cause if epsilon is equal to 0 at anytime, it's possible that you that you never sample certain blocks, and that's not good.",
                    "label": 0
                },
                {
                    "sent": "You want to make sure you cover your space with.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probability.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to find this new regret value are still here, which, which is essentially what we had before.",
                    "label": 0
                },
                {
                    "sent": "Except we're going to do the updates with our sampled kept counterfactual values instead.",
                    "label": 0
                },
                {
                    "sent": "So if you workout the details, this is what your regret equations turnover.",
                    "label": 0
                },
                {
                    "sent": "Yes, your regret updates turn out to be.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Notice this WI here.",
                    "label": 0
                },
                {
                    "sent": "This is the wait what we get on the bottom here is the probability of sampling that that particular terminal history and if we have this product, if we have this sampling profile, the way I've defined it, it's just the product of sampling each action at each node.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Here's where this talk relates to this workshop so.",
                    "label": 0
                },
                {
                    "sent": "Well, listen now if you haven't.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, 'cause this is mostly used as an offline method.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but this is interesting because what we can do is we can define this sampling profile at the opponents nodes.",
                    "label": 0
                },
                {
                    "sent": "We can define the sampling profile to be equal to their current strategy.",
                    "label": 0
                },
                {
                    "sent": "Now what that does?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it turns this so this you can separate between your contribution and the opponent's contribution.",
                    "label": 0
                },
                {
                    "sent": "If if the sampling strategy is equal to the opponent's current strategy at that iteration, this term and that term are equal, so they cancel out.",
                    "label": 0
                },
                {
                    "sent": "That might not see.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seem significant, but what's actually good about that is that now your update equation does not rely on the opponent ever.",
                    "label": 0
                },
                {
                    "sent": "You don't need to know what the opponents.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's very good, because what you can do with that.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is you can do general regret minimization.",
                    "label": 1
                },
                {
                    "sent": "So sort of like what was done with Exp.",
                    "label": 1
                },
                {
                    "sent": "Three in normal form games where you're playing against an unknown opponent you have.",
                    "label": 1
                },
                {
                    "sent": "You don't know anything about his strategy.",
                    "label": 0
                },
                {
                    "sent": "You could even be changing his strategy overtime and what you're doing is we can prove that these updates actually converge or they minimize overall regret, which is good.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, what you can do is if you know the planes T. So if you know how many times you're going to play against this.",
                    "label": 0
                },
                {
                    "sent": "Unknown opponent, you can actually find the optimal exploration epsilon for that given T. So this is future work where.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're hoping to do something cool there, OK?",
                    "label": 0
                },
                {
                    "sent": "Second sampling scheme is called external sampling, so this one actually works better.",
                    "label": 1
                },
                {
                    "sent": "We can show that the regret bound is actually tighter for this one.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Though when I explain it to you, it might seem like it will do worse.",
                    "label": 0
                },
                {
                    "sent": "So how are we going to sample?",
                    "label": 0
                },
                {
                    "sent": "So I told you about an extreme case where we sample where a block is just contains one single terminal history now.",
                    "label": 0
                },
                {
                    "sent": "In external sampling, what we're going to do, we're only going to sample chance and the opponents actions, so we're going to do this tree traversal over over the subtree that's defined by a particular pure strategy that's used by the opponent.",
                    "label": 1
                },
                {
                    "sent": "So a pure strategy is just the deterministic mapping from information sets, the actions, and the way that we're going to sample that deterministic strategy is by using the opponent's current strategy so the opponent's current strategy at some in some iteration.",
                    "label": 0
                },
                {
                    "sent": "Is going to be some mixed distribution.",
                    "label": 0
                },
                {
                    "sent": "We're going to use that to sample a deterministic strategy, and whenever we encounter on an opponent node through our traversals, we're just going to choose the action that was sampled.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're adding some kind of bias here.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the block probabilities results.",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                },
                {
                    "sent": "This is the term that we divide by in our counterfactual value definition.",
                    "label": 0
                },
                {
                    "sent": "The regret update equations actually become simpler because terms cancel out.",
                    "label": 0
                },
                {
                    "sent": "So if we if we apply these regret equations and are sampled nodes, we can show that this.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Converges.",
                    "label": 0
                },
                {
                    "sent": "OK theoretical result.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is we've actually tightened the bound.",
                    "label": 0
                },
                {
                    "sent": "Of the vanilla, see if our algorithm, so the original counterfactual regret algorithm, by defining this term M and this term M What we had before was that the overall regret was less than a bunch of terms.",
                    "label": 0
                },
                {
                    "sent": "But the number of information sets are the the number of information sets was in that term.",
                    "label": 0
                },
                {
                    "sent": "Now we have a better bound.",
                    "label": 0
                },
                {
                    "sent": "We can show that it's bounded by this value M, which we can show.",
                    "label": 0
                },
                {
                    "sent": "It's always either between the root of the number of information sets or the number of information sets.",
                    "label": 0
                },
                {
                    "sent": "And actually for most balanced games, it's going to be the root of the number of information sets, so that's good.",
                    "label": 0
                },
                {
                    "sent": "Of course, in practice we notice that it converges a lot faster than this bound, so in practice that didn't really matter.",
                    "label": 0
                },
                {
                    "sent": "But we have a good theoretical bound.",
                    "label": 0
                },
                {
                    "sent": "Now we've showed a probability connection between sampling case and the non sampling case, and that's here if you notice the difference between these equations.",
                    "label": 0
                },
                {
                    "sent": "So the second sampling scheme I talked about the external sampling scheme we're only a constant factor off where that constant depends on the probability that you want the bound to hold.",
                    "label": 0
                },
                {
                    "sent": "So with bound one with probability 1 -- P. We have that this is true.",
                    "label": 1
                },
                {
                    "sent": "So P is a root in the denominators, so that can still get big butt.",
                    "label": 0
                },
                {
                    "sent": "At least it doesn't blow up some function in the actual game structure.",
                    "label": 0
                },
                {
                    "sent": "Now for outcome sampling we have this extra little Delta.",
                    "label": 0
                },
                {
                    "sent": "Here Delta was the the smallest probability of sampling a terminal history, which can be very small, so this will be large.",
                    "label": 0
                },
                {
                    "sent": "So really, this is a sanity check.",
                    "label": 0
                },
                {
                    "sent": "It does converge, but again, we're off from external sampling by some factor, which is one over the smallest probability of reaching of sampling a particular terminal history.",
                    "label": 0
                },
                {
                    "sent": "So it does converge.",
                    "label": 0
                },
                {
                    "sent": "So the question is really how does this work in practice?",
                    "label": 0
                },
                {
                    "sent": "Because if you look at these regret bounds, you might not be convinced.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the answer is, it works really well in practice and the main idea of the main reason it works well in practice is because these iterations over these sampled blocks take a lot less long than the full iterations of counterfactual regret, because they had they have to go through the entire tree and the point is you're doing these regret updates on these as you go by it.",
                    "label": 0
                },
                {
                    "sent": "So if you can do 10,000 iterations of the sample based counterfactual regret, you're gaining information over those.",
                    "label": 0
                },
                {
                    "sent": "10,000 iterations where you can only do one counterfactual gret iteration.",
                    "label": 0
                },
                {
                    "sent": "In that time, you're not getting as much information, or.",
                    "label": 0
                },
                {
                    "sent": "The success of information successive iterations can take advantage of all the regret updates that you've done in the previous passes.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "It turns out that there's a obvious optimization that you can apply in the counterfactual regret case.",
                    "label": 0
                },
                {
                    "sent": "So think of this red line as being the old stuff.",
                    "label": 0
                },
                {
                    "sent": "This we ran this on one card poker, which is a smallish game.",
                    "label": 0
                },
                {
                    "sent": "We ran it on three other games and the convergence is much faster.",
                    "label": 0
                },
                {
                    "sent": "We can we can get to a convergence point way before counterfactual great in most cases by using sampling.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, larger games.",
                    "label": 0
                },
                {
                    "sent": "This is a larger game.",
                    "label": 0
                },
                {
                    "sent": "Clayton tic tac toe.",
                    "label": 0
                },
                {
                    "sent": "Here we don't see as good results for the outcome sampling case, but we do for the external sampling.",
                    "label": 0
                },
                {
                    "sent": "Again, the first iteration of counterfactual regret takes a lot a long time, but because we have this pruning optimization afterwards, successive iterations get go faster on the Y axis, sorry.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Y axis here we have exploitability which is essentially distance to the Nash equilibrium, so we actually want to be closer to to zero and nodes touched is essentially the accumulative number of nodes.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That we've visited.",
                    "label": 0
                },
                {
                    "sent": "OK, so we see similar things for other games that we tried on like Princess and Monster which is pursuit evasion game an Gooch feel so.",
                    "label": 1
                },
                {
                    "sent": "And in which field we actually see that outcome sampling does better than external sample, so that's good.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bridgeville is an interesting card game, but we especially where you have a bunch of cards.",
                    "label": 0
                },
                {
                    "sent": "Let's say an.",
                    "label": 0
                },
                {
                    "sent": "And you have a deck of point cards on N from one to N. And what you're doing is you're bidding.",
                    "label": 0
                },
                {
                    "sent": "You only see one of the point cards at the top, and you're bidding on that point guard.",
                    "label": 0
                },
                {
                    "sent": "And what you do is you.",
                    "label": 0
                },
                {
                    "sent": "It's a simultaneous move game where you put down your card to bit, and if you get if your card is higher than the opponents, you get the point card and then you just add up all the point cards and whoever has the most points wins.",
                    "label": 0
                },
                {
                    "sent": "We have a less informational version where you don't find out which bid cards were being used throughout the game.",
                    "label": 0
                },
                {
                    "sent": "So one thing I didn't mention about all these four games that we chose.",
                    "label": 0
                },
                {
                    "sent": "The imperfect information comes from different sources.",
                    "label": 0
                },
                {
                    "sent": "So in one card poker, the imperfect information is all due to chance in Layton Tic Tac toe.",
                    "label": 0
                },
                {
                    "sent": "It's due to the opponents moves.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, studio opponents moves in Princess and Monster.",
                    "label": 0
                },
                {
                    "sent": "It's a combination of chance and the opponents moves and push people.",
                    "label": 0
                },
                {
                    "sent": "I think it's just the opponents moves because we actually choose a fixed deck.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so what's the bottom line?",
                    "label": 0
                },
                {
                    "sent": "The bottom line is that because we take less time per iteration than we can, we can leverage this.",
                    "label": 1
                },
                {
                    "sent": "The fact that the updates give us information and we're doing a lot more iterations so we can reuse that information to converge faster than than the vanilla CFR case and future work.",
                    "label": 1
                },
                {
                    "sent": "We want to try and use this for general regret minimization.",
                    "label": 0
                },
                {
                    "sent": "We just haven't yet.",
                    "label": 0
                },
                {
                    "sent": "We want to look at this epsilon greedy profile sampling strategy that we that we defined a little more closely, 'cause we just in our experiments.",
                    "label": 0
                },
                {
                    "sent": "We just chose the best value of epsilon that worked.",
                    "label": 0
                },
                {
                    "sent": "We worked that worked well experimentally, so we don't know much more about that, and one very interesting future work is.",
                    "label": 0
                },
                {
                    "sent": "Is this abstraction over through imperfect recall?",
                    "label": 0
                },
                {
                    "sent": "Case where?",
                    "label": 1
                },
                {
                    "sent": "All the information sets at the current time, like right now information sets are defined by their.",
                    "label": 0
                },
                {
                    "sent": "The sequence that players took to get to that point in the game.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that often those sequences you don't need everything in those sequences.",
                    "label": 0
                },
                {
                    "sent": "In fact, the Markov assumption is actually an extreme.",
                    "label": 0
                },
                {
                    "sent": "Extreme example of that where you only care about the last last state.",
                    "label": 0
                },
                {
                    "sent": "So we want to try and extend that to the.",
                    "label": 0
                },
                {
                    "sent": "We can cut down the number of the memory requirements that we need for our algorithms by by looking at this and we have exciting pericle results on that so far.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that it becomes an imperfect recall game and we have no theory about it.",
                    "label": 0
                },
                {
                    "sent": "We don't know if it will converge in the imperfect recall, we know very little about imperfect recall games, so that's it, thanks.",
                    "label": 0
                },
                {
                    "sent": "Can you give me some information about the why you use this counterfactual?",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "What do you need to yeah?",
                    "label": 0
                },
                {
                    "sent": "This because if OK, so the idea is that.",
                    "label": 0
                },
                {
                    "sent": "You're never if you don't get to that information set because the opponent doesn't play to get there.",
                    "label": 0
                },
                {
                    "sent": "The intuition is that you don't really care about it.",
                    "label": 0
                },
                {
                    "sent": "It if your opponent doesn't play to get there, why would you care about the equilibrium?",
                    "label": 0
                },
                {
                    "sent": "That's below that.",
                    "label": 0
                },
                {
                    "sent": "Of course, the definition of a Nash equilibrium includes what you would do, but there at that point but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we should say that it's zero if you don't get there because we don't really need to know what goes on below.",
                    "label": 0
                },
                {
                    "sent": "And you starting from the initial profile that maybe.",
                    "label": 0
                },
                {
                    "sent": "Far away from the truth, yes, for sure.",
                    "label": 0
                },
                {
                    "sent": "What's the purpose of?",
                    "label": 0
                },
                {
                    "sent": "I mean this argument because at the beginning maybe OK, the quality is not the right one, but maybe the Nash actually put some property or in those.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You don't know, so you should explore everything so you always end up exploring anyway, because because what's actually converging to the Nash is the average profile, so the strategies that each at each iteration.",
                    "label": 0
                },
                {
                    "sent": "They're not those strategies that those are not the ones that are converging.",
                    "label": 0
                },
                {
                    "sent": "Converging to Nash, it's the average like sort of what happens in fictional play, so.",
                    "label": 0
                },
                {
                    "sent": "So my point is, if?",
                    "label": 0
                },
                {
                    "sent": "If so.",
                    "label": 0
                },
                {
                    "sent": "So what you're doing is you're waiting it by the opponents probability at that particular time step, which might be meaningful at that time step, so it's not so much that that's going to be 0 eventually.",
                    "label": 0
                },
                {
                    "sent": "What you could be doing is you could be flipping between two strategies, like, like you would see in a fictitious play.",
                    "label": 0
                },
                {
                    "sent": "So I don't know if I'm answering your question properly.",
                    "label": 0
                },
                {
                    "sent": "Using another different point in so I don't know actually, but I think you might get a regret bound if you don't use counter factual utility.",
                    "label": 0
                },
                {
                    "sent": "I just don't know if it works as well if they are as tight.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think like the mostly the intuition is that you're not going to reach there at that particular time, so.",
                    "label": 0
                },
                {
                    "sent": "You're not going to have any regret by playing.",
                    "label": 0
                },
                {
                    "sent": "You're not going to have a regret for that action that is at an information set under there, because you're not actually going to get there.",
                    "label": 0
                },
                {
                    "sent": "Does that make sense?",
                    "label": 0
                },
                {
                    "sent": "So the counter.",
                    "label": 0
                },
                {
                    "sent": "So the idea behind the counter factual utility is to is to tell you the difference between at a particular information that the difference between playing action versus not playing in action.",
                    "label": 0
                },
                {
                    "sent": "So for any given set of strategies.",
                    "label": 0
                },
                {
                    "sent": "If there are some information sets that you would never get to.",
                    "label": 0
                },
                {
                    "sent": "Then why you then?",
                    "label": 0
                },
                {
                    "sent": "Then you don't have.",
                    "label": 0
                },
                {
                    "sent": "Your regret is not really defined at those.",
                    "label": 0
                },
                {
                    "sent": "It depends on the number of information, so it doesn't depend on the polity to reach.",
                    "label": 0
                },
                {
                    "sent": "Reaches together plays together.",
                    "label": 0
                },
                {
                    "sent": "So it's like you're using a very fancy.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Use this.",
                    "label": 0
                },
                {
                    "sent": "It's like it seems that if you are using a more natural definition of the re read each permission set, you will have the same regret.",
                    "label": 0
                },
                {
                    "sent": "It's possible, yeah, I don't other than the intuition.",
                    "label": 0
                },
                {
                    "sent": "I don't know how the counterfactual comes into the actual regret bounds, but I think it might be apart of regret matching 'cause regret matching is how we update our strategies at each information shot.",
                    "label": 0
                },
                {
                    "sent": "It might actually be, well, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think you could you might.",
                    "label": 0
                },
                {
                    "sent": "You might be able to do a regret matching without the counterfactual utility.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can for sure, but I don't know how they come into the bounds.",
                    "label": 0
                },
                {
                    "sent": "I just yeah, I just know that that's basically the intuition.",
                    "label": 0
                },
                {
                    "sent": "So I guess if.",
                    "label": 0
                },
                {
                    "sent": "If you if you can imagine a sequence where you never reach a certain information that ever again.",
                    "label": 0
                },
                {
                    "sent": "It's possible that we like the opponent, never placed the reach there.",
                    "label": 0
                },
                {
                    "sent": "It's possible that we've decided already that we don't ever want to go there again, like we are.",
                    "label": 0
                },
                {
                    "sent": "Average profile might already be at an equilibrium in that information set.",
                    "label": 0
                },
                {
                    "sent": "So from then on, we're never going to get there.",
                    "label": 0
                },
                {
                    "sent": "You can think of dominated action.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have a dominated action at the first Information Center first.",
                    "label": 0
                },
                {
                    "sent": "Note very obviously dominated action, it will get removed right away.",
                    "label": 0
                },
                {
                    "sent": "Their player will never play to reach that any information sets under there because it's dominated.",
                    "label": 0
                },
                {
                    "sent": "So you definitely so in every Nash equilibrium will have probability zero of taking an action that gets to any information sets below connection.",
                    "label": 0
                },
                {
                    "sent": "So made that I guess that's still part of the intuition I I can't.",
                    "label": 0
                },
                {
                    "sent": "I can't tell you how it enters the regret bounds, but that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Speak again.",
                    "label": 0
                }
            ]
        }
    }
}