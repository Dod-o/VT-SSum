{
    "id": "irqvh2zx4ady3a6cwewxgu6am5k3g5ck",
    "title": "Learning Hierarchical Multi-Category Text Classification Models",
    "info": {
        "author": [
            "Juho Rousu, Department of Computer Science, University of Helsinki"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2005",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/icml05_rousu_lhmct/",
    "segmentation": [
        [
            "It seems we still have a lot of interest.",
            "This morning session is text classification and information extraction.",
            "Our first speaker we have.",
            "U-Haul Russell.",
            "Who is formally in Southampton?",
            "And I believe this work might have been done in Southampton and now he's at University of Helsinki.",
            "Learning hierarchical multi category text classification models thinking.",
            "OK, thank you Ray.",
            "So.",
            "Do something.",
            "Right?",
            "OK, so the top."
        ],
        [
            "Of my talk is how do I go multi label classification?",
            "We are talking about the Union partial parts model specifically, so in this model we have document X.",
            "Some hierarchy.",
            "Here T and the goal is to predict a multi label Y that is a vector.",
            "And this vector, the positive model micro labels whicita components of the of the vector conform to our union of partial parts like like this here.",
            "So we have some internal nodes or leaf knows it could be as well.",
            "That are part of the classification and then the path towards the root is also part of the classification.",
            "OK.",
            "So this is the.",
            "This is the model number."
        ],
        [
            "So how?",
            "Are people usually going about learning this kind of hierarchies?",
            "I guess that.",
            "Most simple ways is just flatten the whole hierarchy and try to learn it's it's micro micro label.",
            "Each component of the component of the hierarchy separately.",
            "So test do it, you also classification learning in each node.",
            "So that's obviously is relatively inexpensive computationally, because you have decomposed attriting, but this approach obvious doesn't make any use of the of the hierarchy that it's completely kind of.",
            "Ignores every every any dependencies which bitter bitter bitter.",
            "Nodes in the hierarchy.",
            "OK, a little bit more clever is to is to do hierarchical training where you train it snowed after hierarchy.",
            "With examples that belong to the parent of the node.",
            "So then you actually can take advantage of some of the dependencies.",
            "But the problem is.",
            "Since you're kind of always routing some of the exam training examples away from your your subtree, you're going into the data fragments and estimation and leaves.",
            "Becomes less less reliable, reliable.",
            "And also the model is not or explicitly train in terms of loss function.",
            "At least it's very hard to say what is the loss function that's that that model is being trained on, so is to improve on this approach this.",
            "OK, the approach we have taken is this much Martin structured output approach that's recently introduced by."
        ],
        [
            "Tuskar and others.",
            "So the.",
            "Goal in this structured output Max marching approach is to separate the correct multi label from the incorrect ones by a large margin.",
            "And also we would like to target margin scale proposed any of the loss of the multi label.",
            "So.",
            "For example, here, if this is the true multi label that we are.",
            "Trying to learn.",
            "Any multi label where we have one mistake should kind of be one margin away from the from the best one and any any labeling that has two errors should be two more margins away from the from the best one.",
            "So we are sort of trying to learn some kind of ranking for the labelings.",
            "OK.",
            "So before going on, I think it's useful to think about loss function for hierarchies that we would like."
        ],
        [
            "Like to use.",
            "So there are.",
            "So if you have this kind of.",
            "Vector.",
            "Through vector on the predicted one, so.",
            "There are a couple of obvious choices.",
            "For the first one is 01 loss.",
            "You just compare whether the vectors are the same.",
            "So if there is any any error in the vector, you will pay.",
            "A loss of 1.",
            "Otherwise it's 0.",
            "So that's.",
            "The problem with that, of course, is that you don't get any separation between between.",
            "That kind of different.",
            "Kind of mistakes how many mistakes you make.",
            "Everything is treated alike.",
            "So that's not very, very good.",
            "For structure tasks, so symmetric, different laws.",
            "It's kind of the other extreme.",
            "You count each micro label separately, so whether you take every node of the hierarchy and see whether there was an error in there or not.",
            "And some of these errors are.",
            "OK, that's that's much better for structured thoughts because you get some kind of creating.",
            "If you'd make a lot of errors in the tree, you will pay a lot.",
            "However, that doesn't take into account the hierarchy in any way.",
            "It could be any any bag of.",
            "Lack of labels that we are predicting.",
            "Here I have two loss functions that actually take the hierarchy into account.",
            "First one is Hirculus suggested by Kesepian Keiynan, others last year, so idea with this loss is that you look.",
            "Along its path of the hierarchy, where is the first mistake being made and you pay?",
            "Where pay for this first mistake?",
            "Not not any any other mistakes on the path.",
            "So that's seems kind of sensible thing to do, 'cause if you make a mistake in the parent, it's kind of unfair that require getting this child corner correct anymore.",
            "OK, in this paper we are using a simplified simplified version of this hierarchical loss.",
            "That's decomposes by the edges, so we only only look don't we don't look at the whole path.",
            "We only look what happened in the parents.",
            "So if we.",
            "Made a mistake in a child node.",
            "We only pay if the parent was correct.",
            "So if the child is is kind of.",
            "OK, making a mistake.",
            "Although the parent was correct so little bit simplified because it doesn't take take.",
            "Into account the whole path so it's not as.",
            "Maybe as pleasing as this this Oracle loss, but it's more tractable computational and that's why we're using it.",
            "OK, that that must further loss functions."
        ],
        [
            "So here are the optimization problems that arise from from this kind of much margin approach for this this this and if you have what's the thought by Torsten Joachims on Monday morning or yesterday if you were in the yesterday's structured output session?",
            "You have seen this this form.",
            "Couple of times already.",
            "So it's actually looks.",
            "Quite a lot for like this short marching SVM.",
            "But we have much, much more complicated constraint.",
            "In fact, we have exponential number of constraint in this problem and that actually makes this problem intractable in this form.",
            "So he has to do all.",
            "So we have.",
            "Term with the loss function.",
            "Is a linear term in quadratic term with the kernel.",
            "Colonel Beef winter between them.",
            "Feature vectors.",
            "And.",
            "This kind of soft margin type box constraint.",
            "OK, so this kind of be solved in this form.",
            "It's way too late, too heavy, heavy to compute.",
            "And people have been suggesting different different models how to how to tackle this problem?",
            "We took the approach by interest by tuskar.",
            "Up light into this.",
            "It do this hierarchical case so.",
            "The scar in his nips paper not marking not marginalized structure, so that instead of talking about this, this.",
            "Dual variables for the whole vector.",
            "To develop variables are now.",
            "Per edge, so we have one.",
            "Dual variable for.",
            "Their edge.",
            "And labeling of the of the ads.",
            "And."
        ],
        [
            "These are obtained as marginals of this.",
            "This originally variables So what we do here is we fix.",
            "We fixed an edge and a labeling for it for it, so it's kind of have four possibilities.",
            "And we compute the sum over each labeling vector that conforms to this edge labeling.",
            "So that's the marginal margin out of there.",
            "Of the dual variable.",
            "So we know the order to use this.",
            "We need to decompose the loss.",
            "And a kernel according to the edges as well.",
            "So we need to have this kind of form for the loss function, so it's a sum over the edges.",
            "So two of the loss functions I showed you earlier form this form.",
            "This is the symmetric difference loss and this simplified hierarchical loss, and we are going to be using them.",
            "Also, the kernel needs to be decomposed by the edges, so we need to have this kind of representation.",
            "So this requires, while it is actually, this is not.",
            "I'm.",
            "OK, maybe I'm a little bit incorrect airport, but the former might not be exactly this, but the idea is is this need to be decomposed so it requires that the feature vector has blocks for edges.",
            "So we do it by.",
            "In this block, we repeat.",
            "A feature vector for the document.",
            "We put it.",
            "We have a.",
            "For each edge four blocks, and we put this feature vector of the document that correct correct block.",
            "So if for example.",
            "The document belongs to the parent and the child.",
            "It will defeat Survector will be put in this block corresponding to that labeling 1 + 1 + 1.",
            "And otherwise will be at 0, so that's here.",
            "Is the indicator kind of picking up which labeling.",
            "Is the correct one?",
            "Otherwise, it's it's it's 0.",
            "OK, so these are the ingredients too.",
            "Decompose."
        ],
        [
            "To problem out of it so, so here's the objective.",
            "It test comes when you plug in.",
            "Plug in this.",
            "Marginal do our valuables in there so you see it's a sum over the edges so it's case linearly in the edges.",
            "No love my exponent exponential size.",
            "Then there's the same box strong constraint as we had earlier, and then something else.",
            "So this is consistency constraints that ensure that.",
            "Our marginal deliverables correspond to the originally running so, so this ensures that we actually are still solving.",
            "Extracting the same problem.",
            "OK, so at this point this is polynomial size and it's actually linear in the edges.",
            "It's way too large to solve it with any of the self QP methods.",
            "Really big big problems.",
            "I mean, it's not straightforward to decompose because you have.",
            "To training examples depending on each other in the kernel obviously and then you have these constraints that tie your edges together, so you cannot really just decompose it by examples or by edges.",
            "The good news is since since we are doing hierarchical classification, prediction or computing this arc marks.",
            "This efficient we can use dynamic programming for that, so that's not the problem.",
            "OK, so in order to tackle this this big problem.",
            "So just use conditional gradient descent.",
            "That is."
        ],
        [
            "Basically, iterative gradient search within the feasible set so the feasible set is now that.",
            "Actually, the marginal polytope of the of the marginalized dual so.",
            "And the update direction in this gradient search is the highest feasible point.",
            "After making making a linear approximation of the quadratic surface so.",
            "We look at the current current gradient and assume that that is the correct one to the kind of Infinity.",
            "So, and we find some vertex of the feasible set, that's the direction and we step towards it somewhat.",
            "Good thing about this scheme is that we actually can decompose the training to single example subspaces.",
            "In that sense that if we update one example only, we don't need to consult, consult what happens with shadow examples once we do some initialization to start the search, so.",
            "So there is some kind of overhead from moving from one example to another to another, But that's not not very, very bad.",
            "OK, so this is the test illustration."
        ],
        [
            "The algorithm, but it does so.",
            "Here we have to.",
            "Quadratic surface here is the feasible set.",
            "So first we look for the gradient.",
            "Make linear approximation of the quartic surface so and find the best point.",
            "So assuming this gradient is the correct one through Infinity, this is our best possible point.",
            "So that's the direction."
        ],
        [
            "Then we go and compute the saddle point along this line.",
            "And that's easy because we have a 1 dimensional line that we are thinking about.",
            "Um?",
            "Step to step saddle point and repeat the whole thing."
        ],
        [
            "This goes on until we're we are happy too."
        ],
        [
            "Either move to another example or or."
        ],
        [
            "Are are finished with optimization, so that's quite a concept.",
            "Really quite simple.",
            "Simple algorithm.",
            "OK, so here are some experi."
        ],
        [
            "Hence we have used it on two datasets, Reuters, MCV one.",
            "This secret family.",
            "That's 34 micro labels, the hierarchy.",
            "In Depth 3 three.",
            "The fields are represented when the document was back of words with TF IDF weighting.",
            "We have to add 2005 two 1500 lumens for training 5000 for testing.",
            "Viper Alfred Peyton data set that's a little bit more.",
            "Michael labels 188 little bit deeper tree.",
            "So here are the statistiques.",
            "There are five.",
            "Algorithms are algorithms that we call hierarchical maximum margin Markov kind of following task course naming convention.",
            "Comparison we have flat SVM.",
            "That's Predix eats.",
            "Note independently Oracle trainers wear.",
            "OK, that's trained with the subset of the of the training it I need it snowed and then new album by case up.",
            "Yankee he hierarchical rasley squares.",
            "Implementation was in Matlab.",
            "And yeah, so."
        ],
        [
            "That's it, OK, so here are the results.",
            "So prediction quality, so here it is, 001 loss.",
            "You can see that so flat SVM has the highest.",
            "01 loss then order other hierarchical models.",
            "And then there are this, our two methods with two different loss functions, with the symmetric difference loss and this hierarchical.",
            "Simplified hierarchical loss.",
            "So here we get.",
            "Little bit better racials than the other ones.",
            "Then there are positional recall and F1 so.",
            "As you can see, flatter SVM is is the most precise when it predicts positive, it usually.",
            "Guess it gets it correct.",
            "But the recall is low, so it doesn't predict positive very often.",
            "Ann, this our methods are in the other extreme, where we get really good recall or quite good recall, but the precision is not quite as high.",
            "And overall, you can see that F1 score.",
            "All these hierarchical methods do well.",
            "And the flat SVM is suffering.",
            "OK, so so then we did another another test to see whether the hierarchical method."
        ],
        [
            "Differ differ at all.",
            "They compute this F1 scores level power level.",
            "Here is.",
            "Let me sort of wiper set we have.",
            "The F1 score for the RT second level third Level 4 level so you can see that.",
            "This structured output methods they start to work really well in the deep nodes.",
            "And that's that's becausw.",
            "They have good recall.",
            "They actually go and predict positive when other the other medals stop predicting positive.",
            "You can see the flat SVM is the most hopeless in and test gives up produce everything negative in interleaves or model and not everything but kind of starts would be very negative and the other he recalled.",
            "Medals are little bit more negative than than the grand.",
            "Structured output methods.",
            "OK.",
            "So that's that's basically the picture.",
            "OK."
        ],
        [
            "Something about the optimization.",
            "Unfortunately, the algorithm tests audio is not the fastest one for the for the.",
            "For this optimized this model.",
            "This is the.",
            "This is the curve of that method in on this wiper data set, so it's OK, but it's not.",
            "It's not very far that it takes a couple of hours to optimize.",
            "This foster one is an algorithm we came up with after after this submission was was done so.",
            "The only change in algorithm is how this update direction is this computer we used.",
            "In this paper we used linear programming to find this direction in this feasible set.",
            "And here we actually utilize the tree and dynamic programming over the tree to find the same direction.",
            "OK, I don't have time to go into detail details but but it really seems to work really well, but it's basically the same algorithm scheme, but kind of the very innermost step has been has been replaced with something way more efficient.",
            "OK so I'm."
        ],
        [
            "Need to make some conclusions so we present that Colonel Pass up rose for hierarchical test text classification when documents can belong to more than one category at the time.",
            "So.",
            "Since we can utilize this dependence structure of Mac micro labels, we get a little bit better prediction accuracy when the hierarchy is deep.",
            "Or actually it seems to be substantial.",
            "Went it went.",
            "Hierarchy is the.",
            "OK, so the optimization.",
            "This means metaphysical by decomposing duration problem and make making incremental conditional gradient search in this sub problem.",
            "And result is we have tractable optimization from medium sized datasets.",
            "So we and we're not quite there to claim it's a large scale.",
            "The optimal implementation is still in math lab and we don't really have a large scale implementation, so I'm interested to see in the future how far we can scale this up, But so this is the status at the moment.",
            "OK, so thank you for your attention.",
            "Any questions?",
            "When will windows?",
            "Many cases there are section 200 size alright.",
            "Really scary one is the big hierarchies in the published, which could be 1000.",
            "They can be lovely.",
            "OK, so can you pieces together, can you say?",
            "Uses for parts of the problem that could be composed still operate right.",
            "OK, so the question was can we kind of tackle this big, really big hierarchies when you have thousands of nodes nodes in the hierarchy, and.",
            "The good thing about this algorithm is that it scales linearly in the size of the hierarchy, so increasing the size of the hierarchy doesn't necessarily kill you.",
            "So.",
            "So linear scaling, kind of.",
            "It's kind of promises that it should be OK, so.",
            "Phillips print this picture."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It seems we still have a lot of interest.",
                    "label": 0
                },
                {
                    "sent": "This morning session is text classification and information extraction.",
                    "label": 0
                },
                {
                    "sent": "Our first speaker we have.",
                    "label": 0
                },
                {
                    "sent": "U-Haul Russell.",
                    "label": 0
                },
                {
                    "sent": "Who is formally in Southampton?",
                    "label": 0
                },
                {
                    "sent": "And I believe this work might have been done in Southampton and now he's at University of Helsinki.",
                    "label": 0
                },
                {
                    "sent": "Learning hierarchical multi category text classification models thinking.",
                    "label": 1
                },
                {
                    "sent": "OK, thank you Ray.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Do something.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, so the top.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of my talk is how do I go multi label classification?",
                    "label": 0
                },
                {
                    "sent": "We are talking about the Union partial parts model specifically, so in this model we have document X.",
                    "label": 1
                },
                {
                    "sent": "Some hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Here T and the goal is to predict a multi label Y that is a vector.",
                    "label": 0
                },
                {
                    "sent": "And this vector, the positive model micro labels whicita components of the of the vector conform to our union of partial parts like like this here.",
                    "label": 1
                },
                {
                    "sent": "So we have some internal nodes or leaf knows it could be as well.",
                    "label": 0
                },
                {
                    "sent": "That are part of the classification and then the path towards the root is also part of the classification.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the model number.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how?",
                    "label": 0
                },
                {
                    "sent": "Are people usually going about learning this kind of hierarchies?",
                    "label": 0
                },
                {
                    "sent": "I guess that.",
                    "label": 0
                },
                {
                    "sent": "Most simple ways is just flatten the whole hierarchy and try to learn it's it's micro micro label.",
                    "label": 0
                },
                {
                    "sent": "Each component of the component of the hierarchy separately.",
                    "label": 0
                },
                {
                    "sent": "So test do it, you also classification learning in each node.",
                    "label": 0
                },
                {
                    "sent": "So that's obviously is relatively inexpensive computationally, because you have decomposed attriting, but this approach obvious doesn't make any use of the of the hierarchy that it's completely kind of.",
                    "label": 0
                },
                {
                    "sent": "Ignores every every any dependencies which bitter bitter bitter.",
                    "label": 0
                },
                {
                    "sent": "Nodes in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "OK, a little bit more clever is to is to do hierarchical training where you train it snowed after hierarchy.",
                    "label": 0
                },
                {
                    "sent": "With examples that belong to the parent of the node.",
                    "label": 1
                },
                {
                    "sent": "So then you actually can take advantage of some of the dependencies.",
                    "label": 0
                },
                {
                    "sent": "But the problem is.",
                    "label": 1
                },
                {
                    "sent": "Since you're kind of always routing some of the exam training examples away from your your subtree, you're going into the data fragments and estimation and leaves.",
                    "label": 0
                },
                {
                    "sent": "Becomes less less reliable, reliable.",
                    "label": 1
                },
                {
                    "sent": "And also the model is not or explicitly train in terms of loss function.",
                    "label": 0
                },
                {
                    "sent": "At least it's very hard to say what is the loss function that's that that model is being trained on, so is to improve on this approach this.",
                    "label": 0
                },
                {
                    "sent": "OK, the approach we have taken is this much Martin structured output approach that's recently introduced by.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tuskar and others.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Goal in this structured output Max marching approach is to separate the correct multi label from the incorrect ones by a large margin.",
                    "label": 1
                },
                {
                    "sent": "And also we would like to target margin scale proposed any of the loss of the multi label.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For example, here, if this is the true multi label that we are.",
                    "label": 0
                },
                {
                    "sent": "Trying to learn.",
                    "label": 0
                },
                {
                    "sent": "Any multi label where we have one mistake should kind of be one margin away from the from the best one and any any labeling that has two errors should be two more margins away from the from the best one.",
                    "label": 0
                },
                {
                    "sent": "So we are sort of trying to learn some kind of ranking for the labelings.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So before going on, I think it's useful to think about loss function for hierarchies that we would like.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like to use.",
                    "label": 0
                },
                {
                    "sent": "So there are.",
                    "label": 0
                },
                {
                    "sent": "So if you have this kind of.",
                    "label": 0
                },
                {
                    "sent": "Vector.",
                    "label": 0
                },
                {
                    "sent": "Through vector on the predicted one, so.",
                    "label": 1
                },
                {
                    "sent": "There are a couple of obvious choices.",
                    "label": 0
                },
                {
                    "sent": "For the first one is 01 loss.",
                    "label": 0
                },
                {
                    "sent": "You just compare whether the vectors are the same.",
                    "label": 0
                },
                {
                    "sent": "So if there is any any error in the vector, you will pay.",
                    "label": 0
                },
                {
                    "sent": "A loss of 1.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's 0.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "The problem with that, of course, is that you don't get any separation between between.",
                    "label": 0
                },
                {
                    "sent": "That kind of different.",
                    "label": 0
                },
                {
                    "sent": "Kind of mistakes how many mistakes you make.",
                    "label": 0
                },
                {
                    "sent": "Everything is treated alike.",
                    "label": 0
                },
                {
                    "sent": "So that's not very, very good.",
                    "label": 0
                },
                {
                    "sent": "For structure tasks, so symmetric, different laws.",
                    "label": 0
                },
                {
                    "sent": "It's kind of the other extreme.",
                    "label": 0
                },
                {
                    "sent": "You count each micro label separately, so whether you take every node of the hierarchy and see whether there was an error in there or not.",
                    "label": 0
                },
                {
                    "sent": "And some of these errors are.",
                    "label": 0
                },
                {
                    "sent": "OK, that's that's much better for structured thoughts because you get some kind of creating.",
                    "label": 0
                },
                {
                    "sent": "If you'd make a lot of errors in the tree, you will pay a lot.",
                    "label": 0
                },
                {
                    "sent": "However, that doesn't take into account the hierarchy in any way.",
                    "label": 0
                },
                {
                    "sent": "It could be any any bag of.",
                    "label": 0
                },
                {
                    "sent": "Lack of labels that we are predicting.",
                    "label": 0
                },
                {
                    "sent": "Here I have two loss functions that actually take the hierarchy into account.",
                    "label": 1
                },
                {
                    "sent": "First one is Hirculus suggested by Kesepian Keiynan, others last year, so idea with this loss is that you look.",
                    "label": 1
                },
                {
                    "sent": "Along its path of the hierarchy, where is the first mistake being made and you pay?",
                    "label": 0
                },
                {
                    "sent": "Where pay for this first mistake?",
                    "label": 0
                },
                {
                    "sent": "Not not any any other mistakes on the path.",
                    "label": 0
                },
                {
                    "sent": "So that's seems kind of sensible thing to do, 'cause if you make a mistake in the parent, it's kind of unfair that require getting this child corner correct anymore.",
                    "label": 1
                },
                {
                    "sent": "OK, in this paper we are using a simplified simplified version of this hierarchical loss.",
                    "label": 0
                },
                {
                    "sent": "That's decomposes by the edges, so we only only look don't we don't look at the whole path.",
                    "label": 1
                },
                {
                    "sent": "We only look what happened in the parents.",
                    "label": 0
                },
                {
                    "sent": "So if we.",
                    "label": 1
                },
                {
                    "sent": "Made a mistake in a child node.",
                    "label": 0
                },
                {
                    "sent": "We only pay if the parent was correct.",
                    "label": 1
                },
                {
                    "sent": "So if the child is is kind of.",
                    "label": 0
                },
                {
                    "sent": "OK, making a mistake.",
                    "label": 0
                },
                {
                    "sent": "Although the parent was correct so little bit simplified because it doesn't take take.",
                    "label": 0
                },
                {
                    "sent": "Into account the whole path so it's not as.",
                    "label": 0
                },
                {
                    "sent": "Maybe as pleasing as this this Oracle loss, but it's more tractable computational and that's why we're using it.",
                    "label": 0
                },
                {
                    "sent": "OK, that that must further loss functions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are the optimization problems that arise from from this kind of much margin approach for this this this and if you have what's the thought by Torsten Joachims on Monday morning or yesterday if you were in the yesterday's structured output session?",
                    "label": 0
                },
                {
                    "sent": "You have seen this this form.",
                    "label": 0
                },
                {
                    "sent": "Couple of times already.",
                    "label": 0
                },
                {
                    "sent": "So it's actually looks.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot for like this short marching SVM.",
                    "label": 0
                },
                {
                    "sent": "But we have much, much more complicated constraint.",
                    "label": 0
                },
                {
                    "sent": "In fact, we have exponential number of constraint in this problem and that actually makes this problem intractable in this form.",
                    "label": 0
                },
                {
                    "sent": "So he has to do all.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "Term with the loss function.",
                    "label": 0
                },
                {
                    "sent": "Is a linear term in quadratic term with the kernel.",
                    "label": 0
                },
                {
                    "sent": "Colonel Beef winter between them.",
                    "label": 0
                },
                {
                    "sent": "Feature vectors.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This kind of soft margin type box constraint.",
                    "label": 0
                },
                {
                    "sent": "OK, so this kind of be solved in this form.",
                    "label": 1
                },
                {
                    "sent": "It's way too late, too heavy, heavy to compute.",
                    "label": 0
                },
                {
                    "sent": "And people have been suggesting different different models how to how to tackle this problem?",
                    "label": 0
                },
                {
                    "sent": "We took the approach by interest by tuskar.",
                    "label": 0
                },
                {
                    "sent": "Up light into this.",
                    "label": 0
                },
                {
                    "sent": "It do this hierarchical case so.",
                    "label": 0
                },
                {
                    "sent": "The scar in his nips paper not marking not marginalized structure, so that instead of talking about this, this.",
                    "label": 1
                },
                {
                    "sent": "Dual variables for the whole vector.",
                    "label": 0
                },
                {
                    "sent": "To develop variables are now.",
                    "label": 0
                },
                {
                    "sent": "Per edge, so we have one.",
                    "label": 0
                },
                {
                    "sent": "Dual variable for.",
                    "label": 0
                },
                {
                    "sent": "Their edge.",
                    "label": 0
                },
                {
                    "sent": "And labeling of the of the ads.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are obtained as marginals of this.",
                    "label": 0
                },
                {
                    "sent": "This originally variables So what we do here is we fix.",
                    "label": 0
                },
                {
                    "sent": "We fixed an edge and a labeling for it for it, so it's kind of have four possibilities.",
                    "label": 0
                },
                {
                    "sent": "And we compute the sum over each labeling vector that conforms to this edge labeling.",
                    "label": 0
                },
                {
                    "sent": "So that's the marginal margin out of there.",
                    "label": 0
                },
                {
                    "sent": "Of the dual variable.",
                    "label": 0
                },
                {
                    "sent": "So we know the order to use this.",
                    "label": 0
                },
                {
                    "sent": "We need to decompose the loss.",
                    "label": 0
                },
                {
                    "sent": "And a kernel according to the edges as well.",
                    "label": 0
                },
                {
                    "sent": "So we need to have this kind of form for the loss function, so it's a sum over the edges.",
                    "label": 0
                },
                {
                    "sent": "So two of the loss functions I showed you earlier form this form.",
                    "label": 0
                },
                {
                    "sent": "This is the symmetric difference loss and this simplified hierarchical loss, and we are going to be using them.",
                    "label": 0
                },
                {
                    "sent": "Also, the kernel needs to be decomposed by the edges, so we need to have this kind of representation.",
                    "label": 0
                },
                {
                    "sent": "So this requires, while it is actually, this is not.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I'm a little bit incorrect airport, but the former might not be exactly this, but the idea is is this need to be decomposed so it requires that the feature vector has blocks for edges.",
                    "label": 0
                },
                {
                    "sent": "So we do it by.",
                    "label": 0
                },
                {
                    "sent": "In this block, we repeat.",
                    "label": 0
                },
                {
                    "sent": "A feature vector for the document.",
                    "label": 0
                },
                {
                    "sent": "We put it.",
                    "label": 0
                },
                {
                    "sent": "We have a.",
                    "label": 0
                },
                {
                    "sent": "For each edge four blocks, and we put this feature vector of the document that correct correct block.",
                    "label": 0
                },
                {
                    "sent": "So if for example.",
                    "label": 0
                },
                {
                    "sent": "The document belongs to the parent and the child.",
                    "label": 0
                },
                {
                    "sent": "It will defeat Survector will be put in this block corresponding to that labeling 1 + 1 + 1.",
                    "label": 0
                },
                {
                    "sent": "And otherwise will be at 0, so that's here.",
                    "label": 0
                },
                {
                    "sent": "Is the indicator kind of picking up which labeling.",
                    "label": 0
                },
                {
                    "sent": "Is the correct one?",
                    "label": 0
                },
                {
                    "sent": "Otherwise, it's it's it's 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the ingredients too.",
                    "label": 0
                },
                {
                    "sent": "Decompose.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To problem out of it so, so here's the objective.",
                    "label": 0
                },
                {
                    "sent": "It test comes when you plug in.",
                    "label": 0
                },
                {
                    "sent": "Plug in this.",
                    "label": 0
                },
                {
                    "sent": "Marginal do our valuables in there so you see it's a sum over the edges so it's case linearly in the edges.",
                    "label": 0
                },
                {
                    "sent": "No love my exponent exponential size.",
                    "label": 0
                },
                {
                    "sent": "Then there's the same box strong constraint as we had earlier, and then something else.",
                    "label": 0
                },
                {
                    "sent": "So this is consistency constraints that ensure that.",
                    "label": 1
                },
                {
                    "sent": "Our marginal deliverables correspond to the originally running so, so this ensures that we actually are still solving.",
                    "label": 1
                },
                {
                    "sent": "Extracting the same problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so at this point this is polynomial size and it's actually linear in the edges.",
                    "label": 0
                },
                {
                    "sent": "It's way too large to solve it with any of the self QP methods.",
                    "label": 1
                },
                {
                    "sent": "Really big big problems.",
                    "label": 1
                },
                {
                    "sent": "I mean, it's not straightforward to decompose because you have.",
                    "label": 1
                },
                {
                    "sent": "To training examples depending on each other in the kernel obviously and then you have these constraints that tie your edges together, so you cannot really just decompose it by examples or by edges.",
                    "label": 0
                },
                {
                    "sent": "The good news is since since we are doing hierarchical classification, prediction or computing this arc marks.",
                    "label": 0
                },
                {
                    "sent": "This efficient we can use dynamic programming for that, so that's not the problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so in order to tackle this this big problem.",
                    "label": 0
                },
                {
                    "sent": "So just use conditional gradient descent.",
                    "label": 0
                },
                {
                    "sent": "That is.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, iterative gradient search within the feasible set so the feasible set is now that.",
                    "label": 1
                },
                {
                    "sent": "Actually, the marginal polytope of the of the marginalized dual so.",
                    "label": 0
                },
                {
                    "sent": "And the update direction in this gradient search is the highest feasible point.",
                    "label": 1
                },
                {
                    "sent": "After making making a linear approximation of the quadratic surface so.",
                    "label": 0
                },
                {
                    "sent": "We look at the current current gradient and assume that that is the correct one to the kind of Infinity.",
                    "label": 0
                },
                {
                    "sent": "So, and we find some vertex of the feasible set, that's the direction and we step towards it somewhat.",
                    "label": 1
                },
                {
                    "sent": "Good thing about this scheme is that we actually can decompose the training to single example subspaces.",
                    "label": 0
                },
                {
                    "sent": "In that sense that if we update one example only, we don't need to consult, consult what happens with shadow examples once we do some initialization to start the search, so.",
                    "label": 0
                },
                {
                    "sent": "So there is some kind of overhead from moving from one example to another to another, But that's not not very, very bad.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the test illustration.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm, but it does so.",
                    "label": 1
                },
                {
                    "sent": "Here we have to.",
                    "label": 0
                },
                {
                    "sent": "Quadratic surface here is the feasible set.",
                    "label": 0
                },
                {
                    "sent": "So first we look for the gradient.",
                    "label": 0
                },
                {
                    "sent": "Make linear approximation of the quartic surface so and find the best point.",
                    "label": 0
                },
                {
                    "sent": "So assuming this gradient is the correct one through Infinity, this is our best possible point.",
                    "label": 0
                },
                {
                    "sent": "So that's the direction.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we go and compute the saddle point along this line.",
                    "label": 0
                },
                {
                    "sent": "And that's easy because we have a 1 dimensional line that we are thinking about.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Step to step saddle point and repeat the whole thing.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This goes on until we're we are happy too.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Either move to another example or or.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are are finished with optimization, so that's quite a concept.",
                    "label": 0
                },
                {
                    "sent": "Really quite simple.",
                    "label": 0
                },
                {
                    "sent": "Simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so here are some experi.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hence we have used it on two datasets, Reuters, MCV one.",
                    "label": 0
                },
                {
                    "sent": "This secret family.",
                    "label": 0
                },
                {
                    "sent": "That's 34 micro labels, the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "In Depth 3 three.",
                    "label": 0
                },
                {
                    "sent": "The fields are represented when the document was back of words with TF IDF weighting.",
                    "label": 0
                },
                {
                    "sent": "We have to add 2005 two 1500 lumens for training 5000 for testing.",
                    "label": 1
                },
                {
                    "sent": "Viper Alfred Peyton data set that's a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Michael labels 188 little bit deeper tree.",
                    "label": 0
                },
                {
                    "sent": "So here are the statistiques.",
                    "label": 0
                },
                {
                    "sent": "There are five.",
                    "label": 1
                },
                {
                    "sent": "Algorithms are algorithms that we call hierarchical maximum margin Markov kind of following task course naming convention.",
                    "label": 0
                },
                {
                    "sent": "Comparison we have flat SVM.",
                    "label": 0
                },
                {
                    "sent": "That's Predix eats.",
                    "label": 0
                },
                {
                    "sent": "Note independently Oracle trainers wear.",
                    "label": 0
                },
                {
                    "sent": "OK, that's trained with the subset of the of the training it I need it snowed and then new album by case up.",
                    "label": 0
                },
                {
                    "sent": "Yankee he hierarchical rasley squares.",
                    "label": 0
                },
                {
                    "sent": "Implementation was in Matlab.",
                    "label": 0
                },
                {
                    "sent": "And yeah, so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it, OK, so here are the results.",
                    "label": 0
                },
                {
                    "sent": "So prediction quality, so here it is, 001 loss.",
                    "label": 0
                },
                {
                    "sent": "You can see that so flat SVM has the highest.",
                    "label": 0
                },
                {
                    "sent": "01 loss then order other hierarchical models.",
                    "label": 0
                },
                {
                    "sent": "And then there are this, our two methods with two different loss functions, with the symmetric difference loss and this hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Simplified hierarchical loss.",
                    "label": 0
                },
                {
                    "sent": "So here we get.",
                    "label": 0
                },
                {
                    "sent": "Little bit better racials than the other ones.",
                    "label": 0
                },
                {
                    "sent": "Then there are positional recall and F1 so.",
                    "label": 0
                },
                {
                    "sent": "As you can see, flatter SVM is is the most precise when it predicts positive, it usually.",
                    "label": 0
                },
                {
                    "sent": "Guess it gets it correct.",
                    "label": 0
                },
                {
                    "sent": "But the recall is low, so it doesn't predict positive very often.",
                    "label": 0
                },
                {
                    "sent": "Ann, this our methods are in the other extreme, where we get really good recall or quite good recall, but the precision is not quite as high.",
                    "label": 0
                },
                {
                    "sent": "And overall, you can see that F1 score.",
                    "label": 0
                },
                {
                    "sent": "All these hierarchical methods do well.",
                    "label": 0
                },
                {
                    "sent": "And the flat SVM is suffering.",
                    "label": 0
                },
                {
                    "sent": "OK, so so then we did another another test to see whether the hierarchical method.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Differ differ at all.",
                    "label": 0
                },
                {
                    "sent": "They compute this F1 scores level power level.",
                    "label": 0
                },
                {
                    "sent": "Here is.",
                    "label": 0
                },
                {
                    "sent": "Let me sort of wiper set we have.",
                    "label": 0
                },
                {
                    "sent": "The F1 score for the RT second level third Level 4 level so you can see that.",
                    "label": 0
                },
                {
                    "sent": "This structured output methods they start to work really well in the deep nodes.",
                    "label": 0
                },
                {
                    "sent": "And that's that's becausw.",
                    "label": 0
                },
                {
                    "sent": "They have good recall.",
                    "label": 0
                },
                {
                    "sent": "They actually go and predict positive when other the other medals stop predicting positive.",
                    "label": 0
                },
                {
                    "sent": "You can see the flat SVM is the most hopeless in and test gives up produce everything negative in interleaves or model and not everything but kind of starts would be very negative and the other he recalled.",
                    "label": 0
                },
                {
                    "sent": "Medals are little bit more negative than than the grand.",
                    "label": 0
                },
                {
                    "sent": "Structured output methods.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's that's basically the picture.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something about the optimization.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the algorithm tests audio is not the fastest one for the for the.",
                    "label": 0
                },
                {
                    "sent": "For this optimized this model.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "This is the curve of that method in on this wiper data set, so it's OK, but it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not very far that it takes a couple of hours to optimize.",
                    "label": 0
                },
                {
                    "sent": "This foster one is an algorithm we came up with after after this submission was was done so.",
                    "label": 0
                },
                {
                    "sent": "The only change in algorithm is how this update direction is this computer we used.",
                    "label": 0
                },
                {
                    "sent": "In this paper we used linear programming to find this direction in this feasible set.",
                    "label": 0
                },
                {
                    "sent": "And here we actually utilize the tree and dynamic programming over the tree to find the same direction.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't have time to go into detail details but but it really seems to work really well, but it's basically the same algorithm scheme, but kind of the very innermost step has been has been replaced with something way more efficient.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Need to make some conclusions so we present that Colonel Pass up rose for hierarchical test text classification when documents can belong to more than one category at the time.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Since we can utilize this dependence structure of Mac micro labels, we get a little bit better prediction accuracy when the hierarchy is deep.",
                    "label": 0
                },
                {
                    "sent": "Or actually it seems to be substantial.",
                    "label": 0
                },
                {
                    "sent": "Went it went.",
                    "label": 0
                },
                {
                    "sent": "Hierarchy is the.",
                    "label": 0
                },
                {
                    "sent": "OK, so the optimization.",
                    "label": 1
                },
                {
                    "sent": "This means metaphysical by decomposing duration problem and make making incremental conditional gradient search in this sub problem.",
                    "label": 0
                },
                {
                    "sent": "And result is we have tractable optimization from medium sized datasets.",
                    "label": 0
                },
                {
                    "sent": "So we and we're not quite there to claim it's a large scale.",
                    "label": 0
                },
                {
                    "sent": "The optimal implementation is still in math lab and we don't really have a large scale implementation, so I'm interested to see in the future how far we can scale this up, But so this is the status at the moment.",
                    "label": 0
                },
                {
                    "sent": "OK, so thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "When will windows?",
                    "label": 0
                },
                {
                    "sent": "Many cases there are section 200 size alright.",
                    "label": 0
                },
                {
                    "sent": "Really scary one is the big hierarchies in the published, which could be 1000.",
                    "label": 0
                },
                {
                    "sent": "They can be lovely.",
                    "label": 0
                },
                {
                    "sent": "OK, so can you pieces together, can you say?",
                    "label": 0
                },
                {
                    "sent": "Uses for parts of the problem that could be composed still operate right.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question was can we kind of tackle this big, really big hierarchies when you have thousands of nodes nodes in the hierarchy, and.",
                    "label": 0
                },
                {
                    "sent": "The good thing about this algorithm is that it scales linearly in the size of the hierarchy, so increasing the size of the hierarchy doesn't necessarily kill you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So linear scaling, kind of.",
                    "label": 0
                },
                {
                    "sent": "It's kind of promises that it should be OK, so.",
                    "label": 0
                },
                {
                    "sent": "Phillips print this picture.",
                    "label": 0
                }
            ]
        }
    }
}