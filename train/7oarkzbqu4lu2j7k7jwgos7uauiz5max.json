{
    "id": "7oarkzbqu4lu2j7k7jwgos7uauiz5max",
    "title": "Discriminative Topic Modeling based on Manifold Learning",
    "info": {
        "author": [
            "Seungil Huh, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd2010_huh_dtnbml/",
    "segmentation": [
        [
            "Hello everyone, my name is Sunil Hall.",
            "I'm from Carnegie Mellon University and the title of the paper is discriminative tapping, modeling based manifold learning.",
            "This work is a joint work with Professor Stephen Feinberg.",
            "So he."
        ],
        [
            "Case overview.",
            "Typing modeling has been popularly used for data analysis in various domains.",
            "Particularly text analysis.",
            "Previous topic models such as P LSA and LDA have shown great success in discovering the ruling in structures and generative generative process of data.",
            "However, this typing model is not take into account the geometric structure of data.",
            "According to the recent research data from images or texts are often located on lowering manifold nonlinear manifolds within the high dimension space of the original data.",
            "Therefore, learning meaningful structure can provide better dimensionality reduction, mapping also an higher displaying power, especially when the number of labeled data is small.",
            "So in our model.",
            "Are we incorporate these two basic components into one model using the formulation of regularly right stopping model?",
            "As a result, we achieved higher discrete power and show better results in semis.",
            "Provides document classification problem.",
            "In fact, there are two previous models that try to combine these two basic components.",
            "However, these two mothers do not reach the full screening power of manifold learning.",
            "I'm going to."
        ],
        [
            "Any detail later.",
            "So first I'm going to explain the two basic components of our model.",
            "Probabilistic latent semantic analysis is a topic model and Laplacian Eigen map says manifold learning algorithm."
        ],
        [
            "So among the topic models we adapt formulation of P LSA.",
            "In PLSA each document is represented by a mixture of basis word distribution called topic.",
            "Which is denoted by the variable, see here.",
            "The generative process appear, let's say, is illustrated in this critical model.",
            "So here variable document variability and word variable W are observed.",
            "But the latent topic variable C is now observed.",
            "To institutional influence.",
            "We delight to find one model is most likely generate the documents.",
            "The more specifically we do like to compute the probabilities, see given the and probabilities of W given Z to find the best model.",
            "We maximize the local likely."
        ],
        [
            "Of entire documents.",
            "Geometrically, tapping models can be interpreted as a projection from the simple expand by the words to the simple expand by the topic.",
            "I in PSA the projection is.",
            "KL divergent projection.",
            "Since the number of topics is generally much less than the number of words typing, others can be considered as dimensionality reduction methods.",
            "But in this dimensionality dimensionality reduction, the manifold structure is not considered.",
            "Among the.",
            "Men."
        ],
        [
            "For this learning algorithms, we adapt the formulation of Laplacian Eigen Maps.",
            "So the basic idea of Laplace and I can make this local consistency.",
            "Intuitively, if two documents have similar word distributions.",
            "But they tend to be near each other on the manifold.",
            "The two formulate Le 1st.",
            "We define the local similarity matrix W. Then we minimize.",
            "The sum of distances between neighboring pairs.",
            "But if we only consider these these.",
            "Neighboring pairs.",
            "All points converge to one point, so.",
            "Eli actually has some constraints to maintain or maximize the distances between neighboring.",
            "As well.",
            "Now we incorporate the."
        ],
        [
            "Two formulations.",
            "Overall appeal FA an Le to obtain both of their advantages, so here is the local likelihood appeal essay.",
            "And the formulation of Le is added as a regularization term here.",
            "To use this by using this regularize formulation.",
            "We can discover the generative process of data due to the capability of PLSA.",
            "Also, I feel that we can achieve the dissipating power.",
            "Better higher displaying power due to the capability of Laplacian Eigen Maps.",
            "People presenting our model.",
            "I introduced two previous models that are."
        ],
        [
            "The same idea.",
            "So here is the regularize local likelihood.",
            "Annapolis, MPLS I defines the regularization regularization term using the objective function of Le here popular easy given the is the topic distribution in a document which is low rank representation of the world world distribution in a document.",
            "And LTM.",
            "Utilizes the KL divergent instead of squared Euclidean distance.",
            "By discovering the local neighborhood structure, these two matters should hire different power than previous stopping matters.",
            "However, these two models do not consider non neighboring relationships.",
            "Therefore, these two models may not effectively resolve the money for structure.",
            "As a result, they do not reach the full discriminating power of manifold learning."
        ],
        [
            "So here is our decorative tapping model.",
            "So in a in the regularization term.",
            "We not only adapt the objective function of LA, but also incorporate the constraints of Eli, so the denominator is the sum of distances between neighboring pairs and numerator is the sum of distances between neighboring pairs.",
            "By using this formulation we can achieve even higher displaying power."
        ],
        [
            "Then the previous models.",
            "So here is our regularised log likelihood.",
            "To fit this model.",
            "We use generalized the EM algorithm.",
            "So the step of generalized VM is exactly same as the step of normal EM algorithm and posterior probabilities can be computed.",
            "The same is PLSA.",
            "In the end step of normal normal in algorithm we maximize the expected value of log likelihood.",
            "But in them step of generalized DM.",
            "We just improve the expected value of log likelihood.",
            "So we have two parameters here.",
            "Fine is a set of probabilities of W given Z and Theta is a set of popular easy given the.",
            "So we need to re estimate these two parameters.",
            "And the expected value of log likelihood can be expressed by Q1 plus Lambda Q2.",
            "Here Q1 is the light from the formulation of P LSA and contains both the parameters.",
            "And Q2 is directly derived from the formulation of Le and contains only the polymer data.",
            "Because the formulation of Le is defined based on low rank representation.",
            "The way to re estimate the Parliament file is the same as PSA.",
            "In theory, after made the primary data, we are going to find the party to improvements between Q1 and Q2, and I'm going to explain this in detail."
        ],
        [
            "So in the second part of M step we need to find the parameter Theta that improves the expectation value of local likelihood which is expressed by Q1 plus Lambda Q 2 and we are going to find the pietta improvement between Q and Q1 and Q2.",
            "The pilot improvement is defined as a change from one state to another that improved at least one objective without worsening the others.",
            "So in this figure, AB&C can be obtained by pretty improvement from valatie.",
            "If we consider two functions killed in Q 2 because neither of Q1 little Q on newer Q 2.",
            "Decreases decreases after updating, but the update 2D or E cannot be applied to improvement because either of Q1 or Q2 decreases after updating and among AB&C.",
            "We prefer the A because a has a higher value of Q 2.",
            "The reason we prefer higher value of Q 2 is that we are interested in the discriminative power of the model and the power is more related to manifold learning and Q2 is derived from the early formulation.",
            "Therefore, higher value of Q2 can lead to the higher discriminating model as long as the generative process of data where is well discovered.",
            "And one of the advantages of our model is that the effective value of local likelihood increases regardless of the.",
            "The regularization parameter Lambda.",
            "Therefore, in our model, the Primal Lambda Linda between."
        ],
        [
            "So to find the product improvements, we first introduce 2 updates.",
            "The basic idea here is that we first.",
            "Introduce updates for each of Q1 and Q2.",
            "Then we are going to somehow combine them to find the party improvement.",
            "So here is the 1st update.",
            "If we apply this update to Parliament data, we can prove that.",
            "Q and monotonically increases from the old theater to the newsletter along the line.",
            "Here is the 2nd update if we apply this update to Parliament data, we can prove that Q2 is nondecreasing after updating.",
            "The proofs are in the paper."
        ],
        [
            "Do you think these two updates we're going to find the party improvement, so let current set of parameters be filler.",
            "0.",
            "First we apply the 2nd update to zero and obtaining Federal One.",
            "And the second second theorem ensures that the new Q 2 is not less than the other Q to the region we apply the 2nd update versus that.",
            "We'd like to obtain the preferred quality improvement.",
            "Then we apply the 1st update an obtaining failure too.",
            "Anne.",
            "The first theorem ensures that Q and increases from 3 to 123 at two along the line.",
            "So we perform a line search from theater 1202.",
            "If you find a part implement during the line search then search we update the current data with the new one.",
            "Otherwise we keep the current data and continue to the next step.",
            "Using this scheme, we can find the party implement and you can update."
        ],
        [
            "Parametric data.",
            "So here is experimental setup we to evaluate our model, we use two different text corpora.",
            "Changes groups in Yahoo use cases.",
            "And to address the real world problem in semi supervised setting, we randomly select a small number of label document for each class and we compare our model with other topic models contain including Laplacian, PLA, Cyan LTM as well as traditional dimensionality reduction methods.",
            "So here is the result onto news."
        ],
        [
            "The first figure shows the result when only one document is labeled for each category and their three plus shows the results when three, 5 and 10 documents are labeled for each category.",
            "And the X axis of this plot.",
            "Is the number of topics or topic matters or?",
            "The reduced dimensionality for general dimensionality reduction methods.",
            "And the red line in the highest position in each plot shows the result of our model and the following dark green and blue line indicates the result of life lesson, PLA scion LTN respectively.",
            "This can be seen in these figures are mother consistently."
        ],
        [
            "Outperforms the previous approaches.",
            "Love lesson.",
            "Sign LTM generally better than P LSA or LTM which are the traditional topping models.",
            "But their performance in a compatible 2 hours because they do not.",
            "Consider the non neighboring relationships."
        ],
        [
            "Papa"
        ],
        [
            "Here is the experimental results.",
            "Are on Yahoo News K cities.",
            "We obtain very similar results.",
            "Now our model shows the best results followed by.",
            "The previous two mothers depression PLSINLTM."
        ],
        [
            "As I mentioned, our model in R model regularization private Lambda Linda, be tuned.",
            "But in the previous two model A plus MPLS ion LTM, the Lambda needs to be tuned, but.",
            "There are, it is not clear how to effectively appropriately determine its value.",
            "In that sense, our model is less sensitive to parameter.",
            "Also, we have two parameters.",
            "One is the number of the nearest neighbors for the similarity metrics and step size for line search.",
            "So these two plus shows the classification accuracy when these two parameters are varied.",
            "This can be seen.",
            "The performance variation is negligible."
        ],
        [
            "When two parameters are changed in reasonable ranges.",
            "So here is summary.",
            "We proposed a topic model that incorporates the complete manifold learning formulation.",
            "In contrast to the previous typing models, we considered non neighboring relationships as well as neighboring relationships.",
            "If the result, we achieved higher displaying power and show better result in semis provides document classification.",
            "Also, we presented a effective on effective motivating algorithm based on generalized human part improvements.",
            "Which enables the reliable discovery of low rank structure.",
            "By minimizing the sensitivity to parameters.",
            "Thank you."
        ],
        [
            "Any questions over here?",
            "Could you repeat the question?",
            "Yet the question is.",
            "The.",
            "In the computational in terms of computer costs, how?",
            "So if we compare the computer cost of our model with others, especially in LDA, how is it so the answer is.",
            "So.",
            "The previous topic models that incorporate the topping model and manifold learning formulation actually takes more time than the basic typing models such as fearless in LDA.",
            "An AR model is his takes more time because in the second update in our model fitting algorithm we involves on iteration.",
            "So our mother takes them a little bit more time than the previous topic models the operation.",
            "Say an pearsei and LTM.",
            "Over the ender mark.",
            "Yeah, so repeat it again, yeah.",
            "So the question is what kind of classifier do I used for evaluation?",
            "So I used SVM here.",
            "Yeah, but I also tested the one nearest neighbor and I obtained very similar results.",
            "So you mean if we apply CVM on the world distribution?",
            "I mean the basic word distribution?",
            "Actually there is the black dots here.",
            "Yeah, so black that indicates the data results when we apply SVM directly to the word distribution.",
            "Well, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, my name is Sunil Hall.",
                    "label": 0
                },
                {
                    "sent": "I'm from Carnegie Mellon University and the title of the paper is discriminative tapping, modeling based manifold learning.",
                    "label": 1
                },
                {
                    "sent": "This work is a joint work with Professor Stephen Feinberg.",
                    "label": 0
                },
                {
                    "sent": "So he.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case overview.",
                    "label": 0
                },
                {
                    "sent": "Typing modeling has been popularly used for data analysis in various domains.",
                    "label": 0
                },
                {
                    "sent": "Particularly text analysis.",
                    "label": 0
                },
                {
                    "sent": "Previous topic models such as P LSA and LDA have shown great success in discovering the ruling in structures and generative generative process of data.",
                    "label": 0
                },
                {
                    "sent": "However, this typing model is not take into account the geometric structure of data.",
                    "label": 0
                },
                {
                    "sent": "According to the recent research data from images or texts are often located on lowering manifold nonlinear manifolds within the high dimension space of the original data.",
                    "label": 0
                },
                {
                    "sent": "Therefore, learning meaningful structure can provide better dimensionality reduction, mapping also an higher displaying power, especially when the number of labeled data is small.",
                    "label": 0
                },
                {
                    "sent": "So in our model.",
                    "label": 0
                },
                {
                    "sent": "Are we incorporate these two basic components into one model using the formulation of regularly right stopping model?",
                    "label": 0
                },
                {
                    "sent": "As a result, we achieved higher discrete power and show better results in semis.",
                    "label": 0
                },
                {
                    "sent": "Provides document classification problem.",
                    "label": 0
                },
                {
                    "sent": "In fact, there are two previous models that try to combine these two basic components.",
                    "label": 0
                },
                {
                    "sent": "However, these two mothers do not reach the full screening power of manifold learning.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any detail later.",
                    "label": 0
                },
                {
                    "sent": "So first I'm going to explain the two basic components of our model.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic latent semantic analysis is a topic model and Laplacian Eigen map says manifold learning algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So among the topic models we adapt formulation of P LSA.",
                    "label": 0
                },
                {
                    "sent": "In PLSA each document is represented by a mixture of basis word distribution called topic.",
                    "label": 0
                },
                {
                    "sent": "Which is denoted by the variable, see here.",
                    "label": 0
                },
                {
                    "sent": "The generative process appear, let's say, is illustrated in this critical model.",
                    "label": 0
                },
                {
                    "sent": "So here variable document variability and word variable W are observed.",
                    "label": 0
                },
                {
                    "sent": "But the latent topic variable C is now observed.",
                    "label": 0
                },
                {
                    "sent": "To institutional influence.",
                    "label": 0
                },
                {
                    "sent": "We delight to find one model is most likely generate the documents.",
                    "label": 0
                },
                {
                    "sent": "The more specifically we do like to compute the probabilities, see given the and probabilities of W given Z to find the best model.",
                    "label": 0
                },
                {
                    "sent": "We maximize the local likely.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of entire documents.",
                    "label": 0
                },
                {
                    "sent": "Geometrically, tapping models can be interpreted as a projection from the simple expand by the words to the simple expand by the topic.",
                    "label": 1
                },
                {
                    "sent": "I in PSA the projection is.",
                    "label": 0
                },
                {
                    "sent": "KL divergent projection.",
                    "label": 0
                },
                {
                    "sent": "Since the number of topics is generally much less than the number of words typing, others can be considered as dimensionality reduction methods.",
                    "label": 1
                },
                {
                    "sent": "But in this dimensionality dimensionality reduction, the manifold structure is not considered.",
                    "label": 0
                },
                {
                    "sent": "Among the.",
                    "label": 0
                },
                {
                    "sent": "Men.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this learning algorithms, we adapt the formulation of Laplacian Eigen Maps.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea of Laplace and I can make this local consistency.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, if two documents have similar word distributions.",
                    "label": 1
                },
                {
                    "sent": "But they tend to be near each other on the manifold.",
                    "label": 1
                },
                {
                    "sent": "The two formulate Le 1st.",
                    "label": 0
                },
                {
                    "sent": "We define the local similarity matrix W. Then we minimize.",
                    "label": 0
                },
                {
                    "sent": "The sum of distances between neighboring pairs.",
                    "label": 0
                },
                {
                    "sent": "But if we only consider these these.",
                    "label": 0
                },
                {
                    "sent": "Neighboring pairs.",
                    "label": 1
                },
                {
                    "sent": "All points converge to one point, so.",
                    "label": 0
                },
                {
                    "sent": "Eli actually has some constraints to maintain or maximize the distances between neighboring.",
                    "label": 0
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "Now we incorporate the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two formulations.",
                    "label": 0
                },
                {
                    "sent": "Overall appeal FA an Le to obtain both of their advantages, so here is the local likelihood appeal essay.",
                    "label": 0
                },
                {
                    "sent": "And the formulation of Le is added as a regularization term here.",
                    "label": 1
                },
                {
                    "sent": "To use this by using this regularize formulation.",
                    "label": 1
                },
                {
                    "sent": "We can discover the generative process of data due to the capability of PLSA.",
                    "label": 0
                },
                {
                    "sent": "Also, I feel that we can achieve the dissipating power.",
                    "label": 0
                },
                {
                    "sent": "Better higher displaying power due to the capability of Laplacian Eigen Maps.",
                    "label": 0
                },
                {
                    "sent": "People presenting our model.",
                    "label": 0
                },
                {
                    "sent": "I introduced two previous models that are.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The same idea.",
                    "label": 0
                },
                {
                    "sent": "So here is the regularize local likelihood.",
                    "label": 0
                },
                {
                    "sent": "Annapolis, MPLS I defines the regularization regularization term using the objective function of Le here popular easy given the is the topic distribution in a document which is low rank representation of the world world distribution in a document.",
                    "label": 0
                },
                {
                    "sent": "And LTM.",
                    "label": 0
                },
                {
                    "sent": "Utilizes the KL divergent instead of squared Euclidean distance.",
                    "label": 1
                },
                {
                    "sent": "By discovering the local neighborhood structure, these two matters should hire different power than previous stopping matters.",
                    "label": 0
                },
                {
                    "sent": "However, these two models do not consider non neighboring relationships.",
                    "label": 0
                },
                {
                    "sent": "Therefore, these two models may not effectively resolve the money for structure.",
                    "label": 0
                },
                {
                    "sent": "As a result, they do not reach the full discriminating power of manifold learning.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is our decorative tapping model.",
                    "label": 0
                },
                {
                    "sent": "So in a in the regularization term.",
                    "label": 0
                },
                {
                    "sent": "We not only adapt the objective function of LA, but also incorporate the constraints of Eli, so the denominator is the sum of distances between neighboring pairs and numerator is the sum of distances between neighboring pairs.",
                    "label": 1
                },
                {
                    "sent": "By using this formulation we can achieve even higher displaying power.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the previous models.",
                    "label": 0
                },
                {
                    "sent": "So here is our regularised log likelihood.",
                    "label": 0
                },
                {
                    "sent": "To fit this model.",
                    "label": 0
                },
                {
                    "sent": "We use generalized the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the step of generalized VM is exactly same as the step of normal EM algorithm and posterior probabilities can be computed.",
                    "label": 1
                },
                {
                    "sent": "The same is PLSA.",
                    "label": 0
                },
                {
                    "sent": "In the end step of normal normal in algorithm we maximize the expected value of log likelihood.",
                    "label": 0
                },
                {
                    "sent": "But in them step of generalized DM.",
                    "label": 0
                },
                {
                    "sent": "We just improve the expected value of log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So we have two parameters here.",
                    "label": 0
                },
                {
                    "sent": "Fine is a set of probabilities of W given Z and Theta is a set of popular easy given the.",
                    "label": 0
                },
                {
                    "sent": "So we need to re estimate these two parameters.",
                    "label": 0
                },
                {
                    "sent": "And the expected value of log likelihood can be expressed by Q1 plus Lambda Q2.",
                    "label": 0
                },
                {
                    "sent": "Here Q1 is the light from the formulation of P LSA and contains both the parameters.",
                    "label": 0
                },
                {
                    "sent": "And Q2 is directly derived from the formulation of Le and contains only the polymer data.",
                    "label": 0
                },
                {
                    "sent": "Because the formulation of Le is defined based on low rank representation.",
                    "label": 1
                },
                {
                    "sent": "The way to re estimate the Parliament file is the same as PSA.",
                    "label": 0
                },
                {
                    "sent": "In theory, after made the primary data, we are going to find the party to improvements between Q1 and Q2, and I'm going to explain this in detail.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the second part of M step we need to find the parameter Theta that improves the expectation value of local likelihood which is expressed by Q1 plus Lambda Q 2 and we are going to find the pietta improvement between Q and Q1 and Q2.",
                    "label": 1
                },
                {
                    "sent": "The pilot improvement is defined as a change from one state to another that improved at least one objective without worsening the others.",
                    "label": 0
                },
                {
                    "sent": "So in this figure, AB&C can be obtained by pretty improvement from valatie.",
                    "label": 0
                },
                {
                    "sent": "If we consider two functions killed in Q 2 because neither of Q1 little Q on newer Q 2.",
                    "label": 0
                },
                {
                    "sent": "Decreases decreases after updating, but the update 2D or E cannot be applied to improvement because either of Q1 or Q2 decreases after updating and among AB&C.",
                    "label": 0
                },
                {
                    "sent": "We prefer the A because a has a higher value of Q 2.",
                    "label": 0
                },
                {
                    "sent": "The reason we prefer higher value of Q 2 is that we are interested in the discriminative power of the model and the power is more related to manifold learning and Q2 is derived from the early formulation.",
                    "label": 0
                },
                {
                    "sent": "Therefore, higher value of Q2 can lead to the higher discriminating model as long as the generative process of data where is well discovered.",
                    "label": 1
                },
                {
                    "sent": "And one of the advantages of our model is that the effective value of local likelihood increases regardless of the.",
                    "label": 0
                },
                {
                    "sent": "The regularization parameter Lambda.",
                    "label": 1
                },
                {
                    "sent": "Therefore, in our model, the Primal Lambda Linda between.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to find the product improvements, we first introduce 2 updates.",
                    "label": 0
                },
                {
                    "sent": "The basic idea here is that we first.",
                    "label": 0
                },
                {
                    "sent": "Introduce updates for each of Q1 and Q2.",
                    "label": 0
                },
                {
                    "sent": "Then we are going to somehow combine them to find the party improvement.",
                    "label": 0
                },
                {
                    "sent": "So here is the 1st update.",
                    "label": 0
                },
                {
                    "sent": "If we apply this update to Parliament data, we can prove that.",
                    "label": 0
                },
                {
                    "sent": "Q and monotonically increases from the old theater to the newsletter along the line.",
                    "label": 0
                },
                {
                    "sent": "Here is the 2nd update if we apply this update to Parliament data, we can prove that Q2 is nondecreasing after updating.",
                    "label": 0
                },
                {
                    "sent": "The proofs are in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do you think these two updates we're going to find the party improvement, so let current set of parameters be filler.",
                    "label": 0
                },
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "First we apply the 2nd update to zero and obtaining Federal One.",
                    "label": 0
                },
                {
                    "sent": "And the second second theorem ensures that the new Q 2 is not less than the other Q to the region we apply the 2nd update versus that.",
                    "label": 0
                },
                {
                    "sent": "We'd like to obtain the preferred quality improvement.",
                    "label": 0
                },
                {
                    "sent": "Then we apply the 1st update an obtaining failure too.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The first theorem ensures that Q and increases from 3 to 123 at two along the line.",
                    "label": 0
                },
                {
                    "sent": "So we perform a line search from theater 1202.",
                    "label": 1
                },
                {
                    "sent": "If you find a part implement during the line search then search we update the current data with the new one.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we keep the current data and continue to the next step.",
                    "label": 0
                },
                {
                    "sent": "Using this scheme, we can find the party implement and you can update.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parametric data.",
                    "label": 0
                },
                {
                    "sent": "So here is experimental setup we to evaluate our model, we use two different text corpora.",
                    "label": 0
                },
                {
                    "sent": "Changes groups in Yahoo use cases.",
                    "label": 0
                },
                {
                    "sent": "And to address the real world problem in semi supervised setting, we randomly select a small number of label document for each class and we compare our model with other topic models contain including Laplacian, PLA, Cyan LTM as well as traditional dimensionality reduction methods.",
                    "label": 1
                },
                {
                    "sent": "So here is the result onto news.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first figure shows the result when only one document is labeled for each category and their three plus shows the results when three, 5 and 10 documents are labeled for each category.",
                    "label": 0
                },
                {
                    "sent": "And the X axis of this plot.",
                    "label": 0
                },
                {
                    "sent": "Is the number of topics or topic matters or?",
                    "label": 0
                },
                {
                    "sent": "The reduced dimensionality for general dimensionality reduction methods.",
                    "label": 0
                },
                {
                    "sent": "And the red line in the highest position in each plot shows the result of our model and the following dark green and blue line indicates the result of life lesson, PLA scion LTN respectively.",
                    "label": 0
                },
                {
                    "sent": "This can be seen in these figures are mother consistently.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Outperforms the previous approaches.",
                    "label": 0
                },
                {
                    "sent": "Love lesson.",
                    "label": 0
                },
                {
                    "sent": "Sign LTM generally better than P LSA or LTM which are the traditional topping models.",
                    "label": 0
                },
                {
                    "sent": "But their performance in a compatible 2 hours because they do not.",
                    "label": 0
                },
                {
                    "sent": "Consider the non neighboring relationships.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Papa",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the experimental results.",
                    "label": 1
                },
                {
                    "sent": "Are on Yahoo News K cities.",
                    "label": 0
                },
                {
                    "sent": "We obtain very similar results.",
                    "label": 0
                },
                {
                    "sent": "Now our model shows the best results followed by.",
                    "label": 0
                },
                {
                    "sent": "The previous two mothers depression PLSINLTM.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I mentioned, our model in R model regularization private Lambda Linda, be tuned.",
                    "label": 0
                },
                {
                    "sent": "But in the previous two model A plus MPLS ion LTM, the Lambda needs to be tuned, but.",
                    "label": 0
                },
                {
                    "sent": "There are, it is not clear how to effectively appropriately determine its value.",
                    "label": 0
                },
                {
                    "sent": "In that sense, our model is less sensitive to parameter.",
                    "label": 1
                },
                {
                    "sent": "Also, we have two parameters.",
                    "label": 0
                },
                {
                    "sent": "One is the number of the nearest neighbors for the similarity metrics and step size for line search.",
                    "label": 0
                },
                {
                    "sent": "So these two plus shows the classification accuracy when these two parameters are varied.",
                    "label": 0
                },
                {
                    "sent": "This can be seen.",
                    "label": 0
                },
                {
                    "sent": "The performance variation is negligible.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When two parameters are changed in reasonable ranges.",
                    "label": 0
                },
                {
                    "sent": "So here is summary.",
                    "label": 0
                },
                {
                    "sent": "We proposed a topic model that incorporates the complete manifold learning formulation.",
                    "label": 1
                },
                {
                    "sent": "In contrast to the previous typing models, we considered non neighboring relationships as well as neighboring relationships.",
                    "label": 0
                },
                {
                    "sent": "If the result, we achieved higher displaying power and show better result in semis provides document classification.",
                    "label": 0
                },
                {
                    "sent": "Also, we presented a effective on effective motivating algorithm based on generalized human part improvements.",
                    "label": 0
                },
                {
                    "sent": "Which enables the reliable discovery of low rank structure.",
                    "label": 1
                },
                {
                    "sent": "By minimizing the sensitivity to parameters.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions over here?",
                    "label": 0
                },
                {
                    "sent": "Could you repeat the question?",
                    "label": 0
                },
                {
                    "sent": "Yet the question is.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "In the computational in terms of computer costs, how?",
                    "label": 0
                },
                {
                    "sent": "So if we compare the computer cost of our model with others, especially in LDA, how is it so the answer is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The previous topic models that incorporate the topping model and manifold learning formulation actually takes more time than the basic typing models such as fearless in LDA.",
                    "label": 0
                },
                {
                    "sent": "An AR model is his takes more time because in the second update in our model fitting algorithm we involves on iteration.",
                    "label": 0
                },
                {
                    "sent": "So our mother takes them a little bit more time than the previous topic models the operation.",
                    "label": 0
                },
                {
                    "sent": "Say an pearsei and LTM.",
                    "label": 0
                },
                {
                    "sent": "Over the ender mark.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so repeat it again, yeah.",
                    "label": 0
                },
                {
                    "sent": "So the question is what kind of classifier do I used for evaluation?",
                    "label": 0
                },
                {
                    "sent": "So I used SVM here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I also tested the one nearest neighbor and I obtained very similar results.",
                    "label": 0
                },
                {
                    "sent": "So you mean if we apply CVM on the world distribution?",
                    "label": 0
                },
                {
                    "sent": "I mean the basic word distribution?",
                    "label": 0
                },
                {
                    "sent": "Actually there is the black dots here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so black that indicates the data results when we apply SVM directly to the word distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}