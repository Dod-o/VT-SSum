{
    "id": "qt6trce6sduntgrntrecfltmq2jkrp53",
    "title": "Multi-task feature learning",
    "info": {
        "author": [
            "Andreas Argyriou, \u00c9cole Centrale Paris"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output",
            "Top->Computer Science->Machine Learning->Preprocessing"
        ]
    },
    "url": "http://videolectures.net/oh06_argyriou_mtfl/",
    "segmentation": [
        [
            "So this is work with Massey and Fairview new.",
            "Thanks.",
            "OK.",
            "So.",
            "And so this is an approach of multi for multi task learning based on doing some kind of having some kind of assumptions about the feature space and some the assumption that we can embed into a low dimensional feature space.",
            "So we want to have a convex formulation of this problem.",
            "And this is the outline modernas.",
            "Now the general multitask multitask learning problem is involves number of tasks where you want to."
        ],
        [
            "Explore the similarities between the different tasks and you want to pull a lot of examples.",
            "And if your examples for its task and join them together in order to get better performance and there have been several applications of these.",
            "Of this scenario.",
            "In this case we have."
        ],
        [
            "Of this, is our notation more or less?",
            "So we have the tasks.",
            "We have an examples for task.",
            "So it's task might have different examples from another task, so we don't assume this.",
            "The input space is the input.",
            "The examples are the same, but we assume for simplicity we have the same number of examples, but we could say we have different numbers of examples.",
            "And we want to estimate the function safety so one function for its.",
            "For its task now we will assume we won't assume something on these functions directly, but there will be some.",
            "We'll see we'll get some similarity.",
            "Some more complex similarity.",
            "After a certain point.",
            "What we want to assume instead is that we want to have a low dimensional feature representation so we can have some features and these features map our inputs our input space.",
            "And essentially there are a few features which are relevant important for our classification or regression.",
            "And we also want to assume some similarity between these features, so.",
            "These features should be equally important, more or less across the tasks.",
            "Otherwise we won't have.",
            "We want to be able to exploit the similarities between the different tasks in to do multi task learning.",
            "For example, the tasks could be completely separate otherwise.",
            "Although there are the scenarios where you have, you could have."
        ],
        [
            "Maybe groups of task of features which are relevant for different tasks.",
            "In this case we have a more simple scenario.",
            "I think that hunger is a wrong file, yeah?",
            "And I.",
            "Maybe maybe another directory?",
            "Yeah.",
            "Who is your favorite?",
            "Maybe?",
            "Pencil and paper.",
            "There is this one.",
            "I think the beginning is the same pretty much.",
            "Thanks.",
            "That's the one you gave me this morning.",
            "I'm.",
            "Yeah, this is different.",
            "Let me kiss.",
            "Can I just?",
            "And just put it, put the memory stick again, OK?",
            "Fixer.",
            "Maybe you can open it straight from there.",
            "I see.",
            "What does fuck?",
            "Yeah, yeah, this is.",
            "OK, so maybe I can make it from here.",
            "OK, so."
        ],
        [
            "So let me backtrack.",
            "So here I also added the notation for the features, so these are the features.",
            "So these V3 are the limitation I used before.",
            "These are the features.",
            "Vision hi, I will assume I have D features, but in reality will will force them to be smaller than to be a smaller number than three which are really important features.",
            "But we will see how to do this later and these are the functions we have so the functions are linear combinations of the features.",
            "And these these coefficients here are important.",
            "'cause this value VIT where I is correspond to the feature and correspond to the task.",
            "So these coefficients tell you how important is it feature.",
            "If it's large, it's the future is important.",
            "And I can put all these importance.",
            "Especially.",
            "We talk about binary and not regression.",
            "Yeah, I don't.",
            "I don't really want to worry about this.",
            "So I use this matrix with A to put all the importances of the features together.",
            "So on the on the Rose you have the features.",
            "The feature index on the columns you have the tasks and rose."
        ],
        [
            "There's a guy I superscript I will be will denote the importance of the of its features across the tasks.",
            "The columns denote the 80, a subscript, D denote the importances for its task.",
            "Across all the features.",
            "And this is a notation I will use now.",
            "The question is, what are assumptions are good assumptions for this.",
            "For this matrix assumptions that somehow correspond to the data we have?",
            "And these."
        ],
        [
            "This is the data, so now I have the same ones, the same two as before.",
            "But I also want to have a convex formulation for this problem.",
            "So this will be a nice feature to have.",
            "To get a convex problem.",
            "And I think essentially want to start by.",
            "Re phrasing these two this data into into the matrix notation.",
            "Of the previous matrix."
        ],
        [
            "Of this matrix.",
            "That the two properties I want can be formulated like this, so most most Rd should be 0 or almost approximately 0, which means that they have a small set of features because as we said, its role corresponds to its feature.",
            "And across across the tasks for its feature, I want to have some kind of uniformity, so I don't want to have big differences for its feature across the tasks, because I said I'm interested in a simple scenario, not that simple scenario, but I'm not interested in a very complex scenario where you have different groupings across the tasks.",
            "Another another thing that will be up.",
            "We be enforced by this is that these?",
            "The ordering of the of the importance of the features across it.",
            "Along the features.",
            "Should be the same across tasks.",
            "So column wise the order should be should go from top to bottom in a similar way.",
            "Face similar.",
            "You don't quantify exactly here, not here later.",
            "Wanted it to be exactly the same order of and I don't know.",
            "I won't quantify this, but.",
            "I want to say that since they will be similar column wise, the ordering will be similar to so.",
            "Yeah, I won't quantify the order in.",
            "Yeah, it's it's more intuitive that we want to have the one feature that is important for for task one should be equal, should be the most important feature for all tasks.",
            "So the first model most important feature.",
            "Should be the most important feature for all tasks, and the second most important, so it's an it's an assumption.",
            "It might be good for some cases, might be might not be good for some cases.",
            "But I don't.",
            "I don't force this order.",
            "Directly and maybe maybe this is not a very good picture, but more or less it's this kind of matrix.",
            "We want to have.",
            "So to quantify this.",
            "Kind of matrix.",
            "I want to have.",
            "I use this one norm with two one mean."
        ],
        [
            "I have a mixed norm, so I first take 1 two norm across the rows and then I take the one norm of the resulting vector.",
            "So this is the.",
            "The formal definition.",
            "But if you see what happens is.",
            "You have this.",
            "To normal on this roll.",
            "So for each row, each feature you can get your computer to Norm and then you compute in one norm which is the sum of all these.",
            "Two norms.",
            "It is the norm.",
            "Basically will give you the importance.",
            "Cumulative importance of the feature if you include all the tasks together so that these norms.",
            "Here the two norms should be ordered according to the feature importances, and essentially what we want is to have a sum which is small, because that will mean we have we have many zeros and few few significant."
        ],
        [
            "Features.",
            "So this is what we want we want.",
            "Then many roles to be to be almost 0.",
            "And these will be induced by the fact that we have in one norm across along the features.",
            "And also we want this uniformity across tasks which is induced by the two norm along the along the task.",
            "Use similarity, I mean it would induce mean if you permute one of those vectors will not change.",
            "That won't change the tuner.",
            "If you permute you mean they say one of their old, the entrance at all.",
            "Yeah, it doesn't, so there's nothing in that constraint that's going to try and.",
            "Yeah, so for its role.",
            "Yeah, we we want to have uniformity on their own.",
            "So we don't care about the rest of the Rose.",
            "Two rows right?",
            "Yeah, if they have exactly the same vector, obviously then the two norms are the same.",
            "Yeah, if you now commute, so maybe there is sort of some entries here than non 0 sum interested to 0 right now you can mute them.",
            "You get the same to you in person.",
            "You can make sense.",
            "Not if you.",
            "I mean, I'm just thinking not having the whole row just mute the entries in this thing.",
            "Movies showing show up arrows.",
            "Measure.",
            "I don't see what reports forcing the.",
            "Sorry, you mean if I have two hours and I show up the Rose.",
            "Thank you previous life destination.",
            "Go back to the one that had the nice picture.",
            "So now your argument is that."
        ],
        [
            "Your take the first, you know two rows in that first block where there are based on yes.",
            "And you're saying, OK, they have the similar two norm, right?",
            "Those take two rows in that first block, right?",
            "I'm not saying that they should have the same similar to Norm, so I'm saying that on this row if I take this role, the First off by itself, I want some uniformity on the values I don't want, so I don't want to read think very red thing here and very blue thing there on their own in its role will be small.",
            "For every role?",
            "Yeah, I want this one.",
            "Yeah, so maybe I should clarify.",
            "Should've clarified that.",
            "So it should be permutation invariant in this sense.",
            "Actually it's not clear why you want to have this.",
            "Maybe you want to have another."
        ],
        [
            "Another criterion, but then you probably need to change this norm here.",
            "So I'm not sure what the good criterium is maybe, but this seems to be good for some purposes.",
            "Here.",
            "So this will 10."
        ],
        [
            "To.",
            "To favor similarity in on its role.",
            "And we know we know this due to the facts about the one Norman 2 model.",
            "So."
        ],
        [
            "Now we we just phrase this as regularization problem.",
            "We just have the same scenario as usually when we have some over terms.",
            "So here L is a convex loss function, the wise are the outputs the size of the features.",
            "As we said, the 80s are there.",
            "These feature importances coefficients and this term the error term is essentially independent of the.",
            "From task between tasks, this is what makes the tasks somehow connected related.",
            "And it's easy to see that there's a convex problem, because this you have this convex to one norm.",
            "So this is a simple problem to solve, because essentially the features are fixed.",
            "So if you have fixed size you can solve this easily.",
            "We just I just plotted here this.",
            "National graphs for some datasets to show that this norm indeed does what we want in the sense that we get a low rank as we increase the regularization parameter, we will get a low rank solution.",
            "Low rank is only low rank, but it will have many zeros on the Rose, but she had.",
            "I just bought the rank approximate rank of the solution.",
            "So indeed, when we when we have this non we it does minimize the rank of this matrix.",
            "But this is not yeah.",
            "Got much information from it.",
            "Yeah so this is Gon line this is their ankles.",
            "Yeah, very much.",
            "In the left lobe, only two rows which are not getting an equal to 0, which means that only two important feature which is shared across all the tasks.",
            "Yeah, these are two different datasets, so one is toy one is real.",
            "So the numbers here do not correspond of course.",
            "But in the in the limit, when gamma is very large you see in console towards one."
        ],
        [
            "So just in a special case of this is L1 regularization be cause for one task, you just have the L1 norm.",
            "Let's we know that this is something that favors Park City and there has been work on that.",
            "So the idea is that it approximates.",
            "This solution of this this combinatorial optimization problem, where the solution is a vector of many many zeros and few non zeros.",
            "OK, so."
        ],
        [
            "This is not the as we said.",
            "This is easy because it's a convex problem, but what we really want to do is to learn the features, so that's a harder problem to allow for the features to be learned and to be in a certain class.",
            "So we start by the scenario of linear or so normal features.",
            "This is what we will discuss about.",
            "Because this will give us a nicer problem and it's a simple initial scenario.",
            "So this is just the I rewrote the problem here, but now you see that you have this U which is the matrix of the features.",
            "So you are the columns of mate of matrix U.",
            "And they use a normal matrix orthogonal matrix.",
            "So this is clearly not non convex becauses you have this constraint and you have this.",
            "Store variables together in this error term.",
            "But we can do.",
            "We can do an easy formulation of this."
        ],
        [
            "Problem so there is basically to to combine these two variables.",
            "And we do it by these two variables.",
            "So WO essentialism.",
            "Is the usual W. We have regularization.",
            "It acts on the input space.",
            "The is matrix that combines both the features and feature importance coefficients.",
            "So somehow if you bundle these together you get a similar problem.",
            "And it's easy to see that we sort of assume that this one normal on a will force A to have many zeros.",
            "So what happens with W is the solution will have a low rank because you rotate a by a unitary matrix.",
            "So we expect to have a low rank solution.",
            "This is a problem we get, so now we have this similar error term within that."
        ],
        [
            "News.",
            "And we have this new norm, this new regularizer, but it depends on both WND.",
            "So it's a kind of more complex regularizer.",
            "We also have some constraints which don't allow this regularizer to be very, very small.",
            "To be 0.",
            "So there then the connection.",
            "The relationship between the tasks is it all appears here 'cause they're terms independent between the tasks.",
            "He said this term forces the WS to be be somehow related.",
            "And it's also it's not hard to prove that this may be surprisingly, this is convex.",
            "This problem is convex cause under these constraints this Nicola Riser is a convex zonal convex in WNT.",
            "So to solve this problem, we can maybe we can do some different methods."
        ],
        [
            "We chose to do some alternating approach since it's convex in both W&D we can minimize.",
            "It is not for both.",
            "It is for both, yeah.",
            "Yeah, it's a bit.",
            "It's not very obvious of this site, but.",
            "OK, well if you take the Hessian order then.",
            "So if you do some linear algebra.",
            "Some people have those colors.",
            "They make it send it into the programming which this and this is not because you have the inverse.",
            "But it is convex, but you can do an interior point method.",
            "Not it's not an SDP, but.",
            "You could do an interior point method because you have a constraint convex problem, right?",
            "So I guess there are some.",
            "Yeah, I'm just checking so one approach would be to maybe to put this in a solver.",
            "I don't know if there is a solver who can handle this kind of.",
            "Function but.",
            "When you say it's not me, definitely mean you mean it's not semi definite has written now or it can't be put into a semi definite form.",
            "Well, perhaps for some loss function it can be actually.",
            "Maybe you can delete it and but if you have this inverse, it's not.",
            "Well.",
            "I mean, if you didn't have the inverse, I think it should.",
            "It could be for certain loss functions, but here I'm not sure whether it is for any.",
            "I'm not sure, please.",
            "I don't think it is.",
            "When maybe there is some loss function somewhere.",
            "Regularization time, as he said that there is a female minus one.",
            "Pretty complicated.",
            "Actually, this is complicated because it also involves both of the WMD.",
            "This kind of thing is not an accident, but maybe this is honestly.",
            "No, we haven't looked into this question really.",
            "Talked whether this is equivalent to an STD.",
            "I think it is for some of those factors.",
            "Anyways, varying this disorder there is low.",
            "Presented.",
            "You're also right, as he is not very useful.",
            "As dipping.",
            "As it because a limit on the number of links and image of the dimensionality.",
            "Or they're not very physical.",
            "Thank you.",
            "But if you are asking for SVM loss, it does seem to be an SDP.",
            "If that's the question.",
            "So basically it was surprising here is that there is an initial problem which is motivated by these.",
            "Sparsity, so let's features across.",
            "There's not complex.",
            "Formulation of it, which is complex, but we haven't in short shorts."
        ],
        [
            "Find.",
            "OK, so we could do this with another approach, but.",
            "We can also use a scalable approach which is minimize over the, then minimize or W and continue iterate.",
            "Which will take you to the minimum, of course, because it's a convex jungle convex problem.",
            "Show"
        ],
        [
            "It's a bit like a super combined, combining supervised and unsupervised steps, so with respect to W, you do the standard supervised learning, an SVM learning or squared loss, regularization and so on.",
            "You do this in dependently over varieties so.",
            "You do this 40 tasks.",
            "And then you, in the second step you learn that the D in a sense.",
            "So you like doing some grouping or correlating of tasks together.",
            "Using this formula, it's because it's you don't even need to.",
            "To use an algorithm for this step, you is an analytical solution and you just compute this using singular value decomposition.",
            "So this converges.",
            "Resolution for WMD.",
            "This fixed.",
            "Regression problems yes yes.",
            "Then if you face and once you fix that video for thismessage.com survey parameters and then aggressively function.",
            "The secret formula for me might be only the 2nd.",
            "Yeah, so so if you said it's independent the appears only here.",
            "So for the second step you just need to minimize this so the formula is simple so the last function does not appear here for in the second step in the first step you fix the so that over to four different T the terms break apart so they are independent and you can do this independently.",
            "Is that reasonable?",
            "Steps also involve some kind of correlation between their ability to correlation?",
            "Fast.",
            "Well, this actually this is a.",
            "This is a trace norm.",
            "This thing at the bottom and this is related to the singular value decomposition of W. So it's it's a bit like.",
            "Sort of.",
            "Correlating the W matrix that is the solution.",
            "Or you're saying what is that?",
            "This is the solution for this step.",
            "You mean, yeah, this is for solving minimizing this term with respect to D. Subject I mean you've got static, some constraints, yes.",
            "Yes she is 1 or less less than one.",
            "But you're saying that is the solution in there, or you can just write it down.",
            "You don't actually have to go do this, no, no, because you you can prove it.",
            "The paper and write down the exact solution, so these are easy.",
            "Part is easy after you have the solution because you have this exact solution.",
            "But here you need to do the standard learning.",
            "So we tried this with some toy data.",
            "One toy data set and one."
        ],
        [
            "Real data set so that data set is.",
            "We wanted to have.",
            "Some tasks, 200 tasks which are sampled from a Gaussian distribution and they were more or less similar.",
            "So then the coefficients of the features are, we assumed simply identity features and for this story data set.",
            "And the coefficients favored only the five, the five features, and all the rest of the features were completely unimportant, so we have zero coefficients for all the rest of the features.",
            "Only 5 features are important.",
            "We put some different samples with these coefficients with different variances.",
            "We had the five examples, but task and 200 tax tasks overall and the output is linear and we had some noise.",
            "We also drew the inputs from zero one uniformly.",
            "So we expect to be able to."
        ],
        [
            "Filter out the body relevant features and to be able to learn only.",
            "The fact that only 5 features are important for this problem.",
            "This shows the test error versus the dimensionally with of the input.",
            "So 45.",
            "It means that we have 5.",
            "Features and no irrelevant features.",
            "For 25, we have 20 relevant features, so it gets harder.",
            "The more irrelevant features we had, but we always get a better, much better performance if we increase the number of tasks.",
            "So for 200 tasks we do much better than.",
            "The top line is it up, there is the completely and completely independent regularization problems and averaging the error, which of course is bad.",
            "But then if you put you do this algorithm with increasing numbers of tasks, you get better and better performance.",
            "So we see.",
            "Increasing number of training but no.",
            "And as an example, and now actually we.",
            "So we divided the same data with.",
            "What I did is I divided the same number of examples of.",
            "And just.",
            "Somehow it should improve again because it is just one time.",
            "It's 1 task, we have all the data right?",
            "So you should have very small test now.",
            "Well I provided.",
            "That is your choice if you want to learn the parameters by so just.",
            "They only said five points per task, right?",
            "So if you increase?",
            "No, but you have five points for effective plan with us.",
            "Then you can choose to learn it is 200 + 1 at a time independently.",
            "Sorry, just my points or you can choose to learn them by this algorithm with T = 200 next December.",
            "So independent means it's completely dependent.",
            "It's just the way of dividing the data.",
            "This is a stand at the point being the independent is going to be independent of the number of tasks, right?",
            "So.",
            "Whether it was tease ten 2500, you get the same line.",
            "Well, yeah.",
            "And we also did some.",
            "We checked what the features we get.",
            "Are we we see that we have less discrepancy from the expected features when we use 200 tasks, so we get a similar kind of graph.",
            "Or if we use a measure of the discrepancy of the feature of the features from the actual features.",
            "So this second experiment."
        ],
        [
            "Do some marketing survey data.",
            "Data set so they asked a number of consumers 100 persons to write the number of PC models.",
            "So they had eight PC models as a train set and the HPC model was characterized by 13 inputs.",
            "Input attributes, which were Hilo binary and they correspond to the price the CPU.",
            "Technical characteristics etc.",
            "And then the output is an integer which shows how how likely it is to purchase for a person to process in the computer.",
            "So here we have."
        ],
        [
            "Test error and as we increase the task so we get the lower the curve to go down so we get what we expected at the performance improves within.",
            "With increasing number of tasks.",
            "What is interesting is we get one one really important feature.",
            "This is the eigenvalues of the D matrix, which corresponds to how many important features there are and.",
            "What the relevant, the relative importance of this feature seems so here we have, and I have sorted eigenvalues, so this is a very important feature and the rest are far less important.",
            "By the way, for running independent regularization tasks we regularization algorithms, we get a much higher error in this problem.",
            "So clearly multitask helps.",
            "But what is interesting is that this important single most important feature has some nice interpretation in this case.",
            "So it says that.",
            "You have this is the feature and these are the input attributes, so these are the."
        ],
        [
            "RAM, CPU, etc.",
            "What happens is that you have a very high.",
            "I coefficient.",
            "At the price and you have less high but important coefficients, negative coefficients for CPU, CDROM, and.",
            "From which are the technical characteristics so that people seem to weigh these to against each other and give important surprise?",
            "So summarize"
        ],
        [
            "Then we tried to do multitask feature learning, so we want to impose a low dimensional feature representation.",
            "We want somehow to have these features importances preserved across the tasks so that we have some similarities between the tasks.",
            "We can formulate this as a convex problem which converge with global solution with an alternating algorithm.",
            "And we get a low rank solution which involves a few salient features.",
            "Now we can.",
            "Maybe we can do this in the future for more complicated scenarios, so we could have nonlinear features, or we could have.",
            "Some more."
        ],
        [
            "Complex relationships in the Matrix A between the features.",
            "The important features across the tasks.",
            "Maybe also we can investigate what the connection could be to Bayesian methods or hierarchical methods.",
            "Any questions?",
            "And you had a.",
            "You had a D. And yet again, I'm right in this information.",
            "So how would you choose?",
            "Together we will fix this or we cross validate in the experiments with cross validate.",
            "But in the theoretical formulation we we consider it fixed.",
            "Because you could also submit in the deep perhaps.",
            "Yeah, well.",
            "OK.",
            "Trace of these.",
            "Yeah, we don't want to do that 'cause.",
            "OK, so we don't want to.",
            "To put this inside because then.",
            "You minimize over the gamma.",
            "Put the comma in the D and.",
            "And then do you want to have some constraint?",
            "Do you see that patients have some constraint like this again?",
            "Yeah, I don't know bout about this.",
            "If it's make, it would make sense.",
            "So this thing that that."
        ],
        [
            "I don't quite understand it.",
            "If you would formulate it in kind of amazing way you would.",
            "You wouldn't have to set this camera with consolidation would also learn it I guess.",
            "Yeah, well we would cross validation you.",
            "It's like Latin.",
            "Yeah, but you would learn it in quite the same way as you learn to deal.",
            "OK, so this final stand, why you kind of separated.",
            "So you learn kind of the kind of weather we think would be kind of.",
            "Regular Weber kind of course.",
            "Influence between tells that Gamma.",
            "Apparently you cannot learn.",
            "Yeah, I don't.",
            "I don't know whether it's a good idea to put it in.",
            "Just pushing gamma into the.",
            "Well, I could do it.",
            "I mean mathematically, like there's nothing that prevents me to do it if I have a constraint here, but.",
            "Maybe something good idea 'cause the?",
            "Then the purpose of the is different from the purpose of gamma, right?",
            "So gamma.",
            "Is a tradeoff between error and.",
            "The simplest of functions, but.",
            "We will favor this regularization.",
            "And then the regularizer will favor fellow regularizer, right?",
            "So probably will get overfitting.",
            "I mean my impression is this matter, check this.",
            "We don't have other ways.",
            "Back in Sunday.",
            "Patient yeah, I think it's a.",
            "It's essentially the 19th.",
            "Is that alright?",
            "So just to check in here this color.",
            "This is kind of related to one night there, except in my case is not convex.",
            "I think here you think the key is the modification essentially is not true there.",
            "Worked out well.",
            "Boys are like trees.",
            "D smart now and everything is kind of LSD.",
            "Seems to me that that mask off.",
            "But from what the model I have, which is not far less, is more like L0 in that case.",
            "Manipulator is actually speak better.",
            "Yeah, I think I think there's Joshua this orginality constraint somewhere right in your model.",
            "The important issue the issue is.",
            "Change trace the to decent power Frieza.",
            "Other than that, I think people may be smaller than one.",
            "It's not complex.",
            "That is probably closer to 1, closer to principal component analysis.",
            "Choosing the truth smaller, I think it's an important, maybe an important difference is that you.",
            "You fix this.",
            "You assume my low rank and with a fixed rank right?",
            "And then you cross validate over this rank, but I think if you fix it.",
            "This is similar to trade you feel confident race condition to be traced the like 0 null space D to the power centers in a normal of deposit the sum of eigenvalue.",
            "Right now you essentially the sum of eigenvalues one normal.",
            "It is there now.",
            "Yeah yeah it's I think I think this is a difference.",
            "Well, you said they were not, yes.",
            "This question in the practical experiments how quickly was the convergence when you did that?",
            "Like convergence, fast to the yeah.",
            "So the problem with this method or potential problem could be that you have this minimize over one, then over the other.",
            "So going orthogonally is not always very good idea.",
            "But here we have to variables essentially, so we move across W and then across the so on.",
            "So it's not that much of a problem and in practice it seems to go.",
            "Converts very fast and as with these also.",
            "As we did for the second step.",
            "So this is a good.",
            "There's a good algorithm for that.",
            "Which you wrote down the solution.",
            "What you mean by the second step?",
            "This concept, I mean that it's easy to compute this becausw."
        ],
        [
            "We have this as with the algorithms with is a good algorithm, yes.",
            "Yeah.",
            "So numerically it's stable.",
            "Typically do.",
            "10 maybe 50 something like that.",
            "So the main main.",
            "My work is there is there and it's proportional to TI guess.",
            "If you go back to the Contacts formulation.",
            "He said if you look at the regularizer."
        ],
        [
            "Just consider the class of all those W's.",
            "We're at.",
            "Where is founded?",
            "Or any of those pairs W Cindy where this is bounded by some constant that would seem to be equivalent.",
            "Just a complicated way of writing the trace norm of W to be bounded.",
            "Yeah, actually it is equivalent to the training, so they have the final say.",
            "Not quite recently sat the same balance equivalent to its equivalent."
        ],
        [
            "This problem so so this is attritional.",
            "So you can solve over the WS using the trace norm.",
            "But there are some.",
            "Even on smoothness there, so I don't know how good it is to how I can do this.",
            "Very, very nicely.",
            "I mean to minimize.",
            "It's normal.",
            "Virginia station here at work.",
            "Perhaps this is wrong because it's just simpler.",
            "Yeah, that's probably simpler to you.",
            "Going to to look for that.",
            "Also, this is this relates to the low rank as I said.",
            "Anything?",
            "What is different in the motivation, naturism and one evaluation is a very.",
            "Jenna problem, so let's see how it is extended to.",
            "But then the result is that is another which has some similar flavor with.",
            "Is working properly filtering by Schreiber and also the work of talk.",
            "I'll donate.",
            "Yeah.",
            "Also, another payment asked me where they had this idea alternatingly.",
            "Do some work by zooming in line right here.",
            "Is answered by step I CA N type algorithms notified show and then you cluster structure function.",
            "Any other questions?",
            "OK thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is work with Massey and Fairview new.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And so this is an approach of multi for multi task learning based on doing some kind of having some kind of assumptions about the feature space and some the assumption that we can embed into a low dimensional feature space.",
                    "label": 0
                },
                {
                    "sent": "So we want to have a convex formulation of this problem.",
                    "label": 0
                },
                {
                    "sent": "And this is the outline modernas.",
                    "label": 0
                },
                {
                    "sent": "Now the general multitask multitask learning problem is involves number of tasks where you want to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Explore the similarities between the different tasks and you want to pull a lot of examples.",
                    "label": 0
                },
                {
                    "sent": "And if your examples for its task and join them together in order to get better performance and there have been several applications of these.",
                    "label": 0
                },
                {
                    "sent": "Of this scenario.",
                    "label": 0
                },
                {
                    "sent": "In this case we have.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this, is our notation more or less?",
                    "label": 0
                },
                {
                    "sent": "So we have the tasks.",
                    "label": 0
                },
                {
                    "sent": "We have an examples for task.",
                    "label": 0
                },
                {
                    "sent": "So it's task might have different examples from another task, so we don't assume this.",
                    "label": 0
                },
                {
                    "sent": "The input space is the input.",
                    "label": 0
                },
                {
                    "sent": "The examples are the same, but we assume for simplicity we have the same number of examples, but we could say we have different numbers of examples.",
                    "label": 0
                },
                {
                    "sent": "And we want to estimate the function safety so one function for its.",
                    "label": 0
                },
                {
                    "sent": "For its task now we will assume we won't assume something on these functions directly, but there will be some.",
                    "label": 0
                },
                {
                    "sent": "We'll see we'll get some similarity.",
                    "label": 0
                },
                {
                    "sent": "Some more complex similarity.",
                    "label": 0
                },
                {
                    "sent": "After a certain point.",
                    "label": 0
                },
                {
                    "sent": "What we want to assume instead is that we want to have a low dimensional feature representation so we can have some features and these features map our inputs our input space.",
                    "label": 0
                },
                {
                    "sent": "And essentially there are a few features which are relevant important for our classification or regression.",
                    "label": 0
                },
                {
                    "sent": "And we also want to assume some similarity between these features, so.",
                    "label": 0
                },
                {
                    "sent": "These features should be equally important, more or less across the tasks.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we won't have.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to exploit the similarities between the different tasks in to do multi task learning.",
                    "label": 0
                },
                {
                    "sent": "For example, the tasks could be completely separate otherwise.",
                    "label": 0
                },
                {
                    "sent": "Although there are the scenarios where you have, you could have.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe groups of task of features which are relevant for different tasks.",
                    "label": 0
                },
                {
                    "sent": "In this case we have a more simple scenario.",
                    "label": 0
                },
                {
                    "sent": "I think that hunger is a wrong file, yeah?",
                    "label": 0
                },
                {
                    "sent": "And I.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe another directory?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Who is your favorite?",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Pencil and paper.",
                    "label": 0
                },
                {
                    "sent": "There is this one.",
                    "label": 0
                },
                {
                    "sent": "I think the beginning is the same pretty much.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "That's the one you gave me this morning.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is different.",
                    "label": 0
                },
                {
                    "sent": "Let me kiss.",
                    "label": 0
                },
                {
                    "sent": "Can I just?",
                    "label": 0
                },
                {
                    "sent": "And just put it, put the memory stick again, OK?",
                    "label": 0
                },
                {
                    "sent": "Fixer.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can open it straight from there.",
                    "label": 0
                },
                {
                    "sent": "I see.",
                    "label": 0
                },
                {
                    "sent": "What does fuck?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, this is.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe I can make it from here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me backtrack.",
                    "label": 0
                },
                {
                    "sent": "So here I also added the notation for the features, so these are the features.",
                    "label": 0
                },
                {
                    "sent": "So these V3 are the limitation I used before.",
                    "label": 0
                },
                {
                    "sent": "These are the features.",
                    "label": 0
                },
                {
                    "sent": "Vision hi, I will assume I have D features, but in reality will will force them to be smaller than to be a smaller number than three which are really important features.",
                    "label": 0
                },
                {
                    "sent": "But we will see how to do this later and these are the functions we have so the functions are linear combinations of the features.",
                    "label": 0
                },
                {
                    "sent": "And these these coefficients here are important.",
                    "label": 0
                },
                {
                    "sent": "'cause this value VIT where I is correspond to the feature and correspond to the task.",
                    "label": 0
                },
                {
                    "sent": "So these coefficients tell you how important is it feature.",
                    "label": 0
                },
                {
                    "sent": "If it's large, it's the future is important.",
                    "label": 0
                },
                {
                    "sent": "And I can put all these importance.",
                    "label": 0
                },
                {
                    "sent": "Especially.",
                    "label": 0
                },
                {
                    "sent": "We talk about binary and not regression.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't really want to worry about this.",
                    "label": 0
                },
                {
                    "sent": "So I use this matrix with A to put all the importances of the features together.",
                    "label": 0
                },
                {
                    "sent": "So on the on the Rose you have the features.",
                    "label": 0
                },
                {
                    "sent": "The feature index on the columns you have the tasks and rose.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a guy I superscript I will be will denote the importance of the of its features across the tasks.",
                    "label": 0
                },
                {
                    "sent": "The columns denote the 80, a subscript, D denote the importances for its task.",
                    "label": 0
                },
                {
                    "sent": "Across all the features.",
                    "label": 0
                },
                {
                    "sent": "And this is a notation I will use now.",
                    "label": 0
                },
                {
                    "sent": "The question is, what are assumptions are good assumptions for this.",
                    "label": 0
                },
                {
                    "sent": "For this matrix assumptions that somehow correspond to the data we have?",
                    "label": 0
                },
                {
                    "sent": "And these.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the data, so now I have the same ones, the same two as before.",
                    "label": 0
                },
                {
                    "sent": "But I also want to have a convex formulation for this problem.",
                    "label": 0
                },
                {
                    "sent": "So this will be a nice feature to have.",
                    "label": 0
                },
                {
                    "sent": "To get a convex problem.",
                    "label": 0
                },
                {
                    "sent": "And I think essentially want to start by.",
                    "label": 0
                },
                {
                    "sent": "Re phrasing these two this data into into the matrix notation.",
                    "label": 0
                },
                {
                    "sent": "Of the previous matrix.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of this matrix.",
                    "label": 0
                },
                {
                    "sent": "That the two properties I want can be formulated like this, so most most Rd should be 0 or almost approximately 0, which means that they have a small set of features because as we said, its role corresponds to its feature.",
                    "label": 0
                },
                {
                    "sent": "And across across the tasks for its feature, I want to have some kind of uniformity, so I don't want to have big differences for its feature across the tasks, because I said I'm interested in a simple scenario, not that simple scenario, but I'm not interested in a very complex scenario where you have different groupings across the tasks.",
                    "label": 0
                },
                {
                    "sent": "Another another thing that will be up.",
                    "label": 0
                },
                {
                    "sent": "We be enforced by this is that these?",
                    "label": 0
                },
                {
                    "sent": "The ordering of the of the importance of the features across it.",
                    "label": 0
                },
                {
                    "sent": "Along the features.",
                    "label": 0
                },
                {
                    "sent": "Should be the same across tasks.",
                    "label": 1
                },
                {
                    "sent": "So column wise the order should be should go from top to bottom in a similar way.",
                    "label": 0
                },
                {
                    "sent": "Face similar.",
                    "label": 0
                },
                {
                    "sent": "You don't quantify exactly here, not here later.",
                    "label": 0
                },
                {
                    "sent": "Wanted it to be exactly the same order of and I don't know.",
                    "label": 0
                },
                {
                    "sent": "I won't quantify this, but.",
                    "label": 0
                },
                {
                    "sent": "I want to say that since they will be similar column wise, the ordering will be similar to so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I won't quantify the order in.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's more intuitive that we want to have the one feature that is important for for task one should be equal, should be the most important feature for all tasks.",
                    "label": 0
                },
                {
                    "sent": "So the first model most important feature.",
                    "label": 0
                },
                {
                    "sent": "Should be the most important feature for all tasks, and the second most important, so it's an it's an assumption.",
                    "label": 0
                },
                {
                    "sent": "It might be good for some cases, might be might not be good for some cases.",
                    "label": 0
                },
                {
                    "sent": "But I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't force this order.",
                    "label": 0
                },
                {
                    "sent": "Directly and maybe maybe this is not a very good picture, but more or less it's this kind of matrix.",
                    "label": 0
                },
                {
                    "sent": "We want to have.",
                    "label": 0
                },
                {
                    "sent": "So to quantify this.",
                    "label": 0
                },
                {
                    "sent": "Kind of matrix.",
                    "label": 0
                },
                {
                    "sent": "I want to have.",
                    "label": 0
                },
                {
                    "sent": "I use this one norm with two one mean.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a mixed norm, so I first take 1 two norm across the rows and then I take the one norm of the resulting vector.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "The formal definition.",
                    "label": 0
                },
                {
                    "sent": "But if you see what happens is.",
                    "label": 0
                },
                {
                    "sent": "You have this.",
                    "label": 0
                },
                {
                    "sent": "To normal on this roll.",
                    "label": 0
                },
                {
                    "sent": "So for each row, each feature you can get your computer to Norm and then you compute in one norm which is the sum of all these.",
                    "label": 0
                },
                {
                    "sent": "Two norms.",
                    "label": 0
                },
                {
                    "sent": "It is the norm.",
                    "label": 0
                },
                {
                    "sent": "Basically will give you the importance.",
                    "label": 0
                },
                {
                    "sent": "Cumulative importance of the feature if you include all the tasks together so that these norms.",
                    "label": 0
                },
                {
                    "sent": "Here the two norms should be ordered according to the feature importances, and essentially what we want is to have a sum which is small, because that will mean we have we have many zeros and few few significant.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Features.",
                    "label": 0
                },
                {
                    "sent": "So this is what we want we want.",
                    "label": 0
                },
                {
                    "sent": "Then many roles to be to be almost 0.",
                    "label": 0
                },
                {
                    "sent": "And these will be induced by the fact that we have in one norm across along the features.",
                    "label": 0
                },
                {
                    "sent": "And also we want this uniformity across tasks which is induced by the two norm along the along the task.",
                    "label": 0
                },
                {
                    "sent": "Use similarity, I mean it would induce mean if you permute one of those vectors will not change.",
                    "label": 0
                },
                {
                    "sent": "That won't change the tuner.",
                    "label": 0
                },
                {
                    "sent": "If you permute you mean they say one of their old, the entrance at all.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it doesn't, so there's nothing in that constraint that's going to try and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so for its role.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we we want to have uniformity on their own.",
                    "label": 0
                },
                {
                    "sent": "So we don't care about the rest of the Rose.",
                    "label": 0
                },
                {
                    "sent": "Two rows right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, if they have exactly the same vector, obviously then the two norms are the same.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you now commute, so maybe there is sort of some entries here than non 0 sum interested to 0 right now you can mute them.",
                    "label": 0
                },
                {
                    "sent": "You get the same to you in person.",
                    "label": 0
                },
                {
                    "sent": "You can make sense.",
                    "label": 0
                },
                {
                    "sent": "Not if you.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm just thinking not having the whole row just mute the entries in this thing.",
                    "label": 0
                },
                {
                    "sent": "Movies showing show up arrows.",
                    "label": 0
                },
                {
                    "sent": "Measure.",
                    "label": 0
                },
                {
                    "sent": "I don't see what reports forcing the.",
                    "label": 0
                },
                {
                    "sent": "Sorry, you mean if I have two hours and I show up the Rose.",
                    "label": 0
                },
                {
                    "sent": "Thank you previous life destination.",
                    "label": 0
                },
                {
                    "sent": "Go back to the one that had the nice picture.",
                    "label": 0
                },
                {
                    "sent": "So now your argument is that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your take the first, you know two rows in that first block where there are based on yes.",
                    "label": 0
                },
                {
                    "sent": "And you're saying, OK, they have the similar two norm, right?",
                    "label": 0
                },
                {
                    "sent": "Those take two rows in that first block, right?",
                    "label": 0
                },
                {
                    "sent": "I'm not saying that they should have the same similar to Norm, so I'm saying that on this row if I take this role, the First off by itself, I want some uniformity on the values I don't want, so I don't want to read think very red thing here and very blue thing there on their own in its role will be small.",
                    "label": 0
                },
                {
                    "sent": "For every role?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I want this one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so maybe I should clarify.",
                    "label": 0
                },
                {
                    "sent": "Should've clarified that.",
                    "label": 0
                },
                {
                    "sent": "So it should be permutation invariant in this sense.",
                    "label": 0
                },
                {
                    "sent": "Actually it's not clear why you want to have this.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to have another.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another criterion, but then you probably need to change this norm here.",
                    "label": 0
                },
                {
                    "sent": "So I'm not sure what the good criterium is maybe, but this seems to be good for some purposes.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So this will 10.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To.",
                    "label": 0
                },
                {
                    "sent": "To favor similarity in on its role.",
                    "label": 0
                },
                {
                    "sent": "And we know we know this due to the facts about the one Norman 2 model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we we just phrase this as regularization problem.",
                    "label": 0
                },
                {
                    "sent": "We just have the same scenario as usually when we have some over terms.",
                    "label": 0
                },
                {
                    "sent": "So here L is a convex loss function, the wise are the outputs the size of the features.",
                    "label": 0
                },
                {
                    "sent": "As we said, the 80s are there.",
                    "label": 0
                },
                {
                    "sent": "These feature importances coefficients and this term the error term is essentially independent of the.",
                    "label": 0
                },
                {
                    "sent": "From task between tasks, this is what makes the tasks somehow connected related.",
                    "label": 0
                },
                {
                    "sent": "And it's easy to see that there's a convex problem, because this you have this convex to one norm.",
                    "label": 1
                },
                {
                    "sent": "So this is a simple problem to solve, because essentially the features are fixed.",
                    "label": 1
                },
                {
                    "sent": "So if you have fixed size you can solve this easily.",
                    "label": 0
                },
                {
                    "sent": "We just I just plotted here this.",
                    "label": 0
                },
                {
                    "sent": "National graphs for some datasets to show that this norm indeed does what we want in the sense that we get a low rank as we increase the regularization parameter, we will get a low rank solution.",
                    "label": 1
                },
                {
                    "sent": "Low rank is only low rank, but it will have many zeros on the Rose, but she had.",
                    "label": 0
                },
                {
                    "sent": "I just bought the rank approximate rank of the solution.",
                    "label": 0
                },
                {
                    "sent": "So indeed, when we when we have this non we it does minimize the rank of this matrix.",
                    "label": 0
                },
                {
                    "sent": "But this is not yeah.",
                    "label": 0
                },
                {
                    "sent": "Got much information from it.",
                    "label": 0
                },
                {
                    "sent": "Yeah so this is Gon line this is their ankles.",
                    "label": 0
                },
                {
                    "sent": "Yeah, very much.",
                    "label": 0
                },
                {
                    "sent": "In the left lobe, only two rows which are not getting an equal to 0, which means that only two important feature which is shared across all the tasks.",
                    "label": 1
                },
                {
                    "sent": "Yeah, these are two different datasets, so one is toy one is real.",
                    "label": 0
                },
                {
                    "sent": "So the numbers here do not correspond of course.",
                    "label": 0
                },
                {
                    "sent": "But in the in the limit, when gamma is very large you see in console towards one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just in a special case of this is L1 regularization be cause for one task, you just have the L1 norm.",
                    "label": 1
                },
                {
                    "sent": "Let's we know that this is something that favors Park City and there has been work on that.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that it approximates.",
                    "label": 0
                },
                {
                    "sent": "This solution of this this combinatorial optimization problem, where the solution is a vector of many many zeros and few non zeros.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is not the as we said.",
                    "label": 0
                },
                {
                    "sent": "This is easy because it's a convex problem, but what we really want to do is to learn the features, so that's a harder problem to allow for the features to be learned and to be in a certain class.",
                    "label": 1
                },
                {
                    "sent": "So we start by the scenario of linear or so normal features.",
                    "label": 0
                },
                {
                    "sent": "This is what we will discuss about.",
                    "label": 0
                },
                {
                    "sent": "Because this will give us a nicer problem and it's a simple initial scenario.",
                    "label": 0
                },
                {
                    "sent": "So this is just the I rewrote the problem here, but now you see that you have this U which is the matrix of the features.",
                    "label": 0
                },
                {
                    "sent": "So you are the columns of mate of matrix U.",
                    "label": 0
                },
                {
                    "sent": "And they use a normal matrix orthogonal matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is clearly not non convex becauses you have this constraint and you have this.",
                    "label": 0
                },
                {
                    "sent": "Store variables together in this error term.",
                    "label": 0
                },
                {
                    "sent": "But we can do.",
                    "label": 0
                },
                {
                    "sent": "We can do an easy formulation of this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem so there is basically to to combine these two variables.",
                    "label": 0
                },
                {
                    "sent": "And we do it by these two variables.",
                    "label": 0
                },
                {
                    "sent": "So WO essentialism.",
                    "label": 0
                },
                {
                    "sent": "Is the usual W. We have regularization.",
                    "label": 0
                },
                {
                    "sent": "It acts on the input space.",
                    "label": 0
                },
                {
                    "sent": "The is matrix that combines both the features and feature importance coefficients.",
                    "label": 0
                },
                {
                    "sent": "So somehow if you bundle these together you get a similar problem.",
                    "label": 0
                },
                {
                    "sent": "And it's easy to see that we sort of assume that this one normal on a will force A to have many zeros.",
                    "label": 0
                },
                {
                    "sent": "So what happens with W is the solution will have a low rank because you rotate a by a unitary matrix.",
                    "label": 0
                },
                {
                    "sent": "So we expect to have a low rank solution.",
                    "label": 0
                },
                {
                    "sent": "This is a problem we get, so now we have this similar error term within that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "News.",
                    "label": 0
                },
                {
                    "sent": "And we have this new norm, this new regularizer, but it depends on both WND.",
                    "label": 0
                },
                {
                    "sent": "So it's a kind of more complex regularizer.",
                    "label": 0
                },
                {
                    "sent": "We also have some constraints which don't allow this regularizer to be very, very small.",
                    "label": 0
                },
                {
                    "sent": "To be 0.",
                    "label": 0
                },
                {
                    "sent": "So there then the connection.",
                    "label": 0
                },
                {
                    "sent": "The relationship between the tasks is it all appears here 'cause they're terms independent between the tasks.",
                    "label": 1
                },
                {
                    "sent": "He said this term forces the WS to be be somehow related.",
                    "label": 0
                },
                {
                    "sent": "And it's also it's not hard to prove that this may be surprisingly, this is convex.",
                    "label": 1
                },
                {
                    "sent": "This problem is convex cause under these constraints this Nicola Riser is a convex zonal convex in WNT.",
                    "label": 0
                },
                {
                    "sent": "So to solve this problem, we can maybe we can do some different methods.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We chose to do some alternating approach since it's convex in both W&D we can minimize.",
                    "label": 0
                },
                {
                    "sent": "It is not for both.",
                    "label": 0
                },
                {
                    "sent": "It is for both, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a bit.",
                    "label": 0
                },
                {
                    "sent": "It's not very obvious of this site, but.",
                    "label": 0
                },
                {
                    "sent": "OK, well if you take the Hessian order then.",
                    "label": 0
                },
                {
                    "sent": "So if you do some linear algebra.",
                    "label": 0
                },
                {
                    "sent": "Some people have those colors.",
                    "label": 0
                },
                {
                    "sent": "They make it send it into the programming which this and this is not because you have the inverse.",
                    "label": 0
                },
                {
                    "sent": "But it is convex, but you can do an interior point method.",
                    "label": 0
                },
                {
                    "sent": "Not it's not an SDP, but.",
                    "label": 0
                },
                {
                    "sent": "You could do an interior point method because you have a constraint convex problem, right?",
                    "label": 0
                },
                {
                    "sent": "So I guess there are some.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm just checking so one approach would be to maybe to put this in a solver.",
                    "label": 0
                },
                {
                    "sent": "I don't know if there is a solver who can handle this kind of.",
                    "label": 0
                },
                {
                    "sent": "Function but.",
                    "label": 0
                },
                {
                    "sent": "When you say it's not me, definitely mean you mean it's not semi definite has written now or it can't be put into a semi definite form.",
                    "label": 0
                },
                {
                    "sent": "Well, perhaps for some loss function it can be actually.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can delete it and but if you have this inverse, it's not.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you didn't have the inverse, I think it should.",
                    "label": 0
                },
                {
                    "sent": "It could be for certain loss functions, but here I'm not sure whether it is for any.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure, please.",
                    "label": 0
                },
                {
                    "sent": "I don't think it is.",
                    "label": 0
                },
                {
                    "sent": "When maybe there is some loss function somewhere.",
                    "label": 0
                },
                {
                    "sent": "Regularization time, as he said that there is a female minus one.",
                    "label": 0
                },
                {
                    "sent": "Pretty complicated.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is complicated because it also involves both of the WMD.",
                    "label": 0
                },
                {
                    "sent": "This kind of thing is not an accident, but maybe this is honestly.",
                    "label": 0
                },
                {
                    "sent": "No, we haven't looked into this question really.",
                    "label": 0
                },
                {
                    "sent": "Talked whether this is equivalent to an STD.",
                    "label": 0
                },
                {
                    "sent": "I think it is for some of those factors.",
                    "label": 0
                },
                {
                    "sent": "Anyways, varying this disorder there is low.",
                    "label": 0
                },
                {
                    "sent": "Presented.",
                    "label": 0
                },
                {
                    "sent": "You're also right, as he is not very useful.",
                    "label": 0
                },
                {
                    "sent": "As dipping.",
                    "label": 0
                },
                {
                    "sent": "As it because a limit on the number of links and image of the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Or they're not very physical.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "But if you are asking for SVM loss, it does seem to be an SDP.",
                    "label": 0
                },
                {
                    "sent": "If that's the question.",
                    "label": 0
                },
                {
                    "sent": "So basically it was surprising here is that there is an initial problem which is motivated by these.",
                    "label": 0
                },
                {
                    "sent": "Sparsity, so let's features across.",
                    "label": 0
                },
                {
                    "sent": "There's not complex.",
                    "label": 0
                },
                {
                    "sent": "Formulation of it, which is complex, but we haven't in short shorts.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find.",
                    "label": 0
                },
                {
                    "sent": "OK, so we could do this with another approach, but.",
                    "label": 0
                },
                {
                    "sent": "We can also use a scalable approach which is minimize over the, then minimize or W and continue iterate.",
                    "label": 0
                },
                {
                    "sent": "Which will take you to the minimum, of course, because it's a convex jungle convex problem.",
                    "label": 0
                },
                {
                    "sent": "Show",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a bit like a super combined, combining supervised and unsupervised steps, so with respect to W, you do the standard supervised learning, an SVM learning or squared loss, regularization and so on.",
                    "label": 0
                },
                {
                    "sent": "You do this in dependently over varieties so.",
                    "label": 0
                },
                {
                    "sent": "You do this 40 tasks.",
                    "label": 0
                },
                {
                    "sent": "And then you, in the second step you learn that the D in a sense.",
                    "label": 1
                },
                {
                    "sent": "So you like doing some grouping or correlating of tasks together.",
                    "label": 1
                },
                {
                    "sent": "Using this formula, it's because it's you don't even need to.",
                    "label": 0
                },
                {
                    "sent": "To use an algorithm for this step, you is an analytical solution and you just compute this using singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "So this converges.",
                    "label": 0
                },
                {
                    "sent": "Resolution for WMD.",
                    "label": 0
                },
                {
                    "sent": "This fixed.",
                    "label": 0
                },
                {
                    "sent": "Regression problems yes yes.",
                    "label": 0
                },
                {
                    "sent": "Then if you face and once you fix that video for thismessage.com survey parameters and then aggressively function.",
                    "label": 0
                },
                {
                    "sent": "The secret formula for me might be only the 2nd.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so if you said it's independent the appears only here.",
                    "label": 0
                },
                {
                    "sent": "So for the second step you just need to minimize this so the formula is simple so the last function does not appear here for in the second step in the first step you fix the so that over to four different T the terms break apart so they are independent and you can do this independently.",
                    "label": 0
                },
                {
                    "sent": "Is that reasonable?",
                    "label": 0
                },
                {
                    "sent": "Steps also involve some kind of correlation between their ability to correlation?",
                    "label": 0
                },
                {
                    "sent": "Fast.",
                    "label": 0
                },
                {
                    "sent": "Well, this actually this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a trace norm.",
                    "label": 0
                },
                {
                    "sent": "This thing at the bottom and this is related to the singular value decomposition of W. So it's it's a bit like.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "Correlating the W matrix that is the solution.",
                    "label": 0
                },
                {
                    "sent": "Or you're saying what is that?",
                    "label": 0
                },
                {
                    "sent": "This is the solution for this step.",
                    "label": 0
                },
                {
                    "sent": "You mean, yeah, this is for solving minimizing this term with respect to D. Subject I mean you've got static, some constraints, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes she is 1 or less less than one.",
                    "label": 0
                },
                {
                    "sent": "But you're saying that is the solution in there, or you can just write it down.",
                    "label": 0
                },
                {
                    "sent": "You don't actually have to go do this, no, no, because you you can prove it.",
                    "label": 0
                },
                {
                    "sent": "The paper and write down the exact solution, so these are easy.",
                    "label": 0
                },
                {
                    "sent": "Part is easy after you have the solution because you have this exact solution.",
                    "label": 0
                },
                {
                    "sent": "But here you need to do the standard learning.",
                    "label": 0
                },
                {
                    "sent": "So we tried this with some toy data.",
                    "label": 0
                },
                {
                    "sent": "One toy data set and one.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Real data set so that data set is.",
                    "label": 0
                },
                {
                    "sent": "We wanted to have.",
                    "label": 0
                },
                {
                    "sent": "Some tasks, 200 tasks which are sampled from a Gaussian distribution and they were more or less similar.",
                    "label": 0
                },
                {
                    "sent": "So then the coefficients of the features are, we assumed simply identity features and for this story data set.",
                    "label": 1
                },
                {
                    "sent": "And the coefficients favored only the five, the five features, and all the rest of the features were completely unimportant, so we have zero coefficients for all the rest of the features.",
                    "label": 0
                },
                {
                    "sent": "Only 5 features are important.",
                    "label": 0
                },
                {
                    "sent": "We put some different samples with these coefficients with different variances.",
                    "label": 0
                },
                {
                    "sent": "We had the five examples, but task and 200 tax tasks overall and the output is linear and we had some noise.",
                    "label": 0
                },
                {
                    "sent": "We also drew the inputs from zero one uniformly.",
                    "label": 0
                },
                {
                    "sent": "So we expect to be able to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Filter out the body relevant features and to be able to learn only.",
                    "label": 0
                },
                {
                    "sent": "The fact that only 5 features are important for this problem.",
                    "label": 1
                },
                {
                    "sent": "This shows the test error versus the dimensionally with of the input.",
                    "label": 1
                },
                {
                    "sent": "So 45.",
                    "label": 0
                },
                {
                    "sent": "It means that we have 5.",
                    "label": 0
                },
                {
                    "sent": "Features and no irrelevant features.",
                    "label": 0
                },
                {
                    "sent": "For 25, we have 20 relevant features, so it gets harder.",
                    "label": 0
                },
                {
                    "sent": "The more irrelevant features we had, but we always get a better, much better performance if we increase the number of tasks.",
                    "label": 0
                },
                {
                    "sent": "So for 200 tasks we do much better than.",
                    "label": 0
                },
                {
                    "sent": "The top line is it up, there is the completely and completely independent regularization problems and averaging the error, which of course is bad.",
                    "label": 0
                },
                {
                    "sent": "But then if you put you do this algorithm with increasing numbers of tasks, you get better and better performance.",
                    "label": 0
                },
                {
                    "sent": "So we see.",
                    "label": 0
                },
                {
                    "sent": "Increasing number of training but no.",
                    "label": 0
                },
                {
                    "sent": "And as an example, and now actually we.",
                    "label": 0
                },
                {
                    "sent": "So we divided the same data with.",
                    "label": 0
                },
                {
                    "sent": "What I did is I divided the same number of examples of.",
                    "label": 0
                },
                {
                    "sent": "And just.",
                    "label": 0
                },
                {
                    "sent": "Somehow it should improve again because it is just one time.",
                    "label": 0
                },
                {
                    "sent": "It's 1 task, we have all the data right?",
                    "label": 0
                },
                {
                    "sent": "So you should have very small test now.",
                    "label": 0
                },
                {
                    "sent": "Well I provided.",
                    "label": 0
                },
                {
                    "sent": "That is your choice if you want to learn the parameters by so just.",
                    "label": 0
                },
                {
                    "sent": "They only said five points per task, right?",
                    "label": 0
                },
                {
                    "sent": "So if you increase?",
                    "label": 0
                },
                {
                    "sent": "No, but you have five points for effective plan with us.",
                    "label": 1
                },
                {
                    "sent": "Then you can choose to learn it is 200 + 1 at a time independently.",
                    "label": 0
                },
                {
                    "sent": "Sorry, just my points or you can choose to learn them by this algorithm with T = 200 next December.",
                    "label": 0
                },
                {
                    "sent": "So independent means it's completely dependent.",
                    "label": 0
                },
                {
                    "sent": "It's just the way of dividing the data.",
                    "label": 0
                },
                {
                    "sent": "This is a stand at the point being the independent is going to be independent of the number of tasks, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Whether it was tease ten 2500, you get the same line.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah.",
                    "label": 0
                },
                {
                    "sent": "And we also did some.",
                    "label": 1
                },
                {
                    "sent": "We checked what the features we get.",
                    "label": 0
                },
                {
                    "sent": "Are we we see that we have less discrepancy from the expected features when we use 200 tasks, so we get a similar kind of graph.",
                    "label": 0
                },
                {
                    "sent": "Or if we use a measure of the discrepancy of the feature of the features from the actual features.",
                    "label": 0
                },
                {
                    "sent": "So this second experiment.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do some marketing survey data.",
                    "label": 0
                },
                {
                    "sent": "Data set so they asked a number of consumers 100 persons to write the number of PC models.",
                    "label": 0
                },
                {
                    "sent": "So they had eight PC models as a train set and the HPC model was characterized by 13 inputs.",
                    "label": 0
                },
                {
                    "sent": "Input attributes, which were Hilo binary and they correspond to the price the CPU.",
                    "label": 0
                },
                {
                    "sent": "Technical characteristics etc.",
                    "label": 0
                },
                {
                    "sent": "And then the output is an integer which shows how how likely it is to purchase for a person to process in the computer.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Test error and as we increase the task so we get the lower the curve to go down so we get what we expected at the performance improves within.",
                    "label": 0
                },
                {
                    "sent": "With increasing number of tasks.",
                    "label": 0
                },
                {
                    "sent": "What is interesting is we get one one really important feature.",
                    "label": 0
                },
                {
                    "sent": "This is the eigenvalues of the D matrix, which corresponds to how many important features there are and.",
                    "label": 0
                },
                {
                    "sent": "What the relevant, the relative importance of this feature seems so here we have, and I have sorted eigenvalues, so this is a very important feature and the rest are far less important.",
                    "label": 0
                },
                {
                    "sent": "By the way, for running independent regularization tasks we regularization algorithms, we get a much higher error in this problem.",
                    "label": 0
                },
                {
                    "sent": "So clearly multitask helps.",
                    "label": 0
                },
                {
                    "sent": "But what is interesting is that this important single most important feature has some nice interpretation in this case.",
                    "label": 1
                },
                {
                    "sent": "So it says that.",
                    "label": 0
                },
                {
                    "sent": "You have this is the feature and these are the input attributes, so these are the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "RAM, CPU, etc.",
                    "label": 0
                },
                {
                    "sent": "What happens is that you have a very high.",
                    "label": 0
                },
                {
                    "sent": "I coefficient.",
                    "label": 0
                },
                {
                    "sent": "At the price and you have less high but important coefficients, negative coefficients for CPU, CDROM, and.",
                    "label": 0
                },
                {
                    "sent": "From which are the technical characteristics so that people seem to weigh these to against each other and give important surprise?",
                    "label": 0
                },
                {
                    "sent": "So summarize",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we tried to do multitask feature learning, so we want to impose a low dimensional feature representation.",
                    "label": 1
                },
                {
                    "sent": "We want somehow to have these features importances preserved across the tasks so that we have some similarities between the tasks.",
                    "label": 1
                },
                {
                    "sent": "We can formulate this as a convex problem which converge with global solution with an alternating algorithm.",
                    "label": 1
                },
                {
                    "sent": "And we get a low rank solution which involves a few salient features.",
                    "label": 0
                },
                {
                    "sent": "Now we can.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can do this in the future for more complicated scenarios, so we could have nonlinear features, or we could have.",
                    "label": 0
                },
                {
                    "sent": "Some more.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Complex relationships in the Matrix A between the features.",
                    "label": 0
                },
                {
                    "sent": "The important features across the tasks.",
                    "label": 0
                },
                {
                    "sent": "Maybe also we can investigate what the connection could be to Bayesian methods or hierarchical methods.",
                    "label": 1
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "And you had a.",
                    "label": 0
                },
                {
                    "sent": "You had a D. And yet again, I'm right in this information.",
                    "label": 0
                },
                {
                    "sent": "So how would you choose?",
                    "label": 0
                },
                {
                    "sent": "Together we will fix this or we cross validate in the experiments with cross validate.",
                    "label": 0
                },
                {
                    "sent": "But in the theoretical formulation we we consider it fixed.",
                    "label": 0
                },
                {
                    "sent": "Because you could also submit in the deep perhaps.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Trace of these.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we don't want to do that 'cause.",
                    "label": 0
                },
                {
                    "sent": "OK, so we don't want to.",
                    "label": 0
                },
                {
                    "sent": "To put this inside because then.",
                    "label": 0
                },
                {
                    "sent": "You minimize over the gamma.",
                    "label": 0
                },
                {
                    "sent": "Put the comma in the D and.",
                    "label": 0
                },
                {
                    "sent": "And then do you want to have some constraint?",
                    "label": 0
                },
                {
                    "sent": "Do you see that patients have some constraint like this again?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know bout about this.",
                    "label": 0
                },
                {
                    "sent": "If it's make, it would make sense.",
                    "label": 0
                },
                {
                    "sent": "So this thing that that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't quite understand it.",
                    "label": 0
                },
                {
                    "sent": "If you would formulate it in kind of amazing way you would.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't have to set this camera with consolidation would also learn it I guess.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well we would cross validation you.",
                    "label": 0
                },
                {
                    "sent": "It's like Latin.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you would learn it in quite the same way as you learn to deal.",
                    "label": 0
                },
                {
                    "sent": "OK, so this final stand, why you kind of separated.",
                    "label": 0
                },
                {
                    "sent": "So you learn kind of the kind of weather we think would be kind of.",
                    "label": 0
                },
                {
                    "sent": "Regular Weber kind of course.",
                    "label": 0
                },
                {
                    "sent": "Influence between tells that Gamma.",
                    "label": 0
                },
                {
                    "sent": "Apparently you cannot learn.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether it's a good idea to put it in.",
                    "label": 0
                },
                {
                    "sent": "Just pushing gamma into the.",
                    "label": 0
                },
                {
                    "sent": "Well, I could do it.",
                    "label": 0
                },
                {
                    "sent": "I mean mathematically, like there's nothing that prevents me to do it if I have a constraint here, but.",
                    "label": 0
                },
                {
                    "sent": "Maybe something good idea 'cause the?",
                    "label": 0
                },
                {
                    "sent": "Then the purpose of the is different from the purpose of gamma, right?",
                    "label": 0
                },
                {
                    "sent": "So gamma.",
                    "label": 0
                },
                {
                    "sent": "Is a tradeoff between error and.",
                    "label": 0
                },
                {
                    "sent": "The simplest of functions, but.",
                    "label": 0
                },
                {
                    "sent": "We will favor this regularization.",
                    "label": 0
                },
                {
                    "sent": "And then the regularizer will favor fellow regularizer, right?",
                    "label": 0
                },
                {
                    "sent": "So probably will get overfitting.",
                    "label": 0
                },
                {
                    "sent": "I mean my impression is this matter, check this.",
                    "label": 0
                },
                {
                    "sent": "We don't have other ways.",
                    "label": 0
                },
                {
                    "sent": "Back in Sunday.",
                    "label": 0
                },
                {
                    "sent": "Patient yeah, I think it's a.",
                    "label": 0
                },
                {
                    "sent": "It's essentially the 19th.",
                    "label": 0
                },
                {
                    "sent": "Is that alright?",
                    "label": 0
                },
                {
                    "sent": "So just to check in here this color.",
                    "label": 0
                },
                {
                    "sent": "This is kind of related to one night there, except in my case is not convex.",
                    "label": 0
                },
                {
                    "sent": "I think here you think the key is the modification essentially is not true there.",
                    "label": 0
                },
                {
                    "sent": "Worked out well.",
                    "label": 0
                },
                {
                    "sent": "Boys are like trees.",
                    "label": 0
                },
                {
                    "sent": "D smart now and everything is kind of LSD.",
                    "label": 0
                },
                {
                    "sent": "Seems to me that that mask off.",
                    "label": 0
                },
                {
                    "sent": "But from what the model I have, which is not far less, is more like L0 in that case.",
                    "label": 0
                },
                {
                    "sent": "Manipulator is actually speak better.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think I think there's Joshua this orginality constraint somewhere right in your model.",
                    "label": 0
                },
                {
                    "sent": "The important issue the issue is.",
                    "label": 0
                },
                {
                    "sent": "Change trace the to decent power Frieza.",
                    "label": 0
                },
                {
                    "sent": "Other than that, I think people may be smaller than one.",
                    "label": 0
                },
                {
                    "sent": "It's not complex.",
                    "label": 0
                },
                {
                    "sent": "That is probably closer to 1, closer to principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "Choosing the truth smaller, I think it's an important, maybe an important difference is that you.",
                    "label": 0
                },
                {
                    "sent": "You fix this.",
                    "label": 0
                },
                {
                    "sent": "You assume my low rank and with a fixed rank right?",
                    "label": 0
                },
                {
                    "sent": "And then you cross validate over this rank, but I think if you fix it.",
                    "label": 0
                },
                {
                    "sent": "This is similar to trade you feel confident race condition to be traced the like 0 null space D to the power centers in a normal of deposit the sum of eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Right now you essentially the sum of eigenvalues one normal.",
                    "label": 0
                },
                {
                    "sent": "It is there now.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah it's I think I think this is a difference.",
                    "label": 0
                },
                {
                    "sent": "Well, you said they were not, yes.",
                    "label": 0
                },
                {
                    "sent": "This question in the practical experiments how quickly was the convergence when you did that?",
                    "label": 0
                },
                {
                    "sent": "Like convergence, fast to the yeah.",
                    "label": 0
                },
                {
                    "sent": "So the problem with this method or potential problem could be that you have this minimize over one, then over the other.",
                    "label": 0
                },
                {
                    "sent": "So going orthogonally is not always very good idea.",
                    "label": 0
                },
                {
                    "sent": "But here we have to variables essentially, so we move across W and then across the so on.",
                    "label": 0
                },
                {
                    "sent": "So it's not that much of a problem and in practice it seems to go.",
                    "label": 0
                },
                {
                    "sent": "Converts very fast and as with these also.",
                    "label": 0
                },
                {
                    "sent": "As we did for the second step.",
                    "label": 0
                },
                {
                    "sent": "So this is a good.",
                    "label": 0
                },
                {
                    "sent": "There's a good algorithm for that.",
                    "label": 0
                },
                {
                    "sent": "Which you wrote down the solution.",
                    "label": 0
                },
                {
                    "sent": "What you mean by the second step?",
                    "label": 0
                },
                {
                    "sent": "This concept, I mean that it's easy to compute this becausw.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have this as with the algorithms with is a good algorithm, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So numerically it's stable.",
                    "label": 0
                },
                {
                    "sent": "Typically do.",
                    "label": 0
                },
                {
                    "sent": "10 maybe 50 something like that.",
                    "label": 0
                },
                {
                    "sent": "So the main main.",
                    "label": 0
                },
                {
                    "sent": "My work is there is there and it's proportional to TI guess.",
                    "label": 0
                },
                {
                    "sent": "If you go back to the Contacts formulation.",
                    "label": 0
                },
                {
                    "sent": "He said if you look at the regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just consider the class of all those W's.",
                    "label": 0
                },
                {
                    "sent": "We're at.",
                    "label": 0
                },
                {
                    "sent": "Where is founded?",
                    "label": 0
                },
                {
                    "sent": "Or any of those pairs W Cindy where this is bounded by some constant that would seem to be equivalent.",
                    "label": 0
                },
                {
                    "sent": "Just a complicated way of writing the trace norm of W to be bounded.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually it is equivalent to the training, so they have the final say.",
                    "label": 0
                },
                {
                    "sent": "Not quite recently sat the same balance equivalent to its equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem so so this is attritional.",
                    "label": 0
                },
                {
                    "sent": "So you can solve over the WS using the trace norm.",
                    "label": 1
                },
                {
                    "sent": "But there are some.",
                    "label": 0
                },
                {
                    "sent": "Even on smoothness there, so I don't know how good it is to how I can do this.",
                    "label": 0
                },
                {
                    "sent": "Very, very nicely.",
                    "label": 0
                },
                {
                    "sent": "I mean to minimize.",
                    "label": 0
                },
                {
                    "sent": "It's normal.",
                    "label": 0
                },
                {
                    "sent": "Virginia station here at work.",
                    "label": 0
                },
                {
                    "sent": "Perhaps this is wrong because it's just simpler.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's probably simpler to you.",
                    "label": 0
                },
                {
                    "sent": "Going to to look for that.",
                    "label": 0
                },
                {
                    "sent": "Also, this is this relates to the low rank as I said.",
                    "label": 0
                },
                {
                    "sent": "Anything?",
                    "label": 0
                },
                {
                    "sent": "What is different in the motivation, naturism and one evaluation is a very.",
                    "label": 0
                },
                {
                    "sent": "Jenna problem, so let's see how it is extended to.",
                    "label": 0
                },
                {
                    "sent": "But then the result is that is another which has some similar flavor with.",
                    "label": 0
                },
                {
                    "sent": "Is working properly filtering by Schreiber and also the work of talk.",
                    "label": 0
                },
                {
                    "sent": "I'll donate.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Also, another payment asked me where they had this idea alternatingly.",
                    "label": 0
                },
                {
                    "sent": "Do some work by zooming in line right here.",
                    "label": 0
                },
                {
                    "sent": "Is answered by step I CA N type algorithms notified show and then you cluster structure function.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                }
            ]
        }
    }
}