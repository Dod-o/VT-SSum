{
    "id": "gai6ty3jx2gbqaqqx7js27g5cclsug2r",
    "title": "Dynamical Binary Latent Variable Models for 3D Human Pose Tracking",
    "info": {
        "author": [
            "Graham Taylor, School of Engineering, University of Guelph"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Computer Vision->Motion and Tracking"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_taylor_dblv/",
    "segmentation": [
        [
            "So, as we've seen in the previous talk, monocular pros tracking is a very difficult problem.",
            "And this is due to inherent ambiguities.",
            "The high dimensional nature of pose and missing and.",
            "Occludin measurements.",
            "And so.",
            "To address these challenges, prior models have played an important role in advancing state of the art tracking."
        ],
        [
            "There have been a number of different prior models proposed in the literature, and typically these models have dealt with single activities, and ideally we'd like to be able to deal with multiple activities as well as dealing with transitions.",
            "Another problem with a lot of the previous work has been that typically these models are trained on relatively small data corpora.",
            "A popular method of our family of models is the GP LVM in GDM models.",
            "Has spoken up in the previous work and these are Ncube to train where N is the number of data data examples.",
            "And so in."
        ],
        [
            "This work, we're proposing a new type of dynamical model called the implicit mixture of conditional restricted Boltzmann machines.",
            "And this model has a number of appealing properties which makes it suitable for use as a prior in tracking.",
            "Our model is applicable to very large datasets.",
            "Stylistic diversity as well as being trained on multiple activities in different subjects."
        ],
        [
            "Are made our model can be trained.",
            "Supervised when activity labels are available and if these labels aren't available, we can train the model unsupervised and in this latter case we're able to automatically discover coherent motion segments with which others have termed movings."
        ],
        [
            "And perhaps most importantly, we're able to perform simultaneous influence inference over pose, an activity, and we've seen already in a few talks this morning.",
            "How useful context is in determining estimation, and so it's obvious that knowing something about the pose can help us infer the activity.",
            "But it's also clear that knowing something about the activity can help us do pose estimation, so This is why we're keen on simultaneously performing inference over both pose an activity.",
            "And so first I'll give an overview of Bayesian filtering tracking with the IMC RBM model, and I'll just go over some of the notation that will be used throughout my talk."
        ],
        [
            "So first we're given a series of image features which we're going to be calling Y.",
            "And what we'd like to do is perform."
        ],
        [
            "Inference over three posts, and you can think of these as joint angles and we're going to do this by X and to be able to constrain the pose.",
            "We're going to introduce two types of latent variables."
        ],
        [
            "The first is Q, which is a discrete activity variable and this takes on one of K states.",
            "This is pictured on the left.",
            "Looks like a stoplight and we also have.",
            "A series of many binary latent variables which are shared among the activities, and these are a distributed representation of the pose in motion.",
            "And we train our prior model with motion capture data and when we're training the prior, we treat the poses observed.",
            "But when we're going to perform tracking, we're going to do joint inference over XQ&Z.",
            "Given the image features."
        ],
        [
            "Our prior model is based on a family of models called the restricted Boltzmann machine and this family of models contains a collection of observed variables which in our case are going to be continuous and represent pose or joint angles and also a collection of latent variables which are binary and we see here that each observed variable is connected to all of the latent variables and as well."
        ],
        [
            "Each latent variable is connected to all of the observed variables, and this bipartite connectivity is the key to be able being able to perform efficient exact inference so conditioned on a setting of the observed variables, the latent variables are independent and vice versa, and so this allows us to do approximate learning via the contrastive divergent algorithm as well, and this family of models has received quite a lot of recent attention in the machine literate learning literature because they're stackable into building deep belief Nets."
        ],
        [
            "And for the remainder of this talk, I'm going to use this simplified vector representation.",
            "We're able to represent the full bipartite connectivity."
        ],
        [
            "The conditional restricted Boltzmann machine extends the RBM to be able to capture temporal dependencies.",
            "And this is done by SIM."
        ],
        [
            "Conditioning on the past previous observations.",
            "And we're going to use this notation XHT to denote the past history, so this could be one step history.",
            "Or it could be more."
        ],
        [
            "And the nice property of this, the PBM, is that conditioning on the past doesn't change easy inference and approximate learning."
        ],
        [
            "And this model has been proposed for motion synthesis and it's being able to capture a wide variety of different styles of motion.",
            "But here in this work we'd like to exploit the natural dependency of pose, an activity, and so we introduce a disc."
        ],
        [
            "Three component variable which is going to modulate the connections in the Sierra BM and basically set the effect of the RBM.",
            "So now that we've introduced this additional variable Q, our model is now defined as a joint distribution over QZ&X.",
            "Given the conditional history."
        ],
        [
            "And in tracking what we really need to do is to find a distribution over the pose given the past history of poses, this is a state evolution.",
            "And so we can arrive at this by marginalizing over the joint likelihood where we're marginally marginalizing over the latent variables Z&Q and we'll see here that this gives us something that's quite familiar.",
            "It looks like a mixture model."
        ],
        [
            "We have a mixing proportion which is defined by these latent variables Q.",
            "And."
        ],
        [
            "Each component of the mixture is a CRBN model.",
            "Now I do."
        ],
        [
            "We have time in this talk to go over a lot of the details about learning and inference in the model, but certainly I'd be happy to do this at the poster.",
            "But I'd like to just point out a few key advantages of the IMC PBM, so we're able to as we can with our BMS and CR BMS perform approximately earning efficiently via contrastive divergent's, and this is the same learning algorithm.",
            "Basically, for all these models, and while these are a relatively new algorithms, there are increasingly being used in machine learning envision an.",
            "There's a growing number of code samples and tutorials available on the web.",
            "And like some of the other prior models that have been considered, these models can be trained on millions of frames in just a few hours and this is reduced in minutes when using GPU's.",
            "And what's important here for doing pose estimation is that we can do Gibbs sampling to perform synthesis quite quickly.",
            "We can do 60 Hertz synthesis.",
            "And then we can also, as I pointed out earlier, train using activity labels or just do pure unsupervised learning."
        ],
        [
            "So, as we've seen in the previous talk, we're going to adopt A Bayesian filtering approach to performing tracking.",
            "And here we are ultimately interested in estimating what's called the filtering distribution.",
            "So this is a distribution over the pose given the previous history of image observations.",
            "And this can be decomposed into product of two terms."
        ],
        [
            "We have a likelihood term which we'll talk about a little bit later, and we also have a predictive distribution.",
            "And the."
        ],
        [
            "The distribution is decomposed recursively into two things.",
            "One is the dynamical model and the other is a posterior.",
            "At the previous time step."
        ],
        [
            "Now the predicted distribution.",
            "This integral here when we have nonlinear dynamics, Anna nonlinear likelihood term is intractable to evaluate analytically, and so we adopt standard form of discrete approximation to the posterior called the particle filter.",
            "So it's left here to define it.",
            "Basically the exact likelihood term that we're going to use as well as the type of dynamical model.",
            "Well, the dynamical model."
        ],
        [
            "Liz the IMC.",
            "GBM as I've already presented and this is defined by the marginal likelihood."
        ],
        [
            "We're going to use standard likelihoods based on edge and silhouette features that are commonly used in the literature.",
            "So now this."
        ],
        [
            "Tribe series of experiments that we've carried out in both the multiview and Monocular 3D tracking settings, and we're using the human Eve Adidas data set as was used in the previous talk, and human EV is nice because it contains both multiview and macular data, with synchronized mocap, and the mocap we're going to use for both learning the prior model training, the IMC PBM, and we're also going to use the mocap data to provide a quantitative evaluation, and we carry out comparisons.",
            "Against the baseline, which is annealed particle filter with smooth zero order dynamics, this is just basically Gaussian propagation.",
            "And also, we're going to carry out experiments to evaluate our approach against other state of the art methods, and performance will always be measured in average joint location error.",
            "So this is a 3D location."
        ],
        [
            "And so the first series of results I'll present here are carried out on the human evil walking data.",
            "I just want to point out that these are 3D results you saw in the last talk, and sort of the neighborhood of 1012.",
            "Those numbers those are referring to 2D pixels.",
            "These results in the previous talk were in the neighborhood of 100 and we've carried out these experiments looking at subject specific and non specific priors."
        ],
        [
            "And so what's important here is that the PBM and the IMC GBM model outperformed both the baseline and several state of the art methods, such as a GP LVM coordinated mixture of factor analyzers, and the motion correlation model.",
            "And what's most interesting?"
        ],
        [
            "Labs is that the MC RBM is able to Excel where we have large amounts of training data in non subject specific priors and we see that compared to the PBM, when we have a single subject we actually it's a more powerful model and tend to overfit, so the PBM does better in the single subject specific setting.",
            "Next we look at a more."
        ],
        [
            "Complicated example where we have a test scenario that has a subject walking and then transitioning to running and we train our models on both walking and jogging data, but they never observe transitions.",
            "And so when we apply this to view this transition situation, we see that the MC RBM significantly outperforms both baseline model in the PBM, and we've trained here on two different types of IMC GBM model.",
            "So the MC RBM 2L.",
            "That's a model with two components, and this has been trained with activity labels and the IMC Obtenu has 10 components, has been trained fully unsupervised.",
            "And the nice thing to see here."
        ],
        [
            "Is that when we train unsupervised without the use of activity labels, we're able to still perform relatively well?",
            "We don't degrade performance too much."
        ],
        [
            "And here's a plot over the entire sequence where the transitioning is happening at approximately frame 400, and what's worth noting here is that the baseline method, which is in blue generally has high variance throughout the sequence, and the standard CRBN, which is in red after the transition happens.",
            "It generally increases in variance as well, but the MC RBM variance are able to also minimize variance.",
            "I'll also note that two of the rows are marked with asterisks, and that's where we were repeatedly.",
            "Experiments with non subject specific priors and we show again that we were able to generalize in this scenario.",
            "So."
        ],
        [
            "Another byproduct of our approach, as I mentioned before, is that we can perform simultaneous activity recognition and pose estimation and so here we see where we've trained the IMC RBM, labeled with two activity variables, one corresponding to walking and the other to jogging.",
            "We can look at the posterior here throughout the sequence over this latent discrete variable Q, and except for right around the transition where there's some uncertainty as to whether it's walking or running, were able to perform inference over this activity reliably."
        ],
        [
            "And here's the more interesting setup where we have trained the model fully unsupervised and it's able to discover these atomic motion segments or movings.",
            "And here we've given it 10 components to work with and we see that it uses the first component and it's discovered that it represents part of The Walking cycle and the second component is being used to represent part of the jogging cycle and the remainder of these components generally are either walking specific or jogging specific.",
            "But we also see that some of these components are shared.",
            "Between the two different types of gate."
        ],
        [
            "And finally, this is the most challenging situation where we have monocular tracking with transitions, and here we apply an IMC GBM that's been trained with activity labels.",
            "Again, it hasn't seen any transitions especially trained on walking and jogging sequences alone.",
            "And we can see here in the video that we can do reliable tracking even in this challenging situation where both the baseline and the PBM fail.",
            "And here we're able to track reliably overall 3 camera views.",
            "And I'm showing the camera view to here."
        ],
        [
            "So in conclusion, I presented a new type of dynamical model for continuous and discrete dynamics.",
            "The model is the implicit mixture of conditional restricted Boltzmann machines.",
            "And we're able to learn our priors from large datasets consisting of diverse motion in many different subjects, and we use the same set of latent variables.",
            "These models have been seen to work effectively as a prior in Bayesian filtering filtering, and they've outperformed both the baseline and state of the art methods.",
            "And one nice property of the IMC GBM as is true with the PBM in CR VM, is that these models are stackable into deep hierarchical models, and so future work will look at building deep dynamical priors.",
            "Based on this IMC RBM framework.",
            "And thank you for your attention.",
            "Question.",
            "So we said we train small large number of training examples.",
            "Yep.",
            "German.",
            "Profits from this large number.",
            "Relations.",
            "Sure, so these experiments we generally trained in the order of 10s of thousands of frames, and that's what's provided generally with the human Eva data set.",
            "So because we've tested on human Eva, sort of as a benchmark against other methods, we've been confined to what's available in human Eva.",
            "Most of my experience with training on large datasets has been for motion synthesis, and that situation we're training on hundreds of thousands of frames, multiple activities, and certainly.",
            "The model's performance is degraded when you're only using a few 100 frames, which is what's typically done like GP LVM type models, and we're, you know, we're being able to generalize more, and we get to 10s of thousands or hundreds of thousands of frames.",
            "I just put that 1,000,000 number out there because it is possible with these types of models that are linear in the number of training examples.",
            "More questions.",
            "Well, I have one.",
            "What kind of advantages do you expect from having a deeper IBM?",
            "So in the picture that I just showed, I've shown this hypothetical model and where we train the lower level not using any type of label information where we were observing all the activities and they just the first level of latent features would then be generic features that could be used for all types of motion and then the upper layer would be able to specialize on particular specific gates.",
            "For example, and so if we look at different types of motion, different styles and so forth, there are parts of them that are shared between different activities, and so by building this hierarchy you think we could exploit on this unsupervised training the commonality's that are shared between different motion styles so so deeper is better than wider.",
            "So I mean I don't want to get into any philosophical arguments here, but certainly in my experience in training these models for motion synthesis, the quality.",
            "The quantitative or qualitative evaluation in terms of the quality of motion is definitely improved when I've used two or three layers as opposed to one.",
            "Anymore questions.",
            "OK, let's thank our speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, as we've seen in the previous talk, monocular pros tracking is a very difficult problem.",
                    "label": 0
                },
                {
                    "sent": "And this is due to inherent ambiguities.",
                    "label": 0
                },
                {
                    "sent": "The high dimensional nature of pose and missing and.",
                    "label": 0
                },
                {
                    "sent": "Occludin measurements.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "To address these challenges, prior models have played an important role in advancing state of the art tracking.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There have been a number of different prior models proposed in the literature, and typically these models have dealt with single activities, and ideally we'd like to be able to deal with multiple activities as well as dealing with transitions.",
                    "label": 0
                },
                {
                    "sent": "Another problem with a lot of the previous work has been that typically these models are trained on relatively small data corpora.",
                    "label": 0
                },
                {
                    "sent": "A popular method of our family of models is the GP LVM in GDM models.",
                    "label": 0
                },
                {
                    "sent": "Has spoken up in the previous work and these are Ncube to train where N is the number of data data examples.",
                    "label": 0
                },
                {
                    "sent": "And so in.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work, we're proposing a new type of dynamical model called the implicit mixture of conditional restricted Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "And this model has a number of appealing properties which makes it suitable for use as a prior in tracking.",
                    "label": 0
                },
                {
                    "sent": "Our model is applicable to very large datasets.",
                    "label": 1
                },
                {
                    "sent": "Stylistic diversity as well as being trained on multiple activities in different subjects.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are made our model can be trained.",
                    "label": 0
                },
                {
                    "sent": "Supervised when activity labels are available and if these labels aren't available, we can train the model unsupervised and in this latter case we're able to automatically discover coherent motion segments with which others have termed movings.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And perhaps most importantly, we're able to perform simultaneous influence inference over pose, an activity, and we've seen already in a few talks this morning.",
                    "label": 0
                },
                {
                    "sent": "How useful context is in determining estimation, and so it's obvious that knowing something about the pose can help us infer the activity.",
                    "label": 0
                },
                {
                    "sent": "But it's also clear that knowing something about the activity can help us do pose estimation, so This is why we're keen on simultaneously performing inference over both pose an activity.",
                    "label": 0
                },
                {
                    "sent": "And so first I'll give an overview of Bayesian filtering tracking with the IMC RBM model, and I'll just go over some of the notation that will be used throughout my talk.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first we're given a series of image features which we're going to be calling Y.",
                    "label": 0
                },
                {
                    "sent": "And what we'd like to do is perform.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inference over three posts, and you can think of these as joint angles and we're going to do this by X and to be able to constrain the pose.",
                    "label": 0
                },
                {
                    "sent": "We're going to introduce two types of latent variables.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first is Q, which is a discrete activity variable and this takes on one of K states.",
                    "label": 1
                },
                {
                    "sent": "This is pictured on the left.",
                    "label": 0
                },
                {
                    "sent": "Looks like a stoplight and we also have.",
                    "label": 0
                },
                {
                    "sent": "A series of many binary latent variables which are shared among the activities, and these are a distributed representation of the pose in motion.",
                    "label": 1
                },
                {
                    "sent": "And we train our prior model with motion capture data and when we're training the prior, we treat the poses observed.",
                    "label": 0
                },
                {
                    "sent": "But when we're going to perform tracking, we're going to do joint inference over XQ&Z.",
                    "label": 0
                },
                {
                    "sent": "Given the image features.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our prior model is based on a family of models called the restricted Boltzmann machine and this family of models contains a collection of observed variables which in our case are going to be continuous and represent pose or joint angles and also a collection of latent variables which are binary and we see here that each observed variable is connected to all of the latent variables and as well.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each latent variable is connected to all of the observed variables, and this bipartite connectivity is the key to be able being able to perform efficient exact inference so conditioned on a setting of the observed variables, the latent variables are independent and vice versa, and so this allows us to do approximate learning via the contrastive divergent algorithm as well, and this family of models has received quite a lot of recent attention in the machine literate learning literature because they're stackable into building deep belief Nets.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the remainder of this talk, I'm going to use this simplified vector representation.",
                    "label": 0
                },
                {
                    "sent": "We're able to represent the full bipartite connectivity.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The conditional restricted Boltzmann machine extends the RBM to be able to capture temporal dependencies.",
                    "label": 0
                },
                {
                    "sent": "And this is done by SIM.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conditioning on the past previous observations.",
                    "label": 1
                },
                {
                    "sent": "And we're going to use this notation XHT to denote the past history, so this could be one step history.",
                    "label": 0
                },
                {
                    "sent": "Or it could be more.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the nice property of this, the PBM, is that conditioning on the past doesn't change easy inference and approximate learning.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this model has been proposed for motion synthesis and it's being able to capture a wide variety of different styles of motion.",
                    "label": 0
                },
                {
                    "sent": "But here in this work we'd like to exploit the natural dependency of pose, an activity, and so we introduce a disc.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Three component variable which is going to modulate the connections in the Sierra BM and basically set the effect of the RBM.",
                    "label": 1
                },
                {
                    "sent": "So now that we've introduced this additional variable Q, our model is now defined as a joint distribution over QZ&X.",
                    "label": 0
                },
                {
                    "sent": "Given the conditional history.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in tracking what we really need to do is to find a distribution over the pose given the past history of poses, this is a state evolution.",
                    "label": 0
                },
                {
                    "sent": "And so we can arrive at this by marginalizing over the joint likelihood where we're marginally marginalizing over the latent variables Z&Q and we'll see here that this gives us something that's quite familiar.",
                    "label": 1
                },
                {
                    "sent": "It looks like a mixture model.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a mixing proportion which is defined by these latent variables Q.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each component of the mixture is a CRBN model.",
                    "label": 0
                },
                {
                    "sent": "Now I do.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have time in this talk to go over a lot of the details about learning and inference in the model, but certainly I'd be happy to do this at the poster.",
                    "label": 0
                },
                {
                    "sent": "But I'd like to just point out a few key advantages of the IMC PBM, so we're able to as we can with our BMS and CR BMS perform approximately earning efficiently via contrastive divergent's, and this is the same learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Basically, for all these models, and while these are a relatively new algorithms, there are increasingly being used in machine learning envision an.",
                    "label": 0
                },
                {
                    "sent": "There's a growing number of code samples and tutorials available on the web.",
                    "label": 1
                },
                {
                    "sent": "And like some of the other prior models that have been considered, these models can be trained on millions of frames in just a few hours and this is reduced in minutes when using GPU's.",
                    "label": 1
                },
                {
                    "sent": "And what's important here for doing pose estimation is that we can do Gibbs sampling to perform synthesis quite quickly.",
                    "label": 0
                },
                {
                    "sent": "We can do 60 Hertz synthesis.",
                    "label": 0
                },
                {
                    "sent": "And then we can also, as I pointed out earlier, train using activity labels or just do pure unsupervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, as we've seen in the previous talk, we're going to adopt A Bayesian filtering approach to performing tracking.",
                    "label": 1
                },
                {
                    "sent": "And here we are ultimately interested in estimating what's called the filtering distribution.",
                    "label": 1
                },
                {
                    "sent": "So this is a distribution over the pose given the previous history of image observations.",
                    "label": 0
                },
                {
                    "sent": "And this can be decomposed into product of two terms.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a likelihood term which we'll talk about a little bit later, and we also have a predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The distribution is decomposed recursively into two things.",
                    "label": 0
                },
                {
                    "sent": "One is the dynamical model and the other is a posterior.",
                    "label": 1
                },
                {
                    "sent": "At the previous time step.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the predicted distribution.",
                    "label": 0
                },
                {
                    "sent": "This integral here when we have nonlinear dynamics, Anna nonlinear likelihood term is intractable to evaluate analytically, and so we adopt standard form of discrete approximation to the posterior called the particle filter.",
                    "label": 1
                },
                {
                    "sent": "So it's left here to define it.",
                    "label": 0
                },
                {
                    "sent": "Basically the exact likelihood term that we're going to use as well as the type of dynamical model.",
                    "label": 1
                },
                {
                    "sent": "Well, the dynamical model.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Liz the IMC.",
                    "label": 0
                },
                {
                    "sent": "GBM as I've already presented and this is defined by the marginal likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to use standard likelihoods based on edge and silhouette features that are commonly used in the literature.",
                    "label": 0
                },
                {
                    "sent": "So now this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tribe series of experiments that we've carried out in both the multiview and Monocular 3D tracking settings, and we're using the human Eve Adidas data set as was used in the previous talk, and human EV is nice because it contains both multiview and macular data, with synchronized mocap, and the mocap we're going to use for both learning the prior model training, the IMC PBM, and we're also going to use the mocap data to provide a quantitative evaluation, and we carry out comparisons.",
                    "label": 1
                },
                {
                    "sent": "Against the baseline, which is annealed particle filter with smooth zero order dynamics, this is just basically Gaussian propagation.",
                    "label": 1
                },
                {
                    "sent": "And also, we're going to carry out experiments to evaluate our approach against other state of the art methods, and performance will always be measured in average joint location error.",
                    "label": 0
                },
                {
                    "sent": "So this is a 3D location.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the first series of results I'll present here are carried out on the human evil walking data.",
                    "label": 0
                },
                {
                    "sent": "I just want to point out that these are 3D results you saw in the last talk, and sort of the neighborhood of 1012.",
                    "label": 0
                },
                {
                    "sent": "Those numbers those are referring to 2D pixels.",
                    "label": 0
                },
                {
                    "sent": "These results in the previous talk were in the neighborhood of 100 and we've carried out these experiments looking at subject specific and non specific priors.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so what's important here is that the PBM and the IMC GBM model outperformed both the baseline and several state of the art methods, such as a GP LVM coordinated mixture of factor analyzers, and the motion correlation model.",
                    "label": 0
                },
                {
                    "sent": "And what's most interesting?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Labs is that the MC RBM is able to Excel where we have large amounts of training data in non subject specific priors and we see that compared to the PBM, when we have a single subject we actually it's a more powerful model and tend to overfit, so the PBM does better in the single subject specific setting.",
                    "label": 0
                },
                {
                    "sent": "Next we look at a more.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complicated example where we have a test scenario that has a subject walking and then transitioning to running and we train our models on both walking and jogging data, but they never observe transitions.",
                    "label": 0
                },
                {
                    "sent": "And so when we apply this to view this transition situation, we see that the MC RBM significantly outperforms both baseline model in the PBM, and we've trained here on two different types of IMC GBM model.",
                    "label": 0
                },
                {
                    "sent": "So the MC RBM 2L.",
                    "label": 0
                },
                {
                    "sent": "That's a model with two components, and this has been trained with activity labels and the IMC Obtenu has 10 components, has been trained fully unsupervised.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing to see here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that when we train unsupervised without the use of activity labels, we're able to still perform relatively well?",
                    "label": 0
                },
                {
                    "sent": "We don't degrade performance too much.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's a plot over the entire sequence where the transitioning is happening at approximately frame 400, and what's worth noting here is that the baseline method, which is in blue generally has high variance throughout the sequence, and the standard CRBN, which is in red after the transition happens.",
                    "label": 0
                },
                {
                    "sent": "It generally increases in variance as well, but the MC RBM variance are able to also minimize variance.",
                    "label": 0
                },
                {
                    "sent": "I'll also note that two of the rows are marked with asterisks, and that's where we were repeatedly.",
                    "label": 0
                },
                {
                    "sent": "Experiments with non subject specific priors and we show again that we were able to generalize in this scenario.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another byproduct of our approach, as I mentioned before, is that we can perform simultaneous activity recognition and pose estimation and so here we see where we've trained the IMC RBM, labeled with two activity variables, one corresponding to walking and the other to jogging.",
                    "label": 0
                },
                {
                    "sent": "We can look at the posterior here throughout the sequence over this latent discrete variable Q, and except for right around the transition where there's some uncertainty as to whether it's walking or running, were able to perform inference over this activity reliably.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's the more interesting setup where we have trained the model fully unsupervised and it's able to discover these atomic motion segments or movings.",
                    "label": 0
                },
                {
                    "sent": "And here we've given it 10 components to work with and we see that it uses the first component and it's discovered that it represents part of The Walking cycle and the second component is being used to represent part of the jogging cycle and the remainder of these components generally are either walking specific or jogging specific.",
                    "label": 0
                },
                {
                    "sent": "But we also see that some of these components are shared.",
                    "label": 0
                },
                {
                    "sent": "Between the two different types of gate.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, this is the most challenging situation where we have monocular tracking with transitions, and here we apply an IMC GBM that's been trained with activity labels.",
                    "label": 1
                },
                {
                    "sent": "Again, it hasn't seen any transitions especially trained on walking and jogging sequences alone.",
                    "label": 1
                },
                {
                    "sent": "And we can see here in the video that we can do reliable tracking even in this challenging situation where both the baseline and the PBM fail.",
                    "label": 0
                },
                {
                    "sent": "And here we're able to track reliably overall 3 camera views.",
                    "label": 0
                },
                {
                    "sent": "And I'm showing the camera view to here.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, I presented a new type of dynamical model for continuous and discrete dynamics.",
                    "label": 1
                },
                {
                    "sent": "The model is the implicit mixture of conditional restricted Boltzmann machines.",
                    "label": 1
                },
                {
                    "sent": "And we're able to learn our priors from large datasets consisting of diverse motion in many different subjects, and we use the same set of latent variables.",
                    "label": 0
                },
                {
                    "sent": "These models have been seen to work effectively as a prior in Bayesian filtering filtering, and they've outperformed both the baseline and state of the art methods.",
                    "label": 0
                },
                {
                    "sent": "And one nice property of the IMC GBM as is true with the PBM in CR VM, is that these models are stackable into deep hierarchical models, and so future work will look at building deep dynamical priors.",
                    "label": 0
                },
                {
                    "sent": "Based on this IMC RBM framework.",
                    "label": 0
                },
                {
                    "sent": "And thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So we said we train small large number of training examples.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "German.",
                    "label": 0
                },
                {
                    "sent": "Profits from this large number.",
                    "label": 0
                },
                {
                    "sent": "Relations.",
                    "label": 0
                },
                {
                    "sent": "Sure, so these experiments we generally trained in the order of 10s of thousands of frames, and that's what's provided generally with the human Eva data set.",
                    "label": 0
                },
                {
                    "sent": "So because we've tested on human Eva, sort of as a benchmark against other methods, we've been confined to what's available in human Eva.",
                    "label": 0
                },
                {
                    "sent": "Most of my experience with training on large datasets has been for motion synthesis, and that situation we're training on hundreds of thousands of frames, multiple activities, and certainly.",
                    "label": 0
                },
                {
                    "sent": "The model's performance is degraded when you're only using a few 100 frames, which is what's typically done like GP LVM type models, and we're, you know, we're being able to generalize more, and we get to 10s of thousands or hundreds of thousands of frames.",
                    "label": 0
                },
                {
                    "sent": "I just put that 1,000,000 number out there because it is possible with these types of models that are linear in the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Well, I have one.",
                    "label": 0
                },
                {
                    "sent": "What kind of advantages do you expect from having a deeper IBM?",
                    "label": 0
                },
                {
                    "sent": "So in the picture that I just showed, I've shown this hypothetical model and where we train the lower level not using any type of label information where we were observing all the activities and they just the first level of latent features would then be generic features that could be used for all types of motion and then the upper layer would be able to specialize on particular specific gates.",
                    "label": 0
                },
                {
                    "sent": "For example, and so if we look at different types of motion, different styles and so forth, there are parts of them that are shared between different activities, and so by building this hierarchy you think we could exploit on this unsupervised training the commonality's that are shared between different motion styles so so deeper is better than wider.",
                    "label": 0
                },
                {
                    "sent": "So I mean I don't want to get into any philosophical arguments here, but certainly in my experience in training these models for motion synthesis, the quality.",
                    "label": 0
                },
                {
                    "sent": "The quantitative or qualitative evaluation in terms of the quality of motion is definitely improved when I've used two or three layers as opposed to one.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank our speaker again.",
                    "label": 0
                }
            ]
        }
    }
}