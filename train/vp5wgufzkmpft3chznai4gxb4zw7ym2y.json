{
    "id": "vp5wgufzkmpft3chznai4gxb4zw7ym2y",
    "title": "How to Grow a Mind: Statistics, Structure and Abstraction",
    "info": {
        "author": [
            "Joshua B. Tenenbaum, Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/nips2010_tenenbaum_hgm/",
    "segmentation": [
        [
            "OK, thank you very much.",
            "It's a great pleasure to be here.",
            "So what I'm going to talk about today is how to grow a mind."
        ],
        [
            "Structure and abstraction and I want to start off by putting names and pictures of people who've been in our group.",
            "Graduate students and postdocs, and some who are still in our group.",
            "The people who have their actual pictures up there.",
            "I think our first authors on the papers that I'll talk about here, but the other people have also contributed in important ways.",
            "And really, this is their work.",
            "I'm just lucky to be able to think about it with them and."
        ],
        [
            "Think about it.",
            "Now, as Dave said, the goal of what we're trying to do here is really about a bidirectional connection between the fields of human learning and machine learning.",
            "We sometimes call it reverse engineering the mind, which means we want to understand how the mind works in our best engineering terms, the same framework we could use to build more intelligent systems in systems which are which do learning and inference in more human like ways.",
            "And we know we're successful when we have real bidirectional idea flow.",
            "Now."
        ],
        [
            "The big question that our research has been trying to address for pretty much as long as I've been working in this field, you could put like this.",
            "How does the mind get so much from so little so across cognition wherever you look, it seems that there's a huge gap between the information content coming in through our senses and what our minds get out of it.",
            "The knowledge, the understanding we get of the world, the capacity for intelligent behavior, acting in the world that we get.",
            "It seems that our minds form rich models, powerful generalizations, strong abstractions from data that's limited in so many ways.",
            "Sparse, noisy, ambiguous.",
            "And the basic problem we want to understand is how do we do that?",
            "Let me just start off by giving you one of my favorite demos."
        ],
        [
            "This, if you've seen recent talks for me, you've probably seen some version of this, but it's always a fun way to start off.",
            "It's inspired it so it's a lab experiment that we've done with adults, but it's inspired by a basic problem of cognitive development, children's learning, how you learn the meaning of words, so anybody who's had a child or been a child knows that children have this remarkable ability to get a word from just one or a few examples, like here's a horse.",
            "Here's a chair.",
            "Here's a bottle.",
            "Here is a telephone, and now they can use that and pick out new examples.",
            "Not perfectly, but amazingly well.",
            "Just from one or a few labeled instances.",
            "So here's our version of this with objects that should look somewhat natural to you but not familiar.",
            "I can teach you a new word in this alien language."
        ],
        [
            "Say the word too far.",
            "You see a few examples there and now you can go through and tell me which other things are tough as in which ones aren't.",
            "So I think I might be able to point at the screen here if you can see this arrow, so tell me is that a 2 for yes or no?",
            "This one.",
            "This one.",
            "Speak up this one.",
            "This one.",
            "I heard someone expressing uncertainty.",
            "The little delay in reaction time there is also a way that psychologists know how to diagnose uncertainty.",
            "So you can see with just a few examples you're very confident, except in a couple of borderline cases.",
            "How is that possible?",
            "By the standards of conventional classification in machine learning, this kind of almost one shot."
        ],
        [
            "And it seems rather amazing and the same problem occurs all over the place in perception.",
            "Like how do we get the 3D structure of the world from a stream of 2D images?",
            "Other areas of concept learning?",
            "How do we learn the hidden properties of objects from interacting with them?",
            "Language.",
            "That language isn't just about learning words, but lower level things like sound structure and higher level structures like syntax.",
            "Again, these problems come up inferring causal relation, so there's been a lot of great progress in statistics and machine learning for inferring causal networks from data.",
            "But children solve this amazing problem.",
            "They are often able to get calls.",
            "Elations again from just the right kind of spatial temporal coincidence of pattern in one or a few events, not even enough to compute by traditional statistics.",
            "Any reliable correlation, let alone make the legendarily hard jump to causation.",
            "Recently, our work and a lot of cognitive scientists have been interested in larger scale systems of knowledge.",
            "What are sometimes called intuitive theories or common sense theories like intuitive physics or intuitive psychology systems of concepts that we use to parse our experience.",
            "So we see things in motion in the world, but we don't just see them as flow fields.",
            "Or or other kinds of kinematics, we see the forces, the dynamics.",
            "We have an intuitive physics when we see other people moving around.",
            "They're not just bodies in motion subject to Newtonian forces, but we interpret their actions in terms of as intentional agents with mental states, beliefs and desires, and it's working backwards to that.",
            "That's a key importance for social interaction."
        ],
        [
            "So just to make another demo of this kind of thing here, look at the black and white video down on the lower right.",
            "This is this is a very old movie and it's just a 22nd clip from several minutes of video, but it's maybe the most famous video in psychology, hydrion Simul recorded this and stop action animation made back in the 1940s and studied the kind of interpretation people put on this.",
            "What you'll see here I'm going to start playing it now is just some shapes in motion, but do you see it as triangles and circles moving?",
            "What do you see?",
            "It looks like a social interaction, right?",
            "It looks like the people are laughing little nervous right?",
            "The big triangle seems like he's kind of bullying and beating up the little triangle and the little circle is hiding.",
            "And now after the little triangle is slightly chased away, the big trend is going after him.",
            "So where does that come from?",
            "This is just basically a 10 dimensional time series.",
            "If you think about the XY positions and orientations and the door position, but yet on top of that very sparse signal, you put all this rich social interpretation.",
            "So these are the kind of problems the scope of what we're trying to solve.",
            "Some are easy."
        ],
        [
            "Our mothers are this ones pretty hard now.",
            "The approach that we pursue you could call learning with knowledge and by this I mean to contrast with the conventional machine learning approach, particularly applied machine learning where basically you do ML when you don't know anything.",
            "But here this is all about learning with rich knowledge.",
            "So the key questions we want to understand these three here.",
            "How does some kind of abstract knowledge?",
            "Whatever it is that fills in the gap of our sense experience guide learning and inference from sparse data?",
            "What form does that knowledge take across different domains and tasks?",
            "How do we represent it and how is that knowledge itself acquired and we are approaching these questions with the tool kit that is mostly familiar to.",
            "Nips crowd right so that includes Bayesian inference to just understand how priors and guide data.",
            "The tools for sort of what you might call learning to learn, or the acquisition of the Priors are again quite popular these days, like hierarchical Bayesian models and Bayesian nonparametrics, and I'll show you using show you things we've been doing that again go between cognitive science and M with those kind of techniques.",
            "What's maybe a little bit on the fringe for nips, but not so much for, say, the more probabilistic AI crowd are defining probabilistic models, not just over the conventional.",
            "Objects of statistics, or say high dimensional vector spaces like in neural networks, but over more structured kinds of knowledge representations.",
            "The sort of data structure is more traditionally associated with computer science in classical symbolic AI.",
            "So you know, we know about graphs, but also grammars and predicate logic schemas, even programs, and defining probabilistic models on top of those data structures, I think, is going to be absolutely critical for human like intelligence.",
            "I'll try to convince you of these.",
            "My version of that as we go on here."
        ],
        [
            "Now I also for Nips I want to draw it an interesting contrast or really it's something that inspired a lot of us.",
            "The those of us studying high level cognition look to the part of the brain that we understand the best.",
            "The visual system basically and see inspiration, but also a difference in what we're doing.",
            "A lot of the best work in understanding, say, visual perception as a kind of statistical inference, indeed kind of Bayesian inference came out of the NIPS community, so there's work that is classic like.",
            "Yeah, you're why Sonero Simoncelli title, since work on emotion perception, or say.",
            "Convert cording and wolpert's work.",
            "What this kind of work is showing is that by making a sort of fairly straightforward Bayesian framework with simple but reasonable priors like a prior that motion in an image is slow and smooth.",
            "You can explain both these amazing successes of, say, human motion perception.",
            "Also, some of the quirks."
        ],
        [
            "You can go even further than this, like for example the work on natural scene statistics.",
            "So this is just one of many papers by Wainwright and Shorts and Simoncelli, where it's not just positing some prior that you pull out of your head, 'cause it's nice to work with, like a Gaussian, but you can go out and measure the actual empirical statistics of natural scenes and show that can be used to predict behavior and neural responses, and you might ask, could we extend that kind of natural scene statistics approach to cognition in some of the early work that this probabilistic cognitive science sort of subfield has been doing?",
            "We do see that, so here's an example of a study that Tom Griffiths and I did.",
            "Or we gave people these.",
            "Basically, these textbook Bayesian statistics problems.",
            "It's basically the problem of interval estimation.",
            "So you see I'll just give you some these examples, but we set them in it."
        ],
        [
            "In everyday setting where people might have, we thought good intuitions.",
            "So suppose you read about a movie that's made $60,000,000 to date.",
            "How much money will it make in total?",
            "Or you see something.",
            "Some baking in the oven for 34 minutes?",
            "How long until it's ready?",
            "You meet someone who is 78 years old?",
            "How long will they live?",
            "And so on in each of these cases, you encounter some phenomenon that has some unknown extent or duration teetotal at some random time or value T less than T total, and you have to guess he total from that one observation, and we can go and look at the natural scene statistics.",
            "So along the top here you see the empirical statistics for these classes of events.",
            "And then on the bottom you see two things.",
            "You see behavioral data.",
            "The little dots are median subjects, predictions of the total duration as a function."
        ],
        [
            "Give an example and then you see the optimal Bayesian predictions from the empirical statistics and it's quite striking how how close this is.",
            "I mean, to get to get this right here people have to do two things.",
            "They have to be Bayesian, but they also have to have the right priors and it's quite interesting that they seem to be sensitive to the different forms of the distributions.",
            "For these different classes of everyday events.",
            "But this approach just going out and measuring the statistics doesn't obviously extend to the more cognitive problems that we're interested in here."
        ],
        [
            "So, like in the case of word learning, what is the right prior?",
            "What's the right hypothese space?",
            "It's not even clear how to define that or go out and measure it in the world and not to mention you had a human learner.",
            "A brain autonomously figured that out, so we can look for inspiration to again."
        ],
        [
            "Asset cognitive psychology, which suggests that natural categories and nameable categories might be organized into something like a hierarchical tree data structure, say with categories at different levels of abstraction.",
            "So here you have Canaries, and they're a bird, and the bird is an animal, and so on.",
            "There's even evidence looking at, say, population responses from the high level visual system that visual categories get organized that way, and we've been able to make models of both adults."
        ],
        [
            "And Children's world learning with that same kind of idea so we can take, for example, these these alien objects and come up with a tree structure that's shown here such that the branch points of the nodes of this tree correspond to hypotheses of nameable categories, and then we can define a prior based on the branch link.",
            "Basically how perceptually distinctive category is.",
            "It's very similar to the probabilistic models using Bayesian phylogenetic's.",
            "So for modeling distributions of properties arising from mutation and the likelihood just comes from assuming the examples are a random uniform sample from the labeled.",
            "Branch and then it's pretty clear how to get this kind of behavior here where you see a few examples that seem to cluster in one distinctive branch, and that tells you how to freezer over there in that part of the tree.",
            "Now we can use this model to make quantitative predictions of peoples judgments for one or a few examples, and I'm showing you just just so you can believe that we can make quantitatively accurate models, people versus this Bayesian model for generalization of a new word from one or a few examples that cluster at different levels of the tree, but I don't want to dwell too much on the experiment because the interesting problems for machine learning are really how do we get to this point?",
            "Where does this hypothesis space in Pryor come from?",
            "So we built it as modelers by asking subjects to judge in a separate experiment to judge the similarity of these objects and then doing hierarchical clustering.",
            "But how does the brain do it mean in a sense, that's just sort of cheating, right?",
            "We all know that that there's an infinite number of features you can compute from a visual image, and depending on which features you use or how you weight them, you'll get very different similarity metrics.",
            "And the real question is basically how do people learn the right notion of similarity?",
            "Or you might ask more deeply given a similarity metric, you can build a tree by hierarchal clustering.",
            "But how do you know you should be building a tree?",
            "Other kinds of domains might be organized in other ways.",
            "It's not like everything in cognition is tree structured, although it's amazing how often that seems to come up as a natural."
        ],
        [
            "Representation, so these are some of the questions that we've been approaching and you can think of them again as kind of questions of learning to learn.",
            "If you look at the literature in children's cognitive development, you can see that children actually do learn these things.",
            "They learn what features of objects to pay attention to for word learning, and they learn to organize things into a tree.",
            "It's not there initially, so for example, Linda Smith and colleagues studied the development of the shape bias, which is this.",
            "Is this phenomena you can see right here.",
            "You show a kid, say a 2 year old this novel object.",
            "They haven't seen attacks, and which other one is attacks on the right.",
            "They'll pick out the one with the same shape but a novel texture material as opposed to, say, one that matches in texture and material but not the same shape.",
            "Kids speaking English have this at two years of age, but younger kids like say 18 month olds don't have this.",
            "So somewhere in there they learn it and they learn other things a little bit older they learn different biases for different kinds of words or entities like a material bias for words for, say, nonsolid substances, like learning the word toothpaste or the word honey or water.",
            "It's more about the texture properties of a material than any kind of shape.",
            "For these non solid substances.",
            "So it's like kids have to be able to learn what counts for similarity, but they have to learn different similarity metrics for different kinds of concepts.",
            "And again, as I've already referred to early on children in learning words don't have a hierarchical structure to categories, but what's called mutual exclusivity.",
            "They assume there's just one way to label each thing, like a flat partition into categories, and then only later, say by age 4, as opposed to age 2, they start to learn that you can have words like mammal or animal or living thing that refer to higher level categories.",
            "So in some work that we did a few years ago with Charles Campany prefers, we made fairly straightforward simple models of these aspects of learning to learn and apply them to cognitive development.",
            "But what I want to tell you guys about is some newer work that we've been doing.",
            "Which is more the same kind of idea, but in the machine learning setting where you can actually appreciate the value of this kind of cognitive learning to learn for the sorts of applications that."
        ],
        [
            "That ML is interested in, so here's some work that I've been doing with Russell Kudinov, who's a postdoc at MIT working with me and also with Antonio Torralba, and we framed it in the setting of, say, querying and retrieving images from a database, but this isn't meant to be primarily a vision project, it's just that this is an application that many of us are familiar with and work on, and the same idea could apply to speech or motor action where we wanted to be able to learn, say, a gesture from one example or a new speech sound, or what someone's voice sounds like the just you only have to hear someone talking for.",
            "A short while to get what their voices like.",
            "So in the context of image database retrieval, you know standard problem is we want to say query with one image, say a cow over there and we want to want to search over many many images and return the other cows and not the other things right?",
            "And what you see on the top is the kind of behavior we'd like to get and in fact that is the result of our systems performance on that query.",
            "But unfortunately if you just take a standard kind of naive similarity based approach, what you get is something that looks like on the bottom.",
            "You get some cows and some chimneys and some fields.",
            "Now why is that?",
            "Well?",
            "You know the standard approach for doing this with just one example.",
            "There's fancy things you can do if you're allowed to query with multiple examples and learn discriminatively, but it's just one example.",
            "The best you can do is something like similarity, so you take some feature space.",
            "Here we have this roughly 50,000 dimensional texture of textures feature space that also was introduced at NIPS in the late 90s by Deb and Viola, and this one works very well in the more discriminative setting.",
            "It's sort of inspiration and basis for, like the Viola Jones face detectors and so on, but in this and it's sort of modeled on the early structure of the visual system with recursively nested.",
            "Or filters in this setting, though you have this 50,000 dimensional feature space and and all you can do is compute similarity to the query and you don't really know which dimensions count, so something which matches the grass in the background is almost as good as something which actually matches the count.",
            "Another way to put this is at a minimum what you need for a concept is not just a prototype and similarity, but something like the concept specific similarity metric.",
            "The particular feature rates that count for this concept that might be different for other concepts, so we've been taking a hierarchical nonparametric Bayes approach to this which we can illustrate like this here."
        ],
        [
            "You could say the intuition is summed up by the slogan.",
            "Similar categories have similar similarity metrics.",
            "So let's say you've seen a bunch of dogs, horses and sheep and cars, vans, and trucks, various kinds of categories, but only one cow.",
            "Well, how do you know how to generalize cow?",
            "If you look at this, it looks like just straight forward Euclidean distance isn't the right thing, but rather you want to do something like this.",
            "You want to wreck."
        ],
        [
            "Guys that say cows are like dogs, horses and sheep and they tend to extend in the horizontal direction but not at the vertical direction.",
            "So our intuition is to try to say capture that in a high dimensional space."
        ],
        [
            "And we do that to save this 50,000 dimensional texture texture space.",
            "So we do that using what's a pretty standard hierarchical model where we have basically Gaussians at each level.",
            "That allows us to capture both sort of mean and variance structure at each of these."
        ],
        [
            "Which you can see illustrated in this picture."
        ],
        [
            "The extra twist compared to, say standard the hierarchical Bayesian statistics, is that we're learning the tree structure, so we only have labeled data at the bottom level.",
            "These so called basic level categories, and we have to learn in an unsupervised way.",
            "These higher level super categories like animal in vehicle those aren't given to us, so we put a nested CRP prior.",
            "It's one of these sort of standard nonparametric priors that allows the number of super classes to be effectively infinite and just introduced just as needed as the data require.",
            "And then we're inferring that latent hierarchy and using that to share.",
            "Information about variances, which is really the similarity metric in addition to the prototypes.",
            "So here's an example of apply."
        ],
        [
            "This to this MSR data set, which is a fairly small data set, but it has a nice kind of sort of superclass structure and you can see that we recovered down in the lower left.",
            "You can see the.",
            "The sort of super classes that seem both semantically and visually plausible, and what you can see on the RC curve on the right hand side is how much this buys you for learning to do one shot learning.",
            "So if you look at the black curve again, none of these are really, you know this isn't like super quality performance because we're just doing one shot.",
            "Learning with a fairly generic representation, but the key is to be able to say get almost as good as we can from just one example, so the black curve is the best we could possibly get from one example in using this representation, the red curve is what you get if you just use a naive Euclidean similarity metric and it's barely better than chance.",
            "But the blue curve is what you get when you've used this hierarchical nonparametric model to learn to learn.",
            "And you see, we're sort of getting to the regime that children are at, where you get almost all of the juice out of just one example.",
            "Now there's various."
        ],
        [
            "Is that are a little bit unsatisfying about that model.",
            "It's I mean it's actually very nice.",
            "Very tractable, really quite scalable, but in some new work that Russ has been doing, we're trying to sort of push beyond this to say, well, don't just learn weights in a high dimensional feature space, but actually constructed features.",
            "Now this work hasn't been written up yet.",
            "I won't talk about it too much.",
            "It's sort of just a teaser if you want to learn more about it, come to our workshop talk on Saturday, but the basic idea here is to actually construct the features in several layers at the bottom layer is that was basically a restricted Boltzmann machine or a deep network that learns in the typical.",
            "Way that deep learning operates in a completely unsupervised, general way across a very large data set and on top of that we build one of these hierarchical nonparametric models which learns higher level features.",
            "Basically topic to topic sort of HTP topic model to model the class specific distributions on the latent variables of the PBM, and that allows us to learn higher level features like parts in a way that's again sensitive to learning."
        ],
        [
            "The hierarchy, so the twist on typical HTP here is that we're doing inference over the latent hierarchy to again learn these super categories, and it does neat things on natural images, but I just want to show you 'cause it's 'cause it's I think, particularly visually striking some things that this is doing with the new data set.",
            "We've."
        ],
        [
            "Did which we call you.",
            "Could we're not really sure what to call it, but possibilities might be emnace plus plus or enlist transpose or something.",
            "It's meant to be like Em missed a good data set for testing a wide range of machine learning approaches, and one that we can actually really nail in a sense of like really get it right.",
            "Unlike natural images which were quite far from that form.",
            "But the reason why I say it's kind of like the transpose of ethnicity, it's much more natural, like the problem that the children's learning face instead of having as an M nest thousands of examples of roughly ten categories.",
            "Here we want to have you know order 10 examples of thousands of categories and we've done it by having people on the Amazon Mechanical Turk.",
            "Look at and then draw their own versions of handwritten characters in many different languages, including some that you might know and some that almost nobody knows.",
            "So I'll just show you some."
        ],
        [
            "The data here."
        ],
        [
            "I'm just going to flash through."
        ],
        [
            "'cause it's pretty fun."
        ],
        [
            "What you can?"
        ],
        [
            "See in each of these cases."
        ],
        [
            "These are just."
        ],
        [
            "Basically each."
        ],
        [
            "Column R7."
        ],
        [
            "Examples."
        ],
        [
            "Of a handwritten."
        ],
        [
            "Character in some Alpha."
        ],
        [
            "And so hopefully you can."
        ],
        [
            "See that there's a lot of similarity across."
        ],
        [
            "Columns."
        ],
        [
            "And there's just.",
            "This is just a small fraction."
        ],
        [
            "The data there's many thousands of categories here and a few examples of each one.",
            "Right, and hopefully you can see to pick any one of these.",
            "I just stopped at random.",
            "Pick any column you like.",
            "Like let's say I don't know.",
            "Where is my arrow?",
            "OK here we go.",
            "So pick this column.",
            "Hopefully you can see from just one or a few examples.",
            "You can get the concept and clearly recognize how the other things in that column belong to the same concept and pretty much everything else in this data set doesn't.",
            "Now, the models that we've been building here I think are starting to actually capture this."
        ],
        [
            "So the images that I'm."
        ],
        [
            "Flashing here are actual."
        ],
        [
            "These learned higher level super classes and you can see just looking at any of them.",
            "It seems to have organized the basic level classes into coach."
        ],
        [
            "Current higher level classes that share the same."
        ],
        [
            "Kind of style or the same?"
        ],
        [
            "And parts."
        ],
        [
            "The young"
        ],
        [
            "Including Braille."
        ],
        [
            "The learned features include sort of low level V1 like features, but these higher level parts in the HTP and you can draw samples from this model.",
            "The same sort of thing."
        ],
        [
            "Used to seeing with, say, deep networks, but now in a class sensitive way.",
            "So here for example on the bottom on the top or real data on the bottom.",
            "What you see are made up examples of made up classes in this style so, so you should hopefully see coherence at both the."
        ],
        [
            "Level and the superclass level and he."
        ],
        [
            "Are made up examples in this."
        ],
        [
            "While here I made up."
        ],
        [
            "Degrees in this style they made."
        ],
        [
            "Examples in that style and."
        ],
        [
            "Just go on forever."
        ],
        [
            "Like this?"
        ],
        [
            "This."
        ],
        [
            "Sort of sensitivity."
        ],
        [
            "To the latent class structure is 1."
        ],
        [
            "I haven't seen."
        ],
        [
            "And in typical."
        ],
        [
            "Deep networks."
        ],
        [
            "It's one that we can get in a nice way by."
        ],
        [
            "Combining these sorts of approach."
        ],
        [
            "Or here we can use."
        ],
        [
            "For this law."
        ],
        [
            "From few examples."
        ],
        [
            "So here I'm showing."
        ],
        [
            "3 examples of a new class that's been."
        ],
        [
            "Called out of the data set and then here are generalizations.",
            "These are conditional samples of the model.",
            "New new things of that sort, and it's done this by recognizing that is of the same type as the ones shown on the bottom and then using the learned prior for that soup."
        ],
        [
            "Class again here I'll show you many examples of each in each case."
        ],
        [
            "Giving 3 examples of."
        ],
        [
            "Totally new class in the models basically."
        ],
        [
            "Kind of getting plowed."
        ],
        [
            "All new instances of that."
        ],
        [
            "Category."
        ],
        [
            "You can compute more objective RC curve type results and I won't really bore you with those, but we really can get quite substantial improvements on more generic approaches.",
            "If you want to learn more about this, come to our transfer learning with rich Models Workshop on Saturday.",
            "We could say that the goal of this workshop is too.",
            "Start a new field called Rich Learning which we work on because then all the others are called poor learning.",
            "Alright, now I want to."
        ],
        [
            "Back to some of these basic learning to learn issues here and let me just check how we're doing on time.",
            "OK, good, so all the models that we've seen so far, including the very simple word learning one in these fancier hierarchical nonparametric models.",
            "But the key step here is learning some kind of latent tree structure that organizes the categories in the higher level categories, and that's a very powerful, compelling idea, but it's not the only kind of representation that the brain or or saying AI system should build up the world.",
            "We want to be able to understand.",
            "How can a system learn.",
            "Let's say for something like this problem of organizing categories and labels, a tree structure is the right way to go, but maybe for other kinds of tasks or domains, some qualitatively different form of structure is the right approach.",
            "So this."
        ],
        [
            "This challenge of discovering the form of structured data is a very compelling one in cognitive development.",
            "As I said, it's one that children have to learn.",
            "In the case of, say, the hierarchical structure of nameable categories, or other cases like you learn, for example, the days of the week or other kinds of temporal categories have more of a cyclical structure or the click structure of social networks.",
            "All of these qualitatively different forms, or in science in key moments of scientific discovery.",
            "The big insight isn't just like a little detail, but total reorganization like going for example, in biology.",
            "What happens, starting with Linnaeus and then into Darwin, recognizing that biological categories shouldn't be arranged into what Europeans had previously called the Great chain of being a 1 dimensional structure with basically rocks down at one end and got up at the other end and going all the way through from fish and birds and mammals and humans and Angels up to God, that one dimensional chain structure is 1 plausible way to.",
            "Think about living things, but when you actually look seriously about at a wide range of data, you can just see that tree structure or take for example Mendeleev's and contemporaries discovery of the periodic structure of the chemical elements.",
            "Again, neither Linnaeus, even Darwin or Mendeleev understood the causal forces that gave rise to this darn was just beginning at the time he started drawing trees like this was just beginning to think about selection in the mature sense.",
            "Mendeleev certainly didn't understand quantum mechanics.",
            "That gives rise to the periodic table, they were just.",
            "Looking at the data and finding the form of structure.",
            "Now we have lots of great unsupervised learning algorithms for finding structure in data, but pretty much all assume a fixed form of structure and then just find the best structure of that form.",
            "So if we want to learn trees, we have hierarchical clustering.",
            "If we want to learn flat cluster structure, we have K means or mixture models.",
            "If we want to learn some kind of dimensional structure, we have both linear and nonlinear manifold techniques.",
            "But maybe what we want is to put it somewhat ambitiously, is something like this and more universal framework that replaces this grab bag toolbox of techniques with a more.",
            "Principled and general purpose approach that's able to not just learn all of these different forms of structure, but figure out which is the right one.",
            "So Charles camp."
        ],
        [
            "Some really neat work when he was in my lab on this project and this was published year two ago in PNS and the basic idea.",
            "There's a couple of ideas here, but one is to come up with a universal language for representing all these different forms of structure in the same terms.",
            "And for that we're using graphs.",
            "Each of you know if you want to talk about dimensional structure or manifold structure trees, either latent hierarchies, are observable trees.",
            "If you want to talk about flat clusters, those are all different forms of graphs."
        ],
        [
            "And then we will define distributions on these graphs to fit to the data.",
            "But another key key idea is that each of these different forms of structure can be generated by a simple process.",
            "What you think of as a graph grammar, like a rule for taking a node in a graph, and it's in and out links and replacing it with two nodes.",
            "So growing a little bit of structure and the appropriate in and out link portion to those two new nodes.",
            "And that's that is a simple generative model for these forms of structures, different graph grammars, different rules are a way to parameterise these different structural forms.",
            "So now we can cast this learn."
        ],
        [
            "Problem in a hierarchical framework like this where the lowest level is the observable data.",
            "You know just a typical matrix and then we have these two latent levels that we're interested in.",
            "There is the level of what we call the structure, which is some graph, and it's a graph of a particular form that's generated by some simple graph grammar.",
            "At the higher level, and by taking single productions or cross products of productions like, Say, a cross product of two chain rules gives you a 2 dimensional grid, I skipped over it before but like for example across down in the lower right."
        ],
        [
            "Across the chain and a ring gives you a cylinder structure.",
            "And So what we want to do is basically do."
        ],
        [
            "Inference at both levels of this hierarchical model as a way of learning the right structure and or the right form of structure, and the best structure of that form.",
            "Or you could think of it as a kind of structural grammar induction, like by analogy to language where you want to learn the grammar at the same time as you have to use that grammar to parse the sentence is in order to even know how well it fits, so I don't have time to tell you about."
        ],
        [
            "The details of our probabilistic model I just refer you to the paper, but the basic idea should be again, pretty much familiar to a Bayesian machine learning crowd.",
            "There's sort of the.",
            "A Bayesian Occam's razor in the probability of a graph structure given a form which tends to prefer simpler forms, like for example the chain structural aflat cluster structure is the simplest to chain structure.",
            "Is is a little bit more complex, but simpler than a tree, and so on.",
            "And then the probability of the data given the structure.",
            "For that, we're using some really neat ideas that again are now just sort of classics in the NIPS community.",
            "The idea that you can use a graph to parameterise a Gaussian process just to capture the notion of a smooth distribution of data by something like the covariance being the some kind of normalized inverse graph.",
            "Ossian and the particular work that's close to this is the work that Jerry Zhu and Lafferty and."
        ],
        [
            "Garmony did, but there's many probably equivalent approaches and what we're doing here is basically searching over this these two levels to find the best grammar and the best graph of that form, and I can tell you details of that later if interested, but just to show you a few results of using this approach given a data set of animals and their features, and these aren't, you know, genetic features.",
            "These are the kind of things that child would see if they walk around the zoo or read a picture book like a Walrus, has flippers or dolphin, has tail, or a dog barks, that sort of thing.",
            "The model is able to is able to identify implausible tree that captures something of the hierarchical structure of biological categories as a normal human child might understand them, but also to identify that it should be a tree as opposed to some other kind of structure.",
            "So given, say for example a data set of more of a political domain, Supreme Court opinions, these are all justices who served on the Rehnquist Court.",
            "Then it identifies not a tree, but more of a chain structure capturing the left right or liberal conservative."
        ],
        [
            "Spectrum if we if we give the model say for example data that comes from a space of faces, these are realistically rendered synthetic faces where we tweaked interface synthesizer.",
            "Two knobs corresponding to dimensions of basically masculinity and a racial dimension.",
            "So you can see if you go sort of from top to bottom here what looks like a light to black dimension and if you go from left to right you can see a sort of subtle masculinity variation.",
            "The ones on the left are supposed to be more heavily masculine faces.",
            "It might help if you think that the ones on the left are like the football team and the ones on the right are more like the tennis team.",
            "And our model is able to identify those two underlying dimensions that there is a dimensional structure and to recover the right dimensions or for example, given given proximity between cities in the on the Earth, it identifies a cylinder which basically captures the representation of latitude and longitude.",
            "One other interest."
        ],
        [
            "Developmental point comes out of this and I think it's also provides a lesson for how we want to develop abstract knowledge in machine learning.",
            "We can look at a learning curve by giving this model more and more data and what we see is that when there's very little data because of the Bayes, Occam's Razor.",
            "Basically, the model is worried about overfitting, so it finds a structure which is qualitatively simpler than the correct one.",
            "Just this flat cluster structure up there on the upper left, and that's the same sort of thing we see with the youngest children at the mutual exclusivity stage.",
            "Everything is just in one category and it captures plausable high level categories like birds and insects and so on.",
            "Then, with a little bit more data, it gets the idea of a tree structure.",
            "It gets the right grammar but not the right tree.",
            "It takes a lot more data that's going down to the lower left to get to work out all the little details, figure out that Penguins shouldn't go in with Dolphins, seals and whales and so on like that.",
            "And this idea of kind of getting the big picture first, it's it's a phenomenon that comes up in a lot of the hierarchical models that we've built for these higher level cognitive phenomena, which we call the blessing of abstraction.",
            "So phrase it's due to Noah Goodman, and it's just really interesting human like learning dynamic where you getting the.",
            "Higher level knowledge.",
            "First, it contrasts with the traditional way that cognitive scientists and machine learning researchers approach the development of abstraction, right?",
            "So think about, say, the way you build a typical deep network as you learn it, layer by layer.",
            "Almost by definition, you're assuming that you learn the simple concrete features 1st and then the more abstract features.",
            "But it's an important feature of more cognitive human learning.",
            "This idea of getting the big picture 1st and it's one that comes distinctively out of these more structured hierarchical models.",
            "Now in the last few minutes what I want to talk about is some of the work that the directions that I'm most excited about for the field, not just for what I'm doing, but for the whole interface between cognitive science, machine learning, and AI.",
            "You can maybe could call it graphical models plus plus or something.",
            "What you could see an we realized in the work that we're doing there with the structural grammar induction and a number of people have been realizing you know over the last few years is that we're sort of approaching the limits of what we can do with our traditional graphical models Toolkit."
        ],
        [
            "Call the statisticians toolkit.",
            "You know this sort of inference over fixed sets of variables.",
            "Random variables linked by simple or at least well understood probability distributions.",
            "You could say that up until that graph grammar induction, everything I said could have been done by a Bayesian statistician.",
            "If they only they were crazy enough.",
            "But this this other thing is, trust me.",
            "Way too crazy for a Bayesian statistician.",
            "And when you start to think about problems of real cognition like say theory of mind or intuitive physics, it's goes way beyond that toolkit and brings us to what looks like they need to have more of a computer scientist toolkit in there with the statistics toolkit.",
            "So there's different names for this approach.",
            "I like the name probabilistic programming, which was the subject of a nips workshop back in 2008 where the idea is to is to really be doing, say, inference over flexible data structures, not just not just traditional notions of random variables or.",
            "Or fixed finite structures like graphs and to be able to go beyond the simple kinds of distributions that we inherit from statistics and stochastic processes to what you might call stochastic programs as the basis for a generative models distributions that in some sense we don't really get analytic insight into until we run them and see how they work.",
            "So we saw.",
            "I mean, well we saw yesterday in my Jordans talk a very nice articulation of this and Mike has been one of the people articulating this most clearly and forcefully.",
            "So yesterday he talked about machine learning for proteins and.",
            "And gave examples of things where you know you can't really describe this in a nice way as a graphical model where you have to use real scientific knowledge, for example like biochemistry or quantum chemistry or evolutionary biology.",
            "Here is an example."
        ],
        [
            "From another paper that Mike was involved with, this is work of per ceilings and colleagues on this infinite PC FG and it's just like but in a much probably more sophisticated way than what we were doing with these graph structural grammars.",
            "Here's a here's an infinite.",
            "Model for parsing and language, where as you can see in their in their figure description.",
            "They just make it quite clear that you can't really represent this as a graphical model, but you can give what looks like a program, basically a symbolic specification over on the left of the generative model.",
            "It just doesn't fit into our graphical models Toolkit.",
            "Now we see."
        ],
        [
            "Most clearly, in the more advanced examples I started off with, like Remember this, this hydrogen symbol example here, I just don't think there's any plausable graphical model here, but there is a plausable probabilistic program to describe this kind of action.",
            "Understanding or theory of mind, and we've made a lot of progress recently that we're quite excited about."
        ],
        [
            "To spell that out, so just to be clear, the way that psychologists approach these kind of problems is you could draw.",
            "It looks kind of like a graphical model.",
            "You could save peoples actions are a function of their beliefs and desires.",
            "If you assume that an agent is rational, that's a mapping from beliefs and desires to actions, and you have to work backwards to figure out the latent mental states, the unobservable beliefs and desires that explain the actions, and you can cast that as a Bayesian inference.",
            "But the real content here is being done by the process that's inside those arrows, and if you want to really capture the idea of irrational agent, you need something like a pond."
        ],
        [
            "The solver, so that's a program, right?",
            "So this is what we've been doing.",
            "We've been basically saying, as a first approximation, intentional agents have things like beliefs and desires which can be cast into utility framework, and then their actions are the result of solving something like a palm DP.",
            "And then we want to work backwards, invert that that process, and it's very related to inverse optimal control and inverse RL, which again have been pioneered in this community."
        ],
        [
            "So just to show you how we're using this to model action understanding in humans, this isn't.",
            "Again, this isn't a model of how people act, it's a model of how people understand intentional agents.",
            "Their intuitive theory of mind.",
            "So for example, here you can see the basics sketch over in the left people see a point like agent, just like in that hydrogen symbol video moving in a simple kind of room or maze.",
            "There may be a wall there may not be a well, there may be a hole in the wall.",
            "There's three goal objects AB&C and they see a trajectory which may get to one of the objects or usually stops somewhere along the way.",
            "And they have to make a judgment when it stops about how likely the agent's goal was to be a B or C, and you can just see it.",
            "Some of some of the data from conditions here.",
            "There's many, many conditions I can't show you all of 'em, but here are a bunch of stimuli in the top row with different environments and different paths.",
            "And then here in the 2nd row are how people's judgments about the agents goal red, green or blue evolve overtime, and you can see all sorts of interesting dynamics going on.",
            "Is the person thinks the agent may be changed his mind, or went back or change their mind about what the agent is saying and the model is able to capture this.",
            "You can see there on the bottom very strikingly, capturing subtle quantitative details up here on the top.",
            "The scatter plot just shows over all the trials over all the data points are very strong.",
            "Correlation between what this model says and what people say, and it's a very simple model.",
            "It just assumes that the agent gets some utility for getting his goal.",
            "He has a small cost, small negative utility for each step he takes, and there's one extra parameter.",
            "There's basically there's two numerical free parameters here.",
            "There's the agent is assumed to be only probabilistically rational, so he doesn't always do the best thing according to his palm DP.",
            "Solver and also he.",
            "He there's some small probability he can change his mind or change his goal at anytime, and you may not notice that or you don't have any outward sign or direct sign of that except how he moves to the space so that these strong switches and goals are explained by that.",
            "Now this actually this here is not actually a palm DP.",
            "It's more of an MVP because we assume that both the observer and the agent have full knowledge.",
            "But if we really want to do theory of mind and make joint inferences about agents beliefs which could be false or could be incomplete because maybe they haven't seen the whole environment, then that takes us to a palm DP.",
            "And so here's this to some recent work I should say.",
            "By the way, this is work of Chris Bakers.",
            "Where yeah?"
        ],
        [
            "We're on, we're actually doing joint inference about basically tracking and belief space, and also inference about an agents preferences.",
            "So the domain where we studied this is similar to the ones you've seen.",
            "We call it the food truck domain so you can see over here on the left a little scenario.",
            "We have some grad student on a hypothetical college campus.",
            "Very small college campus here.",
            "Every day he gets lunch at one of the food trucks.",
            "There's three food trucks, the Korean K, the Lebanese L and the Mexican M. But there's only two parking spots on any one day, at most two, and sometimes only one of these trucks is present on the campus.",
            "So you see Harold start over here.",
            "Let me get my cursor.",
            "So Harold starts here and he goes like this.",
            "Done 1010, Ten ten 1010 long here.",
            "More slowly.",
            "He goes here.",
            "Remember he can have sort of line of sight axis so he can now see what's on the side of the wall and he goes back to K. So the question is.",
            "There's K over here.",
            "There's L here.",
            "What's his favorite food truck?",
            "KL or M?",
            "M right interesting because M isn't anywhere present in the scene.",
            "You know it can be there, but there's no direct perceptual evidence.",
            "There's nothing like he's headed towards me.",
            "You just interpret the fact that he was looking for and then maybe disappointed he didn't see it.",
            "So he went to what his next favorite.",
            "And that's what you can see.",
            "Here is both people in the model's am is the best, then K and then L. And they also make a kind of corresponding belief in France.",
            "He must have actually thought Em was likely to be there.",
            "That's what you see here on this bar.",
            "Otherwise he wouldn't have gone to the trouble to go and look and again across many different scenarios.",
            "We got a pretty good correlation on both preferences and believes not as good as in the simpler case, but still pretty good.",
            "You know, certainly by the standards of what.",
            "Until recently I mean really until this work was only treated as a qualitative."
        ],
        [
            "Tuition this theory of mind.",
            "Some of the other things we've been doing along the same lines is intuitive physics, so you can here's one more demo just to close out.",
            "It's fun and easy.",
            "Just look at this table here with these red and yellow blocks and say if I bump the table in one of the blocks falls off.",
            "Is it more likely to be red or yellow?",
            "Since yell it out so we can hear you?",
            "Red or yellow?",
            "Red"
        ],
        [
            "Yeah, OK, so pretty much everybody agrees, although there's a."
        ],
        [
            "Variation, how does that work?",
            "How did you do that?",
            "So our approach is to is to sort of take physics and scenes seriously.",
            "So like a few other people in the community, there was really nice poster from CMU on this yesterday.",
            "You know, we think that you can understand vision is actually inferring 3D model.",
            "It's really inverse graphics and the hypothesis space and the forward mapping is basically you know, Pixar physics is physics.",
            "So we've been building these ideal observer models in which we assume people have a probabilistic version of Newtonian mechanics and then they can run.",
            "Little simulations, not very accurate, but little simulations to reason about these things, and we can test this, for example, in a similar case we haven't done the red and yellow yet, but for example we can show."
        ],
        [
            "People these towers of blocks and say, is it stable or not?",
            "Will it fall over so these?"
        ],
        [
            "Look pretty stable."
        ],
        [
            "This one no right that will follow."
        ],
        [
            "Over this one."
        ],
        [
            "Probably unstable."
        ],
        [
            "Ones are a little more border."
        ],
        [
            "And we can get again very good correlations just with very rapid presentations between what people, how stable people think this is, and the predictions of this model that somehow assumes that you're approximately inverting the graphics to come up with the 3D block description and running a little brief noisy physics simulation to figure out what's going to happen.",
            "Even the same kind of models can even apply in simple cases to infants and be able to predict infants looking time, which is the."
        ],
        [
            "Standard measure of what infants know, but until recently hasn't been related in any kind of problem in kind of a quantitative way to any model, let alone a probabilistic model.",
            "So just."
        ],
        [
            "To wrap up, then I'll point to what I think is really on the API side.",
            "The technology that's going to allow these little basically first model stabs that we're building and others are building to really hopefully try to capture something about human common sense and take that over into an is the idea of actually building general tools the so called probabilistic programming languages, which give you a way to express all these models and generic inference tools.",
            "Often some kind of approximate inference, a lot of them are using Monte Carlo, but not all of them, and there's basically.",
            "Probabilistic programming language is built on logic like prologue and there's a number of once we built more functional or imperative programming styles like Lisper Matlab and I won't really go through any of the details here, but it's very exciting that these technologies are there.",
            "Certainly not mature, but they are under development and it's going to make it possible to develop and work with these kinds of models and even to frame learning as what you might call a kind of program.",
            "Induction or cognitive development as a kind of program synthesis.",
            "I mean, this is far out stuff I don't want to pretend this is easy or accessible in any really satisfying rigorous way at this point.",
            "But I would venture a prediction that sooner or later we're going to have to view learning this way, because otherwise we just can't explain how this kind of knowledge develops, I think."
        ],
        [
            "Some of the work I showed you like the graph, grammar stuff or other work we're doing with the handwritten characters.",
            "For example, Brendan Lake is doing some work on actually what I think is sort of the right model for the handwritten characters is actually inferring a motor program.",
            "If you look at how people actually draw those those characters from one example, they all draw them pretty much the same way.",
            "They don't just have a visual concept, but it's actually a motor sequence concept.",
            "Or there's a lot of really cool work going on in NLP modeling.",
            "The acquisition of language is basically synthesis of simple kinds of programs.",
            "OK, so I'll wrap up the big."
        ],
        [
            "Problem that we in this field are trying to understand it.",
            "It's really a joint project with machine learning and AI is how the mind or anything like a mind get so much from so little across all these different domains.",
            "We have a toolkit which builds on the core Bayesian statisticians tool Kit and how it's been extended in ML with for example hierarchical and nonparametric models.",
            "But it's also one that has to draw I think on the computer scientist Toolkit, including not just interesting data structures but probabilistic programming languages and to a cognitive scientist.",
            "The value of this is you know what I've shown you here is things like want it nice quantitative models, but even more.",
            "And I think this is also true for AI tools for thinking and thinking a little bit differently.",
            "So instead of getting stuck in these usual debates that have wracked both cognitive science and AI for decades is the mind should be understandable, logical probability or is it nature versus nurture?",
            "Is it learned or is it wired and we can really do?",
            "We don't have to have those debates anymore because we can ask much more interesting questions like how can you actually learn real knowledge with some appropriate kind of probabilistic inference?",
            "Or how can you learn some very rich domain specific knowledge with the domain general mechanism again also for the.",
            "Learning crowd if we want to think about where real abstract knowledge comes from, we have some slightly different ways of thinking about it here.",
            "It doesn't just have to be built up slowly from the ground up, but very abstract knowledge which we can see in real infants, is present quite early on.",
            "I didn't show you that for the Hydra symbol, but next to it was another similar display that works even for preverbal infants.",
            "How abstract knowledge can be learned surprisingly early, and how the idea of having sort of structured symbolic knowledge doesn't have to be brittle handwired, but can actually deal with noise and insert and even potentially be learned.",
            "In some kind of robust way that looks something like growing in mind, thanks."
        ],
        [
            "OK, so first I want to congratulate you on getting the right generalization of deep learning from a single example.",
            "Bingo, but now I've got a question.",
            "If you take a deep belief net and you train it on data that contains 10 long skinny manifolds, it will learn 10 long thin energy ravines in the top level DBM and actually do more than that.",
            "'cause if you look at it at a coarser energy scale, three of those ravines for three, five and eight will sort of form 1 ravine, which is the beginnings of a little bit of tree structure.",
            "So the question is, instead of seducing Russ into putting trees on top of our BMS, why don't you let him do RBM's all the way up and get the trees to emerge as emergent properties of the energy landscape?",
            "So at the deep or at the no.",
            "Sorry, not at the Deep Learning Workshop at the next one on the transfer learning with Rich Models Workshop on Saturday will be explicitly compare this with some deep belief net or deep deep Boltzmann machine.",
            "Russes version of deep belief Nets.",
            "And we are actually trying to compare the man it be interesting to see what kind of structure has to emerge.",
            "I mean we know that when you look at the brain.",
            "You know this is the basic insight of neural networks.",
            "Is is you can't argue with it, right?",
            "What you see in the brain is a bunch of very, very large numbers of spiking units, and somehow those have to in some implicit way represent the kinds of structures were talking about.",
            "If those structures are there at all, and I think it's the interface between these different approaches that can try to yield inside that.",
            "But part of why we were trying to develop both this kind of data set and this kind of task is so we can study that in a serious cognitive way.",
            "You know, I don't.",
            "I don't have any doubt that deep networks.",
            "Can learn these kinds of things, but the question is can they learn them as fast as an as flexibly as children do?",
            "And there we have some reasons to think that this more kind of explicitly structured probabilistic model might be better for that.",
            "But let's, let's talk about that more at the workshop.",
            "Thank you, thanks.",
            "So could you say something about language learning?",
            "What do you want me to say?",
            "You know another result.",
            "It seems like you are.",
            "Trying to create a system which you start with a certain syntactical structure and then you figure out how to tweak the parameters.",
            "That sounds suspiciously like Chomsky's view of language.",
            "Chomsky doesn't think so.",
            "Well, you know the.",
            "That's the way a lot of other people think.",
            "Even yeah.",
            "I mean, it's interesting that.",
            "I mean, I think it actually illustrates the point I'm trying to make on this slide here at this point about sort of going beyond some of these classic dichotomy's, which is that you know, all of us inherit this dichotomous way of thinking, and I've just been.",
            "I've been struck by how the work that we do.",
            "Basically people who are more strongly inclined towards a more sort of empiricist everything learned you say, well, that's kind of interesting, but way too Chomskyan or way to nativists, whereas the people you know.",
            "At MIT, trust me, I have many colleagues like this and they are quite vocal.",
            "They see it, you know, quite the opposite.",
            "Like I gave a talk in Marvin Minsky's class and he said that's all very interesting.",
            "But why do you have to keep saying all that Bayesian stuff?",
            "And it's funny that he didn't notice that there were some other words in there too.",
            "So I think that there are some ways.",
            "I mean, in some sense, at the same time, as are some of my nativist linguists, colleagues say this is way too imperious.",
            "They also say, but it's also even more nativist, and we are, 'cause you posit all this incredibly rich structure to the mind, and I think there are just.",
            "There isn't just a single, maybe another way to view what's going on here.",
            "If there isn't just one dimension.",
            "So what we have in common, I would say with the more sort of classic AI, or Chomskyan approach in language, is saying the mind has the ability, and somehow we must have it from the start to work with very rich kinds of knowledge representations.",
            "But what we have in common with more of machine learning is that we think in many cases, most if not all of that structure is learned in the interesting part of it is.",
            "Actually responds to the data of experience, and the trick is how could that possibly work?",
            "If you look at the early Chomskyan linguistics like before, Chomsky was known for being a very strong nativist in the early days of generative grammar, like back in the late 50s and the 60s.",
            "It looks much more like this.",
            "The child, the idea was wasn't the child as like parameter center, but it was a child as linguist, right?",
            "And the idea was language acquisition was searching through a big space of grammars, and they gave up on that.",
            "Because I'm so I'm told, and it makes sense.",
            "They just thought that will never work.",
            "And I think part of the reason why they they came to perhaps the premature conclusion that it would never work is because they didn't have the right way to the tools to find statistical models over these spaces of structures.",
            "And they didn't have computers.",
            "Basically.",
            "I mean they didn't have what we would now recognizes computers, and so much of what we're doing, just like much of the best work in this field is enabled by the combination of mathematical developments, lot of it from statistics and computing power, which just lets us do things that you know.",
            "Would have been unheard of before.",
            "Josh, can you comment a little on what the next steps are in this kind of science for really determining what are mechanisms of thinking?",
            "Are there?",
            "Yeah, you show nice correlations between human performance and algorithms.",
            "What are the kinds of experiment?",
            "That's a great question.",
            "Thank you for asking it.",
            "Yes, so pretty much all that I've talked about here is what in psychology is often called an ideal observer model, right?",
            "You try to study the functional relation between the inputs and the outputs.",
            "It's the level that is really most directly in common between all the fields were talking about.",
            "But you don't actually say certainly anything about the hardware mechanisms or even about the actual algorithms you know.",
            "I just showed you some probability functions.",
            "I didn't tell you how I optimize them or search or did it did inference or how the brain might do it.",
            "And that's that's basically where as a field we've started.",
            "But we're definitely interested in getting down to these lower, more mechanistic levels of analysis.",
            "It's we see it again, it's kind of in contrast to the neural network approach, which takes more of a bottom up approach where you start with an idea of how the brain works and try to see how cognition emerges.",
            "This is a complementary approach.",
            "Where you started, the more functional level and then say OK Now that you have some evidence that this captures the behavioral phenomena, how could that be implemented in the brain?",
            "And a lot of the most exciting work over the last few years?",
            "It's just a whole other talk, and it's work that I'm doing a little bit, but a lot of it is being done a lot more of it is being done by people like Tom Griffiths or contained many people in the Faneuil there, Ed Vul.",
            "Noah Goodman is interested in this but sometimes called on rational process models where you try to come up with more psychologists.",
            "Called process is really the algorithms of the mind.",
            "And what's neat about this particular group is they're doing it informed by all of the machine learning a probabilistic AI that that you know this community is developing.",
            "So instead of just kind of making up ideas about how the mind works, which is how cognitive psychology has had to do it for decades, you can go and look at the engineering side for hypothesis, you know.",
            "So there's work on saying, do people actually, I guess I even have a slide on this.",
            "Do people do MCMC or do they do important sampling?",
            "Or do they do particle filtering and it's very exciting and cool?",
            "That we actually are starting to build this multiple level picture where we have the kind of probabilistic.",
            "That you know the sort of public, ideal observer level and going down from algorithm control levels and emerging unifying idea that many people are converging on is what I think was called recently by Joseph Fisher.",
            "The sampling hypothesis that the idea that it's really some kind of sophisticated Montecarlo, that is the basis for both understanding how these learning and inference these models can be implemented in the brain.",
            "And I think the AI and statistics side has to support that because it's the Monte Carlo approach to approximate inference that seems to be the best hope for it completely generic tools for working with very.",
            "Richly structured representations.",
            "That's very speculative at this point still.",
            "So I think I have a related question which is.",
            "Or at the computational level, as the models get kind of more and more rich and complicated.",
            "I mean if if you drop a palm DP solver and treat it as a likelihood function.",
            "Where do you go with that in terms of doing more than existence proof?",
            "Like really understanding what it is about that solver that that right?",
            "So that I mean that even I'm not going to take seriously as an actual mechanistic account 'cause you know, Chris goes in, crunches on his model for a day and then comes back with predictions for the experiment.",
            "Yet people do it like that.",
            "But again, it's actually.",
            "I think it is quite consistent with this view.",
            "So here were inspired by work on.",
            "Basically there's a planning is inference tradition, which again was developed by people in this community.",
            "And doing approximate inference as basically simple kinds of sampling based approximate inference.",
            "That's something which can be plausibly embedded as an inner loop in that model.",
            "So basically what you imagine if I want to understand what you're doing I just you could say I kind of put myself in your shoes and draw a few forward samples conditioned on how you're acting.",
            "Try to see what matches up with that, and work backwards to what mental states might have caused it.",
            "And it's a very rough intuitive description.",
            "But so, for example, Noah Goodman has some nice examples of actually implementing that in the Church probabilistic programming language, and it's much more algorithmically plausable.",
            "Approach to this kind of kind of thing.",
            "So how can we get falsifiable hypothesis of human cognition?",
            "So, for example, suppose that you're sampling hypothesis.",
            "Yeah, seems all the data we have suggests that yes, what we're predicting is goes along with what we're observing humans do.",
            "But you know there might be doing something completely different.",
            "And how would we know the difference?",
            "That is to say, do we have to wait until we actually have the neural circuits to understand?",
            "And really whether we're right or not?",
            "Or is there some other way to get it possible?",
            "Yeah, I mean, this is the usual problem of science.",
            "It's it's an interesting illusion that some people seem to be under that.",
            "That science works by falsification.",
            "I won't mention the connections to certain other statistical methodologies that are related, but I think the way real science works is you.",
            "You certainly want falsifiable hypothesis, but the best we can do is come up with ideas and test them out and we can have heuristics for coming up with ideas or various heuristic tools.",
            "You know, a lot of data driven science.",
            "We look for automated ways to generate these, but fundamentally we don't know.",
            "Though I mean, because this was a very broad high level overview talk.",
            "I didn't go into the details of how we actually approached anyone of these projects, so in any one of the projects I showed you, here's a model here Sunday to look at fits.",
            "What we actually do is test a bunch of different models and compare them against each other, both Bayesian and non Bayesian.",
            "You know 'cause a lot of psychologists are.",
            "Not all sold on the whole Bayesian thing and every reviewer reasonably wants us to test out a range of alternative models.",
            "So that's so we do take that approach in any individual project, but as a whole the framework like what I'm putting out here that's not falsifiable anymore than Connectionism, is falsifiable or symbolic AI like you.",
            "Just it's pragmatic.",
            "It's like a scientific paradigm.",
            "You have to test it out, and if it if it seems productive, if it seems to fit a draw together.",
            "Insights from a lot of different data and suggest new experiments in that cycle keeps going.",
            "That's how we know we're making progress, but you know, I think what if you want to know where to go?",
            "How can we possibly know if we're on the right track?",
            "Well, this is partly where and I think this is what's so exciting compared to other approaches that cognitive psychology or neuroscience have had.",
            "The bidirectional idea flow and interchange with machine learning AI is so important because it provides this whole extra source of constraint model has to actually work 'cause the brain works and by.",
            "Building models that we also not only fit to peoples data but also tested in real machine learning and AI settings.",
            "It keeps us honest and keeps that keeps generating.",
            "I hope better and better hypothesis.",
            "OK, I'm sure we should stop.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "It's a great pleasure to be here.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to talk about today is how to grow a mind.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structure and abstraction and I want to start off by putting names and pictures of people who've been in our group.",
                    "label": 0
                },
                {
                    "sent": "Graduate students and postdocs, and some who are still in our group.",
                    "label": 0
                },
                {
                    "sent": "The people who have their actual pictures up there.",
                    "label": 0
                },
                {
                    "sent": "I think our first authors on the papers that I'll talk about here, but the other people have also contributed in important ways.",
                    "label": 0
                },
                {
                    "sent": "And really, this is their work.",
                    "label": 0
                },
                {
                    "sent": "I'm just lucky to be able to think about it with them and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think about it.",
                    "label": 0
                },
                {
                    "sent": "Now, as Dave said, the goal of what we're trying to do here is really about a bidirectional connection between the fields of human learning and machine learning.",
                    "label": 1
                },
                {
                    "sent": "We sometimes call it reverse engineering the mind, which means we want to understand how the mind works in our best engineering terms, the same framework we could use to build more intelligent systems in systems which are which do learning and inference in more human like ways.",
                    "label": 1
                },
                {
                    "sent": "And we know we're successful when we have real bidirectional idea flow.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The big question that our research has been trying to address for pretty much as long as I've been working in this field, you could put like this.",
                    "label": 0
                },
                {
                    "sent": "How does the mind get so much from so little so across cognition wherever you look, it seems that there's a huge gap between the information content coming in through our senses and what our minds get out of it.",
                    "label": 1
                },
                {
                    "sent": "The knowledge, the understanding we get of the world, the capacity for intelligent behavior, acting in the world that we get.",
                    "label": 0
                },
                {
                    "sent": "It seems that our minds form rich models, powerful generalizations, strong abstractions from data that's limited in so many ways.",
                    "label": 0
                },
                {
                    "sent": "Sparse, noisy, ambiguous.",
                    "label": 1
                },
                {
                    "sent": "And the basic problem we want to understand is how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Let me just start off by giving you one of my favorite demos.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This, if you've seen recent talks for me, you've probably seen some version of this, but it's always a fun way to start off.",
                    "label": 0
                },
                {
                    "sent": "It's inspired it so it's a lab experiment that we've done with adults, but it's inspired by a basic problem of cognitive development, children's learning, how you learn the meaning of words, so anybody who's had a child or been a child knows that children have this remarkable ability to get a word from just one or a few examples, like here's a horse.",
                    "label": 0
                },
                {
                    "sent": "Here's a chair.",
                    "label": 0
                },
                {
                    "sent": "Here's a bottle.",
                    "label": 0
                },
                {
                    "sent": "Here is a telephone, and now they can use that and pick out new examples.",
                    "label": 0
                },
                {
                    "sent": "Not perfectly, but amazingly well.",
                    "label": 0
                },
                {
                    "sent": "Just from one or a few labeled instances.",
                    "label": 0
                },
                {
                    "sent": "So here's our version of this with objects that should look somewhat natural to you but not familiar.",
                    "label": 0
                },
                {
                    "sent": "I can teach you a new word in this alien language.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say the word too far.",
                    "label": 0
                },
                {
                    "sent": "You see a few examples there and now you can go through and tell me which other things are tough as in which ones aren't.",
                    "label": 0
                },
                {
                    "sent": "So I think I might be able to point at the screen here if you can see this arrow, so tell me is that a 2 for yes or no?",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Speak up this one.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "I heard someone expressing uncertainty.",
                    "label": 0
                },
                {
                    "sent": "The little delay in reaction time there is also a way that psychologists know how to diagnose uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So you can see with just a few examples you're very confident, except in a couple of borderline cases.",
                    "label": 0
                },
                {
                    "sent": "How is that possible?",
                    "label": 0
                },
                {
                    "sent": "By the standards of conventional classification in machine learning, this kind of almost one shot.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it seems rather amazing and the same problem occurs all over the place in perception.",
                    "label": 0
                },
                {
                    "sent": "Like how do we get the 3D structure of the world from a stream of 2D images?",
                    "label": 1
                },
                {
                    "sent": "Other areas of concept learning?",
                    "label": 1
                },
                {
                    "sent": "How do we learn the hidden properties of objects from interacting with them?",
                    "label": 0
                },
                {
                    "sent": "Language.",
                    "label": 0
                },
                {
                    "sent": "That language isn't just about learning words, but lower level things like sound structure and higher level structures like syntax.",
                    "label": 0
                },
                {
                    "sent": "Again, these problems come up inferring causal relation, so there's been a lot of great progress in statistics and machine learning for inferring causal networks from data.",
                    "label": 0
                },
                {
                    "sent": "But children solve this amazing problem.",
                    "label": 0
                },
                {
                    "sent": "They are often able to get calls.",
                    "label": 0
                },
                {
                    "sent": "Elations again from just the right kind of spatial temporal coincidence of pattern in one or a few events, not even enough to compute by traditional statistics.",
                    "label": 0
                },
                {
                    "sent": "Any reliable correlation, let alone make the legendarily hard jump to causation.",
                    "label": 0
                },
                {
                    "sent": "Recently, our work and a lot of cognitive scientists have been interested in larger scale systems of knowledge.",
                    "label": 0
                },
                {
                    "sent": "What are sometimes called intuitive theories or common sense theories like intuitive physics or intuitive psychology systems of concepts that we use to parse our experience.",
                    "label": 0
                },
                {
                    "sent": "So we see things in motion in the world, but we don't just see them as flow fields.",
                    "label": 1
                },
                {
                    "sent": "Or or other kinds of kinematics, we see the forces, the dynamics.",
                    "label": 0
                },
                {
                    "sent": "We have an intuitive physics when we see other people moving around.",
                    "label": 0
                },
                {
                    "sent": "They're not just bodies in motion subject to Newtonian forces, but we interpret their actions in terms of as intentional agents with mental states, beliefs and desires, and it's working backwards to that.",
                    "label": 0
                },
                {
                    "sent": "That's a key importance for social interaction.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to make another demo of this kind of thing here, look at the black and white video down on the lower right.",
                    "label": 0
                },
                {
                    "sent": "This is this is a very old movie and it's just a 22nd clip from several minutes of video, but it's maybe the most famous video in psychology, hydrion Simul recorded this and stop action animation made back in the 1940s and studied the kind of interpretation people put on this.",
                    "label": 0
                },
                {
                    "sent": "What you'll see here I'm going to start playing it now is just some shapes in motion, but do you see it as triangles and circles moving?",
                    "label": 0
                },
                {
                    "sent": "What do you see?",
                    "label": 0
                },
                {
                    "sent": "It looks like a social interaction, right?",
                    "label": 0
                },
                {
                    "sent": "It looks like the people are laughing little nervous right?",
                    "label": 0
                },
                {
                    "sent": "The big triangle seems like he's kind of bullying and beating up the little triangle and the little circle is hiding.",
                    "label": 0
                },
                {
                    "sent": "And now after the little triangle is slightly chased away, the big trend is going after him.",
                    "label": 0
                },
                {
                    "sent": "So where does that come from?",
                    "label": 0
                },
                {
                    "sent": "This is just basically a 10 dimensional time series.",
                    "label": 0
                },
                {
                    "sent": "If you think about the XY positions and orientations and the door position, but yet on top of that very sparse signal, you put all this rich social interpretation.",
                    "label": 0
                },
                {
                    "sent": "So these are the kind of problems the scope of what we're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "Some are easy.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our mothers are this ones pretty hard now.",
                    "label": 0
                },
                {
                    "sent": "The approach that we pursue you could call learning with knowledge and by this I mean to contrast with the conventional machine learning approach, particularly applied machine learning where basically you do ML when you don't know anything.",
                    "label": 0
                },
                {
                    "sent": "But here this is all about learning with rich knowledge.",
                    "label": 0
                },
                {
                    "sent": "So the key questions we want to understand these three here.",
                    "label": 0
                },
                {
                    "sent": "How does some kind of abstract knowledge?",
                    "label": 1
                },
                {
                    "sent": "Whatever it is that fills in the gap of our sense experience guide learning and inference from sparse data?",
                    "label": 1
                },
                {
                    "sent": "What form does that knowledge take across different domains and tasks?",
                    "label": 1
                },
                {
                    "sent": "How do we represent it and how is that knowledge itself acquired and we are approaching these questions with the tool kit that is mostly familiar to.",
                    "label": 0
                },
                {
                    "sent": "Nips crowd right so that includes Bayesian inference to just understand how priors and guide data.",
                    "label": 0
                },
                {
                    "sent": "The tools for sort of what you might call learning to learn, or the acquisition of the Priors are again quite popular these days, like hierarchical Bayesian models and Bayesian nonparametrics, and I'll show you using show you things we've been doing that again go between cognitive science and M with those kind of techniques.",
                    "label": 0
                },
                {
                    "sent": "What's maybe a little bit on the fringe for nips, but not so much for, say, the more probabilistic AI crowd are defining probabilistic models, not just over the conventional.",
                    "label": 0
                },
                {
                    "sent": "Objects of statistics, or say high dimensional vector spaces like in neural networks, but over more structured kinds of knowledge representations.",
                    "label": 0
                },
                {
                    "sent": "The sort of data structure is more traditionally associated with computer science in classical symbolic AI.",
                    "label": 0
                },
                {
                    "sent": "So you know, we know about graphs, but also grammars and predicate logic schemas, even programs, and defining probabilistic models on top of those data structures, I think, is going to be absolutely critical for human like intelligence.",
                    "label": 0
                },
                {
                    "sent": "I'll try to convince you of these.",
                    "label": 0
                },
                {
                    "sent": "My version of that as we go on here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I also for Nips I want to draw it an interesting contrast or really it's something that inspired a lot of us.",
                    "label": 0
                },
                {
                    "sent": "The those of us studying high level cognition look to the part of the brain that we understand the best.",
                    "label": 0
                },
                {
                    "sent": "The visual system basically and see inspiration, but also a difference in what we're doing.",
                    "label": 0
                },
                {
                    "sent": "A lot of the best work in understanding, say, visual perception as a kind of statistical inference, indeed kind of Bayesian inference came out of the NIPS community, so there's work that is classic like.",
                    "label": 1
                },
                {
                    "sent": "Yeah, you're why Sonero Simoncelli title, since work on emotion perception, or say.",
                    "label": 0
                },
                {
                    "sent": "Convert cording and wolpert's work.",
                    "label": 0
                },
                {
                    "sent": "What this kind of work is showing is that by making a sort of fairly straightforward Bayesian framework with simple but reasonable priors like a prior that motion in an image is slow and smooth.",
                    "label": 0
                },
                {
                    "sent": "You can explain both these amazing successes of, say, human motion perception.",
                    "label": 0
                },
                {
                    "sent": "Also, some of the quirks.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can go even further than this, like for example the work on natural scene statistics.",
                    "label": 1
                },
                {
                    "sent": "So this is just one of many papers by Wainwright and Shorts and Simoncelli, where it's not just positing some prior that you pull out of your head, 'cause it's nice to work with, like a Gaussian, but you can go out and measure the actual empirical statistics of natural scenes and show that can be used to predict behavior and neural responses, and you might ask, could we extend that kind of natural scene statistics approach to cognition in some of the early work that this probabilistic cognitive science sort of subfield has been doing?",
                    "label": 0
                },
                {
                    "sent": "We do see that, so here's an example of a study that Tom Griffiths and I did.",
                    "label": 0
                },
                {
                    "sent": "Or we gave people these.",
                    "label": 0
                },
                {
                    "sent": "Basically, these textbook Bayesian statistics problems.",
                    "label": 0
                },
                {
                    "sent": "It's basically the problem of interval estimation.",
                    "label": 0
                },
                {
                    "sent": "So you see I'll just give you some these examples, but we set them in it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In everyday setting where people might have, we thought good intuitions.",
                    "label": 0
                },
                {
                    "sent": "So suppose you read about a movie that's made $60,000,000 to date.",
                    "label": 1
                },
                {
                    "sent": "How much money will it make in total?",
                    "label": 1
                },
                {
                    "sent": "Or you see something.",
                    "label": 1
                },
                {
                    "sent": "Some baking in the oven for 34 minutes?",
                    "label": 0
                },
                {
                    "sent": "How long until it's ready?",
                    "label": 1
                },
                {
                    "sent": "You meet someone who is 78 years old?",
                    "label": 0
                },
                {
                    "sent": "How long will they live?",
                    "label": 0
                },
                {
                    "sent": "And so on in each of these cases, you encounter some phenomenon that has some unknown extent or duration teetotal at some random time or value T less than T total, and you have to guess he total from that one observation, and we can go and look at the natural scene statistics.",
                    "label": 0
                },
                {
                    "sent": "So along the top here you see the empirical statistics for these classes of events.",
                    "label": 0
                },
                {
                    "sent": "And then on the bottom you see two things.",
                    "label": 0
                },
                {
                    "sent": "You see behavioral data.",
                    "label": 0
                },
                {
                    "sent": "The little dots are median subjects, predictions of the total duration as a function.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give an example and then you see the optimal Bayesian predictions from the empirical statistics and it's quite striking how how close this is.",
                    "label": 0
                },
                {
                    "sent": "I mean, to get to get this right here people have to do two things.",
                    "label": 0
                },
                {
                    "sent": "They have to be Bayesian, but they also have to have the right priors and it's quite interesting that they seem to be sensitive to the different forms of the distributions.",
                    "label": 0
                },
                {
                    "sent": "For these different classes of everyday events.",
                    "label": 0
                },
                {
                    "sent": "But this approach just going out and measuring the statistics doesn't obviously extend to the more cognitive problems that we're interested in here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, like in the case of word learning, what is the right prior?",
                    "label": 1
                },
                {
                    "sent": "What's the right hypothese space?",
                    "label": 0
                },
                {
                    "sent": "It's not even clear how to define that or go out and measure it in the world and not to mention you had a human learner.",
                    "label": 0
                },
                {
                    "sent": "A brain autonomously figured that out, so we can look for inspiration to again.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asset cognitive psychology, which suggests that natural categories and nameable categories might be organized into something like a hierarchical tree data structure, say with categories at different levels of abstraction.",
                    "label": 0
                },
                {
                    "sent": "So here you have Canaries, and they're a bird, and the bird is an animal, and so on.",
                    "label": 0
                },
                {
                    "sent": "There's even evidence looking at, say, population responses from the high level visual system that visual categories get organized that way, and we've been able to make models of both adults.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Children's world learning with that same kind of idea so we can take, for example, these these alien objects and come up with a tree structure that's shown here such that the branch points of the nodes of this tree correspond to hypotheses of nameable categories, and then we can define a prior based on the branch link.",
                    "label": 0
                },
                {
                    "sent": "Basically how perceptually distinctive category is.",
                    "label": 0
                },
                {
                    "sent": "It's very similar to the probabilistic models using Bayesian phylogenetic's.",
                    "label": 0
                },
                {
                    "sent": "So for modeling distributions of properties arising from mutation and the likelihood just comes from assuming the examples are a random uniform sample from the labeled.",
                    "label": 0
                },
                {
                    "sent": "Branch and then it's pretty clear how to get this kind of behavior here where you see a few examples that seem to cluster in one distinctive branch, and that tells you how to freezer over there in that part of the tree.",
                    "label": 0
                },
                {
                    "sent": "Now we can use this model to make quantitative predictions of peoples judgments for one or a few examples, and I'm showing you just just so you can believe that we can make quantitatively accurate models, people versus this Bayesian model for generalization of a new word from one or a few examples that cluster at different levels of the tree, but I don't want to dwell too much on the experiment because the interesting problems for machine learning are really how do we get to this point?",
                    "label": 0
                },
                {
                    "sent": "Where does this hypothesis space in Pryor come from?",
                    "label": 1
                },
                {
                    "sent": "So we built it as modelers by asking subjects to judge in a separate experiment to judge the similarity of these objects and then doing hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "But how does the brain do it mean in a sense, that's just sort of cheating, right?",
                    "label": 0
                },
                {
                    "sent": "We all know that that there's an infinite number of features you can compute from a visual image, and depending on which features you use or how you weight them, you'll get very different similarity metrics.",
                    "label": 0
                },
                {
                    "sent": "And the real question is basically how do people learn the right notion of similarity?",
                    "label": 0
                },
                {
                    "sent": "Or you might ask more deeply given a similarity metric, you can build a tree by hierarchal clustering.",
                    "label": 0
                },
                {
                    "sent": "But how do you know you should be building a tree?",
                    "label": 0
                },
                {
                    "sent": "Other kinds of domains might be organized in other ways.",
                    "label": 1
                },
                {
                    "sent": "It's not like everything in cognition is tree structured, although it's amazing how often that seems to come up as a natural.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Representation, so these are some of the questions that we've been approaching and you can think of them again as kind of questions of learning to learn.",
                    "label": 1
                },
                {
                    "sent": "If you look at the literature in children's cognitive development, you can see that children actually do learn these things.",
                    "label": 0
                },
                {
                    "sent": "They learn what features of objects to pay attention to for word learning, and they learn to organize things into a tree.",
                    "label": 0
                },
                {
                    "sent": "It's not there initially, so for example, Linda Smith and colleagues studied the development of the shape bias, which is this.",
                    "label": 0
                },
                {
                    "sent": "Is this phenomena you can see right here.",
                    "label": 0
                },
                {
                    "sent": "You show a kid, say a 2 year old this novel object.",
                    "label": 0
                },
                {
                    "sent": "They haven't seen attacks, and which other one is attacks on the right.",
                    "label": 0
                },
                {
                    "sent": "They'll pick out the one with the same shape but a novel texture material as opposed to, say, one that matches in texture and material but not the same shape.",
                    "label": 0
                },
                {
                    "sent": "Kids speaking English have this at two years of age, but younger kids like say 18 month olds don't have this.",
                    "label": 0
                },
                {
                    "sent": "So somewhere in there they learn it and they learn other things a little bit older they learn different biases for different kinds of words or entities like a material bias for words for, say, nonsolid substances, like learning the word toothpaste or the word honey or water.",
                    "label": 1
                },
                {
                    "sent": "It's more about the texture properties of a material than any kind of shape.",
                    "label": 0
                },
                {
                    "sent": "For these non solid substances.",
                    "label": 0
                },
                {
                    "sent": "So it's like kids have to be able to learn what counts for similarity, but they have to learn different similarity metrics for different kinds of concepts.",
                    "label": 0
                },
                {
                    "sent": "And again, as I've already referred to early on children in learning words don't have a hierarchical structure to categories, but what's called mutual exclusivity.",
                    "label": 0
                },
                {
                    "sent": "They assume there's just one way to label each thing, like a flat partition into categories, and then only later, say by age 4, as opposed to age 2, they start to learn that you can have words like mammal or animal or living thing that refer to higher level categories.",
                    "label": 0
                },
                {
                    "sent": "So in some work that we did a few years ago with Charles Campany prefers, we made fairly straightforward simple models of these aspects of learning to learn and apply them to cognitive development.",
                    "label": 0
                },
                {
                    "sent": "But what I want to tell you guys about is some newer work that we've been doing.",
                    "label": 0
                },
                {
                    "sent": "Which is more the same kind of idea, but in the machine learning setting where you can actually appreciate the value of this kind of cognitive learning to learn for the sorts of applications that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That ML is interested in, so here's some work that I've been doing with Russell Kudinov, who's a postdoc at MIT working with me and also with Antonio Torralba, and we framed it in the setting of, say, querying and retrieving images from a database, but this isn't meant to be primarily a vision project, it's just that this is an application that many of us are familiar with and work on, and the same idea could apply to speech or motor action where we wanted to be able to learn, say, a gesture from one example or a new speech sound, or what someone's voice sounds like the just you only have to hear someone talking for.",
                    "label": 0
                },
                {
                    "sent": "A short while to get what their voices like.",
                    "label": 0
                },
                {
                    "sent": "So in the context of image database retrieval, you know standard problem is we want to say query with one image, say a cow over there and we want to want to search over many many images and return the other cows and not the other things right?",
                    "label": 0
                },
                {
                    "sent": "And what you see on the top is the kind of behavior we'd like to get and in fact that is the result of our systems performance on that query.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately if you just take a standard kind of naive similarity based approach, what you get is something that looks like on the bottom.",
                    "label": 0
                },
                {
                    "sent": "You get some cows and some chimneys and some fields.",
                    "label": 0
                },
                {
                    "sent": "Now why is that?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "You know the standard approach for doing this with just one example.",
                    "label": 0
                },
                {
                    "sent": "There's fancy things you can do if you're allowed to query with multiple examples and learn discriminatively, but it's just one example.",
                    "label": 0
                },
                {
                    "sent": "The best you can do is something like similarity, so you take some feature space.",
                    "label": 0
                },
                {
                    "sent": "Here we have this roughly 50,000 dimensional texture of textures feature space that also was introduced at NIPS in the late 90s by Deb and Viola, and this one works very well in the more discriminative setting.",
                    "label": 0
                },
                {
                    "sent": "It's sort of inspiration and basis for, like the Viola Jones face detectors and so on, but in this and it's sort of modeled on the early structure of the visual system with recursively nested.",
                    "label": 0
                },
                {
                    "sent": "Or filters in this setting, though you have this 50,000 dimensional feature space and and all you can do is compute similarity to the query and you don't really know which dimensions count, so something which matches the grass in the background is almost as good as something which actually matches the count.",
                    "label": 0
                },
                {
                    "sent": "Another way to put this is at a minimum what you need for a concept is not just a prototype and similarity, but something like the concept specific similarity metric.",
                    "label": 0
                },
                {
                    "sent": "The particular feature rates that count for this concept that might be different for other concepts, so we've been taking a hierarchical nonparametric Bayes approach to this which we can illustrate like this here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You could say the intuition is summed up by the slogan.",
                    "label": 0
                },
                {
                    "sent": "Similar categories have similar similarity metrics.",
                    "label": 0
                },
                {
                    "sent": "So let's say you've seen a bunch of dogs, horses and sheep and cars, vans, and trucks, various kinds of categories, but only one cow.",
                    "label": 0
                },
                {
                    "sent": "Well, how do you know how to generalize cow?",
                    "label": 0
                },
                {
                    "sent": "If you look at this, it looks like just straight forward Euclidean distance isn't the right thing, but rather you want to do something like this.",
                    "label": 0
                },
                {
                    "sent": "You want to wreck.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Guys that say cows are like dogs, horses and sheep and they tend to extend in the horizontal direction but not at the vertical direction.",
                    "label": 0
                },
                {
                    "sent": "So our intuition is to try to say capture that in a high dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we do that to save this 50,000 dimensional texture texture space.",
                    "label": 0
                },
                {
                    "sent": "So we do that using what's a pretty standard hierarchical model where we have basically Gaussians at each level.",
                    "label": 0
                },
                {
                    "sent": "That allows us to capture both sort of mean and variance structure at each of these.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which you can see illustrated in this picture.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The extra twist compared to, say standard the hierarchical Bayesian statistics, is that we're learning the tree structure, so we only have labeled data at the bottom level.",
                    "label": 0
                },
                {
                    "sent": "These so called basic level categories, and we have to learn in an unsupervised way.",
                    "label": 1
                },
                {
                    "sent": "These higher level super categories like animal in vehicle those aren't given to us, so we put a nested CRP prior.",
                    "label": 0
                },
                {
                    "sent": "It's one of these sort of standard nonparametric priors that allows the number of super classes to be effectively infinite and just introduced just as needed as the data require.",
                    "label": 0
                },
                {
                    "sent": "And then we're inferring that latent hierarchy and using that to share.",
                    "label": 0
                },
                {
                    "sent": "Information about variances, which is really the similarity metric in addition to the prototypes.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of apply.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This to this MSR data set, which is a fairly small data set, but it has a nice kind of sort of superclass structure and you can see that we recovered down in the lower left.",
                    "label": 0
                },
                {
                    "sent": "You can see the.",
                    "label": 0
                },
                {
                    "sent": "The sort of super classes that seem both semantically and visually plausible, and what you can see on the RC curve on the right hand side is how much this buys you for learning to do one shot learning.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the black curve again, none of these are really, you know this isn't like super quality performance because we're just doing one shot.",
                    "label": 0
                },
                {
                    "sent": "Learning with a fairly generic representation, but the key is to be able to say get almost as good as we can from just one example, so the black curve is the best we could possibly get from one example in using this representation, the red curve is what you get if you just use a naive Euclidean similarity metric and it's barely better than chance.",
                    "label": 0
                },
                {
                    "sent": "But the blue curve is what you get when you've used this hierarchical nonparametric model to learn to learn.",
                    "label": 0
                },
                {
                    "sent": "And you see, we're sort of getting to the regime that children are at, where you get almost all of the juice out of just one example.",
                    "label": 0
                },
                {
                    "sent": "Now there's various.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that are a little bit unsatisfying about that model.",
                    "label": 0
                },
                {
                    "sent": "It's I mean it's actually very nice.",
                    "label": 0
                },
                {
                    "sent": "Very tractable, really quite scalable, but in some new work that Russ has been doing, we're trying to sort of push beyond this to say, well, don't just learn weights in a high dimensional feature space, but actually constructed features.",
                    "label": 0
                },
                {
                    "sent": "Now this work hasn't been written up yet.",
                    "label": 0
                },
                {
                    "sent": "I won't talk about it too much.",
                    "label": 0
                },
                {
                    "sent": "It's sort of just a teaser if you want to learn more about it, come to our workshop talk on Saturday, but the basic idea here is to actually construct the features in several layers at the bottom layer is that was basically a restricted Boltzmann machine or a deep network that learns in the typical.",
                    "label": 0
                },
                {
                    "sent": "Way that deep learning operates in a completely unsupervised, general way across a very large data set and on top of that we build one of these hierarchical nonparametric models which learns higher level features.",
                    "label": 0
                },
                {
                    "sent": "Basically topic to topic sort of HTP topic model to model the class specific distributions on the latent variables of the PBM, and that allows us to learn higher level features like parts in a way that's again sensitive to learning.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The hierarchy, so the twist on typical HTP here is that we're doing inference over the latent hierarchy to again learn these super categories, and it does neat things on natural images, but I just want to show you 'cause it's 'cause it's I think, particularly visually striking some things that this is doing with the new data set.",
                    "label": 0
                },
                {
                    "sent": "We've.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did which we call you.",
                    "label": 0
                },
                {
                    "sent": "Could we're not really sure what to call it, but possibilities might be emnace plus plus or enlist transpose or something.",
                    "label": 0
                },
                {
                    "sent": "It's meant to be like Em missed a good data set for testing a wide range of machine learning approaches, and one that we can actually really nail in a sense of like really get it right.",
                    "label": 0
                },
                {
                    "sent": "Unlike natural images which were quite far from that form.",
                    "label": 0
                },
                {
                    "sent": "But the reason why I say it's kind of like the transpose of ethnicity, it's much more natural, like the problem that the children's learning face instead of having as an M nest thousands of examples of roughly ten categories.",
                    "label": 0
                },
                {
                    "sent": "Here we want to have you know order 10 examples of thousands of categories and we've done it by having people on the Amazon Mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "Look at and then draw their own versions of handwritten characters in many different languages, including some that you might know and some that almost nobody knows.",
                    "label": 0
                },
                {
                    "sent": "So I'll just show you some.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data here.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just going to flash through.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause it's pretty fun.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you can?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See in each of these cases.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are just.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically each.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Column R7.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of a handwritten.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Character in some Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so hopefully you can.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See that there's a lot of similarity across.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Columns.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's just.",
                    "label": 0
                },
                {
                    "sent": "This is just a small fraction.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data there's many thousands of categories here and a few examples of each one.",
                    "label": 0
                },
                {
                    "sent": "Right, and hopefully you can see to pick any one of these.",
                    "label": 0
                },
                {
                    "sent": "I just stopped at random.",
                    "label": 0
                },
                {
                    "sent": "Pick any column you like.",
                    "label": 0
                },
                {
                    "sent": "Like let's say I don't know.",
                    "label": 0
                },
                {
                    "sent": "Where is my arrow?",
                    "label": 0
                },
                {
                    "sent": "OK here we go.",
                    "label": 0
                },
                {
                    "sent": "So pick this column.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you can see from just one or a few examples.",
                    "label": 0
                },
                {
                    "sent": "You can get the concept and clearly recognize how the other things in that column belong to the same concept and pretty much everything else in this data set doesn't.",
                    "label": 0
                },
                {
                    "sent": "Now, the models that we've been building here I think are starting to actually capture this.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the images that I'm.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Flashing here are actual.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These learned higher level super classes and you can see just looking at any of them.",
                    "label": 0
                },
                {
                    "sent": "It seems to have organized the basic level classes into coach.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Current higher level classes that share the same.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of style or the same?",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And parts.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The young",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Including Braille.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The learned features include sort of low level V1 like features, but these higher level parts in the HTP and you can draw samples from this model.",
                    "label": 0
                },
                {
                    "sent": "The same sort of thing.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Used to seeing with, say, deep networks, but now in a class sensitive way.",
                    "label": 0
                },
                {
                    "sent": "So here for example on the bottom on the top or real data on the bottom.",
                    "label": 0
                },
                {
                    "sent": "What you see are made up examples of made up classes in this style so, so you should hopefully see coherence at both the.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Level and the superclass level and he.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are made up examples in this.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "While here I made up.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Degrees in this style they made.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples in that style and.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just go on forever.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of sensitivity.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the latent class structure is 1.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I haven't seen.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in typical.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deep networks.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's one that we can get in a nice way by.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Combining these sorts of approach.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or here we can use.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this law.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From few examples.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I'm showing.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3 examples of a new class that's been.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Called out of the data set and then here are generalizations.",
                    "label": 0
                },
                {
                    "sent": "These are conditional samples of the model.",
                    "label": 0
                },
                {
                    "sent": "New new things of that sort, and it's done this by recognizing that is of the same type as the ones shown on the bottom and then using the learned prior for that soup.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Class again here I'll show you many examples of each in each case.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Giving 3 examples of.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Totally new class in the models basically.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of getting plowed.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All new instances of that.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Category.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can compute more objective RC curve type results and I won't really bore you with those, but we really can get quite substantial improvements on more generic approaches.",
                    "label": 0
                },
                {
                    "sent": "If you want to learn more about this, come to our transfer learning with rich Models Workshop on Saturday.",
                    "label": 0
                },
                {
                    "sent": "We could say that the goal of this workshop is too.",
                    "label": 0
                },
                {
                    "sent": "Start a new field called Rich Learning which we work on because then all the others are called poor learning.",
                    "label": 0
                },
                {
                    "sent": "Alright, now I want to.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back to some of these basic learning to learn issues here and let me just check how we're doing on time.",
                    "label": 0
                },
                {
                    "sent": "OK, good, so all the models that we've seen so far, including the very simple word learning one in these fancier hierarchical nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "But the key step here is learning some kind of latent tree structure that organizes the categories in the higher level categories, and that's a very powerful, compelling idea, but it's not the only kind of representation that the brain or or saying AI system should build up the world.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to understand.",
                    "label": 0
                },
                {
                    "sent": "How can a system learn.",
                    "label": 0
                },
                {
                    "sent": "Let's say for something like this problem of organizing categories and labels, a tree structure is the right way to go, but maybe for other kinds of tasks or domains, some qualitatively different form of structure is the right approach.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This challenge of discovering the form of structured data is a very compelling one in cognitive development.",
                    "label": 0
                },
                {
                    "sent": "As I said, it's one that children have to learn.",
                    "label": 0
                },
                {
                    "sent": "In the case of, say, the hierarchical structure of nameable categories, or other cases like you learn, for example, the days of the week or other kinds of temporal categories have more of a cyclical structure or the click structure of social networks.",
                    "label": 0
                },
                {
                    "sent": "All of these qualitatively different forms, or in science in key moments of scientific discovery.",
                    "label": 0
                },
                {
                    "sent": "The big insight isn't just like a little detail, but total reorganization like going for example, in biology.",
                    "label": 0
                },
                {
                    "sent": "What happens, starting with Linnaeus and then into Darwin, recognizing that biological categories shouldn't be arranged into what Europeans had previously called the Great chain of being a 1 dimensional structure with basically rocks down at one end and got up at the other end and going all the way through from fish and birds and mammals and humans and Angels up to God, that one dimensional chain structure is 1 plausible way to.",
                    "label": 0
                },
                {
                    "sent": "Think about living things, but when you actually look seriously about at a wide range of data, you can just see that tree structure or take for example Mendeleev's and contemporaries discovery of the periodic structure of the chemical elements.",
                    "label": 0
                },
                {
                    "sent": "Again, neither Linnaeus, even Darwin or Mendeleev understood the causal forces that gave rise to this darn was just beginning at the time he started drawing trees like this was just beginning to think about selection in the mature sense.",
                    "label": 0
                },
                {
                    "sent": "Mendeleev certainly didn't understand quantum mechanics.",
                    "label": 0
                },
                {
                    "sent": "That gives rise to the periodic table, they were just.",
                    "label": 0
                },
                {
                    "sent": "Looking at the data and finding the form of structure.",
                    "label": 0
                },
                {
                    "sent": "Now we have lots of great unsupervised learning algorithms for finding structure in data, but pretty much all assume a fixed form of structure and then just find the best structure of that form.",
                    "label": 0
                },
                {
                    "sent": "So if we want to learn trees, we have hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "If we want to learn flat cluster structure, we have K means or mixture models.",
                    "label": 0
                },
                {
                    "sent": "If we want to learn some kind of dimensional structure, we have both linear and nonlinear manifold techniques.",
                    "label": 0
                },
                {
                    "sent": "But maybe what we want is to put it somewhat ambitiously, is something like this and more universal framework that replaces this grab bag toolbox of techniques with a more.",
                    "label": 0
                },
                {
                    "sent": "Principled and general purpose approach that's able to not just learn all of these different forms of structure, but figure out which is the right one.",
                    "label": 0
                },
                {
                    "sent": "So Charles camp.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some really neat work when he was in my lab on this project and this was published year two ago in PNS and the basic idea.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of ideas here, but one is to come up with a universal language for representing all these different forms of structure in the same terms.",
                    "label": 0
                },
                {
                    "sent": "And for that we're using graphs.",
                    "label": 0
                },
                {
                    "sent": "Each of you know if you want to talk about dimensional structure or manifold structure trees, either latent hierarchies, are observable trees.",
                    "label": 0
                },
                {
                    "sent": "If you want to talk about flat clusters, those are all different forms of graphs.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we will define distributions on these graphs to fit to the data.",
                    "label": 0
                },
                {
                    "sent": "But another key key idea is that each of these different forms of structure can be generated by a simple process.",
                    "label": 0
                },
                {
                    "sent": "What you think of as a graph grammar, like a rule for taking a node in a graph, and it's in and out links and replacing it with two nodes.",
                    "label": 0
                },
                {
                    "sent": "So growing a little bit of structure and the appropriate in and out link portion to those two new nodes.",
                    "label": 0
                },
                {
                    "sent": "And that's that is a simple generative model for these forms of structures, different graph grammars, different rules are a way to parameterise these different structural forms.",
                    "label": 0
                },
                {
                    "sent": "So now we can cast this learn.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem in a hierarchical framework like this where the lowest level is the observable data.",
                    "label": 0
                },
                {
                    "sent": "You know just a typical matrix and then we have these two latent levels that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "There is the level of what we call the structure, which is some graph, and it's a graph of a particular form that's generated by some simple graph grammar.",
                    "label": 0
                },
                {
                    "sent": "At the higher level, and by taking single productions or cross products of productions like, Say, a cross product of two chain rules gives you a 2 dimensional grid, I skipped over it before but like for example across down in the lower right.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Across the chain and a ring gives you a cylinder structure.",
                    "label": 0
                },
                {
                    "sent": "And So what we want to do is basically do.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inference at both levels of this hierarchical model as a way of learning the right structure and or the right form of structure, and the best structure of that form.",
                    "label": 0
                },
                {
                    "sent": "Or you could think of it as a kind of structural grammar induction, like by analogy to language where you want to learn the grammar at the same time as you have to use that grammar to parse the sentence is in order to even know how well it fits, so I don't have time to tell you about.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The details of our probabilistic model I just refer you to the paper, but the basic idea should be again, pretty much familiar to a Bayesian machine learning crowd.",
                    "label": 0
                },
                {
                    "sent": "There's sort of the.",
                    "label": 0
                },
                {
                    "sent": "A Bayesian Occam's razor in the probability of a graph structure given a form which tends to prefer simpler forms, like for example the chain structural aflat cluster structure is the simplest to chain structure.",
                    "label": 0
                },
                {
                    "sent": "Is is a little bit more complex, but simpler than a tree, and so on.",
                    "label": 0
                },
                {
                    "sent": "And then the probability of the data given the structure.",
                    "label": 0
                },
                {
                    "sent": "For that, we're using some really neat ideas that again are now just sort of classics in the NIPS community.",
                    "label": 0
                },
                {
                    "sent": "The idea that you can use a graph to parameterise a Gaussian process just to capture the notion of a smooth distribution of data by something like the covariance being the some kind of normalized inverse graph.",
                    "label": 0
                },
                {
                    "sent": "Ossian and the particular work that's close to this is the work that Jerry Zhu and Lafferty and.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Garmony did, but there's many probably equivalent approaches and what we're doing here is basically searching over this these two levels to find the best grammar and the best graph of that form, and I can tell you details of that later if interested, but just to show you a few results of using this approach given a data set of animals and their features, and these aren't, you know, genetic features.",
                    "label": 0
                },
                {
                    "sent": "These are the kind of things that child would see if they walk around the zoo or read a picture book like a Walrus, has flippers or dolphin, has tail, or a dog barks, that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "The model is able to is able to identify implausible tree that captures something of the hierarchical structure of biological categories as a normal human child might understand them, but also to identify that it should be a tree as opposed to some other kind of structure.",
                    "label": 0
                },
                {
                    "sent": "So given, say for example a data set of more of a political domain, Supreme Court opinions, these are all justices who served on the Rehnquist Court.",
                    "label": 0
                },
                {
                    "sent": "Then it identifies not a tree, but more of a chain structure capturing the left right or liberal conservative.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spectrum if we if we give the model say for example data that comes from a space of faces, these are realistically rendered synthetic faces where we tweaked interface synthesizer.",
                    "label": 0
                },
                {
                    "sent": "Two knobs corresponding to dimensions of basically masculinity and a racial dimension.",
                    "label": 0
                },
                {
                    "sent": "So you can see if you go sort of from top to bottom here what looks like a light to black dimension and if you go from left to right you can see a sort of subtle masculinity variation.",
                    "label": 0
                },
                {
                    "sent": "The ones on the left are supposed to be more heavily masculine faces.",
                    "label": 0
                },
                {
                    "sent": "It might help if you think that the ones on the left are like the football team and the ones on the right are more like the tennis team.",
                    "label": 0
                },
                {
                    "sent": "And our model is able to identify those two underlying dimensions that there is a dimensional structure and to recover the right dimensions or for example, given given proximity between cities in the on the Earth, it identifies a cylinder which basically captures the representation of latitude and longitude.",
                    "label": 0
                },
                {
                    "sent": "One other interest.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Developmental point comes out of this and I think it's also provides a lesson for how we want to develop abstract knowledge in machine learning.",
                    "label": 0
                },
                {
                    "sent": "We can look at a learning curve by giving this model more and more data and what we see is that when there's very little data because of the Bayes, Occam's Razor.",
                    "label": 0
                },
                {
                    "sent": "Basically, the model is worried about overfitting, so it finds a structure which is qualitatively simpler than the correct one.",
                    "label": 0
                },
                {
                    "sent": "Just this flat cluster structure up there on the upper left, and that's the same sort of thing we see with the youngest children at the mutual exclusivity stage.",
                    "label": 0
                },
                {
                    "sent": "Everything is just in one category and it captures plausable high level categories like birds and insects and so on.",
                    "label": 0
                },
                {
                    "sent": "Then, with a little bit more data, it gets the idea of a tree structure.",
                    "label": 0
                },
                {
                    "sent": "It gets the right grammar but not the right tree.",
                    "label": 0
                },
                {
                    "sent": "It takes a lot more data that's going down to the lower left to get to work out all the little details, figure out that Penguins shouldn't go in with Dolphins, seals and whales and so on like that.",
                    "label": 0
                },
                {
                    "sent": "And this idea of kind of getting the big picture first, it's it's a phenomenon that comes up in a lot of the hierarchical models that we've built for these higher level cognitive phenomena, which we call the blessing of abstraction.",
                    "label": 0
                },
                {
                    "sent": "So phrase it's due to Noah Goodman, and it's just really interesting human like learning dynamic where you getting the.",
                    "label": 0
                },
                {
                    "sent": "Higher level knowledge.",
                    "label": 0
                },
                {
                    "sent": "First, it contrasts with the traditional way that cognitive scientists and machine learning researchers approach the development of abstraction, right?",
                    "label": 0
                },
                {
                    "sent": "So think about, say, the way you build a typical deep network as you learn it, layer by layer.",
                    "label": 0
                },
                {
                    "sent": "Almost by definition, you're assuming that you learn the simple concrete features 1st and then the more abstract features.",
                    "label": 0
                },
                {
                    "sent": "But it's an important feature of more cognitive human learning.",
                    "label": 0
                },
                {
                    "sent": "This idea of getting the big picture 1st and it's one that comes distinctively out of these more structured hierarchical models.",
                    "label": 0
                },
                {
                    "sent": "Now in the last few minutes what I want to talk about is some of the work that the directions that I'm most excited about for the field, not just for what I'm doing, but for the whole interface between cognitive science, machine learning, and AI.",
                    "label": 0
                },
                {
                    "sent": "You can maybe could call it graphical models plus plus or something.",
                    "label": 0
                },
                {
                    "sent": "What you could see an we realized in the work that we're doing there with the structural grammar induction and a number of people have been realizing you know over the last few years is that we're sort of approaching the limits of what we can do with our traditional graphical models Toolkit.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Call the statisticians toolkit.",
                    "label": 0
                },
                {
                    "sent": "You know this sort of inference over fixed sets of variables.",
                    "label": 0
                },
                {
                    "sent": "Random variables linked by simple or at least well understood probability distributions.",
                    "label": 0
                },
                {
                    "sent": "You could say that up until that graph grammar induction, everything I said could have been done by a Bayesian statistician.",
                    "label": 0
                },
                {
                    "sent": "If they only they were crazy enough.",
                    "label": 0
                },
                {
                    "sent": "But this this other thing is, trust me.",
                    "label": 0
                },
                {
                    "sent": "Way too crazy for a Bayesian statistician.",
                    "label": 0
                },
                {
                    "sent": "And when you start to think about problems of real cognition like say theory of mind or intuitive physics, it's goes way beyond that toolkit and brings us to what looks like they need to have more of a computer scientist toolkit in there with the statistics toolkit.",
                    "label": 0
                },
                {
                    "sent": "So there's different names for this approach.",
                    "label": 0
                },
                {
                    "sent": "I like the name probabilistic programming, which was the subject of a nips workshop back in 2008 where the idea is to is to really be doing, say, inference over flexible data structures, not just not just traditional notions of random variables or.",
                    "label": 0
                },
                {
                    "sent": "Or fixed finite structures like graphs and to be able to go beyond the simple kinds of distributions that we inherit from statistics and stochastic processes to what you might call stochastic programs as the basis for a generative models distributions that in some sense we don't really get analytic insight into until we run them and see how they work.",
                    "label": 0
                },
                {
                    "sent": "So we saw.",
                    "label": 0
                },
                {
                    "sent": "I mean, well we saw yesterday in my Jordans talk a very nice articulation of this and Mike has been one of the people articulating this most clearly and forcefully.",
                    "label": 0
                },
                {
                    "sent": "So yesterday he talked about machine learning for proteins and.",
                    "label": 0
                },
                {
                    "sent": "And gave examples of things where you know you can't really describe this in a nice way as a graphical model where you have to use real scientific knowledge, for example like biochemistry or quantum chemistry or evolutionary biology.",
                    "label": 0
                },
                {
                    "sent": "Here is an example.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From another paper that Mike was involved with, this is work of per ceilings and colleagues on this infinite PC FG and it's just like but in a much probably more sophisticated way than what we were doing with these graph structural grammars.",
                    "label": 0
                },
                {
                    "sent": "Here's a here's an infinite.",
                    "label": 0
                },
                {
                    "sent": "Model for parsing and language, where as you can see in their in their figure description.",
                    "label": 0
                },
                {
                    "sent": "They just make it quite clear that you can't really represent this as a graphical model, but you can give what looks like a program, basically a symbolic specification over on the left of the generative model.",
                    "label": 0
                },
                {
                    "sent": "It just doesn't fit into our graphical models Toolkit.",
                    "label": 0
                },
                {
                    "sent": "Now we see.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most clearly, in the more advanced examples I started off with, like Remember this, this hydrogen symbol example here, I just don't think there's any plausable graphical model here, but there is a plausable probabilistic program to describe this kind of action.",
                    "label": 0
                },
                {
                    "sent": "Understanding or theory of mind, and we've made a lot of progress recently that we're quite excited about.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To spell that out, so just to be clear, the way that psychologists approach these kind of problems is you could draw.",
                    "label": 0
                },
                {
                    "sent": "It looks kind of like a graphical model.",
                    "label": 0
                },
                {
                    "sent": "You could save peoples actions are a function of their beliefs and desires.",
                    "label": 0
                },
                {
                    "sent": "If you assume that an agent is rational, that's a mapping from beliefs and desires to actions, and you have to work backwards to figure out the latent mental states, the unobservable beliefs and desires that explain the actions, and you can cast that as a Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "But the real content here is being done by the process that's inside those arrows, and if you want to really capture the idea of irrational agent, you need something like a pond.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The solver, so that's a program, right?",
                    "label": 0
                },
                {
                    "sent": "So this is what we've been doing.",
                    "label": 0
                },
                {
                    "sent": "We've been basically saying, as a first approximation, intentional agents have things like beliefs and desires which can be cast into utility framework, and then their actions are the result of solving something like a palm DP.",
                    "label": 0
                },
                {
                    "sent": "And then we want to work backwards, invert that that process, and it's very related to inverse optimal control and inverse RL, which again have been pioneered in this community.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to show you how we're using this to model action understanding in humans, this isn't.",
                    "label": 0
                },
                {
                    "sent": "Again, this isn't a model of how people act, it's a model of how people understand intentional agents.",
                    "label": 0
                },
                {
                    "sent": "Their intuitive theory of mind.",
                    "label": 0
                },
                {
                    "sent": "So for example, here you can see the basics sketch over in the left people see a point like agent, just like in that hydrogen symbol video moving in a simple kind of room or maze.",
                    "label": 0
                },
                {
                    "sent": "There may be a wall there may not be a well, there may be a hole in the wall.",
                    "label": 0
                },
                {
                    "sent": "There's three goal objects AB&C and they see a trajectory which may get to one of the objects or usually stops somewhere along the way.",
                    "label": 0
                },
                {
                    "sent": "And they have to make a judgment when it stops about how likely the agent's goal was to be a B or C, and you can just see it.",
                    "label": 0
                },
                {
                    "sent": "Some of some of the data from conditions here.",
                    "label": 0
                },
                {
                    "sent": "There's many, many conditions I can't show you all of 'em, but here are a bunch of stimuli in the top row with different environments and different paths.",
                    "label": 0
                },
                {
                    "sent": "And then here in the 2nd row are how people's judgments about the agents goal red, green or blue evolve overtime, and you can see all sorts of interesting dynamics going on.",
                    "label": 0
                },
                {
                    "sent": "Is the person thinks the agent may be changed his mind, or went back or change their mind about what the agent is saying and the model is able to capture this.",
                    "label": 0
                },
                {
                    "sent": "You can see there on the bottom very strikingly, capturing subtle quantitative details up here on the top.",
                    "label": 0
                },
                {
                    "sent": "The scatter plot just shows over all the trials over all the data points are very strong.",
                    "label": 0
                },
                {
                    "sent": "Correlation between what this model says and what people say, and it's a very simple model.",
                    "label": 0
                },
                {
                    "sent": "It just assumes that the agent gets some utility for getting his goal.",
                    "label": 0
                },
                {
                    "sent": "He has a small cost, small negative utility for each step he takes, and there's one extra parameter.",
                    "label": 0
                },
                {
                    "sent": "There's basically there's two numerical free parameters here.",
                    "label": 0
                },
                {
                    "sent": "There's the agent is assumed to be only probabilistically rational, so he doesn't always do the best thing according to his palm DP.",
                    "label": 0
                },
                {
                    "sent": "Solver and also he.",
                    "label": 0
                },
                {
                    "sent": "He there's some small probability he can change his mind or change his goal at anytime, and you may not notice that or you don't have any outward sign or direct sign of that except how he moves to the space so that these strong switches and goals are explained by that.",
                    "label": 0
                },
                {
                    "sent": "Now this actually this here is not actually a palm DP.",
                    "label": 0
                },
                {
                    "sent": "It's more of an MVP because we assume that both the observer and the agent have full knowledge.",
                    "label": 0
                },
                {
                    "sent": "But if we really want to do theory of mind and make joint inferences about agents beliefs which could be false or could be incomplete because maybe they haven't seen the whole environment, then that takes us to a palm DP.",
                    "label": 0
                },
                {
                    "sent": "And so here's this to some recent work I should say.",
                    "label": 0
                },
                {
                    "sent": "By the way, this is work of Chris Bakers.",
                    "label": 0
                },
                {
                    "sent": "Where yeah?",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're on, we're actually doing joint inference about basically tracking and belief space, and also inference about an agents preferences.",
                    "label": 0
                },
                {
                    "sent": "So the domain where we studied this is similar to the ones you've seen.",
                    "label": 0
                },
                {
                    "sent": "We call it the food truck domain so you can see over here on the left a little scenario.",
                    "label": 0
                },
                {
                    "sent": "We have some grad student on a hypothetical college campus.",
                    "label": 0
                },
                {
                    "sent": "Very small college campus here.",
                    "label": 0
                },
                {
                    "sent": "Every day he gets lunch at one of the food trucks.",
                    "label": 0
                },
                {
                    "sent": "There's three food trucks, the Korean K, the Lebanese L and the Mexican M. But there's only two parking spots on any one day, at most two, and sometimes only one of these trucks is present on the campus.",
                    "label": 0
                },
                {
                    "sent": "So you see Harold start over here.",
                    "label": 0
                },
                {
                    "sent": "Let me get my cursor.",
                    "label": 0
                },
                {
                    "sent": "So Harold starts here and he goes like this.",
                    "label": 0
                },
                {
                    "sent": "Done 1010, Ten ten 1010 long here.",
                    "label": 0
                },
                {
                    "sent": "More slowly.",
                    "label": 0
                },
                {
                    "sent": "He goes here.",
                    "label": 0
                },
                {
                    "sent": "Remember he can have sort of line of sight axis so he can now see what's on the side of the wall and he goes back to K. So the question is.",
                    "label": 0
                },
                {
                    "sent": "There's K over here.",
                    "label": 0
                },
                {
                    "sent": "There's L here.",
                    "label": 0
                },
                {
                    "sent": "What's his favorite food truck?",
                    "label": 0
                },
                {
                    "sent": "KL or M?",
                    "label": 0
                },
                {
                    "sent": "M right interesting because M isn't anywhere present in the scene.",
                    "label": 0
                },
                {
                    "sent": "You know it can be there, but there's no direct perceptual evidence.",
                    "label": 0
                },
                {
                    "sent": "There's nothing like he's headed towards me.",
                    "label": 0
                },
                {
                    "sent": "You just interpret the fact that he was looking for and then maybe disappointed he didn't see it.",
                    "label": 0
                },
                {
                    "sent": "So he went to what his next favorite.",
                    "label": 0
                },
                {
                    "sent": "And that's what you can see.",
                    "label": 0
                },
                {
                    "sent": "Here is both people in the model's am is the best, then K and then L. And they also make a kind of corresponding belief in France.",
                    "label": 0
                },
                {
                    "sent": "He must have actually thought Em was likely to be there.",
                    "label": 0
                },
                {
                    "sent": "That's what you see here on this bar.",
                    "label": 0
                },
                {
                    "sent": "Otherwise he wouldn't have gone to the trouble to go and look and again across many different scenarios.",
                    "label": 0
                },
                {
                    "sent": "We got a pretty good correlation on both preferences and believes not as good as in the simpler case, but still pretty good.",
                    "label": 0
                },
                {
                    "sent": "You know, certainly by the standards of what.",
                    "label": 0
                },
                {
                    "sent": "Until recently I mean really until this work was only treated as a qualitative.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tuition this theory of mind.",
                    "label": 0
                },
                {
                    "sent": "Some of the other things we've been doing along the same lines is intuitive physics, so you can here's one more demo just to close out.",
                    "label": 0
                },
                {
                    "sent": "It's fun and easy.",
                    "label": 0
                },
                {
                    "sent": "Just look at this table here with these red and yellow blocks and say if I bump the table in one of the blocks falls off.",
                    "label": 0
                },
                {
                    "sent": "Is it more likely to be red or yellow?",
                    "label": 0
                },
                {
                    "sent": "Since yell it out so we can hear you?",
                    "label": 0
                },
                {
                    "sent": "Red or yellow?",
                    "label": 0
                },
                {
                    "sent": "Red",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, OK, so pretty much everybody agrees, although there's a.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variation, how does that work?",
                    "label": 0
                },
                {
                    "sent": "How did you do that?",
                    "label": 0
                },
                {
                    "sent": "So our approach is to is to sort of take physics and scenes seriously.",
                    "label": 0
                },
                {
                    "sent": "So like a few other people in the community, there was really nice poster from CMU on this yesterday.",
                    "label": 0
                },
                {
                    "sent": "You know, we think that you can understand vision is actually inferring 3D model.",
                    "label": 0
                },
                {
                    "sent": "It's really inverse graphics and the hypothesis space and the forward mapping is basically you know, Pixar physics is physics.",
                    "label": 0
                },
                {
                    "sent": "So we've been building these ideal observer models in which we assume people have a probabilistic version of Newtonian mechanics and then they can run.",
                    "label": 0
                },
                {
                    "sent": "Little simulations, not very accurate, but little simulations to reason about these things, and we can test this, for example, in a similar case we haven't done the red and yellow yet, but for example we can show.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People these towers of blocks and say, is it stable or not?",
                    "label": 0
                },
                {
                    "sent": "Will it fall over so these?",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look pretty stable.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one no right that will follow.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over this one.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probably unstable.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ones are a little more border.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can get again very good correlations just with very rapid presentations between what people, how stable people think this is, and the predictions of this model that somehow assumes that you're approximately inverting the graphics to come up with the 3D block description and running a little brief noisy physics simulation to figure out what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "Even the same kind of models can even apply in simple cases to infants and be able to predict infants looking time, which is the.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Standard measure of what infants know, but until recently hasn't been related in any kind of problem in kind of a quantitative way to any model, let alone a probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So just.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To wrap up, then I'll point to what I think is really on the API side.",
                    "label": 0
                },
                {
                    "sent": "The technology that's going to allow these little basically first model stabs that we're building and others are building to really hopefully try to capture something about human common sense and take that over into an is the idea of actually building general tools the so called probabilistic programming languages, which give you a way to express all these models and generic inference tools.",
                    "label": 0
                },
                {
                    "sent": "Often some kind of approximate inference, a lot of them are using Monte Carlo, but not all of them, and there's basically.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic programming language is built on logic like prologue and there's a number of once we built more functional or imperative programming styles like Lisper Matlab and I won't really go through any of the details here, but it's very exciting that these technologies are there.",
                    "label": 0
                },
                {
                    "sent": "Certainly not mature, but they are under development and it's going to make it possible to develop and work with these kinds of models and even to frame learning as what you might call a kind of program.",
                    "label": 0
                },
                {
                    "sent": "Induction or cognitive development as a kind of program synthesis.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is far out stuff I don't want to pretend this is easy or accessible in any really satisfying rigorous way at this point.",
                    "label": 0
                },
                {
                    "sent": "But I would venture a prediction that sooner or later we're going to have to view learning this way, because otherwise we just can't explain how this kind of knowledge develops, I think.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of the work I showed you like the graph, grammar stuff or other work we're doing with the handwritten characters.",
                    "label": 0
                },
                {
                    "sent": "For example, Brendan Lake is doing some work on actually what I think is sort of the right model for the handwritten characters is actually inferring a motor program.",
                    "label": 0
                },
                {
                    "sent": "If you look at how people actually draw those those characters from one example, they all draw them pretty much the same way.",
                    "label": 0
                },
                {
                    "sent": "They don't just have a visual concept, but it's actually a motor sequence concept.",
                    "label": 0
                },
                {
                    "sent": "Or there's a lot of really cool work going on in NLP modeling.",
                    "label": 0
                },
                {
                    "sent": "The acquisition of language is basically synthesis of simple kinds of programs.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll wrap up the big.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem that we in this field are trying to understand it.",
                    "label": 0
                },
                {
                    "sent": "It's really a joint project with machine learning and AI is how the mind or anything like a mind get so much from so little across all these different domains.",
                    "label": 0
                },
                {
                    "sent": "We have a toolkit which builds on the core Bayesian statisticians tool Kit and how it's been extended in ML with for example hierarchical and nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "But it's also one that has to draw I think on the computer scientist Toolkit, including not just interesting data structures but probabilistic programming languages and to a cognitive scientist.",
                    "label": 0
                },
                {
                    "sent": "The value of this is you know what I've shown you here is things like want it nice quantitative models, but even more.",
                    "label": 0
                },
                {
                    "sent": "And I think this is also true for AI tools for thinking and thinking a little bit differently.",
                    "label": 0
                },
                {
                    "sent": "So instead of getting stuck in these usual debates that have wracked both cognitive science and AI for decades is the mind should be understandable, logical probability or is it nature versus nurture?",
                    "label": 0
                },
                {
                    "sent": "Is it learned or is it wired and we can really do?",
                    "label": 0
                },
                {
                    "sent": "We don't have to have those debates anymore because we can ask much more interesting questions like how can you actually learn real knowledge with some appropriate kind of probabilistic inference?",
                    "label": 0
                },
                {
                    "sent": "Or how can you learn some very rich domain specific knowledge with the domain general mechanism again also for the.",
                    "label": 0
                },
                {
                    "sent": "Learning crowd if we want to think about where real abstract knowledge comes from, we have some slightly different ways of thinking about it here.",
                    "label": 0
                },
                {
                    "sent": "It doesn't just have to be built up slowly from the ground up, but very abstract knowledge which we can see in real infants, is present quite early on.",
                    "label": 0
                },
                {
                    "sent": "I didn't show you that for the Hydra symbol, but next to it was another similar display that works even for preverbal infants.",
                    "label": 0
                },
                {
                    "sent": "How abstract knowledge can be learned surprisingly early, and how the idea of having sort of structured symbolic knowledge doesn't have to be brittle handwired, but can actually deal with noise and insert and even potentially be learned.",
                    "label": 0
                },
                {
                    "sent": "In some kind of robust way that looks something like growing in mind, thanks.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first I want to congratulate you on getting the right generalization of deep learning from a single example.",
                    "label": 0
                },
                {
                    "sent": "Bingo, but now I've got a question.",
                    "label": 0
                },
                {
                    "sent": "If you take a deep belief net and you train it on data that contains 10 long skinny manifolds, it will learn 10 long thin energy ravines in the top level DBM and actually do more than that.",
                    "label": 0
                },
                {
                    "sent": "'cause if you look at it at a coarser energy scale, three of those ravines for three, five and eight will sort of form 1 ravine, which is the beginnings of a little bit of tree structure.",
                    "label": 0
                },
                {
                    "sent": "So the question is, instead of seducing Russ into putting trees on top of our BMS, why don't you let him do RBM's all the way up and get the trees to emerge as emergent properties of the energy landscape?",
                    "label": 0
                },
                {
                    "sent": "So at the deep or at the no.",
                    "label": 0
                },
                {
                    "sent": "Sorry, not at the Deep Learning Workshop at the next one on the transfer learning with Rich Models Workshop on Saturday will be explicitly compare this with some deep belief net or deep deep Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "Russes version of deep belief Nets.",
                    "label": 0
                },
                {
                    "sent": "And we are actually trying to compare the man it be interesting to see what kind of structure has to emerge.",
                    "label": 0
                },
                {
                    "sent": "I mean we know that when you look at the brain.",
                    "label": 0
                },
                {
                    "sent": "You know this is the basic insight of neural networks.",
                    "label": 0
                },
                {
                    "sent": "Is is you can't argue with it, right?",
                    "label": 0
                },
                {
                    "sent": "What you see in the brain is a bunch of very, very large numbers of spiking units, and somehow those have to in some implicit way represent the kinds of structures were talking about.",
                    "label": 0
                },
                {
                    "sent": "If those structures are there at all, and I think it's the interface between these different approaches that can try to yield inside that.",
                    "label": 0
                },
                {
                    "sent": "But part of why we were trying to develop both this kind of data set and this kind of task is so we can study that in a serious cognitive way.",
                    "label": 0
                },
                {
                    "sent": "You know, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't have any doubt that deep networks.",
                    "label": 0
                },
                {
                    "sent": "Can learn these kinds of things, but the question is can they learn them as fast as an as flexibly as children do?",
                    "label": 0
                },
                {
                    "sent": "And there we have some reasons to think that this more kind of explicitly structured probabilistic model might be better for that.",
                    "label": 0
                },
                {
                    "sent": "But let's, let's talk about that more at the workshop.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thanks.",
                    "label": 0
                },
                {
                    "sent": "So could you say something about language learning?",
                    "label": 0
                },
                {
                    "sent": "What do you want me to say?",
                    "label": 0
                },
                {
                    "sent": "You know another result.",
                    "label": 0
                },
                {
                    "sent": "It seems like you are.",
                    "label": 0
                },
                {
                    "sent": "Trying to create a system which you start with a certain syntactical structure and then you figure out how to tweak the parameters.",
                    "label": 0
                },
                {
                    "sent": "That sounds suspiciously like Chomsky's view of language.",
                    "label": 0
                },
                {
                    "sent": "Chomsky doesn't think so.",
                    "label": 0
                },
                {
                    "sent": "Well, you know the.",
                    "label": 0
                },
                {
                    "sent": "That's the way a lot of other people think.",
                    "label": 0
                },
                {
                    "sent": "Even yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's interesting that.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think it actually illustrates the point I'm trying to make on this slide here at this point about sort of going beyond some of these classic dichotomy's, which is that you know, all of us inherit this dichotomous way of thinking, and I've just been.",
                    "label": 0
                },
                {
                    "sent": "I've been struck by how the work that we do.",
                    "label": 0
                },
                {
                    "sent": "Basically people who are more strongly inclined towards a more sort of empiricist everything learned you say, well, that's kind of interesting, but way too Chomskyan or way to nativists, whereas the people you know.",
                    "label": 0
                },
                {
                    "sent": "At MIT, trust me, I have many colleagues like this and they are quite vocal.",
                    "label": 0
                },
                {
                    "sent": "They see it, you know, quite the opposite.",
                    "label": 0
                },
                {
                    "sent": "Like I gave a talk in Marvin Minsky's class and he said that's all very interesting.",
                    "label": 0
                },
                {
                    "sent": "But why do you have to keep saying all that Bayesian stuff?",
                    "label": 0
                },
                {
                    "sent": "And it's funny that he didn't notice that there were some other words in there too.",
                    "label": 0
                },
                {
                    "sent": "So I think that there are some ways.",
                    "label": 0
                },
                {
                    "sent": "I mean, in some sense, at the same time, as are some of my nativist linguists, colleagues say this is way too imperious.",
                    "label": 0
                },
                {
                    "sent": "They also say, but it's also even more nativist, and we are, 'cause you posit all this incredibly rich structure to the mind, and I think there are just.",
                    "label": 0
                },
                {
                    "sent": "There isn't just a single, maybe another way to view what's going on here.",
                    "label": 0
                },
                {
                    "sent": "If there isn't just one dimension.",
                    "label": 0
                },
                {
                    "sent": "So what we have in common, I would say with the more sort of classic AI, or Chomskyan approach in language, is saying the mind has the ability, and somehow we must have it from the start to work with very rich kinds of knowledge representations.",
                    "label": 0
                },
                {
                    "sent": "But what we have in common with more of machine learning is that we think in many cases, most if not all of that structure is learned in the interesting part of it is.",
                    "label": 0
                },
                {
                    "sent": "Actually responds to the data of experience, and the trick is how could that possibly work?",
                    "label": 0
                },
                {
                    "sent": "If you look at the early Chomskyan linguistics like before, Chomsky was known for being a very strong nativist in the early days of generative grammar, like back in the late 50s and the 60s.",
                    "label": 0
                },
                {
                    "sent": "It looks much more like this.",
                    "label": 0
                },
                {
                    "sent": "The child, the idea was wasn't the child as like parameter center, but it was a child as linguist, right?",
                    "label": 0
                },
                {
                    "sent": "And the idea was language acquisition was searching through a big space of grammars, and they gave up on that.",
                    "label": 0
                },
                {
                    "sent": "Because I'm so I'm told, and it makes sense.",
                    "label": 0
                },
                {
                    "sent": "They just thought that will never work.",
                    "label": 0
                },
                {
                    "sent": "And I think part of the reason why they they came to perhaps the premature conclusion that it would never work is because they didn't have the right way to the tools to find statistical models over these spaces of structures.",
                    "label": 0
                },
                {
                    "sent": "And they didn't have computers.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "I mean they didn't have what we would now recognizes computers, and so much of what we're doing, just like much of the best work in this field is enabled by the combination of mathematical developments, lot of it from statistics and computing power, which just lets us do things that you know.",
                    "label": 0
                },
                {
                    "sent": "Would have been unheard of before.",
                    "label": 0
                },
                {
                    "sent": "Josh, can you comment a little on what the next steps are in this kind of science for really determining what are mechanisms of thinking?",
                    "label": 0
                },
                {
                    "sent": "Are there?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you show nice correlations between human performance and algorithms.",
                    "label": 0
                },
                {
                    "sent": "What are the kinds of experiment?",
                    "label": 0
                },
                {
                    "sent": "That's a great question.",
                    "label": 0
                },
                {
                    "sent": "Thank you for asking it.",
                    "label": 0
                },
                {
                    "sent": "Yes, so pretty much all that I've talked about here is what in psychology is often called an ideal observer model, right?",
                    "label": 0
                },
                {
                    "sent": "You try to study the functional relation between the inputs and the outputs.",
                    "label": 0
                },
                {
                    "sent": "It's the level that is really most directly in common between all the fields were talking about.",
                    "label": 0
                },
                {
                    "sent": "But you don't actually say certainly anything about the hardware mechanisms or even about the actual algorithms you know.",
                    "label": 0
                },
                {
                    "sent": "I just showed you some probability functions.",
                    "label": 0
                },
                {
                    "sent": "I didn't tell you how I optimize them or search or did it did inference or how the brain might do it.",
                    "label": 0
                },
                {
                    "sent": "And that's that's basically where as a field we've started.",
                    "label": 0
                },
                {
                    "sent": "But we're definitely interested in getting down to these lower, more mechanistic levels of analysis.",
                    "label": 0
                },
                {
                    "sent": "It's we see it again, it's kind of in contrast to the neural network approach, which takes more of a bottom up approach where you start with an idea of how the brain works and try to see how cognition emerges.",
                    "label": 0
                },
                {
                    "sent": "This is a complementary approach.",
                    "label": 0
                },
                {
                    "sent": "Where you started, the more functional level and then say OK Now that you have some evidence that this captures the behavioral phenomena, how could that be implemented in the brain?",
                    "label": 0
                },
                {
                    "sent": "And a lot of the most exciting work over the last few years?",
                    "label": 0
                },
                {
                    "sent": "It's just a whole other talk, and it's work that I'm doing a little bit, but a lot of it is being done a lot more of it is being done by people like Tom Griffiths or contained many people in the Faneuil there, Ed Vul.",
                    "label": 0
                },
                {
                    "sent": "Noah Goodman is interested in this but sometimes called on rational process models where you try to come up with more psychologists.",
                    "label": 0
                },
                {
                    "sent": "Called process is really the algorithms of the mind.",
                    "label": 0
                },
                {
                    "sent": "And what's neat about this particular group is they're doing it informed by all of the machine learning a probabilistic AI that that you know this community is developing.",
                    "label": 0
                },
                {
                    "sent": "So instead of just kind of making up ideas about how the mind works, which is how cognitive psychology has had to do it for decades, you can go and look at the engineering side for hypothesis, you know.",
                    "label": 0
                },
                {
                    "sent": "So there's work on saying, do people actually, I guess I even have a slide on this.",
                    "label": 0
                },
                {
                    "sent": "Do people do MCMC or do they do important sampling?",
                    "label": 0
                },
                {
                    "sent": "Or do they do particle filtering and it's very exciting and cool?",
                    "label": 0
                },
                {
                    "sent": "That we actually are starting to build this multiple level picture where we have the kind of probabilistic.",
                    "label": 0
                },
                {
                    "sent": "That you know the sort of public, ideal observer level and going down from algorithm control levels and emerging unifying idea that many people are converging on is what I think was called recently by Joseph Fisher.",
                    "label": 0
                },
                {
                    "sent": "The sampling hypothesis that the idea that it's really some kind of sophisticated Montecarlo, that is the basis for both understanding how these learning and inference these models can be implemented in the brain.",
                    "label": 0
                },
                {
                    "sent": "And I think the AI and statistics side has to support that because it's the Monte Carlo approach to approximate inference that seems to be the best hope for it completely generic tools for working with very.",
                    "label": 0
                },
                {
                    "sent": "Richly structured representations.",
                    "label": 0
                },
                {
                    "sent": "That's very speculative at this point still.",
                    "label": 0
                },
                {
                    "sent": "So I think I have a related question which is.",
                    "label": 0
                },
                {
                    "sent": "Or at the computational level, as the models get kind of more and more rich and complicated.",
                    "label": 0
                },
                {
                    "sent": "I mean if if you drop a palm DP solver and treat it as a likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Where do you go with that in terms of doing more than existence proof?",
                    "label": 0
                },
                {
                    "sent": "Like really understanding what it is about that solver that that right?",
                    "label": 0
                },
                {
                    "sent": "So that I mean that even I'm not going to take seriously as an actual mechanistic account 'cause you know, Chris goes in, crunches on his model for a day and then comes back with predictions for the experiment.",
                    "label": 0
                },
                {
                    "sent": "Yet people do it like that.",
                    "label": 0
                },
                {
                    "sent": "But again, it's actually.",
                    "label": 0
                },
                {
                    "sent": "I think it is quite consistent with this view.",
                    "label": 0
                },
                {
                    "sent": "So here were inspired by work on.",
                    "label": 0
                },
                {
                    "sent": "Basically there's a planning is inference tradition, which again was developed by people in this community.",
                    "label": 0
                },
                {
                    "sent": "And doing approximate inference as basically simple kinds of sampling based approximate inference.",
                    "label": 0
                },
                {
                    "sent": "That's something which can be plausibly embedded as an inner loop in that model.",
                    "label": 0
                },
                {
                    "sent": "So basically what you imagine if I want to understand what you're doing I just you could say I kind of put myself in your shoes and draw a few forward samples conditioned on how you're acting.",
                    "label": 0
                },
                {
                    "sent": "Try to see what matches up with that, and work backwards to what mental states might have caused it.",
                    "label": 0
                },
                {
                    "sent": "And it's a very rough intuitive description.",
                    "label": 0
                },
                {
                    "sent": "But so, for example, Noah Goodman has some nice examples of actually implementing that in the Church probabilistic programming language, and it's much more algorithmically plausable.",
                    "label": 0
                },
                {
                    "sent": "Approach to this kind of kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So how can we get falsifiable hypothesis of human cognition?",
                    "label": 0
                },
                {
                    "sent": "So, for example, suppose that you're sampling hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Yeah, seems all the data we have suggests that yes, what we're predicting is goes along with what we're observing humans do.",
                    "label": 0
                },
                {
                    "sent": "But you know there might be doing something completely different.",
                    "label": 0
                },
                {
                    "sent": "And how would we know the difference?",
                    "label": 0
                },
                {
                    "sent": "That is to say, do we have to wait until we actually have the neural circuits to understand?",
                    "label": 0
                },
                {
                    "sent": "And really whether we're right or not?",
                    "label": 0
                },
                {
                    "sent": "Or is there some other way to get it possible?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, this is the usual problem of science.",
                    "label": 0
                },
                {
                    "sent": "It's it's an interesting illusion that some people seem to be under that.",
                    "label": 0
                },
                {
                    "sent": "That science works by falsification.",
                    "label": 0
                },
                {
                    "sent": "I won't mention the connections to certain other statistical methodologies that are related, but I think the way real science works is you.",
                    "label": 0
                },
                {
                    "sent": "You certainly want falsifiable hypothesis, but the best we can do is come up with ideas and test them out and we can have heuristics for coming up with ideas or various heuristic tools.",
                    "label": 0
                },
                {
                    "sent": "You know, a lot of data driven science.",
                    "label": 0
                },
                {
                    "sent": "We look for automated ways to generate these, but fundamentally we don't know.",
                    "label": 0
                },
                {
                    "sent": "Though I mean, because this was a very broad high level overview talk.",
                    "label": 0
                },
                {
                    "sent": "I didn't go into the details of how we actually approached anyone of these projects, so in any one of the projects I showed you, here's a model here Sunday to look at fits.",
                    "label": 0
                },
                {
                    "sent": "What we actually do is test a bunch of different models and compare them against each other, both Bayesian and non Bayesian.",
                    "label": 0
                },
                {
                    "sent": "You know 'cause a lot of psychologists are.",
                    "label": 0
                },
                {
                    "sent": "Not all sold on the whole Bayesian thing and every reviewer reasonably wants us to test out a range of alternative models.",
                    "label": 0
                },
                {
                    "sent": "So that's so we do take that approach in any individual project, but as a whole the framework like what I'm putting out here that's not falsifiable anymore than Connectionism, is falsifiable or symbolic AI like you.",
                    "label": 0
                },
                {
                    "sent": "Just it's pragmatic.",
                    "label": 0
                },
                {
                    "sent": "It's like a scientific paradigm.",
                    "label": 0
                },
                {
                    "sent": "You have to test it out, and if it if it seems productive, if it seems to fit a draw together.",
                    "label": 0
                },
                {
                    "sent": "Insights from a lot of different data and suggest new experiments in that cycle keeps going.",
                    "label": 0
                },
                {
                    "sent": "That's how we know we're making progress, but you know, I think what if you want to know where to go?",
                    "label": 0
                },
                {
                    "sent": "How can we possibly know if we're on the right track?",
                    "label": 0
                },
                {
                    "sent": "Well, this is partly where and I think this is what's so exciting compared to other approaches that cognitive psychology or neuroscience have had.",
                    "label": 0
                },
                {
                    "sent": "The bidirectional idea flow and interchange with machine learning AI is so important because it provides this whole extra source of constraint model has to actually work 'cause the brain works and by.",
                    "label": 0
                },
                {
                    "sent": "Building models that we also not only fit to peoples data but also tested in real machine learning and AI settings.",
                    "label": 0
                },
                {
                    "sent": "It keeps us honest and keeps that keeps generating.",
                    "label": 0
                },
                {
                    "sent": "I hope better and better hypothesis.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm sure we should stop.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}