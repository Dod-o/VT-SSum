{
    "id": "hz4ctbr4mzrmsn7s6gxz3l7kadm74ptc",
    "title": "Searching the Web with Low Space Approximations",
    "info": {
        "author": [
            "Andras Benczur, Hungarian Academy of Science"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2006",
        "category": [
            "Top->Computer Science->Web Search"
        ]
    },
    "url": "http://videolectures.net/fws06_benczur_swlsa/",
    "segmentation": [
        [
            "So sorry for not waking up early enough to have my slide on in time, but I can quickly.",
            "Cash that delayed by the previous thoughts of already said everything that I was about to present the preliminaries and definitions, so I will.",
            "I will have a very fast talk based on the previously costs."
        ],
        [
            "What I will be talking about this?",
            "Yeah, go back for a second.",
            "I will talk about about results with my students and colleagues from from Budapest, and for sloppiness I will.",
            "Talk about all these results as we, but it's a subset of these people and I may not be an author on all the papers and I will survey some of our results so you will.",
            "You will hear this talk from Thomas a few days later next week, so I will just show a few pieces out of them and instead I will talk more about some preliminaries and some new results of Damash there.",
            "That's still a manuscript."
        ],
        [
            "So I will be talking about personalized Pagerank similarity search and.",
            "And singular value decomposition low dimensional.",
            "Approximations and what's common in these topics is the first is relevance to web information retrieval.",
            "We've already had three dogs that were all talking about that.",
            "For example, even about that they're using in spam hunting and our previous result is mentioned.",
            "I will not be talking about this.",
            "This paper, not because you've already heard about that.",
            "The life is not planning to do that.",
            "Anne.",
            "Another common in all these problems is that you have to at some stage deal with ambion matrices.",
            "That's infeasible to store well.",
            "Of course, it's it's incredibly infeasible for the entire web, but even for the experiments that we play with, for example, Stanford Web base with 80 or even more million pages ambion, it's too much.",
            "And hence we need randomized approximation and the typical solutions that we basically had so far were all based on sampling and I will talk a bit about that and the new results that I will show is instead of sampling, use sketching and random projections to improve sampling based results.",
            "So the key.",
            "Conclusion of my talk should be used sketching and projections instead of sampling to get better results."
        ],
        [
            "First part of my talk will be about personalized page rank.",
            "The definition to quickly go through the definition slides is you've got a teleportation distribution.",
            "The only thing you have to pay attention to is so far instead of my damping factor C. Each talk use Alpha or something else so civil be my teleportation probability and.",
            "So personalized page rank is linear, so it suffices to personalize all single nodes, so I will be talking about how to compute personalized Pagerank.",
            "Their teleportation is to a single node.",
            "And.",
            "And our algorithms will have an unrestricted choice on this set of vertices V. Many papers presented the idea of writing personalized Pagerank as summation of paths that start from.",
            "The teleportation node U and reach to the anode very computing the personalized page rank of the personalized on you.",
            "So it's a fact some with geometric distribution of the length of the paths and the probability of taking that path.",
            "And this path summation formula leads to our previous results of sample according to this distribution, sample paths at this leads us the first algorithm that has no restriction of what to personalized on, so you don't have to select.",
            "Set of hub nodes or special topics just any vertex.",
            "It's a relative approximation with ratio one plus minus epsilon with a small probability of being out of bounds, and it uses this much of a space for a single node U.",
            "So if you're personalizing to everything, then it's an times this amount and you have to remember is silent to the minus second and well log in is you have to represent a node by bits.",
            "But the main thing is this excellent, excellent for the minus.",
            "Second, that's revealed improved."
        ],
        [
            "Hard to improve what's.",
            "What's the key idea that we should do?",
            "Power iteration works.",
            "Top down we're personalizing on you.",
            "We are starting our paths at you and going downwards.",
            "And in this way we are propagating large variance downwards.",
            "John Breeden presented dynamic programming the dining programming algorithm as a little small part of that of their paper that says do a bottom up approach and compute personalized page rank personalizing on U by averaging personalization on use our neighbors.",
            "So compute first personalization.",
            "On all these nodes and then average are.",
            "In this way we are just averaging the errors so errors never accumulate.",
            "The the problem with implementing this algorithm is given that the web is a small word, the non zeros in this kind of an iteration grow very quickly as we are reaching more and more neighbors, and that's the point where our new algorithms."
        ],
        [
            "Come into picture at first algorithm that we could call the sloppy attendant who runs changes down to the nearest euro, which is epsilon in our case an if this algorithm requires just this amount of space to store us sparse vector.",
            "Compared to the epsilon to the minus second, so that fixes this part and the algorithm.",
            "It's almost as simple as I said, so that's the key idea.",
            "Of course you need a bit more technicalities that I'm hiding, but the main thing is it's probably harder to prove a matching communication complexity lower bound which says this is the amount of space you really have to use in order to be able to serve.",
            "List queries so in smaller space you just won't be able to tell what's the top page rank personalized on a given vertex.",
            "Another algorithm we have which could be called the Drunken Surfer.",
            "It mixes drunken surfer mixes of memories, adds up memories of a random hash of pages.",
            "This is just saying user count Min Sketch in another way, use one over data surfers and use the minimum Volt.",
            "Anne.",
            "Now this can be plugged into dynamic programming because of the linearity of sketches, and this uses that amount of space again instead of minus.",
            "Second is just one over F silent, and this is the algorithm that's based off them off for single value queries for a query of given a personalized node, you watch the personal aspect drink of my page V."
        ],
        [
            "OK, next topic SIM rank.",
            "Perhaps here I should spend more time on defining the preliminaries.",
            "SIM rank is based on the idea of Jhanvi damn that 2 pages are similar if they are pointed to bark by similar pages, meaning that the similarity of pages you want and you too it's the summation over the neighbors of U-1 and U2.",
            "Neighbors, we wanted a few, one and V2 of.",
            "YouTube With the decay factor of of C. This corresponds to, again path summation.",
            "Actually submission of path path pairs.",
            "So if I'm interested in personalized page rank of of SIM rank between V1 and V2 I'm I have to start at the same node yuan.",
            "Sample paths that pass pairs of equal lengths that get to my target and that will be that will have the expectation as they as these seem, rank is up there.",
            "So this leads to, again an algorithm that's gonna nap, silent to the minus second in this space requirements and."
        ],
        [
            "Our new idea is let's try to use our personalized page rank sketching and and rounding algorithms.",
            "In order to be able to compute the same rank, we're trying to reduce SIM rank to personalized page rank.",
            "Based on.",
            "The idea is that they are both just acccounting, so let's why not try.",
            "Account pass from the one and V2.",
            "That may need.",
            "OK, that's the difference.",
            "Is that in this summation, I'm having paths that not just meet at a starting point, but maybe several other times.",
            "This can be can also define this self similarity of an old V. SIM rank of we will itself by definition should be one, but it could be extended like in this way, saying, well, OK, a page can be more similar to itself if it's pointed to by more similar pages.",
            "And self similarity of iteration.",
            "T + 1 means we can have.",
            "We must have at least three plus while meeting points, so I'm requiring at least T and adding one more.",
            "And we can obtain some rank by inclusion exclusion by substracting adding different teas.",
            "This leads us to an algorithm that's got lots of technicalities, in particular to.",
            "To combine the inclusion, exclusion and and the scalar products with the approximation bounds.",
            "But actually it turns out this inclusion exclusion converges for small damping factors that's used in practice anyway.",
            "And it leads us to.",
            "To space optimal SIM rank algorithms.",
            "At."
        ],
        [
            "So let's have an example.",
            "Maybe this is the most complicated concept.",
            "In the page I'm part of my talk, let's see this graph.",
            "The simmer, simmer and between V1 and V2 it should be 1 / 12.",
            "It should be this 1 / 4 * 1 / 3.",
            "But if I'm just scalar taking the scalar product of personalized stage ranks than my path will go on and will have.",
            "And a power series or longer bath that should actually stop at the first meeting point.",
            "Sine overcounting, with this factor 3 / 2 in this particular case.",
            "Self similarity of each UI you want you to use three.",
            "Here is 1 / 2 same power series.",
            "And if I'm computing Requireing 1, two and even more meeting points that I'm getting 1 / 4, one over 8 and so on.",
            "So the self similarity of you I will be 2 / 3 and that's exactly I'm have to multiplying with to get rid of the.",
            "Overcounting here sign multiplying with the self similarity to get rid of overcounting.",
            "K Max stop."
        ],
        [
            "Peak singular value decomposition and the very recent result of domash singular value decomposition is already mentioned in at least two previous talks.",
            "It's got important applications like spectral clustering, climbers hits algorithm, latent semantic indexing and so on.",
            "The task is let's find a low rank matrix that minimizes the Frobenius error.",
            "Some squares error of the input matrix.",
            "The solution to this minimization problem is the singular value decomposition that's got known algorithms, but for example, they've got running times like this.",
            "It's very slow for the applications we're shooting for.",
            "Sampling based results were known and the best sampling based results can go down to very small.",
            "Fractions of error, but the error is of the four of is relative to the Frobenius norm of the original matrix and not relative to the.",
            "To the output error, so this the input Frobenius error can be significantly larger than what I'm shooting for.",
            "So I'm shooting for an accuracy of low rank approximation, get that can be approximated with some amount and I'm not saying I'm approximating with slightly more than that, but I'm saying I might be approximation approximating with something much larger than that."
        ],
        [
            "K so recent results are fixing this gap.",
            "Actually, three independent results appeared as men.",
            "They are sparse.",
            "I know manuscripts at the time heart lead.",
            "So memory Anne Anne Thomas and.",
            "The idea is to project the input to an R dimensional subspace and then run SVD with much improved running time that depend on the size of this subspace.",
            "To compare, for example these two results, they have very similar outlook.",
            "But Arceus for example pass effective?",
            "It's better suited for streaming data 'cause it needs only reading twice.",
            "Reading the Matrix compared to K log K. It's even better in the dimension of the projection that we need to take in order to run SVD before.",
            "And.",
            "And we are using random linear combination of rows.",
            "In comparison with random sampling in this result, our results heavily builds on many previous results and the next two slides I'm trying to show the idea of what's the difference between sampling for SVD computation and random linear combinations.",
            "So let's see."
        ],
        [
            "Slow sampling as an idea that was already used.",
            "In several primary results.",
            "And the core idea is the core staff.",
            "In an SVD computation is approximating a matrix product.",
            "In random sampling and taking this matrix.",
            "And considering the product as the sum of dyads.",
            "And I'm sampling dyads.",
            "It's a product of of an ice column and the ice room.",
            "For example, the blue column and the blue roll, and I'm sampling a few, hopefully large dietze that reduces the number of terms I have to eventually compute an sampling must depend on the data I have to sample.",
            "Hopefully large dietze.",
            "And actually dependence of the sampling on the data is the key problem that sampling based algorithms have to face."
        ],
        [
            "Format thanks and trying to speed up.",
            "So the core idea of.",
            "Offer so our idea is instead let's let's compute.",
            "Each entry of the Matrix as dot as dot products of the ice road and the Jakes Column, and use low dimensional low distortion embeddings.",
            "In order to compute the DOT products with shorter vectors, so I'm having this long vector.",
            "I'm making random projection to random combination linear combinations and now I'm just multiplying.",
            "22 drywall vectors, and these embeddings that we are using is data independent so we don't have to make assumptions of trying to select large dietze and so on.",
            "It's completely independent of what the input data is.",
            "K ask."
        ],
        [
            "Conclusion We have seen space optimal summaries for fully personalized page rank and SIM rank for Bella, at least the practically useful ranges of decay factors and the fast relative error SVD algorithm and these algorithm in the hard both had low space approximations of large vectors and the norms were either the maximum.",
            "Or the two norms.",
            "The set of pay."
        ],
        [
            "As I was talking about is rather spam rank paper, I just quickly mention we had the two sampling based page rank ansim rank results."
        ],
        [
            "The new dog that you will hear next week and the manuscript and also paper that contains techniques for.",
            "Communication complexity lower bound."
        ],
        [
            "So thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So sorry for not waking up early enough to have my slide on in time, but I can quickly.",
                    "label": 0
                },
                {
                    "sent": "Cash that delayed by the previous thoughts of already said everything that I was about to present the preliminaries and definitions, so I will.",
                    "label": 0
                },
                {
                    "sent": "I will have a very fast talk based on the previously costs.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I will be talking about this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, go back for a second.",
                    "label": 0
                },
                {
                    "sent": "I will talk about about results with my students and colleagues from from Budapest, and for sloppiness I will.",
                    "label": 0
                },
                {
                    "sent": "Talk about all these results as we, but it's a subset of these people and I may not be an author on all the papers and I will survey some of our results so you will.",
                    "label": 0
                },
                {
                    "sent": "You will hear this talk from Thomas a few days later next week, so I will just show a few pieces out of them and instead I will talk more about some preliminaries and some new results of Damash there.",
                    "label": 0
                },
                {
                    "sent": "That's still a manuscript.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will be talking about personalized Pagerank similarity search and.",
                    "label": 1
                },
                {
                    "sent": "And singular value decomposition low dimensional.",
                    "label": 0
                },
                {
                    "sent": "Approximations and what's common in these topics is the first is relevance to web information retrieval.",
                    "label": 0
                },
                {
                    "sent": "We've already had three dogs that were all talking about that.",
                    "label": 0
                },
                {
                    "sent": "For example, even about that they're using in spam hunting and our previous result is mentioned.",
                    "label": 0
                },
                {
                    "sent": "I will not be talking about this.",
                    "label": 0
                },
                {
                    "sent": "This paper, not because you've already heard about that.",
                    "label": 0
                },
                {
                    "sent": "The life is not planning to do that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Another common in all these problems is that you have to at some stage deal with ambion matrices.",
                    "label": 0
                },
                {
                    "sent": "That's infeasible to store well.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's it's incredibly infeasible for the entire web, but even for the experiments that we play with, for example, Stanford Web base with 80 or even more million pages ambion, it's too much.",
                    "label": 0
                },
                {
                    "sent": "And hence we need randomized approximation and the typical solutions that we basically had so far were all based on sampling and I will talk a bit about that and the new results that I will show is instead of sampling, use sketching and random projections to improve sampling based results.",
                    "label": 0
                },
                {
                    "sent": "So the key.",
                    "label": 0
                },
                {
                    "sent": "Conclusion of my talk should be used sketching and projections instead of sampling to get better results.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First part of my talk will be about personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "The definition to quickly go through the definition slides is you've got a teleportation distribution.",
                    "label": 0
                },
                {
                    "sent": "The only thing you have to pay attention to is so far instead of my damping factor C. Each talk use Alpha or something else so civil be my teleportation probability and.",
                    "label": 0
                },
                {
                    "sent": "So personalized page rank is linear, so it suffices to personalize all single nodes, so I will be talking about how to compute personalized Pagerank.",
                    "label": 0
                },
                {
                    "sent": "Their teleportation is to a single node.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And our algorithms will have an unrestricted choice on this set of vertices V. Many papers presented the idea of writing personalized Pagerank as summation of paths that start from.",
                    "label": 0
                },
                {
                    "sent": "The teleportation node U and reach to the anode very computing the personalized page rank of the personalized on you.",
                    "label": 0
                },
                {
                    "sent": "So it's a fact some with geometric distribution of the length of the paths and the probability of taking that path.",
                    "label": 0
                },
                {
                    "sent": "And this path summation formula leads to our previous results of sample according to this distribution, sample paths at this leads us the first algorithm that has no restriction of what to personalized on, so you don't have to select.",
                    "label": 0
                },
                {
                    "sent": "Set of hub nodes or special topics just any vertex.",
                    "label": 0
                },
                {
                    "sent": "It's a relative approximation with ratio one plus minus epsilon with a small probability of being out of bounds, and it uses this much of a space for a single node U.",
                    "label": 0
                },
                {
                    "sent": "So if you're personalizing to everything, then it's an times this amount and you have to remember is silent to the minus second and well log in is you have to represent a node by bits.",
                    "label": 0
                },
                {
                    "sent": "But the main thing is this excellent, excellent for the minus.",
                    "label": 0
                },
                {
                    "sent": "Second, that's revealed improved.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hard to improve what's.",
                    "label": 0
                },
                {
                    "sent": "What's the key idea that we should do?",
                    "label": 0
                },
                {
                    "sent": "Power iteration works.",
                    "label": 0
                },
                {
                    "sent": "Top down we're personalizing on you.",
                    "label": 0
                },
                {
                    "sent": "We are starting our paths at you and going downwards.",
                    "label": 0
                },
                {
                    "sent": "And in this way we are propagating large variance downwards.",
                    "label": 0
                },
                {
                    "sent": "John Breeden presented dynamic programming the dining programming algorithm as a little small part of that of their paper that says do a bottom up approach and compute personalized page rank personalizing on U by averaging personalization on use our neighbors.",
                    "label": 0
                },
                {
                    "sent": "So compute first personalization.",
                    "label": 0
                },
                {
                    "sent": "On all these nodes and then average are.",
                    "label": 0
                },
                {
                    "sent": "In this way we are just averaging the errors so errors never accumulate.",
                    "label": 0
                },
                {
                    "sent": "The the problem with implementing this algorithm is given that the web is a small word, the non zeros in this kind of an iteration grow very quickly as we are reaching more and more neighbors, and that's the point where our new algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come into picture at first algorithm that we could call the sloppy attendant who runs changes down to the nearest euro, which is epsilon in our case an if this algorithm requires just this amount of space to store us sparse vector.",
                    "label": 0
                },
                {
                    "sent": "Compared to the epsilon to the minus second, so that fixes this part and the algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's almost as simple as I said, so that's the key idea.",
                    "label": 0
                },
                {
                    "sent": "Of course you need a bit more technicalities that I'm hiding, but the main thing is it's probably harder to prove a matching communication complexity lower bound which says this is the amount of space you really have to use in order to be able to serve.",
                    "label": 0
                },
                {
                    "sent": "List queries so in smaller space you just won't be able to tell what's the top page rank personalized on a given vertex.",
                    "label": 0
                },
                {
                    "sent": "Another algorithm we have which could be called the Drunken Surfer.",
                    "label": 0
                },
                {
                    "sent": "It mixes drunken surfer mixes of memories, adds up memories of a random hash of pages.",
                    "label": 0
                },
                {
                    "sent": "This is just saying user count Min Sketch in another way, use one over data surfers and use the minimum Volt.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Now this can be plugged into dynamic programming because of the linearity of sketches, and this uses that amount of space again instead of minus.",
                    "label": 0
                },
                {
                    "sent": "Second is just one over F silent, and this is the algorithm that's based off them off for single value queries for a query of given a personalized node, you watch the personal aspect drink of my page V.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, next topic SIM rank.",
                    "label": 0
                },
                {
                    "sent": "Perhaps here I should spend more time on defining the preliminaries.",
                    "label": 0
                },
                {
                    "sent": "SIM rank is based on the idea of Jhanvi damn that 2 pages are similar if they are pointed to bark by similar pages, meaning that the similarity of pages you want and you too it's the summation over the neighbors of U-1 and U2.",
                    "label": 0
                },
                {
                    "sent": "Neighbors, we wanted a few, one and V2 of.",
                    "label": 0
                },
                {
                    "sent": "YouTube With the decay factor of of C. This corresponds to, again path summation.",
                    "label": 0
                },
                {
                    "sent": "Actually submission of path path pairs.",
                    "label": 0
                },
                {
                    "sent": "So if I'm interested in personalized page rank of of SIM rank between V1 and V2 I'm I have to start at the same node yuan.",
                    "label": 0
                },
                {
                    "sent": "Sample paths that pass pairs of equal lengths that get to my target and that will be that will have the expectation as they as these seem, rank is up there.",
                    "label": 0
                },
                {
                    "sent": "So this leads to, again an algorithm that's gonna nap, silent to the minus second in this space requirements and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our new idea is let's try to use our personalized page rank sketching and and rounding algorithms.",
                    "label": 0
                },
                {
                    "sent": "In order to be able to compute the same rank, we're trying to reduce SIM rank to personalized page rank.",
                    "label": 0
                },
                {
                    "sent": "Based on.",
                    "label": 0
                },
                {
                    "sent": "The idea is that they are both just acccounting, so let's why not try.",
                    "label": 0
                },
                {
                    "sent": "Account pass from the one and V2.",
                    "label": 0
                },
                {
                    "sent": "That may need.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the difference.",
                    "label": 0
                },
                {
                    "sent": "Is that in this summation, I'm having paths that not just meet at a starting point, but maybe several other times.",
                    "label": 0
                },
                {
                    "sent": "This can be can also define this self similarity of an old V. SIM rank of we will itself by definition should be one, but it could be extended like in this way, saying, well, OK, a page can be more similar to itself if it's pointed to by more similar pages.",
                    "label": 0
                },
                {
                    "sent": "And self similarity of iteration.",
                    "label": 0
                },
                {
                    "sent": "T + 1 means we can have.",
                    "label": 0
                },
                {
                    "sent": "We must have at least three plus while meeting points, so I'm requiring at least T and adding one more.",
                    "label": 0
                },
                {
                    "sent": "And we can obtain some rank by inclusion exclusion by substracting adding different teas.",
                    "label": 0
                },
                {
                    "sent": "This leads us to an algorithm that's got lots of technicalities, in particular to.",
                    "label": 0
                },
                {
                    "sent": "To combine the inclusion, exclusion and and the scalar products with the approximation bounds.",
                    "label": 0
                },
                {
                    "sent": "But actually it turns out this inclusion exclusion converges for small damping factors that's used in practice anyway.",
                    "label": 0
                },
                {
                    "sent": "And it leads us to.",
                    "label": 0
                },
                {
                    "sent": "To space optimal SIM rank algorithms.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's have an example.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is the most complicated concept.",
                    "label": 0
                },
                {
                    "sent": "In the page I'm part of my talk, let's see this graph.",
                    "label": 0
                },
                {
                    "sent": "The simmer, simmer and between V1 and V2 it should be 1 / 12.",
                    "label": 0
                },
                {
                    "sent": "It should be this 1 / 4 * 1 / 3.",
                    "label": 0
                },
                {
                    "sent": "But if I'm just scalar taking the scalar product of personalized stage ranks than my path will go on and will have.",
                    "label": 0
                },
                {
                    "sent": "And a power series or longer bath that should actually stop at the first meeting point.",
                    "label": 0
                },
                {
                    "sent": "Sine overcounting, with this factor 3 / 2 in this particular case.",
                    "label": 0
                },
                {
                    "sent": "Self similarity of each UI you want you to use three.",
                    "label": 0
                },
                {
                    "sent": "Here is 1 / 2 same power series.",
                    "label": 0
                },
                {
                    "sent": "And if I'm computing Requireing 1, two and even more meeting points that I'm getting 1 / 4, one over 8 and so on.",
                    "label": 0
                },
                {
                    "sent": "So the self similarity of you I will be 2 / 3 and that's exactly I'm have to multiplying with to get rid of the.",
                    "label": 0
                },
                {
                    "sent": "Overcounting here sign multiplying with the self similarity to get rid of overcounting.",
                    "label": 0
                },
                {
                    "sent": "K Max stop.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Peak singular value decomposition and the very recent result of domash singular value decomposition is already mentioned in at least two previous talks.",
                    "label": 0
                },
                {
                    "sent": "It's got important applications like spectral clustering, climbers hits algorithm, latent semantic indexing and so on.",
                    "label": 0
                },
                {
                    "sent": "The task is let's find a low rank matrix that minimizes the Frobenius error.",
                    "label": 0
                },
                {
                    "sent": "Some squares error of the input matrix.",
                    "label": 0
                },
                {
                    "sent": "The solution to this minimization problem is the singular value decomposition that's got known algorithms, but for example, they've got running times like this.",
                    "label": 0
                },
                {
                    "sent": "It's very slow for the applications we're shooting for.",
                    "label": 0
                },
                {
                    "sent": "Sampling based results were known and the best sampling based results can go down to very small.",
                    "label": 0
                },
                {
                    "sent": "Fractions of error, but the error is of the four of is relative to the Frobenius norm of the original matrix and not relative to the.",
                    "label": 0
                },
                {
                    "sent": "To the output error, so this the input Frobenius error can be significantly larger than what I'm shooting for.",
                    "label": 0
                },
                {
                    "sent": "So I'm shooting for an accuracy of low rank approximation, get that can be approximated with some amount and I'm not saying I'm approximating with slightly more than that, but I'm saying I might be approximation approximating with something much larger than that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K so recent results are fixing this gap.",
                    "label": 0
                },
                {
                    "sent": "Actually, three independent results appeared as men.",
                    "label": 0
                },
                {
                    "sent": "They are sparse.",
                    "label": 0
                },
                {
                    "sent": "I know manuscripts at the time heart lead.",
                    "label": 0
                },
                {
                    "sent": "So memory Anne Anne Thomas and.",
                    "label": 0
                },
                {
                    "sent": "The idea is to project the input to an R dimensional subspace and then run SVD with much improved running time that depend on the size of this subspace.",
                    "label": 0
                },
                {
                    "sent": "To compare, for example these two results, they have very similar outlook.",
                    "label": 0
                },
                {
                    "sent": "But Arceus for example pass effective?",
                    "label": 0
                },
                {
                    "sent": "It's better suited for streaming data 'cause it needs only reading twice.",
                    "label": 0
                },
                {
                    "sent": "Reading the Matrix compared to K log K. It's even better in the dimension of the projection that we need to take in order to run SVD before.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And we are using random linear combination of rows.",
                    "label": 0
                },
                {
                    "sent": "In comparison with random sampling in this result, our results heavily builds on many previous results and the next two slides I'm trying to show the idea of what's the difference between sampling for SVD computation and random linear combinations.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slow sampling as an idea that was already used.",
                    "label": 0
                },
                {
                    "sent": "In several primary results.",
                    "label": 0
                },
                {
                    "sent": "And the core idea is the core staff.",
                    "label": 1
                },
                {
                    "sent": "In an SVD computation is approximating a matrix product.",
                    "label": 0
                },
                {
                    "sent": "In random sampling and taking this matrix.",
                    "label": 0
                },
                {
                    "sent": "And considering the product as the sum of dyads.",
                    "label": 0
                },
                {
                    "sent": "And I'm sampling dyads.",
                    "label": 1
                },
                {
                    "sent": "It's a product of of an ice column and the ice room.",
                    "label": 0
                },
                {
                    "sent": "For example, the blue column and the blue roll, and I'm sampling a few, hopefully large dietze that reduces the number of terms I have to eventually compute an sampling must depend on the data I have to sample.",
                    "label": 1
                },
                {
                    "sent": "Hopefully large dietze.",
                    "label": 0
                },
                {
                    "sent": "And actually dependence of the sampling on the data is the key problem that sampling based algorithms have to face.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Format thanks and trying to speed up.",
                    "label": 0
                },
                {
                    "sent": "So the core idea of.",
                    "label": 1
                },
                {
                    "sent": "Offer so our idea is instead let's let's compute.",
                    "label": 0
                },
                {
                    "sent": "Each entry of the Matrix as dot as dot products of the ice road and the Jakes Column, and use low dimensional low distortion embeddings.",
                    "label": 1
                },
                {
                    "sent": "In order to compute the DOT products with shorter vectors, so I'm having this long vector.",
                    "label": 1
                },
                {
                    "sent": "I'm making random projection to random combination linear combinations and now I'm just multiplying.",
                    "label": 0
                },
                {
                    "sent": "22 drywall vectors, and these embeddings that we are using is data independent so we don't have to make assumptions of trying to select large dietze and so on.",
                    "label": 0
                },
                {
                    "sent": "It's completely independent of what the input data is.",
                    "label": 0
                },
                {
                    "sent": "K ask.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusion We have seen space optimal summaries for fully personalized page rank and SIM rank for Bella, at least the practically useful ranges of decay factors and the fast relative error SVD algorithm and these algorithm in the hard both had low space approximations of large vectors and the norms were either the maximum.",
                    "label": 1
                },
                {
                    "sent": "Or the two norms.",
                    "label": 0
                },
                {
                    "sent": "The set of pay.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I was talking about is rather spam rank paper, I just quickly mention we had the two sampling based page rank ansim rank results.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The new dog that you will hear next week and the manuscript and also paper that contains techniques for.",
                    "label": 0
                },
                {
                    "sent": "Communication complexity lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}