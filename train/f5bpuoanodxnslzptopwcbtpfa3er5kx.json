{
    "id": "f5bpuoanodxnslzptopwcbtpfa3er5kx",
    "title": "Visualizing and Understanding Convolutional Networks",
    "info": {
        "author": [
            "Matthew Zeiler, Computer Science Department, New York University (NYU)"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2014_zeiler_convolutional_networks/",
    "segmentation": [
        [
            "So this talks about really visualizing and understanding what is being learned, and convolutional networks.",
            "And this is done by myself and Rob Fergus from NYU Yuan was instrumental to the clarifies image net win last year."
        ],
        [
            "So we propose a new method to look at the black box, which is traditionally what people see as a convolutional network, and we can see what's being learned at each layer, and from that we gain a better understanding of what layers are important and how to improve the overall network."
        ],
        [
            "So a lot of people in this room will be familiar with this work by now.",
            "By 2012, image net competition.",
            "This paper on deep convolutional networks really changed the field of computer vision and gave monumental performance gains by using deep networks trained on GPU's with the millions of images in image net, this was done now to Toronto by Alex Kryszewski, alias Let's cover and Jeff Hinton."
        ],
        [
            "But the ideas in that paper, aside from a few, haven't changed since the 80s, an ever since the 80s.",
            "Nobody has really understood what's going on in these networks.",
            "There's been some hypothesis, but it's been treated as a black box, so we wanted to alleviate that and dig into what's being learned in these networks.",
            "So the basics of a convolutional network, our template layer, or a convolution layer that slides templates that are learned throughout training over the pixels, the image.",
            "And I could use some activations.",
            "Those are then passed through a nonlinear layer so that you can learn more complex functions than just linear functions and then a pooling layer is typically used on top of that, which takes local regions and keeps the strongest activation out of those regions.",
            "And that gives you some set of feature Maps.",
            "And this is kind of the repeatable pattern that you do over and over again.",
            "Applying convolutions to these feature Maps now and finally you get a prediction for 1000 classes, for example."
        ],
        [
            "So we're going to talk about what these models learn and then see which parts can be improved upon by looking at the visualizations.",
            "And then we do some other experiments on how well they generalize."
        ],
        [
            "So our probe into these models is a deconvolutional network.",
            "This is some work that Robin I did in the first half of my PhD, and it's really the inverse of a convolutional network.",
            "The idea was to train unsupervised using sparse coding in a convolutional way that you could stack up and learn hierarchical features and.",
            "In this work, we're actually not using the unsupervised learning.",
            "We're not doing sparse coding, we're just using these as a probe into the convolutional network.",
            "So we take a convolutional network that's been trained.",
            "If we pass an image up, get some activations, and then those are treated as inputs to the top of a deconvolutional network, and that goes through a non pooling non linearity and convolution just like the inverse of a convolutional network."
        ],
        [
            "So one thing we have to tackle is doing none pooling operation and if we look at activations in a convolutional network, that's going feedforward up from the image.",
            "They look something like this where there's strong activations denoted by longer bars and small bars that are small activations, and the pooling regions are typically small, like 2 by two or three by three.",
            "And when we do the pooling we traditionally use Max pooling.",
            "Take the strongest activation.",
            "Here it's regardless of sign and we store that can pass it up to the next layer in the convolution net, but something else that we stored in order to do the visualizations is where that Max came from, and that's crucial for being able to take a top down reconstruction and putting it back into its correct location.",
            "This preserves sharp lines in the input and prevents blurriness in the visualizations.",
            "This is very analogous to the gradient of a Max pooling layer.",
            "If you look at backdrop in a convolutional network."
        ],
        [
            "So starting with the first layer, everybody seen these before just oriented edges.",
            "These can be learned in convolutional networks like as shown here, but also deconvolutional networks, autoencoders, sparse coding, a variety of different methods get the same thing when processing the pixels directly, so this is nothing new.",
            "You get different colors in different orientations, different frequencies."
        ],
        [
            "But now we want to look at higher layers.",
            "So what we do is we take an input image with our trained confident.",
            "It's a new image that's never been seen before.",
            "We pass it up to get some activations.",
            "So in this case we're trying to visualize the third layer of the network and the second feature map of that layer so we can look at each feature map independently.",
            "So we take those activations.",
            "We zero out all the other feature Maps 'cause we're focusing only on the second one, and we also zero out all the activations other than the strongest one.",
            "And we can look at the second strongest third strongest and you'll see that in future slides.",
            "And we take those activations as input to the deconvolutional network at the top of the convolutional network, and we do the UN pooling non non linearity and the convolutions with the transpose of the convolutional network filters.",
            "But"
        ],
        [
            "Don't just look at a single image.",
            "We take a whole validation set of images and this gives us more insight into what these features in high levels are invariant too.",
            "So we run the feed forward inference in the confident up to the third layer.",
            "For example, get the activations for all these and then if we're looking at one feature map, we look throughout the validation set of activations for the strongest and reconstruct that, and then the second strongest reconstruct that, so there's only one activation at the top of the convolutional network that does all the visualization."
        ],
        [
            "So we can use this approach to actually find patches as well, so starting with the first layer we look at where the strongest activation for each filter is, and you can see the filters in the bottom right and on the left side you can see image patches in three by three regions, so the strongest activation is the top left and then the 2nd row on the left column is the 2nd strongest activation in each three by three, all the way to the 9th strongest activation and you can see this significant grouping either by color or by orientation.",
            "So we're really seeing that the filters correspond to.",
            "Directly to the types of patches that fire these features."
        ],
        [
            "But we can move to the second layer now and see much more interesting things.",
            "This is now using the Deconvolutional network top down with a single activation for each of the 256 different feature Maps in the second layer.",
            "So this is the strongest activation that fired to each of those 256 features and you can see things like corners and parallel lines, gradings, curves, circles, variety of different mid level primitives and these are things people have always thought should be learned by these networks that build up with pooling.",
            "And convolutions on top of small edges."
        ],
        [
            "We can also look at the expansion of each feature.",
            "This is just a handful of them randomly selected and look at the top 9.",
            "Visualizations that activate that feature most strongly.",
            "And you can see.",
            "Things like these color regions.",
            "You can see more structured regions, but if we actually look at the patches now because we can actually localize in the image where these came."
        ],
        [
            "These things"
        ],
        [
            "Looks like colored blobs are actually sensitive disc."
        ],
        [
            "In tone and."
        ],
        [
            "Oops."
        ],
        [
            "In this example."
        ],
        [
            "In other examples, there are."
        ],
        [
            "More sensitive to circles."
        ],
        [
            "Spirals different other color."
        ],
        [
            "Is on different shapes."
        ],
        [
            "Moving to the third layer."
        ],
        [
            "It's gone through two pooling layers, so it has a much larger receptive field.",
            "Now it's learning parts of objects and you can see much more complex structure than the second layer.",
            "An much more than the 1st."
        ],
        [
            "Now looking at the grouping again, you can still see grouping behavior.",
            "And keep in mind this is very far away from the labels and now it's a couple errors above the pixels, so it's some intermediate layer, but it still learning this grouping behavior and it's finding that that is very important.",
            "And you can see it learning parts of objects like faces down in the bottom left."
        ],
        [
            "And if we look at the patches."
        ],
        [
            "You can see learning other detectors like this one.",
            "Which."
        ],
        [
            "Firing on clouds."
        ],
        [
            "Anne."
        ],
        [
            "I'm not sure."
        ],
        [
            "Haven't looked at every 1000 category, but I doubt Cloud is one of them is just."
        ],
        [
            "Happens to be in so many other categories that it's important to have a feature to recognize that."
        ],
        [
            "Moving up to the fourth layer, there was no pooling between the 3rd and the 4th, so the receptive field didn't change that much, but you're seeing more and more specific object features.",
            "You see a lot of monkey faces, dogfaces human faces, parts of large, significant parts of objects."
        ],
        [
            "And the grouping now groups these things so you can see this feature.",
            "This is 1 out of 384 features, is very sensitive to dogfaces.",
            "And there's other ones more sensitive to monkey faces, and even this is a fourth layer.",
            "Look at this bottom green fee."
        ],
        [
            "Sure, it's sensitive to very small green features within the pixels of the image.",
            "This is something we never really thought would be happening in these networks."
        ],
        [
            "Propagating up very small patches of green all the way up to the fourth layer, and finally the fifth layer.",
            "This the last convolutional layer of the of this model.",
            "It's getting much closer to the labels now, so it's only a couple layers.",
            "Couple of fully connected layers away from the labels, so now it has to."
        ],
        [
            "Making more discriminative groupings and you can see things like keyboards, which is one of the classes, is presented in the middle here.",
            "You can see on the right hand side here."
        ],
        [
            "Basically a Husky detector."
        ],
        [
            "You can see very subtle things in this feature.",
            "Which is."
        ],
        [
            "The background water that's."
        ],
        [
            "Anne."
        ],
        [
            "A variety of different images."
        ],
        [
            "Which is important for determining whether it's a bikini or a seal, for example.",
            "So not."
        ],
        [
            "Direct categories that are being classified, but things that are very important to understand those categories."
        ],
        [
            "So those are the types of things you learn throughout the network, and we wanted to make sure these features are really representative of what strongly activates the features in these high layers.",
            "So to do that we tried an exclusion experiment where we blocked out a region in the image and that block was slid over the image.",
            "It was basically a Block 128, the mean pixel.",
            "And we looked at how the probabilities that the output of the network and high level activations were affected by this."
        ],
        [
            "So for this example image of a Pomeranian, that's the true label.",
            "We looked at the probability of Pomeranian on the bottom left.",
            "Each pixel in this diagram is a location of the block in the pixels of the image.",
            "So you can see when the block overlaps with the face of the Pomeranian, the probability of Pomeranian significantly drops almost to zero and on the right hand side we're showing.",
            "What is the most probable class at that time?",
            "And you can see that it switches from Pomeranian to tennis ball which is the next best guess at that point."
        ],
        [
            "And then we looked at these visualizations, which is shown on the right.",
            "This is one of the strongest visualizations of this Pomeranian.",
            "And you can see it fires.",
            "These are the second strongest on different animals, or different Pomeranians maybe.",
            "But we can look at the activation of that feature map in the fifth layer while we're sliding this block, which is shown on the left and again when we block out the face, it drops the activation which was used to generate this visualization.",
            "So it's really showing that the visualization is tuned very carefully to the part of the image that looks like a Pomeranian."
        ],
        [
            "And we could try this for other classes.",
            "So say car wheel like this if we block it out on the bottom left, you'll see that whole region, the probability of car wheel drops and on the right it really starts guessing whether it's a racer or cab."
        ],
        [
            "But we can look at features that are not related to the true label.",
            "So this feature fires on a variety of different text.",
            "And when we block out any text in the image, you can see three different places where there's text.",
            "It significantly drops that feature, so even though.",
            "I'm not sure if there's actually text in the data set, even as one of the categories, but it's found that text is important for a variety of different classes, and it's learned that at the fifth layer."
        ],
        [
            "And finally, this is an interesting example where the true label here could be many different things, but it's labeled as Afghan hound, and when you block out the dog, the probability drops as we see in the other examples.",
            "But if you block out the faces of the other people in the picture, then the probability of the hound actually increases significantly, so that's indicating that there's some competition here in the network, and we can kind of leverage that to improve classifications, possibly."
        ],
        [
            "And finally, another feature that's not related to the true class.",
            "It's firing on all different types of faces."
        ],
        [
            "So how do we use?"
        ],
        [
            "To improve performance, so we looked at Alice Kryszewski's model and it learned filters like the far left where one was very significantly unnormalized.",
            "And we also saw that there was some dead pixels and some pics are some filters in the first layer that were very specific 'cause they were 11 by 11 filters.",
            "And in the second layer we saw a lot of blocking artifacts due to the large pooling strides, so we use these visualization and there's actually some very simple features.",
            "We use these to really learn how to improve the model, so we constrain the RMS during training so we didn't have these blowup of filters.",
            "We made the filters smaller so they learn more interesting things and there is no longer dead filters.",
            "And we made the pooling stride smaller, and this reduced the blocking artifacts.",
            "And this gave a significant boost in performance."
        ],
        [
            "And this was crucial to the ZF entry, which plays third last year and also to the clarify entry, which was the winner last year."
        ],
        [
            "And some other recent work has also used the same idea of making smaller strides, such as this year's localization winner in second place classification.",
            "Winner Andrew Howard has top three entries in both image Nets, and they've even used these visualization techniques for saliency detection.",
            "And the final result."
        ],
        [
            "We show in this paper."
        ],
        [
            "Was taking these trained Imagenet features and applying to a new data set and we looked at how."
        ],
        [
            "How many images we needed to match state of the art.",
            "We found that with only six images, we could match state of the art performance.",
            "So in."
        ],
        [
            "Marie, we presented a visualization technique to understand these networks and better improve them, and we even talked to neuro scientists who are really excited about this, 'cause they might give new insights into how the brain works.",
            "Thanks."
        ],
        [
            "I'm wondering what these visualizations show is going on in the examples from the paper intriguing properties of neural networks, where you can make small modifications to an image of a bus, for example, and turn it quickly into an ostrich, and it seems to be just adding some artifactual noise.",
            "This is a paper also by Rob Fergus.",
            "Do your visualizations show why these tiny modifications are able to switch the category so dramatically?",
            "I think in general these networks are really sensitive to exact pixels.",
            "For example, as we're sliding those blocks over the image, it was very sensitive to exactly aligning with the Pomeranian face and only a small shift over the probability was still high.",
            "So I think just in general, and we've seen this with different types of transformations as well, even JPEG encoding so they're very sensitive.",
            "Thank you.",
            "So when you were doing this window, did you do the blurring or blackout or what do you think is better?",
            "So all we tried was taking that square block and setting it to 128.",
            "Before we subtracted the mean of the image.",
            "So essentially 0 after we subtracted the mean.",
            "We didn't do any type of blurring around it or anything like that.",
            "OK, if there's no more questions for now, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this talks about really visualizing and understanding what is being learned, and convolutional networks.",
                    "label": 0
                },
                {
                    "sent": "And this is done by myself and Rob Fergus from NYU Yuan was instrumental to the clarifies image net win last year.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we propose a new method to look at the black box, which is traditionally what people see as a convolutional network, and we can see what's being learned at each layer, and from that we gain a better understanding of what layers are important and how to improve the overall network.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a lot of people in this room will be familiar with this work by now.",
                    "label": 0
                },
                {
                    "sent": "By 2012, image net competition.",
                    "label": 0
                },
                {
                    "sent": "This paper on deep convolutional networks really changed the field of computer vision and gave monumental performance gains by using deep networks trained on GPU's with the millions of images in image net, this was done now to Toronto by Alex Kryszewski, alias Let's cover and Jeff Hinton.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the ideas in that paper, aside from a few, haven't changed since the 80s, an ever since the 80s.",
                    "label": 0
                },
                {
                    "sent": "Nobody has really understood what's going on in these networks.",
                    "label": 0
                },
                {
                    "sent": "There's been some hypothesis, but it's been treated as a black box, so we wanted to alleviate that and dig into what's being learned in these networks.",
                    "label": 0
                },
                {
                    "sent": "So the basics of a convolutional network, our template layer, or a convolution layer that slides templates that are learned throughout training over the pixels, the image.",
                    "label": 1
                },
                {
                    "sent": "And I could use some activations.",
                    "label": 0
                },
                {
                    "sent": "Those are then passed through a nonlinear layer so that you can learn more complex functions than just linear functions and then a pooling layer is typically used on top of that, which takes local regions and keeps the strongest activation out of those regions.",
                    "label": 0
                },
                {
                    "sent": "And that gives you some set of feature Maps.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of the repeatable pattern that you do over and over again.",
                    "label": 1
                },
                {
                    "sent": "Applying convolutions to these feature Maps now and finally you get a prediction for 1000 classes, for example.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to talk about what these models learn and then see which parts can be improved upon by looking at the visualizations.",
                    "label": 0
                },
                {
                    "sent": "And then we do some other experiments on how well they generalize.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our probe into these models is a deconvolutional network.",
                    "label": 0
                },
                {
                    "sent": "This is some work that Robin I did in the first half of my PhD, and it's really the inverse of a convolutional network.",
                    "label": 0
                },
                {
                    "sent": "The idea was to train unsupervised using sparse coding in a convolutional way that you could stack up and learn hierarchical features and.",
                    "label": 0
                },
                {
                    "sent": "In this work, we're actually not using the unsupervised learning.",
                    "label": 1
                },
                {
                    "sent": "We're not doing sparse coding, we're just using these as a probe into the convolutional network.",
                    "label": 0
                },
                {
                    "sent": "So we take a convolutional network that's been trained.",
                    "label": 0
                },
                {
                    "sent": "If we pass an image up, get some activations, and then those are treated as inputs to the top of a deconvolutional network, and that goes through a non pooling non linearity and convolution just like the inverse of a convolutional network.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing we have to tackle is doing none pooling operation and if we look at activations in a convolutional network, that's going feedforward up from the image.",
                    "label": 0
                },
                {
                    "sent": "They look something like this where there's strong activations denoted by longer bars and small bars that are small activations, and the pooling regions are typically small, like 2 by two or three by three.",
                    "label": 0
                },
                {
                    "sent": "And when we do the pooling we traditionally use Max pooling.",
                    "label": 1
                },
                {
                    "sent": "Take the strongest activation.",
                    "label": 0
                },
                {
                    "sent": "Here it's regardless of sign and we store that can pass it up to the next layer in the convolution net, but something else that we stored in order to do the visualizations is where that Max came from, and that's crucial for being able to take a top down reconstruction and putting it back into its correct location.",
                    "label": 0
                },
                {
                    "sent": "This preserves sharp lines in the input and prevents blurriness in the visualizations.",
                    "label": 0
                },
                {
                    "sent": "This is very analogous to the gradient of a Max pooling layer.",
                    "label": 0
                },
                {
                    "sent": "If you look at backdrop in a convolutional network.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So starting with the first layer, everybody seen these before just oriented edges.",
                    "label": 0
                },
                {
                    "sent": "These can be learned in convolutional networks like as shown here, but also deconvolutional networks, autoencoders, sparse coding, a variety of different methods get the same thing when processing the pixels directly, so this is nothing new.",
                    "label": 0
                },
                {
                    "sent": "You get different colors in different orientations, different frequencies.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But now we want to look at higher layers.",
                    "label": 1
                },
                {
                    "sent": "So what we do is we take an input image with our trained confident.",
                    "label": 0
                },
                {
                    "sent": "It's a new image that's never been seen before.",
                    "label": 0
                },
                {
                    "sent": "We pass it up to get some activations.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're trying to visualize the third layer of the network and the second feature map of that layer so we can look at each feature map independently.",
                    "label": 0
                },
                {
                    "sent": "So we take those activations.",
                    "label": 0
                },
                {
                    "sent": "We zero out all the other feature Maps 'cause we're focusing only on the second one, and we also zero out all the activations other than the strongest one.",
                    "label": 0
                },
                {
                    "sent": "And we can look at the second strongest third strongest and you'll see that in future slides.",
                    "label": 0
                },
                {
                    "sent": "And we take those activations as input to the deconvolutional network at the top of the convolutional network, and we do the UN pooling non non linearity and the convolutions with the transpose of the convolutional network filters.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't just look at a single image.",
                    "label": 0
                },
                {
                    "sent": "We take a whole validation set of images and this gives us more insight into what these features in high levels are invariant too.",
                    "label": 0
                },
                {
                    "sent": "So we run the feed forward inference in the confident up to the third layer.",
                    "label": 0
                },
                {
                    "sent": "For example, get the activations for all these and then if we're looking at one feature map, we look throughout the validation set of activations for the strongest and reconstruct that, and then the second strongest reconstruct that, so there's only one activation at the top of the convolutional network that does all the visualization.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can use this approach to actually find patches as well, so starting with the first layer we look at where the strongest activation for each filter is, and you can see the filters in the bottom right and on the left side you can see image patches in three by three regions, so the strongest activation is the top left and then the 2nd row on the left column is the 2nd strongest activation in each three by three, all the way to the 9th strongest activation and you can see this significant grouping either by color or by orientation.",
                    "label": 0
                },
                {
                    "sent": "So we're really seeing that the filters correspond to.",
                    "label": 0
                },
                {
                    "sent": "Directly to the types of patches that fire these features.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can move to the second layer now and see much more interesting things.",
                    "label": 0
                },
                {
                    "sent": "This is now using the Deconvolutional network top down with a single activation for each of the 256 different feature Maps in the second layer.",
                    "label": 0
                },
                {
                    "sent": "So this is the strongest activation that fired to each of those 256 features and you can see things like corners and parallel lines, gradings, curves, circles, variety of different mid level primitives and these are things people have always thought should be learned by these networks that build up with pooling.",
                    "label": 0
                },
                {
                    "sent": "And convolutions on top of small edges.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also look at the expansion of each feature.",
                    "label": 0
                },
                {
                    "sent": "This is just a handful of them randomly selected and look at the top 9.",
                    "label": 0
                },
                {
                    "sent": "Visualizations that activate that feature most strongly.",
                    "label": 0
                },
                {
                    "sent": "And you can see.",
                    "label": 0
                },
                {
                    "sent": "Things like these color regions.",
                    "label": 0
                },
                {
                    "sent": "You can see more structured regions, but if we actually look at the patches now because we can actually localize in the image where these came.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These things",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looks like colored blobs are actually sensitive disc.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In tone and.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oops.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this example.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In other examples, there are.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More sensitive to circles.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spirals different other color.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is on different shapes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moving to the third layer.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's gone through two pooling layers, so it has a much larger receptive field.",
                    "label": 0
                },
                {
                    "sent": "Now it's learning parts of objects and you can see much more complex structure than the second layer.",
                    "label": 0
                },
                {
                    "sent": "An much more than the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now looking at the grouping again, you can still see grouping behavior.",
                    "label": 0
                },
                {
                    "sent": "And keep in mind this is very far away from the labels and now it's a couple errors above the pixels, so it's some intermediate layer, but it still learning this grouping behavior and it's finding that that is very important.",
                    "label": 0
                },
                {
                    "sent": "And you can see it learning parts of objects like faces down in the bottom left.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we look at the patches.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see learning other detectors like this one.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Firing on clouds.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not sure.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Haven't looked at every 1000 category, but I doubt Cloud is one of them is just.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Happens to be in so many other categories that it's important to have a feature to recognize that.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moving up to the fourth layer, there was no pooling between the 3rd and the 4th, so the receptive field didn't change that much, but you're seeing more and more specific object features.",
                    "label": 0
                },
                {
                    "sent": "You see a lot of monkey faces, dogfaces human faces, parts of large, significant parts of objects.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the grouping now groups these things so you can see this feature.",
                    "label": 0
                },
                {
                    "sent": "This is 1 out of 384 features, is very sensitive to dogfaces.",
                    "label": 0
                },
                {
                    "sent": "And there's other ones more sensitive to monkey faces, and even this is a fourth layer.",
                    "label": 0
                },
                {
                    "sent": "Look at this bottom green fee.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, it's sensitive to very small green features within the pixels of the image.",
                    "label": 0
                },
                {
                    "sent": "This is something we never really thought would be happening in these networks.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Propagating up very small patches of green all the way up to the fourth layer, and finally the fifth layer.",
                    "label": 0
                },
                {
                    "sent": "This the last convolutional layer of the of this model.",
                    "label": 0
                },
                {
                    "sent": "It's getting much closer to the labels now, so it's only a couple layers.",
                    "label": 0
                },
                {
                    "sent": "Couple of fully connected layers away from the labels, so now it has to.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Making more discriminative groupings and you can see things like keyboards, which is one of the classes, is presented in the middle here.",
                    "label": 0
                },
                {
                    "sent": "You can see on the right hand side here.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically a Husky detector.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see very subtle things in this feature.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The background water that's.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A variety of different images.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is important for determining whether it's a bikini or a seal, for example.",
                    "label": 0
                },
                {
                    "sent": "So not.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Direct categories that are being classified, but things that are very important to understand those categories.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So those are the types of things you learn throughout the network, and we wanted to make sure these features are really representative of what strongly activates the features in these high layers.",
                    "label": 0
                },
                {
                    "sent": "So to do that we tried an exclusion experiment where we blocked out a region in the image and that block was slid over the image.",
                    "label": 0
                },
                {
                    "sent": "It was basically a Block 128, the mean pixel.",
                    "label": 0
                },
                {
                    "sent": "And we looked at how the probabilities that the output of the network and high level activations were affected by this.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for this example image of a Pomeranian, that's the true label.",
                    "label": 0
                },
                {
                    "sent": "We looked at the probability of Pomeranian on the bottom left.",
                    "label": 0
                },
                {
                    "sent": "Each pixel in this diagram is a location of the block in the pixels of the image.",
                    "label": 0
                },
                {
                    "sent": "So you can see when the block overlaps with the face of the Pomeranian, the probability of Pomeranian significantly drops almost to zero and on the right hand side we're showing.",
                    "label": 0
                },
                {
                    "sent": "What is the most probable class at that time?",
                    "label": 0
                },
                {
                    "sent": "And you can see that it switches from Pomeranian to tennis ball which is the next best guess at that point.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we looked at these visualizations, which is shown on the right.",
                    "label": 0
                },
                {
                    "sent": "This is one of the strongest visualizations of this Pomeranian.",
                    "label": 0
                },
                {
                    "sent": "And you can see it fires.",
                    "label": 0
                },
                {
                    "sent": "These are the second strongest on different animals, or different Pomeranians maybe.",
                    "label": 0
                },
                {
                    "sent": "But we can look at the activation of that feature map in the fifth layer while we're sliding this block, which is shown on the left and again when we block out the face, it drops the activation which was used to generate this visualization.",
                    "label": 0
                },
                {
                    "sent": "So it's really showing that the visualization is tuned very carefully to the part of the image that looks like a Pomeranian.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we could try this for other classes.",
                    "label": 0
                },
                {
                    "sent": "So say car wheel like this if we block it out on the bottom left, you'll see that whole region, the probability of car wheel drops and on the right it really starts guessing whether it's a racer or cab.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can look at features that are not related to the true label.",
                    "label": 0
                },
                {
                    "sent": "So this feature fires on a variety of different text.",
                    "label": 0
                },
                {
                    "sent": "And when we block out any text in the image, you can see three different places where there's text.",
                    "label": 0
                },
                {
                    "sent": "It significantly drops that feature, so even though.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if there's actually text in the data set, even as one of the categories, but it's found that text is important for a variety of different classes, and it's learned that at the fifth layer.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, this is an interesting example where the true label here could be many different things, but it's labeled as Afghan hound, and when you block out the dog, the probability drops as we see in the other examples.",
                    "label": 0
                },
                {
                    "sent": "But if you block out the faces of the other people in the picture, then the probability of the hound actually increases significantly, so that's indicating that there's some competition here in the network, and we can kind of leverage that to improve classifications, possibly.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, another feature that's not related to the true class.",
                    "label": 0
                },
                {
                    "sent": "It's firing on all different types of faces.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we use?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To improve performance, so we looked at Alice Kryszewski's model and it learned filters like the far left where one was very significantly unnormalized.",
                    "label": 0
                },
                {
                    "sent": "And we also saw that there was some dead pixels and some pics are some filters in the first layer that were very specific 'cause they were 11 by 11 filters.",
                    "label": 0
                },
                {
                    "sent": "And in the second layer we saw a lot of blocking artifacts due to the large pooling strides, so we use these visualization and there's actually some very simple features.",
                    "label": 0
                },
                {
                    "sent": "We use these to really learn how to improve the model, so we constrain the RMS during training so we didn't have these blowup of filters.",
                    "label": 0
                },
                {
                    "sent": "We made the filters smaller so they learn more interesting things and there is no longer dead filters.",
                    "label": 0
                },
                {
                    "sent": "And we made the pooling stride smaller, and this reduced the blocking artifacts.",
                    "label": 0
                },
                {
                    "sent": "And this gave a significant boost in performance.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this was crucial to the ZF entry, which plays third last year and also to the clarify entry, which was the winner last year.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And some other recent work has also used the same idea of making smaller strides, such as this year's localization winner in second place classification.",
                    "label": 0
                },
                {
                    "sent": "Winner Andrew Howard has top three entries in both image Nets, and they've even used these visualization techniques for saliency detection.",
                    "label": 0
                },
                {
                    "sent": "And the final result.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We show in this paper.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was taking these trained Imagenet features and applying to a new data set and we looked at how.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How many images we needed to match state of the art.",
                    "label": 1
                },
                {
                    "sent": "We found that with only six images, we could match state of the art performance.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Marie, we presented a visualization technique to understand these networks and better improve them, and we even talked to neuro scientists who are really excited about this, 'cause they might give new insights into how the brain works.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm wondering what these visualizations show is going on in the examples from the paper intriguing properties of neural networks, where you can make small modifications to an image of a bus, for example, and turn it quickly into an ostrich, and it seems to be just adding some artifactual noise.",
                    "label": 0
                },
                {
                    "sent": "This is a paper also by Rob Fergus.",
                    "label": 0
                },
                {
                    "sent": "Do your visualizations show why these tiny modifications are able to switch the category so dramatically?",
                    "label": 0
                },
                {
                    "sent": "I think in general these networks are really sensitive to exact pixels.",
                    "label": 0
                },
                {
                    "sent": "For example, as we're sliding those blocks over the image, it was very sensitive to exactly aligning with the Pomeranian face and only a small shift over the probability was still high.",
                    "label": 0
                },
                {
                    "sent": "So I think just in general, and we've seen this with different types of transformations as well, even JPEG encoding so they're very sensitive.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So when you were doing this window, did you do the blurring or blackout or what do you think is better?",
                    "label": 0
                },
                {
                    "sent": "So all we tried was taking that square block and setting it to 128.",
                    "label": 0
                },
                {
                    "sent": "Before we subtracted the mean of the image.",
                    "label": 0
                },
                {
                    "sent": "So essentially 0 after we subtracted the mean.",
                    "label": 0
                },
                {
                    "sent": "We didn't do any type of blurring around it or anything like that.",
                    "label": 0
                },
                {
                    "sent": "OK, if there's no more questions for now, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}