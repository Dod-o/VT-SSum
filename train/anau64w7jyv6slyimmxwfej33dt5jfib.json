{
    "id": "anau64w7jyv6slyimmxwfej33dt5jfib",
    "title": "Convex Variational Bayesian Inference for Large Scale Generalized Linear Models",
    "info": {
        "author": [
            "Hannes Nickisch, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_nickisch_cvb/",
    "segmentation": [
        [
            "Yeah, good morning and welcome to talk.",
            "This is joint work with the Mattia Ziga sitting in the audience there and it's concerned with the generalized linear models.",
            "And there are two contributions we have.",
            "First of all, we discuss convexity properties of a variational inference problem and Secondly we propose an efficient algorithm to solve the inference problem.",
            "So the motivation for us to."
        ],
        [
            "So for that work is is, the following is twofold there with two examples.",
            "Consider you want to measure want to measure an image like this either by a digital camera or by an MRI scanner.",
            "So device measuring linear projections of the image, and you have.",
            "In that scenario you know that Pixel image have highly highly non Gaussian statistics.",
            "So it's a high dimensional connected Markov random field, it's.",
            "There's a lot of correlation in there, and the question we ask is."
        ],
        [
            "What aspects of the image should you measure such that the reconstruction is as faithful as possible so?"
        ],
        [
            "Secondly, which falls also second example which was also under the umbrella of their general in their model is active learning for binary classification.",
            "Suppose you are you're given feature vectors and some feature space and you have to have to compute the classification boundary and you want to do it probabilistically.",
            "Then active learning consists of.",
            "Choosing amongst the examples which one you query the labels for, so we have a fixed budget of a."
        ],
        [
            "Of numbers of labels.",
            "So also we look at which wich labels would you suggest and also if the number of feature features is a very high and the dimensionality of the feature space is very high, then this is a.",
            "This is an interesting problem.",
            "So just to summarize, it's a we want to make inference and non Gaussian models and we are concerned with experimental design and those models so."
        ],
        [
            "How do we formalize this issue?",
            "We have things are shown in that in that vector graph.",
            "Here we have unknown variables, unknown latent variables U and we have green Gaussian potentials and we have magenta and non Gaussian potentials.",
            "So the the general linear model.",
            "And the corresponding posterior of that model takes the following form.",
            "You have one multivariate Gaussian and you have a product of individual non Gaussian terms.",
            "And the experimental design question I mentioned in the motor."
        ],
        [
            "Patience slide boils down to deciding which which linear projection X or B shall I measure such that I acquire most information about the unknown variables and to do so we need information about the covariance of the posterior.",
            "So to instantiate the two examples I talked about."
        ],
        [
            "In the MRI imaging example, the unknown variables U would constitute the pixels of the image.",
            "The measurements acquired by the scanner would be like Fourier coefficients of that image.",
            "The measurement design is encoded in the Matrix X.",
            "This would be."
        ],
        [
            "So Gaussian Gaussian potentials on those and to include prior knowledge about the image, we could place sparsity potentials on multiscale gradients of the image.",
            "The second example.",
            "Active learning for binary classification.",
            "In that case, the unknown variables would would be the weights of the linear classifier and we could play.",
            "So we've placed the newly potentials on linear projections of these weights.",
            "N."
        ],
        [
            "And either we can impose a Gaussian or sparsity prior on these classifier weights.",
            "So inference would then consist of computing the posterior, and this would allow us to determine which which measurement to make in the next next step."
        ],
        [
            "So the strategy to compute this posterior to get to get a handle on that posterior is replacing non Gaussian sites by by Gaussian sites and this is this is done by means of legendra Fenchel lower side bounding so every every non Gaussian site.",
            "For example we could use plus Bernoulli or students T sites.",
            "All those are super Gaussians means that it can be lower bounded by Gaussians of.",
            "Of different widths gamma.",
            "And our strategy to get Attractable is to replace the non Gaussian sites by like by Gaussian sites.",
            "So this lower bounding works as follow.",
            "You choose a choose a width gamma and then then you somehow scale it up until it touches the non non Gaussian part.",
            "So we get a lower bound on the on the non Gaussian potential.",
            "And the H function which will occur later also in the talk, some of the scaling the height of the lower bound.",
            "So we've got.",
            "We now have individual bounds on each of these."
        ],
        [
            "Non Gaussian sites.",
            "And if you if you combine those then you get.",
            "I mean, if you look at the the partition function then you can replace the non Gaussian parts by these Gaussian lower bounds and then you get a lower bound joint lower bound on the whole partition function.",
            "Enter.",
            "And since this is a Gaussian term and this is also a Gaussian term, this whole object is just a Gaussian integral.",
            "And if you do it and if you look at the function inside the exponential, our variational criterion reduces to minimizing that objective.",
            "So fire is the variational criterion we wish to minimize, such as to get as close as possible to the lock partition function.",
            "So and just to briefly mention what these expressions are, this lock, determinant, coupling term and a is the inverse covariance matrix of our posterior.",
            "The sum of these individual scaling terms come from come from the from the lower bound scaling of the non Gaussian sites and this term here is written as a minimization, but it's, but it's only a convenient way of writing the multiplication with the with the inverse of a so.",
            "To to make things more more visible.",
            "So now we have now bfor inference problem.",
            "We want to minimize this criterion and now we come to the contribution of that work.",
            "It's we show in the in the paper we showed that this variational criterion is convex whenever the corresponding map inference problem is convex.",
            "And Secondly, we propose an algorithm to minimize that criterion, so to convex."
        ],
        [
            "We look at these at these three different terms.",
            "The first term."
        ],
        [
            "The art term.",
            "If you look at it, it's a.",
            "It's a quadratic and you and it's a quadratic over linear in gamma, so this this matrix here has the vector gamma containing the individual width of the sites as diagonal entries, so its accrediting over linear, which means it's jointly convex and this this convexity is preserved and minimizations.",
            "So this term is convex.",
            "Secondly, we prove in the paper that the lock determined."
        ],
        [
            "Term is also convex in gamma.",
            "And and finally, these these some of these scaling functions."
        ],
        [
            "The only term depending on on the sites on the on, the specific forms of the sites and we show in the paper that whenever the sites are log concave, these scaling functions are convex.",
            "And to put it together, this whole means you have a sum of three convex parts.",
            "Then this whole optimization variational optimization problem is convex.",
            "Whenever the map problem is convex."
        ],
        [
            "OK, now we are left with the criterion and and we want to.",
            "We want to find the optimal optimum of that criteria.",
            "I think it's a.",
            "It's a convex function, so you might.",
            "You might be tempted to say that that it's already solved, but it's, but it's not.",
            "It's not that easy.",
            "We have like the green part which is, which decouples in gamma, so this is a function.",
            "This is a sum over individual gamma component and here you can rewrite it as a sum of squares over over over over gammas.",
            "So this is decoupling part and this in red.",
            "Here is a very highly coupled part so it's occurs."
        ],
        [
            "On the diagonal and.",
            "And if you want to compute the gradient of the whole of the whole objective, then that's the part which causes trouble.",
            "So if you computing the gradient means computing the diagonal of a very very huge matrix, and this is, this means cubic scaling.",
            "Remember, for example in the image Ng image example, we have say 100 thousands of."
        ],
        [
            "Or even more pixels.",
            "And then then this is a very very costly operation and even even even doing so approximately is very time consuming.",
            "So it's basically it's computing Gaussian variances.",
            "So previous coordination methods do the following.",
            "They fix all all gammas except for except for one, and optimizing for one single gamma means is as expensive as solving a linear system of size, number of variables.",
            "So we would have to solve linear systems of that size, and this is also.",
            "Very expensive.",
            "And our strategy to to minimize that function is is the following.",
            "We identify.",
            "We identify the coupling part of this log determinant part and D couple it by upper bounding it.",
            "So the idea is the following, not only that, log determinant of a is convex in gamma, it is also concave in gamma inverse.",
            "And this and this is allow allows us by means of.",
            "Other another legend relative to Upper bound.",
            "This concave part by a linear function, linear in gamma inverse.",
            "So and once we upper bound it this linear function decouples and then the optimization becomes simpler.",
            "So the resulting algorithm is a double loop algorithm."
        ],
        [
            "It takes has the following form.",
            "So the this objective is split into sketched here, then concave and concave and a convex part and the outer loop step consists of upper bounding the concave part by linear function by means of legend, reality and the inner loop step is then.",
            "Consists of optimizing the decoupled objective, so you would you would upper bound here the concave part by the by the linear function and then you would get this modified criterion and this modified criterion is minimize much simpler than the function as a whole.",
            "I mean, there are a lot more details involved and they are also described on the poster, but this is to provide you with a rough idea what this decoupling looks like.",
            "Um, yeah, summarize the."
        ],
        [
            "This double loop algorithm if you if you look at the usual gradient based approach, you would have to compute a lot of a lot of gradients and then you would.",
            "Then you would compute a decent direction, whereas in this double loop algorithm we only have a kind of fixed number of iterations and we make and we compute the gradient also approximately and this and the inner loop optimization we do.",
            "It's a, it's a simple optimization, so it's an iterative Leafly reweighted least squares problem, and people can do it on very large scales.",
            "So the idea is to make efficient use of these expensive computations.",
            "So now.",
            "We we were able to buy these ideas, we are able to optimize for the criterion and this and this leads leads posterior approximation and but our initial goal was."
        ],
        [
            "Experimental design based on that approximate posterior and this is done by means of my means of the information gain is gone.",
            "So we look at the expected relative entropy between the approximate posterior and the approximate posterior with the with another tentative candidate included.",
            "For example, in the."
        ],
        [
            "Magic resonance imaging example.",
            "This is expected to enter relative entropy is A is an integral over over Gaussian distribution in certain circles in expectation."
        ],
        [
            "And and the classification example of this is a sum over the possible labels of the of the feature vector.",
            "So and this this information gain score now allows us to compare the."
        ],
        [
            "The expected amount of information gain of the candidate and we can pick the most or the expected most informative candidate.",
            "So and finally the design loop looks as follows.",
            "You compute or update your approximation to the posterior, then you then you score your candidates and you pick the most informative candidate.",
            "And finally in the third step you do you do the measurement for this candidate and then you, and then you iterate over all these steps.",
            "So then we.",
            "We applied the this variational methodology to to.",
            "Reclassification active learning so we have."
        ],
        [
            "We picked datasets where the number of variables is in the 10s of thousands and also the number of features is very high.",
            "And what we see is.",
            "By means of that by that.",
            "Approach we get like moderate gains over the over and picking picking these.",
            "These examples at random and using our optimized scores."
        ],
        [
            "So this brings me to the conclusions.",
            "The main contribution of the paper is.",
            "To show that our variational relaxation is a well known variation of relaxation is convex whenever the corresponding map estimation problem is convex, then we propose a scalable algorithm to solve for that for the optimization problem and an experiments.",
            "We show that this this methodology allows to drive really large scale models.",
            "So because the large scale.",
            "Where it is quite bold but I want to remember that I want to remind you that the computational primitives we use in that in that approach is they are well understood and they really scale too large to large dimensions, so we use conjugate gradients for the inner loop optimization and we lose use the land search algorithm to estimate the variances in the outer loop.",
            "And they all they are only limited by the speed of matrix vector multiplications with these system matrices.",
            "X&B that means if you you don't have to store X or B, you only have to be able to multiply with them efficiently.",
            "And then this whole approach is is scalable?",
            "Thanks for attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, good morning and welcome to talk.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with the Mattia Ziga sitting in the audience there and it's concerned with the generalized linear models.",
                    "label": 1
                },
                {
                    "sent": "And there are two contributions we have.",
                    "label": 0
                },
                {
                    "sent": "First of all, we discuss convexity properties of a variational inference problem and Secondly we propose an efficient algorithm to solve the inference problem.",
                    "label": 0
                },
                {
                    "sent": "So the motivation for us to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for that work is is, the following is twofold there with two examples.",
                    "label": 0
                },
                {
                    "sent": "Consider you want to measure want to measure an image like this either by a digital camera or by an MRI scanner.",
                    "label": 0
                },
                {
                    "sent": "So device measuring linear projections of the image, and you have.",
                    "label": 0
                },
                {
                    "sent": "In that scenario you know that Pixel image have highly highly non Gaussian statistics.",
                    "label": 0
                },
                {
                    "sent": "So it's a high dimensional connected Markov random field, it's.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of correlation in there, and the question we ask is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What aspects of the image should you measure such that the reconstruction is as faithful as possible so?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Secondly, which falls also second example which was also under the umbrella of their general in their model is active learning for binary classification.",
                    "label": 0
                },
                {
                    "sent": "Suppose you are you're given feature vectors and some feature space and you have to have to compute the classification boundary and you want to do it probabilistically.",
                    "label": 0
                },
                {
                    "sent": "Then active learning consists of.",
                    "label": 0
                },
                {
                    "sent": "Choosing amongst the examples which one you query the labels for, so we have a fixed budget of a.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of numbers of labels.",
                    "label": 0
                },
                {
                    "sent": "So also we look at which wich labels would you suggest and also if the number of feature features is a very high and the dimensionality of the feature space is very high, then this is a.",
                    "label": 0
                },
                {
                    "sent": "This is an interesting problem.",
                    "label": 0
                },
                {
                    "sent": "So just to summarize, it's a we want to make inference and non Gaussian models and we are concerned with experimental design and those models so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we formalize this issue?",
                    "label": 0
                },
                {
                    "sent": "We have things are shown in that in that vector graph.",
                    "label": 0
                },
                {
                    "sent": "Here we have unknown variables, unknown latent variables U and we have green Gaussian potentials and we have magenta and non Gaussian potentials.",
                    "label": 0
                },
                {
                    "sent": "So the the general linear model.",
                    "label": 1
                },
                {
                    "sent": "And the corresponding posterior of that model takes the following form.",
                    "label": 0
                },
                {
                    "sent": "You have one multivariate Gaussian and you have a product of individual non Gaussian terms.",
                    "label": 1
                },
                {
                    "sent": "And the experimental design question I mentioned in the motor.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patience slide boils down to deciding which which linear projection X or B shall I measure such that I acquire most information about the unknown variables and to do so we need information about the covariance of the posterior.",
                    "label": 0
                },
                {
                    "sent": "So to instantiate the two examples I talked about.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the MRI imaging example, the unknown variables U would constitute the pixels of the image.",
                    "label": 0
                },
                {
                    "sent": "The measurements acquired by the scanner would be like Fourier coefficients of that image.",
                    "label": 0
                },
                {
                    "sent": "The measurement design is encoded in the Matrix X.",
                    "label": 0
                },
                {
                    "sent": "This would be.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Gaussian Gaussian potentials on those and to include prior knowledge about the image, we could place sparsity potentials on multiscale gradients of the image.",
                    "label": 0
                },
                {
                    "sent": "The second example.",
                    "label": 0
                },
                {
                    "sent": "Active learning for binary classification.",
                    "label": 0
                },
                {
                    "sent": "In that case, the unknown variables would would be the weights of the linear classifier and we could play.",
                    "label": 0
                },
                {
                    "sent": "So we've placed the newly potentials on linear projections of these weights.",
                    "label": 0
                },
                {
                    "sent": "N.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And either we can impose a Gaussian or sparsity prior on these classifier weights.",
                    "label": 0
                },
                {
                    "sent": "So inference would then consist of computing the posterior, and this would allow us to determine which which measurement to make in the next next step.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the strategy to compute this posterior to get to get a handle on that posterior is replacing non Gaussian sites by by Gaussian sites and this is this is done by means of legendra Fenchel lower side bounding so every every non Gaussian site.",
                    "label": 0
                },
                {
                    "sent": "For example we could use plus Bernoulli or students T sites.",
                    "label": 0
                },
                {
                    "sent": "All those are super Gaussians means that it can be lower bounded by Gaussians of.",
                    "label": 0
                },
                {
                    "sent": "Of different widths gamma.",
                    "label": 0
                },
                {
                    "sent": "And our strategy to get Attractable is to replace the non Gaussian sites by like by Gaussian sites.",
                    "label": 0
                },
                {
                    "sent": "So this lower bounding works as follow.",
                    "label": 0
                },
                {
                    "sent": "You choose a choose a width gamma and then then you somehow scale it up until it touches the non non Gaussian part.",
                    "label": 0
                },
                {
                    "sent": "So we get a lower bound on the on the non Gaussian potential.",
                    "label": 0
                },
                {
                    "sent": "And the H function which will occur later also in the talk, some of the scaling the height of the lower bound.",
                    "label": 0
                },
                {
                    "sent": "So we've got.",
                    "label": 0
                },
                {
                    "sent": "We now have individual bounds on each of these.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Non Gaussian sites.",
                    "label": 0
                },
                {
                    "sent": "And if you if you combine those then you get.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you look at the the partition function then you can replace the non Gaussian parts by these Gaussian lower bounds and then you get a lower bound joint lower bound on the whole partition function.",
                    "label": 0
                },
                {
                    "sent": "Enter.",
                    "label": 0
                },
                {
                    "sent": "And since this is a Gaussian term and this is also a Gaussian term, this whole object is just a Gaussian integral.",
                    "label": 0
                },
                {
                    "sent": "And if you do it and if you look at the function inside the exponential, our variational criterion reduces to minimizing that objective.",
                    "label": 0
                },
                {
                    "sent": "So fire is the variational criterion we wish to minimize, such as to get as close as possible to the lock partition function.",
                    "label": 0
                },
                {
                    "sent": "So and just to briefly mention what these expressions are, this lock, determinant, coupling term and a is the inverse covariance matrix of our posterior.",
                    "label": 0
                },
                {
                    "sent": "The sum of these individual scaling terms come from come from the from the lower bound scaling of the non Gaussian sites and this term here is written as a minimization, but it's, but it's only a convenient way of writing the multiplication with the with the inverse of a so.",
                    "label": 0
                },
                {
                    "sent": "To to make things more more visible.",
                    "label": 0
                },
                {
                    "sent": "So now we have now bfor inference problem.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize this criterion and now we come to the contribution of that work.",
                    "label": 0
                },
                {
                    "sent": "It's we show in the in the paper we showed that this variational criterion is convex whenever the corresponding map inference problem is convex.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, we propose an algorithm to minimize that criterion, so to convex.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We look at these at these three different terms.",
                    "label": 0
                },
                {
                    "sent": "The first term.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The art term.",
                    "label": 0
                },
                {
                    "sent": "If you look at it, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a quadratic and you and it's a quadratic over linear in gamma, so this this matrix here has the vector gamma containing the individual width of the sites as diagonal entries, so its accrediting over linear, which means it's jointly convex and this this convexity is preserved and minimizations.",
                    "label": 0
                },
                {
                    "sent": "So this term is convex.",
                    "label": 0
                },
                {
                    "sent": "Secondly, we prove in the paper that the lock determined.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Term is also convex in gamma.",
                    "label": 0
                },
                {
                    "sent": "And and finally, these these some of these scaling functions.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The only term depending on on the sites on the on, the specific forms of the sites and we show in the paper that whenever the sites are log concave, these scaling functions are convex.",
                    "label": 0
                },
                {
                    "sent": "And to put it together, this whole means you have a sum of three convex parts.",
                    "label": 0
                },
                {
                    "sent": "Then this whole optimization variational optimization problem is convex.",
                    "label": 0
                },
                {
                    "sent": "Whenever the map problem is convex.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we are left with the criterion and and we want to.",
                    "label": 0
                },
                {
                    "sent": "We want to find the optimal optimum of that criteria.",
                    "label": 0
                },
                {
                    "sent": "I think it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a convex function, so you might.",
                    "label": 0
                },
                {
                    "sent": "You might be tempted to say that that it's already solved, but it's, but it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not that easy.",
                    "label": 0
                },
                {
                    "sent": "We have like the green part which is, which decouples in gamma, so this is a function.",
                    "label": 0
                },
                {
                    "sent": "This is a sum over individual gamma component and here you can rewrite it as a sum of squares over over over over gammas.",
                    "label": 0
                },
                {
                    "sent": "So this is decoupling part and this in red.",
                    "label": 0
                },
                {
                    "sent": "Here is a very highly coupled part so it's occurs.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the diagonal and.",
                    "label": 0
                },
                {
                    "sent": "And if you want to compute the gradient of the whole of the whole objective, then that's the part which causes trouble.",
                    "label": 0
                },
                {
                    "sent": "So if you computing the gradient means computing the diagonal of a very very huge matrix, and this is, this means cubic scaling.",
                    "label": 0
                },
                {
                    "sent": "Remember, for example in the image Ng image example, we have say 100 thousands of.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or even more pixels.",
                    "label": 0
                },
                {
                    "sent": "And then then this is a very very costly operation and even even even doing so approximately is very time consuming.",
                    "label": 0
                },
                {
                    "sent": "So it's basically it's computing Gaussian variances.",
                    "label": 0
                },
                {
                    "sent": "So previous coordination methods do the following.",
                    "label": 0
                },
                {
                    "sent": "They fix all all gammas except for except for one, and optimizing for one single gamma means is as expensive as solving a linear system of size, number of variables.",
                    "label": 0
                },
                {
                    "sent": "So we would have to solve linear systems of that size, and this is also.",
                    "label": 1
                },
                {
                    "sent": "Very expensive.",
                    "label": 0
                },
                {
                    "sent": "And our strategy to to minimize that function is is the following.",
                    "label": 0
                },
                {
                    "sent": "We identify.",
                    "label": 0
                },
                {
                    "sent": "We identify the coupling part of this log determinant part and D couple it by upper bounding it.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the following, not only that, log determinant of a is convex in gamma, it is also concave in gamma inverse.",
                    "label": 1
                },
                {
                    "sent": "And this and this is allow allows us by means of.",
                    "label": 0
                },
                {
                    "sent": "Other another legend relative to Upper bound.",
                    "label": 0
                },
                {
                    "sent": "This concave part by a linear function, linear in gamma inverse.",
                    "label": 0
                },
                {
                    "sent": "So and once we upper bound it this linear function decouples and then the optimization becomes simpler.",
                    "label": 0
                },
                {
                    "sent": "So the resulting algorithm is a double loop algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It takes has the following form.",
                    "label": 0
                },
                {
                    "sent": "So the this objective is split into sketched here, then concave and concave and a convex part and the outer loop step consists of upper bounding the concave part by linear function by means of legend, reality and the inner loop step is then.",
                    "label": 0
                },
                {
                    "sent": "Consists of optimizing the decoupled objective, so you would you would upper bound here the concave part by the by the linear function and then you would get this modified criterion and this modified criterion is minimize much simpler than the function as a whole.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are a lot more details involved and they are also described on the poster, but this is to provide you with a rough idea what this decoupling looks like.",
                    "label": 0
                },
                {
                    "sent": "Um, yeah, summarize the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This double loop algorithm if you if you look at the usual gradient based approach, you would have to compute a lot of a lot of gradients and then you would.",
                    "label": 1
                },
                {
                    "sent": "Then you would compute a decent direction, whereas in this double loop algorithm we only have a kind of fixed number of iterations and we make and we compute the gradient also approximately and this and the inner loop optimization we do.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a simple optimization, so it's an iterative Leafly reweighted least squares problem, and people can do it on very large scales.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to make efficient use of these expensive computations.",
                    "label": 1
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "We we were able to buy these ideas, we are able to optimize for the criterion and this and this leads leads posterior approximation and but our initial goal was.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experimental design based on that approximate posterior and this is done by means of my means of the information gain is gone.",
                    "label": 1
                },
                {
                    "sent": "So we look at the expected relative entropy between the approximate posterior and the approximate posterior with the with another tentative candidate included.",
                    "label": 0
                },
                {
                    "sent": "For example, in the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Magic resonance imaging example.",
                    "label": 0
                },
                {
                    "sent": "This is expected to enter relative entropy is A is an integral over over Gaussian distribution in certain circles in expectation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and the classification example of this is a sum over the possible labels of the of the feature vector.",
                    "label": 0
                },
                {
                    "sent": "So and this this information gain score now allows us to compare the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The expected amount of information gain of the candidate and we can pick the most or the expected most informative candidate.",
                    "label": 0
                },
                {
                    "sent": "So and finally the design loop looks as follows.",
                    "label": 0
                },
                {
                    "sent": "You compute or update your approximation to the posterior, then you then you score your candidates and you pick the most informative candidate.",
                    "label": 0
                },
                {
                    "sent": "And finally in the third step you do you do the measurement for this candidate and then you, and then you iterate over all these steps.",
                    "label": 0
                },
                {
                    "sent": "So then we.",
                    "label": 0
                },
                {
                    "sent": "We applied the this variational methodology to to.",
                    "label": 0
                },
                {
                    "sent": "Reclassification active learning so we have.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We picked datasets where the number of variables is in the 10s of thousands and also the number of features is very high.",
                    "label": 0
                },
                {
                    "sent": "And what we see is.",
                    "label": 0
                },
                {
                    "sent": "By means of that by that.",
                    "label": 0
                },
                {
                    "sent": "Approach we get like moderate gains over the over and picking picking these.",
                    "label": 0
                },
                {
                    "sent": "These examples at random and using our optimized scores.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this brings me to the conclusions.",
                    "label": 0
                },
                {
                    "sent": "The main contribution of the paper is.",
                    "label": 0
                },
                {
                    "sent": "To show that our variational relaxation is a well known variation of relaxation is convex whenever the corresponding map estimation problem is convex, then we propose a scalable algorithm to solve for that for the optimization problem and an experiments.",
                    "label": 1
                },
                {
                    "sent": "We show that this this methodology allows to drive really large scale models.",
                    "label": 0
                },
                {
                    "sent": "So because the large scale.",
                    "label": 0
                },
                {
                    "sent": "Where it is quite bold but I want to remember that I want to remind you that the computational primitives we use in that in that approach is they are well understood and they really scale too large to large dimensions, so we use conjugate gradients for the inner loop optimization and we lose use the land search algorithm to estimate the variances in the outer loop.",
                    "label": 1
                },
                {
                    "sent": "And they all they are only limited by the speed of matrix vector multiplications with these system matrices.",
                    "label": 0
                },
                {
                    "sent": "X&B that means if you you don't have to store X or B, you only have to be able to multiply with them efficiently.",
                    "label": 0
                },
                {
                    "sent": "And then this whole approach is is scalable?",
                    "label": 0
                },
                {
                    "sent": "Thanks for attention.",
                    "label": 0
                }
            ]
        }
    }
}