{
    "id": "3bxmgc6ybtyxghqfxpiepjkjerdgpids",
    "title": "Joint Visual-Textual Sentiment Analysis Based on Cross-modality Attention Mechanism",
    "info": {
        "author": [
            "Xuelin Zhu, Bandung Institute of Technology"
        ],
        "published": "Jan. 29, 2019",
        "recorded": "January 2019",
        "category": [
            "Top->Computers->Multimedia"
        ]
    },
    "url": "http://videolectures.net/multimediamodeling2019_zhu_attention_mechanism/",
    "segmentation": [
        [
            "Good morning everyone.",
            "It's my great honor to have this chance please.",
            "Then our plan, our work here and our work is about joining the visual textual sentiment analysis based on Crossmodal attention mechanism and I'm the first author from Southeastern University in China."
        ],
        [
            "And avenues are working in this final part, first in the."
        ],
        [
            "Production."
        ],
        [
            "And with growing popularity of social social network model, more people sharing their experiences and opinions about events and topics in in many social network platforms and then statistics indicates that about 25% O Jays contains image information and 99% or images contain textual information."
        ],
        [
            "Due to the complexity and wearability over user generated content, the performance over sentiment analysis based on single model TI mean here is image or text still lags behind our satisfaction.",
            "And join the visual textual sentiment analysis is challenging since image and text made in evil inconsistent sentiment here in several examples you can see that example A.",
            "The text carries are carriers or positively positive sentiment, but the image image is neutral.",
            "In contrast example be the neutral.",
            "The image carries positive sentiment while the text is neutral.",
            "However, there is more troublesome data example.",
            "See, we can't seem to express a positive sentiment.",
            "According to that, people smile in picture in picture.",
            "However, if we take a look at the text.",
            "I know that this image text pairs try to express a stronger negative sentiment, so I think that visual information attached to your mission should differ in their contribution to sentiment analysis."
        ],
        [
            "And there is a little work."
        ],
        [
            "I will give a brief introduction about some existing work.",
            "First is early Fusion and later future.",
            "Here is some typically.",
            "I'm a typical work."
        ],
        [
            "And then like previous studies, so many.",
            "The company follow a common follow someone, two paradigms, namely only future analytic future.",
            "Only function employees feature Fusion techniques to learn a joint.",
            "Visual, textual, sentiment, sentiment, semantic representation for sentiment analysis and data Fusion, traces, image and text information separately.",
            "By leveraging different domain specific techniques and subsequently utilized all modalities, sentimental able to obtain the ultimate results.",
            "However, due to the semantic gap between racial and textual information, the performance of only Fusion analytic function is limited.",
            "And here is the attention model for multi model tasks and then here is some typical work."
        ],
        [
            "And automatically image captioning and multi model matching between image and sentence have showing the advance or deep neural networks in understanding and jointly modeling visual vision and text content and inspired some ideas over joint feature learning design over attention model and so on."
        ],
        [
            "And here is the summary.",
            "The performance of any future and future is limited when image text pals carry inconsistent sentiment.",
            "And then so far very few I studies have considered that the visual and textual information should differ in their contribution to the sentiment analysis."
        ],
        [
            "And then there is a I will talk about the details of our model."
        ],
        [
            "And then forgiven.",
            "Forgiven image text pal.",
            "We know that not both text and image contributes equally to the sentiment classification.",
            "And then.",
            "And now we believe that which the the intuition underlying our work is that visual information and several key emotional words in sequence many determine the path, sentiment, sentiment, polarity, and then there are two in order to accomplish or.",
            "A robustus sentiment classifier we need to solve 2 problems, 2K problems.",
            "One problem is that how true Bridge is a semantic gap between visual information and textual information.",
            "And the second problem is that how to assign reasonable weights to visual information and textual information."
        ],
        [
            "And then here we use bidirectional recurrent neural network for sentiment embedded before further videos.",
            "How we use it an I will give up briefly introduction about bidirectional recurrent neural network given given the input level sequence X one X2XT.",
            "Regional you continue on network consists of two according to new network, Lemony or forward recurrent neural network and or backward you currently work.",
            "And forward according to new network, raise their votes in its original order, limited from X1 to Lt and the backward recurrent neural network University in the reverse order, namely from FT to X1, and then the bidirectional recurrent neural network concatenates the forward hidden state and backward head state to generate his over hidden stage so that this hidden state can summarize summarize not only preceding word information, but also.",
            "The following words information."
        ],
        [
            "And then the picture that the immediate features are expected by convolutional neural network.",
            "And as I felt it was a pipe directional.",
            "You continually at work and here and there is a is a is a fully connected layer.",
            "Utah.",
            "You can you can project the the image you representation into a specific dimension so that it can be used as a as the input of bidirectional counter network and then we expect that the forward recurrent neural network can take the.",
            "I can take the info, can take the image information into consideration for computing the texture working state and the backward and the backwater requiring the neural network can take the textual information into consideration.",
            "For computing the visual hidden state."
        ],
        [
            "And again and then we use we introduce a.",
            "Across model to attention mechanism, here is a hacker is a context segment context vector.",
            "We expect that you can.",
            "You can discover how this how the visual information and it tells you which is.",
            "The sentiment.",
            "Sentiment classification is contributing to settlement classification.",
            "And then more specifically, sorry.",
            "More specifically, here actually is is transformed, and, unusually to the UI, and then the inner product overview I Anna used you say is.",
            "Is used to measure the relevance between between them in other words.",
            "The similarity, the similarity between UI and the sentiment context vector.",
            "Is used to measure the contribution of UI to the sentiment classification.",
            "And here the.",
            "The Alpha the weight of I is get is obtained by a softmax function and then and then joint representation as is calculated is calculated as the.",
            "As a weighted sum over hidden State and finally of perception is built for sentimental classification."
        ],
        [
            "And then in the experiments.",
            "I beg you."
        ],
        [
            "Experience in these three parts."
        ],
        [
            "Unfortunately that we are to evaluate to evaluate our model.",
            "We build two data set film, Flickr and Getty Images respectively.",
            "You can get more information about this data in these two links and here is the statistics over these two datasets 22 datasets."
        ],
        [
            "And then we compare our model with the.",
            "With several advanced methods such as only Fusion data function, TS, TLC embedded and cross modality, consistent regression and in future."
        ],
        [
            "And then, in addition, we also add A at two rounds of our model to explore how to explore.",
            "That effectively is over over the semantic embedded learning and cross modality attention mechanism.",
            "OK."
        ],
        [
            "And here you are out on the Getty test into that we can find that we can find that our our process model achieve the best result on our metrics.",
            "And if we take a, if we look at this to models.",
            "And as well as above.",
            "Now compared to Inz model the the.",
            "Then you just implement all performance achieved by a embedding model shows that.",
            "Assuming that the semantic learning the semantic embedding, learning.",
            "Temperature the semantic gap between image and attacks the effectively and is also needed to a better.",
            "Better attention model.",
            "And if we compare this to model, we can find that.",
            "Because as a attention model is capable over assigning.",
            "Hannibal weights to visual enter information that's needed to better classification performance."
        ],
        [
            "The.",
            "Results carried out on the visual testing data set.",
            "All models.",
            "All models are the performance or overall these models are declined because over cause that this data set is more noisy.",
            "And then our model also.",
            "Achieve the best result and a similar result.",
            "Similar conclusion can be gathered by can we get?",
            "Can we gotta buy compare this to models and it is too."
        ],
        [
            "And again, when we evaluate our model on the immediate taxpayers will oppose it sentiment.",
            "First, how to find this this this kind of image test pairs with the opposite sentiment?",
            "We are here.",
            "We use our entire model and fine tune the Cabinet model to predict to Beijing the sentimental labor over text and image respectively.",
            "And here.",
            "Here is either use out.",
            "Overall models on the.",
            "Images of text pals on the immediate test pass with a positive sentiment.",
            "We can find that all our models several different performance degradation when image text the carries carries inconsistent sentiments.",
            "Our model still keeps a constant, consistent performance."
        ],
        [
            "And here is our which I will also try to visualize the attention weights calculated calculated by attention model and this.",
            "Example, an example be showed several.",
            "Several several top top ranked positive and negative examples.",
            "Calculated calculated by Lions embedding model.",
            "We can find it at our model.",
            "Pay for preferred images with clear clear facial expression and text with the sentimental words."
        ],
        [
            "And you mention about there.",
            "Here is also another.",
            "Another quantitative analysis is used to check our to check our model.",
            "Our purpose model when image and text carries carries inconsistent.",
            "You consistent sentiment here, here.",
            "Figure C University shows several examples.",
            "Who is?",
            "Who is sentimental you dominated by by image and figure the issue with several examples whose sentiment is dominated by text.",
            "And now we can find it that our model can stand can assign assign Catholic flexibly, assign weights for this.",
            "For both text information and.",
            "Visual information"
        ],
        [
            "Finally, the conclusion."
        ],
        [
            "We prove that bidirectional you can't do that with capable over so many embedded learning and bridging semantic gap between image information and text information, and the cross model attention model is qualified qualified for automatically assigning weights to visual and tells you information and extensive impairments validate the superiority over the proposal model, especially when images and text carries a positive sentiment.",
            "And."
        ],
        [
            "So thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "It's my great honor to have this chance please.",
                    "label": 0
                },
                {
                    "sent": "Then our plan, our work here and our work is about joining the visual textual sentiment analysis based on Crossmodal attention mechanism and I'm the first author from Southeastern University in China.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And avenues are working in this final part, first in the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Production.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with growing popularity of social social network model, more people sharing their experiences and opinions about events and topics in in many social network platforms and then statistics indicates that about 25% O Jays contains image information and 99% or images contain textual information.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Due to the complexity and wearability over user generated content, the performance over sentiment analysis based on single model TI mean here is image or text still lags behind our satisfaction.",
                    "label": 0
                },
                {
                    "sent": "And join the visual textual sentiment analysis is challenging since image and text made in evil inconsistent sentiment here in several examples you can see that example A.",
                    "label": 1
                },
                {
                    "sent": "The text carries are carriers or positively positive sentiment, but the image image is neutral.",
                    "label": 0
                },
                {
                    "sent": "In contrast example be the neutral.",
                    "label": 0
                },
                {
                    "sent": "The image carries positive sentiment while the text is neutral.",
                    "label": 0
                },
                {
                    "sent": "However, there is more troublesome data example.",
                    "label": 0
                },
                {
                    "sent": "See, we can't seem to express a positive sentiment.",
                    "label": 0
                },
                {
                    "sent": "According to that, people smile in picture in picture.",
                    "label": 0
                },
                {
                    "sent": "However, if we take a look at the text.",
                    "label": 1
                },
                {
                    "sent": "I know that this image text pairs try to express a stronger negative sentiment, so I think that visual information attached to your mission should differ in their contribution to sentiment analysis.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there is a little work.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will give a brief introduction about some existing work.",
                    "label": 0
                },
                {
                    "sent": "First is early Fusion and later future.",
                    "label": 1
                },
                {
                    "sent": "Here is some typically.",
                    "label": 0
                },
                {
                    "sent": "I'm a typical work.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then like previous studies, so many.",
                    "label": 0
                },
                {
                    "sent": "The company follow a common follow someone, two paradigms, namely only future analytic future.",
                    "label": 0
                },
                {
                    "sent": "Only function employees feature Fusion techniques to learn a joint.",
                    "label": 1
                },
                {
                    "sent": "Visual, textual, sentiment, sentiment, semantic representation for sentiment analysis and data Fusion, traces, image and text information separately.",
                    "label": 1
                },
                {
                    "sent": "By leveraging different domain specific techniques and subsequently utilized all modalities, sentimental able to obtain the ultimate results.",
                    "label": 1
                },
                {
                    "sent": "However, due to the semantic gap between racial and textual information, the performance of only Fusion analytic function is limited.",
                    "label": 1
                },
                {
                    "sent": "And here is the attention model for multi model tasks and then here is some typical work.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And automatically image captioning and multi model matching between image and sentence have showing the advance or deep neural networks in understanding and jointly modeling visual vision and text content and inspired some ideas over joint feature learning design over attention model and so on.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is the summary.",
                    "label": 0
                },
                {
                    "sent": "The performance of any future and future is limited when image text pals carry inconsistent sentiment.",
                    "label": 1
                },
                {
                    "sent": "And then so far very few I studies have considered that the visual and textual information should differ in their contribution to the sentiment analysis.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there is a I will talk about the details of our model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then forgiven.",
                    "label": 0
                },
                {
                    "sent": "Forgiven image text pal.",
                    "label": 0
                },
                {
                    "sent": "We know that not both text and image contributes equally to the sentiment classification.",
                    "label": 1
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And now we believe that which the the intuition underlying our work is that visual information and several key emotional words in sequence many determine the path, sentiment, sentiment, polarity, and then there are two in order to accomplish or.",
                    "label": 0
                },
                {
                    "sent": "A robustus sentiment classifier we need to solve 2 problems, 2K problems.",
                    "label": 1
                },
                {
                    "sent": "One problem is that how true Bridge is a semantic gap between visual information and textual information.",
                    "label": 0
                },
                {
                    "sent": "And the second problem is that how to assign reasonable weights to visual information and textual information.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then here we use bidirectional recurrent neural network for sentiment embedded before further videos.",
                    "label": 0
                },
                {
                    "sent": "How we use it an I will give up briefly introduction about bidirectional recurrent neural network given given the input level sequence X one X2XT.",
                    "label": 1
                },
                {
                    "sent": "Regional you continue on network consists of two according to new network, Lemony or forward recurrent neural network and or backward you currently work.",
                    "label": 0
                },
                {
                    "sent": "And forward according to new network, raise their votes in its original order, limited from X1 to Lt and the backward recurrent neural network University in the reverse order, namely from FT to X1, and then the bidirectional recurrent neural network concatenates the forward hidden state and backward head state to generate his over hidden stage so that this hidden state can summarize summarize not only preceding word information, but also.",
                    "label": 0
                },
                {
                    "sent": "The following words information.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the picture that the immediate features are expected by convolutional neural network.",
                    "label": 0
                },
                {
                    "sent": "And as I felt it was a pipe directional.",
                    "label": 0
                },
                {
                    "sent": "You continually at work and here and there is a is a is a fully connected layer.",
                    "label": 0
                },
                {
                    "sent": "Utah.",
                    "label": 0
                },
                {
                    "sent": "You can you can project the the image you representation into a specific dimension so that it can be used as a as the input of bidirectional counter network and then we expect that the forward recurrent neural network can take the.",
                    "label": 1
                },
                {
                    "sent": "I can take the info, can take the image information into consideration for computing the texture working state and the backward and the backwater requiring the neural network can take the textual information into consideration.",
                    "label": 0
                },
                {
                    "sent": "For computing the visual hidden state.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again and then we use we introduce a.",
                    "label": 0
                },
                {
                    "sent": "Across model to attention mechanism, here is a hacker is a context segment context vector.",
                    "label": 1
                },
                {
                    "sent": "We expect that you can.",
                    "label": 0
                },
                {
                    "sent": "You can discover how this how the visual information and it tells you which is.",
                    "label": 0
                },
                {
                    "sent": "The sentiment.",
                    "label": 0
                },
                {
                    "sent": "Sentiment classification is contributing to settlement classification.",
                    "label": 0
                },
                {
                    "sent": "And then more specifically, sorry.",
                    "label": 0
                },
                {
                    "sent": "More specifically, here actually is is transformed, and, unusually to the UI, and then the inner product overview I Anna used you say is.",
                    "label": 0
                },
                {
                    "sent": "Is used to measure the relevance between between them in other words.",
                    "label": 0
                },
                {
                    "sent": "The similarity, the similarity between UI and the sentiment context vector.",
                    "label": 0
                },
                {
                    "sent": "Is used to measure the contribution of UI to the sentiment classification.",
                    "label": 0
                },
                {
                    "sent": "And here the.",
                    "label": 0
                },
                {
                    "sent": "The Alpha the weight of I is get is obtained by a softmax function and then and then joint representation as is calculated is calculated as the.",
                    "label": 0
                },
                {
                    "sent": "As a weighted sum over hidden State and finally of perception is built for sentimental classification.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then in the experiments.",
                    "label": 0
                },
                {
                    "sent": "I beg you.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experience in these three parts.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unfortunately that we are to evaluate to evaluate our model.",
                    "label": 0
                },
                {
                    "sent": "We build two data set film, Flickr and Getty Images respectively.",
                    "label": 1
                },
                {
                    "sent": "You can get more information about this data in these two links and here is the statistics over these two datasets 22 datasets.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we compare our model with the.",
                    "label": 0
                },
                {
                    "sent": "With several advanced methods such as only Fusion data function, TS, TLC embedded and cross modality, consistent regression and in future.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then, in addition, we also add A at two rounds of our model to explore how to explore.",
                    "label": 1
                },
                {
                    "sent": "That effectively is over over the semantic embedded learning and cross modality attention mechanism.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here you are out on the Getty test into that we can find that we can find that our our process model achieve the best result on our metrics.",
                    "label": 1
                },
                {
                    "sent": "And if we take a, if we look at this to models.",
                    "label": 0
                },
                {
                    "sent": "And as well as above.",
                    "label": 0
                },
                {
                    "sent": "Now compared to Inz model the the.",
                    "label": 1
                },
                {
                    "sent": "Then you just implement all performance achieved by a embedding model shows that.",
                    "label": 1
                },
                {
                    "sent": "Assuming that the semantic learning the semantic embedding, learning.",
                    "label": 0
                },
                {
                    "sent": "Temperature the semantic gap between image and attacks the effectively and is also needed to a better.",
                    "label": 0
                },
                {
                    "sent": "Better attention model.",
                    "label": 0
                },
                {
                    "sent": "And if we compare this to model, we can find that.",
                    "label": 0
                },
                {
                    "sent": "Because as a attention model is capable over assigning.",
                    "label": 0
                },
                {
                    "sent": "Hannibal weights to visual enter information that's needed to better classification performance.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Results carried out on the visual testing data set.",
                    "label": 0
                },
                {
                    "sent": "All models.",
                    "label": 0
                },
                {
                    "sent": "All models are the performance or overall these models are declined because over cause that this data set is more noisy.",
                    "label": 0
                },
                {
                    "sent": "And then our model also.",
                    "label": 0
                },
                {
                    "sent": "Achieve the best result and a similar result.",
                    "label": 0
                },
                {
                    "sent": "Similar conclusion can be gathered by can we get?",
                    "label": 0
                },
                {
                    "sent": "Can we gotta buy compare this to models and it is too.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, when we evaluate our model on the immediate taxpayers will oppose it sentiment.",
                    "label": 0
                },
                {
                    "sent": "First, how to find this this this kind of image test pairs with the opposite sentiment?",
                    "label": 0
                },
                {
                    "sent": "We are here.",
                    "label": 0
                },
                {
                    "sent": "We use our entire model and fine tune the Cabinet model to predict to Beijing the sentimental labor over text and image respectively.",
                    "label": 0
                },
                {
                    "sent": "And here.",
                    "label": 0
                },
                {
                    "sent": "Here is either use out.",
                    "label": 0
                },
                {
                    "sent": "Overall models on the.",
                    "label": 0
                },
                {
                    "sent": "Images of text pals on the immediate test pass with a positive sentiment.",
                    "label": 0
                },
                {
                    "sent": "We can find that all our models several different performance degradation when image text the carries carries inconsistent sentiments.",
                    "label": 0
                },
                {
                    "sent": "Our model still keeps a constant, consistent performance.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is our which I will also try to visualize the attention weights calculated calculated by attention model and this.",
                    "label": 0
                },
                {
                    "sent": "Example, an example be showed several.",
                    "label": 0
                },
                {
                    "sent": "Several several top top ranked positive and negative examples.",
                    "label": 1
                },
                {
                    "sent": "Calculated calculated by Lions embedding model.",
                    "label": 1
                },
                {
                    "sent": "We can find it at our model.",
                    "label": 0
                },
                {
                    "sent": "Pay for preferred images with clear clear facial expression and text with the sentimental words.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you mention about there.",
                    "label": 0
                },
                {
                    "sent": "Here is also another.",
                    "label": 0
                },
                {
                    "sent": "Another quantitative analysis is used to check our to check our model.",
                    "label": 0
                },
                {
                    "sent": "Our purpose model when image and text carries carries inconsistent.",
                    "label": 0
                },
                {
                    "sent": "You consistent sentiment here, here.",
                    "label": 0
                },
                {
                    "sent": "Figure C University shows several examples.",
                    "label": 0
                },
                {
                    "sent": "Who is?",
                    "label": 0
                },
                {
                    "sent": "Who is sentimental you dominated by by image and figure the issue with several examples whose sentiment is dominated by text.",
                    "label": 0
                },
                {
                    "sent": "And now we can find it that our model can stand can assign assign Catholic flexibly, assign weights for this.",
                    "label": 0
                },
                {
                    "sent": "For both text information and.",
                    "label": 0
                },
                {
                    "sent": "Visual information",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, the conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We prove that bidirectional you can't do that with capable over so many embedded learning and bridging semantic gap between image information and text information, and the cross model attention model is qualified qualified for automatically assigning weights to visual and tells you information and extensive impairments validate the superiority over the proposal model, especially when images and text carries a positive sentiment.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        }
    }
}