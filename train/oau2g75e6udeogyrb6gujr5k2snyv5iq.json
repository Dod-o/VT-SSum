{
    "id": "oau2g75e6udeogyrb6gujr5k2snyv5iq",
    "title": "Robust Runtime Optimization and Skew-Resistant Execution of Analytical SPARQL Queries on Pig",
    "info": {
        "author": [
            "Spyros Kotoulas, IBM Research"
        ],
        "published": "Dec. 3, 2012",
        "recorded": "November 2012",
        "category": [
            "Top->Computer Science->Semantic Web->SPARQL"
        ]
    },
    "url": "http://videolectures.net/iswc2012_kotoulas_analytical_sparql/",
    "segmentation": [
        [
            "Good morning, I am Spirit controllers and I'm with IBM research.",
            "I will talk about robust random query optimization, insecurities and execution of Spark queries on page.",
            "So I did something very novel.",
            "I included the figure in the title slide.",
            "This is to explain the affiliations of the authors.",
            "So this work was started when when Giacoppo and I were visiting Peter at Yahoo Research Barcelona.",
            "Then we continue this work while at the free University.",
            "The state I'm Saddam and together with Peter Bunker is also working at CWI at some point during the later stages of this work, I moved IBM reasons this is very nice figure."
        ],
        [
            "So who is familiar with MapReduce?",
            "OK, so I don't need to explain it nice."
        ],
        [
            "Play something that weighs about a big.",
            "OK, then I need to explain it so Apache Apache pick is a library, let's say on top of Hadoop.",
            "That allows you to write.",
            "You write your programming scripting language and then this is automatically translated to MapReduce jobs.",
            "You can imagine that you have something like very simple SQL statements and they run one after another, so you would have a script where you would say load this data.",
            "So a lot of this data source put it in this relation joint relations and so on.",
            "So what?"
        ],
        [
            "Big Latin pig do so it offers is nice simple scripting language that interfaces to any Hadoop cluster.",
            "What it also does it tries to minimize the number of jobs by grouping operations by grouping operations so it would not make one job to load files, another job to join them if it is possibly has a compiler that will match as many of them as possible.",
            "There's something that big Latin does not do, so the pig Latin does not look at the queries before starting execution.",
            "Since it doesn't look at the queries before starting execution, it doesn't really do any kind of query optimization, so the order that you give for the order that you have in your script will be more or less the order that is run.",
            "It will try to run things in parallel, but it will not reorder operations or do anything like this."
        ],
        [
            "We are not the first to propose using Pig in order to run sparkle queries.",
            "The community has already worked a bit on this so there is work that map sparkly 1.02 pig an in this conference.",
            "Actually at one of the workshops in the beginning there was some work in indexing part of the input in H base so that you have some sort of hybrid between pure MapReduce solution and something using an SQL store.",
            "In some other work people have worked on consolidating operations to reduce the number of the number of jobs.",
            "So what this in essence means is that instead of using the standard drawings that you have in big, they would implement some custom drawings that would reduce the number of jobs.",
            "What the community has not done and what we're addressing in this work, is performing complex queries and by complex queries I mean both syntactically, let's say so we are supporting SPARQL 1.1.",
            "And in terms of complexity, so we have used queries that require normally have quite high runtime.",
            "They do multiple aggregations, filters and so on.",
            "Second issue that has not been addressed.",
            "These join order optimization.",
            "So exactly because you don't have statistiques in big people have not looked on how you can optimize in this work.",
            "We are solving technique to do dynamic query optimization based on hard open pick."
        ],
        [
            "In a nutshell, what is that we really do wait?",
            "What is the domain of art?",
            "What is a domain?",
            "What is the target domain of our technique?",
            "So we are interested in being able to answer very complicated sparkle queries that you would be able to use in an ETL scenario, so we do not mind about simple queries.",
            "Our approach is actually going to be awful for single.",
            "For simple queries.",
            "The reason for this is that we start by indexing nothing.",
            "So we start by just having car data on disk.",
            "And then our goal is to answer a relatively small set of very expensive queries.",
            "We do not keep any index, we just keep files on disk.",
            "So you would want to use something like this in ETL scenario."
        ],
        [
            "The problems that we address in this domain are the following.",
            "So first of all there has been there is vast literature in cost models for query executions both in sparkle and in relational databases, but when using pig, this cost model is different.",
            "For example, you have to pay a very high startup cost.",
            "Every job that you run can easily have an overhead of one minute or even more.",
            "So we need the cost model that's appropriate for a pig in MapReduce.",
            "Second, we don't have statistics, so since we don't have statistics, it is not easy to to select the best execution strategy a priority.",
            "And last, we all know that in RDF we have very significant data.",
            "Secure and given the size of typical Hadoop clusters, this becomes a very big problem, so you don't want one of your nodes working and getting all the load while the others are waiting."
        ],
        [
            "I will just give examples of the intuition of foreign are going for our algorithm in all of these cases.",
            "So first of all query cost estimates.",
            "Here in this figure you see what the database would typically do, so you have to perform three joins.",
            "It would order them from the most elective to the least selective, so it would say, OK, let's first join C&D and then what we get we saw in joining with B and so on, right?",
            "The reason for this is straightforward.",
            "You won't have the fewest possible intermediate results now in big situations would be different."
        ],
        [
            "Because of the parallel nature of the machine, it might actually make much more sense to to make a tree that is more balanced so that you can run more things in parallel so."
        ],
        [
            "In this situation, I would have to run in a very.",
            "In a scenario, would have to make 3 jobs, one for its drawing.",
            "In this one."
        ],
        [
            "I would have to make $3 three jobs, but then two of them would be in parallel so they would run in parallel."
        ],
        [
            "The second and third problem that I mentioned, we can only other, some at runtime.",
            "First of all, these not having statistiques so typically what database or an RDF store would do is that who it would keep all kind of statistiques cardinalities of statement patterns and hit ratios between statement patterns.",
            "Bloom filters, you name it.",
            "There many many techniques and they're all very useful and can improve performance very much.",
            "In our scenario we decided not to preprocess the data.",
            "If we have not preprocessed data, we don't have any statistics.",
            "So one possible solution that you could have is to just go and sample the joints.",
            "So you say, OK, let's run the query on 1% of the data, or 2% of 10% of the data and then see how this works out.",
            "And then I will have some statistics.",
            "The problem with this is that you quickly run out of samples and with the data skew this gives additional problems.",
            "I will explain later and I will give an example of what these problems may be.",
            "The third problem that we have is how do we actually perform the zones?",
            "So in the MapReduce cluster you don't have that much opportunity for dynamics.",
            "If you started obit will run the way you started it.",
            "So before you start doing a join you have to decide what kind of join you're doing even more.",
            "If you want to group several joints then you have to know what implementation you're going to use for its own.",
            "For example, in pig.",
            "You can have a standard joint which is group.",
            "Yeah, you read your input and then you use a reduce function in order to execute a joint.",
            "Or you can do everything in the map.",
            "Again I will go into more detail about this later."
        ],
        [
            "So for both the second the same, the third problem, our general approach, let's say, is what we call iterative sampling.",
            "You can also call it dynamic query optimization.",
            "There many different names that you could call it.",
            "So what we're doing is we're selecting the most promising job.",
            "The most promising joins that we could do.",
            "At this point we sampled them.",
            "Mixing and next thing we're doing is we are fully executing the query plans leading up to the argument of this joint, so we don't have to have the entire input of these joints, but we can let's say.",
            "Take a bit of risk and say OK in the future we think that these drawings will be useful and will only run up to the point where I can run these stories.",
            "Third thing we do is that we sample this promising joins and we tend to the first point.",
            "So you can think of it as a cycle.",
            "You select some path that you want to explore.",
            "You think it is promising you calculate the prerequisites for this path and then you look ahead.",
            "So you sampling you say OK, Now I know how it would work if I would follow this path and then you keep on doing this until you execute the entire query or until you have enough confidence on your execution strategy that now you say OK, there is no point in doing this.",
            "Iteration anymore, I just have a good idea of how this supposed to run and I will run it.",
            "Of course, in this case it's very important to reuse result so many times it can be that we're exploring two different paths, but they require that the execution of the same jobs over the same join.",
            "We store all of these results and we try to reuse them as much as possible and we take that into consideration our cost model.",
            "So the cost of something is not the cost in principle is a cost in practice, or how much it would actually cost you.",
            "Now if you perform this operation.",
            "If you have precalculated something.",
            "The cost of this operation is just reading it."
        ],
        [
            "I mentioned before that the sampling gives issues in the parallel machine, and especially in RDF.",
            "Sampling can join to estimate the outcome.",
            "So to estimate how many results you can get is not as trivial as it might look, so I have an example in the top right where we have two relations.",
            "So we want to join one with two and then the letters I have there are the joint keys.",
            "So let's say that we pick 50% of the sample in the top case will be 50%, and we see that in case number in the case number one, it doesn't make much difference.",
            "Most of the keys that we have are a in the two, we actually pick A and some other ones 'cause we have in the inhalation #1A is very popular in relation #2 is not so popular and from this our estimate, let's say is that OK we get 10 hits so.",
            "We match the two ace 10 times.",
            "At the bottom of the same figure, again we get 50% out of these two relations.",
            "The first one again, it doesn't matter.",
            "We will pick some ace.",
            "It cannot go to my to log from the second relation.",
            "We don't be gay, so we just happen to pick another part of the sample will get 0 results.",
            "This becomes a very big issue and this makes sampling in this scenario not so trivial.",
            "So what have we done so we have taken a well known algorithm from databases called bifocal sampling.",
            "A.",
            "We have adopted it to pig and to sparkle queries in particular.",
            "So how does it work?",
            "We have again to relations and we split the input into things that are popular in one relational, popular in both relations an not popular at all.",
            "For the ones that are not popular at all, we actually don't have much of a problem.",
            "We can just run them as they are.",
            "They don't cause much issue then what we really need to.",
            "I don't want to explain the algorithm completely, but what we really need to make sure is that we don't lose any joins with.",
            "Popular time, so in this case that I have in the example that I have.",
            "We know that day is very popular in relation one in less than one.",
            "That's not a problem because no matter what sample we get, we will get a because these are popular, but the cause is popular in relation one.",
            "We should definitely take it in relation to so that are joining our drawing estimate.",
            "So the cardinality's that we get are as accurate as possible."
        ],
        [
            "The third problem I mentioned was load balancing.",
            "During this choice you can imagine that."
        ],
        [
            "If you would want to do joins between such relations in parallel, you would get similar problem, but now at join execution time.",
            "So if you would partition by these keys that node where all the age would go will have a very serious load issue, it would."
        ],
        [
            "To do actually all the work and the others would do more or less nothing.",
            "A big installer big you can perform a join into ways.",
            "Standard Standard Join is you partition your data using the during the map phase and then you.",
            "Big U.",
            "Send the input to the related producer and they need to reduce her.",
            "Does it store by itself?",
            "But there is a second way you can do it.",
            "You can do what they call replicated join, so I replicated join works if one of the two sides is small enough to fit in main memory.",
            "So in this case what you do is you say OK I will keep side 2IN memory and then everybody during the map phase will join with cyto so you don't partition only one side so and distribute it to all the nodes so that.",
            "You can make the join in the map phase, so this is very fast because you actually don't have this awful face at all, but it has a prerequisite that one side actually has to fit in the main memory, and this it works in many cases it's very fast, is very nice, but as the size of your input increases it works less and less."
        ],
        [
            "What do we do about this?",
            "So what we did is we combine these two joins.",
            "Intuitively, you could say that we split our input into two different kinds of joins.",
            "So what we're doing is we're taking the popular terms and we're doing a replicated joint.",
            "So we have to relations.",
            "Pick the popular terms, join everything with the popular terms using care applicated join which is quite fast and what is not popular.",
            "We're using a standard join.",
            "This reduces any kind of load balancing problems that you might have because the very popular temps are done in replicated joint which do not suffer from load balancing problems and actually in some cases makes the process a bit faster."
        ],
        [
            "Covered on the experiments or using different platforms, so we have used dust for part of the dash for cluster at the fire investigator said down which contain which consisted of 32 dual core nodes with four gigabytes of RAM.",
            "It's so this is a very modest cluster.",
            "We have also run experiments on a very large cluster in Yahoo, consisting of around 3 1/2 thousand nodes, each one of them had the eight cores and 16 gigabytes of RAM.",
            "So the interesting things we think with this cluster is that it is used as later 80 said it is shared between many people at Yahoo.",
            "So you just send the request for a job and you get back the results.",
            "Why the class to find time to refine starter on it?",
            "In in sort, the duskfall allows you to have an isolated environment.",
            "the Yahoo Cluster allows you to test two things.",
            "It allows you to test how your approach scales with a very large number of nodes and also how your approach works.",
            "If you have contention for the resources of the cluster.",
            "Because it is a sad cluster and you don't have much control over when your jobs are on, so we have compared.",
            "Our approach with runic with doors on the biggest machine we could get, so this is for socket with 10 calls per socket server and one terabyte of RAM.",
            "Currently this is more or less one of the largest servers you can.",
            "You can get away running filters are seven so you see the results that we have for all of these systems.",
            "At the left side of the Figure fest we see that in many cases the dashed forward was actually faster than the Yahoo cluster and this might look counter intuitive.",
            "Why would they die?",
            "Why would be very small cluster be so much bigger than this gigantic cluster?",
            "Again, it has to do with contention and job startup costs.",
            "So even if you have training half thousand machines starting starting a job still takes one minute.",
            "Actually, whether you have 50 machines or 5000 machines, if most of the overhead is starting this job's size doesn't make much of a difference.",
            "But we see that OK as the size of the input growth for the Yahoo cluster, the execution time is more or less, it is barely affected.",
            "We see that the column with one billion triples on the Yahoo cluster at a 10 billion triples on the Yahoo cluster.",
            "Some of them are faster for the 10 billion.",
            "Someone faster for the 1 billion.",
            "So the scalability in this case is not impaired.",
            "Again, this has to do with the size of a cluster.",
            "You'll have so much parallelization that.",
            "Increasing the size of your data.",
            "Does not have that significant influence.",
            "Now if we compare between the two systems, so our approach in Victoria, so on using Bpay Sobti is more or less the most computationally intensive benchmark.",
            "Right now it has lots of aggregates.",
            "It has generally quite expensive operations.",
            "This is why you see that they run times both for our system and if it was all quite high.",
            "So to run the single batch of queries on this BSB MBI, we see that the approach it takes around 8 hours and if it was it takes 2 hours.",
            "So we are actually four times slower than the very big server.",
            "And you can see also in some queries our approach is much much much slower.",
            "So something take some half a second in Everett was only takes us about this is of course to be expected because we actually read the input every time.",
            "But the key in this case is that the loading time over it was so for this data set was 61 hours and for us to zero we don't just don't load anything, we don't.",
            "We don't have any kind of index, we just we have the data and we we start.",
            "So I mean it's very difficult to make general assessment out of this, but we just say that our system is very good when you want to have relatively few queries that are really expensive over large amounts of data."
        ],
        [
            "We also try that approach on an even larger data set, so this is 26 billion triple web crawl from Java and for this we try to find model the very challenging queries so you can see that this theory that these queries are very selective and actually I doubt there is anything fixed in them.",
            "So there on time for these queries was again very manageable given the size of the data and the complexity of this query.",
            "So in the computational complexity of this query.",
            "So we were very happy as you can see."
        ],
        [
            "Start the conclusion.",
            "We have developed an approach to execute the sparkly 1.1 queries and the novelty that we offer approaches that we made algorithms to have to query.",
            "Plan while taking into consideration how big works and how MapReduce works in general and we did not rely on any precalculated statistics because we just didn't have any.",
            "So we developed a dynamic query optimization method.",
            "In given the size of the infrastructure we have, we walked into dealing with you and with sampling.",
            "So we did that.",
            "We implemented the robust and accurate sampling method that works in this situations and we also implemented some joints that can deal with very large clusters, so I don't have the results here.",
            "But even for this very, very unselective queries that we had the load balancing of our system was very good, so we did not have any problems with load balancing.",
            "We have verified using real web data that our approach is suitable for doing this kind of complex analysis task or ETL like queries.",
            "Actually, if you would have a simpler retail scenario will be even faster if you would just want to fill then transform data, it will be much faster, But then you would not have many joints so we did not.",
            "We decided to skip that part.",
            "Now, compared to a standard RDF store running the largest largest server we could find, we just say our approach is better for posting few expensive queries because we just don't have loading time.",
            "Otherwise, if you want to learn to run small queries, you should not be using MapReduce in the first place."
        ],
        [
            "For future work, what we have not taken into account the following.",
            "So in some work in some works they have a made custom function to group joins in few hours or a few adults.",
            "Also it is possibly knowing that the joints that you are making a specific to MapReduce to group them in a more smart way.",
            "We did not go there, we just use the operators that were already in pig.",
            "Circle there is.",
            "There is opportunity.",
            "Let's say to reduce the number of jobs that we have by actually indexing part of the input.",
            "Now again this is a tradeoff.",
            "Becausw indexing has a costs.",
            "It would be nice to investigate this trade off or even make a system that can adapt to.",
            "So let's say to the workload that you have or what you need to do.",
            "And finally, this kind of approach is specifically suited to do sort of rule based reasoning just becausw.",
            "It allows you to run very very large joints that have some very skewed data distribution and so on.",
            "We do not do anything for a recursiveness or anything like this, but if you if your logic is simple enough, such an infrastructure would be very nice to in order to materialize some simple rules and so on."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning, I am Spirit controllers and I'm with IBM research.",
                    "label": 0
                },
                {
                    "sent": "I will talk about robust random query optimization, insecurities and execution of Spark queries on page.",
                    "label": 1
                },
                {
                    "sent": "So I did something very novel.",
                    "label": 0
                },
                {
                    "sent": "I included the figure in the title slide.",
                    "label": 0
                },
                {
                    "sent": "This is to explain the affiliations of the authors.",
                    "label": 0
                },
                {
                    "sent": "So this work was started when when Giacoppo and I were visiting Peter at Yahoo Research Barcelona.",
                    "label": 0
                },
                {
                    "sent": "Then we continue this work while at the free University.",
                    "label": 0
                },
                {
                    "sent": "The state I'm Saddam and together with Peter Bunker is also working at CWI at some point during the later stages of this work, I moved IBM reasons this is very nice figure.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So who is familiar with MapReduce?",
                    "label": 0
                },
                {
                    "sent": "OK, so I don't need to explain it nice.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Play something that weighs about a big.",
                    "label": 0
                },
                {
                    "sent": "OK, then I need to explain it so Apache Apache pick is a library, let's say on top of Hadoop.",
                    "label": 0
                },
                {
                    "sent": "That allows you to write.",
                    "label": 0
                },
                {
                    "sent": "You write your programming scripting language and then this is automatically translated to MapReduce jobs.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that you have something like very simple SQL statements and they run one after another, so you would have a script where you would say load this data.",
                    "label": 0
                },
                {
                    "sent": "So a lot of this data source put it in this relation joint relations and so on.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Big Latin pig do so it offers is nice simple scripting language that interfaces to any Hadoop cluster.",
                    "label": 1
                },
                {
                    "sent": "What it also does it tries to minimize the number of jobs by grouping operations by grouping operations so it would not make one job to load files, another job to join them if it is possibly has a compiler that will match as many of them as possible.",
                    "label": 1
                },
                {
                    "sent": "There's something that big Latin does not do, so the pig Latin does not look at the queries before starting execution.",
                    "label": 0
                },
                {
                    "sent": "Since it doesn't look at the queries before starting execution, it doesn't really do any kind of query optimization, so the order that you give for the order that you have in your script will be more or less the order that is run.",
                    "label": 0
                },
                {
                    "sent": "It will try to run things in parallel, but it will not reorder operations or do anything like this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are not the first to propose using Pig in order to run sparkle queries.",
                    "label": 0
                },
                {
                    "sent": "The community has already worked a bit on this so there is work that map sparkly 1.02 pig an in this conference.",
                    "label": 0
                },
                {
                    "sent": "Actually at one of the workshops in the beginning there was some work in indexing part of the input in H base so that you have some sort of hybrid between pure MapReduce solution and something using an SQL store.",
                    "label": 0
                },
                {
                    "sent": "In some other work people have worked on consolidating operations to reduce the number of the number of jobs.",
                    "label": 0
                },
                {
                    "sent": "So what this in essence means is that instead of using the standard drawings that you have in big, they would implement some custom drawings that would reduce the number of jobs.",
                    "label": 0
                },
                {
                    "sent": "What the community has not done and what we're addressing in this work, is performing complex queries and by complex queries I mean both syntactically, let's say so we are supporting SPARQL 1.1.",
                    "label": 1
                },
                {
                    "sent": "And in terms of complexity, so we have used queries that require normally have quite high runtime.",
                    "label": 0
                },
                {
                    "sent": "They do multiple aggregations, filters and so on.",
                    "label": 0
                },
                {
                    "sent": "Second issue that has not been addressed.",
                    "label": 0
                },
                {
                    "sent": "These join order optimization.",
                    "label": 0
                },
                {
                    "sent": "So exactly because you don't have statistiques in big people have not looked on how you can optimize in this work.",
                    "label": 0
                },
                {
                    "sent": "We are solving technique to do dynamic query optimization based on hard open pick.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a nutshell, what is that we really do wait?",
                    "label": 1
                },
                {
                    "sent": "What is the domain of art?",
                    "label": 0
                },
                {
                    "sent": "What is a domain?",
                    "label": 0
                },
                {
                    "sent": "What is the target domain of our technique?",
                    "label": 0
                },
                {
                    "sent": "So we are interested in being able to answer very complicated sparkle queries that you would be able to use in an ETL scenario, so we do not mind about simple queries.",
                    "label": 1
                },
                {
                    "sent": "Our approach is actually going to be awful for single.",
                    "label": 0
                },
                {
                    "sent": "For simple queries.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is that we start by indexing nothing.",
                    "label": 0
                },
                {
                    "sent": "So we start by just having car data on disk.",
                    "label": 0
                },
                {
                    "sent": "And then our goal is to answer a relatively small set of very expensive queries.",
                    "label": 0
                },
                {
                    "sent": "We do not keep any index, we just keep files on disk.",
                    "label": 0
                },
                {
                    "sent": "So you would want to use something like this in ETL scenario.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problems that we address in this domain are the following.",
                    "label": 0
                },
                {
                    "sent": "So first of all there has been there is vast literature in cost models for query executions both in sparkle and in relational databases, but when using pig, this cost model is different.",
                    "label": 0
                },
                {
                    "sent": "For example, you have to pay a very high startup cost.",
                    "label": 0
                },
                {
                    "sent": "Every job that you run can easily have an overhead of one minute or even more.",
                    "label": 0
                },
                {
                    "sent": "So we need the cost model that's appropriate for a pig in MapReduce.",
                    "label": 1
                },
                {
                    "sent": "Second, we don't have statistics, so since we don't have statistics, it is not easy to to select the best execution strategy a priority.",
                    "label": 0
                },
                {
                    "sent": "And last, we all know that in RDF we have very significant data.",
                    "label": 0
                },
                {
                    "sent": "Secure and given the size of typical Hadoop clusters, this becomes a very big problem, so you don't want one of your nodes working and getting all the load while the others are waiting.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will just give examples of the intuition of foreign are going for our algorithm in all of these cases.",
                    "label": 0
                },
                {
                    "sent": "So first of all query cost estimates.",
                    "label": 1
                },
                {
                    "sent": "Here in this figure you see what the database would typically do, so you have to perform three joins.",
                    "label": 0
                },
                {
                    "sent": "It would order them from the most elective to the least selective, so it would say, OK, let's first join C&D and then what we get we saw in joining with B and so on, right?",
                    "label": 1
                },
                {
                    "sent": "The reason for this is straightforward.",
                    "label": 0
                },
                {
                    "sent": "You won't have the fewest possible intermediate results now in big situations would be different.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because of the parallel nature of the machine, it might actually make much more sense to to make a tree that is more balanced so that you can run more things in parallel so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this situation, I would have to run in a very.",
                    "label": 0
                },
                {
                    "sent": "In a scenario, would have to make 3 jobs, one for its drawing.",
                    "label": 0
                },
                {
                    "sent": "In this one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would have to make $3 three jobs, but then two of them would be in parallel so they would run in parallel.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second and third problem that I mentioned, we can only other, some at runtime.",
                    "label": 1
                },
                {
                    "sent": "First of all, these not having statistiques so typically what database or an RDF store would do is that who it would keep all kind of statistiques cardinalities of statement patterns and hit ratios between statement patterns.",
                    "label": 0
                },
                {
                    "sent": "Bloom filters, you name it.",
                    "label": 0
                },
                {
                    "sent": "There many many techniques and they're all very useful and can improve performance very much.",
                    "label": 1
                },
                {
                    "sent": "In our scenario we decided not to preprocess the data.",
                    "label": 0
                },
                {
                    "sent": "If we have not preprocessed data, we don't have any statistics.",
                    "label": 0
                },
                {
                    "sent": "So one possible solution that you could have is to just go and sample the joints.",
                    "label": 1
                },
                {
                    "sent": "So you say, OK, let's run the query on 1% of the data, or 2% of 10% of the data and then see how this works out.",
                    "label": 0
                },
                {
                    "sent": "And then I will have some statistics.",
                    "label": 1
                },
                {
                    "sent": "The problem with this is that you quickly run out of samples and with the data skew this gives additional problems.",
                    "label": 1
                },
                {
                    "sent": "I will explain later and I will give an example of what these problems may be.",
                    "label": 0
                },
                {
                    "sent": "The third problem that we have is how do we actually perform the zones?",
                    "label": 0
                },
                {
                    "sent": "So in the MapReduce cluster you don't have that much opportunity for dynamics.",
                    "label": 0
                },
                {
                    "sent": "If you started obit will run the way you started it.",
                    "label": 0
                },
                {
                    "sent": "So before you start doing a join you have to decide what kind of join you're doing even more.",
                    "label": 0
                },
                {
                    "sent": "If you want to group several joints then you have to know what implementation you're going to use for its own.",
                    "label": 1
                },
                {
                    "sent": "For example, in pig.",
                    "label": 0
                },
                {
                    "sent": "You can have a standard joint which is group.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you read your input and then you use a reduce function in order to execute a joint.",
                    "label": 0
                },
                {
                    "sent": "Or you can do everything in the map.",
                    "label": 0
                },
                {
                    "sent": "Again I will go into more detail about this later.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for both the second the same, the third problem, our general approach, let's say, is what we call iterative sampling.",
                    "label": 0
                },
                {
                    "sent": "You can also call it dynamic query optimization.",
                    "label": 0
                },
                {
                    "sent": "There many different names that you could call it.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is we're selecting the most promising job.",
                    "label": 0
                },
                {
                    "sent": "The most promising joins that we could do.",
                    "label": 1
                },
                {
                    "sent": "At this point we sampled them.",
                    "label": 0
                },
                {
                    "sent": "Mixing and next thing we're doing is we are fully executing the query plans leading up to the argument of this joint, so we don't have to have the entire input of these joints, but we can let's say.",
                    "label": 1
                },
                {
                    "sent": "Take a bit of risk and say OK in the future we think that these drawings will be useful and will only run up to the point where I can run these stories.",
                    "label": 1
                },
                {
                    "sent": "Third thing we do is that we sample this promising joins and we tend to the first point.",
                    "label": 0
                },
                {
                    "sent": "So you can think of it as a cycle.",
                    "label": 0
                },
                {
                    "sent": "You select some path that you want to explore.",
                    "label": 0
                },
                {
                    "sent": "You think it is promising you calculate the prerequisites for this path and then you look ahead.",
                    "label": 0
                },
                {
                    "sent": "So you sampling you say OK, Now I know how it would work if I would follow this path and then you keep on doing this until you execute the entire query or until you have enough confidence on your execution strategy that now you say OK, there is no point in doing this.",
                    "label": 0
                },
                {
                    "sent": "Iteration anymore, I just have a good idea of how this supposed to run and I will run it.",
                    "label": 0
                },
                {
                    "sent": "Of course, in this case it's very important to reuse result so many times it can be that we're exploring two different paths, but they require that the execution of the same jobs over the same join.",
                    "label": 0
                },
                {
                    "sent": "We store all of these results and we try to reuse them as much as possible and we take that into consideration our cost model.",
                    "label": 0
                },
                {
                    "sent": "So the cost of something is not the cost in principle is a cost in practice, or how much it would actually cost you.",
                    "label": 0
                },
                {
                    "sent": "Now if you perform this operation.",
                    "label": 0
                },
                {
                    "sent": "If you have precalculated something.",
                    "label": 0
                },
                {
                    "sent": "The cost of this operation is just reading it.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mentioned before that the sampling gives issues in the parallel machine, and especially in RDF.",
                    "label": 0
                },
                {
                    "sent": "Sampling can join to estimate the outcome.",
                    "label": 1
                },
                {
                    "sent": "So to estimate how many results you can get is not as trivial as it might look, so I have an example in the top right where we have two relations.",
                    "label": 1
                },
                {
                    "sent": "So we want to join one with two and then the letters I have there are the joint keys.",
                    "label": 0
                },
                {
                    "sent": "So let's say that we pick 50% of the sample in the top case will be 50%, and we see that in case number in the case number one, it doesn't make much difference.",
                    "label": 0
                },
                {
                    "sent": "Most of the keys that we have are a in the two, we actually pick A and some other ones 'cause we have in the inhalation #1A is very popular in relation #2 is not so popular and from this our estimate, let's say is that OK we get 10 hits so.",
                    "label": 0
                },
                {
                    "sent": "We match the two ace 10 times.",
                    "label": 1
                },
                {
                    "sent": "At the bottom of the same figure, again we get 50% out of these two relations.",
                    "label": 0
                },
                {
                    "sent": "The first one again, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "We will pick some ace.",
                    "label": 0
                },
                {
                    "sent": "It cannot go to my to log from the second relation.",
                    "label": 1
                },
                {
                    "sent": "We don't be gay, so we just happen to pick another part of the sample will get 0 results.",
                    "label": 0
                },
                {
                    "sent": "This becomes a very big issue and this makes sampling in this scenario not so trivial.",
                    "label": 0
                },
                {
                    "sent": "So what have we done so we have taken a well known algorithm from databases called bifocal sampling.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "We have adopted it to pig and to sparkle queries in particular.",
                    "label": 0
                },
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "We have again to relations and we split the input into things that are popular in one relational, popular in both relations an not popular at all.",
                    "label": 0
                },
                {
                    "sent": "For the ones that are not popular at all, we actually don't have much of a problem.",
                    "label": 0
                },
                {
                    "sent": "We can just run them as they are.",
                    "label": 0
                },
                {
                    "sent": "They don't cause much issue then what we really need to.",
                    "label": 0
                },
                {
                    "sent": "I don't want to explain the algorithm completely, but what we really need to make sure is that we don't lose any joins with.",
                    "label": 0
                },
                {
                    "sent": "Popular time, so in this case that I have in the example that I have.",
                    "label": 0
                },
                {
                    "sent": "We know that day is very popular in relation one in less than one.",
                    "label": 0
                },
                {
                    "sent": "That's not a problem because no matter what sample we get, we will get a because these are popular, but the cause is popular in relation one.",
                    "label": 0
                },
                {
                    "sent": "We should definitely take it in relation to so that are joining our drawing estimate.",
                    "label": 0
                },
                {
                    "sent": "So the cardinality's that we get are as accurate as possible.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The third problem I mentioned was load balancing.",
                    "label": 0
                },
                {
                    "sent": "During this choice you can imagine that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you would want to do joins between such relations in parallel, you would get similar problem, but now at join execution time.",
                    "label": 0
                },
                {
                    "sent": "So if you would partition by these keys that node where all the age would go will have a very serious load issue, it would.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To do actually all the work and the others would do more or less nothing.",
                    "label": 1
                },
                {
                    "sent": "A big installer big you can perform a join into ways.",
                    "label": 1
                },
                {
                    "sent": "Standard Standard Join is you partition your data using the during the map phase and then you.",
                    "label": 1
                },
                {
                    "sent": "Big U.",
                    "label": 0
                },
                {
                    "sent": "Send the input to the related producer and they need to reduce her.",
                    "label": 0
                },
                {
                    "sent": "Does it store by itself?",
                    "label": 0
                },
                {
                    "sent": "But there is a second way you can do it.",
                    "label": 0
                },
                {
                    "sent": "You can do what they call replicated join, so I replicated join works if one of the two sides is small enough to fit in main memory.",
                    "label": 0
                },
                {
                    "sent": "So in this case what you do is you say OK I will keep side 2IN memory and then everybody during the map phase will join with cyto so you don't partition only one side so and distribute it to all the nodes so that.",
                    "label": 0
                },
                {
                    "sent": "You can make the join in the map phase, so this is very fast because you actually don't have this awful face at all, but it has a prerequisite that one side actually has to fit in the main memory, and this it works in many cases it's very fast, is very nice, but as the size of your input increases it works less and less.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What do we do about this?",
                    "label": 0
                },
                {
                    "sent": "So what we did is we combine these two joins.",
                    "label": 1
                },
                {
                    "sent": "Intuitively, you could say that we split our input into two different kinds of joins.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is we're taking the popular terms and we're doing a replicated joint.",
                    "label": 1
                },
                {
                    "sent": "So we have to relations.",
                    "label": 0
                },
                {
                    "sent": "Pick the popular terms, join everything with the popular terms using care applicated join which is quite fast and what is not popular.",
                    "label": 0
                },
                {
                    "sent": "We're using a standard join.",
                    "label": 1
                },
                {
                    "sent": "This reduces any kind of load balancing problems that you might have because the very popular temps are done in replicated joint which do not suffer from load balancing problems and actually in some cases makes the process a bit faster.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Covered on the experiments or using different platforms, so we have used dust for part of the dash for cluster at the fire investigator said down which contain which consisted of 32 dual core nodes with four gigabytes of RAM.",
                    "label": 0
                },
                {
                    "sent": "It's so this is a very modest cluster.",
                    "label": 0
                },
                {
                    "sent": "We have also run experiments on a very large cluster in Yahoo, consisting of around 3 1/2 thousand nodes, each one of them had the eight cores and 16 gigabytes of RAM.",
                    "label": 0
                },
                {
                    "sent": "So the interesting things we think with this cluster is that it is used as later 80 said it is shared between many people at Yahoo.",
                    "label": 0
                },
                {
                    "sent": "So you just send the request for a job and you get back the results.",
                    "label": 0
                },
                {
                    "sent": "Why the class to find time to refine starter on it?",
                    "label": 0
                },
                {
                    "sent": "In in sort, the duskfall allows you to have an isolated environment.",
                    "label": 0
                },
                {
                    "sent": "the Yahoo Cluster allows you to test two things.",
                    "label": 0
                },
                {
                    "sent": "It allows you to test how your approach scales with a very large number of nodes and also how your approach works.",
                    "label": 0
                },
                {
                    "sent": "If you have contention for the resources of the cluster.",
                    "label": 0
                },
                {
                    "sent": "Because it is a sad cluster and you don't have much control over when your jobs are on, so we have compared.",
                    "label": 0
                },
                {
                    "sent": "Our approach with runic with doors on the biggest machine we could get, so this is for socket with 10 calls per socket server and one terabyte of RAM.",
                    "label": 0
                },
                {
                    "sent": "Currently this is more or less one of the largest servers you can.",
                    "label": 0
                },
                {
                    "sent": "You can get away running filters are seven so you see the results that we have for all of these systems.",
                    "label": 0
                },
                {
                    "sent": "At the left side of the Figure fest we see that in many cases the dashed forward was actually faster than the Yahoo cluster and this might look counter intuitive.",
                    "label": 0
                },
                {
                    "sent": "Why would they die?",
                    "label": 0
                },
                {
                    "sent": "Why would be very small cluster be so much bigger than this gigantic cluster?",
                    "label": 0
                },
                {
                    "sent": "Again, it has to do with contention and job startup costs.",
                    "label": 0
                },
                {
                    "sent": "So even if you have training half thousand machines starting starting a job still takes one minute.",
                    "label": 0
                },
                {
                    "sent": "Actually, whether you have 50 machines or 5000 machines, if most of the overhead is starting this job's size doesn't make much of a difference.",
                    "label": 0
                },
                {
                    "sent": "But we see that OK as the size of the input growth for the Yahoo cluster, the execution time is more or less, it is barely affected.",
                    "label": 0
                },
                {
                    "sent": "We see that the column with one billion triples on the Yahoo cluster at a 10 billion triples on the Yahoo cluster.",
                    "label": 0
                },
                {
                    "sent": "Some of them are faster for the 10 billion.",
                    "label": 0
                },
                {
                    "sent": "Someone faster for the 1 billion.",
                    "label": 0
                },
                {
                    "sent": "So the scalability in this case is not impaired.",
                    "label": 0
                },
                {
                    "sent": "Again, this has to do with the size of a cluster.",
                    "label": 0
                },
                {
                    "sent": "You'll have so much parallelization that.",
                    "label": 0
                },
                {
                    "sent": "Increasing the size of your data.",
                    "label": 0
                },
                {
                    "sent": "Does not have that significant influence.",
                    "label": 0
                },
                {
                    "sent": "Now if we compare between the two systems, so our approach in Victoria, so on using Bpay Sobti is more or less the most computationally intensive benchmark.",
                    "label": 0
                },
                {
                    "sent": "Right now it has lots of aggregates.",
                    "label": 0
                },
                {
                    "sent": "It has generally quite expensive operations.",
                    "label": 0
                },
                {
                    "sent": "This is why you see that they run times both for our system and if it was all quite high.",
                    "label": 0
                },
                {
                    "sent": "So to run the single batch of queries on this BSB MBI, we see that the approach it takes around 8 hours and if it was it takes 2 hours.",
                    "label": 1
                },
                {
                    "sent": "So we are actually four times slower than the very big server.",
                    "label": 1
                },
                {
                    "sent": "And you can see also in some queries our approach is much much much slower.",
                    "label": 0
                },
                {
                    "sent": "So something take some half a second in Everett was only takes us about this is of course to be expected because we actually read the input every time.",
                    "label": 1
                },
                {
                    "sent": "But the key in this case is that the loading time over it was so for this data set was 61 hours and for us to zero we don't just don't load anything, we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't have any kind of index, we just we have the data and we we start.",
                    "label": 0
                },
                {
                    "sent": "So I mean it's very difficult to make general assessment out of this, but we just say that our system is very good when you want to have relatively few queries that are really expensive over large amounts of data.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also try that approach on an even larger data set, so this is 26 billion triple web crawl from Java and for this we try to find model the very challenging queries so you can see that this theory that these queries are very selective and actually I doubt there is anything fixed in them.",
                    "label": 1
                },
                {
                    "sent": "So there on time for these queries was again very manageable given the size of the data and the complexity of this query.",
                    "label": 0
                },
                {
                    "sent": "So in the computational complexity of this query.",
                    "label": 0
                },
                {
                    "sent": "So we were very happy as you can see.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start the conclusion.",
                    "label": 0
                },
                {
                    "sent": "We have developed an approach to execute the sparkly 1.1 queries and the novelty that we offer approaches that we made algorithms to have to query.",
                    "label": 1
                },
                {
                    "sent": "Plan while taking into consideration how big works and how MapReduce works in general and we did not rely on any precalculated statistics because we just didn't have any.",
                    "label": 1
                },
                {
                    "sent": "So we developed a dynamic query optimization method.",
                    "label": 0
                },
                {
                    "sent": "In given the size of the infrastructure we have, we walked into dealing with you and with sampling.",
                    "label": 0
                },
                {
                    "sent": "So we did that.",
                    "label": 0
                },
                {
                    "sent": "We implemented the robust and accurate sampling method that works in this situations and we also implemented some joints that can deal with very large clusters, so I don't have the results here.",
                    "label": 0
                },
                {
                    "sent": "But even for this very, very unselective queries that we had the load balancing of our system was very good, so we did not have any problems with load balancing.",
                    "label": 0
                },
                {
                    "sent": "We have verified using real web data that our approach is suitable for doing this kind of complex analysis task or ETL like queries.",
                    "label": 1
                },
                {
                    "sent": "Actually, if you would have a simpler retail scenario will be even faster if you would just want to fill then transform data, it will be much faster, But then you would not have many joints so we did not.",
                    "label": 1
                },
                {
                    "sent": "We decided to skip that part.",
                    "label": 0
                },
                {
                    "sent": "Now, compared to a standard RDF store running the largest largest server we could find, we just say our approach is better for posting few expensive queries because we just don't have loading time.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if you want to learn to run small queries, you should not be using MapReduce in the first place.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For future work, what we have not taken into account the following.",
                    "label": 1
                },
                {
                    "sent": "So in some work in some works they have a made custom function to group joins in few hours or a few adults.",
                    "label": 0
                },
                {
                    "sent": "Also it is possibly knowing that the joints that you are making a specific to MapReduce to group them in a more smart way.",
                    "label": 0
                },
                {
                    "sent": "We did not go there, we just use the operators that were already in pig.",
                    "label": 0
                },
                {
                    "sent": "Circle there is.",
                    "label": 0
                },
                {
                    "sent": "There is opportunity.",
                    "label": 0
                },
                {
                    "sent": "Let's say to reduce the number of jobs that we have by actually indexing part of the input.",
                    "label": 1
                },
                {
                    "sent": "Now again this is a tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Becausw indexing has a costs.",
                    "label": 0
                },
                {
                    "sent": "It would be nice to investigate this trade off or even make a system that can adapt to.",
                    "label": 0
                },
                {
                    "sent": "So let's say to the workload that you have or what you need to do.",
                    "label": 0
                },
                {
                    "sent": "And finally, this kind of approach is specifically suited to do sort of rule based reasoning just becausw.",
                    "label": 0
                },
                {
                    "sent": "It allows you to run very very large joints that have some very skewed data distribution and so on.",
                    "label": 0
                },
                {
                    "sent": "We do not do anything for a recursiveness or anything like this, but if you if your logic is simple enough, such an infrastructure would be very nice to in order to materialize some simple rules and so on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}