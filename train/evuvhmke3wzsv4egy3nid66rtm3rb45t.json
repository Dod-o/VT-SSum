{
    "id": "evuvhmke3wzsv4egy3nid66rtm3rb45t",
    "title": "Introduction to Reinforcement Learning",
    "info": {
        "author": [
            "Joelle Pineau, McGill University"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/",
    "segmentation": [
        [
            "Good morning everyone, so I think my task for today is to set you on a path to understanding a little bit more.",
            "What is reinforcement learning and my personal objective will be to try to tease apart in what way reinforcement learning is different from supervised learning.",
            "I'm assuming most of you are very familiar with supervised learning and it turns out there's some interesting subtleties about reinforcement learning which will explore and we'll talk also about.",
            "Deep Q Network QN which have been much in the press in the recent year for success on solving various games from Atari to computer go and.",
            "All goes well.",
            "We'll do all that this morning and then in the next session Peter Beale will come in give you a sense of a different class of families for RL which deal more with searching over the policy space."
        ],
        [
            "So let's get started.",
            "I'm starting quite from the basic because I seem to get the feedback that a few of you were not familiar with RL at all, so bear with me if you know all of the basic.",
            "Hopefully there will still be some interesting bits a little bit later on.",
            "The main goal of reinforcement learning is to deal with agents that have to learn a sequence of actions.",
            "So right away I really important element is that you can't assume that your choice of action is going to be independent.",
            "At each time step, there's this notion that there's a sequence of decisions that need to be made, and there's some dependencies between these choices, and so usually we assume that information from the environment is captured in a notion of state, and that there's a notion of a reward, and so that's a feedback signal that tell us how well we're doing in terms of our choice of actions and overtime through exposure to examples, the agent can somehow statistically estimate the relationship between the choices of actions in certain states.",
            "And how is that related to reward, and in particular the quantity we seek to optimize in this case is the sum of rewards over the lifetime of the agent or over the lifetime of the task of the agents.",
            "So we write down the expectation over a particular policy.",
            "Pi pies, the strategy that the agent is choosing the strategy the agent is using to choose these actions.",
            "So we're taking the expectation over this policy, and we have an argmax argument over here.",
            "That expression means we want to find the policy that maximizes that expression, so we're looking at the sum of rewards in that framework is somewhat inspired by some of the early work in psychology going back to Pavlof, who sort of laid out some of the conceptual ideas from that, but it was formalized mathematically in the late 50s by Bellman, and more recently by Rich Sutton and Barto in the late."
        ],
        [
            "These there's a large spectrum of applications of reinforcement learning.",
            "Most of the applications we're hearing about in recent years have to do with games solving games, but it turns out there's many other interesting problems in robotics, for example, where people have been using PRL to optimize the sequence of choices for the robotic agent as well as in optimizing treatment design, in particular for chronic diseases where it's not just a question of choosing treatment a versus treatment B, but there's really a notion that you have to change the treatment.",
            "Overtime as the disease progresses."
        ],
        [
            "So for many years, sort of the biggest success of PRL was really this system called TD Gammon, which was developed by Jerry Tesoro in the mid 90s while he was at IBM in TD Gammon was quite interesting in that it was a example where NRL algorithm trained in this case by playing games against itself, actually learned how to reach a level of play that exceeded that of the best human players, and the reward function was very, very simple, right?",
            "The agent would play the game, so pick a sequence of actions.",
            "And at the end it would receive a feedback of either it won the game and then got a reward of plus 100 or lost the game and then it got a reward of minus 100.",
            "But for all the intermediate time step, the agent received no feedback, no supervision about how well it was doing, and so that's a very very sparse signal in terms of our loss function.",
            "If you compared to supervised learning every time you're presented with an example, you see an object, you predict that this is a dog, and if it's not a dog, you know right away.",
            "In this case, the agent may take dozens of.",
            "Actions before knowing whether it wins or loses.",
            "And it doesn't have direct feedback on which of these actions along the way was crucial to winning or losing, so it needs to learn that aspect directly from the data.",
            "In this case, we'll talk a little bit more about the architecture later.",
            "Once I've built up a little bit more of the formalism, but perhaps it's interesting to know that it was actually playing 1.5 million.",
            "Sorry, so 10 to the six in the million are redundant here.",
            "1.5 million games against itself."
        ],
        [
            "More recently, we've had some will see if this video plays my computer in demo mode has been a little bit difficult about this results showing that we could actually use reinforcement learning to reach human level play in several Atari games.",
            "So in this case there's an emulator for the old Atari console, and we actually train an agent to play these games, and in this case the little gun firing at the bottom that's controlled by the agent after being trained.",
            "So what you're seeing is actually playing.",
            "Quite well Anet can do that across several dozens of different games.",
            "Now, right now the IRL agent for each of these games are trained separately and one of the challenges going forward, and they're starting to be a lot of work on.",
            "This is how do we leverage with the agent has learned on one game to learn much faster on the other games, there's clear evidence that humans are doing that we don't yet have the right architectures for our machines to be doing this."
        ],
        [
            "And perhaps most famously is the result in the last year in March of this year of the Alphago system that was deployed by Deep Mind in a series of game against the leading Go player.",
            "And we all know the outcome of that machine.",
            "Did much better than the human player in this and again at the core of this was a set of algorithms based on deep learning an on reinforcement learning, the intersection of which is deeper L and has been really a very hot field in the last year."
        ],
        [
            "I want to take a second to really argue for paying attention to IRL outside of solving computer games in our particular group, we've spent a number of years trying to figure out how we could optimize neurostimulation policies for reducing the incidence of epilepsy in people who have epileptic seizure.",
            "So in this particular case, the sequence of decisions that we're making relates to how much stimulation and the timing of the simulation to deliver directly to the brain and the state that we are observing is real time.",
            "EG information taken from the brain and so in that case we've been able to show in an animal model we don't yet do this on cute babies.",
            "We have rats which can be kind of cute also.",
            "Even in vitro model where we take a slice of the right brain and put it in a particular preparation, then we can plug in our recording electrodes.",
            "We can plug in our simulation electrode, feed that back to the laptop and the algorithm learns an in the end deploys of strategy and we can show that we can actually reduce the incidence of seizures quite effectively by automatically learning from trial and error on this particular case."
        ],
        [
            "So just to sum up, essentially, you know there's this question.",
            "When should I be using RL versus some other form of machine learning?",
            "And here are my few tips.",
            "One, you know you have to be in a situation where your data comes in the form of a trajectory, not independence.",
            "Examples so we are throwing out this IID assumption, which is so crucial to supervised learning.",
            "The second thing is there has to be a notion that the decisions have a sequential aspect also.",
            "So we're making a sequence of these choices of actions.",
            "We have to be in a case where we have some feedback about this state.",
            "It can be impartial, it can be noisy, it can be delayed all the way at the end like it was in TD Gammon, or it can be at every step of the way, where every time your agent does something, there's someone that says good robot, Bad Robot.",
            "All of these frameworks are allowed, and then perhaps a little bit more subtle.",
            "You have to have a sense that there's going to be a gain in terms of learning when you're optimizing that action choice over maybe a portion of the trajectory, there has to be a notion of generalizability.",
            "Otherwise you can just treat the whole trajectory itself as a supervised learning example, and so if you think you can share some information across pieces of the trajectory, then there all framework provides some useful mathematical formalism."
        ],
        [
            "For handling that.",
            "And so in terms of contrast in reinforcement learning versus supervised learning, whereas in supervised learning you have some input, some outputs and then your training signal is sun comes in the form of some loss function that's expressed after each example.",
            "In RL we still have inputs or outputs.",
            "We usually call them actions and our training system or training signal.",
            "We usually call a reward, but there's a notion that there's a dependency between the inputs that is fed through the environment.",
            "Perhaps it's useful to clarify that I'm going to assume and most of the RL literature assumes, that time is a discrete quantity, so there's kind of a Clock ticking and every tick of the Clock.",
            "This state changes in an action is taken in a reward signal is observed.",
            "So in this case at each click of the Clock after an action, the environment changes the state and then the agent gets to see some version of that, maybe a full observation of this, maybe a partial observation of this thing.",
            "Of training signal is expressed and some of the challenges that we experience in this framework are one that we essentially have to solve jointly.",
            "The problems of learning and planning.",
            "When you're dealing with classical planning systems, you assume that the relation between your choice of actions and the effect on your States and your goals are explicitly specified by some language.",
            "Often some kind of symbolic language.",
            "In this case, we don't know that relationship, so we have to learn the effects of our actions.",
            "We have to learn when reward can be gained, but we also have to do planning in the sense of picking a sequence of actions.",
            "Another important point is that the distribution of your data changes with your choice of actions.",
            "So if I have a robot that always is flying robot that always chooses to go up, then the information I get about the world is going to be of a certain kind.",
            "If I have a robot that always chooses to go down the information I get about the world, the rewards and my states are going to be different, and so there's an effect between that which we have to take into consideration when we do our learning.",
            "We don't assume we have.",
            "Identically distributed data.",
            "The distribution over the data changes as a function of the policy and finally, and perhaps this is the point that has most limited the progress of our L compared to other fields of machine learning is the fact that you need access to an environment.",
            "Most of the work you can't do from just a static batch of data and will discuss this in more detail as we go forward, but you essentially need to be able to apply these actions and observe the effect of these.",
            "Actions in terms of the next state in terms of the reward.",
            "Without this, if you work from just a batch of data, you risk through a sequence of action that hasn't been explored in your batch of data.",
            "Remember, point #2 your data distribution changes if you try a action strategy that takes you this way when all of your data was collected from over here you are going outside of the range of your data set and you really can't do anything, so we need access to the environment to be able to try out your policy's both for learning and for testing purposes.",
            "And so for many years it's a lot harder between research groups to be exchanging environments as opposed to exchanging datasets.",
            "I have a robot in my lab, I can't distribute it to labs everywhere so that they can replicate my experiment, and so this is really been very nice.",
            "In particular in terms of the Atari simulator.",
            "This is finally an environment that can be simulated that we can be transferred quite compactly, so that has really helped the progress of the field.",
            "I think in the last couple of years."
        ],
        [
            "Formally speaking, reinforcement learning is cast as a Markov decision process.",
            "We have our set of states or set of actions that probabilistic effects of those state and actions are system, and as a state takes an action it goes to next state.",
            "We assume that described by a probability distribution, we have a reward function.",
            "We've talked about.",
            "This is usually a real number that reward function as you see by this graphical model can be a function.",
            "Both of my current state, my previous state, my previous action, all of these things can factor in.",
            "And usually I'm not going to talk about it much, but we assume we have some initial state distribution.",
            "We're going to condition our learning on this initial state distribution."
        ],
        [
            "A key property which were making use of in this case is the Markov property.",
            "I'm assuming many of you have seen it before, but in case this is falling through the cracks of your learning, the Markov property essentially states that any information you need to predict the future is contained in the current state, so there's no advantage when you're trying to predict the state at time T to condition on the whole history of previous states we visited the state team at T -- 1 right before contains all the necessary information, and I could add.",
            "Actions in that in terms of conditioning, typically for MVP on the right hand of the equation, we condition on the previous state in action and on the left hand you know we could condition on the whole history of state and actions and so.",
            "But this is a simple Markov property.",
            "Have you seen it for Markov chains and Markov models?",
            "And so on."
        ],
        [
            "So I mentioned earlier that in terms of performance, the metric we're using is really the notion of optimizing the utility of a trajectory over a set of states.",
            "So I'm going to define U of T to be the utility of my trajectory starting from state T, and I'm going to talk about two types of cases.",
            "One of them is what I'm going to call episodic task, not just me.",
            "The literature called that episodic tasks.",
            "So in an episodic task we assume that the task has a finite time horizon.",
            "So there's a set of time steps.",
            "I'll call it capital T and what we're interested is maximizing utility over the sum of these rewards from time step one to time step capital T and the utility can also be defined at intermediate time points through the trajectory.",
            "So U of T little T means that it's the utility from some time point in the middle of the trajectory until the end.",
            "So this formalism for episodic tasks can be used, for example for games and agent that goes through a maze.",
            "Anything where we know there's a finite length to the trajectory.",
            "In contrast to that, we have what we call continuing tasks, and in continuing task we assume that that task can actually go on forever, and so we don't necessarily have a known and time point.",
            "That utility may some.",
            "Until a sequence of rewards till the end of time and you'll notice a new symbol has appeared.",
            "Now I'm introducing gamma and gamma is what we call a discount factor in the role of gamma.",
            "You'll see is mathematically speaking and there's other roles too.",
            "But mathematically, if I want to do this infinite sum, it's not going to go so well.",
            "We're not going to have a bounded value unless we make sure that there are some conditions in place that this sum is bounded, and so gamma is usually a number less than one between zero and one, but at least slightly less than one.",
            "Such that this particular sum has a finite interpretation, and So what we do with the discount."
        ],
        [
            "Factor essentially is to make sure that information later on, sorry Rewards obtained later on in the trajectory have essentially less important because gamma is less than one, have less important in the utility than the rewards acquired in the early time steps, and there's sort of two different interpretations why one reason we put it in there is because mathematically is very convenient.",
            "It gives us in finite sum in terms of problem domain there's a couple of interpretations, one of them is you can actually re express your problem such that there's a probability of dying at.",
            "Each time step, and this is going to be equivalent to having this particular gamma factor in the other way, to think about it is more like in terms of inflation, right?",
            "If I offer you $100 today or I say, well, come back in a year and I'll give you $100 between these two things you have related preferences and depending on the ratio of these preferences, this is what gamma represents in this case."
        ],
        [
            "I've talked about the fact that we are aiming to pick actions according to strategy, so in terms of introducing a little bit of notation, we usually use \u03c0 to denote the strategy.",
            "In this case, I'm expressing a stochastic strategy, so my policy is going to be conditioned on the state and I'm going to have a probability of picking every action in every state, but I can also have a deterministic policy, and in that case the policy is a straight mapping from state to action.",
            "The policy is what I'm trying to learn.",
            "I don't know this from the unset.",
            "And again I want to find the policy pie that maximizes my expected total reward.",
            "Note here that there are many policies.",
            "The number of policy is influenced by a couple factors.",
            "One of them is the number of actions you have right at every state.",
            "You need to consider all of these actions.",
            "Another way to think about is the number of policies you have also depends on the length of your trajectory.",
            "And so, in general, there's many, many policies that we may need."
        ],
        [
            "Need to consider so this is like a simple case of an MDP.",
            "This is an alternate representation from the graphical model that I gave you, and sometimes you'll see em DPS represent in this particular notion, right?",
            "Some of you may be in any one of these states.",
            "Maybe some of you are unemployed.",
            "I won't ask you to raise your hands, some of you are coming from industry, some of you are in grad school.",
            "A few of us are in academia, and in each of these states depending on what choice of actions you do, you may have an arrow.",
            "That indicates the transition probability.",
            "There's one particular crucial error that's missing in this graph.",
            "It hasn't been updated recently.",
            "There seems to be a high trend from academia to industry that is lacking on this particular graph.",
            "I am aiming for a world where the decision is to stay in academia for a good while, and so this is the graph I proposed today, but you can choose to learn a different graph, so in each of these cases, right for each if you wanted to have a full specification of your environment.",
            "And when I talk about exchanging environments between lab, this is the type of specification that I'm talking about.",
            "You need to have a defined probability distribution out of every state that defines the probabilities of going to all of the other States and you also need to have a reward.",
            "In this particular case, the reward is expressed just as a function of state.",
            "Presumably there's like a + 10 reward.",
            "I'm not giving you the units here plus 10 reward for going to industry versus A plus.",
            "One reward for going to academia.",
            "In some cases, the reward may be just the function of state, but it also could be a function of state and action depending on what your domain is."
        ],
        [
            "The next little piece of notation I want to introduce the notion of a value function.",
            "So if we want to estimate a policy, we need to have a sense of if policy A versus policy B.",
            "How are we going to evaluate them?",
            "I've already introduced the notion of utility, right utilities?",
            "The actual observation of the sequence of reward that you have.",
            "The value function is the expectation of that sequence of reward over the policy Pi.",
            "So if you contrast utility utility would be just the sum of RT.",
            "Our little tee up to our big T whereas the value function is the expectation of that sum for particular policy, so I'm talking here.",
            "The value of a policy and that value is defined for all of the states in your system."
        ],
        [
            "So if we look a little bit more closely at the value of a policy, if we top, I just rewrote the expression that I had a little bit earlier.",
            "I have my sum of rewards that I'm taking you through a little bit of notation because there's some very nice structure in our value function that will be useful for building up the algorithmic knowledge later on.",
            "So in this case, at the top row I have the same expression I had on the last one, right?",
            "The expectation over my policy of my sum of reward given that my state at time T is a particular state S. I'm defining it here for the case of episodic task finite horizon that go up to time T, but I'll generalize it to the continuous task a little bit later on in the second step, all we're doing is separating the expectation of the reward at the first time step versus the expectation of the reward for the rest of the trajectory and the expectation of the reward at the first time step can be.",
            "Gained by just taking expectation over the policy for that immediate reward.",
            "So that's the first part of the equation, and then in the second part of the equation what you have to notice is it's essentially the same as the first line.",
            "What I'm calling the future expected sum of rewards essentially the same, but I've removed the first term, but I'm still conditioning on times T."
        ],
        [
            "Now I'm going to take the expectation of going from St to S T + 1 to take that expectation.",
            "I factor in my policy.",
            "I factor in my transition probability and now the term that you have at the end on the right side looks a whole lot like the term you have at the top."
        ],
        [
            "And so we have a recurrence notion here, where you can actually define the value at time at state S as a function of these expectations, and the value at times S + 1.",
            "This is a case of a dynamic programming algorithm, right?",
            "My values at state S are a function of my values at my other States and I take the expectation in terms of those other states.",
            "I can transition to.",
            "You have a question.",
            "Oh yeah, that should be plus one, sorry, yes, thank you.",
            "The first term would be expected value be conditioned on S as well.",
            "Yeah, you should condition on this for that, yeah thanks.",
            "And that goes away when you once you take that expectation, yeah."
        ],
        [
            "So now I have the expectation of a policy and I've rewritten it.",
            "In this case, I've taken introduced the Gamma discount factor, so the discount factor is over your future and so this is the same form of the equation separating the immediate reward.",
            "The future expected reward.",
            "But we have the discounting that shows up before the future expected reward only need to discount one because the other discounts the Gamma Square and so on are folded into the V at S prime.",
            "In this particular case, sometimes my value function is expressed as a function of States and sometimes we express it as a function of state and actions.",
            "And in that case I just take away the expectation over the policy and I instantiate it for a particular action in the first time step.",
            "So this is what we call Bellman's equation.",
            "Bellman's equation comes in slightly different forms and flavors, but it also always has this Canonical form of being expressed as a function of immediate reward and future rewards in the future.",
            "Rewards are expressed using this recursive form."
        ],
        [
            "And so what's interesting to notice now is if I express the value of my policy for all of my states.",
            "I essentially have a system of equations, and when I fix the policy.",
            "I have a system of linear equations and I have N variables and equations, where N is my number of states, so I can actually write it as a system of equation.",
            "I can solve it in close form if my number of state is relatively small and if I have some conditions in terms of the inverse being well behaved.",
            "This isn't usually how we solve it, because most of the time the problems we're interested in have too many states for doing that."
        ],
        [
            "So what we usually do is set up an iterative process, take advantage of the dynamic programming form of it, and set it up as an iterative procedure.",
            "So in that case I start with some initial guess of what my value might be at all of the states.",
            "It turns out that these iteration algorithms are a lot less sensitive to the initial value than what you might be used to for training neural network, and for reasons that will explain a little bit later, we can just set everything to 0, or you can just set it to the immediate reward.",
            "So once you have your initial guess, then you iterate your Bellman equation so you update your value function and so as the value function that state S + 1 gets updated.",
            "So in the next round the value function at state S gets updated again.",
            "You repeat that until some small changes is detected.",
            "Usually we look at the maximum change over all of the states for the changes in the value function.",
            "What's below some threshold?",
            "We stopped doing the iteration.",
            "So I want you to note this is just an algorithm for policy evaluation.",
            "I haven't yet told you how to pick one policy versus another.",
            "All I've told you so far is if I fix the policy.",
            "This is how you're going to get the estimation of the value function for that policy."
        ],
        [
            "We can actually show that this particular procedure is going to converge.",
            "I'm doing this in the case where I have all of my states being a discrete set, so our convergence proof I'm not going to go through all the details of it, but it's relatively simple.",
            "Essentially, the important point is that we can show that the difference between the convergence points, the fixed point of my value function, and the one of the case iteration.",
            "As I'm doing these iterates actually depends on the.",
            "Same quantity.",
            "Times my factor gamma, but because gamma is less than one, I actually have a contraction on that property so that difference between where my value function is and the optimal value function gets smaller at every point due to this discount factor.",
            "So eventually it's going to go to zero.",
            "You are going to be able."
        ],
        [
            "To learn your value function, and so there's actually a nice correspondence between the value function and the policy.",
            "In particular, we use the star to denote the optimal value function.",
            "This is the one that has the maximum possible value.",
            "That the star is obtained by finding the policy that maximizes the value function.",
            "So I've given you couple ways to find that value function to estimate that V Pi V star is the best policy, the maximization point of that policy.",
            "Other than one and necessary condition?",
            "Or could you put some other conditions like contraction of the transition matrix to get the result?",
            "You can certainly put some other condition.",
            "An easy one is finite horizon.",
            "As long as you have a finite horizon you have a finite sum, and so.",
            "But Gamma is certainly the most convenient one and.",
            "And so what's interesting to note is any policy that's going to achieve that value function.",
            "We're going to call it an optimal policy.",
            "We're going to write that Pi star, so V stars optimal value function.",
            "Pi stars are optimal policy, and usually the value function is unique.",
            "There's going to be a unique value function, but the policy doesn't need to be unique.",
            "It's possible in a state there's two different action choices that both yield maximum value."
        ],
        [
            "The other thing that's interesting to note is if somehow you know the optimal value function so you know these are and.",
            "I'm going to assume for now, you also know the model, the rewards, the transition, your discount factor.",
            "If you know we start, then we can obtain the optimal policy very fast.",
            "Right, it's basically one round of Bellman equation, so the complexity of that is linear in your number of actions.",
            "It's quadratic in your number of states.",
            "If you know the value function, we get, the policy will call that essentially for free.",
            "Similarly, if you know the policy, if someone hands you an optimal policy, then you can actually extract the value function very easily again with a single application of Bellman equation.",
            "And I've written down here the case both for the stochastic policy and below the case for the deterministic policy you essentially sub in the policy for your action choice and you get your value function by 1.",
            "Round again of Bellman equation.",
            "And so this is important because today when you see reinforcement learning in the morning, I'm going to talk mostly about value function methods.",
            "So these are methods that Salvar El by trying to estimate the value function.",
            "But the first part of this tells you if I solve for the value function, then I can compute the policy very easily.",
            "In the next session, Peter Bill is going to tell you about policy search method policy optimization method so that class of method actually works, spends all the hard work finding the best policy, but once he's found the best policy, he can actually get the value function very easily.",
            "Also, and we will also discuss.",
            "I think Peter is going to discuss this.",
            "What we call actor critic methods and they work by simultaneously optimizing the value function and the policy and using essentially one to regularize the learning of the other.",
            "But for today, for the first session this morning, I'm mostly talking about."
        ],
        [
            "Value methods so I've given you an iterative algorithm to estimate the value of a given policy.",
            "Now I'm giving you a framework slightly under specified for finding the best possible policy, and that is the following.",
            "Let's say I'm going to start with an initial policy.",
            "Pick a random guess, randomly choose what action to do in every state.",
            "Will call back by zero, and iteratively.",
            "We're going to compute V of PIE for that policy right.",
            "Previous slide I showed you how to do that.",
            "This is easy, so I compute the value for that policy.",
            "I'm going to compute a new policy that's greedy with respect to V. Piso.",
            "New policy that improves a little bit better and how we do that.",
            "All the interesting pieces are in that line.",
            "That's really one of the things Peter is going to talk about.",
            "I'm not going to say a lot more, but I want you to have in mind the generic framework for what we call policy iteration.",
            "In the case where we have a discrete set of States and action, this notion of computing a new policy boils down to just a single round of the Bellman equation again, and we're going to iterate.",
            "Until the policy doesn't change.",
            "If in two rounds my policy hasn't changed, well, if my policy doesn't change, my value is not going to change either, right?",
            "This is a domestic computation, and so we can stop when the policy doesn't change."
        ],
        [
            "In contrast, the value iteration method works on doing similar iterations in this case on the value function.",
            "So in this case I start with a thumb arbitrary choice of my value function.",
            "And at every round I'm going to pick as my value.",
            "The bellman equation for the case of the best possible action.",
            "So if you compare this algorithm to the one I presented for policy evaluation before the main differences, I've introduced a maximization over the choice of action in my Belmond iteration, and so in this one, at every round I get a better value function.",
            "But I also allow the policy to change, and in this case I stop when my change in value function is below some threshold, we can show that under some condition this algorithm is going to converge to the true.",
            "Optimal value function."
        ],
        [
            "I'm going to take a pause and see if there's any quick questions before we move on.",
            "Yes.",
            "It's simply better unlikely.",
            "Yeah, let me say not very much.",
            "Now in a little bit later on that, but yes, I'm assuming I know T or alternate."
        ],
        [
            "I'm assuming from the way I've described the value iterate."
        ],
        [
            "I'm an algorithm, right?",
            "I have my discount factor in there and so I'm assuming I have a continuing task and I don't worry about what's T and my termination condition depends on my convergence on my value function.",
            "OK."
        ],
        [
            "So let me run through a really quick example.",
            "For those of you who haven't seen value iteration, this is really very simple.",
            "We have a robot moving around in a grid world.",
            "It can't go in the blue box and it can either end in the minus 10 state.",
            "Let's call that you know on the pit or it can get plus one in a particular goal, and there's some essentially slight noise in the transition model.",
            "So if it aims to go from the lower left corner an it takes an action going East, it ends up in the square beside it.",
            "With .7 probability with .1 probability ends up somewhere a little bit off from that, and I'm going to assume a discount effect."
        ],
        [
            "0.9.",
            "So this is my value iteration at the very first round, right?",
            "My value function?",
            "My Bellman equation says your value is the immediate reward plus.",
            "Some transition probability in the value of the next state.",
            "Well, all my states were initialized at zero and so in my first round when I get in my value function is essentially just the reward function.",
            "And."
        ],
        [
            "The second round that information gets propagated to all the neighboring state.",
            "When I update the value function at those states, I get to observe the immediate reward, which is 0 plus some reward from the adjacent safe date.",
            "And I can also measure what we call the Bellman residual, which is the difference.",
            "The maximum difference in value over all of the states.",
            "So in this."
        ],
        [
            "Yes I have a bellman residual that .9 and if I do that over a few more iterations here, I'm at iteration #5.",
            "We see essentially the value information diffusing through the graph.",
            "In terms of the structure of the state space, my Bellman residual.",
            "If all goes well should reduce at every time step something to check if you're implementing value."
        ],
        [
            "Iteration after about 20 iterations, things have gotten pretty stable.",
            "My bellman residual is quite small.",
            "I have an estimate of the value everywhere.",
            "I don't have a policy.",
            "How do I get a policy?",
            "Not so hard, right?",
            "One version of Bellman equation I look in every state.",
            "Should I go up or should I go left or should I go route because there's symmetry in my noise in my transition model.",
            "I can essentially always go towards the adjacent square that has higher value.",
            "And we."
        ],
        [
            "And look at that in slightly larger cases, right?",
            "This is an example.",
            "I have four different rooms, each of them have been broken into 16 states or so, and I start in the first case in the first iteration, where my goal is and after a couple iterations, that information propagates diffuses again across the graph.",
            "Based on the structure of the framework."
        ],
        [
            "Perhaps it's useful to think about the fact that in this case I'm assuming that at every round I'm doing a full estimation of the value function over all of the states at each of my iteration.",
            "Maybe that's not so useful, in particular if you look at any."
        ],
        [
            "Example like that where the value hasn't changed anywhere else in the outer states.",
            "Doing a value update in all of these other states probably isn't a very good use of your computation."
        ],
        [
            "There's version of value iteration called call it a synchronous value iteration, where essentially you're doing your updates in according to Priority queue in that priority queue depends on how you visited the States before, and so we can adjust how we do this to be a little bit more efficient in some cases.",
            "Particular focusing on the states that are actually possible under some trajectory rather than trying to do that for all of the states at the same time.",
            "And if you think of a system like Alphago, I'm caricaturing a little bit here, but they had a lot of trajectories from expert games of go between two human experts and those games essentially define trajectory through the state space, and those spend quite a bit of time characterizing and learning those strategies in a way to control the distribution of states that you expect to see in a game.",
            "And focus your learning around states along that distribution rather than try to learn uniformly for all the states which isn't possible in a game like go, where the number of states is astronomical."
        ],
        [
            "OK, this is kind of wraps up my basic tutorial level on RL.",
            "If you want to know more.",
            "If this is giving you just enough of a taste, there's two books that I recommend.",
            "One of them is the basic Sutton and Barto which most of you are probably familiar with.",
            "Its available freely on Rich Sutton website.",
            "There's version edition number two with slightly different notation that is also available.",
            "That is a little bit more work in progress, but sufficiently mature that you can probably use it alternately, as chabas party has put out a reinforcement learning textbook that I quite like also.",
            "It's a little bit more terse, a little bit more mathematical, but you might want to have a look at that one.",
            "Also available freely on Shabbos website."
        ],
        [
            "So let me focus a little bit more on.",
            "Some more challenging problems that arrive.",
            "I would say there is essentially different classes of challenges for reinforcement learning.",
            "One of them is figuring out how we're going to design our problem domain, so we have the problem of state representation.",
            "How do we represent our information and that one shares a lot of similarity with other supervised learning problem and I will get I promise to deep reinforcement learning where we throw in a neural net in this framework.",
            "In this will be where it's most useful in a sense.",
            "How do we represent the state information for a system?",
            "We also often need to choose our actions, but for today I'm going to assume that the choice of action is given by the description of the problem.",
            "Picking the right reward function can be more difficult, often in problems.",
            "In robotics, it's not so bad.",
            "There's a lot of problems in optimization logistics where it's not so bad also, but when we tackle, for example, we do some work on using reinforcement learning to train conversational agents, AI agent with a person having a conversation, picking out the right rib Ward signal for when did the agent say the right thing, and then.",
            "Agent didn't say the right thing can be a lot more difficult, so we have quite a few challenges.",
            "Another area where there's an interesting challenges is figuring out how do we acquire the data we need for training.",
            "How do we figure out what is the distributions of states that are important and how does that bias the inference that we're making in terms of the learning in special?",
            "In the case of robotic system, you can have high cost actions, right?",
            "I talked about a robot flying robot that was going to explore downwards motion continuously.",
            "You really don't want to do that too often 'cause at some point if you're crashing a robot everyday, your supervisor is not going to be very happy.",
            "So you have to figure out how to manage this aspect, and in particular how to deal with the cases where the reward function comes very late turns out to be difficult.",
            "And the last point in terms of validation confidence measure shows up in some application particular the medical application when we're trying to optimize treatments for different conditions, our clinical partners often want to know what is the margin of error on that particular policy.",
            "Can I guarantee that I will not do more harm than good to my patient and so we need to think about how we're going to do that in the framework of RL?"
        ],
        [
            "I'm going to spend a little bit of time because I think that's useful.",
            "To you as you tackle the literature on what I'll call the horel lingo.",
            "There's a lot of words that get thrown around, and they're like little single Knowles.",
            "And if you don't understand these little signals, you may not be able to pick out what are the difficulties in a particular problem.",
            "And as you start designing your own IRL problems, you may need to think about some of these.",
            "So I've picked out a list that I could think about.",
            "Maybe it's not an exhaustive list, but will try to distinguish between these things, so those of you who have seen RL may have seen some of these distinctions and I'll try to tease apart what's the difference between these things.",
            "And why does it matter and what are the things to be looking out for?",
            "Depending on which of these scenarios you're in?"
        ],
        [
            "So the first one I've already talked about, episodic versus continuous task.",
            "I would say in that particular case, the thing to be most concerned about is if you have a finite horizon, what is that horizon that you're looking for?",
            "And if you read into the literature a little bit more carefully, a subtlety that you will find is that if you are in an episodic task domain.",
            "It's actually important to keep the value function from each of your iterations separately.",
            "And when you're deploying your system to actually deploy a different value function for each of your horizon, whereas in the continuous cast where we folded in the discount factor, you don't need to do that.",
            "You have a discount factor, and you can actually maintain just a single value over all of your state at this sort of convergence horizon.",
            "But in the episodic task, if you don't maintain these separate value function at each of the rounds, then we can actually show that you're not going to obtain the best solution that you can.",
            "There's some really interesting thoughts on the continuous task case that have started to come out in the last year, and in particular the notion of how to set your discount factor.",
            "For many years.",
            "We assume that the discount factor was just given with the environment, but there's some really interesting thoughts as to setting your discount factor a little bit more aggressively during learning allows you to achieve a better bias variance tradeoff in terms of your estimation of your value function.",
            "Or you could visit after that it asks, can't you just include T as part of the state?",
            "You can include the and that's exactly what this comes out too.",
            "If you're unfolding them, yeah.",
            "Yeah, and there's actually some nice results.",
            "Also, that shows that there may be some advantage when you're diploid in your system to mix between these different policies.",
            "Pinochet has some nice results on that in the last few years.",
            "OK, second."
        ],
        [
            "Useful distinction is between tabular and function approximation cases, right?",
            "The tabular case is really the case where you have a discrete set of States and you can write it out as a vector and you can have a table of your value function.",
            "The case where you have too many states to write it out.",
            "Is often the more interesting case is what we call the function approximation case, and that covers both when there are accountable number but very large number of states.",
            "In the case where your states are actually continuous multi dimensional.",
            "What's important in this case is that many of the theoretical properties that are shown about our algorithms, whether it's be about convergence properties or about sample complexity properties.",
            "How many examples do I need to see before I learn a good function?",
            "Many of those are mostly for the case of tabular, and we have many fewer theoretical results in terms of convergence.",
            "An learnability for the function approximation case.",
            "So in many cases you have to be a little bit aware of this when you read some of the literature."
        ],
        [
            "Distinction #3 batch versus on line in the batch case, we're assuming that all of your data comes at once, so trajectories from your agent that have been recorded beforehand.",
            "Under some policy you may know the policy.",
            "You may not know the policy, though you can usually estimated from the data, so that's the batch case for many years the Community was really focused on the on line learning case, really aiming towards this notion of lifelong AI agents.",
            "And in that case, the agent takes an action.",
            "We get the reward information and we want that value estimation to be updated in real time as the information comes through.",
            "So there's sort of a cycle of acting, observing the transition, adjusting our Q function, and that cycle repeats throughout the life of the agent, and so there's a set of algorithms that are really amenable more for this on line learning cases in the second online learning case, and one thing to worry about in the online learning cases, really, that.",
            "At each round, as you're adjusting your Q function, you usually use that Q function to choose your next set of actions, so your distribution over the data changes overtime.",
            "In the second case, in a way that you sometimes have to worry about and may not mix well with function approximation dash.",
            "Same thing is offline.",
            "Yes, batch offline is the same thing."
        ],
        [
            "Let me expand a little bit more on online learning because a lot of the literature deals with this particular case.",
            "In particular, Rich Sutton has developed a very large body of work for this a case, and some of the collaborators doing on some of the other RL researchers.",
            "When you're doing on line learning, you're going to get your information one piece at a time, so you're going to need to estimate your value function on line, and there's different classes of method for doing that.",
            "One of them is.",
            "You can treat each new trajectory as a Monte Carlo sample, and essentially have a value function estimate that gets adjusted after each experience.",
            "So in this notation, V is really the my estimate of the value function that I maintain overtime, and you is really the estimate of the utility from the most recent trajectory.",
            "Now, if you recall your definition of UU for St is the utility, so it's the sum of rewards from the point S little T all the way to the end of that trajectory.",
            "So often you have to if you use a true Monte Carlo approach, you have to wait till the end of that trajectory to figure out what was your utility of that and then do your update and we have Alpha, which is a learning rate in this case.",
            "So we're doing essentially a gradient update on line update on this particular case.",
            "It's not a Bellman equation in the sense.",
            "It's really just a difference equation.",
            "And in the second case what we have without temporal difference learning and the point of temporal difference learning is really avoid having to wait all the way to the end of the trajectory.",
            "So instead of looking at the error between my actual utility that I see and my value function that I'm predicting in this case, the utility gets replaced by the sum of the immediate reward plus my estimate of the utility.",
            "And so I have in this case an.",
            "Update that I can apply on every time step.",
            "I don't need to wait till the end of the trajectory to do my update.",
            "This is what we call the temporal difference update TD learning for many cases."
        ],
        [
            "So there's a couple of different flavors of TD learning I've given you here.",
            "The equation for what we call the tabular case, so again, this is the case where you can enumerate all the States and for the version with features.",
            "So in this case I'm assuming that I have a function that is approximating my value function.",
            "The easiest way to think about it for now is just assume that my value function is just a linear regression.",
            "So in this case data would be the weights on my linear regression.",
            "And I can actually update these weights in real time.",
            "What's interesting to note in this particular case?",
            "If you look at here, I have essentially a notion akin to the supervised loss that you've seen often, and so I have V, which is my prediction of whether the value function should be at St an over here I have mine, you estimate based on my immediate observed reward, an again on the value function, but in this case at the next time step.",
            "There's."
        ],
        [
            "The version called TD Lambda TD Lambda takes advantage of the fact that I want to put an update based on the new information right in T."
        ],
        [
            "Zero if I observe some salient new reward information, I only put that update at State St. And for that update to get propagated to the state that I visited the foresty, I'm going to have on another day in a different trajectory to come back again to this S T -- 1 followed by this St and so it's not a very efficient use of."
        ],
        [
            "That new information so TD Lambda essentially seeks to propagate that information to all the states you visited recently, and so there's a notion of eligibility, so you maintain an eligibility over each of the state at each time point.",
            "That eligibility decreases as a function of your discount factor, and also this factor Lambda.",
            "So you can set in this case Lambda from anywhere from zero to 1.",
            "If you set it to 0, you get the TD algorithm I just described previously, and if I set it to one, you can actually show that you essentially recover a Monte Carlo estimate of the value function.",
            "So based on eligibility you update all of the states that have some eligibility greater than zero based on the new salient information from the new sample you've seen."
        ],
        [
            "So this is essentially roughly the algorithm that went into TD Gammon plus some function approximation, so it was a TD version with the feature space, and those features were not a linear regression.",
            "In this case, they were a feedforward neural network, but the correction of that feedforward neural network at the top.",
            "Wasn't the standard supervised criteria, it was the TD error, so it was looking at the difference between the prediction of the value function of that network and the prediction that would be obtained from the observed reward plus the value at the next time step, again estimated by that network.",
            "Any questions about the first few?",
            "Distinctions, yeah, I don't understand eligibility mathematically."
        ],
        [
            "Interpretation.",
            "Which part of it's like maybe?",
            "What is supposed to do?",
            "What does it represent mathematically?",
            "It's just the value of the notion of eligibility.",
            "It's you can think of it as like a priority of how much you should update that state, and that priority decreases overtime as a factor of Lambda.",
            "So usually you get it, you update all of them according to some diminishing rule overtime."
        ],
        [
            "OK, a few more distinctions to come.",
            "On policy versus off policy, so I've."
        ],
        [
            "Already made the point that the distribution over the data changes based on the policy that you have and so essentially you know in the basic case we assume that we have a big enough batch that it covers all of the cases.",
            "So when we've collected our batch, we've actually made sure to cover all of the different policy that we need and now we can have accurate estimates of the value of an action in a state over the whole state space.",
            "In reality, that's often not going to be the case, and so there may be some need to correct for the distribution of the policy.",
            "One simple way to do that there's other ways to do it, but one simple way that's been doing that's been proposed is to actually use important sampling to reweigh the different probabilities.",
            "So in this case I distinguish between the policy that I'm actually trying to evaluate right now.",
            "Pie in my behavior policy, which is the policy under which my data was collected.",
            "There's some challenges in applying this in practice.",
            "When your factors become two different, you're important factors get very small, you end up having high variance in your estimation, so you have to be a little bit careful, but at least I want you to be aware of the fact that if your data is collected under a particular policy, and then you try to apply it in a different policy, you have to be concerned about the differences between the distribution somehow.",
            "Find ways to compensate for that and."
        ],
        [
            "This really arises in problems where you have very large amount of exploration, so if you have a very simple domain where you can afford to try all the actions and all the different states.",
            "Then it's not really a problem and you can afford to do full exploration and having your data set enough data to characterize all the different policies.",
            "But as your policy space gets very very large, this is what becomes a challenge."
        ],
        [
            "And so there's really this notion that you have to trade off between exploration and exploitation, whereas in exploration you're picking actions to gain more information, but that are not necessarily optimal.",
            "You can look at your function, your value function to see what's the best action to take, but you're not necessarily picking these best action in a way to further acquire information and exploitation mode.",
            "You sort of say, well, I'm going to use my value function.",
            "My current estimation to choose my actions and do it based on that.",
            "And so, assuming there's enough time this afternoon, I'll talk more.",
            "I'll go at more length on exploration versus exploitation, particular what we know how to do in terms of exploration, how to be more efficient in picking our actions to explore the domain when we can't afford to be totally exhaustive about."
        ],
        [
            "The last distinction is really one between your often here distinction between model based versus model free reinforcement learning, and I'm not going to go at length into the pros and cons of each of these, but really point out that the model based option is really the case where you take all of your data similar data data about your trajectories, and you use that data to estimate the probabilistic model in the reward model.",
            "And once you've estimated these models, you fix them and then you run essentially planning algorithms based on these estimated models.",
            "So this would be the model based approach and then the model free approach.",
            "You don't worry at all about learning the correct dynamic model of your environment.",
            "You worry directly about learning either a good value or good policy independent of the model, and these are very gross characterization.",
            "There's methods that makes a bit of both in various different ways."
        ],
        [
            "Maybe the last distinction is really about what we call value based methods and policy based method and Peter is going to talk about the policy based methods a little bit later and so today.",
            "Really what I'm talking about is the methods that use dynamic programming like approaches.",
            "And are trying to directly estimate the value function.",
            "But as we saw a little bit earlier, you can actually switch once you have a good value function, you can tease out a policy and once you have a good policy you can tease out a value function.",
            "The choice of what method to use, whether it's a value based method or a policy based method is really I think related to the characteristics of your domain.",
            "So in many cases if you have a lot more structure in your policy and it's easier to express.",
            "Prior information in the space of policy, then, probably the policy estimation methods might be better.",
            "If you have a very small set of actions, particular small set of discrete actions as we do in the Atari system for example, then maybe the value based methods are the way to go, so it's not a clear cut sense, and as I mentioned, a little bit earlier, there's these actor critic methods that actually makes features of both of them, and that may well be the way to go as we move forward, there's been a lot more interesting."
        ],
        [
            "Methods going forward.",
            "Any questions on the?",
            "RL lingo you see a little bit more clearly through that, yes?",
            "For a quality based rather than value.",
            "It really depends on the question is in the case of a conversational agent, it would be more policy based or value based, and I think it really depends on the structure that you want to assume for your conversation, and so if you're having a dialogue agent that can only select from a small set of specific speech acts as has been done in the literature for several years, then the value based methods have been reasonably successful as we're moving towards conversational agent that are going to generate natural language in the space of natural language is much bigger and we're really looking more, probably at architectures that are going to look like actor critic methods.",
            "That makes some element of value function to estimate the value of the different speech acts, but that allow us to have a much more fluid notion, a much richer notion of what's our policy.",
            "OK, so I'm assuming you have the lingo down for the most part and you know when to worry about certain things, and in particular once you have a problem in RL problem you want to tackle, how to start thinking of, whether the different dimensions along which you need to characterize your."
        ],
        [
            "Problem what I want to talk about in the last 15 minutes or 20 minutes is really how do we deal with very large state spaces.",
            "So we have our Q function and earlier on when I give you the equation for the Q function I wrote it out as the Bellman equation.",
            "Your immediate reward.",
            "You maximize your future expectation over your future state in the value of your future state and I'm going to discuss the case where your Q function can be learned directly approximated essentially as a rich set of functions.",
            "So here I have the linear version of it.",
            "Where theaters are my weights and flies or some kind of feature vector an we're going to assume at first that maybe these feature vectors are given designed by hand features of the board.",
            "In the case of TD Gammon, that was the case that there was sort of a 200 dimensional input vector that where they had selected.",
            "What were the right features to think about?",
            "And for many years computer goal was also done along these lines where they had some sense of what are the features you need to think about right?",
            "Local patches of black and white pieces.",
            "Encoded and they would have a vector that describes the board in that way."
        ],
        [
            "So the framework for incorporating.",
            "In this case, function approximation so large state space function approximation, But I'm going to do it from a batch case in.",
            "This is to really set it up as a supervised learning problem, so there's a family of approaches called fitted Q iteration, where as an input to your system you have the state in action information.",
            "As the output you have essentially your prediction of the Q function, so the output for one particular example, your data is essentially you take your trajectory and you cut it up into these experienced pieces and each experience piece consists of a state and action are award in the next state.",
            "So that's an experience.",
            "Peace and your batch is the collection of these experience pieces and for fitted Q iteration we're going to roughly assume that you've mixed them all up so they're not in the order of your trajectory, so they're roughly in dip.",
            "And if your batch is big enough so our input is your state in action and your output is essentially the estimate of your Q function, so your immediate reward and then the Max action over the next state and your loss is measured by the same type of loss that we were using when we were doing the TD updates.",
            "So I have my immediate reward plus my estimate of my future reward, and then I have my current estimate at S and think of now doing linear regression or using neural networks.",
            "There's a very nice paper by Damien Urns where he explores the use of random forests for doing this, and so what's really important to note here, I think.",
            "Is the fact that your estimate of the Q function actually appears twice?",
            "In your loss, right?",
            "Usually in supervised learning you wouldn't have this term over here.",
            "If you think of a problem that has a horizon of one, you just pick one action and you observe the reward.",
            "Then you see whether your Q function predicts that reward well, you're not going to have that term over here, just going to have our NQ.",
            "But now I have my Q function that appears twice, and the reason this is problematic is because in the beginning when your Q function is really poorly characterized, it means that your measure of loss is really poor and you can have instability in terms of your.",
            "Learning in particular, in your case where R is very, very sparse.",
            "So think of cases where you don't get any reward all the way till the end of your goal game.",
            "Like 200 moves later.",
            "Then all of this information in terms of trying to predict your loss is really just noise all the way till you get a reward signal and so that learning problem is the reason why for many years trying to use neural networks to do function approximation in RL was really problematic and it turned out there were a few people who seem to know the magic sauce.",
            "To make it work, but now we've I'll tell you a few of these magic ingredients.",
            "So here do you optimize only the last Q or you optimize your expected data appearing in the two places in computer grading?",
            "In this case, you just take the Q estimates directly.",
            "We don't.",
            "I'm not making the assumption yet that we're computing a gradient, but in some cases you optimize that loss.",
            "You should compute gradient with respect to all the appearances of data in.",
            "This is so if you're doing it as a neural network, you compute a gradient.",
            "If you're doing it using random forest, you don't write, you use a different criteria for minimizing the error, and so.",
            "But if you're doing neural networks yet, so you can compute your gradient with respect to the parameters of your network using that expression on both sides.",
            "Yes.",
            "Yep.",
            "A while ago by Rampar and collaborators have studies.",
            "What do you should optimize?",
            "Both hit us or only one of them, and I think in his paper declining swor it's better to just optimize one of them, not both.",
            "Discuss it a little bit because that was one of the key ingredients for a lot of the DQN work and so."
        ],
        [
            "So just to go back to the arcade learning environment, 'cause it was one of the most prominent cases of where they use function approximation.",
            "In that case, you're trying to play these Atari videos just to give you a sense of how they're doing it.",
            "They're really playing it with information at the pixel level, so they have the images frame by frame.",
            "The action set is relatively small, depending on the games.",
            "But they were trying to do that for many many different game."
        ],
        [
            "And so the question in this case was how do we pick out the features?",
            "And because they're going to do that over several games, they don't want to recreate the set of features manually for all of these."
        ],
        [
            "Games and the time was ripe for applying convolutional neural networks in this case, so their Q function is essentially the output of a convolutional neural Nets.",
            "In this case, I'm not going to go in the particular architecture, but you can look at the paper if you're interested.",
            "It's essentially trained with stochastic gradient descent, so they get a big batch of data with a lot of exploration, and then they train this to predict the Q function."
        ],
        [
            "We can look in this case at the training score, and so it's important to know that the training score is always measured once you fixed your Q function, and now you're just running it in test mode where you use that Q function to pick out the action.",
            "So usually compare the Q value for the different actions and you pick the one that has maximal Q value.",
            "So this isn't so convenient if you have continuous action space or a very large accent space because you need to operationalize that maximization over the choice of action, But in Atari it's not a problem because it's a relatively small action space and what you see is learning curves that look a little bit like that.",
            "Your score goes up in this case.",
            "We've changed the number of training episodes that are allowed."
        ],
        [
            "And if you look in the paper, you'll see a lot of results that look like this, where they're essentially listing all the different archery games that they've worked on, and they compare the performance of a human player versus the AI player and above this line are all the games where AI is better, and below are the games where the humans is better.",
            "And so there's some pretty lengthy list of results.",
            "What I want to emphasize in the few minutes we have."
        ],
        [
            "That is really the useful ideas that were introduced."
        ],
        [
            "Have stable learning in this type of architecture.",
            "Some of them had been a little bit announced in the literature before, but this work really crystallized the importance of them.",
            "The first one is probably the notion of experience replay, so it deals a little bit with your question of how do we get near batches of data?",
            "The notion of experience replays that you have a very very large batch, but every time every round you do your queue updates to actually sample a subset of that batch drawing randomly.",
            "Essentially decorrelating your samples 'cause your samples were obtained along trajectories, but you don't want to take a piece of the mini batch that all comes from one particular part of the trajectory you want to have our presentation over the whole space, and so you ran them.",
            "A mini batch of experience.",
            "Do your updates using that.",
            "There's been some more recent work that shows that you should actually do prioritized experience replay, meaning that you shouldn't sample uniformly over all of your big batch of data, but you should sample pieces.",
            "In proportion to different factors, they have a few different criteria.",
            "In the paper, one of them is suggesting that you should actually sample state switch, where you've observed a larger TD error.",
            "Because these are probably the ones where your network is not doing a good prediction, so you should emphasize the training under those pieces of the problem."
        ],
        [
            "So experience replay seems to be.",
            "Quite useful, a couple other useful tips for stability.",
            "One of them is gradient clipping.",
            "I'm assuming everyone is familiar with that.",
            "We've talked about it earlier this week.",
            "The other one is this notion of a periodic update, which was a little bit foreshadowed.",
            "In previous work, and the idea is really to once periodically really fix your network of Q values.",
            "I'll call this Q Theta minus an.",
            "We're going to use that network to calculate the error, so that's going to be the right side of my error equation and then all of my updates.",
            "In fact, I think in this case they're fixing both queues in to calculate the error.",
            "So you fix your queue for both the left and the right side.",
            "Correct me if I'm wrong, but I'm pretty sure that's what they do.",
            "They fix both of them and then they apply all of their Q updates to some other network which keeps on changing and once in awhile after a few iterations they take the updated network and then they start using that as a reference network.",
            "And that seems to really be useful for stabilizing of the learning everyone has been using, that including in some policy search methods.",
            "Another one of the last ones is this work on double DQN.",
            "Essentially, there's a nice work by Hayden has helped in some of this colleagues.",
            "Most of this work is done a deep mind these days, where they've shown that your Q values.",
            "Are actually often biased and their biased by the fact that in your estimate of the Q value there's a Max function and that Max is a Max over Q.",
            "So if there's some error in your estimation of the Q and you're taking Max over that overtime, you're going to introduce some bias as you converges that bias goes away.",
            "But in the early phases of learning, that can be quite problematic, and so in this particular case they use a correction where they use a different queue function.",
            "They essentially have two Q functions in parallel.",
            "Use one to select the action and a different one to calculate the error, and so that mitigates the bias.",
            "In this particular case."
        ],
        [
            "And if you look at the empirical results on the top over here, I'm showing actually the value estimates.",
            "With the Q ANAN with double DQN in blue.",
            "And so you see that the standard document actually overestimates the value quite a bit in some of the games.",
            "Not in all of the games, but there's some games where it really overestimates the value, and if you look at the score once you correct so in blue again the double DQ, and once you correct for that by you can achieve much better performance on some of these games, and these ones essentially converge to bad solution because their early estimates of the Q function were too biased.",
            "So that'll take you in class to say bias only on the opposite direction, intends to underestimate.",
            "But apparently for some reason you're picking the maximum.",
            "Estimating is not only the lessons it seems to yeah cause less problem and the last one."
        ],
        [
            "Apps I'll mention in terms of the tricks that seem to yield some advantage is this dueling Q network paper that was presented at ICML this summer.",
            "In this case appear I have essentially the standard EQ and architecture with the content structure and at the end you're just predicting the Q value in the dueling network.",
            "You separately predict not the Q function, right?",
            "Q is state action pair, but you predict the value function.",
            "So the best of the queues, right V is Max.",
            "Over Q, you predict the value function and then you predict the advantage function.",
            "I haven't defined the advantage function, but it's in the literature going back to the early 1990s.",
            "That notion of advantage function is really the difference between your value function and your Q function.",
            "So at the best action that advantage is going to be 0 an.",
            "Otherwise the advantage is going to be the gap between the Q value of that action in my value function.",
            "So in this network they separately predict VNA and then they compute cube I just.",
            "Adding up VNA.",
            "And there's an extra term there to constrain a little bit.",
            "The equation.",
            "I've taken it out just for clarity sake, but the lower level of the architecture are common and just at the last level, which I think is fully connected.",
            "They separately learn Viennet, and the notion is that V can actually be shared across all the actions, and so it's essentially like a stabilizing bias that goes into your estimation of your Q function, and that again tends to produce."
        ],
        [
            "Good results if you look at the graph.",
            "You know this is dueling Q networks versus simple DQN and this is the whole bunch of games where dueling does better.",
            "This is vanilla DQ and if you compare dueling versus the prioritized queue and so that has prioritized experience replay plus the double DQ and then there's a smaller number of games.",
            "But there's still some games where the dueling architecture seems to give quite a bit of a boost in this case."
        ],
        [
            "It is not all work in Atari.",
            "More recently people have started looking at doing rlin Minecraft.",
            "There's some nice work coming out of University of Michigan quickly.",
            "Satinder Singh and their students.",
            "Paper in this case what's interesting in Minecraft versus Atari that in Minecraft you have a restricted field of view, right?",
            "Your agencies, some local environment in attire you essentially see everything that you need to.",
            "So if you have the frame you have all of the information you don't have to deal with partial observability.",
            "In Minecraft, you do have to deal with partial observability and so they have played with various different architectures that include convents, but also including notions of memory and context and context can be thought a little bit akin to.",
            "Attention.",
            "In this particular case, then, of course, as you add memory in context and more links between all this, your architecture gets richer and richer and you get better results.",
            "So if you're interested in knowing a little bit more, there's some online videos.",
            "There's a spicy melt paper, but the notion of attention and memory that they're introducing are somewhat related to the things that were discussed earlier this week.",
            "So just to tell you that these ideas are permeating into the URL field as well."
        ],
        [
            "There's a lot of work and attention on these games.",
            "There's also people working on Mario and Star Craft and Doom.",
            "It seems to be quite rich, in particular 3D games with partial observe ability seems to be the next challenge for many things I happen to think that it's important to work on other kinds of PRL problem.",
            "There's a reason these domains, though, have really enabled us to make a lot of progress.",
            "One of them is that in all these cases, we have access to simulator, and as I mentioned earlier, you can share that simulator between research teams.",
            "So it makes it a lot easier in terms of research progress to be able to do that.",
            "Many of these domains are nearly deterministic, and so again, if you don't have a lot of stochasticity over the actions based probably easier to learn, and we also have a relatively small set of actions, so most of these.",
            "Our results that used EQN type of architectures tend to be with cases where you have a relatively small set of actions.",
            "Otherwise doing the maximization over the action can be problematic in terms if you do care about real world application."
        ],
        [
            "I think there's really promising results in not promising results.",
            "Yet.",
            "I should say very rich investigation domain on the side of conversation systems.",
            "We're doing some of that work in our lab.",
            "There's many other groups working on that, but in general I think around NLP type of tasks.",
            "There's a lot of interesting work to be done."
        ],
        [
            "I think."
        ],
        [
            "I'm going to wrap up over here and maybe we have a couple of minutes for questions before copy."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning everyone, so I think my task for today is to set you on a path to understanding a little bit more.",
                    "label": 0
                },
                {
                    "sent": "What is reinforcement learning and my personal objective will be to try to tease apart in what way reinforcement learning is different from supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming most of you are very familiar with supervised learning and it turns out there's some interesting subtleties about reinforcement learning which will explore and we'll talk also about.",
                    "label": 0
                },
                {
                    "sent": "Deep Q Network QN which have been much in the press in the recent year for success on solving various games from Atari to computer go and.",
                    "label": 0
                },
                {
                    "sent": "All goes well.",
                    "label": 0
                },
                {
                    "sent": "We'll do all that this morning and then in the next session Peter Beale will come in give you a sense of a different class of families for RL which deal more with searching over the policy space.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's get started.",
                    "label": 0
                },
                {
                    "sent": "I'm starting quite from the basic because I seem to get the feedback that a few of you were not familiar with RL at all, so bear with me if you know all of the basic.",
                    "label": 0
                },
                {
                    "sent": "Hopefully there will still be some interesting bits a little bit later on.",
                    "label": 0
                },
                {
                    "sent": "The main goal of reinforcement learning is to deal with agents that have to learn a sequence of actions.",
                    "label": 1
                },
                {
                    "sent": "So right away I really important element is that you can't assume that your choice of action is going to be independent.",
                    "label": 0
                },
                {
                    "sent": "At each time step, there's this notion that there's a sequence of decisions that need to be made, and there's some dependencies between these choices, and so usually we assume that information from the environment is captured in a notion of state, and that there's a notion of a reward, and so that's a feedback signal that tell us how well we're doing in terms of our choice of actions and overtime through exposure to examples, the agent can somehow statistically estimate the relationship between the choices of actions in certain states.",
                    "label": 0
                },
                {
                    "sent": "And how is that related to reward, and in particular the quantity we seek to optimize in this case is the sum of rewards over the lifetime of the agent or over the lifetime of the task of the agents.",
                    "label": 0
                },
                {
                    "sent": "So we write down the expectation over a particular policy.",
                    "label": 0
                },
                {
                    "sent": "Pi pies, the strategy that the agent is choosing the strategy the agent is using to choose these actions.",
                    "label": 0
                },
                {
                    "sent": "So we're taking the expectation over this policy, and we have an argmax argument over here.",
                    "label": 0
                },
                {
                    "sent": "That expression means we want to find the policy that maximizes that expression, so we're looking at the sum of rewards in that framework is somewhat inspired by some of the early work in psychology going back to Pavlof, who sort of laid out some of the conceptual ideas from that, but it was formalized mathematically in the late 50s by Bellman, and more recently by Rich Sutton and Barto in the late.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These there's a large spectrum of applications of reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "Most of the applications we're hearing about in recent years have to do with games solving games, but it turns out there's many other interesting problems in robotics, for example, where people have been using PRL to optimize the sequence of choices for the robotic agent as well as in optimizing treatment design, in particular for chronic diseases where it's not just a question of choosing treatment a versus treatment B, but there's really a notion that you have to change the treatment.",
                    "label": 0
                },
                {
                    "sent": "Overtime as the disease progresses.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for many years, sort of the biggest success of PRL was really this system called TD Gammon, which was developed by Jerry Tesoro in the mid 90s while he was at IBM in TD Gammon was quite interesting in that it was a example where NRL algorithm trained in this case by playing games against itself, actually learned how to reach a level of play that exceeded that of the best human players, and the reward function was very, very simple, right?",
                    "label": 0
                },
                {
                    "sent": "The agent would play the game, so pick a sequence of actions.",
                    "label": 0
                },
                {
                    "sent": "And at the end it would receive a feedback of either it won the game and then got a reward of plus 100 or lost the game and then it got a reward of minus 100.",
                    "label": 0
                },
                {
                    "sent": "But for all the intermediate time step, the agent received no feedback, no supervision about how well it was doing, and so that's a very very sparse signal in terms of our loss function.",
                    "label": 0
                },
                {
                    "sent": "If you compared to supervised learning every time you're presented with an example, you see an object, you predict that this is a dog, and if it's not a dog, you know right away.",
                    "label": 0
                },
                {
                    "sent": "In this case, the agent may take dozens of.",
                    "label": 0
                },
                {
                    "sent": "Actions before knowing whether it wins or loses.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't have direct feedback on which of these actions along the way was crucial to winning or losing, so it needs to learn that aspect directly from the data.",
                    "label": 0
                },
                {
                    "sent": "In this case, we'll talk a little bit more about the architecture later.",
                    "label": 0
                },
                {
                    "sent": "Once I've built up a little bit more of the formalism, but perhaps it's interesting to know that it was actually playing 1.5 million.",
                    "label": 0
                },
                {
                    "sent": "Sorry, so 10 to the six in the million are redundant here.",
                    "label": 0
                },
                {
                    "sent": "1.5 million games against itself.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More recently, we've had some will see if this video plays my computer in demo mode has been a little bit difficult about this results showing that we could actually use reinforcement learning to reach human level play in several Atari games.",
                    "label": 0
                },
                {
                    "sent": "So in this case there's an emulator for the old Atari console, and we actually train an agent to play these games, and in this case the little gun firing at the bottom that's controlled by the agent after being trained.",
                    "label": 0
                },
                {
                    "sent": "So what you're seeing is actually playing.",
                    "label": 0
                },
                {
                    "sent": "Quite well Anet can do that across several dozens of different games.",
                    "label": 0
                },
                {
                    "sent": "Now, right now the IRL agent for each of these games are trained separately and one of the challenges going forward, and they're starting to be a lot of work on.",
                    "label": 0
                },
                {
                    "sent": "This is how do we leverage with the agent has learned on one game to learn much faster on the other games, there's clear evidence that humans are doing that we don't yet have the right architectures for our machines to be doing this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And perhaps most famously is the result in the last year in March of this year of the Alphago system that was deployed by Deep Mind in a series of game against the leading Go player.",
                    "label": 0
                },
                {
                    "sent": "And we all know the outcome of that machine.",
                    "label": 0
                },
                {
                    "sent": "Did much better than the human player in this and again at the core of this was a set of algorithms based on deep learning an on reinforcement learning, the intersection of which is deeper L and has been really a very hot field in the last year.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to take a second to really argue for paying attention to IRL outside of solving computer games in our particular group, we've spent a number of years trying to figure out how we could optimize neurostimulation policies for reducing the incidence of epilepsy in people who have epileptic seizure.",
                    "label": 0
                },
                {
                    "sent": "So in this particular case, the sequence of decisions that we're making relates to how much stimulation and the timing of the simulation to deliver directly to the brain and the state that we are observing is real time.",
                    "label": 0
                },
                {
                    "sent": "EG information taken from the brain and so in that case we've been able to show in an animal model we don't yet do this on cute babies.",
                    "label": 0
                },
                {
                    "sent": "We have rats which can be kind of cute also.",
                    "label": 0
                },
                {
                    "sent": "Even in vitro model where we take a slice of the right brain and put it in a particular preparation, then we can plug in our recording electrodes.",
                    "label": 0
                },
                {
                    "sent": "We can plug in our simulation electrode, feed that back to the laptop and the algorithm learns an in the end deploys of strategy and we can show that we can actually reduce the incidence of seizures quite effectively by automatically learning from trial and error on this particular case.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to sum up, essentially, you know there's this question.",
                    "label": 0
                },
                {
                    "sent": "When should I be using RL versus some other form of machine learning?",
                    "label": 0
                },
                {
                    "sent": "And here are my few tips.",
                    "label": 0
                },
                {
                    "sent": "One, you know you have to be in a situation where your data comes in the form of a trajectory, not independence.",
                    "label": 1
                },
                {
                    "sent": "Examples so we are throwing out this IID assumption, which is so crucial to supervised learning.",
                    "label": 0
                },
                {
                    "sent": "The second thing is there has to be a notion that the decisions have a sequential aspect also.",
                    "label": 0
                },
                {
                    "sent": "So we're making a sequence of these choices of actions.",
                    "label": 1
                },
                {
                    "sent": "We have to be in a case where we have some feedback about this state.",
                    "label": 0
                },
                {
                    "sent": "It can be impartial, it can be noisy, it can be delayed all the way at the end like it was in TD Gammon, or it can be at every step of the way, where every time your agent does something, there's someone that says good robot, Bad Robot.",
                    "label": 0
                },
                {
                    "sent": "All of these frameworks are allowed, and then perhaps a little bit more subtle.",
                    "label": 0
                },
                {
                    "sent": "You have to have a sense that there's going to be a gain in terms of learning when you're optimizing that action choice over maybe a portion of the trajectory, there has to be a notion of generalizability.",
                    "label": 1
                },
                {
                    "sent": "Otherwise you can just treat the whole trajectory itself as a supervised learning example, and so if you think you can share some information across pieces of the trajectory, then there all framework provides some useful mathematical formalism.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For handling that.",
                    "label": 0
                },
                {
                    "sent": "And so in terms of contrast in reinforcement learning versus supervised learning, whereas in supervised learning you have some input, some outputs and then your training signal is sun comes in the form of some loss function that's expressed after each example.",
                    "label": 1
                },
                {
                    "sent": "In RL we still have inputs or outputs.",
                    "label": 1
                },
                {
                    "sent": "We usually call them actions and our training system or training signal.",
                    "label": 0
                },
                {
                    "sent": "We usually call a reward, but there's a notion that there's a dependency between the inputs that is fed through the environment.",
                    "label": 0
                },
                {
                    "sent": "Perhaps it's useful to clarify that I'm going to assume and most of the RL literature assumes, that time is a discrete quantity, so there's kind of a Clock ticking and every tick of the Clock.",
                    "label": 0
                },
                {
                    "sent": "This state changes in an action is taken in a reward signal is observed.",
                    "label": 0
                },
                {
                    "sent": "So in this case at each click of the Clock after an action, the environment changes the state and then the agent gets to see some version of that, maybe a full observation of this, maybe a partial observation of this thing.",
                    "label": 0
                },
                {
                    "sent": "Of training signal is expressed and some of the challenges that we experience in this framework are one that we essentially have to solve jointly.",
                    "label": 0
                },
                {
                    "sent": "The problems of learning and planning.",
                    "label": 0
                },
                {
                    "sent": "When you're dealing with classical planning systems, you assume that the relation between your choice of actions and the effect on your States and your goals are explicitly specified by some language.",
                    "label": 0
                },
                {
                    "sent": "Often some kind of symbolic language.",
                    "label": 0
                },
                {
                    "sent": "In this case, we don't know that relationship, so we have to learn the effects of our actions.",
                    "label": 0
                },
                {
                    "sent": "We have to learn when reward can be gained, but we also have to do planning in the sense of picking a sequence of actions.",
                    "label": 0
                },
                {
                    "sent": "Another important point is that the distribution of your data changes with your choice of actions.",
                    "label": 0
                },
                {
                    "sent": "So if I have a robot that always is flying robot that always chooses to go up, then the information I get about the world is going to be of a certain kind.",
                    "label": 0
                },
                {
                    "sent": "If I have a robot that always chooses to go down the information I get about the world, the rewards and my states are going to be different, and so there's an effect between that which we have to take into consideration when we do our learning.",
                    "label": 0
                },
                {
                    "sent": "We don't assume we have.",
                    "label": 0
                },
                {
                    "sent": "Identically distributed data.",
                    "label": 0
                },
                {
                    "sent": "The distribution over the data changes as a function of the policy and finally, and perhaps this is the point that has most limited the progress of our L compared to other fields of machine learning is the fact that you need access to an environment.",
                    "label": 0
                },
                {
                    "sent": "Most of the work you can't do from just a static batch of data and will discuss this in more detail as we go forward, but you essentially need to be able to apply these actions and observe the effect of these.",
                    "label": 0
                },
                {
                    "sent": "Actions in terms of the next state in terms of the reward.",
                    "label": 0
                },
                {
                    "sent": "Without this, if you work from just a batch of data, you risk through a sequence of action that hasn't been explored in your batch of data.",
                    "label": 0
                },
                {
                    "sent": "Remember, point #2 your data distribution changes if you try a action strategy that takes you this way when all of your data was collected from over here you are going outside of the range of your data set and you really can't do anything, so we need access to the environment to be able to try out your policy's both for learning and for testing purposes.",
                    "label": 0
                },
                {
                    "sent": "And so for many years it's a lot harder between research groups to be exchanging environments as opposed to exchanging datasets.",
                    "label": 0
                },
                {
                    "sent": "I have a robot in my lab, I can't distribute it to labs everywhere so that they can replicate my experiment, and so this is really been very nice.",
                    "label": 0
                },
                {
                    "sent": "In particular in terms of the Atari simulator.",
                    "label": 0
                },
                {
                    "sent": "This is finally an environment that can be simulated that we can be transferred quite compactly, so that has really helped the progress of the field.",
                    "label": 0
                },
                {
                    "sent": "I think in the last couple of years.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formally speaking, reinforcement learning is cast as a Markov decision process.",
                    "label": 1
                },
                {
                    "sent": "We have our set of states or set of actions that probabilistic effects of those state and actions are system, and as a state takes an action it goes to next state.",
                    "label": 1
                },
                {
                    "sent": "We assume that described by a probability distribution, we have a reward function.",
                    "label": 0
                },
                {
                    "sent": "We've talked about.",
                    "label": 0
                },
                {
                    "sent": "This is usually a real number that reward function as you see by this graphical model can be a function.",
                    "label": 0
                },
                {
                    "sent": "Both of my current state, my previous state, my previous action, all of these things can factor in.",
                    "label": 0
                },
                {
                    "sent": "And usually I'm not going to talk about it much, but we assume we have some initial state distribution.",
                    "label": 0
                },
                {
                    "sent": "We're going to condition our learning on this initial state distribution.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A key property which were making use of in this case is the Markov property.",
                    "label": 1
                },
                {
                    "sent": "I'm assuming many of you have seen it before, but in case this is falling through the cracks of your learning, the Markov property essentially states that any information you need to predict the future is contained in the current state, so there's no advantage when you're trying to predict the state at time T to condition on the whole history of previous states we visited the state team at T -- 1 right before contains all the necessary information, and I could add.",
                    "label": 0
                },
                {
                    "sent": "Actions in that in terms of conditioning, typically for MVP on the right hand of the equation, we condition on the previous state in action and on the left hand you know we could condition on the whole history of state and actions and so.",
                    "label": 0
                },
                {
                    "sent": "But this is a simple Markov property.",
                    "label": 0
                },
                {
                    "sent": "Have you seen it for Markov chains and Markov models?",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I mentioned earlier that in terms of performance, the metric we're using is really the notion of optimizing the utility of a trajectory over a set of states.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to define U of T to be the utility of my trajectory starting from state T, and I'm going to talk about two types of cases.",
                    "label": 1
                },
                {
                    "sent": "One of them is what I'm going to call episodic task, not just me.",
                    "label": 0
                },
                {
                    "sent": "The literature called that episodic tasks.",
                    "label": 0
                },
                {
                    "sent": "So in an episodic task we assume that the task has a finite time horizon.",
                    "label": 0
                },
                {
                    "sent": "So there's a set of time steps.",
                    "label": 0
                },
                {
                    "sent": "I'll call it capital T and what we're interested is maximizing utility over the sum of these rewards from time step one to time step capital T and the utility can also be defined at intermediate time points through the trajectory.",
                    "label": 0
                },
                {
                    "sent": "So U of T little T means that it's the utility from some time point in the middle of the trajectory until the end.",
                    "label": 0
                },
                {
                    "sent": "So this formalism for episodic tasks can be used, for example for games and agent that goes through a maze.",
                    "label": 1
                },
                {
                    "sent": "Anything where we know there's a finite length to the trajectory.",
                    "label": 0
                },
                {
                    "sent": "In contrast to that, we have what we call continuing tasks, and in continuing task we assume that that task can actually go on forever, and so we don't necessarily have a known and time point.",
                    "label": 0
                },
                {
                    "sent": "That utility may some.",
                    "label": 0
                },
                {
                    "sent": "Until a sequence of rewards till the end of time and you'll notice a new symbol has appeared.",
                    "label": 0
                },
                {
                    "sent": "Now I'm introducing gamma and gamma is what we call a discount factor in the role of gamma.",
                    "label": 0
                },
                {
                    "sent": "You'll see is mathematically speaking and there's other roles too.",
                    "label": 0
                },
                {
                    "sent": "But mathematically, if I want to do this infinite sum, it's not going to go so well.",
                    "label": 0
                },
                {
                    "sent": "We're not going to have a bounded value unless we make sure that there are some conditions in place that this sum is bounded, and so gamma is usually a number less than one between zero and one, but at least slightly less than one.",
                    "label": 0
                },
                {
                    "sent": "Such that this particular sum has a finite interpretation, and So what we do with the discount.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Factor essentially is to make sure that information later on, sorry Rewards obtained later on in the trajectory have essentially less important because gamma is less than one, have less important in the utility than the rewards acquired in the early time steps, and there's sort of two different interpretations why one reason we put it in there is because mathematically is very convenient.",
                    "label": 0
                },
                {
                    "sent": "It gives us in finite sum in terms of problem domain there's a couple of interpretations, one of them is you can actually re express your problem such that there's a probability of dying at.",
                    "label": 0
                },
                {
                    "sent": "Each time step, and this is going to be equivalent to having this particular gamma factor in the other way, to think about it is more like in terms of inflation, right?",
                    "label": 0
                },
                {
                    "sent": "If I offer you $100 today or I say, well, come back in a year and I'll give you $100 between these two things you have related preferences and depending on the ratio of these preferences, this is what gamma represents in this case.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've talked about the fact that we are aiming to pick actions according to strategy, so in terms of introducing a little bit of notation, we usually use \u03c0 to denote the strategy.",
                    "label": 0
                },
                {
                    "sent": "In this case, I'm expressing a stochastic strategy, so my policy is going to be conditioned on the state and I'm going to have a probability of picking every action in every state, but I can also have a deterministic policy, and in that case the policy is a straight mapping from state to action.",
                    "label": 0
                },
                {
                    "sent": "The policy is what I'm trying to learn.",
                    "label": 0
                },
                {
                    "sent": "I don't know this from the unset.",
                    "label": 0
                },
                {
                    "sent": "And again I want to find the policy pie that maximizes my expected total reward.",
                    "label": 1
                },
                {
                    "sent": "Note here that there are many policies.",
                    "label": 1
                },
                {
                    "sent": "The number of policy is influenced by a couple factors.",
                    "label": 0
                },
                {
                    "sent": "One of them is the number of actions you have right at every state.",
                    "label": 0
                },
                {
                    "sent": "You need to consider all of these actions.",
                    "label": 0
                },
                {
                    "sent": "Another way to think about is the number of policies you have also depends on the length of your trajectory.",
                    "label": 0
                },
                {
                    "sent": "And so, in general, there's many, many policies that we may need.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Need to consider so this is like a simple case of an MDP.",
                    "label": 0
                },
                {
                    "sent": "This is an alternate representation from the graphical model that I gave you, and sometimes you'll see em DPS represent in this particular notion, right?",
                    "label": 0
                },
                {
                    "sent": "Some of you may be in any one of these states.",
                    "label": 0
                },
                {
                    "sent": "Maybe some of you are unemployed.",
                    "label": 0
                },
                {
                    "sent": "I won't ask you to raise your hands, some of you are coming from industry, some of you are in grad school.",
                    "label": 1
                },
                {
                    "sent": "A few of us are in academia, and in each of these states depending on what choice of actions you do, you may have an arrow.",
                    "label": 0
                },
                {
                    "sent": "That indicates the transition probability.",
                    "label": 0
                },
                {
                    "sent": "There's one particular crucial error that's missing in this graph.",
                    "label": 0
                },
                {
                    "sent": "It hasn't been updated recently.",
                    "label": 0
                },
                {
                    "sent": "There seems to be a high trend from academia to industry that is lacking on this particular graph.",
                    "label": 0
                },
                {
                    "sent": "I am aiming for a world where the decision is to stay in academia for a good while, and so this is the graph I proposed today, but you can choose to learn a different graph, so in each of these cases, right for each if you wanted to have a full specification of your environment.",
                    "label": 0
                },
                {
                    "sent": "And when I talk about exchanging environments between lab, this is the type of specification that I'm talking about.",
                    "label": 1
                },
                {
                    "sent": "You need to have a defined probability distribution out of every state that defines the probabilities of going to all of the other States and you also need to have a reward.",
                    "label": 0
                },
                {
                    "sent": "In this particular case, the reward is expressed just as a function of state.",
                    "label": 0
                },
                {
                    "sent": "Presumably there's like a + 10 reward.",
                    "label": 0
                },
                {
                    "sent": "I'm not giving you the units here plus 10 reward for going to industry versus A plus.",
                    "label": 1
                },
                {
                    "sent": "One reward for going to academia.",
                    "label": 1
                },
                {
                    "sent": "In some cases, the reward may be just the function of state, but it also could be a function of state and action depending on what your domain is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next little piece of notation I want to introduce the notion of a value function.",
                    "label": 0
                },
                {
                    "sent": "So if we want to estimate a policy, we need to have a sense of if policy A versus policy B.",
                    "label": 1
                },
                {
                    "sent": "How are we going to evaluate them?",
                    "label": 0
                },
                {
                    "sent": "I've already introduced the notion of utility, right utilities?",
                    "label": 0
                },
                {
                    "sent": "The actual observation of the sequence of reward that you have.",
                    "label": 0
                },
                {
                    "sent": "The value function is the expectation of that sequence of reward over the policy Pi.",
                    "label": 0
                },
                {
                    "sent": "So if you contrast utility utility would be just the sum of RT.",
                    "label": 0
                },
                {
                    "sent": "Our little tee up to our big T whereas the value function is the expectation of that sum for particular policy, so I'm talking here.",
                    "label": 0
                },
                {
                    "sent": "The value of a policy and that value is defined for all of the states in your system.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we look a little bit more closely at the value of a policy, if we top, I just rewrote the expression that I had a little bit earlier.",
                    "label": 1
                },
                {
                    "sent": "I have my sum of rewards that I'm taking you through a little bit of notation because there's some very nice structure in our value function that will be useful for building up the algorithmic knowledge later on.",
                    "label": 0
                },
                {
                    "sent": "So in this case, at the top row I have the same expression I had on the last one, right?",
                    "label": 0
                },
                {
                    "sent": "The expectation over my policy of my sum of reward given that my state at time T is a particular state S. I'm defining it here for the case of episodic task finite horizon that go up to time T, but I'll generalize it to the continuous task a little bit later on in the second step, all we're doing is separating the expectation of the reward at the first time step versus the expectation of the reward for the rest of the trajectory and the expectation of the reward at the first time step can be.",
                    "label": 0
                },
                {
                    "sent": "Gained by just taking expectation over the policy for that immediate reward.",
                    "label": 0
                },
                {
                    "sent": "So that's the first part of the equation, and then in the second part of the equation what you have to notice is it's essentially the same as the first line.",
                    "label": 0
                },
                {
                    "sent": "What I'm calling the future expected sum of rewards essentially the same, but I've removed the first term, but I'm still conditioning on times T.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to take the expectation of going from St to S T + 1 to take that expectation.",
                    "label": 0
                },
                {
                    "sent": "I factor in my policy.",
                    "label": 0
                },
                {
                    "sent": "I factor in my transition probability and now the term that you have at the end on the right side looks a whole lot like the term you have at the top.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we have a recurrence notion here, where you can actually define the value at time at state S as a function of these expectations, and the value at times S + 1.",
                    "label": 0
                },
                {
                    "sent": "This is a case of a dynamic programming algorithm, right?",
                    "label": 1
                },
                {
                    "sent": "My values at state S are a function of my values at my other States and I take the expectation in terms of those other states.",
                    "label": 0
                },
                {
                    "sent": "I can transition to.",
                    "label": 0
                },
                {
                    "sent": "You have a question.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, that should be plus one, sorry, yes, thank you.",
                    "label": 0
                },
                {
                    "sent": "The first term would be expected value be conditioned on S as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you should condition on this for that, yeah thanks.",
                    "label": 0
                },
                {
                    "sent": "And that goes away when you once you take that expectation, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I have the expectation of a policy and I've rewritten it.",
                    "label": 1
                },
                {
                    "sent": "In this case, I've taken introduced the Gamma discount factor, so the discount factor is over your future and so this is the same form of the equation separating the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "The future expected reward.",
                    "label": 0
                },
                {
                    "sent": "But we have the discounting that shows up before the future expected reward only need to discount one because the other discounts the Gamma Square and so on are folded into the V at S prime.",
                    "label": 0
                },
                {
                    "sent": "In this particular case, sometimes my value function is expressed as a function of States and sometimes we express it as a function of state and actions.",
                    "label": 0
                },
                {
                    "sent": "And in that case I just take away the expectation over the policy and I instantiate it for a particular action in the first time step.",
                    "label": 1
                },
                {
                    "sent": "So this is what we call Bellman's equation.",
                    "label": 0
                },
                {
                    "sent": "Bellman's equation comes in slightly different forms and flavors, but it also always has this Canonical form of being expressed as a function of immediate reward and future rewards in the future.",
                    "label": 0
                },
                {
                    "sent": "Rewards are expressed using this recursive form.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so what's interesting to notice now is if I express the value of my policy for all of my states.",
                    "label": 1
                },
                {
                    "sent": "I essentially have a system of equations, and when I fix the policy.",
                    "label": 0
                },
                {
                    "sent": "I have a system of linear equations and I have N variables and equations, where N is my number of states, so I can actually write it as a system of equation.",
                    "label": 1
                },
                {
                    "sent": "I can solve it in close form if my number of state is relatively small and if I have some conditions in terms of the inverse being well behaved.",
                    "label": 0
                },
                {
                    "sent": "This isn't usually how we solve it, because most of the time the problems we're interested in have too many states for doing that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we usually do is set up an iterative process, take advantage of the dynamic programming form of it, and set it up as an iterative procedure.",
                    "label": 0
                },
                {
                    "sent": "So in that case I start with some initial guess of what my value might be at all of the states.",
                    "label": 1
                },
                {
                    "sent": "It turns out that these iteration algorithms are a lot less sensitive to the initial value than what you might be used to for training neural network, and for reasons that will explain a little bit later, we can just set everything to 0, or you can just set it to the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "So once you have your initial guess, then you iterate your Bellman equation so you update your value function and so as the value function that state S + 1 gets updated.",
                    "label": 0
                },
                {
                    "sent": "So in the next round the value function at state S gets updated again.",
                    "label": 0
                },
                {
                    "sent": "You repeat that until some small changes is detected.",
                    "label": 0
                },
                {
                    "sent": "Usually we look at the maximum change over all of the states for the changes in the value function.",
                    "label": 0
                },
                {
                    "sent": "What's below some threshold?",
                    "label": 0
                },
                {
                    "sent": "We stopped doing the iteration.",
                    "label": 0
                },
                {
                    "sent": "So I want you to note this is just an algorithm for policy evaluation.",
                    "label": 0
                },
                {
                    "sent": "I haven't yet told you how to pick one policy versus another.",
                    "label": 0
                },
                {
                    "sent": "All I've told you so far is if I fix the policy.",
                    "label": 1
                },
                {
                    "sent": "This is how you're going to get the estimation of the value function for that policy.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can actually show that this particular procedure is going to converge.",
                    "label": 0
                },
                {
                    "sent": "I'm doing this in the case where I have all of my states being a discrete set, so our convergence proof I'm not going to go through all the details of it, but it's relatively simple.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the important point is that we can show that the difference between the convergence points, the fixed point of my value function, and the one of the case iteration.",
                    "label": 0
                },
                {
                    "sent": "As I'm doing these iterates actually depends on the.",
                    "label": 0
                },
                {
                    "sent": "Same quantity.",
                    "label": 0
                },
                {
                    "sent": "Times my factor gamma, but because gamma is less than one, I actually have a contraction on that property so that difference between where my value function is and the optimal value function gets smaller at every point due to this discount factor.",
                    "label": 0
                },
                {
                    "sent": "So eventually it's going to go to zero.",
                    "label": 0
                },
                {
                    "sent": "You are going to be able.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To learn your value function, and so there's actually a nice correspondence between the value function and the policy.",
                    "label": 0
                },
                {
                    "sent": "In particular, we use the star to denote the optimal value function.",
                    "label": 1
                },
                {
                    "sent": "This is the one that has the maximum possible value.",
                    "label": 0
                },
                {
                    "sent": "That the star is obtained by finding the policy that maximizes the value function.",
                    "label": 0
                },
                {
                    "sent": "So I've given you couple ways to find that value function to estimate that V Pi V star is the best policy, the maximization point of that policy.",
                    "label": 0
                },
                {
                    "sent": "Other than one and necessary condition?",
                    "label": 0
                },
                {
                    "sent": "Or could you put some other conditions like contraction of the transition matrix to get the result?",
                    "label": 0
                },
                {
                    "sent": "You can certainly put some other condition.",
                    "label": 0
                },
                {
                    "sent": "An easy one is finite horizon.",
                    "label": 0
                },
                {
                    "sent": "As long as you have a finite horizon you have a finite sum, and so.",
                    "label": 0
                },
                {
                    "sent": "But Gamma is certainly the most convenient one and.",
                    "label": 0
                },
                {
                    "sent": "And so what's interesting to note is any policy that's going to achieve that value function.",
                    "label": 1
                },
                {
                    "sent": "We're going to call it an optimal policy.",
                    "label": 0
                },
                {
                    "sent": "We're going to write that Pi star, so V stars optimal value function.",
                    "label": 1
                },
                {
                    "sent": "Pi stars are optimal policy, and usually the value function is unique.",
                    "label": 0
                },
                {
                    "sent": "There's going to be a unique value function, but the policy doesn't need to be unique.",
                    "label": 0
                },
                {
                    "sent": "It's possible in a state there's two different action choices that both yield maximum value.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other thing that's interesting to note is if somehow you know the optimal value function so you know these are and.",
                    "label": 1
                },
                {
                    "sent": "I'm going to assume for now, you also know the model, the rewards, the transition, your discount factor.",
                    "label": 0
                },
                {
                    "sent": "If you know we start, then we can obtain the optimal policy very fast.",
                    "label": 1
                },
                {
                    "sent": "Right, it's basically one round of Bellman equation, so the complexity of that is linear in your number of actions.",
                    "label": 0
                },
                {
                    "sent": "It's quadratic in your number of states.",
                    "label": 0
                },
                {
                    "sent": "If you know the value function, we get, the policy will call that essentially for free.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if you know the policy, if someone hands you an optimal policy, then you can actually extract the value function very easily again with a single application of Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "And I've written down here the case both for the stochastic policy and below the case for the deterministic policy you essentially sub in the policy for your action choice and you get your value function by 1.",
                    "label": 0
                },
                {
                    "sent": "Round again of Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "And so this is important because today when you see reinforcement learning in the morning, I'm going to talk mostly about value function methods.",
                    "label": 1
                },
                {
                    "sent": "So these are methods that Salvar El by trying to estimate the value function.",
                    "label": 0
                },
                {
                    "sent": "But the first part of this tells you if I solve for the value function, then I can compute the policy very easily.",
                    "label": 0
                },
                {
                    "sent": "In the next session, Peter Bill is going to tell you about policy search method policy optimization method so that class of method actually works, spends all the hard work finding the best policy, but once he's found the best policy, he can actually get the value function very easily.",
                    "label": 0
                },
                {
                    "sent": "Also, and we will also discuss.",
                    "label": 0
                },
                {
                    "sent": "I think Peter is going to discuss this.",
                    "label": 0
                },
                {
                    "sent": "What we call actor critic methods and they work by simultaneously optimizing the value function and the policy and using essentially one to regularize the learning of the other.",
                    "label": 0
                },
                {
                    "sent": "But for today, for the first session this morning, I'm mostly talking about.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Value methods so I've given you an iterative algorithm to estimate the value of a given policy.",
                    "label": 0
                },
                {
                    "sent": "Now I'm giving you a framework slightly under specified for finding the best possible policy, and that is the following.",
                    "label": 0
                },
                {
                    "sent": "Let's say I'm going to start with an initial policy.",
                    "label": 1
                },
                {
                    "sent": "Pick a random guess, randomly choose what action to do in every state.",
                    "label": 0
                },
                {
                    "sent": "Will call back by zero, and iteratively.",
                    "label": 0
                },
                {
                    "sent": "We're going to compute V of PIE for that policy right.",
                    "label": 0
                },
                {
                    "sent": "Previous slide I showed you how to do that.",
                    "label": 0
                },
                {
                    "sent": "This is easy, so I compute the value for that policy.",
                    "label": 0
                },
                {
                    "sent": "I'm going to compute a new policy that's greedy with respect to V. Piso.",
                    "label": 1
                },
                {
                    "sent": "New policy that improves a little bit better and how we do that.",
                    "label": 0
                },
                {
                    "sent": "All the interesting pieces are in that line.",
                    "label": 0
                },
                {
                    "sent": "That's really one of the things Peter is going to talk about.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to say a lot more, but I want you to have in mind the generic framework for what we call policy iteration.",
                    "label": 0
                },
                {
                    "sent": "In the case where we have a discrete set of States and action, this notion of computing a new policy boils down to just a single round of the Bellman equation again, and we're going to iterate.",
                    "label": 0
                },
                {
                    "sent": "Until the policy doesn't change.",
                    "label": 0
                },
                {
                    "sent": "If in two rounds my policy hasn't changed, well, if my policy doesn't change, my value is not going to change either, right?",
                    "label": 0
                },
                {
                    "sent": "This is a domestic computation, and so we can stop when the policy doesn't change.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In contrast, the value iteration method works on doing similar iterations in this case on the value function.",
                    "label": 1
                },
                {
                    "sent": "So in this case I start with a thumb arbitrary choice of my value function.",
                    "label": 0
                },
                {
                    "sent": "And at every round I'm going to pick as my value.",
                    "label": 1
                },
                {
                    "sent": "The bellman equation for the case of the best possible action.",
                    "label": 0
                },
                {
                    "sent": "So if you compare this algorithm to the one I presented for policy evaluation before the main differences, I've introduced a maximization over the choice of action in my Belmond iteration, and so in this one, at every round I get a better value function.",
                    "label": 0
                },
                {
                    "sent": "But I also allow the policy to change, and in this case I stop when my change in value function is below some threshold, we can show that under some condition this algorithm is going to converge to the true.",
                    "label": 1
                },
                {
                    "sent": "Optimal value function.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to take a pause and see if there's any quick questions before we move on.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It's simply better unlikely.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let me say not very much.",
                    "label": 0
                },
                {
                    "sent": "Now in a little bit later on that, but yes, I'm assuming I know T or alternate.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm assuming from the way I've described the value iterate.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm an algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "I have my discount factor in there and so I'm assuming I have a continuing task and I don't worry about what's T and my termination condition depends on my convergence on my value function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me run through a really quick example.",
                    "label": 0
                },
                {
                    "sent": "For those of you who haven't seen value iteration, this is really very simple.",
                    "label": 0
                },
                {
                    "sent": "We have a robot moving around in a grid world.",
                    "label": 0
                },
                {
                    "sent": "It can't go in the blue box and it can either end in the minus 10 state.",
                    "label": 0
                },
                {
                    "sent": "Let's call that you know on the pit or it can get plus one in a particular goal, and there's some essentially slight noise in the transition model.",
                    "label": 0
                },
                {
                    "sent": "So if it aims to go from the lower left corner an it takes an action going East, it ends up in the square beside it.",
                    "label": 0
                },
                {
                    "sent": "With .7 probability with .1 probability ends up somewhere a little bit off from that, and I'm going to assume a discount effect.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "0.9.",
                    "label": 0
                },
                {
                    "sent": "So this is my value iteration at the very first round, right?",
                    "label": 1
                },
                {
                    "sent": "My value function?",
                    "label": 0
                },
                {
                    "sent": "My Bellman equation says your value is the immediate reward plus.",
                    "label": 0
                },
                {
                    "sent": "Some transition probability in the value of the next state.",
                    "label": 0
                },
                {
                    "sent": "Well, all my states were initialized at zero and so in my first round when I get in my value function is essentially just the reward function.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second round that information gets propagated to all the neighboring state.",
                    "label": 0
                },
                {
                    "sent": "When I update the value function at those states, I get to observe the immediate reward, which is 0 plus some reward from the adjacent safe date.",
                    "label": 0
                },
                {
                    "sent": "And I can also measure what we call the Bellman residual, which is the difference.",
                    "label": 1
                },
                {
                    "sent": "The maximum difference in value over all of the states.",
                    "label": 0
                },
                {
                    "sent": "So in this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes I have a bellman residual that .9 and if I do that over a few more iterations here, I'm at iteration #5.",
                    "label": 1
                },
                {
                    "sent": "We see essentially the value information diffusing through the graph.",
                    "label": 0
                },
                {
                    "sent": "In terms of the structure of the state space, my Bellman residual.",
                    "label": 0
                },
                {
                    "sent": "If all goes well should reduce at every time step something to check if you're implementing value.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Iteration after about 20 iterations, things have gotten pretty stable.",
                    "label": 0
                },
                {
                    "sent": "My bellman residual is quite small.",
                    "label": 1
                },
                {
                    "sent": "I have an estimate of the value everywhere.",
                    "label": 0
                },
                {
                    "sent": "I don't have a policy.",
                    "label": 0
                },
                {
                    "sent": "How do I get a policy?",
                    "label": 0
                },
                {
                    "sent": "Not so hard, right?",
                    "label": 0
                },
                {
                    "sent": "One version of Bellman equation I look in every state.",
                    "label": 0
                },
                {
                    "sent": "Should I go up or should I go left or should I go route because there's symmetry in my noise in my transition model.",
                    "label": 0
                },
                {
                    "sent": "I can essentially always go towards the adjacent square that has higher value.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And look at that in slightly larger cases, right?",
                    "label": 0
                },
                {
                    "sent": "This is an example.",
                    "label": 0
                },
                {
                    "sent": "I have four different rooms, each of them have been broken into 16 states or so, and I start in the first case in the first iteration, where my goal is and after a couple iterations, that information propagates diffuses again across the graph.",
                    "label": 0
                },
                {
                    "sent": "Based on the structure of the framework.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Perhaps it's useful to think about the fact that in this case I'm assuming that at every round I'm doing a full estimation of the value function over all of the states at each of my iteration.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's not so useful, in particular if you look at any.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example like that where the value hasn't changed anywhere else in the outer states.",
                    "label": 0
                },
                {
                    "sent": "Doing a value update in all of these other states probably isn't a very good use of your computation.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's version of value iteration called call it a synchronous value iteration, where essentially you're doing your updates in according to Priority queue in that priority queue depends on how you visited the States before, and so we can adjust how we do this to be a little bit more efficient in some cases.",
                    "label": 0
                },
                {
                    "sent": "Particular focusing on the states that are actually possible under some trajectory rather than trying to do that for all of the states at the same time.",
                    "label": 1
                },
                {
                    "sent": "And if you think of a system like Alphago, I'm caricaturing a little bit here, but they had a lot of trajectories from expert games of go between two human experts and those games essentially define trajectory through the state space, and those spend quite a bit of time characterizing and learning those strategies in a way to control the distribution of states that you expect to see in a game.",
                    "label": 0
                },
                {
                    "sent": "And focus your learning around states along that distribution rather than try to learn uniformly for all the states which isn't possible in a game like go, where the number of states is astronomical.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is kind of wraps up my basic tutorial level on RL.",
                    "label": 0
                },
                {
                    "sent": "If you want to know more.",
                    "label": 1
                },
                {
                    "sent": "If this is giving you just enough of a taste, there's two books that I recommend.",
                    "label": 0
                },
                {
                    "sent": "One of them is the basic Sutton and Barto which most of you are probably familiar with.",
                    "label": 0
                },
                {
                    "sent": "Its available freely on Rich Sutton website.",
                    "label": 0
                },
                {
                    "sent": "There's version edition number two with slightly different notation that is also available.",
                    "label": 0
                },
                {
                    "sent": "That is a little bit more work in progress, but sufficiently mature that you can probably use it alternately, as chabas party has put out a reinforcement learning textbook that I quite like also.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more terse, a little bit more mathematical, but you might want to have a look at that one.",
                    "label": 0
                },
                {
                    "sent": "Also available freely on Shabbos website.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me focus a little bit more on.",
                    "label": 0
                },
                {
                    "sent": "Some more challenging problems that arrive.",
                    "label": 0
                },
                {
                    "sent": "I would say there is essentially different classes of challenges for reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "One of them is figuring out how we're going to design our problem domain, so we have the problem of state representation.",
                    "label": 1
                },
                {
                    "sent": "How do we represent our information and that one shares a lot of similarity with other supervised learning problem and I will get I promise to deep reinforcement learning where we throw in a neural net in this framework.",
                    "label": 0
                },
                {
                    "sent": "In this will be where it's most useful in a sense.",
                    "label": 0
                },
                {
                    "sent": "How do we represent the state information for a system?",
                    "label": 0
                },
                {
                    "sent": "We also often need to choose our actions, but for today I'm going to assume that the choice of action is given by the description of the problem.",
                    "label": 0
                },
                {
                    "sent": "Picking the right reward function can be more difficult, often in problems.",
                    "label": 0
                },
                {
                    "sent": "In robotics, it's not so bad.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of problems in optimization logistics where it's not so bad also, but when we tackle, for example, we do some work on using reinforcement learning to train conversational agents, AI agent with a person having a conversation, picking out the right rib Ward signal for when did the agent say the right thing, and then.",
                    "label": 0
                },
                {
                    "sent": "Agent didn't say the right thing can be a lot more difficult, so we have quite a few challenges.",
                    "label": 0
                },
                {
                    "sent": "Another area where there's an interesting challenges is figuring out how do we acquire the data we need for training.",
                    "label": 0
                },
                {
                    "sent": "How do we figure out what is the distributions of states that are important and how does that bias the inference that we're making in terms of the learning in special?",
                    "label": 1
                },
                {
                    "sent": "In the case of robotic system, you can have high cost actions, right?",
                    "label": 0
                },
                {
                    "sent": "I talked about a robot flying robot that was going to explore downwards motion continuously.",
                    "label": 0
                },
                {
                    "sent": "You really don't want to do that too often 'cause at some point if you're crashing a robot everyday, your supervisor is not going to be very happy.",
                    "label": 0
                },
                {
                    "sent": "So you have to figure out how to manage this aspect, and in particular how to deal with the cases where the reward function comes very late turns out to be difficult.",
                    "label": 0
                },
                {
                    "sent": "And the last point in terms of validation confidence measure shows up in some application particular the medical application when we're trying to optimize treatments for different conditions, our clinical partners often want to know what is the margin of error on that particular policy.",
                    "label": 0
                },
                {
                    "sent": "Can I guarantee that I will not do more harm than good to my patient and so we need to think about how we're going to do that in the framework of RL?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to spend a little bit of time because I think that's useful.",
                    "label": 0
                },
                {
                    "sent": "To you as you tackle the literature on what I'll call the horel lingo.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of words that get thrown around, and they're like little single Knowles.",
                    "label": 0
                },
                {
                    "sent": "And if you don't understand these little signals, you may not be able to pick out what are the difficulties in a particular problem.",
                    "label": 0
                },
                {
                    "sent": "And as you start designing your own IRL problems, you may need to think about some of these.",
                    "label": 0
                },
                {
                    "sent": "So I've picked out a list that I could think about.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not an exhaustive list, but will try to distinguish between these things, so those of you who have seen RL may have seen some of these distinctions and I'll try to tease apart what's the difference between these things.",
                    "label": 0
                },
                {
                    "sent": "And why does it matter and what are the things to be looking out for?",
                    "label": 0
                },
                {
                    "sent": "Depending on which of these scenarios you're in?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first one I've already talked about, episodic versus continuous task.",
                    "label": 0
                },
                {
                    "sent": "I would say in that particular case, the thing to be most concerned about is if you have a finite horizon, what is that horizon that you're looking for?",
                    "label": 0
                },
                {
                    "sent": "And if you read into the literature a little bit more carefully, a subtlety that you will find is that if you are in an episodic task domain.",
                    "label": 0
                },
                {
                    "sent": "It's actually important to keep the value function from each of your iterations separately.",
                    "label": 1
                },
                {
                    "sent": "And when you're deploying your system to actually deploy a different value function for each of your horizon, whereas in the continuous cast where we folded in the discount factor, you don't need to do that.",
                    "label": 1
                },
                {
                    "sent": "You have a discount factor, and you can actually maintain just a single value over all of your state at this sort of convergence horizon.",
                    "label": 0
                },
                {
                    "sent": "But in the episodic task, if you don't maintain these separate value function at each of the rounds, then we can actually show that you're not going to obtain the best solution that you can.",
                    "label": 0
                },
                {
                    "sent": "There's some really interesting thoughts on the continuous task case that have started to come out in the last year, and in particular the notion of how to set your discount factor.",
                    "label": 1
                },
                {
                    "sent": "For many years.",
                    "label": 0
                },
                {
                    "sent": "We assume that the discount factor was just given with the environment, but there's some really interesting thoughts as to setting your discount factor a little bit more aggressively during learning allows you to achieve a better bias variance tradeoff in terms of your estimation of your value function.",
                    "label": 0
                },
                {
                    "sent": "Or you could visit after that it asks, can't you just include T as part of the state?",
                    "label": 0
                },
                {
                    "sent": "You can include the and that's exactly what this comes out too.",
                    "label": 0
                },
                {
                    "sent": "If you're unfolding them, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and there's actually some nice results.",
                    "label": 0
                },
                {
                    "sent": "Also, that shows that there may be some advantage when you're diploid in your system to mix between these different policies.",
                    "label": 0
                },
                {
                    "sent": "Pinochet has some nice results on that in the last few years.",
                    "label": 0
                },
                {
                    "sent": "OK, second.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Useful distinction is between tabular and function approximation cases, right?",
                    "label": 1
                },
                {
                    "sent": "The tabular case is really the case where you have a discrete set of States and you can write it out as a vector and you can have a table of your value function.",
                    "label": 0
                },
                {
                    "sent": "The case where you have too many states to write it out.",
                    "label": 1
                },
                {
                    "sent": "Is often the more interesting case is what we call the function approximation case, and that covers both when there are accountable number but very large number of states.",
                    "label": 0
                },
                {
                    "sent": "In the case where your states are actually continuous multi dimensional.",
                    "label": 0
                },
                {
                    "sent": "What's important in this case is that many of the theoretical properties that are shown about our algorithms, whether it's be about convergence properties or about sample complexity properties.",
                    "label": 1
                },
                {
                    "sent": "How many examples do I need to see before I learn a good function?",
                    "label": 1
                },
                {
                    "sent": "Many of those are mostly for the case of tabular, and we have many fewer theoretical results in terms of convergence.",
                    "label": 0
                },
                {
                    "sent": "An learnability for the function approximation case.",
                    "label": 0
                },
                {
                    "sent": "So in many cases you have to be a little bit aware of this when you read some of the literature.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distinction #3 batch versus on line in the batch case, we're assuming that all of your data comes at once, so trajectories from your agent that have been recorded beforehand.",
                    "label": 0
                },
                {
                    "sent": "Under some policy you may know the policy.",
                    "label": 0
                },
                {
                    "sent": "You may not know the policy, though you can usually estimated from the data, so that's the batch case for many years the Community was really focused on the on line learning case, really aiming towards this notion of lifelong AI agents.",
                    "label": 0
                },
                {
                    "sent": "And in that case, the agent takes an action.",
                    "label": 0
                },
                {
                    "sent": "We get the reward information and we want that value estimation to be updated in real time as the information comes through.",
                    "label": 0
                },
                {
                    "sent": "So there's sort of a cycle of acting, observing the transition, adjusting our Q function, and that cycle repeats throughout the life of the agent, and so there's a set of algorithms that are really amenable more for this on line learning cases in the second online learning case, and one thing to worry about in the online learning cases, really, that.",
                    "label": 0
                },
                {
                    "sent": "At each round, as you're adjusting your Q function, you usually use that Q function to choose your next set of actions, so your distribution over the data changes overtime.",
                    "label": 0
                },
                {
                    "sent": "In the second case, in a way that you sometimes have to worry about and may not mix well with function approximation dash.",
                    "label": 0
                },
                {
                    "sent": "Same thing is offline.",
                    "label": 0
                },
                {
                    "sent": "Yes, batch offline is the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me expand a little bit more on online learning because a lot of the literature deals with this particular case.",
                    "label": 0
                },
                {
                    "sent": "In particular, Rich Sutton has developed a very large body of work for this a case, and some of the collaborators doing on some of the other RL researchers.",
                    "label": 0
                },
                {
                    "sent": "When you're doing on line learning, you're going to get your information one piece at a time, so you're going to need to estimate your value function on line, and there's different classes of method for doing that.",
                    "label": 0
                },
                {
                    "sent": "One of them is.",
                    "label": 0
                },
                {
                    "sent": "You can treat each new trajectory as a Monte Carlo sample, and essentially have a value function estimate that gets adjusted after each experience.",
                    "label": 0
                },
                {
                    "sent": "So in this notation, V is really the my estimate of the value function that I maintain overtime, and you is really the estimate of the utility from the most recent trajectory.",
                    "label": 0
                },
                {
                    "sent": "Now, if you recall your definition of UU for St is the utility, so it's the sum of rewards from the point S little T all the way to the end of that trajectory.",
                    "label": 0
                },
                {
                    "sent": "So often you have to if you use a true Monte Carlo approach, you have to wait till the end of that trajectory to figure out what was your utility of that and then do your update and we have Alpha, which is a learning rate in this case.",
                    "label": 0
                },
                {
                    "sent": "So we're doing essentially a gradient update on line update on this particular case.",
                    "label": 0
                },
                {
                    "sent": "It's not a Bellman equation in the sense.",
                    "label": 1
                },
                {
                    "sent": "It's really just a difference equation.",
                    "label": 0
                },
                {
                    "sent": "And in the second case what we have without temporal difference learning and the point of temporal difference learning is really avoid having to wait all the way to the end of the trajectory.",
                    "label": 0
                },
                {
                    "sent": "So instead of looking at the error between my actual utility that I see and my value function that I'm predicting in this case, the utility gets replaced by the sum of the immediate reward plus my estimate of the utility.",
                    "label": 0
                },
                {
                    "sent": "And so I have in this case an.",
                    "label": 0
                },
                {
                    "sent": "Update that I can apply on every time step.",
                    "label": 0
                },
                {
                    "sent": "I don't need to wait till the end of the trajectory to do my update.",
                    "label": 1
                },
                {
                    "sent": "This is what we call the temporal difference update TD learning for many cases.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's a couple of different flavors of TD learning I've given you here.",
                    "label": 1
                },
                {
                    "sent": "The equation for what we call the tabular case, so again, this is the case where you can enumerate all the States and for the version with features.",
                    "label": 0
                },
                {
                    "sent": "So in this case I'm assuming that I have a function that is approximating my value function.",
                    "label": 0
                },
                {
                    "sent": "The easiest way to think about it for now is just assume that my value function is just a linear regression.",
                    "label": 0
                },
                {
                    "sent": "So in this case data would be the weights on my linear regression.",
                    "label": 0
                },
                {
                    "sent": "And I can actually update these weights in real time.",
                    "label": 0
                },
                {
                    "sent": "What's interesting to note in this particular case?",
                    "label": 0
                },
                {
                    "sent": "If you look at here, I have essentially a notion akin to the supervised loss that you've seen often, and so I have V, which is my prediction of whether the value function should be at St an over here I have mine, you estimate based on my immediate observed reward, an again on the value function, but in this case at the next time step.",
                    "label": 1
                },
                {
                    "sent": "There's.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The version called TD Lambda TD Lambda takes advantage of the fact that I want to put an update based on the new information right in T.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zero if I observe some salient new reward information, I only put that update at State St. And for that update to get propagated to the state that I visited the foresty, I'm going to have on another day in a different trajectory to come back again to this S T -- 1 followed by this St and so it's not a very efficient use of.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That new information so TD Lambda essentially seeks to propagate that information to all the states you visited recently, and so there's a notion of eligibility, so you maintain an eligibility over each of the state at each time point.",
                    "label": 0
                },
                {
                    "sent": "That eligibility decreases as a function of your discount factor, and also this factor Lambda.",
                    "label": 0
                },
                {
                    "sent": "So you can set in this case Lambda from anywhere from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "If you set it to 0, you get the TD algorithm I just described previously, and if I set it to one, you can actually show that you essentially recover a Monte Carlo estimate of the value function.",
                    "label": 0
                },
                {
                    "sent": "So based on eligibility you update all of the states that have some eligibility greater than zero based on the new salient information from the new sample you've seen.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is essentially roughly the algorithm that went into TD Gammon plus some function approximation, so it was a TD version with the feature space, and those features were not a linear regression.",
                    "label": 0
                },
                {
                    "sent": "In this case, they were a feedforward neural network, but the correction of that feedforward neural network at the top.",
                    "label": 0
                },
                {
                    "sent": "Wasn't the standard supervised criteria, it was the TD error, so it was looking at the difference between the prediction of the value function of that network and the prediction that would be obtained from the observed reward plus the value at the next time step, again estimated by that network.",
                    "label": 0
                },
                {
                    "sent": "Any questions about the first few?",
                    "label": 0
                },
                {
                    "sent": "Distinctions, yeah, I don't understand eligibility mathematically.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interpretation.",
                    "label": 0
                },
                {
                    "sent": "Which part of it's like maybe?",
                    "label": 0
                },
                {
                    "sent": "What is supposed to do?",
                    "label": 0
                },
                {
                    "sent": "What does it represent mathematically?",
                    "label": 0
                },
                {
                    "sent": "It's just the value of the notion of eligibility.",
                    "label": 0
                },
                {
                    "sent": "It's you can think of it as like a priority of how much you should update that state, and that priority decreases overtime as a factor of Lambda.",
                    "label": 0
                },
                {
                    "sent": "So usually you get it, you update all of them according to some diminishing rule overtime.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, a few more distinctions to come.",
                    "label": 0
                },
                {
                    "sent": "On policy versus off policy, so I've.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Already made the point that the distribution over the data changes based on the policy that you have and so essentially you know in the basic case we assume that we have a big enough batch that it covers all of the cases.",
                    "label": 0
                },
                {
                    "sent": "So when we've collected our batch, we've actually made sure to cover all of the different policy that we need and now we can have accurate estimates of the value of an action in a state over the whole state space.",
                    "label": 1
                },
                {
                    "sent": "In reality, that's often not going to be the case, and so there may be some need to correct for the distribution of the policy.",
                    "label": 1
                },
                {
                    "sent": "One simple way to do that there's other ways to do it, but one simple way that's been doing that's been proposed is to actually use important sampling to reweigh the different probabilities.",
                    "label": 1
                },
                {
                    "sent": "So in this case I distinguish between the policy that I'm actually trying to evaluate right now.",
                    "label": 0
                },
                {
                    "sent": "Pie in my behavior policy, which is the policy under which my data was collected.",
                    "label": 0
                },
                {
                    "sent": "There's some challenges in applying this in practice.",
                    "label": 0
                },
                {
                    "sent": "When your factors become two different, you're important factors get very small, you end up having high variance in your estimation, so you have to be a little bit careful, but at least I want you to be aware of the fact that if your data is collected under a particular policy, and then you try to apply it in a different policy, you have to be concerned about the differences between the distribution somehow.",
                    "label": 0
                },
                {
                    "sent": "Find ways to compensate for that and.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This really arises in problems where you have very large amount of exploration, so if you have a very simple domain where you can afford to try all the actions and all the different states.",
                    "label": 0
                },
                {
                    "sent": "Then it's not really a problem and you can afford to do full exploration and having your data set enough data to characterize all the different policies.",
                    "label": 0
                },
                {
                    "sent": "But as your policy space gets very very large, this is what becomes a challenge.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so there's really this notion that you have to trade off between exploration and exploitation, whereas in exploration you're picking actions to gain more information, but that are not necessarily optimal.",
                    "label": 0
                },
                {
                    "sent": "You can look at your function, your value function to see what's the best action to take, but you're not necessarily picking these best action in a way to further acquire information and exploitation mode.",
                    "label": 0
                },
                {
                    "sent": "You sort of say, well, I'm going to use my value function.",
                    "label": 0
                },
                {
                    "sent": "My current estimation to choose my actions and do it based on that.",
                    "label": 0
                },
                {
                    "sent": "And so, assuming there's enough time this afternoon, I'll talk more.",
                    "label": 0
                },
                {
                    "sent": "I'll go at more length on exploration versus exploitation, particular what we know how to do in terms of exploration, how to be more efficient in picking our actions to explore the domain when we can't afford to be totally exhaustive about.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last distinction is really one between your often here distinction between model based versus model free reinforcement learning, and I'm not going to go at length into the pros and cons of each of these, but really point out that the model based option is really the case where you take all of your data similar data data about your trajectories, and you use that data to estimate the probabilistic model in the reward model.",
                    "label": 0
                },
                {
                    "sent": "And once you've estimated these models, you fix them and then you run essentially planning algorithms based on these estimated models.",
                    "label": 0
                },
                {
                    "sent": "So this would be the model based approach and then the model free approach.",
                    "label": 0
                },
                {
                    "sent": "You don't worry at all about learning the correct dynamic model of your environment.",
                    "label": 1
                },
                {
                    "sent": "You worry directly about learning either a good value or good policy independent of the model, and these are very gross characterization.",
                    "label": 1
                },
                {
                    "sent": "There's methods that makes a bit of both in various different ways.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe the last distinction is really about what we call value based methods and policy based method and Peter is going to talk about the policy based methods a little bit later and so today.",
                    "label": 0
                },
                {
                    "sent": "Really what I'm talking about is the methods that use dynamic programming like approaches.",
                    "label": 1
                },
                {
                    "sent": "And are trying to directly estimate the value function.",
                    "label": 1
                },
                {
                    "sent": "But as we saw a little bit earlier, you can actually switch once you have a good value function, you can tease out a policy and once you have a good policy you can tease out a value function.",
                    "label": 0
                },
                {
                    "sent": "The choice of what method to use, whether it's a value based method or a policy based method is really I think related to the characteristics of your domain.",
                    "label": 0
                },
                {
                    "sent": "So in many cases if you have a lot more structure in your policy and it's easier to express.",
                    "label": 0
                },
                {
                    "sent": "Prior information in the space of policy, then, probably the policy estimation methods might be better.",
                    "label": 0
                },
                {
                    "sent": "If you have a very small set of actions, particular small set of discrete actions as we do in the Atari system for example, then maybe the value based methods are the way to go, so it's not a clear cut sense, and as I mentioned, a little bit earlier, there's these actor critic methods that actually makes features of both of them, and that may well be the way to go as we move forward, there's been a lot more interesting.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Methods going forward.",
                    "label": 0
                },
                {
                    "sent": "Any questions on the?",
                    "label": 0
                },
                {
                    "sent": "RL lingo you see a little bit more clearly through that, yes?",
                    "label": 1
                },
                {
                    "sent": "For a quality based rather than value.",
                    "label": 0
                },
                {
                    "sent": "It really depends on the question is in the case of a conversational agent, it would be more policy based or value based, and I think it really depends on the structure that you want to assume for your conversation, and so if you're having a dialogue agent that can only select from a small set of specific speech acts as has been done in the literature for several years, then the value based methods have been reasonably successful as we're moving towards conversational agent that are going to generate natural language in the space of natural language is much bigger and we're really looking more, probably at architectures that are going to look like actor critic methods.",
                    "label": 0
                },
                {
                    "sent": "That makes some element of value function to estimate the value of the different speech acts, but that allow us to have a much more fluid notion, a much richer notion of what's our policy.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm assuming you have the lingo down for the most part and you know when to worry about certain things, and in particular once you have a problem in RL problem you want to tackle, how to start thinking of, whether the different dimensions along which you need to characterize your.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem what I want to talk about in the last 15 minutes or 20 minutes is really how do we deal with very large state spaces.",
                    "label": 1
                },
                {
                    "sent": "So we have our Q function and earlier on when I give you the equation for the Q function I wrote it out as the Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "Your immediate reward.",
                    "label": 0
                },
                {
                    "sent": "You maximize your future expectation over your future state in the value of your future state and I'm going to discuss the case where your Q function can be learned directly approximated essentially as a rich set of functions.",
                    "label": 0
                },
                {
                    "sent": "So here I have the linear version of it.",
                    "label": 0
                },
                {
                    "sent": "Where theaters are my weights and flies or some kind of feature vector an we're going to assume at first that maybe these feature vectors are given designed by hand features of the board.",
                    "label": 0
                },
                {
                    "sent": "In the case of TD Gammon, that was the case that there was sort of a 200 dimensional input vector that where they had selected.",
                    "label": 0
                },
                {
                    "sent": "What were the right features to think about?",
                    "label": 0
                },
                {
                    "sent": "And for many years computer goal was also done along these lines where they had some sense of what are the features you need to think about right?",
                    "label": 0
                },
                {
                    "sent": "Local patches of black and white pieces.",
                    "label": 0
                },
                {
                    "sent": "Encoded and they would have a vector that describes the board in that way.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the framework for incorporating.",
                    "label": 0
                },
                {
                    "sent": "In this case, function approximation so large state space function approximation, But I'm going to do it from a batch case in.",
                    "label": 0
                },
                {
                    "sent": "This is to really set it up as a supervised learning problem, so there's a family of approaches called fitted Q iteration, where as an input to your system you have the state in action information.",
                    "label": 0
                },
                {
                    "sent": "As the output you have essentially your prediction of the Q function, so the output for one particular example, your data is essentially you take your trajectory and you cut it up into these experienced pieces and each experience piece consists of a state and action are award in the next state.",
                    "label": 0
                },
                {
                    "sent": "So that's an experience.",
                    "label": 0
                },
                {
                    "sent": "Peace and your batch is the collection of these experience pieces and for fitted Q iteration we're going to roughly assume that you've mixed them all up so they're not in the order of your trajectory, so they're roughly in dip.",
                    "label": 0
                },
                {
                    "sent": "And if your batch is big enough so our input is your state in action and your output is essentially the estimate of your Q function, so your immediate reward and then the Max action over the next state and your loss is measured by the same type of loss that we were using when we were doing the TD updates.",
                    "label": 0
                },
                {
                    "sent": "So I have my immediate reward plus my estimate of my future reward, and then I have my current estimate at S and think of now doing linear regression or using neural networks.",
                    "label": 0
                },
                {
                    "sent": "There's a very nice paper by Damien Urns where he explores the use of random forests for doing this, and so what's really important to note here, I think.",
                    "label": 0
                },
                {
                    "sent": "Is the fact that your estimate of the Q function actually appears twice?",
                    "label": 0
                },
                {
                    "sent": "In your loss, right?",
                    "label": 0
                },
                {
                    "sent": "Usually in supervised learning you wouldn't have this term over here.",
                    "label": 0
                },
                {
                    "sent": "If you think of a problem that has a horizon of one, you just pick one action and you observe the reward.",
                    "label": 0
                },
                {
                    "sent": "Then you see whether your Q function predicts that reward well, you're not going to have that term over here, just going to have our NQ.",
                    "label": 0
                },
                {
                    "sent": "But now I have my Q function that appears twice, and the reason this is problematic is because in the beginning when your Q function is really poorly characterized, it means that your measure of loss is really poor and you can have instability in terms of your.",
                    "label": 0
                },
                {
                    "sent": "Learning in particular, in your case where R is very, very sparse.",
                    "label": 0
                },
                {
                    "sent": "So think of cases where you don't get any reward all the way till the end of your goal game.",
                    "label": 0
                },
                {
                    "sent": "Like 200 moves later.",
                    "label": 0
                },
                {
                    "sent": "Then all of this information in terms of trying to predict your loss is really just noise all the way till you get a reward signal and so that learning problem is the reason why for many years trying to use neural networks to do function approximation in RL was really problematic and it turned out there were a few people who seem to know the magic sauce.",
                    "label": 0
                },
                {
                    "sent": "To make it work, but now we've I'll tell you a few of these magic ingredients.",
                    "label": 0
                },
                {
                    "sent": "So here do you optimize only the last Q or you optimize your expected data appearing in the two places in computer grading?",
                    "label": 0
                },
                {
                    "sent": "In this case, you just take the Q estimates directly.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "I'm not making the assumption yet that we're computing a gradient, but in some cases you optimize that loss.",
                    "label": 0
                },
                {
                    "sent": "You should compute gradient with respect to all the appearances of data in.",
                    "label": 0
                },
                {
                    "sent": "This is so if you're doing it as a neural network, you compute a gradient.",
                    "label": 0
                },
                {
                    "sent": "If you're doing it using random forest, you don't write, you use a different criteria for minimizing the error, and so.",
                    "label": 0
                },
                {
                    "sent": "But if you're doing neural networks yet, so you can compute your gradient with respect to the parameters of your network using that expression on both sides.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "A while ago by Rampar and collaborators have studies.",
                    "label": 0
                },
                {
                    "sent": "What do you should optimize?",
                    "label": 0
                },
                {
                    "sent": "Both hit us or only one of them, and I think in his paper declining swor it's better to just optimize one of them, not both.",
                    "label": 0
                },
                {
                    "sent": "Discuss it a little bit because that was one of the key ingredients for a lot of the DQN work and so.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to go back to the arcade learning environment, 'cause it was one of the most prominent cases of where they use function approximation.",
                    "label": 0
                },
                {
                    "sent": "In that case, you're trying to play these Atari videos just to give you a sense of how they're doing it.",
                    "label": 0
                },
                {
                    "sent": "They're really playing it with information at the pixel level, so they have the images frame by frame.",
                    "label": 0
                },
                {
                    "sent": "The action set is relatively small, depending on the games.",
                    "label": 0
                },
                {
                    "sent": "But they were trying to do that for many many different game.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the question in this case was how do we pick out the features?",
                    "label": 0
                },
                {
                    "sent": "And because they're going to do that over several games, they don't want to recreate the set of features manually for all of these.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Games and the time was ripe for applying convolutional neural networks in this case, so their Q function is essentially the output of a convolutional neural Nets.",
                    "label": 0
                },
                {
                    "sent": "In this case, I'm not going to go in the particular architecture, but you can look at the paper if you're interested.",
                    "label": 0
                },
                {
                    "sent": "It's essentially trained with stochastic gradient descent, so they get a big batch of data with a lot of exploration, and then they train this to predict the Q function.",
                    "label": 1
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can look in this case at the training score, and so it's important to know that the training score is always measured once you fixed your Q function, and now you're just running it in test mode where you use that Q function to pick out the action.",
                    "label": 0
                },
                {
                    "sent": "So usually compare the Q value for the different actions and you pick the one that has maximal Q value.",
                    "label": 0
                },
                {
                    "sent": "So this isn't so convenient if you have continuous action space or a very large accent space because you need to operationalize that maximization over the choice of action, But in Atari it's not a problem because it's a relatively small action space and what you see is learning curves that look a little bit like that.",
                    "label": 0
                },
                {
                    "sent": "Your score goes up in this case.",
                    "label": 0
                },
                {
                    "sent": "We've changed the number of training episodes that are allowed.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you look in the paper, you'll see a lot of results that look like this, where they're essentially listing all the different archery games that they've worked on, and they compare the performance of a human player versus the AI player and above this line are all the games where AI is better, and below are the games where the humans is better.",
                    "label": 0
                },
                {
                    "sent": "And so there's some pretty lengthy list of results.",
                    "label": 0
                },
                {
                    "sent": "What I want to emphasize in the few minutes we have.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is really the useful ideas that were introduced.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have stable learning in this type of architecture.",
                    "label": 0
                },
                {
                    "sent": "Some of them had been a little bit announced in the literature before, but this work really crystallized the importance of them.",
                    "label": 0
                },
                {
                    "sent": "The first one is probably the notion of experience replay, so it deals a little bit with your question of how do we get near batches of data?",
                    "label": 0
                },
                {
                    "sent": "The notion of experience replays that you have a very very large batch, but every time every round you do your queue updates to actually sample a subset of that batch drawing randomly.",
                    "label": 0
                },
                {
                    "sent": "Essentially decorrelating your samples 'cause your samples were obtained along trajectories, but you don't want to take a piece of the mini batch that all comes from one particular part of the trajectory you want to have our presentation over the whole space, and so you ran them.",
                    "label": 0
                },
                {
                    "sent": "A mini batch of experience.",
                    "label": 0
                },
                {
                    "sent": "Do your updates using that.",
                    "label": 0
                },
                {
                    "sent": "There's been some more recent work that shows that you should actually do prioritized experience replay, meaning that you shouldn't sample uniformly over all of your big batch of data, but you should sample pieces.",
                    "label": 1
                },
                {
                    "sent": "In proportion to different factors, they have a few different criteria.",
                    "label": 1
                },
                {
                    "sent": "In the paper, one of them is suggesting that you should actually sample state switch, where you've observed a larger TD error.",
                    "label": 0
                },
                {
                    "sent": "Because these are probably the ones where your network is not doing a good prediction, so you should emphasize the training under those pieces of the problem.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So experience replay seems to be.",
                    "label": 1
                },
                {
                    "sent": "Quite useful, a couple other useful tips for stability.",
                    "label": 1
                },
                {
                    "sent": "One of them is gradient clipping.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming everyone is familiar with that.",
                    "label": 0
                },
                {
                    "sent": "We've talked about it earlier this week.",
                    "label": 0
                },
                {
                    "sent": "The other one is this notion of a periodic update, which was a little bit foreshadowed.",
                    "label": 0
                },
                {
                    "sent": "In previous work, and the idea is really to once periodically really fix your network of Q values.",
                    "label": 0
                },
                {
                    "sent": "I'll call this Q Theta minus an.",
                    "label": 0
                },
                {
                    "sent": "We're going to use that network to calculate the error, so that's going to be the right side of my error equation and then all of my updates.",
                    "label": 0
                },
                {
                    "sent": "In fact, I think in this case they're fixing both queues in to calculate the error.",
                    "label": 0
                },
                {
                    "sent": "So you fix your queue for both the left and the right side.",
                    "label": 0
                },
                {
                    "sent": "Correct me if I'm wrong, but I'm pretty sure that's what they do.",
                    "label": 0
                },
                {
                    "sent": "They fix both of them and then they apply all of their Q updates to some other network which keeps on changing and once in awhile after a few iterations they take the updated network and then they start using that as a reference network.",
                    "label": 0
                },
                {
                    "sent": "And that seems to really be useful for stabilizing of the learning everyone has been using, that including in some policy search methods.",
                    "label": 0
                },
                {
                    "sent": "Another one of the last ones is this work on double DQN.",
                    "label": 0
                },
                {
                    "sent": "Essentially, there's a nice work by Hayden has helped in some of this colleagues.",
                    "label": 0
                },
                {
                    "sent": "Most of this work is done a deep mind these days, where they've shown that your Q values.",
                    "label": 0
                },
                {
                    "sent": "Are actually often biased and their biased by the fact that in your estimate of the Q value there's a Max function and that Max is a Max over Q.",
                    "label": 0
                },
                {
                    "sent": "So if there's some error in your estimation of the Q and you're taking Max over that overtime, you're going to introduce some bias as you converges that bias goes away.",
                    "label": 0
                },
                {
                    "sent": "But in the early phases of learning, that can be quite problematic, and so in this particular case they use a correction where they use a different queue function.",
                    "label": 0
                },
                {
                    "sent": "They essentially have two Q functions in parallel.",
                    "label": 0
                },
                {
                    "sent": "Use one to select the action and a different one to calculate the error, and so that mitigates the bias.",
                    "label": 0
                },
                {
                    "sent": "In this particular case.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you look at the empirical results on the top over here, I'm showing actually the value estimates.",
                    "label": 0
                },
                {
                    "sent": "With the Q ANAN with double DQN in blue.",
                    "label": 0
                },
                {
                    "sent": "And so you see that the standard document actually overestimates the value quite a bit in some of the games.",
                    "label": 0
                },
                {
                    "sent": "Not in all of the games, but there's some games where it really overestimates the value, and if you look at the score once you correct so in blue again the double DQ, and once you correct for that by you can achieve much better performance on some of these games, and these ones essentially converge to bad solution because their early estimates of the Q function were too biased.",
                    "label": 0
                },
                {
                    "sent": "So that'll take you in class to say bias only on the opposite direction, intends to underestimate.",
                    "label": 0
                },
                {
                    "sent": "But apparently for some reason you're picking the maximum.",
                    "label": 0
                },
                {
                    "sent": "Estimating is not only the lessons it seems to yeah cause less problem and the last one.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apps I'll mention in terms of the tricks that seem to yield some advantage is this dueling Q network paper that was presented at ICML this summer.",
                    "label": 0
                },
                {
                    "sent": "In this case appear I have essentially the standard EQ and architecture with the content structure and at the end you're just predicting the Q value in the dueling network.",
                    "label": 0
                },
                {
                    "sent": "You separately predict not the Q function, right?",
                    "label": 0
                },
                {
                    "sent": "Q is state action pair, but you predict the value function.",
                    "label": 0
                },
                {
                    "sent": "So the best of the queues, right V is Max.",
                    "label": 0
                },
                {
                    "sent": "Over Q, you predict the value function and then you predict the advantage function.",
                    "label": 0
                },
                {
                    "sent": "I haven't defined the advantage function, but it's in the literature going back to the early 1990s.",
                    "label": 0
                },
                {
                    "sent": "That notion of advantage function is really the difference between your value function and your Q function.",
                    "label": 0
                },
                {
                    "sent": "So at the best action that advantage is going to be 0 an.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the advantage is going to be the gap between the Q value of that action in my value function.",
                    "label": 0
                },
                {
                    "sent": "So in this network they separately predict VNA and then they compute cube I just.",
                    "label": 0
                },
                {
                    "sent": "Adding up VNA.",
                    "label": 0
                },
                {
                    "sent": "And there's an extra term there to constrain a little bit.",
                    "label": 0
                },
                {
                    "sent": "The equation.",
                    "label": 0
                },
                {
                    "sent": "I've taken it out just for clarity sake, but the lower level of the architecture are common and just at the last level, which I think is fully connected.",
                    "label": 0
                },
                {
                    "sent": "They separately learn Viennet, and the notion is that V can actually be shared across all the actions, and so it's essentially like a stabilizing bias that goes into your estimation of your Q function, and that again tends to produce.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good results if you look at the graph.",
                    "label": 0
                },
                {
                    "sent": "You know this is dueling Q networks versus simple DQN and this is the whole bunch of games where dueling does better.",
                    "label": 0
                },
                {
                    "sent": "This is vanilla DQ and if you compare dueling versus the prioritized queue and so that has prioritized experience replay plus the double DQ and then there's a smaller number of games.",
                    "label": 0
                },
                {
                    "sent": "But there's still some games where the dueling architecture seems to give quite a bit of a boost in this case.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is not all work in Atari.",
                    "label": 0
                },
                {
                    "sent": "More recently people have started looking at doing rlin Minecraft.",
                    "label": 0
                },
                {
                    "sent": "There's some nice work coming out of University of Michigan quickly.",
                    "label": 0
                },
                {
                    "sent": "Satinder Singh and their students.",
                    "label": 0
                },
                {
                    "sent": "Paper in this case what's interesting in Minecraft versus Atari that in Minecraft you have a restricted field of view, right?",
                    "label": 0
                },
                {
                    "sent": "Your agencies, some local environment in attire you essentially see everything that you need to.",
                    "label": 0
                },
                {
                    "sent": "So if you have the frame you have all of the information you don't have to deal with partial observability.",
                    "label": 0
                },
                {
                    "sent": "In Minecraft, you do have to deal with partial observability and so they have played with various different architectures that include convents, but also including notions of memory and context and context can be thought a little bit akin to.",
                    "label": 0
                },
                {
                    "sent": "Attention.",
                    "label": 0
                },
                {
                    "sent": "In this particular case, then, of course, as you add memory in context and more links between all this, your architecture gets richer and richer and you get better results.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in knowing a little bit more, there's some online videos.",
                    "label": 0
                },
                {
                    "sent": "There's a spicy melt paper, but the notion of attention and memory that they're introducing are somewhat related to the things that were discussed earlier this week.",
                    "label": 0
                },
                {
                    "sent": "So just to tell you that these ideas are permeating into the URL field as well.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a lot of work and attention on these games.",
                    "label": 0
                },
                {
                    "sent": "There's also people working on Mario and Star Craft and Doom.",
                    "label": 0
                },
                {
                    "sent": "It seems to be quite rich, in particular 3D games with partial observe ability seems to be the next challenge for many things I happen to think that it's important to work on other kinds of PRL problem.",
                    "label": 0
                },
                {
                    "sent": "There's a reason these domains, though, have really enabled us to make a lot of progress.",
                    "label": 0
                },
                {
                    "sent": "One of them is that in all these cases, we have access to simulator, and as I mentioned earlier, you can share that simulator between research teams.",
                    "label": 0
                },
                {
                    "sent": "So it makes it a lot easier in terms of research progress to be able to do that.",
                    "label": 0
                },
                {
                    "sent": "Many of these domains are nearly deterministic, and so again, if you don't have a lot of stochasticity over the actions based probably easier to learn, and we also have a relatively small set of actions, so most of these.",
                    "label": 0
                },
                {
                    "sent": "Our results that used EQN type of architectures tend to be with cases where you have a relatively small set of actions.",
                    "label": 0
                },
                {
                    "sent": "Otherwise doing the maximization over the action can be problematic in terms if you do care about real world application.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think there's really promising results in not promising results.",
                    "label": 0
                },
                {
                    "sent": "Yet.",
                    "label": 0
                },
                {
                    "sent": "I should say very rich investigation domain on the side of conversation systems.",
                    "label": 0
                },
                {
                    "sent": "We're doing some of that work in our lab.",
                    "label": 0
                },
                {
                    "sent": "There's many other groups working on that, but in general I think around NLP type of tasks.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of interesting work to be done.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to wrap up over here and maybe we have a couple of minutes for questions before copy.",
                    "label": 0
                }
            ]
        }
    }
}