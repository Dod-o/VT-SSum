{
    "id": "ojhnndwn76sgq3s3q7jgec55npoab3bj",
    "title": "Chordal Sparsity in Semidefinite Programming and Machine Learning",
    "info": {
        "author": [
            "Lieven Vandenberghe, Electrical Engineering Department, University of California, Los Angeles, UCLA"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Mathematics->Graph Theory"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_vandenberghe_css/",
    "segmentation": [
        [
            "Not just for inviting me for this session.",
            "So this is joint work with Martin Anderson, who is a PhD student at UCLA and welcomed Al who is currently at anybody technology in Denmark."
        ],
        [
            "So the talk will be on large scale convex optimization using interior point methods.",
            "Ann will exploit results on coral sparsity that are actually very well known in machine learning, so this topic probably fits best with the third question in the on the list on using it on using techniques that are actually well known in related to well known techniques and machine learning for large scale optimization.",
            "So I'll start with an introduction of coral sparsity, and then later in the talk I'll look at two applications in more detail, semidefinite programming and then also dense quadratic programming.",
            "So a chordal graph is."
        ],
        [
            "Defined as follows.",
            "It's an undirected graph.",
            "And it's called coral.",
            "If every cycle of length greater than three has a court, so an edge joining too.",
            "Nonadjacent edges nonadjacent nodes, so this would be.",
            "Non chordal because that's a cycle of length greater than three that doesn't have a record.",
            "This is a chordal graph because every cycle of length greater than three has occurred.",
            "So it's also known as triangulated or decomposable and other names.",
            "So if this refers to sparsity pattern, then the graph represents the sparsity in a symmetric matrix, so the nodes correspond to the rows and columns in the matrix.",
            "And the."
        ],
        [
            "Edges represent non zeros in the graph, so we have an edge between two nodes in the graph.",
            "If there is a non zero in position.",
            "That position in the graph.",
            "In the matrix.",
            "So these are two examples.",
            "Basic examples of coral sparsity patterns that are not.",
            "Very interesting as sparsity patterns, but they will be important for applications later in the talk.",
            "And in some sense of the most basic or Canonical coral sparsity patterns, one is an arrow pattern where you have a diagonal matrix and then a few dense a number of one or more dense columns and rows, and this would be the corresponding graph and it can easily check that's horrible graph.",
            "So an arrow or a block arrow sparsity pattern is in coral sparsity pattern."
        ],
        [
            "Another example is a band matrix which corresponds to this graph, so again, you can easily check from the graph that it's coral.",
            "And so that's another example of a coral sparsity pattern.",
            "This would be less obvious if you look at the sparsity pattern, so this is a sparsity pattern of a graph of a matrix of order 17.",
            "This is the corresponding node graph, and then you can see certainly that this part is triangulated or coral, and here is a click and from the graphic and also see that it's coral."
        ],
        [
            "So coral graphs of course.",
            "I've have been known in different areas for a long time, specially in linear algebra and also in machine learning and statistics.",
            "In linear algebra there.",
            "Known for the most maybe most important property is that if a positive definite matrix has a coral sparsity pattern, then it can be factored using salesky factorization with zero filling.",
            "And that's a very practical, useful equivalent definition of a coral sparsity pattern.",
            "Coral sparsity means exactly that it can be factored with zero filling.",
            "If you use the right elimination ordering.",
            "And that's been known since the 70s.",
            "There is a related problem in linear algebra that's known as the positive definite matrix completion problem, and also that will discuss later, and it also has a simple solution if the sparsity pattern of the specified part in the matrix is coral.",
            "In machine learning, it's rises in the graphical models for example, and maximum likelihood estimation in Gaussian graphical model has a simple solution if the sparsity pattern or the topology of the graph is squirrel, and that's basically equivalent to the positive definite matrix completion in the Roger Brown.",
            "Related problem is Euclidean distance completion problem has been studied and also in machine learning.",
            "And in optimization, it's been used extensively for sparse semidefinite programming, but people have developed techniques for exploiting coral sparsity in interior point methods for sparse semidefinite programming.",
            "There are also a few papers where people try to use coral techniques for sparse preconditioners, for example, and sparse quasi Newton updates.",
            "They tried to use the linear algebra techniques related to chorales.",
            "Sparsity, too, for example define or derive sparse preconditioners.",
            "So many of these."
        ],
        [
            "Algorithms and techniques I will see are most easily explained if you represent the graph by click tree.",
            "And so this is some some definition.",
            "A click, of course, is a maximal complete sub graph in the matrix in the graph.",
            "And then the clicks of a chordal graph has a very very interesting property.",
            "So they can be ordered in a tree with as its nodes.",
            "The cliques in a graph and they the tree has this so called running intersection property.",
            "And that means that in this tree, the intersection of a click with its parents in the tree.",
            "Is equal to the intersection of the click with the Union of all the cliques that preceded in the tree.",
            "So that's a basic property that follows from coral structure, and that's not not true for general sparsity patterns."
        ],
        [
            "So this would be an example that illustrates this property.",
            "This is the core of sparsity pattern.",
            "Of order 17.",
            "These are all the clicks in the in the graph.",
            "Ordered in a tree.",
            "And then there running intersection property.",
            "For example for this click with the nodes 15678, which is.",
            "This corresponds to this complete or dense.",
            "Diagonal sub lock on the of the matrix.",
            "So the running intersection property means that the intersection of this click with its parent, so that's nodes.",
            "One, part six is also the same as in collection of this week, with all the clicks that preceded.",
            "So if you take the union of the all the clicks that preceded.",
            "And then the intersection with the click W, then you get the same intersection.",
            "So that's an important property, because that is basically the reason why we can formulate algorithms for many problems that are related to coral graphs by local, by recursive algorithms over this click tree, very processor click Cemetery and topological order or reverse topological order, and for each click you do some operations on the click and it's parent or the click and it's children, and so that's how all these algorithms will discuss will work.",
            "Um?",
            "So too, so the basic most important result of COBOL classes in linear algebra is that it's possible to factor and positive definite matrix with the coral sparsity pattern with zero fill in.",
            "So that depends on certain you have to choose."
        ],
        [
            "The right elimination order and this would be an example of a coral sparsity pattern that's ordered using a perfect elimination order.",
            "So we took the client to obtain such an order.",
            "You can take the click tree number, the clicks in a topological order starting at the root.",
            "And then number in the notes and each click consecutively.",
            "So we started 123, then the next would be this one.",
            "So the new node in this click is gets the next number.",
            "Then you take the next click is the new nodes and that click get numbers 5, six and so on.",
            "So if you and number the nodes in the graph in the graph or the rows and columns in the matrix like this, then he gets a perfect elimination order foreign Cholesky factorization as our thresholds where artist upper triangular.",
            "And the algorithm would.",
            "More less work like this.",
            "You start at the last node in the in the last click or the end of the matrix and then you can eliminate the elements non zeros below the diagonal.",
            "With the usual method for Cholesky factorization and that will introduce no filling in the upper triangular part.",
            "And you can just fill in the non zeros and start via recursion on this click tree starting at the end node filled in on zero elements with the Cholesky factor of the matrix.",
            "Um?",
            "So I'll skip the details, but the important thing is that the to obtain such as salesky factorization you follow a recursion on the click tree.",
            "And you process the clicks in reverse topological order.",
            "So you will start at the end node.",
            "In the click tree and then visit the clicks in reverse topological order.",
            "So that's all we'll need for.",
            "As for background on coral sparsity.",
            "And the next section."
        ],
        [
            "Talk about some additional results and also some connections with convex optimization.",
            "So many of these results that are related to coral."
        ],
        [
            "Sparsity patterns of matrices.",
            "Fit nicely in convex optimization framework where you use cone programming as a standard.",
            "Format for convex optimization.",
            "So these are some definitions that will use, so SN is a set of symmetric matrices of order N. And then a subscript V will denote a sparsity pattern, so it's the set of all symmetric matrices of order in with fixed sparsity pattern V. So for example, they could be banned it, and then it's a space of all banded matrices with that bandwidth.",
            "For example, tridiagonal matrices of order.",
            "So we always assume that the non zeros in V included diagonal elements, so that's needed for some.",
            "Natural in all these applications and it's also needed for some technical conditions.",
            "And the P of with subscript fee denotes the projection of a matrix on the sparsity pattern.",
            "So for example, if the sparsity pattern is tridiagonal.",
            "You project the Matrix on a tridiagonal pattern by setting everything outside the three diagonals equal to 0.",
            "That's what the projection on that sparsity pattern means.",
            "And in all of this.",
            "Sparsity pattern defines the possible non zeros in the matrix.",
            "So if we say an element is a non zero it means it can be.",
            "It's allowed to be non 0.",
            "If you say in Element Zero in a sparsity pattern then it's always has to be 0.",
            "So with this definitions we can define 2 interesting matrix cones that are convex.",
            "One is the set of positive semidefinite matrices with that given sparsity pattern.",
            "For example, triangle positive semidefinite matrices.",
            "And we'll denote that with an extra subscript plus.",
            "And the second cone is the set of matrices with that sparsity pattern that have a positive semidefinite completion.",
            "I will use this subscript C to denote the second column.",
            "So with this projection rotation we can easily write the second cone as follows.",
            "It's the projection of all positive semidefinite matrices on the sparsity Pattern V. That's the set of all matrices that have a positive semidefinite completion.",
            "And that sparsity pattern V. And then it turns out that these cones are dual cone for the usual inner product, so I'll use the trace of the product of the two matrices and I'll use this notation.",
            "That is quite standard in optimization, as this dot product is the inner product of two symmetric matrices.",
            "So there are dual cones using the."
        ],
        [
            "The standard definition of dual cone, so the matrices in these pair of matrices in these two cones always have a non negative inner product.",
            "So for example, if you take triangle sparsity pattern, then that's the matrix with a tridiagonal sparsity pattern.",
            "In R3 it's not positive semidefinite, so it's not in the first cone.",
            "But it has a positive semidefinite completion because if we replace the zero with one, we get a positive semidefinite matrix.",
            "And that's a simple example that shows that the two cones are not equal, so they're not self dual in general.",
            "And.",
            "So they're not equal and there are dual dual pair of cones.",
            "So if you want to formulate optimization problems in terms of in a convex cone programming format, then the ingredients we need to formulate interior point methods are that for each cone, the primary dual cone.",
            "The important and overlooked value function and be able to evaluate its gradient and its Hessian.",
            "So if we can do this for these two cones.",
            "So if you start with the post, it's me."
        ],
        [
            "If not, then we can just use the standard local rhythmic barrier function for positive semidefinite matrices.",
            "So minus logged out of S. And the only difference here is we restrict S to the subspace of matrices where the given sparsity pattern is, for example the tridiagonal positive semidefinite matrices.",
            "Then the gradient is defined like this.",
            "It's the projection of the inverse of S on the sparsity pattern.",
            "So if we take this function but on the set of all positive semidefinite matrices, then the gradient is just the negative of the inverse of S. If you restrict the function to given sparsity pattern and the gradient is the projection of the gradient on that sparsity pattern.",
            "So that's a first important question.",
            "If we want to use this barrier function.",
            "On this matrix cone we have to be able to evaluate this gradient efficiently.",
            "And that means we have to.",
            "We're interested in the elements of the inverse of the matrix.",
            "But only the elements in the sparsity pattern of S. So if V is a tridiagonal sparsity pattern.",
            "Then S would be a positive definite tridiagonal matrix.",
            "Its inverse is dense in general.",
            "But we're only interested in the elements of the inverse on the three diagonals.",
            "And so that will be important to just be able to compute those elements without computing all the rest of the matrix that we have.",
            "The inverse that we're not interested in.",
            "And then if so, the importance of coral sparsity pattern will be exactly that.",
            "If V is, coral is the sparsity pattern is coral, then there exists a simple recursive methods.",
            "For evaluating this gradients.",
            "No rush to evaluate the projection of the inverse of the sparsity pattern.",
            "So for example, if for a bandits sparsity pattern you can buy via recursion on the click tree, compute the elements of the universe in the band Diagonal Band without having to compute any other elements in the in the inverse.",
            "And that is only true for.",
            "That's true for coral sparsity patterns.",
            "Skip the method, but it's very similar to the Cholesky factorization method.",
            "And then the Hessian in Interior Point methods is also important to be able to evaluate the Hessian, or at least apply the Hessian.",
            "So the this denotes the Hessian matrix is applied to a symmetric symmetric matrix Y.",
            "So that's defined like this.",
            "We compute the inverse times Y times as inverse, and then again we have to project this on the sparsity pattern.",
            "And it's important to be able to at least evaluate this quantity for a given Y in a given S. Again, without evaluating any elements outside the sparsity pattern, and also without requiring the full inverse of S. And again, that's possible if the sparsity pattern is coral, and that's why we are interested in coral sparsity patterns.",
            "And so I'll skip the methods, but you can think of this as following from the chain rule of differentiation, because the Cholesky factorization with zero filling it gives us an efficient method for evaluating the barrier function computers, Cholesky factorization and take the logarithms of the diagonal elements in the factor.",
            "And then by the chain rule, it's not surprising that you can always also take the gradient or the Hessian VR similar recursion."
        ],
        [
            "So that's for the first one.",
            "The second cone is the cone of completable matrices.",
            "And then we can use the standard definition of a dual cone corresponding to account for the primal and it's given by the conjugate or the genre transform of the primal barrier function.",
            "So that's defined like this as an optimization optimal value of an optimization problem.",
            "Where we if you want to evaluate the dual, very function at C. So that's the matrix with a positive semi definite or positive definite completion.",
            "Then we have to maximize this function over S where S and is in the primal cone.",
            "And so the result of this optimization problem gives us the dual barrier function.",
            "And it can also be written like this if this hat is the matrixes that maximizes this for a given Z.",
            "So there's different ways of writing this solution of this optimization problem.",
            "It's also from the optimality conditions.",
            "You can write it like this.",
            "The optimal S that maximizes this function for a given Z satisfies this condition, so you fix Z and it solves this nonlinear equation in South, so it's a sparse positive definite matrix.",
            "You specify the projection of its inverse, and from that you want to compute the matrix as, so it's sort of the dual of the evaluation of the gradient of the primal barrier.",
            "There were given an essay revaluate, See given us and that's a non linear equation in this.",
            "Ann from duality and the optimality conditions you can also show that S had inverse is the maximum determined completion, positive definite completion of Z.",
            "So another way to think of this value function is that you take the matrix Z.",
            "You compute it's positive definite completion.",
            "And then the.",
            "Log out of that matrix gives us the do very function.",
            "And then from results on Legendary transforms and conjugate transforms, you also get the gradient question.",
            "You know this matrix is had.",
            "So again, the conclusion is that in general, for general sparsity patterns to evaluate this dual very function, you will have to solve this optimization problem numerically.",
            "For a coral sparsity pattern, you can actually solve this optimization problem almost analytically or by a simple finite recursion of the click tree.",
            "And that gives us the value of the barrier function and also the gradient and Hessian.",
            "So that's what we need in the second part of the talk.",
            "So the.",
            "Interesting properties of coral sparsity patterns for these applications or that we have a number of matrix computations that are very easy to solve.",
            "The sparse sparsity patterns, coral and it can be solved by recursions.",
            "Finite recursions of in the click tryan topological order or reverse topological order.",
            "And that's the list of the problems that for which this is true.",
            "We had the Cholesky factor."
        ],
        [
            "Station we can compute the projection of an inverse without computing other entries of the.",
            "Inverse.",
            "We can do the inverse of this gradient evaluation.",
            "So given the projected inverse, we can compute matrix Y that has this projected inverse.",
            "And then these two are basically linearizations of these two steps and they rise in the Hessian.",
            "Passions for the Do 2 barrier functions.",
            "And together they give us efficient methods for computing the gradient and hessian of algorithmic.",
            "Very function of these two cones.",
            "And we have a library of that implements algorithms for all these operations.",
            "So in the second part of the talk, I want to apply this to two applications.",
            "First sparse semidefinite programming.",
            "And to introduce this.",
            "Or start with some general discussion of convex optimization cone programming.",
            "So the cone programming format for is a general format for convex."
        ],
        [
            "Optimization.",
            "Anne."
        ],
        [
            "It's has been widely used since the early 90s since.",
            "Our God is an you're in estrus book on Interior Point methods because they use this to derive or to extend linear programming, primal dual methods, or primal and dual methods to general nonlinear convex optimization.",
            "So the idea is that we write a general convex optimization problem as a linear program with generalized inequality constraints.",
            "So this will be a linear program in standard form we minimize the inner product of CNX.",
            "We have equality constraints on X, and then X is non negative with respect to some convex cone.",
            "So X negative simply means X is in the XL negative means X is in the corner.",
            "And then this will be the dual problem at the maximization problem A is the adjoint or the transpose of A and an inequality in the dual is the inequality respect to the dual corner.",
            "So that's the definition of a primal and dual cone program.",
            "So this is has been widely used since the 90s and then very quickly people settled on three types of cones in particular."
        ],
        [
            "So the standard non negative orthant that gives us linear programming.",
            "Then the 2nd order cone, which is defined like this, is set of vectors.",
            "That satisfies this inequality U.",
            "The first component is greater than the Euclidean norm of the second.",
            "The rest of the elements.",
            "And then the semidefinite cone.",
            "And the reason why people worked on these three cones in particular is that they're not their self dual and they also are more than self dual.",
            "They satisfy some symmetry properties that make it possible to define or formulate symmetric primal dual methods for these three cones.",
            "And also form 3 levels of complexity.",
            "So semidefinite programming is a more general problem because the others can be embedded in an SDP.",
            "And also it's important that the complexity, the linear algebra complexity is goes up from each step.",
            "So if you can formulate the problem as an SCP, it's always good to do it as an SCP and not to write it as an SCP, because that would be much more expensive.",
            "So that also."
        ],
        [
            "So is sometimes a little surprising or leads to some surprising or.",
            "Observations, because for example, if you take an SDP with band structure, so it's a standard SDP.",
            "And the matrix C and the cost function and all the constraints or bandits?",
            "Then we know that if the bandwidth is 1, if it's a diagonal matrix, then it's just an LP and the cost of an interior point methods.",
            "If everything else is fixed, linear algebra cost of the interior point method is linear in the number of dimension of X.",
            "If you take a bandit SDP, even with small bandwidth, and you try any of the standard SDP solvers, then you see that the cost is grows, at least as in squared instead of in.",
            "And that's a little counterintuitive, because you would expect that, for example, a tridiagonal SDP is not much harder to solve than a diagonal one in terms of the linear algebra after iteration.",
            "But the reason is that all these methods choose the SDP they bandits.",
            "SDP cannot be embedded in an SCP, so you have to go to SDP, the highest level of complexity.",
            "And then that's the complexity of an SDP solver exploiting."
        ],
        [
            "Or city in the coefficients.",
            "So an alternative, so that's motivates this these two problems, so we're interested in solving cone programming problems over the two matrix cones that I defined.",
            "The cone of positive semidefinite matrices with a given sparsity pattern.",
            "And the cone of computable matrices with that sparsity pattern.",
            "So user this cone computable matrix cone as a primal cone and the other one is a dual cone.",
            "So this actually corresponds also.",
            "So now we have a non self dual pair of optimization problems because the cones are not equal.",
            "So we cannot use a primal dual symmetric methods, but we can still use a primal method or a dual interior point method.",
            "So the question here that we try to answer mostly experimentally.",
            "Is the following.",
            "You have two approaches you can think of two approaches of exploiting or solving these two problems.",
            "One is to follow the standard method of embedding these cones in the general positive semidefinite.",
            "So do some positive semidefinite cones in primal and dual.",
            "And then exploit sparsity and coefficient matrices to exploit this sparsity structure.",
            "So then you solve it as."
        ],
        [
            "The current pair of sparse DPS and then you can apply a primal dual symmetric method.",
            "A difficulty in implementing it is that the primal variable X, the matrix variable is in general dense.",
            "Even if the matrix is C&A, are very sparse, and even if they have a code of sparsity pattern, so the primal variable will be dense.",
            "And if it's in the matrix dimension is very large.",
            "You have to be careful how you implement the interior point method.",
            "To avoid having to deal with dense matrix X.",
            "And the dual problem if all the coefficients A&CI have a sparsity pattern, then we can also restrict S to the same sparsity pattern.",
            "So this will be sparse, but inverse is still dense in general, and again that complicates the formulation of primal dual interior point methods.",
            "So we can try to solve this DPS and then try to exploit the sparsity at the linear algebra level as much as you can.",
            "So that's the standard approach and semidefinite programming.",
            "So the other approach that we will discuss here is to solve them as these two problems as a pair of non self dual cone optimization problems.",
            "So the disadvantage is that we cannot use symmetric primal dual methods because the cones are not self dual.",
            "But we can still use primal or dual methods.",
            "We immediately get a natural sort of reduction of dimension because the dimension of the space of the inequality's are formulated.",
            "Is now just the number of non zeros in the pattern.",
            "So if you have a triangle or a banded matrix then the number of the dimension of this space grows linearly with the dimension of the matrix.",
            "And if yes coral, we can use these methods that I discussed before for evaluating the primal and dual gradients and their primal various and their first and second derivatives.",
            "So just one slide on Interior point methods.",
            "So all interior point methods, primal, dual, primal, dual.",
            "Try to follow the central path.",
            "Approximately.",
            "And then it by some variation of Newton's methods, and then at each step you have to solve a dense generally.",
            "Then set of linear equations called assure complement equation.",
            "So if user primal methods then typically the sure complement matrix here is defined like this.",
            "It's the inner product of.",
            "The coefficient matrices and then the inverse hessian of the barrier applied to the coefficient matrices.",
            "So we have to.",
            "That's the element in this matrix.",
            "For a dual scaling methods you get the dual very function and a similar definition.",
            "So we'll use the coral methods that we'll discuss before to evaluate these."
        ],
        [
            "Hession to apply these sessions or inverse sessions.",
            "Efficiently.",
            "So this is will give results for an implementation that is also available software and more benchmarks.",
            "So it follows a path following methods as the primal and dual version.",
            "And so this is less important.",
            "But we implanted implemented two techniques for solving this should complement equation.",
            "One is just."
        ],
        [
            "Build this matrix H and formats do Cholesky factorization.",
            "Another one is equivalent to the augmented system approach in optimization.",
            "So where we directly get the factorization of H without actually forming it explicitly.",
            "And we'll compare with Interior Point methods for sparse SDP and always list the linear algebra upper step preparation, because that's easier to compare.",
            "So if you first look at 2."
        ],
        [
            "Examples that really have a quarrel sparsity pattern.",
            "So we've seen a band structure is coral and then these two lines here give the complexity of the implementation of the coral approaches with the two methods for solving the short complement equation.",
            "So we see that as expected.",
            "Now the complexity is linear.",
            "As a function of the matrix size.",
            "So here we use matrices of order N. The bandwidth is fixed and also the number of constraints and the DP is fixed, so all dimensions are fixed except the size of the matrix.",
            "And then we see it's linear and other the other packages, as we've seen before, have a higher complexity.",
            "Another interesting example."
        ],
        [
            "That actually has a coral sparsity pattern is.",
            "Matrix norm minimization.",
            "So this is the second example.",
            "So we take a matrix mapping F. So FA Maps vector X2 matrix of size P * Q.",
            "If you're interested in minimizing the matrix norm of this FN function of X.",
            "Of the affine matrix function of X, so that can be written as an SDP.",
            "You have to introduce a new variable P and you minimize these subject to matrix inequality constraint.",
            "And this has an arrow block pattern.",
            "Because this F X + G is in a matrix.",
            "The transpose in this block, and then if you fill the smaller of these two identity matrices, then it's actually a block arrow pattern.",
            "We've seen that's coral.",
            "If I had a matrix, the number of columns is just one, then this reduces to just of course least squares problem.",
            "But then here we would have an SCP.",
            "Where we have only, which is equivalent to an arrow pattern, which is the single column.",
            "So this is an extension of SCP.",
            "If you like to multiple columns.",
            "And it's also interesting because this type of.",
            "Sparsity pattern also rises in other interesting problems, especially in robust optimization."
        ],
        [
            "So again, if we generate random problems with all dimensions fixed except the width of the.",
            "This the dense columns.",
            "Here the number of dense columns in the block arrow pattern.",
            "Then we see that this gives us a method that's linear in the size of the complexity linear in the size of the matrix and other packages are.",
            "I have a complexity increases more rapidly.",
            "So these are two."
        ],
        [
            "Examples of bandit and matrix norm minimization, where the sparsity pattern is actually coral.",
            "You can also use it for non correspond sparsity patterns if you first embed the sparsity pattern in coral sparsity pattern.",
            "And in practical way of method for doing this is to 1st apply a fill reducing ordering.",
            "From salesky factorization packages.",
            "And then compute a symbolic Cholesky factorization, and then the sparkly pattern of the Cholesky factor will actually be an embedding of the coral embedding of the original sparsity pattern.",
            "So we generated some problems with sparsity parents from an set of test matrices.",
            "The Rangers go from thousand 2000 to 30,000.",
            "We took a small number of constraints and random sparse data, so all the coefficients are very sparse within relative to the sparsity pad.",
            "Have you looked at two embeddings?",
            "Actually, the AMD embedding and then the variation on it.",
            "So these are two smaller for smaller problems from range from size 2000 to 4000.",
            "These are actually the results of the embedding."
        ],
        [
            "So you see that the both embeddings increase the sparse the number of nonzeros.",
            "Obviously the second one increases it more than the first one.",
            "But still the second embedding will give us better results because this second embedding groups some small cliques together to improve the efficiency.",
            "And then the other.",
            "So these are the times preparation for the coral.",
            "Methods.",
            "The other packages have complexity at various widely sode SDP does very well on these problems because they are very sparse.",
            "Andy SDP uses and.",
            "Low rank exploits low rank structure in the matrices so."
        ],
        [
            "Very well on these problems because the techniques it applies actually work very well on these matrices.",
            "So in this order, larger problems with the largest one is size 30 thousands.",
            "And so these are the results for the coral methods.",
            "So the conclusion is that we can actually solve quite large handle, very large sparse matrices if they can be efficiently embedded in a coral sparsity pattern.",
            "And.",
            "There are many other techniques you would need to use to really.",
            "Solve wider range of large sparse as DPS, so in general you would like to combine it with for example to techniques and SDP that exploits low rank structure and so on."
        ],
        [
            "So this is a summary of this second part of the talk, so we will talk.",
            "We discussed cone programming problems with coral matrix cones.",
            "And it's interesting, I think, for a number of reasons.",
            "One is, it's includes the standard cones, linear Programming, 2nd order cone, and SDP and all combinations compositions of them.",
            "So basic and look at all these standard cones as just special cases of one single type of matrix cone is the coral sparse matrix cones.",
            "It's interesting because there are some interesting applications of sparsity patterns that actually are coral.",
            "For example, the block arrow sparsity patterns that are useful in robust optimization.",
            "And it's also useful for general sparse semidefinite programming becausw.",
            "It works well, at least the experiments that.",
            "We run.",
            "And so in combination with other sparse sparse exporting SDP techniques, it's can be very useful.",
            "And the benchmark or a full set of benchmarks in the paper are also available from our website.",
            "So then the last part of the talk is shorter, but and it's sort of recent work that's not quite finished and a little preliminary, but I."
        ],
        [
            "I wanted to include it because it's more directly related to machine learning.",
            "Done the same."
        ],
        [
            "Definite programming topic.",
            "So the problem is as follows.",
            "So we know that in machine learning and kernel techniques, then often you have to solve a very large dense.",
            "Optimization problem, for example, quadratic programming problem made it very large dense.",
            "Hessian matrix Q.",
            "And with interior Point methods that gets very expensive if the number of training vectors.",
            "So the size of Q.",
            "It's about say 10,000 because at each iteration of an interior point method you have to solve and positive definite set of linear equations.",
            "With coefficient Q + D, so that's very expensive.",
            "If Q is dense and.",
            "And is large.",
            "So then of course, there are several techniques that people have discussed too.",
            "Be able to solve large dance kewpies like this.",
            "So one method for example is to approximate Q by a low rank mate."
        ],
        [
            "Six or by a diagonal plus low rank matrix.",
            "Another technique is of course to use first order methods instead of interior point methods.",
            "But here we wanted to try something similar to the low rank idea.",
            "And the question is, how well would it try if you replaced Q with an approximation that has a sparse inverse?",
            "So Q is denser, will approximated by a dense matrix, but with the sparse inverse, because we have a very natural or simple way to approximate a matrix with the sparse inverse.",
            "If we choose the sparsity pattern of the universe to be coral.",
            "So that's this is what we'll try, so will replace Q with Q~ and approximation and Q till is actually the inverse of a sparse matrix.",
            "S&S is defined by solving this optimization problem.",
            "So he fixed queue, that's the actual kernel matrix.",
            "You solve this optimization problem in S. That's a sparse matrix or with the constraint that it's sparse with your given sparsity pattern.",
            "And then the inverse of S is the approximate kernel matrix that will use.",
            "And then as some interpretation.",
            "So if you interpret Q as a covariance of and Gaussian.",
            "Distribution, then Q filter will be the growth and distribution covariance of the Gaussian distribution that's closest to queue in relative entropy.",
            "Or Cuba.",
            "Collaborative vergence subject works partially constraint on the inverse of Q~ so that's one interpretation, another one with that Q~ So the approximate kernel is the maximum determined completion of Q.",
            "Or the projection of Q on the sparsity problem.",
            "So for example, if you choose a bandit pattern, as will do in the experiments.",
            "Then it means you evaluate the kernel matrix on the band in the diagonal band.",
            "You don't evaluate the rest of the kernel matrix, but you completed with maximum determined positive definite completion.",
            "And then that approximate Q is or that Q is the cutillo that we use in the.",
            "QP.",
            "And of course it's also known as covariant selection.",
            "So again, if you choose fee to be coral, for example banded, then that's interesting for two reasons.",
            "So Q till the inverse has a zero fill in salesky factorization, so that's useful in the interior point methods.",
            "And also we can compute the Q~ implicitly.",
            "But actually the Cholesky factors directly from the projection of Q on the sparsity pattern by recursion over the click three, we only have to know actually projection of Q on the sparsity pattern to do that."
        ],
        [
            "So then if you use that look, go back to the QP so the SVM QP with the proximate Q~ so Q till there's a dense matrix but has a sparse inverse.",
            "Then you can each iteration in the Interior Point solver becomes quite cheap because you can write the.",
            "The equations you need to solve like this as the universe of Q~ plus a diagonal.",
            "As coefficient matrix.",
            "So if you kill, the invert has a sparse.",
            "Go to sparsity pattern, then can it can be factored with hero philenor.",
            "This entire matrix can be factored with zero filling so we can do this very efficiently and you can solve very large QPS even though they are dense.",
            "Util is still dense.",
            "And can be very large, but of course we use the fact that the inverse of Q~ is spot.",
            "So we have one experiment on the M. This data set where we use the full data set of 60,000 examples.",
            "So the idea here is that we tried to use the full training set.",
            "And still use an interior point method.",
            "And what we tried here is to solve the problem."
        ],
        [
            "Stages so first we take the full training set.",
            "We are applied as used approximate kernel.",
            "The post the completion kernel with very small bandwidth of 1 hundreds.",
            "And then so the Q pivot is approximate completion queue till then.",
            "Then the next step is we take the support vectors from the first solution and solve a smaller problem with only those support vectors as training sets.",
            "And a larger bandwidth of 400 in the second step, and again solve the QP with the bandits or the inverse bandit.",
            "Matrix utility.",
            "And then the last step we saw the full QP with only the support vectors from Step 2.",
            "So this is the error we obtain after.",
            "Of course, the entire combination of three."
        ],
        [
            "It is only an approximation approximate solution of the actual QP.",
            "So, but if you compare with actually the solution computed of the full QP computed with SVM for the same values of the parameters and, and so on.",
            "I see that the number of support vectors we obtain at the end is compatible and also the error is compatible.",
            "So that's the number of reported active Error 3.",
            "One interesting thing is in this first stage we approximate kernel.",
            "Which we are inverse bandits.",
            "So that the number of supporters actually larger than the actual QP.",
            "And I need to go down in each of these stages when the error in the first few stages are errors that are actually for a classification based on classifier data.",
            "So if you solve the approximate fees of several ways to result in a classification and this is for one of all.",
            "Really try other many other values of parameter give you the parameters.",
            "Women for the other.",
            "If you don't think with this one.",
            "But we didn't really do more parents the.",
            "Choose the best values of the parameters and also maybe more stages.",
            "CPU time this is the CPU time of the state."
        ],
        [
            "So the first stage you look so happy with 60,000 variables, but it's had an inverter bandit.",
            "Q Pickup told quite quickly and cost of course is the same throwables go is the problem of the same dimension with the same bandwidth, so the difference is just you too.",
            "This new problem is taking one more recreation and the others.",
            "Then in stage two we use the support vector force from stage one's training sets, so that's the size of the problem.",
            "We solve with bandwidth 400.",
            "And then the time is it's more expensive.",
            "And ideally per iteration, that should be still linear in because the bandwidth is fixed and in a stage three we saw the full QP with the support vectors of stage one as training set and then the problem is it's a smaller problem, so it's quite fast approval and that only creases of the group.",
            "Pair the first one with number 4, then you see that's exactly 8 because it's twice as large.",
            "So the combination of the three is actually compatible again with lip SVM, or actually faster on these examples and SVM, even though we use in interior point methods for solving these problems.",
            "So that."
        ],
        [
            "That's it, the to summarize the to talk.",
            "We looked at some applications of matrix techniques that involve exploit coral sparsity.",
            "So the key idea is that there are several interesting matrix problems that are solved using finite recursive algorithms.",
            "If this party partners coral, so one is a Cholesky factorization with zero filling.",
            "Then also the computation of the values and the gradients and hessians of the two barrier functions.",
            "So very function for the cone of positive semidefinite matrices with the given sparsity pattern, and then the completable matrices.",
            "And then there's these techniques can also be used for non coral patterns in combination with sort of coral embedding methods.",
            "And I discuss two applications.",
            "One is interior point methods for these two sparse matrix account.",
            "And then also in general is an interesting application optimization is I think the approximating dense, very large dense matrices.",
            "So for example hessions of optimization problems, convex optimization problems by.",
            "Dense approximations that have a sparse inverse.",
            "Thank you.",
            "Yeah.",
            "Well, there's a basic result.",
            "For example, for a normal graphical model that the maximum likelihood estimate of the covariance.",
            "Has a simple.",
            "Recursive solution if or almost closed form solution if the sparsity pattern is coral.",
            "So it's used.",
            "Open gas, also for other problems.",
            "Remind yeah so for us it indeed this positive path.",
            "It's.",
            "There's a.",
            "And also to find order easier.",
            "Human resources and everything.",
            "Mindy approximate kernel is defined as a solution of an SD card.",
            "That's a very simple is required because the problem that we have to solve to find the approximate kernel is basically the same as the positive definite completion for a coral matrix, and in that case it's easy if you choose a non coral pattern then you would have to solve an optimization problem numerically to find this completion and then it wouldn't be interesting.",
            "But for example, for bandit it's quite easy.",
            "Recently I heard.",
            "Scalability.",
            "Yeah, look it up.",
            "General.",
            "To reduce it or two.",
            "Yeah, you can always embed it and then for example in SDP set all these extra coefficients to 0.",
            "So you introduce possible non zeros.",
            "But then, for the SDP is still equivalent.",
            "It's not an approximation.",
            "Because all the coefficient matrices are.",
            "Spot, so even if you embedded in a full dense matrix and by end symmetric matrices, it will still be equivalent."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not just for inviting me for this session.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with Martin Anderson, who is a PhD student at UCLA and welcomed Al who is currently at anybody technology in Denmark.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the talk will be on large scale convex optimization using interior point methods.",
                    "label": 0
                },
                {
                    "sent": "Ann will exploit results on coral sparsity that are actually very well known in machine learning, so this topic probably fits best with the third question in the on the list on using it on using techniques that are actually well known in related to well known techniques and machine learning for large scale optimization.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with an introduction of coral sparsity, and then later in the talk I'll look at two applications in more detail, semidefinite programming and then also dense quadratic programming.",
                    "label": 1
                },
                {
                    "sent": "So a chordal graph is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Defined as follows.",
                    "label": 0
                },
                {
                    "sent": "It's an undirected graph.",
                    "label": 0
                },
                {
                    "sent": "And it's called coral.",
                    "label": 0
                },
                {
                    "sent": "If every cycle of length greater than three has a court, so an edge joining too.",
                    "label": 0
                },
                {
                    "sent": "Nonadjacent edges nonadjacent nodes, so this would be.",
                    "label": 0
                },
                {
                    "sent": "Non chordal because that's a cycle of length greater than three that doesn't have a record.",
                    "label": 0
                },
                {
                    "sent": "This is a chordal graph because every cycle of length greater than three has occurred.",
                    "label": 1
                },
                {
                    "sent": "So it's also known as triangulated or decomposable and other names.",
                    "label": 0
                },
                {
                    "sent": "So if this refers to sparsity pattern, then the graph represents the sparsity in a symmetric matrix, so the nodes correspond to the rows and columns in the matrix.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edges represent non zeros in the graph, so we have an edge between two nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "If there is a non zero in position.",
                    "label": 0
                },
                {
                    "sent": "That position in the graph.",
                    "label": 0
                },
                {
                    "sent": "In the matrix.",
                    "label": 0
                },
                {
                    "sent": "So these are two examples.",
                    "label": 0
                },
                {
                    "sent": "Basic examples of coral sparsity patterns that are not.",
                    "label": 0
                },
                {
                    "sent": "Very interesting as sparsity patterns, but they will be important for applications later in the talk.",
                    "label": 0
                },
                {
                    "sent": "And in some sense of the most basic or Canonical coral sparsity patterns, one is an arrow pattern where you have a diagonal matrix and then a few dense a number of one or more dense columns and rows, and this would be the corresponding graph and it can easily check that's horrible graph.",
                    "label": 0
                },
                {
                    "sent": "So an arrow or a block arrow sparsity pattern is in coral sparsity pattern.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example is a band matrix which corresponds to this graph, so again, you can easily check from the graph that it's coral.",
                    "label": 0
                },
                {
                    "sent": "And so that's another example of a coral sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "This would be less obvious if you look at the sparsity pattern, so this is a sparsity pattern of a graph of a matrix of order 17.",
                    "label": 0
                },
                {
                    "sent": "This is the corresponding node graph, and then you can see certainly that this part is triangulated or coral, and here is a click and from the graphic and also see that it's coral.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So coral graphs of course.",
                    "label": 0
                },
                {
                    "sent": "I've have been known in different areas for a long time, specially in linear algebra and also in machine learning and statistics.",
                    "label": 0
                },
                {
                    "sent": "In linear algebra there.",
                    "label": 0
                },
                {
                    "sent": "Known for the most maybe most important property is that if a positive definite matrix has a coral sparsity pattern, then it can be factored using salesky factorization with zero filling.",
                    "label": 0
                },
                {
                    "sent": "And that's a very practical, useful equivalent definition of a coral sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "Coral sparsity means exactly that it can be factored with zero filling.",
                    "label": 0
                },
                {
                    "sent": "If you use the right elimination ordering.",
                    "label": 0
                },
                {
                    "sent": "And that's been known since the 70s.",
                    "label": 0
                },
                {
                    "sent": "There is a related problem in linear algebra that's known as the positive definite matrix completion problem, and also that will discuss later, and it also has a simple solution if the sparsity pattern of the specified part in the matrix is coral.",
                    "label": 0
                },
                {
                    "sent": "In machine learning, it's rises in the graphical models for example, and maximum likelihood estimation in Gaussian graphical model has a simple solution if the sparsity pattern or the topology of the graph is squirrel, and that's basically equivalent to the positive definite matrix completion in the Roger Brown.",
                    "label": 1
                },
                {
                    "sent": "Related problem is Euclidean distance completion problem has been studied and also in machine learning.",
                    "label": 1
                },
                {
                    "sent": "And in optimization, it's been used extensively for sparse semidefinite programming, but people have developed techniques for exploiting coral sparsity in interior point methods for sparse semidefinite programming.",
                    "label": 1
                },
                {
                    "sent": "There are also a few papers where people try to use coral techniques for sparse preconditioners, for example, and sparse quasi Newton updates.",
                    "label": 0
                },
                {
                    "sent": "They tried to use the linear algebra techniques related to chorales.",
                    "label": 1
                },
                {
                    "sent": "Sparsity, too, for example define or derive sparse preconditioners.",
                    "label": 0
                },
                {
                    "sent": "So many of these.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms and techniques I will see are most easily explained if you represent the graph by click tree.",
                    "label": 0
                },
                {
                    "sent": "And so this is some some definition.",
                    "label": 0
                },
                {
                    "sent": "A click, of course, is a maximal complete sub graph in the matrix in the graph.",
                    "label": 0
                },
                {
                    "sent": "And then the clicks of a chordal graph has a very very interesting property.",
                    "label": 0
                },
                {
                    "sent": "So they can be ordered in a tree with as its nodes.",
                    "label": 0
                },
                {
                    "sent": "The cliques in a graph and they the tree has this so called running intersection property.",
                    "label": 0
                },
                {
                    "sent": "And that means that in this tree, the intersection of a click with its parents in the tree.",
                    "label": 0
                },
                {
                    "sent": "Is equal to the intersection of the click with the Union of all the cliques that preceded in the tree.",
                    "label": 0
                },
                {
                    "sent": "So that's a basic property that follows from coral structure, and that's not not true for general sparsity patterns.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this would be an example that illustrates this property.",
                    "label": 0
                },
                {
                    "sent": "This is the core of sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "Of order 17.",
                    "label": 0
                },
                {
                    "sent": "These are all the clicks in the in the graph.",
                    "label": 0
                },
                {
                    "sent": "Ordered in a tree.",
                    "label": 0
                },
                {
                    "sent": "And then there running intersection property.",
                    "label": 0
                },
                {
                    "sent": "For example for this click with the nodes 15678, which is.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to this complete or dense.",
                    "label": 0
                },
                {
                    "sent": "Diagonal sub lock on the of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So the running intersection property means that the intersection of this click with its parent, so that's nodes.",
                    "label": 0
                },
                {
                    "sent": "One, part six is also the same as in collection of this week, with all the clicks that preceded.",
                    "label": 0
                },
                {
                    "sent": "So if you take the union of the all the clicks that preceded.",
                    "label": 0
                },
                {
                    "sent": "And then the intersection with the click W, then you get the same intersection.",
                    "label": 0
                },
                {
                    "sent": "So that's an important property, because that is basically the reason why we can formulate algorithms for many problems that are related to coral graphs by local, by recursive algorithms over this click tree, very processor click Cemetery and topological order or reverse topological order, and for each click you do some operations on the click and it's parent or the click and it's children, and so that's how all these algorithms will discuss will work.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So too, so the basic most important result of COBOL classes in linear algebra is that it's possible to factor and positive definite matrix with the coral sparsity pattern with zero fill in.",
                    "label": 0
                },
                {
                    "sent": "So that depends on certain you have to choose.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The right elimination order and this would be an example of a coral sparsity pattern that's ordered using a perfect elimination order.",
                    "label": 1
                },
                {
                    "sent": "So we took the client to obtain such an order.",
                    "label": 0
                },
                {
                    "sent": "You can take the click tree number, the clicks in a topological order starting at the root.",
                    "label": 0
                },
                {
                    "sent": "And then number in the notes and each click consecutively.",
                    "label": 0
                },
                {
                    "sent": "So we started 123, then the next would be this one.",
                    "label": 0
                },
                {
                    "sent": "So the new node in this click is gets the next number.",
                    "label": 0
                },
                {
                    "sent": "Then you take the next click is the new nodes and that click get numbers 5, six and so on.",
                    "label": 0
                },
                {
                    "sent": "So if you and number the nodes in the graph in the graph or the rows and columns in the matrix like this, then he gets a perfect elimination order foreign Cholesky factorization as our thresholds where artist upper triangular.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm would.",
                    "label": 0
                },
                {
                    "sent": "More less work like this.",
                    "label": 0
                },
                {
                    "sent": "You start at the last node in the in the last click or the end of the matrix and then you can eliminate the elements non zeros below the diagonal.",
                    "label": 1
                },
                {
                    "sent": "With the usual method for Cholesky factorization and that will introduce no filling in the upper triangular part.",
                    "label": 0
                },
                {
                    "sent": "And you can just fill in the non zeros and start via recursion on this click tree starting at the end node filled in on zero elements with the Cholesky factor of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I'll skip the details, but the important thing is that the to obtain such as salesky factorization you follow a recursion on the click tree.",
                    "label": 0
                },
                {
                    "sent": "And you process the clicks in reverse topological order.",
                    "label": 1
                },
                {
                    "sent": "So you will start at the end node.",
                    "label": 0
                },
                {
                    "sent": "In the click tree and then visit the clicks in reverse topological order.",
                    "label": 0
                },
                {
                    "sent": "So that's all we'll need for.",
                    "label": 0
                },
                {
                    "sent": "As for background on coral sparsity.",
                    "label": 0
                },
                {
                    "sent": "And the next section.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about some additional results and also some connections with convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So many of these results that are related to coral.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sparsity patterns of matrices.",
                    "label": 0
                },
                {
                    "sent": "Fit nicely in convex optimization framework where you use cone programming as a standard.",
                    "label": 0
                },
                {
                    "sent": "Format for convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So these are some definitions that will use, so SN is a set of symmetric matrices of order N. And then a subscript V will denote a sparsity pattern, so it's the set of all symmetric matrices of order in with fixed sparsity pattern V. So for example, they could be banned it, and then it's a space of all banded matrices with that bandwidth.",
                    "label": 0
                },
                {
                    "sent": "For example, tridiagonal matrices of order.",
                    "label": 0
                },
                {
                    "sent": "So we always assume that the non zeros in V included diagonal elements, so that's needed for some.",
                    "label": 0
                },
                {
                    "sent": "Natural in all these applications and it's also needed for some technical conditions.",
                    "label": 0
                },
                {
                    "sent": "And the P of with subscript fee denotes the projection of a matrix on the sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "So for example, if the sparsity pattern is tridiagonal.",
                    "label": 0
                },
                {
                    "sent": "You project the Matrix on a tridiagonal pattern by setting everything outside the three diagonals equal to 0.",
                    "label": 0
                },
                {
                    "sent": "That's what the projection on that sparsity pattern means.",
                    "label": 0
                },
                {
                    "sent": "And in all of this.",
                    "label": 0
                },
                {
                    "sent": "Sparsity pattern defines the possible non zeros in the matrix.",
                    "label": 0
                },
                {
                    "sent": "So if we say an element is a non zero it means it can be.",
                    "label": 0
                },
                {
                    "sent": "It's allowed to be non 0.",
                    "label": 0
                },
                {
                    "sent": "If you say in Element Zero in a sparsity pattern then it's always has to be 0.",
                    "label": 0
                },
                {
                    "sent": "So with this definitions we can define 2 interesting matrix cones that are convex.",
                    "label": 0
                },
                {
                    "sent": "One is the set of positive semidefinite matrices with that given sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "For example, triangle positive semidefinite matrices.",
                    "label": 0
                },
                {
                    "sent": "And we'll denote that with an extra subscript plus.",
                    "label": 0
                },
                {
                    "sent": "And the second cone is the set of matrices with that sparsity pattern that have a positive semidefinite completion.",
                    "label": 0
                },
                {
                    "sent": "I will use this subscript C to denote the second column.",
                    "label": 0
                },
                {
                    "sent": "So with this projection rotation we can easily write the second cone as follows.",
                    "label": 0
                },
                {
                    "sent": "It's the projection of all positive semidefinite matrices on the sparsity Pattern V. That's the set of all matrices that have a positive semidefinite completion.",
                    "label": 1
                },
                {
                    "sent": "And that sparsity pattern V. And then it turns out that these cones are dual cone for the usual inner product, so I'll use the trace of the product of the two matrices and I'll use this notation.",
                    "label": 0
                },
                {
                    "sent": "That is quite standard in optimization, as this dot product is the inner product of two symmetric matrices.",
                    "label": 0
                },
                {
                    "sent": "So there are dual cones using the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The standard definition of dual cone, so the matrices in these pair of matrices in these two cones always have a non negative inner product.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you take triangle sparsity pattern, then that's the matrix with a tridiagonal sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "In R3 it's not positive semidefinite, so it's not in the first cone.",
                    "label": 0
                },
                {
                    "sent": "But it has a positive semidefinite completion because if we replace the zero with one, we get a positive semidefinite matrix.",
                    "label": 1
                },
                {
                    "sent": "And that's a simple example that shows that the two cones are not equal, so they're not self dual in general.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So they're not equal and there are dual dual pair of cones.",
                    "label": 0
                },
                {
                    "sent": "So if you want to formulate optimization problems in terms of in a convex cone programming format, then the ingredients we need to formulate interior point methods are that for each cone, the primary dual cone.",
                    "label": 0
                },
                {
                    "sent": "The important and overlooked value function and be able to evaluate its gradient and its Hessian.",
                    "label": 0
                },
                {
                    "sent": "So if we can do this for these two cones.",
                    "label": 0
                },
                {
                    "sent": "So if you start with the post, it's me.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If not, then we can just use the standard local rhythmic barrier function for positive semidefinite matrices.",
                    "label": 0
                },
                {
                    "sent": "So minus logged out of S. And the only difference here is we restrict S to the subspace of matrices where the given sparsity pattern is, for example the tridiagonal positive semidefinite matrices.",
                    "label": 0
                },
                {
                    "sent": "Then the gradient is defined like this.",
                    "label": 0
                },
                {
                    "sent": "It's the projection of the inverse of S on the sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "So if we take this function but on the set of all positive semidefinite matrices, then the gradient is just the negative of the inverse of S. If you restrict the function to given sparsity pattern and the gradient is the projection of the gradient on that sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "So that's a first important question.",
                    "label": 0
                },
                {
                    "sent": "If we want to use this barrier function.",
                    "label": 0
                },
                {
                    "sent": "On this matrix cone we have to be able to evaluate this gradient efficiently.",
                    "label": 0
                },
                {
                    "sent": "And that means we have to.",
                    "label": 0
                },
                {
                    "sent": "We're interested in the elements of the inverse of the matrix.",
                    "label": 0
                },
                {
                    "sent": "But only the elements in the sparsity pattern of S. So if V is a tridiagonal sparsity pattern.",
                    "label": 1
                },
                {
                    "sent": "Then S would be a positive definite tridiagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "Its inverse is dense in general.",
                    "label": 0
                },
                {
                    "sent": "But we're only interested in the elements of the inverse on the three diagonals.",
                    "label": 0
                },
                {
                    "sent": "And so that will be important to just be able to compute those elements without computing all the rest of the matrix that we have.",
                    "label": 0
                },
                {
                    "sent": "The inverse that we're not interested in.",
                    "label": 0
                },
                {
                    "sent": "And then if so, the importance of coral sparsity pattern will be exactly that.",
                    "label": 0
                },
                {
                    "sent": "If V is, coral is the sparsity pattern is coral, then there exists a simple recursive methods.",
                    "label": 0
                },
                {
                    "sent": "For evaluating this gradients.",
                    "label": 0
                },
                {
                    "sent": "No rush to evaluate the projection of the inverse of the sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "So for example, if for a bandits sparsity pattern you can buy via recursion on the click tree, compute the elements of the universe in the band Diagonal Band without having to compute any other elements in the in the inverse.",
                    "label": 0
                },
                {
                    "sent": "And that is only true for.",
                    "label": 0
                },
                {
                    "sent": "That's true for coral sparsity patterns.",
                    "label": 0
                },
                {
                    "sent": "Skip the method, but it's very similar to the Cholesky factorization method.",
                    "label": 0
                },
                {
                    "sent": "And then the Hessian in Interior Point methods is also important to be able to evaluate the Hessian, or at least apply the Hessian.",
                    "label": 0
                },
                {
                    "sent": "So the this denotes the Hessian matrix is applied to a symmetric symmetric matrix Y.",
                    "label": 0
                },
                {
                    "sent": "So that's defined like this.",
                    "label": 0
                },
                {
                    "sent": "We compute the inverse times Y times as inverse, and then again we have to project this on the sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "And it's important to be able to at least evaluate this quantity for a given Y in a given S. Again, without evaluating any elements outside the sparsity pattern, and also without requiring the full inverse of S. And again, that's possible if the sparsity pattern is coral, and that's why we are interested in coral sparsity patterns.",
                    "label": 0
                },
                {
                    "sent": "And so I'll skip the methods, but you can think of this as following from the chain rule of differentiation, because the Cholesky factorization with zero filling it gives us an efficient method for evaluating the barrier function computers, Cholesky factorization and take the logarithms of the diagonal elements in the factor.",
                    "label": 0
                },
                {
                    "sent": "And then by the chain rule, it's not surprising that you can always also take the gradient or the Hessian VR similar recursion.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's for the first one.",
                    "label": 0
                },
                {
                    "sent": "The second cone is the cone of completable matrices.",
                    "label": 0
                },
                {
                    "sent": "And then we can use the standard definition of a dual cone corresponding to account for the primal and it's given by the conjugate or the genre transform of the primal barrier function.",
                    "label": 0
                },
                {
                    "sent": "So that's defined like this as an optimization optimal value of an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Where we if you want to evaluate the dual, very function at C. So that's the matrix with a positive semi definite or positive definite completion.",
                    "label": 0
                },
                {
                    "sent": "Then we have to maximize this function over S where S and is in the primal cone.",
                    "label": 0
                },
                {
                    "sent": "And so the result of this optimization problem gives us the dual barrier function.",
                    "label": 0
                },
                {
                    "sent": "And it can also be written like this if this hat is the matrixes that maximizes this for a given Z.",
                    "label": 0
                },
                {
                    "sent": "So there's different ways of writing this solution of this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's also from the optimality conditions.",
                    "label": 0
                },
                {
                    "sent": "You can write it like this.",
                    "label": 0
                },
                {
                    "sent": "The optimal S that maximizes this function for a given Z satisfies this condition, so you fix Z and it solves this nonlinear equation in South, so it's a sparse positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "You specify the projection of its inverse, and from that you want to compute the matrix as, so it's sort of the dual of the evaluation of the gradient of the primal barrier.",
                    "label": 0
                },
                {
                    "sent": "There were given an essay revaluate, See given us and that's a non linear equation in this.",
                    "label": 0
                },
                {
                    "sent": "Ann from duality and the optimality conditions you can also show that S had inverse is the maximum determined completion, positive definite completion of Z.",
                    "label": 1
                },
                {
                    "sent": "So another way to think of this value function is that you take the matrix Z.",
                    "label": 0
                },
                {
                    "sent": "You compute it's positive definite completion.",
                    "label": 0
                },
                {
                    "sent": "And then the.",
                    "label": 0
                },
                {
                    "sent": "Log out of that matrix gives us the do very function.",
                    "label": 0
                },
                {
                    "sent": "And then from results on Legendary transforms and conjugate transforms, you also get the gradient question.",
                    "label": 0
                },
                {
                    "sent": "You know this matrix is had.",
                    "label": 0
                },
                {
                    "sent": "So again, the conclusion is that in general, for general sparsity patterns to evaluate this dual very function, you will have to solve this optimization problem numerically.",
                    "label": 0
                },
                {
                    "sent": "For a coral sparsity pattern, you can actually solve this optimization problem almost analytically or by a simple finite recursion of the click tree.",
                    "label": 1
                },
                {
                    "sent": "And that gives us the value of the barrier function and also the gradient and Hessian.",
                    "label": 0
                },
                {
                    "sent": "So that's what we need in the second part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Interesting properties of coral sparsity patterns for these applications or that we have a number of matrix computations that are very easy to solve.",
                    "label": 0
                },
                {
                    "sent": "The sparse sparsity patterns, coral and it can be solved by recursions.",
                    "label": 0
                },
                {
                    "sent": "Finite recursions of in the click tryan topological order or reverse topological order.",
                    "label": 0
                },
                {
                    "sent": "And that's the list of the problems that for which this is true.",
                    "label": 0
                },
                {
                    "sent": "We had the Cholesky factor.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Station we can compute the projection of an inverse without computing other entries of the.",
                    "label": 1
                },
                {
                    "sent": "Inverse.",
                    "label": 0
                },
                {
                    "sent": "We can do the inverse of this gradient evaluation.",
                    "label": 0
                },
                {
                    "sent": "So given the projected inverse, we can compute matrix Y that has this projected inverse.",
                    "label": 0
                },
                {
                    "sent": "And then these two are basically linearizations of these two steps and they rise in the Hessian.",
                    "label": 0
                },
                {
                    "sent": "Passions for the Do 2 barrier functions.",
                    "label": 1
                },
                {
                    "sent": "And together they give us efficient methods for computing the gradient and hessian of algorithmic.",
                    "label": 0
                },
                {
                    "sent": "Very function of these two cones.",
                    "label": 0
                },
                {
                    "sent": "And we have a library of that implements algorithms for all these operations.",
                    "label": 0
                },
                {
                    "sent": "So in the second part of the talk, I want to apply this to two applications.",
                    "label": 0
                },
                {
                    "sent": "First sparse semidefinite programming.",
                    "label": 0
                },
                {
                    "sent": "And to introduce this.",
                    "label": 0
                },
                {
                    "sent": "Or start with some general discussion of convex optimization cone programming.",
                    "label": 0
                },
                {
                    "sent": "So the cone programming format for is a general format for convex.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimization.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's has been widely used since the early 90s since.",
                    "label": 0
                },
                {
                    "sent": "Our God is an you're in estrus book on Interior Point methods because they use this to derive or to extend linear programming, primal dual methods, or primal and dual methods to general nonlinear convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we write a general convex optimization problem as a linear program with generalized inequality constraints.",
                    "label": 0
                },
                {
                    "sent": "So this will be a linear program in standard form we minimize the inner product of CNX.",
                    "label": 0
                },
                {
                    "sent": "We have equality constraints on X, and then X is non negative with respect to some convex cone.",
                    "label": 0
                },
                {
                    "sent": "So X negative simply means X is in the XL negative means X is in the corner.",
                    "label": 0
                },
                {
                    "sent": "And then this will be the dual problem at the maximization problem A is the adjoint or the transpose of A and an inequality in the dual is the inequality respect to the dual corner.",
                    "label": 1
                },
                {
                    "sent": "So that's the definition of a primal and dual cone program.",
                    "label": 0
                },
                {
                    "sent": "So this is has been widely used since the 90s and then very quickly people settled on three types of cones in particular.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the standard non negative orthant that gives us linear programming.",
                    "label": 1
                },
                {
                    "sent": "Then the 2nd order cone, which is defined like this, is set of vectors.",
                    "label": 0
                },
                {
                    "sent": "That satisfies this inequality U.",
                    "label": 0
                },
                {
                    "sent": "The first component is greater than the Euclidean norm of the second.",
                    "label": 0
                },
                {
                    "sent": "The rest of the elements.",
                    "label": 0
                },
                {
                    "sent": "And then the semidefinite cone.",
                    "label": 0
                },
                {
                    "sent": "And the reason why people worked on these three cones in particular is that they're not their self dual and they also are more than self dual.",
                    "label": 0
                },
                {
                    "sent": "They satisfy some symmetry properties that make it possible to define or formulate symmetric primal dual methods for these three cones.",
                    "label": 0
                },
                {
                    "sent": "And also form 3 levels of complexity.",
                    "label": 1
                },
                {
                    "sent": "So semidefinite programming is a more general problem because the others can be embedded in an SDP.",
                    "label": 0
                },
                {
                    "sent": "And also it's important that the complexity, the linear algebra complexity is goes up from each step.",
                    "label": 1
                },
                {
                    "sent": "So if you can formulate the problem as an SCP, it's always good to do it as an SCP and not to write it as an SCP, because that would be much more expensive.",
                    "label": 0
                },
                {
                    "sent": "So that also.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is sometimes a little surprising or leads to some surprising or.",
                    "label": 0
                },
                {
                    "sent": "Observations, because for example, if you take an SDP with band structure, so it's a standard SDP.",
                    "label": 1
                },
                {
                    "sent": "And the matrix C and the cost function and all the constraints or bandits?",
                    "label": 0
                },
                {
                    "sent": "Then we know that if the bandwidth is 1, if it's a diagonal matrix, then it's just an LP and the cost of an interior point methods.",
                    "label": 1
                },
                {
                    "sent": "If everything else is fixed, linear algebra cost of the interior point method is linear in the number of dimension of X.",
                    "label": 0
                },
                {
                    "sent": "If you take a bandit SDP, even with small bandwidth, and you try any of the standard SDP solvers, then you see that the cost is grows, at least as in squared instead of in.",
                    "label": 0
                },
                {
                    "sent": "And that's a little counterintuitive, because you would expect that, for example, a tridiagonal SDP is not much harder to solve than a diagonal one in terms of the linear algebra after iteration.",
                    "label": 0
                },
                {
                    "sent": "But the reason is that all these methods choose the SDP they bandits.",
                    "label": 0
                },
                {
                    "sent": "SDP cannot be embedded in an SCP, so you have to go to SDP, the highest level of complexity.",
                    "label": 0
                },
                {
                    "sent": "And then that's the complexity of an SDP solver exploiting.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or city in the coefficients.",
                    "label": 0
                },
                {
                    "sent": "So an alternative, so that's motivates this these two problems, so we're interested in solving cone programming problems over the two matrix cones that I defined.",
                    "label": 0
                },
                {
                    "sent": "The cone of positive semidefinite matrices with a given sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "And the cone of computable matrices with that sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "So user this cone computable matrix cone as a primal cone and the other one is a dual cone.",
                    "label": 0
                },
                {
                    "sent": "So this actually corresponds also.",
                    "label": 0
                },
                {
                    "sent": "So now we have a non self dual pair of optimization problems because the cones are not equal.",
                    "label": 0
                },
                {
                    "sent": "So we cannot use a primal dual symmetric methods, but we can still use a primal method or a dual interior point method.",
                    "label": 0
                },
                {
                    "sent": "So the question here that we try to answer mostly experimentally.",
                    "label": 0
                },
                {
                    "sent": "Is the following.",
                    "label": 0
                },
                {
                    "sent": "You have two approaches you can think of two approaches of exploiting or solving these two problems.",
                    "label": 0
                },
                {
                    "sent": "One is to follow the standard method of embedding these cones in the general positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "So do some positive semidefinite cones in primal and dual.",
                    "label": 0
                },
                {
                    "sent": "And then exploit sparsity and coefficient matrices to exploit this sparsity structure.",
                    "label": 0
                },
                {
                    "sent": "So then you solve it as.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The current pair of sparse DPS and then you can apply a primal dual symmetric method.",
                    "label": 1
                },
                {
                    "sent": "A difficulty in implementing it is that the primal variable X, the matrix variable is in general dense.",
                    "label": 0
                },
                {
                    "sent": "Even if the matrix is C&A, are very sparse, and even if they have a code of sparsity pattern, so the primal variable will be dense.",
                    "label": 1
                },
                {
                    "sent": "And if it's in the matrix dimension is very large.",
                    "label": 0
                },
                {
                    "sent": "You have to be careful how you implement the interior point method.",
                    "label": 0
                },
                {
                    "sent": "To avoid having to deal with dense matrix X.",
                    "label": 0
                },
                {
                    "sent": "And the dual problem if all the coefficients A&CI have a sparsity pattern, then we can also restrict S to the same sparsity pattern.",
                    "label": 1
                },
                {
                    "sent": "So this will be sparse, but inverse is still dense in general, and again that complicates the formulation of primal dual interior point methods.",
                    "label": 0
                },
                {
                    "sent": "So we can try to solve this DPS and then try to exploit the sparsity at the linear algebra level as much as you can.",
                    "label": 0
                },
                {
                    "sent": "So that's the standard approach and semidefinite programming.",
                    "label": 0
                },
                {
                    "sent": "So the other approach that we will discuss here is to solve them as these two problems as a pair of non self dual cone optimization problems.",
                    "label": 1
                },
                {
                    "sent": "So the disadvantage is that we cannot use symmetric primal dual methods because the cones are not self dual.",
                    "label": 0
                },
                {
                    "sent": "But we can still use primal or dual methods.",
                    "label": 0
                },
                {
                    "sent": "We immediately get a natural sort of reduction of dimension because the dimension of the space of the inequality's are formulated.",
                    "label": 0
                },
                {
                    "sent": "Is now just the number of non zeros in the pattern.",
                    "label": 0
                },
                {
                    "sent": "So if you have a triangle or a banded matrix then the number of the dimension of this space grows linearly with the dimension of the matrix.",
                    "label": 0
                },
                {
                    "sent": "And if yes coral, we can use these methods that I discussed before for evaluating the primal and dual gradients and their primal various and their first and second derivatives.",
                    "label": 0
                },
                {
                    "sent": "So just one slide on Interior point methods.",
                    "label": 0
                },
                {
                    "sent": "So all interior point methods, primal, dual, primal, dual.",
                    "label": 0
                },
                {
                    "sent": "Try to follow the central path.",
                    "label": 0
                },
                {
                    "sent": "Approximately.",
                    "label": 0
                },
                {
                    "sent": "And then it by some variation of Newton's methods, and then at each step you have to solve a dense generally.",
                    "label": 0
                },
                {
                    "sent": "Then set of linear equations called assure complement equation.",
                    "label": 0
                },
                {
                    "sent": "So if user primal methods then typically the sure complement matrix here is defined like this.",
                    "label": 0
                },
                {
                    "sent": "It's the inner product of.",
                    "label": 0
                },
                {
                    "sent": "The coefficient matrices and then the inverse hessian of the barrier applied to the coefficient matrices.",
                    "label": 0
                },
                {
                    "sent": "So we have to.",
                    "label": 0
                },
                {
                    "sent": "That's the element in this matrix.",
                    "label": 0
                },
                {
                    "sent": "For a dual scaling methods you get the dual very function and a similar definition.",
                    "label": 0
                },
                {
                    "sent": "So we'll use the coral methods that we'll discuss before to evaluate these.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hession to apply these sessions or inverse sessions.",
                    "label": 0
                },
                {
                    "sent": "Efficiently.",
                    "label": 0
                },
                {
                    "sent": "So this is will give results for an implementation that is also available software and more benchmarks.",
                    "label": 0
                },
                {
                    "sent": "So it follows a path following methods as the primal and dual version.",
                    "label": 1
                },
                {
                    "sent": "And so this is less important.",
                    "label": 1
                },
                {
                    "sent": "But we implanted implemented two techniques for solving this should complement equation.",
                    "label": 0
                },
                {
                    "sent": "One is just.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Build this matrix H and formats do Cholesky factorization.",
                    "label": 0
                },
                {
                    "sent": "Another one is equivalent to the augmented system approach in optimization.",
                    "label": 0
                },
                {
                    "sent": "So where we directly get the factorization of H without actually forming it explicitly.",
                    "label": 0
                },
                {
                    "sent": "And we'll compare with Interior Point methods for sparse SDP and always list the linear algebra upper step preparation, because that's easier to compare.",
                    "label": 1
                },
                {
                    "sent": "So if you first look at 2.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examples that really have a quarrel sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "So we've seen a band structure is coral and then these two lines here give the complexity of the implementation of the coral approaches with the two methods for solving the short complement equation.",
                    "label": 0
                },
                {
                    "sent": "So we see that as expected.",
                    "label": 0
                },
                {
                    "sent": "Now the complexity is linear.",
                    "label": 0
                },
                {
                    "sent": "As a function of the matrix size.",
                    "label": 0
                },
                {
                    "sent": "So here we use matrices of order N. The bandwidth is fixed and also the number of constraints and the DP is fixed, so all dimensions are fixed except the size of the matrix.",
                    "label": 1
                },
                {
                    "sent": "And then we see it's linear and other the other packages, as we've seen before, have a higher complexity.",
                    "label": 0
                },
                {
                    "sent": "Another interesting example.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That actually has a coral sparsity pattern is.",
                    "label": 0
                },
                {
                    "sent": "Matrix norm minimization.",
                    "label": 0
                },
                {
                    "sent": "So this is the second example.",
                    "label": 0
                },
                {
                    "sent": "So we take a matrix mapping F. So FA Maps vector X2 matrix of size P * Q.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in minimizing the matrix norm of this FN function of X.",
                    "label": 1
                },
                {
                    "sent": "Of the affine matrix function of X, so that can be written as an SDP.",
                    "label": 0
                },
                {
                    "sent": "You have to introduce a new variable P and you minimize these subject to matrix inequality constraint.",
                    "label": 0
                },
                {
                    "sent": "And this has an arrow block pattern.",
                    "label": 0
                },
                {
                    "sent": "Because this F X + G is in a matrix.",
                    "label": 0
                },
                {
                    "sent": "The transpose in this block, and then if you fill the smaller of these two identity matrices, then it's actually a block arrow pattern.",
                    "label": 0
                },
                {
                    "sent": "We've seen that's coral.",
                    "label": 0
                },
                {
                    "sent": "If I had a matrix, the number of columns is just one, then this reduces to just of course least squares problem.",
                    "label": 0
                },
                {
                    "sent": "But then here we would have an SCP.",
                    "label": 0
                },
                {
                    "sent": "Where we have only, which is equivalent to an arrow pattern, which is the single column.",
                    "label": 0
                },
                {
                    "sent": "So this is an extension of SCP.",
                    "label": 0
                },
                {
                    "sent": "If you like to multiple columns.",
                    "label": 0
                },
                {
                    "sent": "And it's also interesting because this type of.",
                    "label": 0
                },
                {
                    "sent": "Sparsity pattern also rises in other interesting problems, especially in robust optimization.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, if we generate random problems with all dimensions fixed except the width of the.",
                    "label": 0
                },
                {
                    "sent": "This the dense columns.",
                    "label": 0
                },
                {
                    "sent": "Here the number of dense columns in the block arrow pattern.",
                    "label": 0
                },
                {
                    "sent": "Then we see that this gives us a method that's linear in the size of the complexity linear in the size of the matrix and other packages are.",
                    "label": 1
                },
                {
                    "sent": "I have a complexity increases more rapidly.",
                    "label": 0
                },
                {
                    "sent": "So these are two.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Examples of bandit and matrix norm minimization, where the sparsity pattern is actually coral.",
                    "label": 0
                },
                {
                    "sent": "You can also use it for non correspond sparsity patterns if you first embed the sparsity pattern in coral sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "And in practical way of method for doing this is to 1st apply a fill reducing ordering.",
                    "label": 0
                },
                {
                    "sent": "From salesky factorization packages.",
                    "label": 0
                },
                {
                    "sent": "And then compute a symbolic Cholesky factorization, and then the sparkly pattern of the Cholesky factor will actually be an embedding of the coral embedding of the original sparsity pattern.",
                    "label": 1
                },
                {
                    "sent": "So we generated some problems with sparsity parents from an set of test matrices.",
                    "label": 0
                },
                {
                    "sent": "The Rangers go from thousand 2000 to 30,000.",
                    "label": 0
                },
                {
                    "sent": "We took a small number of constraints and random sparse data, so all the coefficients are very sparse within relative to the sparsity pad.",
                    "label": 0
                },
                {
                    "sent": "Have you looked at two embeddings?",
                    "label": 0
                },
                {
                    "sent": "Actually, the AMD embedding and then the variation on it.",
                    "label": 0
                },
                {
                    "sent": "So these are two smaller for smaller problems from range from size 2000 to 4000.",
                    "label": 0
                },
                {
                    "sent": "These are actually the results of the embedding.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you see that the both embeddings increase the sparse the number of nonzeros.",
                    "label": 0
                },
                {
                    "sent": "Obviously the second one increases it more than the first one.",
                    "label": 0
                },
                {
                    "sent": "But still the second embedding will give us better results because this second embedding groups some small cliques together to improve the efficiency.",
                    "label": 0
                },
                {
                    "sent": "And then the other.",
                    "label": 0
                },
                {
                    "sent": "So these are the times preparation for the coral.",
                    "label": 0
                },
                {
                    "sent": "Methods.",
                    "label": 0
                },
                {
                    "sent": "The other packages have complexity at various widely sode SDP does very well on these problems because they are very sparse.",
                    "label": 0
                },
                {
                    "sent": "Andy SDP uses and.",
                    "label": 0
                },
                {
                    "sent": "Low rank exploits low rank structure in the matrices so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very well on these problems because the techniques it applies actually work very well on these matrices.",
                    "label": 0
                },
                {
                    "sent": "So in this order, larger problems with the largest one is size 30 thousands.",
                    "label": 0
                },
                {
                    "sent": "And so these are the results for the coral methods.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion is that we can actually solve quite large handle, very large sparse matrices if they can be efficiently embedded in a coral sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There are many other techniques you would need to use to really.",
                    "label": 0
                },
                {
                    "sent": "Solve wider range of large sparse as DPS, so in general you would like to combine it with for example to techniques and SDP that exploits low rank structure and so on.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a summary of this second part of the talk, so we will talk.",
                    "label": 0
                },
                {
                    "sent": "We discussed cone programming problems with coral matrix cones.",
                    "label": 0
                },
                {
                    "sent": "And it's interesting, I think, for a number of reasons.",
                    "label": 0
                },
                {
                    "sent": "One is, it's includes the standard cones, linear Programming, 2nd order cone, and SDP and all combinations compositions of them.",
                    "label": 0
                },
                {
                    "sent": "So basic and look at all these standard cones as just special cases of one single type of matrix cone is the coral sparse matrix cones.",
                    "label": 0
                },
                {
                    "sent": "It's interesting because there are some interesting applications of sparsity patterns that actually are coral.",
                    "label": 0
                },
                {
                    "sent": "For example, the block arrow sparsity patterns that are useful in robust optimization.",
                    "label": 0
                },
                {
                    "sent": "And it's also useful for general sparse semidefinite programming becausw.",
                    "label": 0
                },
                {
                    "sent": "It works well, at least the experiments that.",
                    "label": 0
                },
                {
                    "sent": "We run.",
                    "label": 0
                },
                {
                    "sent": "And so in combination with other sparse sparse exporting SDP techniques, it's can be very useful.",
                    "label": 1
                },
                {
                    "sent": "And the benchmark or a full set of benchmarks in the paper are also available from our website.",
                    "label": 0
                },
                {
                    "sent": "So then the last part of the talk is shorter, but and it's sort of recent work that's not quite finished and a little preliminary, but I.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I wanted to include it because it's more directly related to machine learning.",
                    "label": 0
                },
                {
                    "sent": "Done the same.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Definite programming topic.",
                    "label": 0
                },
                {
                    "sent": "So the problem is as follows.",
                    "label": 0
                },
                {
                    "sent": "So we know that in machine learning and kernel techniques, then often you have to solve a very large dense.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem, for example, quadratic programming problem made it very large dense.",
                    "label": 0
                },
                {
                    "sent": "Hessian matrix Q.",
                    "label": 0
                },
                {
                    "sent": "And with interior Point methods that gets very expensive if the number of training vectors.",
                    "label": 0
                },
                {
                    "sent": "So the size of Q.",
                    "label": 0
                },
                {
                    "sent": "It's about say 10,000 because at each iteration of an interior point method you have to solve and positive definite set of linear equations.",
                    "label": 0
                },
                {
                    "sent": "With coefficient Q + D, so that's very expensive.",
                    "label": 0
                },
                {
                    "sent": "If Q is dense and.",
                    "label": 1
                },
                {
                    "sent": "And is large.",
                    "label": 0
                },
                {
                    "sent": "So then of course, there are several techniques that people have discussed too.",
                    "label": 0
                },
                {
                    "sent": "Be able to solve large dance kewpies like this.",
                    "label": 0
                },
                {
                    "sent": "So one method for example is to approximate Q by a low rank mate.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Six or by a diagonal plus low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Another technique is of course to use first order methods instead of interior point methods.",
                    "label": 0
                },
                {
                    "sent": "But here we wanted to try something similar to the low rank idea.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how well would it try if you replaced Q with an approximation that has a sparse inverse?",
                    "label": 0
                },
                {
                    "sent": "So Q is denser, will approximated by a dense matrix, but with the sparse inverse, because we have a very natural or simple way to approximate a matrix with the sparse inverse.",
                    "label": 0
                },
                {
                    "sent": "If we choose the sparsity pattern of the universe to be coral.",
                    "label": 0
                },
                {
                    "sent": "So that's this is what we'll try, so will replace Q with Q~ and approximation and Q till is actually the inverse of a sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "S&S is defined by solving this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So he fixed queue, that's the actual kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "You solve this optimization problem in S. That's a sparse matrix or with the constraint that it's sparse with your given sparsity pattern.",
                    "label": 0
                },
                {
                    "sent": "And then the inverse of S is the approximate kernel matrix that will use.",
                    "label": 0
                },
                {
                    "sent": "And then as some interpretation.",
                    "label": 0
                },
                {
                    "sent": "So if you interpret Q as a covariance of and Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Distribution, then Q filter will be the growth and distribution covariance of the Gaussian distribution that's closest to queue in relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Or Cuba.",
                    "label": 0
                },
                {
                    "sent": "Collaborative vergence subject works partially constraint on the inverse of Q~ so that's one interpretation, another one with that Q~ So the approximate kernel is the maximum determined completion of Q.",
                    "label": 0
                },
                {
                    "sent": "Or the projection of Q on the sparsity problem.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you choose a bandit pattern, as will do in the experiments.",
                    "label": 0
                },
                {
                    "sent": "Then it means you evaluate the kernel matrix on the band in the diagonal band.",
                    "label": 0
                },
                {
                    "sent": "You don't evaluate the rest of the kernel matrix, but you completed with maximum determined positive definite completion.",
                    "label": 0
                },
                {
                    "sent": "And then that approximate Q is or that Q is the cutillo that we use in the.",
                    "label": 1
                },
                {
                    "sent": "QP.",
                    "label": 0
                },
                {
                    "sent": "And of course it's also known as covariant selection.",
                    "label": 1
                },
                {
                    "sent": "So again, if you choose fee to be coral, for example banded, then that's interesting for two reasons.",
                    "label": 0
                },
                {
                    "sent": "So Q till the inverse has a zero fill in salesky factorization, so that's useful in the interior point methods.",
                    "label": 1
                },
                {
                    "sent": "And also we can compute the Q~ implicitly.",
                    "label": 0
                },
                {
                    "sent": "But actually the Cholesky factors directly from the projection of Q on the sparsity pattern by recursion over the click three, we only have to know actually projection of Q on the sparsity pattern to do that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then if you use that look, go back to the QP so the SVM QP with the proximate Q~ so Q till there's a dense matrix but has a sparse inverse.",
                    "label": 1
                },
                {
                    "sent": "Then you can each iteration in the Interior Point solver becomes quite cheap because you can write the.",
                    "label": 0
                },
                {
                    "sent": "The equations you need to solve like this as the universe of Q~ plus a diagonal.",
                    "label": 0
                },
                {
                    "sent": "As coefficient matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you kill, the invert has a sparse.",
                    "label": 1
                },
                {
                    "sent": "Go to sparsity pattern, then can it can be factored with hero philenor.",
                    "label": 0
                },
                {
                    "sent": "This entire matrix can be factored with zero filling so we can do this very efficiently and you can solve very large QPS even though they are dense.",
                    "label": 0
                },
                {
                    "sent": "Util is still dense.",
                    "label": 0
                },
                {
                    "sent": "And can be very large, but of course we use the fact that the inverse of Q~ is spot.",
                    "label": 0
                },
                {
                    "sent": "So we have one experiment on the M. This data set where we use the full data set of 60,000 examples.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that we tried to use the full training set.",
                    "label": 0
                },
                {
                    "sent": "And still use an interior point method.",
                    "label": 0
                },
                {
                    "sent": "And what we tried here is to solve the problem.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stages so first we take the full training set.",
                    "label": 0
                },
                {
                    "sent": "We are applied as used approximate kernel.",
                    "label": 0
                },
                {
                    "sent": "The post the completion kernel with very small bandwidth of 1 hundreds.",
                    "label": 0
                },
                {
                    "sent": "And then so the Q pivot is approximate completion queue till then.",
                    "label": 0
                },
                {
                    "sent": "Then the next step is we take the support vectors from the first solution and solve a smaller problem with only those support vectors as training sets.",
                    "label": 0
                },
                {
                    "sent": "And a larger bandwidth of 400 in the second step, and again solve the QP with the bandits or the inverse bandit.",
                    "label": 0
                },
                {
                    "sent": "Matrix utility.",
                    "label": 0
                },
                {
                    "sent": "And then the last step we saw the full QP with only the support vectors from Step 2.",
                    "label": 0
                },
                {
                    "sent": "So this is the error we obtain after.",
                    "label": 0
                },
                {
                    "sent": "Of course, the entire combination of three.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is only an approximation approximate solution of the actual QP.",
                    "label": 0
                },
                {
                    "sent": "So, but if you compare with actually the solution computed of the full QP computed with SVM for the same values of the parameters and, and so on.",
                    "label": 0
                },
                {
                    "sent": "I see that the number of support vectors we obtain at the end is compatible and also the error is compatible.",
                    "label": 0
                },
                {
                    "sent": "So that's the number of reported active Error 3.",
                    "label": 0
                },
                {
                    "sent": "One interesting thing is in this first stage we approximate kernel.",
                    "label": 0
                },
                {
                    "sent": "Which we are inverse bandits.",
                    "label": 0
                },
                {
                    "sent": "So that the number of supporters actually larger than the actual QP.",
                    "label": 0
                },
                {
                    "sent": "And I need to go down in each of these stages when the error in the first few stages are errors that are actually for a classification based on classifier data.",
                    "label": 0
                },
                {
                    "sent": "So if you solve the approximate fees of several ways to result in a classification and this is for one of all.",
                    "label": 0
                },
                {
                    "sent": "Really try other many other values of parameter give you the parameters.",
                    "label": 0
                },
                {
                    "sent": "Women for the other.",
                    "label": 0
                },
                {
                    "sent": "If you don't think with this one.",
                    "label": 0
                },
                {
                    "sent": "But we didn't really do more parents the.",
                    "label": 0
                },
                {
                    "sent": "Choose the best values of the parameters and also maybe more stages.",
                    "label": 0
                },
                {
                    "sent": "CPU time this is the CPU time of the state.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first stage you look so happy with 60,000 variables, but it's had an inverter bandit.",
                    "label": 0
                },
                {
                    "sent": "Q Pickup told quite quickly and cost of course is the same throwables go is the problem of the same dimension with the same bandwidth, so the difference is just you too.",
                    "label": 0
                },
                {
                    "sent": "This new problem is taking one more recreation and the others.",
                    "label": 0
                },
                {
                    "sent": "Then in stage two we use the support vector force from stage one's training sets, so that's the size of the problem.",
                    "label": 0
                },
                {
                    "sent": "We solve with bandwidth 400.",
                    "label": 0
                },
                {
                    "sent": "And then the time is it's more expensive.",
                    "label": 0
                },
                {
                    "sent": "And ideally per iteration, that should be still linear in because the bandwidth is fixed and in a stage three we saw the full QP with the support vectors of stage one as training set and then the problem is it's a smaller problem, so it's quite fast approval and that only creases of the group.",
                    "label": 0
                },
                {
                    "sent": "Pair the first one with number 4, then you see that's exactly 8 because it's twice as large.",
                    "label": 0
                },
                {
                    "sent": "So the combination of the three is actually compatible again with lip SVM, or actually faster on these examples and SVM, even though we use in interior point methods for solving these problems.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's it, the to summarize the to talk.",
                    "label": 0
                },
                {
                    "sent": "We looked at some applications of matrix techniques that involve exploit coral sparsity.",
                    "label": 0
                },
                {
                    "sent": "So the key idea is that there are several interesting matrix problems that are solved using finite recursive algorithms.",
                    "label": 1
                },
                {
                    "sent": "If this party partners coral, so one is a Cholesky factorization with zero filling.",
                    "label": 1
                },
                {
                    "sent": "Then also the computation of the values and the gradients and hessians of the two barrier functions.",
                    "label": 0
                },
                {
                    "sent": "So very function for the cone of positive semidefinite matrices with the given sparsity pattern, and then the completable matrices.",
                    "label": 0
                },
                {
                    "sent": "And then there's these techniques can also be used for non coral patterns in combination with sort of coral embedding methods.",
                    "label": 0
                },
                {
                    "sent": "And I discuss two applications.",
                    "label": 0
                },
                {
                    "sent": "One is interior point methods for these two sparse matrix account.",
                    "label": 0
                },
                {
                    "sent": "And then also in general is an interesting application optimization is I think the approximating dense, very large dense matrices.",
                    "label": 0
                },
                {
                    "sent": "So for example hessions of optimization problems, convex optimization problems by.",
                    "label": 0
                },
                {
                    "sent": "Dense approximations that have a sparse inverse.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 1
                },
                {
                    "sent": "Well, there's a basic result.",
                    "label": 0
                },
                {
                    "sent": "For example, for a normal graphical model that the maximum likelihood estimate of the covariance.",
                    "label": 0
                },
                {
                    "sent": "Has a simple.",
                    "label": 0
                },
                {
                    "sent": "Recursive solution if or almost closed form solution if the sparsity pattern is coral.",
                    "label": 0
                },
                {
                    "sent": "So it's used.",
                    "label": 0
                },
                {
                    "sent": "Open gas, also for other problems.",
                    "label": 0
                },
                {
                    "sent": "Remind yeah so for us it indeed this positive path.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "There's a.",
                    "label": 0
                },
                {
                    "sent": "And also to find order easier.",
                    "label": 0
                },
                {
                    "sent": "Human resources and everything.",
                    "label": 0
                },
                {
                    "sent": "Mindy approximate kernel is defined as a solution of an SD card.",
                    "label": 0
                },
                {
                    "sent": "That's a very simple is required because the problem that we have to solve to find the approximate kernel is basically the same as the positive definite completion for a coral matrix, and in that case it's easy if you choose a non coral pattern then you would have to solve an optimization problem numerically to find this completion and then it wouldn't be interesting.",
                    "label": 0
                },
                {
                    "sent": "But for example, for bandit it's quite easy.",
                    "label": 0
                },
                {
                    "sent": "Recently I heard.",
                    "label": 0
                },
                {
                    "sent": "Scalability.",
                    "label": 0
                },
                {
                    "sent": "Yeah, look it up.",
                    "label": 0
                },
                {
                    "sent": "General.",
                    "label": 0
                },
                {
                    "sent": "To reduce it or two.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can always embed it and then for example in SDP set all these extra coefficients to 0.",
                    "label": 0
                },
                {
                    "sent": "So you introduce possible non zeros.",
                    "label": 0
                },
                {
                    "sent": "But then, for the SDP is still equivalent.",
                    "label": 0
                },
                {
                    "sent": "It's not an approximation.",
                    "label": 0
                },
                {
                    "sent": "Because all the coefficient matrices are.",
                    "label": 0
                },
                {
                    "sent": "Spot, so even if you embedded in a full dense matrix and by end symmetric matrices, it will still be equivalent.",
                    "label": 0
                }
            ]
        }
    }
}