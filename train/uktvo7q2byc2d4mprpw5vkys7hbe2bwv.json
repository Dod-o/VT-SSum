{
    "id": "uktvo7q2byc2d4mprpw5vkys7hbe2bwv",
    "title": "Model Selection in Exploration",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_langford_exploration/",
    "segmentation": [
        [
            "OK, so this is a very difficult talk to give because I committed to a little bit too much.",
            "Nevertheless, I think I got most away through what I committed to.",
            "Dealing with this very difficult.",
            "This is inherently technical subject.",
            "If we're thinking about model selection.",
            "We need to 1st think about learning and model selection have made a operation on top of learning.",
            "That means you need to start thinking very carefully about convergence rate means and things."
        ],
        [
            "That so I'm going to go slow if you have questions, you should definitely ask them.",
            "I'm going to start with just supervised.",
            "Erm, learning because this is something that we also be familiar with, and then we can define what what sort of model selection means here and then we can go into exploration settings.",
            "OK, so.",
            "We have a set of hypothesis H. Each about this takes features as input and gives you the label's output.",
            "It should be pretty standard.",
            "We have a distribution over events, so this is X is features and Y as a label which is zero or one."
        ],
        [
            "OK, so for each hypothesis H. There's some true error rate, which is the probability that a random sample drawn from D errors is incorrect.",
            "And then we have an empirical error rate, which is just the fraction of time that we air on particular data set."
        ],
        [
            "So now we can define the minimize here the year in minimizer given a data set.",
            "So this should be a capital S here.",
            "So we have.",
            "The hypothesis in our hypothesis set which has the smallest empirical error rate.",
            "There will be a chat of us.",
            "OK, so now we can easily prove a theorem which says for all the policies for all distributions.",
            "The empirical best apophysis satisfies this inequality.",
            "The difference between the.",
            "True error rate in the.",
            "Sorry the tree right here.",
            "An empirical error here is bounded by.",
            "This square root turn right and the key thing here is you have.",
            "You have, as long as the number of policies and you have the number of samples here, right?",
            "So the log it's compared the number of samples and so you win because.",
            "Because of the lock.",
            "This is really what makes learning possible.",
            "Excellent.",
            "OK, so this should be.",
            "I'll be familiar.",
            "I'm hoping it's familiars.",
            "Is this familiar to everyone?",
            "Just not familiar to you.",
            "Good."
        ],
        [
            "OK, so now let's talk about model selection.",
            "So here we have a set of positive sets, a call at M. And we need to define some operation on this set.",
            "So you might imagine that you should do some sort of ERM thing, but that would not work very well.",
            "And the reason why that would not work very well is because.",
            "One of your poster sets might be the set of all hypothesis.",
            "And then if you choose the about this.",
            "In any H which minimizes your error, you're going to lose because you're going to overfit.",
            "Log number prophecies is very large when you have all hypotheses.",
            "So instead that we needed to do something nontrivial, we need to do bound minimization.",
            "So we have.",
            "The empirical error rate of the best hypothesis in a particular set of most sensitive prophecies, and we're going to add in.",
            "It's kind of upper bound, confidence term, right?",
            "And the key thing here is that we depend upon.",
            "Besides the path set.",
            "So a small offset with a good error rate is a preferred thing."
        ],
        [
            "So now you can prove.",
            "For all major sets of prophecies.",
            "For all distributions.",
            "We have our minimizer of this bound.",
            "Thing.",
            "We have this this inequality holding right now the only difference is that we have this plus log.",
            "Number of bath sets.",
            "So this is good.",
            "So now we need to think a little bit about what model selection."
        ],
        [
            "Things.",
            "Particularly, we lost a little bit when we did model selection.",
            "We lost because.",
            "Log #50 space login number of this sets is larger than the log number of prophecies.",
            "If we had known in advance which hypothesis set was best, then we would not have to pay this.",
            "So we're paying some sort of costs, which is just like this.",
            "In the worst case.",
            "OK, so the cost of cost of model selection is the cost of not knowing the right hypothesis set in advance.",
            "So it's up to definition.",
            "And that's the core thing I'm going to carry over to the exploration setting.",
            "Actually, do several exploration settings."
        ],
        [
            "OK, so now we want to think about exploration.",
            "And there is no one exploration setting.",
            "There's a lot of different expiration settings Internet that the answers change when you go from one exploration setting to another.",
            "So.",
            "I think it helps to kind of exercise your intuition to go through these different settings and figure out what happens in each of these settings and then then.",
            "Try to figure out what's going on.",
            "So I'm going to go through two active learning settings and two contextual bandit settings.",
            "Each of these is either realizable, meaning that there is some perfect predictor or agnostic mean that you don't care either.",
            "There may not be a perfect predictor that there's.",
            "The structure of the hypothesis set is unrelated to the distribution generating the data in general.",
            "OK, so the first one is realizable active learning."
        ],
        [
            "Realize that learning has been around for a long time.",
            "The most basic version of it you have a pool of examples.",
            "But there's no labels right?",
            "And now you need to request labels until you find a near optimal hypothesis in your set of policies.",
            "And I guess generally you think of the unlabeled example is essentially free, and what you care about is the number of labeled examples that you need to request.",
            "So because we're in the realizable setting, we're guaranteed that at least one hypothesis has 0 error.",
            "Somebody should ask a question.",
            "This question you want us to ask or you just I feel lonely here.",
            "You did great."
        ],
        [
            "OK, so it gets complex pretty quickly.",
            "So in active learning, the label complexity is.",
            "Julie, dependent upon both H&D now turns out this is also true for supervised learning to some extent.",
            "But if you want to show that active learning does anything sort of non trivial compared to supervised learning, you need to introduce you need to consider this dependence very actively.",
            "OK, so send Joey had a paper on course sample complexity bounds for active learning.",
            "And the result in that paper was that the.",
            "Number of labels required to get something epsilon accurate with something like this, it's log number prophecies.",
            "Log one over epsilon so the weapon is great.",
            "But there's a row in that row is kind of a funny thing.",
            "You have to understand what the role is.",
            "So the row is a splitting rate.",
            "And the way that you think about row is you first say.",
            "I'm going to.",
            "Take pairs of bases in my path to set.",
            "Such that the distance between these policies is at least epsilon.",
            "The distance is defined as the probability.",
            "That the hypothesis disagree.",
            "Right?",
            "And now a particular unlabeled example.",
            "Row splits Q.",
            "This is a Q.",
            "If whatever, whatever the label is.",
            "Eliminate at least a row fraction of Q. OK, so you have pairs.",
            "You know that some hypothesis are well, at least when I bought.",
            "This is perfect.",
            "That means if any hypothesis disagrees with any label, you can throw it away.",
            "Appears illuminated if either of its policies disagree with the label.",
            "So the question is, how many pairs do we throw away?",
            "Or which fraction of the pairs we throw away?",
            "And row is sort of 1 minus Roque controls.",
            "The fraction that we throw away.",
            "And then what you can prove is that if you're throwing away.",
            "A significant fraction.",
            "Pairs you quickly converge.",
            "So either log one over epsilon structure.",
            "Information gain.",
            "So this is not precisely information gain.",
            "This is a kind of a more discrete version of it.",
            "So you throw away a pair.",
            "If I have a hypothesis in pairs, is this is invalid, yeah?",
            "What why do you do it that way rather than just?",
            "Well, you do not steal finds it.",
            "Yeah, yeah, so it's it's a little bit tricky here, so it's not sufficient to throw away a significant fraction of hypotheses.",
            "Either way, because.",
            "Let's count Ricky.",
            "It may be that there aren't many.",
            "There are ex is which which which don't throw away a significant fraction of hypothesis, but both raising affection pairs and then something useful happens.",
            "QM7 somehow related to packing setup hypothesis space.",
            "You have to find packing set for me.",
            "OK, so like covering goal.",
            "OK, so suppose you have a separate set and then you want to choose a subset of elements that sets such that for the covering and all elements of that set can be covered based on some notion of Northeast dance like that, but here the packing is that.",
            "So I guess the difference is that we're working with pairs of prophecies.",
            "But otherwise, you could certainly run this argument on a cover of Q.",
            "But I don't think it's actually necessary.",
            "Because you know what you can show is that if you're showing away, throwing away a significant fraction, then you're making progress in terms of.",
            "The error rate of the remaining sets of bases.",
            "Another related question is that what determines role to be small or large?",
            "That's the structure of the vast space.",
            "So some of these spaces are very friendly to active learning and some are not.",
            "Yeah, the idea.",
            "Diminish the size of age.",
            "Take spread.",
            "A subset of page that is very split up according to your distance and maybe.",
            "Small edge maybe?",
            "OK, so this is the splitting rate and this gives us some notion of what the sample complexity is here."
        ],
        [
            "And now we want to think about model selection.",
            "OK, so we have a set of pasta spaces before.",
            "And then we make a basic observation.",
            "Observation is that this community.",
            "Does not depend upon the labels.",
            "Right and now we can."
        ],
        [
            "Go back.",
            "Then go.",
            "That means.",
            "This quantity doesn't depend on the labels.",
            "And then we can."
        ],
        [
            "Go home.",
            "There's a really simple algorithm for doing this.",
            "We can just.",
            "Compute.",
            "Choose the smallest and then run active learning on it.",
            "Tomorrow is your lower bound of overall exercise low."
        ],
        [
            "No, not overall exits over the X is that you are going to ask for labels on.",
            "Yeah, so in the paper there's actually more complicated definition saying that every tile examples you see arose building X, but.",
            "What's important is that you can grow split."
        ],
        [
            "OK, so.",
            "Something kind of magical happened here.",
            "Because we have this reliability assumption, we can just choose the best path to set to do active learning on.",
            "And then go with it."
        ],
        [
            "So unlabeled complexity could grow up, but the cost?",
            "Of labels is 0.",
            "So that's kind of surprising.",
            "And then you think back to.",
            "Because you need to measure this to high precision on a lot of different both sets.",
            "Yeah.",
            "So this is assuming we know exactly then all we have to estimate it.",
            "I'm assuming that we know exactly here, but.",
            "If you have a sufficiently large pool, you can estimate it to sufficiently large accuracy.",
            "Yeah, you could chase a lot of epsilons down there if you wanted to.",
            "May want to call it Alpha because we have epsilon are used.",
            "OK, so."
        ],
        [
            "So now we're kind of confused by this, because supervised learning, which seemed simpler had a non zero label complexity and non 0 cost of model selection.",
            "So now we think back to supervised model selection and if we add in realize ability turns out the same kind of algorithm works fine.",
            "So the cost is also zero for reliability for realizable election in supervised learning.",
            "OK. Work that.",
            "They were shamans.",
            "Did using a kind of distance measure like yours?",
            "For model selection with supervised learning, but I don't think he had zero.",
            "Well he may not have the reliability.",
            "The reliability is very powerful.",
            "OK so this is realizable active learning.",
            "Community fights also zero in realisable surprised, and so in.",
            "So let's go back to."
        ],
        [
            "Supervised learning so.",
            "If you're in the realizable case, this value here is 0.",
            "Right?",
            "And I guess you can get rid of the square root and maybe the two something like that.",
            "But the point is that this quantity here is computed is computable without any examples.",
            "And that means that you can just find the smallest path to set.",
            "It's realizable so.",
            "And slow down small subset.",
            "Alright, so you have a set of both sets.",
            "Said is here.",
            "You can just find the smallest about the set and.",
            "Go with it.",
            "That's cheating a bit.",
            "We're using reliability assumption quite a bit.",
            "Each model so of course I will take this model, it's a bit.",
            "I think that's more impressive in activities here.",
            "Yeah, so now you can ask yourself.",
            "What happens?",
            "I mean, the same issue comes up in active learning, right?",
            "So I've defined reliability."
        ],
        [
            "Their way, which says that every hypothesis said every hypothesis has a perfect apophysis.",
            "Right, that's one way to extend reliability into sort of mini posets regime.",
            "You could also say, well, suppose that only one, but this set has a.",
            "Has it and I claim that's equivalent to agnostic case.",
            "Because you can have an enormous gigantic about this set which contains the true path sis.",
            "But you can never learn from because it's just enormous.",
            "Even log of number of paths is too large.",
            "And then you suck.",
            "OK so this is realizable active learning."
        ],
        [
            "And now we want to think about agnostic active learning.",
            "This is a bit newer.",
            "If you, if you look at."
        ],
        [
            "The the algorithms for realizable active learning.",
            "There would be things which involve.",
            "Choosing an X asking for the label and then throwing away all inconsistent hypotheses.",
            "For agnostic active learning."
        ],
        [
            "Have to be a lot more careful because.",
            "You can't throw out the good ex, right?",
            "So there's been a sequence of papers defining how we can do agnostic active learning in a nontrivial way.",
            "The setting is sort of.",
            "The setting is converge to kind of stream setting where you see a stream of unlabeled examples, and for each you choose when you see it to either request a label or not.",
            "So it's an online setting, so there's no going back and choosing a different.",
            "No, not making no revisiting decisions.",
            "And then you want to run online until you find a natural optimal hypothesis."
        ],
        [
            "Um?",
            "So that's it's pretty forward.",
            "This is the style of online agnostic active learning algorithm that converge to buy, at least by this paper and.",
            "And similar things are used for these later papers.",
            "You start with a set of bases.",
            "And now we're going to run through unlabeled examples.",
            "You see the unlabeled example.",
            "You have your current set of offseason.",
            "You ask, does there exist two apostasies which disagree on this unlabeled example?",
            "And if so, you query.",
            "An when you query you get some information.",
            "You can compute sample complexity bounds on the difference in Area 2 prophecies.",
            "The difference is important.",
            "You can't do it for individual hypotheses, but you can do it for the difference efficiently.",
            "And that allows you to throw away some apostasy so you can do some sort of filtration process here.",
            "And throw a hypothesis which have been proved sub optimal.",
            "Do you know the error rates?",
            "No.",
            "I mean the agnostic.",
            "Learning learning relative to the best in class.",
            "Yeah, well, we're learning it relative to the best in class, so we're just trying to.",
            "So epsilon optimal means.",
            "The office that you return at the end is within Epsilon in true error rate of the best in the class.",
            "Right?"
        ],
        [
            "OK, so once again the label complexity is complex.",
            "If you don't make any, if you don't.",
            "If you don't try to introduce.",
            "These strange things like this data, which is a disagreement coefficient.",
            "Then there's a lower bound which says that active learning is no in the agnostic case is no better than supervised learning in agnostic case.",
            "So the minimum error rate of our apophysis over across the set.",
            "Is going to be one of the parameters in our same complexity bound?",
            "OK, So what does this look like?",
            "So first of all I should mention one other thing which is that."
        ],
        [
            "You can prove for this style of algorithm that the.",
            "After you run over T unlabeled examples, if you're asking for the label every time, you'd get some error rate with supervised learning.",
            "And you can show that you get an error rate using this approach, which is within a factor of 2.",
            "And they will just always be true, although we might end up if we're unlucky in our choice of path set or choice of a problem, we might end up asking for every label, so it's equivalent supervised learning."
        ],
        [
            "OK, so now the question is how many labels do we have to pay for in order to achieve that error rate or that regret which is within a factor of 2 of what you would get with supervised learning?",
            "And the claim is, it's this kind of beastly thing.",
            "OK, so first of all M is a supervised label complexity.",
            "You can think of this as T. Maybe T is a better variable to think about.",
            "So you have.",
            "This disagreement coefficient will define in a moment you have the minimum error rate times T. So this is sort of.",
            "The level of querying that you expect due to noise.",
            "Because if you have a is the best path is an error rate of new.",
            "Then, after M unlabeled examples, there's going to be new in examples where the label disagrees.",
            "And so you end up needing to query.",
            "You don't know in advance that they can disagree, but.",
            "Yeah.",
            "OK, and then we have log of T. Times log number, but these times log T plus log one over Delta.",
            "So the important thing is we have a log here.",
            "We have a log here and we have a Theta there and if new is small enough this all ends up being better than him.",
            "But but new needs to be small enough.",
            "There is a disagreement coefficient, right?",
            "So I need to define that for you."
        ],
        [
            "It's a little bit different.",
            "So this is the formula.",
            "So you pick a radius around the optimal hypothesis.",
            "And.",
            "You ask, does there exist another hypothesis within a ball of this radius which disagrees with the optimal hypothesis?",
            "OK, and now you want to know what is the probability.",
            "That there is some X for which you get a disagreement.",
            "How many did everything?",
            "They are.",
            "OK so you think?",
            "Takes a little while to unpack.",
            "So if you only care bout hypotheses, so imagine that we're running this algorithm.",
            "We're kind of throwing away bad hypotheses.",
            "The ones which are are very bad will be thrown away quickly.",
            "The ones which remain will be decent, and we want to keep throwing things away, so there's some radius that we care about at some point in the algorithm, because maybe all the hypothesis of an error rate of have a regret of R or less.",
            "OK, so if we only care about policies which are within R of the optimal, this is according to the distance defined on the bothies.",
            "Which fraction of the X are disagreed upon?",
            "It's.",
            "That's what the numerator is.",
            "And then you need to divide by R to kind of make the units workout.",
            "So you want sort of unit list thing, right?",
            "So this will grow with R. But if you divide by R, then you know this quantity may not.",
            "Grow with our.",
            "Yeah.",
            "Space right yes?"
        ],
        [
            "In the previous case I did it's right here.",
            "OK, so you can find it so you can define the codes and stuff.",
            "So we have a distance metric just induced by the unlabeled data and that means we have balls and everything else.",
            "So part of the question is that if they knew the minimum, so if the new is 0, would you get the same result as here?"
        ],
        [
            "I think not naively.",
            "So if new is 0, this is like log.",
            "I think it's close actually.",
            "Theater is the definition of Theta is different.",
            "No, you don't get the same result because it's not the same algorithm.",
            "So we need a more passive algorithm in the agnostic setting.",
            "So in the in the realizable setting we can just.",
            "You can query anything and worst case just doesn't help you throw away better batteries.",
            "But in the agnostic setting, you need to be very careful about how you measure the difference in error rates.",
            "Just make sure it makes sense.",
            "Respect to the final distribution.",
            "So you don't recover the realizable case.",
            "The function of all four."
        ],
        [
            "No, we would take a Max.",
            "If you have margin right then usually even our goes to zero data goes.",
            "Yeah you you can exclude the ours which are very small so it's the Max over R is greater than some epsilon.",
            "It could be, but it can also be too.",
            "Right, so for intervals, the line is too.",
            "I guess the thing which is maybe interesting about this is that every example where we know how to do agnostic active learning, the disagreement coefficient characterizes why it's possible.",
            "OK, so this is."
        ],
        [
            "Agnostic active learning and now the question is what is model selection mean?",
            "And we have a problem.",
            "The problem is that new depends upon the labels.",
            "So we cannot.",
            "We can't do the trick we did before.",
            "Um?",
            "And now we're pretty stuck."
        ],
        [
            "Ugh.",
            "The best thing I know how to do is something like round Robin.",
            "So you have an agnostic active learner on each individual hypothesis.",
            "And then you just rotate through different.",
            "Hypothesis sets.",
            "Checking to see if it queries and if it queries you, get to Y and then you update that particular one.",
            "So this is not a very satisfying solution.",
            "But this is the best that I know.",
            "But you can also use a band.",
            "You I know you can't, so this is a funny thing about active learning.",
            "In active learning, if you're trying to do active learning, path is set.",
            "It is possible to return.",
            "A.",
            "Normal hypothesis faster than you can evaluate its error rate.",
            "Not related to.",
            "Do you like the learning process?",
            "Is the minimum of.",
            "Diane, I'm in love you bye for this class.",
            "So if you have a lower bound on some at age, you know that new is.",
            "In the worst case, at least that.",
            "She's not perfect.",
            "I see what you're saying.",
            "So what you're saying is you can get a lower bound estimate on new and they can use that to eliminate some of these sets.",
            "And that's true.",
            "I don't know how to use that kind of analysis too.",
            "Getting better than in the worst case.",
            "But we're actually not done yet, even with round Robin, because each individual active learning algorithm may have some particular candidate apophysis.",
            "But you don't know which of those is is the best.",
            "So you need to run active learning one more time on those and you can interleave that with.",
            "With this and I think you can still manage to not sell things down too much more, but."
        ],
        [
            "But this is.",
            "A label complexity which is met.",
            "Magnitude of them worse.",
            "At that stuff now this seems terrible.",
            "In many ways it is.",
            "Let me mention that it is better than another dumb strategy which is just taking all the office all policies and mushing them together into one big enough to set.",
            "So it is there you get and if there it's Infinity instead of merely magnitude of in.",
            "So we kind of we lost here and we lost a lot more than we lost in the supervised case.",
            "If there's a way to get around this, that would be pretty interesting that, but I don't see how to get around this."
        ],
        [
            "OK, so now let's switch to contextual bandits.",
            "So the first additional bedrooms each before.",
            "Which is schema quite awhile ago there's several variants of each before there's a high probability version, and then this is a new set of algorithms, which is which are very different from each.",
            "Before I'm going to be talking about each P4P, which is the minor variation of before we talked about in this paper.",
            "The reason is because I want things to be compatible, so each before has a regret bound, which holds an expectation but not in high probability.",
            "And here we modify things to hold my probability.",
            "OK, so in the contextual bandit setting, it's a different setting.",
            "It's similar, though we see examples and now each step we're going to choose an individual action.",
            "And then we get some particular loss for that action.",
            "So compared to active learning, we're choosing whether not to query here, we're always querying in some sense of recording a particular action, and we don't get feedback about the other actions.",
            "So there's an exploration problem because we don't get feedback about the other actions.",
            "So each before P looks something like this, we start with a weight which is 1 on every individual hypothesis.",
            "And then we run through examples.",
            "First step is we normalize the weights and we compute some sort of distribution over the prophecies.",
            "This is modified slightly to impose a minimum probability distribution probability on any individual action.",
            "But look for that.",
            "Then we observe our features.",
            "We draw our hypothesis.",
            "And we we act as a checks me drink.",
            "I think we're good here.",
            "It's more difficult than expected, doesn't seem to be A twist up.",
            "Yeah, yeah.",
            "I shouldn't use a USB key.",
            "Alright, I'll wait.",
            "Good idea.",
            "Everywhere.",
            "We found another can opener.",
            "OK, so we.",
            "We observe X.",
            "We draw our hypothesis from this distribution.",
            "We act according to their office, and we observe a particular reward.",
            "And now we update our distribution.",
            "So this is a little bit hard to read.",
            "There's some minimum probability which I didn't really tell you about up here that we imposed.",
            "So we have the minimum probability times the reward divided by the probability of taking the action.",
            "So in order to compute this piece of Bay need to integrate over the draws of H. So many different ages could choose the same A and you want to add up all those probabilities.",
            "And then we have an indicator function of.",
            "Whether or not they passed this, they were updating.",
            "Agrees with hypothesis that we chose.",
            "And then we add a little bit more and then it succeeded by probability.",
            "OK, so this is this is the outline of the algorithm.",
            "Gives you a sense of how things work."
        ],
        [
            "And now we can prove.",
            "Is there?",
            "For all assets.",
            "For all sequences, this is a stronger quantification then we used before.",
            "With high probability.",
            "The average pround regret.",
            "Is something like this?",
            "So this is exactly what you saw before with the.",
            "With supervised learning, it's actually worse in the constants and in general.",
            "I guess you have more than two actions, but we only have two actions here, so it's a constant so suppressed.",
            "So typically you would have a depends on the number of actions dividing T essentially, so you can think of it as multiplying log network prophecies.",
            "OK, so this is a standard ESB, four P analysis."
        ],
        [
            "And now the question is.",
            "How does model selection work?"
        ],
        [
            "And now we go back here and we see this is a regret.",
            "This is the difference between error rate of the best and the error rate that we achieved with our conditional banded algorithm.",
            "And we can't know that in advance."
        ],
        [
            "So we're stuck again."
        ],
        [
            "And again, all that I have for you is round Robin.",
            "And this seems to be.",
            "I mean the structure of."
        ],
        [
            "The XP 4 so algorithms is really conducive to wanting to work better because.",
            "Can generalize he sympathies to be arbitrary experts, and these experts can output a distribution over actions.",
            "Just like each before P produces, it produces distribution over actions implicitly here.",
            "And you would love to plug things together in some sort of.",
            "You know modular way, but it doesn't work.",
            "The regret bands don't plug together.",
            "And the counterexample is.",
            "Suppose you have a small hypothese ET and you have a really large path to set.",
            "Right, so if you're going to compete with a small pasta set need to converge quickly, do something which is equivalent to the best in that small pasta set.",
            "But we're going to achieve a."
        ],
        [
            "Arute regret.",
            "That means that the probability of.",
            "Of querying."
        ],
        [
            "According to the expert, the other pasta set has to be something like one over root tea.",
            "And that means that you can't converge quickly enough on the other on the big about to set to also compete with the big path to set in some sort of.",
            "More rapid than round Robin fashion."
        ],
        [
            "OK, so that's going to."
        ],
        [
            "Hand WAVY we should probably workout a lower bound explicitly, but yeah.",
            "You have a distance between the different purposes sets, right?",
            "Yeah, well you have a distance between the different, but it says we could.",
            "You could restrict yourself to explore the guides which are sufficiently close to the best individual in the in the smallest set.",
            "So first of all, you don't know what the best individual in the smaller set is.",
            "You could restrict the bigger buses set to be things close to the smaller pasta set.",
            "But now we essentially have what we're no longer trying to compete with the bigger path to set.",
            "Right, so I think the game I'm trying to play is.",
            "I don't know what the pasta set which has set in supposed to compete with in advance.",
            "And now I want to whatever the truth is.",
            "I want to compete with that one without paying too much penalty compared to if I had known it in advance.",
            "So it's kind of like the regret of model selection or something like that.",
            "This is what I'm trying to define and get at.",
            "And."
        ],
        [
            "Yeah, we're kind of stuck here, yeah?",
            "Right on top of this set, yeah won't work because of this exploration example.",
            "I just told you.",
            "If you are playing a multi armed bandit.",
            "Um?",
            "The trouble is that so you have two levels right?",
            "So you have the top level and you have a lower level.",
            "And if you're going to get a root T regret at the top level, that means you're going to starve at least one of your arms down to one over root probability of sampling.",
            "If you starve, you should serve one of your algorithms down to one over the probability sampling.",
            "If you're starving an algorithm to 1 / T probability sampling.",
            "It can't learn quickly enough to actually achieve.",
            "The area, so instead of getting like a root T, you get A at the.",
            "3/4 regret.",
            "So if you only get a sample every 1 / T samples, then it's like time slows down and you have to kind of.",
            "You're going to get a sample 1 / M. So if I'm using round Robin, yes it's 1 / N. But if I'm using round Robin then.",
            "Then then yes, I see.",
            "So what I could do is I could run Exp 3 on the output of round Robin.",
            "That would be a fun thing to do, but then I'm still going to lose with this factor of him because I'm playing round Robin.",
            "We are going to conduction the best.",
            "This subclass, so you don't converge to the best.",
            "In the past, this subclass and less you explore enough, and then I bought this subclass.",
            "You don't explore enough my pasta subclass in less.",
            "You actually act according to the bathtub glass often enough.",
            "Yeah.",
            "OK, so."
        ],
        [
            "Yeah.",
            "Let's switch to reliable initial minutes so we have a new paper.",
            "Which deals with the unstructured, realizable case.",
            "So you have.",
            "So as you back up a moment, first of all, we're good.",
            "What does reliability mean in the connection and setting right?",
            "Because?",
            "Central clear.",
            "What I mean is that instead of having a set of policies, they have a set of regressors.",
            "And there's one of them, which predicts the expected loss correctly for every XNA.",
            "OK, so there can be noise in the actual losses, but you need to predict the expected loss correctly.",
            "Um?",
            "So this is kind of a weaker notion of reliability than we used before with active learning."
        ],
        [
            "And now.",
            "Is this new algorithm regressor elimination?",
            "We start with a set of regressors.",
            "We're going to be doing some sort of filtering operation, so we're going to be.",
            "Getting new sets at each round.",
            "We're going to choose a distribution over the set of regressors.",
            "And this is kind of a delicate operation that turns out to be possible.",
            "You need to achieve near uniformity over the actions.",
            "Chosen by Regressors in their remaining set.",
            "So what does it mean for us to choose an action or issues in action?",
            "If you take the argmax over?",
            "Overall, the actions in you.",
            "You take the arguments over the actions, right?",
            "So we have an F of.",
            "X, a.",
            "Take argmax of, a this particular action?",
            "We can say that Regressor choose that action on that X.",
            "So we can convert regressors into policies.",
            "OK, so that we.",
            "We choose a distribution over aggressors, kind of delicately.",
            "We observe X.",
            "We draw F from P. We act according to the argmax action.",
            "Observer reward and then we do some sort of filtration here.",
            "And I guess the key thing here is it turns out that.",
            "You can throw away all regressors, which is like a constant over T. Worse than the best.",
            "It's a very aggressive filtration, but.",
            "Just to be OK because of the reliability assumption.",
            "So this is the last exploration setting that we're looking at.",
            "Are there any questions about this?",
            "First step, is it actually possible to get or to do this?",
            "This one.",
            "Yes, it is possible.",
            "The argument is pretty nontrivial.",
            "You need to use a generalization of an moments mini Max.",
            "So the argument that it exists is nonconstructive, but then once you know that it exists, you have optimization methods which are guaranteed to give you the solution.",
            "OK, so."
        ],
        [
            "Would you improve is essentially the same kind of regret bound as you get with XP4 or P?",
            "So it's kind of a let down aggressor.",
            "They realize reception didn't give us anything more.",
            "But"
        ],
        [
            "If you're very careful with your quantifiers, you can show that sometimes is much better.",
            "So in particular, you can get rid of.",
            "The dependence on T and there was also depends on the number of actions, and they can also be gotten rid of.",
            "You're lucky so.",
            "Having these this realizable case can be powerful, but it's not necessarily it's.",
            "It could be just as bad as a 64P.",
            "OK, so.",
            "So that's our regret bound.",
            "And now we need to."
        ],
        [
            "About model selection and this turns out to be easy also.",
            "So we can just.",
            "Every individual.",
            "So it has a optimal, so this is computable without looking at your examples.",
            "So you can just choose the best and go with it."
        ],
        [
            "That means that our cost is a model selection to 0.",
            "What about if you don't know which has the realisable?",
            "They aren't always?",
            "Then I think you're going to fall into the same kind of agnostic case as before.",
            "As you can see, the realizable one could be in.",
            "Some sort of strange set that.",
            "That's true, hold on.",
            "Yeah, he just don't know which set it's in, so you need to use round Robin.",
            "As far as I can tell.",
            "And if you don't use round Robin, if using exploration method then he starts starving the algorithms of the information they need in order to actually converge.",
            "OK, so."
        ],
        [
            "So I'm bout done.",
            "This is the results that I discussed.",
            "I think everything should be convinced you should be convinced of everything except for possibly these.",
            "But I guess I strongly expect it will see that once you workout a lower bound and upper bound.",
            "OK, so."
        ],
        [
            "This was.",
            "Interesting, you always kind of want to talk about something you don't quite know, because then you learn something.",
            "So I had no idea that realized he was realizability.",
            "With such a powerful thing respecting model selection.",
            "It's extremely powerful.",
            "And even without it.",
            "In exploration settings, simply model selection becomes very hard.",
            "Question.",
            "So is there any middle ground between these two?",
            "Because realizability tells that there is exactly this sort of the best one.",
            "So the answer has to be yes.",
            "But what stating that in a natural way seems to?",
            "I don't know how to do.",
            "Some sort of almost realized ability or something I don't know.",
            "Well, maybe you can say that.",
            "The best type of assume that all the spaces.",
            "Excellent close to the realisable.",
            "Yeah, and then maybe some of these bounding things you're mentioning would become useful.",
            "You're not talking about nested classes.",
            "Yeah, I could necessary parties are special cases would have talked about.",
            "I also didn't talk about Pacbase things, but that's also essentially special case because you can discretize the prior in the back bays and get a set of hypothesis sets.",
            "OK.",
            "Sort of accuracy to possibly decrease with the increasing complexity, and so you know would be unreasonable away in the realizable case is unreasonable.",
            "Obviously just wanted to be in every hypothesis class, and if you were able to relax that that."
        ],
        [
            "You can't, but then maybe if you had some.",
            "So I think one thing that you sort of need to do before you can get a better simply analysis.",
            "So we need a better algorithm we need.",
            "We have this algorithm for diagnostic case, which is not the same as the algorithm for that for the realizable case.",
            "And now the question is, how do we get an algorithm which works for the agnostic case?",
            "But it also gives you a good analysis for the realizable case.",
            "Yeah.",
            "And not, and not each of them and things.",
            "But say one of them does.",
            "Given the administrative Office classes in one of the offsets in one of the main Zero policies.",
            "Smoked realisable case like multiple different things not very self contained.",
            "That might be an interesting something to work with.",
            "Just infinite plus.",
            "It's eventually broken it.",
            "Wouldn't you run into the same problem you?",
            "Spoke of it, you know, the one could come in.",
            "Very, very late, yeah?",
            "Dress just basements.",
            "Get some samples in the index of the opposite glass.",
            "The first one that this is realisable.",
            "Yeah, so I guess that would be kind of a more of a relaxed version of round Robin, right?",
            "So instead of depending on the number of both sets, you depend upon the index of the first above to set.",
            "There might be feasible something along those lines.",
            "Yeah, I guess I'm I'm done.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a very difficult talk to give because I committed to a little bit too much.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, I think I got most away through what I committed to.",
                    "label": 0
                },
                {
                    "sent": "Dealing with this very difficult.",
                    "label": 0
                },
                {
                    "sent": "This is inherently technical subject.",
                    "label": 0
                },
                {
                    "sent": "If we're thinking about model selection.",
                    "label": 1
                },
                {
                    "sent": "We need to 1st think about learning and model selection have made a operation on top of learning.",
                    "label": 0
                },
                {
                    "sent": "That means you need to start thinking very carefully about convergence rate means and things.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That so I'm going to go slow if you have questions, you should definitely ask them.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start with just supervised.",
                    "label": 0
                },
                {
                    "sent": "Erm, learning because this is something that we also be familiar with, and then we can define what what sort of model selection means here and then we can go into exploration settings.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We have a set of hypothesis H. Each about this takes features as input and gives you the label's output.",
                    "label": 0
                },
                {
                    "sent": "It should be pretty standard.",
                    "label": 0
                },
                {
                    "sent": "We have a distribution over events, so this is X is features and Y as a label which is zero or one.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for each hypothesis H. There's some true error rate, which is the probability that a random sample drawn from D errors is incorrect.",
                    "label": 0
                },
                {
                    "sent": "And then we have an empirical error rate, which is just the fraction of time that we air on particular data set.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we can define the minimize here the year in minimizer given a data set.",
                    "label": 0
                },
                {
                    "sent": "So this should be a capital S here.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis in our hypothesis set which has the smallest empirical error rate.",
                    "label": 0
                },
                {
                    "sent": "There will be a chat of us.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we can easily prove a theorem which says for all the policies for all distributions.",
                    "label": 0
                },
                {
                    "sent": "The empirical best apophysis satisfies this inequality.",
                    "label": 0
                },
                {
                    "sent": "The difference between the.",
                    "label": 0
                },
                {
                    "sent": "True error rate in the.",
                    "label": 1
                },
                {
                    "sent": "Sorry the tree right here.",
                    "label": 1
                },
                {
                    "sent": "An empirical error here is bounded by.",
                    "label": 0
                },
                {
                    "sent": "This square root turn right and the key thing here is you have.",
                    "label": 0
                },
                {
                    "sent": "You have, as long as the number of policies and you have the number of samples here, right?",
                    "label": 0
                },
                {
                    "sent": "So the log it's compared the number of samples and so you win because.",
                    "label": 0
                },
                {
                    "sent": "Because of the lock.",
                    "label": 0
                },
                {
                    "sent": "This is really what makes learning possible.",
                    "label": 0
                },
                {
                    "sent": "Excellent.",
                    "label": 0
                },
                {
                    "sent": "OK, so this should be.",
                    "label": 0
                },
                {
                    "sent": "I'll be familiar.",
                    "label": 0
                },
                {
                    "sent": "I'm hoping it's familiars.",
                    "label": 0
                },
                {
                    "sent": "Is this familiar to everyone?",
                    "label": 0
                },
                {
                    "sent": "Just not familiar to you.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's talk about model selection.",
                    "label": 1
                },
                {
                    "sent": "So here we have a set of positive sets, a call at M. And we need to define some operation on this set.",
                    "label": 1
                },
                {
                    "sent": "So you might imagine that you should do some sort of ERM thing, but that would not work very well.",
                    "label": 0
                },
                {
                    "sent": "And the reason why that would not work very well is because.",
                    "label": 0
                },
                {
                    "sent": "One of your poster sets might be the set of all hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And then if you choose the about this.",
                    "label": 0
                },
                {
                    "sent": "In any H which minimizes your error, you're going to lose because you're going to overfit.",
                    "label": 0
                },
                {
                    "sent": "Log number prophecies is very large when you have all hypotheses.",
                    "label": 0
                },
                {
                    "sent": "So instead that we needed to do something nontrivial, we need to do bound minimization.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "The empirical error rate of the best hypothesis in a particular set of most sensitive prophecies, and we're going to add in.",
                    "label": 0
                },
                {
                    "sent": "It's kind of upper bound, confidence term, right?",
                    "label": 0
                },
                {
                    "sent": "And the key thing here is that we depend upon.",
                    "label": 0
                },
                {
                    "sent": "Besides the path set.",
                    "label": 0
                },
                {
                    "sent": "So a small offset with a good error rate is a preferred thing.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now you can prove.",
                    "label": 0
                },
                {
                    "sent": "For all major sets of prophecies.",
                    "label": 0
                },
                {
                    "sent": "For all distributions.",
                    "label": 0
                },
                {
                    "sent": "We have our minimizer of this bound.",
                    "label": 0
                },
                {
                    "sent": "Thing.",
                    "label": 0
                },
                {
                    "sent": "We have this this inequality holding right now the only difference is that we have this plus log.",
                    "label": 0
                },
                {
                    "sent": "Number of bath sets.",
                    "label": 0
                },
                {
                    "sent": "So this is good.",
                    "label": 0
                },
                {
                    "sent": "So now we need to think a little bit about what model selection.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "Particularly, we lost a little bit when we did model selection.",
                    "label": 1
                },
                {
                    "sent": "We lost because.",
                    "label": 0
                },
                {
                    "sent": "Log #50 space login number of this sets is larger than the log number of prophecies.",
                    "label": 0
                },
                {
                    "sent": "If we had known in advance which hypothesis set was best, then we would not have to pay this.",
                    "label": 0
                },
                {
                    "sent": "So we're paying some sort of costs, which is just like this.",
                    "label": 0
                },
                {
                    "sent": "In the worst case.",
                    "label": 0
                },
                {
                    "sent": "OK, so the cost of cost of model selection is the cost of not knowing the right hypothesis set in advance.",
                    "label": 1
                },
                {
                    "sent": "So it's up to definition.",
                    "label": 0
                },
                {
                    "sent": "And that's the core thing I'm going to carry over to the exploration setting.",
                    "label": 0
                },
                {
                    "sent": "Actually, do several exploration settings.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we want to think about exploration.",
                    "label": 0
                },
                {
                    "sent": "And there is no one exploration setting.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of different expiration settings Internet that the answers change when you go from one exploration setting to another.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think it helps to kind of exercise your intuition to go through these different settings and figure out what happens in each of these settings and then then.",
                    "label": 0
                },
                {
                    "sent": "Try to figure out what's going on.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to go through two active learning settings and two contextual bandit settings.",
                    "label": 0
                },
                {
                    "sent": "Each of these is either realizable, meaning that there is some perfect predictor or agnostic mean that you don't care either.",
                    "label": 0
                },
                {
                    "sent": "There may not be a perfect predictor that there's.",
                    "label": 0
                },
                {
                    "sent": "The structure of the hypothesis set is unrelated to the distribution generating the data in general.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first one is realizable active learning.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Realize that learning has been around for a long time.",
                    "label": 0
                },
                {
                    "sent": "The most basic version of it you have a pool of examples.",
                    "label": 1
                },
                {
                    "sent": "But there's no labels right?",
                    "label": 0
                },
                {
                    "sent": "And now you need to request labels until you find a near optimal hypothesis in your set of policies.",
                    "label": 0
                },
                {
                    "sent": "And I guess generally you think of the unlabeled example is essentially free, and what you care about is the number of labeled examples that you need to request.",
                    "label": 0
                },
                {
                    "sent": "So because we're in the realizable setting, we're guaranteed that at least one hypothesis has 0 error.",
                    "label": 0
                },
                {
                    "sent": "Somebody should ask a question.",
                    "label": 0
                },
                {
                    "sent": "This question you want us to ask or you just I feel lonely here.",
                    "label": 0
                },
                {
                    "sent": "You did great.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it gets complex pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "So in active learning, the label complexity is.",
                    "label": 1
                },
                {
                    "sent": "Julie, dependent upon both H&D now turns out this is also true for supervised learning to some extent.",
                    "label": 0
                },
                {
                    "sent": "But if you want to show that active learning does anything sort of non trivial compared to supervised learning, you need to introduce you need to consider this dependence very actively.",
                    "label": 0
                },
                {
                    "sent": "OK, so send Joey had a paper on course sample complexity bounds for active learning.",
                    "label": 0
                },
                {
                    "sent": "And the result in that paper was that the.",
                    "label": 0
                },
                {
                    "sent": "Number of labels required to get something epsilon accurate with something like this, it's log number prophecies.",
                    "label": 0
                },
                {
                    "sent": "Log one over epsilon so the weapon is great.",
                    "label": 0
                },
                {
                    "sent": "But there's a row in that row is kind of a funny thing.",
                    "label": 0
                },
                {
                    "sent": "You have to understand what the role is.",
                    "label": 0
                },
                {
                    "sent": "So the row is a splitting rate.",
                    "label": 0
                },
                {
                    "sent": "And the way that you think about row is you first say.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Take pairs of bases in my path to set.",
                    "label": 0
                },
                {
                    "sent": "Such that the distance between these policies is at least epsilon.",
                    "label": 0
                },
                {
                    "sent": "The distance is defined as the probability.",
                    "label": 0
                },
                {
                    "sent": "That the hypothesis disagree.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And now a particular unlabeled example.",
                    "label": 0
                },
                {
                    "sent": "Row splits Q.",
                    "label": 0
                },
                {
                    "sent": "This is a Q.",
                    "label": 0
                },
                {
                    "sent": "If whatever, whatever the label is.",
                    "label": 1
                },
                {
                    "sent": "Eliminate at least a row fraction of Q. OK, so you have pairs.",
                    "label": 0
                },
                {
                    "sent": "You know that some hypothesis are well, at least when I bought.",
                    "label": 0
                },
                {
                    "sent": "This is perfect.",
                    "label": 0
                },
                {
                    "sent": "That means if any hypothesis disagrees with any label, you can throw it away.",
                    "label": 0
                },
                {
                    "sent": "Appears illuminated if either of its policies disagree with the label.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how many pairs do we throw away?",
                    "label": 0
                },
                {
                    "sent": "Or which fraction of the pairs we throw away?",
                    "label": 0
                },
                {
                    "sent": "And row is sort of 1 minus Roque controls.",
                    "label": 0
                },
                {
                    "sent": "The fraction that we throw away.",
                    "label": 0
                },
                {
                    "sent": "And then what you can prove is that if you're throwing away.",
                    "label": 0
                },
                {
                    "sent": "A significant fraction.",
                    "label": 0
                },
                {
                    "sent": "Pairs you quickly converge.",
                    "label": 0
                },
                {
                    "sent": "So either log one over epsilon structure.",
                    "label": 0
                },
                {
                    "sent": "Information gain.",
                    "label": 0
                },
                {
                    "sent": "So this is not precisely information gain.",
                    "label": 0
                },
                {
                    "sent": "This is a kind of a more discrete version of it.",
                    "label": 0
                },
                {
                    "sent": "So you throw away a pair.",
                    "label": 0
                },
                {
                    "sent": "If I have a hypothesis in pairs, is this is invalid, yeah?",
                    "label": 0
                },
                {
                    "sent": "What why do you do it that way rather than just?",
                    "label": 0
                },
                {
                    "sent": "Well, you do not steal finds it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so it's it's a little bit tricky here, so it's not sufficient to throw away a significant fraction of hypotheses.",
                    "label": 0
                },
                {
                    "sent": "Either way, because.",
                    "label": 0
                },
                {
                    "sent": "Let's count Ricky.",
                    "label": 0
                },
                {
                    "sent": "It may be that there aren't many.",
                    "label": 0
                },
                {
                    "sent": "There are ex is which which which don't throw away a significant fraction of hypothesis, but both raising affection pairs and then something useful happens.",
                    "label": 0
                },
                {
                    "sent": "QM7 somehow related to packing setup hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "You have to find packing set for me.",
                    "label": 0
                },
                {
                    "sent": "OK, so like covering goal.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose you have a separate set and then you want to choose a subset of elements that sets such that for the covering and all elements of that set can be covered based on some notion of Northeast dance like that, but here the packing is that.",
                    "label": 0
                },
                {
                    "sent": "So I guess the difference is that we're working with pairs of prophecies.",
                    "label": 0
                },
                {
                    "sent": "But otherwise, you could certainly run this argument on a cover of Q.",
                    "label": 0
                },
                {
                    "sent": "But I don't think it's actually necessary.",
                    "label": 0
                },
                {
                    "sent": "Because you know what you can show is that if you're showing away, throwing away a significant fraction, then you're making progress in terms of.",
                    "label": 0
                },
                {
                    "sent": "The error rate of the remaining sets of bases.",
                    "label": 0
                },
                {
                    "sent": "Another related question is that what determines role to be small or large?",
                    "label": 0
                },
                {
                    "sent": "That's the structure of the vast space.",
                    "label": 0
                },
                {
                    "sent": "So some of these spaces are very friendly to active learning and some are not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the idea.",
                    "label": 0
                },
                {
                    "sent": "Diminish the size of age.",
                    "label": 0
                },
                {
                    "sent": "Take spread.",
                    "label": 0
                },
                {
                    "sent": "A subset of page that is very split up according to your distance and maybe.",
                    "label": 0
                },
                {
                    "sent": "Small edge maybe?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the splitting rate and this gives us some notion of what the sample complexity is here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we want to think about model selection.",
                    "label": 1
                },
                {
                    "sent": "OK, so we have a set of pasta spaces before.",
                    "label": 0
                },
                {
                    "sent": "And then we make a basic observation.",
                    "label": 0
                },
                {
                    "sent": "Observation is that this community.",
                    "label": 0
                },
                {
                    "sent": "Does not depend upon the labels.",
                    "label": 0
                },
                {
                    "sent": "Right and now we can.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back.",
                    "label": 0
                },
                {
                    "sent": "Then go.",
                    "label": 0
                },
                {
                    "sent": "That means.",
                    "label": 0
                },
                {
                    "sent": "This quantity doesn't depend on the labels.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go home.",
                    "label": 0
                },
                {
                    "sent": "There's a really simple algorithm for doing this.",
                    "label": 0
                },
                {
                    "sent": "We can just.",
                    "label": 0
                },
                {
                    "sent": "Compute.",
                    "label": 0
                },
                {
                    "sent": "Choose the smallest and then run active learning on it.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow is your lower bound of overall exercise low.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, not overall exits over the X is that you are going to ask for labels on.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in the paper there's actually more complicated definition saying that every tile examples you see arose building X, but.",
                    "label": 0
                },
                {
                    "sent": "What's important is that you can grow split.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Something kind of magical happened here.",
                    "label": 0
                },
                {
                    "sent": "Because we have this reliability assumption, we can just choose the best path to set to do active learning on.",
                    "label": 0
                },
                {
                    "sent": "And then go with it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So unlabeled complexity could grow up, but the cost?",
                    "label": 1
                },
                {
                    "sent": "Of labels is 0.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of surprising.",
                    "label": 0
                },
                {
                    "sent": "And then you think back to.",
                    "label": 0
                },
                {
                    "sent": "Because you need to measure this to high precision on a lot of different both sets.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 1
                },
                {
                    "sent": "So this is assuming we know exactly then all we have to estimate it.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming that we know exactly here, but.",
                    "label": 0
                },
                {
                    "sent": "If you have a sufficiently large pool, you can estimate it to sufficiently large accuracy.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could chase a lot of epsilons down there if you wanted to.",
                    "label": 0
                },
                {
                    "sent": "May want to call it Alpha because we have epsilon are used.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we're kind of confused by this, because supervised learning, which seemed simpler had a non zero label complexity and non 0 cost of model selection.",
                    "label": 1
                },
                {
                    "sent": "So now we think back to supervised model selection and if we add in realize ability turns out the same kind of algorithm works fine.",
                    "label": 0
                },
                {
                    "sent": "So the cost is also zero for reliability for realizable election in supervised learning.",
                    "label": 1
                },
                {
                    "sent": "OK. Work that.",
                    "label": 0
                },
                {
                    "sent": "They were shamans.",
                    "label": 1
                },
                {
                    "sent": "Did using a kind of distance measure like yours?",
                    "label": 0
                },
                {
                    "sent": "For model selection with supervised learning, but I don't think he had zero.",
                    "label": 0
                },
                {
                    "sent": "Well he may not have the reliability.",
                    "label": 0
                },
                {
                    "sent": "The reliability is very powerful.",
                    "label": 1
                },
                {
                    "sent": "OK so this is realizable active learning.",
                    "label": 0
                },
                {
                    "sent": "Community fights also zero in realisable surprised, and so in.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Supervised learning so.",
                    "label": 0
                },
                {
                    "sent": "If you're in the realizable case, this value here is 0.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And I guess you can get rid of the square root and maybe the two something like that.",
                    "label": 0
                },
                {
                    "sent": "But the point is that this quantity here is computed is computable without any examples.",
                    "label": 0
                },
                {
                    "sent": "And that means that you can just find the smallest path to set.",
                    "label": 0
                },
                {
                    "sent": "It's realizable so.",
                    "label": 0
                },
                {
                    "sent": "And slow down small subset.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you have a set of both sets.",
                    "label": 0
                },
                {
                    "sent": "Said is here.",
                    "label": 0
                },
                {
                    "sent": "You can just find the smallest about the set and.",
                    "label": 0
                },
                {
                    "sent": "Go with it.",
                    "label": 0
                },
                {
                    "sent": "That's cheating a bit.",
                    "label": 0
                },
                {
                    "sent": "We're using reliability assumption quite a bit.",
                    "label": 0
                },
                {
                    "sent": "Each model so of course I will take this model, it's a bit.",
                    "label": 0
                },
                {
                    "sent": "I think that's more impressive in activities here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so now you can ask yourself.",
                    "label": 0
                },
                {
                    "sent": "What happens?",
                    "label": 0
                },
                {
                    "sent": "I mean, the same issue comes up in active learning, right?",
                    "label": 0
                },
                {
                    "sent": "So I've defined reliability.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Their way, which says that every hypothesis said every hypothesis has a perfect apophysis.",
                    "label": 0
                },
                {
                    "sent": "Right, that's one way to extend reliability into sort of mini posets regime.",
                    "label": 0
                },
                {
                    "sent": "You could also say, well, suppose that only one, but this set has a.",
                    "label": 0
                },
                {
                    "sent": "Has it and I claim that's equivalent to agnostic case.",
                    "label": 0
                },
                {
                    "sent": "Because you can have an enormous gigantic about this set which contains the true path sis.",
                    "label": 0
                },
                {
                    "sent": "But you can never learn from because it's just enormous.",
                    "label": 0
                },
                {
                    "sent": "Even log of number of paths is too large.",
                    "label": 0
                },
                {
                    "sent": "And then you suck.",
                    "label": 0
                },
                {
                    "sent": "OK so this is realizable active learning.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we want to think about agnostic active learning.",
                    "label": 1
                },
                {
                    "sent": "This is a bit newer.",
                    "label": 0
                },
                {
                    "sent": "If you, if you look at.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The the algorithms for realizable active learning.",
                    "label": 1
                },
                {
                    "sent": "There would be things which involve.",
                    "label": 1
                },
                {
                    "sent": "Choosing an X asking for the label and then throwing away all inconsistent hypotheses.",
                    "label": 0
                },
                {
                    "sent": "For agnostic active learning.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have to be a lot more careful because.",
                    "label": 0
                },
                {
                    "sent": "You can't throw out the good ex, right?",
                    "label": 0
                },
                {
                    "sent": "So there's been a sequence of papers defining how we can do agnostic active learning in a nontrivial way.",
                    "label": 1
                },
                {
                    "sent": "The setting is sort of.",
                    "label": 1
                },
                {
                    "sent": "The setting is converge to kind of stream setting where you see a stream of unlabeled examples, and for each you choose when you see it to either request a label or not.",
                    "label": 0
                },
                {
                    "sent": "So it's an online setting, so there's no going back and choosing a different.",
                    "label": 1
                },
                {
                    "sent": "No, not making no revisiting decisions.",
                    "label": 0
                },
                {
                    "sent": "And then you want to run online until you find a natural optimal hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So that's it's pretty forward.",
                    "label": 0
                },
                {
                    "sent": "This is the style of online agnostic active learning algorithm that converge to buy, at least by this paper and.",
                    "label": 1
                },
                {
                    "sent": "And similar things are used for these later papers.",
                    "label": 0
                },
                {
                    "sent": "You start with a set of bases.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to run through unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "You see the unlabeled example.",
                    "label": 0
                },
                {
                    "sent": "You have your current set of offseason.",
                    "label": 0
                },
                {
                    "sent": "You ask, does there exist two apostasies which disagree on this unlabeled example?",
                    "label": 0
                },
                {
                    "sent": "And if so, you query.",
                    "label": 0
                },
                {
                    "sent": "An when you query you get some information.",
                    "label": 1
                },
                {
                    "sent": "You can compute sample complexity bounds on the difference in Area 2 prophecies.",
                    "label": 0
                },
                {
                    "sent": "The difference is important.",
                    "label": 0
                },
                {
                    "sent": "You can't do it for individual hypotheses, but you can do it for the difference efficiently.",
                    "label": 0
                },
                {
                    "sent": "And that allows you to throw away some apostasy so you can do some sort of filtration process here.",
                    "label": 0
                },
                {
                    "sent": "And throw a hypothesis which have been proved sub optimal.",
                    "label": 0
                },
                {
                    "sent": "Do you know the error rates?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "I mean the agnostic.",
                    "label": 0
                },
                {
                    "sent": "Learning learning relative to the best in class.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, we're learning it relative to the best in class, so we're just trying to.",
                    "label": 0
                },
                {
                    "sent": "So epsilon optimal means.",
                    "label": 0
                },
                {
                    "sent": "The office that you return at the end is within Epsilon in true error rate of the best in the class.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so once again the label complexity is complex.",
                    "label": 1
                },
                {
                    "sent": "If you don't make any, if you don't.",
                    "label": 0
                },
                {
                    "sent": "If you don't try to introduce.",
                    "label": 0
                },
                {
                    "sent": "These strange things like this data, which is a disagreement coefficient.",
                    "label": 0
                },
                {
                    "sent": "Then there's a lower bound which says that active learning is no in the agnostic case is no better than supervised learning in agnostic case.",
                    "label": 1
                },
                {
                    "sent": "So the minimum error rate of our apophysis over across the set.",
                    "label": 0
                },
                {
                    "sent": "Is going to be one of the parameters in our same complexity bound?",
                    "label": 0
                },
                {
                    "sent": "OK, So what does this look like?",
                    "label": 0
                },
                {
                    "sent": "So first of all I should mention one other thing which is that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can prove for this style of algorithm that the.",
                    "label": 0
                },
                {
                    "sent": "After you run over T unlabeled examples, if you're asking for the label every time, you'd get some error rate with supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And you can show that you get an error rate using this approach, which is within a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "And they will just always be true, although we might end up if we're unlucky in our choice of path set or choice of a problem, we might end up asking for every label, so it's equivalent supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now the question is how many labels do we have to pay for in order to achieve that error rate or that regret which is within a factor of 2 of what you would get with supervised learning?",
                    "label": 0
                },
                {
                    "sent": "And the claim is, it's this kind of beastly thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all M is a supervised label complexity.",
                    "label": 1
                },
                {
                    "sent": "You can think of this as T. Maybe T is a better variable to think about.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 1
                },
                {
                    "sent": "This disagreement coefficient will define in a moment you have the minimum error rate times T. So this is sort of.",
                    "label": 0
                },
                {
                    "sent": "The level of querying that you expect due to noise.",
                    "label": 0
                },
                {
                    "sent": "Because if you have a is the best path is an error rate of new.",
                    "label": 0
                },
                {
                    "sent": "Then, after M unlabeled examples, there's going to be new in examples where the label disagrees.",
                    "label": 0
                },
                {
                    "sent": "And so you end up needing to query.",
                    "label": 0
                },
                {
                    "sent": "You don't know in advance that they can disagree, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we have log of T. Times log number, but these times log T plus log one over Delta.",
                    "label": 0
                },
                {
                    "sent": "So the important thing is we have a log here.",
                    "label": 0
                },
                {
                    "sent": "We have a log here and we have a Theta there and if new is small enough this all ends up being better than him.",
                    "label": 0
                },
                {
                    "sent": "But but new needs to be small enough.",
                    "label": 0
                },
                {
                    "sent": "There is a disagreement coefficient, right?",
                    "label": 0
                },
                {
                    "sent": "So I need to define that for you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a little bit different.",
                    "label": 0
                },
                {
                    "sent": "So this is the formula.",
                    "label": 0
                },
                {
                    "sent": "So you pick a radius around the optimal hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You ask, does there exist another hypothesis within a ball of this radius which disagrees with the optimal hypothesis?",
                    "label": 0
                },
                {
                    "sent": "OK, and now you want to know what is the probability.",
                    "label": 0
                },
                {
                    "sent": "That there is some X for which you get a disagreement.",
                    "label": 0
                },
                {
                    "sent": "How many did everything?",
                    "label": 0
                },
                {
                    "sent": "They are.",
                    "label": 0
                },
                {
                    "sent": "OK so you think?",
                    "label": 0
                },
                {
                    "sent": "Takes a little while to unpack.",
                    "label": 0
                },
                {
                    "sent": "So if you only care bout hypotheses, so imagine that we're running this algorithm.",
                    "label": 0
                },
                {
                    "sent": "We're kind of throwing away bad hypotheses.",
                    "label": 0
                },
                {
                    "sent": "The ones which are are very bad will be thrown away quickly.",
                    "label": 0
                },
                {
                    "sent": "The ones which remain will be decent, and we want to keep throwing things away, so there's some radius that we care about at some point in the algorithm, because maybe all the hypothesis of an error rate of have a regret of R or less.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we only care about policies which are within R of the optimal, this is according to the distance defined on the bothies.",
                    "label": 1
                },
                {
                    "sent": "Which fraction of the X are disagreed upon?",
                    "label": 1
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "That's what the numerator is.",
                    "label": 0
                },
                {
                    "sent": "And then you need to divide by R to kind of make the units workout.",
                    "label": 0
                },
                {
                    "sent": "So you want sort of unit list thing, right?",
                    "label": 0
                },
                {
                    "sent": "So this will grow with R. But if you divide by R, then you know this quantity may not.",
                    "label": 0
                },
                {
                    "sent": "Grow with our.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Space right yes?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the previous case I did it's right here.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can find it so you can define the codes and stuff.",
                    "label": 0
                },
                {
                    "sent": "So we have a distance metric just induced by the unlabeled data and that means we have balls and everything else.",
                    "label": 0
                },
                {
                    "sent": "So part of the question is that if they knew the minimum, so if the new is 0, would you get the same result as here?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think not naively.",
                    "label": 0
                },
                {
                    "sent": "So if new is 0, this is like log.",
                    "label": 0
                },
                {
                    "sent": "I think it's close actually.",
                    "label": 0
                },
                {
                    "sent": "Theater is the definition of Theta is different.",
                    "label": 0
                },
                {
                    "sent": "No, you don't get the same result because it's not the same algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we need a more passive algorithm in the agnostic setting.",
                    "label": 0
                },
                {
                    "sent": "So in the in the realizable setting we can just.",
                    "label": 0
                },
                {
                    "sent": "You can query anything and worst case just doesn't help you throw away better batteries.",
                    "label": 0
                },
                {
                    "sent": "But in the agnostic setting, you need to be very careful about how you measure the difference in error rates.",
                    "label": 0
                },
                {
                    "sent": "Just make sure it makes sense.",
                    "label": 0
                },
                {
                    "sent": "Respect to the final distribution.",
                    "label": 0
                },
                {
                    "sent": "So you don't recover the realizable case.",
                    "label": 0
                },
                {
                    "sent": "The function of all four.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, we would take a Max.",
                    "label": 0
                },
                {
                    "sent": "If you have margin right then usually even our goes to zero data goes.",
                    "label": 0
                },
                {
                    "sent": "Yeah you you can exclude the ours which are very small so it's the Max over R is greater than some epsilon.",
                    "label": 0
                },
                {
                    "sent": "It could be, but it can also be too.",
                    "label": 0
                },
                {
                    "sent": "Right, so for intervals, the line is too.",
                    "label": 0
                },
                {
                    "sent": "I guess the thing which is maybe interesting about this is that every example where we know how to do agnostic active learning, the disagreement coefficient characterizes why it's possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Agnostic active learning and now the question is what is model selection mean?",
                    "label": 1
                },
                {
                    "sent": "And we have a problem.",
                    "label": 0
                },
                {
                    "sent": "The problem is that new depends upon the labels.",
                    "label": 0
                },
                {
                    "sent": "So we cannot.",
                    "label": 0
                },
                {
                    "sent": "We can't do the trick we did before.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And now we're pretty stuck.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ugh.",
                    "label": 0
                },
                {
                    "sent": "The best thing I know how to do is something like round Robin.",
                    "label": 1
                },
                {
                    "sent": "So you have an agnostic active learner on each individual hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And then you just rotate through different.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis sets.",
                    "label": 0
                },
                {
                    "sent": "Checking to see if it queries and if it queries you, get to Y and then you update that particular one.",
                    "label": 0
                },
                {
                    "sent": "So this is not a very satisfying solution.",
                    "label": 0
                },
                {
                    "sent": "But this is the best that I know.",
                    "label": 1
                },
                {
                    "sent": "But you can also use a band.",
                    "label": 0
                },
                {
                    "sent": "You I know you can't, so this is a funny thing about active learning.",
                    "label": 0
                },
                {
                    "sent": "In active learning, if you're trying to do active learning, path is set.",
                    "label": 0
                },
                {
                    "sent": "It is possible to return.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Normal hypothesis faster than you can evaluate its error rate.",
                    "label": 0
                },
                {
                    "sent": "Not related to.",
                    "label": 0
                },
                {
                    "sent": "Do you like the learning process?",
                    "label": 0
                },
                {
                    "sent": "Is the minimum of.",
                    "label": 0
                },
                {
                    "sent": "Diane, I'm in love you bye for this class.",
                    "label": 0
                },
                {
                    "sent": "So if you have a lower bound on some at age, you know that new is.",
                    "label": 0
                },
                {
                    "sent": "In the worst case, at least that.",
                    "label": 0
                },
                {
                    "sent": "She's not perfect.",
                    "label": 0
                },
                {
                    "sent": "I see what you're saying.",
                    "label": 0
                },
                {
                    "sent": "So what you're saying is you can get a lower bound estimate on new and they can use that to eliminate some of these sets.",
                    "label": 0
                },
                {
                    "sent": "And that's true.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to use that kind of analysis too.",
                    "label": 0
                },
                {
                    "sent": "Getting better than in the worst case.",
                    "label": 0
                },
                {
                    "sent": "But we're actually not done yet, even with round Robin, because each individual active learning algorithm may have some particular candidate apophysis.",
                    "label": 0
                },
                {
                    "sent": "But you don't know which of those is is the best.",
                    "label": 0
                },
                {
                    "sent": "So you need to run active learning one more time on those and you can interleave that with.",
                    "label": 0
                },
                {
                    "sent": "With this and I think you can still manage to not sell things down too much more, but.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is.",
                    "label": 0
                },
                {
                    "sent": "A label complexity which is met.",
                    "label": 0
                },
                {
                    "sent": "Magnitude of them worse.",
                    "label": 0
                },
                {
                    "sent": "At that stuff now this seems terrible.",
                    "label": 0
                },
                {
                    "sent": "In many ways it is.",
                    "label": 0
                },
                {
                    "sent": "Let me mention that it is better than another dumb strategy which is just taking all the office all policies and mushing them together into one big enough to set.",
                    "label": 0
                },
                {
                    "sent": "So it is there you get and if there it's Infinity instead of merely magnitude of in.",
                    "label": 0
                },
                {
                    "sent": "So we kind of we lost here and we lost a lot more than we lost in the supervised case.",
                    "label": 0
                },
                {
                    "sent": "If there's a way to get around this, that would be pretty interesting that, but I don't see how to get around this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now let's switch to contextual bandits.",
                    "label": 0
                },
                {
                    "sent": "So the first additional bedrooms each before.",
                    "label": 0
                },
                {
                    "sent": "Which is schema quite awhile ago there's several variants of each before there's a high probability version, and then this is a new set of algorithms, which is which are very different from each.",
                    "label": 0
                },
                {
                    "sent": "Before I'm going to be talking about each P4P, which is the minor variation of before we talked about in this paper.",
                    "label": 0
                },
                {
                    "sent": "The reason is because I want things to be compatible, so each before has a regret bound, which holds an expectation but not in high probability.",
                    "label": 0
                },
                {
                    "sent": "And here we modify things to hold my probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the contextual bandit setting, it's a different setting.",
                    "label": 0
                },
                {
                    "sent": "It's similar, though we see examples and now each step we're going to choose an individual action.",
                    "label": 1
                },
                {
                    "sent": "And then we get some particular loss for that action.",
                    "label": 1
                },
                {
                    "sent": "So compared to active learning, we're choosing whether not to query here, we're always querying in some sense of recording a particular action, and we don't get feedback about the other actions.",
                    "label": 0
                },
                {
                    "sent": "So there's an exploration problem because we don't get feedback about the other actions.",
                    "label": 0
                },
                {
                    "sent": "So each before P looks something like this, we start with a weight which is 1 on every individual hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And then we run through examples.",
                    "label": 0
                },
                {
                    "sent": "First step is we normalize the weights and we compute some sort of distribution over the prophecies.",
                    "label": 0
                },
                {
                    "sent": "This is modified slightly to impose a minimum probability distribution probability on any individual action.",
                    "label": 0
                },
                {
                    "sent": "But look for that.",
                    "label": 0
                },
                {
                    "sent": "Then we observe our features.",
                    "label": 0
                },
                {
                    "sent": "We draw our hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And we we act as a checks me drink.",
                    "label": 0
                },
                {
                    "sent": "I think we're good here.",
                    "label": 0
                },
                {
                    "sent": "It's more difficult than expected, doesn't seem to be A twist up.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "I shouldn't use a USB key.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'll wait.",
                    "label": 0
                },
                {
                    "sent": "Good idea.",
                    "label": 1
                },
                {
                    "sent": "Everywhere.",
                    "label": 0
                },
                {
                    "sent": "We found another can opener.",
                    "label": 0
                },
                {
                    "sent": "OK, so we.",
                    "label": 0
                },
                {
                    "sent": "We observe X.",
                    "label": 0
                },
                {
                    "sent": "We draw our hypothesis from this distribution.",
                    "label": 1
                },
                {
                    "sent": "We act according to their office, and we observe a particular reward.",
                    "label": 0
                },
                {
                    "sent": "And now we update our distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit hard to read.",
                    "label": 0
                },
                {
                    "sent": "There's some minimum probability which I didn't really tell you about up here that we imposed.",
                    "label": 0
                },
                {
                    "sent": "So we have the minimum probability times the reward divided by the probability of taking the action.",
                    "label": 0
                },
                {
                    "sent": "So in order to compute this piece of Bay need to integrate over the draws of H. So many different ages could choose the same A and you want to add up all those probabilities.",
                    "label": 0
                },
                {
                    "sent": "And then we have an indicator function of.",
                    "label": 0
                },
                {
                    "sent": "Whether or not they passed this, they were updating.",
                    "label": 0
                },
                {
                    "sent": "Agrees with hypothesis that we chose.",
                    "label": 0
                },
                {
                    "sent": "And then we add a little bit more and then it succeeded by probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is the outline of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Gives you a sense of how things work.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we can prove.",
                    "label": 0
                },
                {
                    "sent": "Is there?",
                    "label": 0
                },
                {
                    "sent": "For all assets.",
                    "label": 0
                },
                {
                    "sent": "For all sequences, this is a stronger quantification then we used before.",
                    "label": 1
                },
                {
                    "sent": "With high probability.",
                    "label": 0
                },
                {
                    "sent": "The average pround regret.",
                    "label": 0
                },
                {
                    "sent": "Is something like this?",
                    "label": 0
                },
                {
                    "sent": "So this is exactly what you saw before with the.",
                    "label": 0
                },
                {
                    "sent": "With supervised learning, it's actually worse in the constants and in general.",
                    "label": 0
                },
                {
                    "sent": "I guess you have more than two actions, but we only have two actions here, so it's a constant so suppressed.",
                    "label": 1
                },
                {
                    "sent": "So typically you would have a depends on the number of actions dividing T essentially, so you can think of it as multiplying log network prophecies.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a standard ESB, four P analysis.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now the question is.",
                    "label": 0
                },
                {
                    "sent": "How does model selection work?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we go back here and we see this is a regret.",
                    "label": 0
                },
                {
                    "sent": "This is the difference between error rate of the best and the error rate that we achieved with our conditional banded algorithm.",
                    "label": 0
                },
                {
                    "sent": "And we can't know that in advance.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're stuck again.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, all that I have for you is round Robin.",
                    "label": 0
                },
                {
                    "sent": "And this seems to be.",
                    "label": 0
                },
                {
                    "sent": "I mean the structure of.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The XP 4 so algorithms is really conducive to wanting to work better because.",
                    "label": 0
                },
                {
                    "sent": "Can generalize he sympathies to be arbitrary experts, and these experts can output a distribution over actions.",
                    "label": 0
                },
                {
                    "sent": "Just like each before P produces, it produces distribution over actions implicitly here.",
                    "label": 0
                },
                {
                    "sent": "And you would love to plug things together in some sort of.",
                    "label": 0
                },
                {
                    "sent": "You know modular way, but it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "The regret bands don't plug together.",
                    "label": 0
                },
                {
                    "sent": "And the counterexample is.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have a small hypothese ET and you have a really large path to set.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you're going to compete with a small pasta set need to converge quickly, do something which is equivalent to the best in that small pasta set.",
                    "label": 0
                },
                {
                    "sent": "But we're going to achieve a.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arute regret.",
                    "label": 0
                },
                {
                    "sent": "That means that the probability of.",
                    "label": 0
                },
                {
                    "sent": "Of querying.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "According to the expert, the other pasta set has to be something like one over root tea.",
                    "label": 0
                },
                {
                    "sent": "And that means that you can't converge quickly enough on the other on the big about to set to also compete with the big path to set in some sort of.",
                    "label": 0
                },
                {
                    "sent": "More rapid than round Robin fashion.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's going to.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hand WAVY we should probably workout a lower bound explicitly, but yeah.",
                    "label": 0
                },
                {
                    "sent": "You have a distance between the different purposes sets, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, well you have a distance between the different, but it says we could.",
                    "label": 0
                },
                {
                    "sent": "You could restrict yourself to explore the guides which are sufficiently close to the best individual in the in the smallest set.",
                    "label": 0
                },
                {
                    "sent": "So first of all, you don't know what the best individual in the smaller set is.",
                    "label": 0
                },
                {
                    "sent": "You could restrict the bigger buses set to be things close to the smaller pasta set.",
                    "label": 0
                },
                {
                    "sent": "But now we essentially have what we're no longer trying to compete with the bigger path to set.",
                    "label": 0
                },
                {
                    "sent": "Right, so I think the game I'm trying to play is.",
                    "label": 0
                },
                {
                    "sent": "I don't know what the pasta set which has set in supposed to compete with in advance.",
                    "label": 0
                },
                {
                    "sent": "And now I want to whatever the truth is.",
                    "label": 0
                },
                {
                    "sent": "I want to compete with that one without paying too much penalty compared to if I had known it in advance.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like the regret of model selection or something like that.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm trying to define and get at.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, we're kind of stuck here, yeah?",
                    "label": 0
                },
                {
                    "sent": "Right on top of this set, yeah won't work because of this exploration example.",
                    "label": 0
                },
                {
                    "sent": "I just told you.",
                    "label": 0
                },
                {
                    "sent": "If you are playing a multi armed bandit.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The trouble is that so you have two levels right?",
                    "label": 0
                },
                {
                    "sent": "So you have the top level and you have a lower level.",
                    "label": 0
                },
                {
                    "sent": "And if you're going to get a root T regret at the top level, that means you're going to starve at least one of your arms down to one over root probability of sampling.",
                    "label": 0
                },
                {
                    "sent": "If you starve, you should serve one of your algorithms down to one over the probability sampling.",
                    "label": 0
                },
                {
                    "sent": "If you're starving an algorithm to 1 / T probability sampling.",
                    "label": 0
                },
                {
                    "sent": "It can't learn quickly enough to actually achieve.",
                    "label": 0
                },
                {
                    "sent": "The area, so instead of getting like a root T, you get A at the.",
                    "label": 0
                },
                {
                    "sent": "3/4 regret.",
                    "label": 0
                },
                {
                    "sent": "So if you only get a sample every 1 / T samples, then it's like time slows down and you have to kind of.",
                    "label": 0
                },
                {
                    "sent": "You're going to get a sample 1 / M. So if I'm using round Robin, yes it's 1 / N. But if I'm using round Robin then.",
                    "label": 0
                },
                {
                    "sent": "Then then yes, I see.",
                    "label": 0
                },
                {
                    "sent": "So what I could do is I could run Exp 3 on the output of round Robin.",
                    "label": 0
                },
                {
                    "sent": "That would be a fun thing to do, but then I'm still going to lose with this factor of him because I'm playing round Robin.",
                    "label": 0
                },
                {
                    "sent": "We are going to conduction the best.",
                    "label": 0
                },
                {
                    "sent": "This subclass, so you don't converge to the best.",
                    "label": 0
                },
                {
                    "sent": "In the past, this subclass and less you explore enough, and then I bought this subclass.",
                    "label": 0
                },
                {
                    "sent": "You don't explore enough my pasta subclass in less.",
                    "label": 0
                },
                {
                    "sent": "You actually act according to the bathtub glass often enough.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Let's switch to reliable initial minutes so we have a new paper.",
                    "label": 1
                },
                {
                    "sent": "Which deals with the unstructured, realizable case.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "So as you back up a moment, first of all, we're good.",
                    "label": 0
                },
                {
                    "sent": "What does reliability mean in the connection and setting right?",
                    "label": 0
                },
                {
                    "sent": "Because?",
                    "label": 0
                },
                {
                    "sent": "Central clear.",
                    "label": 0
                },
                {
                    "sent": "What I mean is that instead of having a set of policies, they have a set of regressors.",
                    "label": 1
                },
                {
                    "sent": "And there's one of them, which predicts the expected loss correctly for every XNA.",
                    "label": 0
                },
                {
                    "sent": "OK, so there can be noise in the actual losses, but you need to predict the expected loss correctly.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a weaker notion of reliability than we used before with active learning.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "Is this new algorithm regressor elimination?",
                    "label": 0
                },
                {
                    "sent": "We start with a set of regressors.",
                    "label": 1
                },
                {
                    "sent": "We're going to be doing some sort of filtering operation, so we're going to be.",
                    "label": 0
                },
                {
                    "sent": "Getting new sets at each round.",
                    "label": 0
                },
                {
                    "sent": "We're going to choose a distribution over the set of regressors.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of a delicate operation that turns out to be possible.",
                    "label": 0
                },
                {
                    "sent": "You need to achieve near uniformity over the actions.",
                    "label": 0
                },
                {
                    "sent": "Chosen by Regressors in their remaining set.",
                    "label": 1
                },
                {
                    "sent": "So what does it mean for us to choose an action or issues in action?",
                    "label": 0
                },
                {
                    "sent": "If you take the argmax over?",
                    "label": 0
                },
                {
                    "sent": "Overall, the actions in you.",
                    "label": 0
                },
                {
                    "sent": "You take the arguments over the actions, right?",
                    "label": 0
                },
                {
                    "sent": "So we have an F of.",
                    "label": 0
                },
                {
                    "sent": "X, a.",
                    "label": 0
                },
                {
                    "sent": "Take argmax of, a this particular action?",
                    "label": 0
                },
                {
                    "sent": "We can say that Regressor choose that action on that X.",
                    "label": 0
                },
                {
                    "sent": "So we can convert regressors into policies.",
                    "label": 1
                },
                {
                    "sent": "OK, so that we.",
                    "label": 0
                },
                {
                    "sent": "We choose a distribution over aggressors, kind of delicately.",
                    "label": 0
                },
                {
                    "sent": "We observe X.",
                    "label": 0
                },
                {
                    "sent": "We draw F from P. We act according to the argmax action.",
                    "label": 0
                },
                {
                    "sent": "Observer reward and then we do some sort of filtration here.",
                    "label": 0
                },
                {
                    "sent": "And I guess the key thing here is it turns out that.",
                    "label": 0
                },
                {
                    "sent": "You can throw away all regressors, which is like a constant over T. Worse than the best.",
                    "label": 0
                },
                {
                    "sent": "It's a very aggressive filtration, but.",
                    "label": 0
                },
                {
                    "sent": "Just to be OK because of the reliability assumption.",
                    "label": 0
                },
                {
                    "sent": "So this is the last exploration setting that we're looking at.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about this?",
                    "label": 0
                },
                {
                    "sent": "First step, is it actually possible to get or to do this?",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Yes, it is possible.",
                    "label": 0
                },
                {
                    "sent": "The argument is pretty nontrivial.",
                    "label": 0
                },
                {
                    "sent": "You need to use a generalization of an moments mini Max.",
                    "label": 0
                },
                {
                    "sent": "So the argument that it exists is nonconstructive, but then once you know that it exists, you have optimization methods which are guaranteed to give you the solution.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would you improve is essentially the same kind of regret bound as you get with XP4 or P?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a let down aggressor.",
                    "label": 0
                },
                {
                    "sent": "They realize reception didn't give us anything more.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you're very careful with your quantifiers, you can show that sometimes is much better.",
                    "label": 1
                },
                {
                    "sent": "So in particular, you can get rid of.",
                    "label": 1
                },
                {
                    "sent": "The dependence on T and there was also depends on the number of actions, and they can also be gotten rid of.",
                    "label": 0
                },
                {
                    "sent": "You're lucky so.",
                    "label": 0
                },
                {
                    "sent": "Having these this realizable case can be powerful, but it's not necessarily it's.",
                    "label": 0
                },
                {
                    "sent": "It could be just as bad as a 64P.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So that's our regret bound.",
                    "label": 0
                },
                {
                    "sent": "And now we need to.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About model selection and this turns out to be easy also.",
                    "label": 1
                },
                {
                    "sent": "So we can just.",
                    "label": 0
                },
                {
                    "sent": "Every individual.",
                    "label": 0
                },
                {
                    "sent": "So it has a optimal, so this is computable without looking at your examples.",
                    "label": 0
                },
                {
                    "sent": "So you can just choose the best and go with it.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That means that our cost is a model selection to 0.",
                    "label": 0
                },
                {
                    "sent": "What about if you don't know which has the realisable?",
                    "label": 0
                },
                {
                    "sent": "They aren't always?",
                    "label": 0
                },
                {
                    "sent": "Then I think you're going to fall into the same kind of agnostic case as before.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the realizable one could be in.",
                    "label": 0
                },
                {
                    "sent": "Some sort of strange set that.",
                    "label": 0
                },
                {
                    "sent": "That's true, hold on.",
                    "label": 0
                },
                {
                    "sent": "Yeah, he just don't know which set it's in, so you need to use round Robin.",
                    "label": 0
                },
                {
                    "sent": "As far as I can tell.",
                    "label": 0
                },
                {
                    "sent": "And if you don't use round Robin, if using exploration method then he starts starving the algorithms of the information they need in order to actually converge.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm bout done.",
                    "label": 0
                },
                {
                    "sent": "This is the results that I discussed.",
                    "label": 0
                },
                {
                    "sent": "I think everything should be convinced you should be convinced of everything except for possibly these.",
                    "label": 0
                },
                {
                    "sent": "But I guess I strongly expect it will see that once you workout a lower bound and upper bound.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This was.",
                    "label": 0
                },
                {
                    "sent": "Interesting, you always kind of want to talk about something you don't quite know, because then you learn something.",
                    "label": 0
                },
                {
                    "sent": "So I had no idea that realized he was realizability.",
                    "label": 0
                },
                {
                    "sent": "With such a powerful thing respecting model selection.",
                    "label": 1
                },
                {
                    "sent": "It's extremely powerful.",
                    "label": 0
                },
                {
                    "sent": "And even without it.",
                    "label": 1
                },
                {
                    "sent": "In exploration settings, simply model selection becomes very hard.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So is there any middle ground between these two?",
                    "label": 0
                },
                {
                    "sent": "Because realizability tells that there is exactly this sort of the best one.",
                    "label": 0
                },
                {
                    "sent": "So the answer has to be yes.",
                    "label": 0
                },
                {
                    "sent": "But what stating that in a natural way seems to?",
                    "label": 0
                },
                {
                    "sent": "I don't know how to do.",
                    "label": 0
                },
                {
                    "sent": "Some sort of almost realized ability or something I don't know.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe you can say that.",
                    "label": 0
                },
                {
                    "sent": "The best type of assume that all the spaces.",
                    "label": 0
                },
                {
                    "sent": "Excellent close to the realisable.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then maybe some of these bounding things you're mentioning would become useful.",
                    "label": 0
                },
                {
                    "sent": "You're not talking about nested classes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I could necessary parties are special cases would have talked about.",
                    "label": 0
                },
                {
                    "sent": "I also didn't talk about Pacbase things, but that's also essentially special case because you can discretize the prior in the back bays and get a set of hypothesis sets.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Sort of accuracy to possibly decrease with the increasing complexity, and so you know would be unreasonable away in the realizable case is unreasonable.",
                    "label": 0
                },
                {
                    "sent": "Obviously just wanted to be in every hypothesis class, and if you were able to relax that that.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can't, but then maybe if you had some.",
                    "label": 0
                },
                {
                    "sent": "So I think one thing that you sort of need to do before you can get a better simply analysis.",
                    "label": 0
                },
                {
                    "sent": "So we need a better algorithm we need.",
                    "label": 0
                },
                {
                    "sent": "We have this algorithm for diagnostic case, which is not the same as the algorithm for that for the realizable case.",
                    "label": 0
                },
                {
                    "sent": "And now the question is, how do we get an algorithm which works for the agnostic case?",
                    "label": 0
                },
                {
                    "sent": "But it also gives you a good analysis for the realizable case.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And not, and not each of them and things.",
                    "label": 0
                },
                {
                    "sent": "But say one of them does.",
                    "label": 0
                },
                {
                    "sent": "Given the administrative Office classes in one of the offsets in one of the main Zero policies.",
                    "label": 0
                },
                {
                    "sent": "Smoked realisable case like multiple different things not very self contained.",
                    "label": 0
                },
                {
                    "sent": "That might be an interesting something to work with.",
                    "label": 0
                },
                {
                    "sent": "Just infinite plus.",
                    "label": 0
                },
                {
                    "sent": "It's eventually broken it.",
                    "label": 0
                },
                {
                    "sent": "Wouldn't you run into the same problem you?",
                    "label": 0
                },
                {
                    "sent": "Spoke of it, you know, the one could come in.",
                    "label": 0
                },
                {
                    "sent": "Very, very late, yeah?",
                    "label": 0
                },
                {
                    "sent": "Dress just basements.",
                    "label": 0
                },
                {
                    "sent": "Get some samples in the index of the opposite glass.",
                    "label": 0
                },
                {
                    "sent": "The first one that this is realisable.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I guess that would be kind of a more of a relaxed version of round Robin, right?",
                    "label": 0
                },
                {
                    "sent": "So instead of depending on the number of both sets, you depend upon the index of the first above to set.",
                    "label": 0
                },
                {
                    "sent": "There might be feasible something along those lines.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess I'm I'm done.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}