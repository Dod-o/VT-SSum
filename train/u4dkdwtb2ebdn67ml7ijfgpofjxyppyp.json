{
    "id": "u4dkdwtb2ebdn67ml7ijfgpofjxyppyp",
    "title": "Modeling Relational Data with Graph Convolutional Networks",
    "info": {
        "author": [
            "Thomas Kipf, Dept. of Social Science Informatics, University of Amsterdam"
        ],
        "published": "July 10, 2018",
        "recorded": "June 2018",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2018_kipf_convolutional_networks/",
    "segmentation": [
        [
            "Yeah, I'm a PhD student at University of Amsterdam.",
            "In this work was together with a number of collaborators both at the VU Amsterdam, which is the other big University in Amsterdam, and my Co supervisor Ivanti Tofus Naughty University of Edinburgh and my supervisor Max Spelling.",
            "So the idea behind this work is to all the motivation behind this work is.",
            "So how can we use like the progress and has recently been seen in neural network based learning?",
            "How can we use these kinds of methods on relational data or specifically on knowledge graphs and knowledge bases?",
            "And."
        ],
        [
            "Let me start with some quick introduction, motivation behind what I mean by deep learning and classical sense of the last few years we've seen maybe deep learning on grid structure data.",
            "So mostly there have been big successes in 2012, for example with the image net competition, where deep neural networks have excelled at classifying image data.",
            "And around the same time similar successes have been observed in field service, speech processing, or natural language processing, where the input you can think of it also is like kind of a grid structure data.",
            "Just you have a very if a sequence.",
            "And you have very regular kind of data structure, so you don't have any kind of graph structure behind it.",
            "And then.",
            "Few days later they were big successes in terms of modeling games like the game of goal or using games like Atari Games simulators.",
            "We can also think of them.",
            "You apply these deep neural networks that are very well optimized for this grid structure data to extract information from them.",
            "And these models mostly rely on 2 main principles that we think of, so they use some kind of.",
            "Weight sharing that basically means if you slide a filter over the data, you will share a lot of information in your loan parameters and if either very effective to learn."
        ],
        [
            "And at the same time you learn a hierarchical hierarchy of features, like if the input image has edges and later you learn compositions of edges like parts of faces in the later learn faces in an image.",
            "And what I mean by deep learning or end to end learning versus traditional learning kind of approach."
        ],
        [
            "This is most machine learning approaches in the passer.",
            "Even today, they rely on a certain pipeline of 1st.",
            "You try to extract some kinds of features from your data, like you build an embedding model.",
            "For example, you try to embed your data in some kind of space.",
            "Many freezes, embeddings and then later on top of that you learn a classifier which is separate model.",
            "And so the promise of sort of deep learning or end to end learning is you merge these two into one single step, so you would directly operate on the raw data and you have one model that he directly trained to optimize specific goal, like image classification for this case.",
            "But as we know like not all data comes in the form of grids."
        ],
        [
            "So we have a lot of real world examples of data that comes in the form of some kind of graph like you have social networks, citation networks, communication networks, or even like something like multi agent systems.",
            "When you want to build self driving cars."
        ],
        [
            "Um?",
            "You have molecular data.",
            "Which you can represent.",
            "This graphs have protein interaction networks which are very important in biology.",
            "And my work is mostly focusing on here that I'm presenting a sore, but I'll work mostly focused on this knowledge graphs."
        ],
        [
            "Um?",
            "Last years of something like road map.",
            "So many, many examples that you can bring up."
        ],
        [
            "And the standard deep learning approach is they don't really work for this kind of data like convolutional neural networks for example.",
            "And let me focus on this part here now.",
            "So the method that we use now."
        ],
        [
            "Work is.",
            "Based on graph neural networks or graph convolutional networks, these two terms basically mean the same.",
            "If you heard either of them and the bigger picture for these types of models is your past messages.",
            "So you have a multi layer model and at every layer in the model you pass messages between neighboring nodes and these messages there in the form of some nonlinear function so.",
            "Some neural network kind of function that you then learn in the end, and then you pass this message is in your grammar at their results.",
            "I'm going to get into more detail later, so the notation that I'm going to use here is you have a graph, a set of vertices and edges, and equivalently you can also represent this in this case as an adjacency matrix and a feature matrix if you have any kinds of features on your data points.",
            "So let's say for every node you could have an F dimensional feature vector, which would then summarizing this feature matrix."
        ],
        [
            "And let's quickly have a brief look on how we would usually do like convolutional neural network on grid structured data.",
            "So there in a classical picture you would slide a filter over the image and this filter multiplies parameter values with the input of the image.",
            "But it can equivalently represent this as sort of a graph transformation or message passing operation, where, let's say, if you want to update the center pixel.",
            "Here the pixel in the middle order node in the middle you pass messages from the neighboring nodes.",
            "And.",
            "Now, these messages here are these.",
            "Vectors, age, they are hidden representation.",
            "In this neural networks or activations in this model, and if you want to perform a single update of the pixel in the middle, you transform each of these messages.",
            "These vectors without learned parameter matrix W and then you Add all the results up.",
            "So in the end you have an update like this, so you.",
            "Transform each of the neighbors individually by some certain transformation rule depending on what their relation is.",
            "If they're like an upper left neighbor and upper right neighbor of this pixel, and then you get the final.",
            "Linear map of these neighboring pixels and you passing through some nonlinear function, here denoted by a sigmoid.",
            "And this could be any kind of non linearity that is differentiable."
        ],
        [
            "Now, how do we generalize this to graphs?",
            "So here consider this undirected graph, and we want to update the node in the middle in red.",
            "And.",
            "Now since if if you just have an undirected graph, we don't have any attributes, it goes as follows.",
            "So we don't have any information about an upper left neighbor and upright neighbor, so we treat all of them the same.",
            "And then we transform each of these neighbor values by some linear transform.",
            "You could also extend this to nonlinear transform, but most of the time we find linear is good enough and gives you some yeast for implementing it.",
            "And.",
            "You normalize these messages now by some normalization factor, because if you would leave this normalization constant out then you could imagine that you have, like one.",
            "Note suddenly can have 1000 neighbors or million neighbors, and you need to make sure that all these activations are mounted so you can.",
            "Have can you straighten optimization without any gradients suddenly blowing up in your model?",
            "Now 1 issue still here is that if you want to scale this to really big graphs, you certainly would have to sum over incoming messages from all of your neighbors if we want to just know what the next update is for this one node so we can fix this issue.",
            "Addressed this issue by Subsampling Neighbors, which is something we haven't done in this work, but there has been related work that showed this for undirected graphs.",
            "Really been traded for Knowledge Graph, so this would be something interesting to try.",
            "Now just a list of properties from this model, so we have on the one hand now weight sharing like in convolutional neural networks for images, which basically means we use the same parameter matrix W 1.",
            "Here irrespective of the location where we are in the graph.",
            "And it's also invariant to permutations, so if you just change the order in which you present the neighbors, you get the same output.",
            "And you have linear complexity in terms of the number of the edges, so only to touch each edge once for every layer.",
            "And if you want to use this for learning on knowledge graphs, for example, you can apply this both in a setting where you know the full graph in advance and try to make predictions on that, or even you can train on a part of the graph and then make predictions on a different part of it 'cause the model is, it transfers its inductive and distance.",
            "The problem is that.",
            "This model right here would not have support for Edge features, so it doesn't know about any kind of relational features between neighbors.",
            "And if you want to build a deep model where you want to agglomerate information from multiple hops in your neighborhood, you would also need some kind of mechanism to make sure the gradients back propagate.",
            "If you want to train this model, they don't go to zero.",
            "If you make very deep models.",
            "So."
        ],
        [
            "So now I'm coming to all work here.",
            "We essentially presented a relational extension of this model so we can run this on knowledge graphs and now let me.",
            "Just introduce quickly what I mean by a knowledge base here.",
            "So we represented as a directed labeled graph.",
            "In this case, where we have a set of vertices, edges, and a set of relations.",
            "And each edge we represent as is triple where we have like a subject and object.",
            "VI VJ and a relation.",
            "And we have a number of different relation types.",
            "So for example, you have this example down here.",
            "So something like this could look like relation in this case in this representation.",
            "And.",
            "The extension of the model that we use in this case is very simple, so it's like going back to how we did it on images.",
            "So there we have if you interpreted set of having a upper left neighbor in upper right neighbor, then.",
            "Already you have different relations and transform them differently, and now we just translate this to what it means on a general graph where you have different relations type.",
            "So essentially what the formula here means is we assign a separate transformation for each edge type or for each relation type.",
            "So each relation gets transformed with a separate parameter matrix and the normalization constant.",
            "We also adapt for the separate parameter matrix and we only sum over.",
            "So each summation here only goes over the neighbors of a particular.",
            "Message type or a particular edge type.",
            "So we call this model to relational GCN in this case.",
            "Or are GCN.",
            "And.",
            "One issue that we tried to address and."
        ],
        [
            "Work also apart from introducing the simple extension is how to.",
            "Make this scale to bigger graph, so without subsampling, because subsampling can introduce some unwanted variant, so we wanted to see how big can we scale this without even going into the subsampling regime.",
            "And here now we represent a knowledge base as.",
            "Because we need some vectorized representation of that we represented as a set of adjacency matrices, and we have one adjacency matrix for each edge type.",
            "And.",
            "Still be half if you want to be half a feature matrix that summarizes initial entity features, for example.",
            "And we can elegantly summarize this formula that we had in the last slide by just using matrix multiplications of the adjacency matrices were not introduced.",
            "And the good part here is that so it still has linear complexity if we use a sparse representation of the adjacency matrix.",
            "And since we're concerned of running this at scale and on big data, and we have at the same time we need to calculate these expensive dense matrix multiplication, Ch times W and these.",
            "Need to be efficiently run on GPU typically so we can using some GPU libraries for like some deep learning libraries that now support also sparse dense matrix multiplication.",
            "We can efficiently implement this on GPU and let it run on larger datasets.",
            "So one last problem that occurs with this type of representation is that we need one large weight matrix for each relation type, and if you have like 1000 relations then suddenly have a lot of parameters.",
            "So we introduced two types of addressing this one, two solutions for addressing this.",
            "One is we use some form of weight sharing, so we share weights between relations.",
            "So we learn a set of basis relations an we just learn how they are combined in.",
            "Like every every relation is a combination.",
            "Now a linear combination of a set of basis relations and each of these spaces relations they have one parameter matrix, so this reduces.",
            "This largely reduces the number of parameters below the other one is they experimented with the sparsity constraints, so we just say the relation matrices they need to be blocked diagonal matrices, so they're in the end very sparse, and this the intuition behind this is that this block diagonal matrices to operate now.",
            "Only on certain dimensions of these hidden feature vectors, which also for image task has been found to work reasonably well."
        ],
        [
            "And.",
            "Now the idea of how to do entity classification or link prediction to typical tasks that you can think of in knowledge base is how we apply.",
            "This model is in a classical pipeline you would first extract embeddings, so we first run some kind of graph kernel or some kind of hand design feature descriptor or sum, like vert avec or skip gram based model that extracts features, embeddings and any trainer classifier on top of these frozen embeddings.",
            "So instead now with this end to end approach we.",
            "1st So we run this model, get an output at the end so we have multiple layers of these.",
            "Graph convolutions we call them, and at the end you have for each entity in the graph you have 1 hidden representation, which is essentially just a feature vector embedding.",
            "But now this embedding can be directly fed into the classification loss.",
            "For example of entity classification loss or a link prediction loss.",
            "And you can optimize them directly.",
            "Like now if you because everything is differentiable in this model, you can simply run gradient descent on all the parameters of this model.",
            "So in the end the model will figure out how to use.",
            "The graph structure and the features to optimize this specific loss directly without going through two step pipeline.",
            "And you can then, essentially, if you peek into the embedding said you get these hidden layers.",
            "You see that these embeddings are optimized for specific tasks like embeddings.",
            "For classification we have embeddings for link prediction.",
            "And just to visualize this."
        ],
        [
            "We had a toy experiment from some earlier work that I want to show you here that if you take an untrained model with just random parameters before you train it and feed in some graph like he's just a toy graph where some communities are marked with colors and if you just feed in this type of graph in this model and you inspect the output features you get, then even though the model is just randomly initialized and not trained yet, you get some some type of separation in this output embedding space.",
            "But this is without training yet, and this is of course depending on the initialization of many factors.",
            "But now if you want to optimize this to specif."
        ],
        [
            "We go like we want to classify certain nodes in this graph or entities.",
            "Then let's say you give it for each of these four different communities.",
            "Here you give it one label, one labeled node, and then you ask it to optimize embedding so that it can classify the unlabeled nodes."
        ],
        [
            "And when we look at what's happening now, we zoomed out of this initial embedding space, because these are very small randomized values and we start training with gradient descent.",
            "And we optimize for these embeddings to be good for a classification task, and now for each color is only one labeled.",
            "Note that it gets like gradient information from and the embeddings are now pulled towards the edges, so embeddings are.",
            "So the embedding so very easy to use for the classifier.",
            "Another classifier can essentially just draw 2 lines in that as a good classification in this case."
        ],
        [
            "And in the case for Knowledge Graph.",
            "So if you want to do entity classification, we have the input graph.",
            "We put our encoder on top and at the end we have a node specific loss.",
            "And we evaluate this loss on labeled notes only and the loss here is a cross entropy error.",
            "You could also use a mean squared error whatever you like in this case.",
            "And.",
            "Similarly, you can build a model for link prediction, which we did in this paper where you take the input.",
            "You put this encoder model on top, so this is only to be differentiable models and then you can have a decoder model.",
            "So very simple decoder in this case that predicts, so the decoder's job is to take two node embeddings and compare them in such a way that it can give you a score.",
            "As of how they are related.",
            "So.",
            "This mode is in a knowledge completion and knowledge graph completion literature often use model for this case and here.",
            "Essentially just take a bilinear product of two embeddings and transformative.",
            "Learned weight matrix in between and in this small dissuade metrics to be taken diagonal.",
            "But you can also think of taking a non diagonal matrix.",
            "And this can be trained by again a cross entropy error.",
            "Now this is binary.",
            "Therefore it looks a bit more complicated and a normalization now over all the edges.",
            "And to train this you also need to provide negative examples.",
            "So what we do in this case is we corrupt either an object or a subject in the relation just randomly corrupted and this entry is a negative example.",
            "So for each positive example we construct a certain number of negative examples and then we can train this.",
            "So in experiments we see."
        ],
        [
            "For entity classification, we tested this on a number of benchmark datasets that go from very small graphs with like 30,000 edges to big graphs with up to six 6 million edges.",
            "And.",
            "In this case, up to 133 relation types.",
            "And we compare this against some classical baselines like.",
            "First of all, Henderson feature, baseline, kernel based baseline, and a embedding based baseline.",
            "And what we see in the end is that so the results here.",
            "The results here a little bit mixed.",
            "So we see that on some of the datasets we get like, especially on FB we get a very big improvement.",
            "Where is another datasets.",
            "The improvements at the small or even.",
            "Losing compared to the feature based baseline on mutek.",
            "So we try to understand what's going wrong here and how it can be improved in future work.",
            "And the issue with the model we as we formulated right now is that it doesn't really understand the difference between a regular node in a graph or end like an entity and a type node, and so the type nodes live.",
            "So mu tect BGS is to datasets have very many type notes, and these have a very high node degree.",
            "And since we average all these messages, there's a lot of.",
            "Information being lost in this process, and so we think that.",
            "Recently there has been some.",
            "Work on attention mechanisms in these types of models, and we think that this can to some degree alleviate this issue."
        ],
        [
            "Now for link prediction experiments, the task is now to recover missing relations in a graph, so we assume that our knowledge graph is incomplete and we want to predict missing relations in this graph.",
            "And we optimize this encoder decoder architecture with a dismal decoder.",
            "And in this mode you would directly optimize these embeddings ES and go here and in our model we provide these with encoder model.",
            "So the encoder in this case propagates measure across the graph gets the embedding and then runs the decoder which allows it to accumulate evidence from multiple steps of reasoning.",
            "For this graph you could think of it in a way.",
            "And we also run on Sambol method which is not showing this slide but in later slide and what we see here is on.",
            "Freebase data said where inverse relations have been removed from the test set.",
            "So there's an issue with the benchmark datasets that has recently been discovered in link prediction at like a large fraction of the test set data is actually in the training set.",
            "If you just reverse the link so these are removed here in this data set and then we see that the model gets quite a substantial improvement over all this decoder baselines that only use single step.",
            "So they basically only directly optimize these embeddings and run a single.",
            "Step 4 in prediction versus we accumulate evidence of a multiple steps of propagation in this graph.",
            "So I'm going to only very briefly touch."
        ],
        [
            "Upon details like we train this model also with Edge drop out so to regularize it, we randomly drop edges in the encoder so it's similar to a denoising autoencoder way.",
            "We trained it in the end.",
            "And we also ran this model on the original data set where these inverse edges have not been removed and there our original model.",
            "What is comperable to this modern?",
            "The performance, but we found that if you combine the two in sort of an assemble model, we get.",
            "Improved results but.",
            "This mode has a very does a very good job at inferring these inverse links here in this case, but as you see, even the link field is a baseline here that just takes features of a relation and does a very good job at inferring these inverse relations.",
            "It's a hand assigned feature baseline, and it does.",
            "It gives you the best performance here across all these.",
            "Obviously learning based methods.",
            "Alright, I'm going to conclude here."
        ],
        [
            "And.",
            "I can summarize this that end to end learning or deep learning on this type of data on relational data is a competitive approach compared to what we call classical approaches where you have a multi step pipeline and for link prediction we see that.",
            "Enriching these triplet scoring methods that are sort of the state of the art in the field.",
            "If an encoder can even improve their performance, and it also enables to.",
            "Aggregate evidence over multiple steps of information propagation graph.",
            "So for future work, it would be interesting to look into attention mechanisms to get rid of these issues of high degree nodes that just average all kinds of information together and then you lose this information effectively.",
            "And at the same time, scalability something we have only managed to scale this to something like 3 million 6 million edges.",
            "I think in this model.",
            "But if you want to go to really big databases then or near big knowledge basis then you would have to use some kind of sub sampling techniques and there have been a number of papers this year like Fast GC and in this case it does some type of important sampling to reduce the variance.",
            "In September 17 notes.",
            "And if you want to learn."
        ],
        [
            "Or about this work, then have a look at our paper.",
            "This is a preprint version, which is a little bit longer on the archive, and we released the code for both the entity classification and the link prediction setting.",
            "And before I go to questions session just one very big brief mention.",
            "So there is a new review article on this graph networks or graph neural network section that just came out today from Deep Mind, and if you're interested in this field and want to learn about the latest developments, I think this is a very good overview, so there's a lot of people from deep mind on there and other.",
            "Places, and I think this could also be interesting for the Knowledge Graph community in general.",
            "Alright, thank you.",
            "Questions.",
            "I so very, very interesting applications.",
            "Also very interesting theory.",
            "Thanks for the talk.",
            "One question that I have is the unsupervised setting that you have, so if you're not optimizing on some specific task.",
            "If I understand correctly, all you do is just propagate once through the network, right?",
            "And this kind of updates the random weights that you assigned in the beginning.",
            "Is this correct?",
            "Or is this?",
            "Yeah, so I wouldn't smarter way to also optimize this.",
            "Also, like in a standard backpropagation way.",
            "Yeah, some kind of loss function that come up with.",
            "That's a very good question.",
            "So in this is delicious.",
            "For example, in a sense, so here I would not recommend using this unsupervised setting for any practical purposes.",
            "So if you care about just having embeddings of your graph with this kind of method and unsupervised way, what it can do is remove edges in the input and just try to predict them back in the decoder, which is like link prediction model and then you get fairly usable general embeddings which accumulate evidence or multiple steps in the graph.",
            "So this is something we call graph out enclosures in the field.",
            "OK, Frank.",
            "So you said you suffer from 2 problems, one of which he said.",
            "Well, maybe this attention mechanisms will help.",
            "Yeah, the other one is district that you you introduce these base relations now to scale down a number of large matrices that you have to manipulate exactly.",
            "Oh, how do you choose these or how do they get chosen?",
            "This set of base relations.",
            "So we treat us a hyperparameter.",
            "So we have two hyperparameters.",
            "One is whether to choose the block decomposition or the basis decomposition.",
            "And then the number of blocks or the number of basis functions we do, we just do a hyperparameter sweep on.",
            "OK, so that's the number of base relations, the number of base relations, and then how do you choose which base relations with relations?",
            "Oh, this is optimizable via gradient descent.",
            "OK, so this directly trained.",
            "OK, more questions.",
            "Somewhere.",
            "I would have a small question.",
            "I'm not familiar with this graph convolutional neural networks, but I was wondering, did anybody tried to include a larger neighborhood instead of only the neighbor node?",
            "So let's say neighbors that are indirectly related so you step tools that we.",
            "Yeah, their number of variations, so you could just, for example, redefine a knowledge graph is having like second order neighbor relations.",
            "Then this model would give you effectively a larger receptive field in each layer.",
            "Then there was a model that explicitly take into higher order neighborhoods, not for knowledge graph, so that's so there hasn't been much work on knowledge graphs yet.",
            "But for undirected graphs or molecules, people have tried including higher order neighborhoods, and in practice it's always a tradeoff like you always want a bigger receptive field, so you have two ways of going to getting this.",
            "On the one hand, you can.",
            "Increase the depth of your model.",
            "Just do more of these layers.",
            "Each layer will add one neighborhood, or you can have more neighborhoods per layer.",
            "By defining these kinds of 2nd order neighbor relations and typically you will need to tune these hyperparameters and just find out.",
            "In practice it's useful.",
            "OK, thank you.",
            "So whole front is again one.",
            "So, so one thing that was very noticeable in the keynote this morning was that this work really took into account the schema types, right?",
            "So they were treated differently and they have a particular meaning and ethical influence on the graph and on the conclusions you could draw from the graph, and algorithms were aware of the of that schema.",
            "In your approach, you don't do this.",
            "You just treat all neighbors equally whether they are schema links or not exactly, and then your attention mechanism, which kind of a fix for some of the problems that causes, is still agnostic towards the schema.",
            "So what would you think about actually admitting that you are manipulating knowledge graphs and actually take take the schema of treat ischemic links differently?",
            "I think that's a very good idea, so we tried initially for this work also to.",
            "Include tree type nodes not as neighbors, but treat them as features of a node and that gave us comperable results, but it led to some issues that you sometimes have a hierarchy of types and you can.",
            "It wasn't really clear to us how to how to feature eyes these in this case, so essentially it was a bit messy that model so we didn't put it in the paper, but you get similar results in this case, and because this graph is in smaller, it actually is faster optimized.",
            "So I think it's a good idea.",
            "OK, so let's thank the speaker again.",
            "So we have to."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, I'm a PhD student at University of Amsterdam.",
                    "label": 0
                },
                {
                    "sent": "In this work was together with a number of collaborators both at the VU Amsterdam, which is the other big University in Amsterdam, and my Co supervisor Ivanti Tofus Naughty University of Edinburgh and my supervisor Max Spelling.",
                    "label": 0
                },
                {
                    "sent": "So the idea behind this work is to all the motivation behind this work is.",
                    "label": 0
                },
                {
                    "sent": "So how can we use like the progress and has recently been seen in neural network based learning?",
                    "label": 0
                },
                {
                    "sent": "How can we use these kinds of methods on relational data or specifically on knowledge graphs and knowledge bases?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me start with some quick introduction, motivation behind what I mean by deep learning and classical sense of the last few years we've seen maybe deep learning on grid structure data.",
                    "label": 0
                },
                {
                    "sent": "So mostly there have been big successes in 2012, for example with the image net competition, where deep neural networks have excelled at classifying image data.",
                    "label": 0
                },
                {
                    "sent": "And around the same time similar successes have been observed in field service, speech processing, or natural language processing, where the input you can think of it also is like kind of a grid structure data.",
                    "label": 1
                },
                {
                    "sent": "Just you have a very if a sequence.",
                    "label": 0
                },
                {
                    "sent": "And you have very regular kind of data structure, so you don't have any kind of graph structure behind it.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Few days later they were big successes in terms of modeling games like the game of goal or using games like Atari Games simulators.",
                    "label": 0
                },
                {
                    "sent": "We can also think of them.",
                    "label": 0
                },
                {
                    "sent": "You apply these deep neural networks that are very well optimized for this grid structure data to extract information from them.",
                    "label": 0
                },
                {
                    "sent": "And these models mostly rely on 2 main principles that we think of, so they use some kind of.",
                    "label": 0
                },
                {
                    "sent": "Weight sharing that basically means if you slide a filter over the data, you will share a lot of information in your loan parameters and if either very effective to learn.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And at the same time you learn a hierarchical hierarchy of features, like if the input image has edges and later you learn compositions of edges like parts of faces in the later learn faces in an image.",
                    "label": 0
                },
                {
                    "sent": "And what I mean by deep learning or end to end learning versus traditional learning kind of approach.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is most machine learning approaches in the passer.",
                    "label": 0
                },
                {
                    "sent": "Even today, they rely on a certain pipeline of 1st.",
                    "label": 0
                },
                {
                    "sent": "You try to extract some kinds of features from your data, like you build an embedding model.",
                    "label": 0
                },
                {
                    "sent": "For example, you try to embed your data in some kind of space.",
                    "label": 0
                },
                {
                    "sent": "Many freezes, embeddings and then later on top of that you learn a classifier which is separate model.",
                    "label": 0
                },
                {
                    "sent": "And so the promise of sort of deep learning or end to end learning is you merge these two into one single step, so you would directly operate on the raw data and you have one model that he directly trained to optimize specific goal, like image classification for this case.",
                    "label": 0
                },
                {
                    "sent": "But as we know like not all data comes in the form of grids.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have a lot of real world examples of data that comes in the form of some kind of graph like you have social networks, citation networks, communication networks, or even like something like multi agent systems.",
                    "label": 0
                },
                {
                    "sent": "When you want to build self driving cars.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You have molecular data.",
                    "label": 0
                },
                {
                    "sent": "Which you can represent.",
                    "label": 0
                },
                {
                    "sent": "This graphs have protein interaction networks which are very important in biology.",
                    "label": 0
                },
                {
                    "sent": "And my work is mostly focusing on here that I'm presenting a sore, but I'll work mostly focused on this knowledge graphs.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Last years of something like road map.",
                    "label": 0
                },
                {
                    "sent": "So many, many examples that you can bring up.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the standard deep learning approach is they don't really work for this kind of data like convolutional neural networks for example.",
                    "label": 1
                },
                {
                    "sent": "And let me focus on this part here now.",
                    "label": 0
                },
                {
                    "sent": "So the method that we use now.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work is.",
                    "label": 0
                },
                {
                    "sent": "Based on graph neural networks or graph convolutional networks, these two terms basically mean the same.",
                    "label": 0
                },
                {
                    "sent": "If you heard either of them and the bigger picture for these types of models is your past messages.",
                    "label": 0
                },
                {
                    "sent": "So you have a multi layer model and at every layer in the model you pass messages between neighboring nodes and these messages there in the form of some nonlinear function so.",
                    "label": 0
                },
                {
                    "sent": "Some neural network kind of function that you then learn in the end, and then you pass this message is in your grammar at their results.",
                    "label": 0
                },
                {
                    "sent": "I'm going to get into more detail later, so the notation that I'm going to use here is you have a graph, a set of vertices and edges, and equivalently you can also represent this in this case as an adjacency matrix and a feature matrix if you have any kinds of features on your data points.",
                    "label": 0
                },
                {
                    "sent": "So let's say for every node you could have an F dimensional feature vector, which would then summarizing this feature matrix.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let's quickly have a brief look on how we would usually do like convolutional neural network on grid structured data.",
                    "label": 1
                },
                {
                    "sent": "So there in a classical picture you would slide a filter over the image and this filter multiplies parameter values with the input of the image.",
                    "label": 0
                },
                {
                    "sent": "But it can equivalently represent this as sort of a graph transformation or message passing operation, where, let's say, if you want to update the center pixel.",
                    "label": 0
                },
                {
                    "sent": "Here the pixel in the middle order node in the middle you pass messages from the neighboring nodes.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Now, these messages here are these.",
                    "label": 0
                },
                {
                    "sent": "Vectors, age, they are hidden representation.",
                    "label": 1
                },
                {
                    "sent": "In this neural networks or activations in this model, and if you want to perform a single update of the pixel in the middle, you transform each of these messages.",
                    "label": 0
                },
                {
                    "sent": "These vectors without learned parameter matrix W and then you Add all the results up.",
                    "label": 0
                },
                {
                    "sent": "So in the end you have an update like this, so you.",
                    "label": 0
                },
                {
                    "sent": "Transform each of the neighbors individually by some certain transformation rule depending on what their relation is.",
                    "label": 0
                },
                {
                    "sent": "If they're like an upper left neighbor and upper right neighbor of this pixel, and then you get the final.",
                    "label": 0
                },
                {
                    "sent": "Linear map of these neighboring pixels and you passing through some nonlinear function, here denoted by a sigmoid.",
                    "label": 0
                },
                {
                    "sent": "And this could be any kind of non linearity that is differentiable.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, how do we generalize this to graphs?",
                    "label": 0
                },
                {
                    "sent": "So here consider this undirected graph, and we want to update the node in the middle in red.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Now since if if you just have an undirected graph, we don't have any attributes, it goes as follows.",
                    "label": 0
                },
                {
                    "sent": "So we don't have any information about an upper left neighbor and upright neighbor, so we treat all of them the same.",
                    "label": 0
                },
                {
                    "sent": "And then we transform each of these neighbor values by some linear transform.",
                    "label": 0
                },
                {
                    "sent": "You could also extend this to nonlinear transform, but most of the time we find linear is good enough and gives you some yeast for implementing it.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You normalize these messages now by some normalization factor, because if you would leave this normalization constant out then you could imagine that you have, like one.",
                    "label": 0
                },
                {
                    "sent": "Note suddenly can have 1000 neighbors or million neighbors, and you need to make sure that all these activations are mounted so you can.",
                    "label": 0
                },
                {
                    "sent": "Have can you straighten optimization without any gradients suddenly blowing up in your model?",
                    "label": 0
                },
                {
                    "sent": "Now 1 issue still here is that if you want to scale this to really big graphs, you certainly would have to sum over incoming messages from all of your neighbors if we want to just know what the next update is for this one node so we can fix this issue.",
                    "label": 0
                },
                {
                    "sent": "Addressed this issue by Subsampling Neighbors, which is something we haven't done in this work, but there has been related work that showed this for undirected graphs.",
                    "label": 0
                },
                {
                    "sent": "Really been traded for Knowledge Graph, so this would be something interesting to try.",
                    "label": 0
                },
                {
                    "sent": "Now just a list of properties from this model, so we have on the one hand now weight sharing like in convolutional neural networks for images, which basically means we use the same parameter matrix W 1.",
                    "label": 0
                },
                {
                    "sent": "Here irrespective of the location where we are in the graph.",
                    "label": 0
                },
                {
                    "sent": "And it's also invariant to permutations, so if you just change the order in which you present the neighbors, you get the same output.",
                    "label": 0
                },
                {
                    "sent": "And you have linear complexity in terms of the number of the edges, so only to touch each edge once for every layer.",
                    "label": 0
                },
                {
                    "sent": "And if you want to use this for learning on knowledge graphs, for example, you can apply this both in a setting where you know the full graph in advance and try to make predictions on that, or even you can train on a part of the graph and then make predictions on a different part of it 'cause the model is, it transfers its inductive and distance.",
                    "label": 0
                },
                {
                    "sent": "The problem is that.",
                    "label": 1
                },
                {
                    "sent": "This model right here would not have support for Edge features, so it doesn't know about any kind of relational features between neighbors.",
                    "label": 0
                },
                {
                    "sent": "And if you want to build a deep model where you want to agglomerate information from multiple hops in your neighborhood, you would also need some kind of mechanism to make sure the gradients back propagate.",
                    "label": 0
                },
                {
                    "sent": "If you want to train this model, they don't go to zero.",
                    "label": 0
                },
                {
                    "sent": "If you make very deep models.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm coming to all work here.",
                    "label": 0
                },
                {
                    "sent": "We essentially presented a relational extension of this model so we can run this on knowledge graphs and now let me.",
                    "label": 1
                },
                {
                    "sent": "Just introduce quickly what I mean by a knowledge base here.",
                    "label": 0
                },
                {
                    "sent": "So we represented as a directed labeled graph.",
                    "label": 1
                },
                {
                    "sent": "In this case, where we have a set of vertices, edges, and a set of relations.",
                    "label": 0
                },
                {
                    "sent": "And each edge we represent as is triple where we have like a subject and object.",
                    "label": 1
                },
                {
                    "sent": "VI VJ and a relation.",
                    "label": 0
                },
                {
                    "sent": "And we have a number of different relation types.",
                    "label": 0
                },
                {
                    "sent": "So for example, you have this example down here.",
                    "label": 0
                },
                {
                    "sent": "So something like this could look like relation in this case in this representation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The extension of the model that we use in this case is very simple, so it's like going back to how we did it on images.",
                    "label": 0
                },
                {
                    "sent": "So there we have if you interpreted set of having a upper left neighbor in upper right neighbor, then.",
                    "label": 0
                },
                {
                    "sent": "Already you have different relations and transform them differently, and now we just translate this to what it means on a general graph where you have different relations type.",
                    "label": 0
                },
                {
                    "sent": "So essentially what the formula here means is we assign a separate transformation for each edge type or for each relation type.",
                    "label": 0
                },
                {
                    "sent": "So each relation gets transformed with a separate parameter matrix and the normalization constant.",
                    "label": 1
                },
                {
                    "sent": "We also adapt for the separate parameter matrix and we only sum over.",
                    "label": 0
                },
                {
                    "sent": "So each summation here only goes over the neighbors of a particular.",
                    "label": 0
                },
                {
                    "sent": "Message type or a particular edge type.",
                    "label": 0
                },
                {
                    "sent": "So we call this model to relational GCN in this case.",
                    "label": 0
                },
                {
                    "sent": "Or are GCN.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "One issue that we tried to address and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work also apart from introducing the simple extension is how to.",
                    "label": 0
                },
                {
                    "sent": "Make this scale to bigger graph, so without subsampling, because subsampling can introduce some unwanted variant, so we wanted to see how big can we scale this without even going into the subsampling regime.",
                    "label": 0
                },
                {
                    "sent": "And here now we represent a knowledge base as.",
                    "label": 0
                },
                {
                    "sent": "Because we need some vectorized representation of that we represented as a set of adjacency matrices, and we have one adjacency matrix for each edge type.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Still be half if you want to be half a feature matrix that summarizes initial entity features, for example.",
                    "label": 0
                },
                {
                    "sent": "And we can elegantly summarize this formula that we had in the last slide by just using matrix multiplications of the adjacency matrices were not introduced.",
                    "label": 0
                },
                {
                    "sent": "And the good part here is that so it still has linear complexity if we use a sparse representation of the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "And since we're concerned of running this at scale and on big data, and we have at the same time we need to calculate these expensive dense matrix multiplication, Ch times W and these.",
                    "label": 0
                },
                {
                    "sent": "Need to be efficiently run on GPU typically so we can using some GPU libraries for like some deep learning libraries that now support also sparse dense matrix multiplication.",
                    "label": 0
                },
                {
                    "sent": "We can efficiently implement this on GPU and let it run on larger datasets.",
                    "label": 0
                },
                {
                    "sent": "So one last problem that occurs with this type of representation is that we need one large weight matrix for each relation type, and if you have like 1000 relations then suddenly have a lot of parameters.",
                    "label": 0
                },
                {
                    "sent": "So we introduced two types of addressing this one, two solutions for addressing this.",
                    "label": 0
                },
                {
                    "sent": "One is we use some form of weight sharing, so we share weights between relations.",
                    "label": 0
                },
                {
                    "sent": "So we learn a set of basis relations an we just learn how they are combined in.",
                    "label": 0
                },
                {
                    "sent": "Like every every relation is a combination.",
                    "label": 0
                },
                {
                    "sent": "Now a linear combination of a set of basis relations and each of these spaces relations they have one parameter matrix, so this reduces.",
                    "label": 0
                },
                {
                    "sent": "This largely reduces the number of parameters below the other one is they experimented with the sparsity constraints, so we just say the relation matrices they need to be blocked diagonal matrices, so they're in the end very sparse, and this the intuition behind this is that this block diagonal matrices to operate now.",
                    "label": 0
                },
                {
                    "sent": "Only on certain dimensions of these hidden feature vectors, which also for image task has been found to work reasonably well.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Now the idea of how to do entity classification or link prediction to typical tasks that you can think of in knowledge base is how we apply.",
                    "label": 1
                },
                {
                    "sent": "This model is in a classical pipeline you would first extract embeddings, so we first run some kind of graph kernel or some kind of hand design feature descriptor or sum, like vert avec or skip gram based model that extracts features, embeddings and any trainer classifier on top of these frozen embeddings.",
                    "label": 0
                },
                {
                    "sent": "So instead now with this end to end approach we.",
                    "label": 0
                },
                {
                    "sent": "1st So we run this model, get an output at the end so we have multiple layers of these.",
                    "label": 0
                },
                {
                    "sent": "Graph convolutions we call them, and at the end you have for each entity in the graph you have 1 hidden representation, which is essentially just a feature vector embedding.",
                    "label": 0
                },
                {
                    "sent": "But now this embedding can be directly fed into the classification loss.",
                    "label": 0
                },
                {
                    "sent": "For example of entity classification loss or a link prediction loss.",
                    "label": 1
                },
                {
                    "sent": "And you can optimize them directly.",
                    "label": 0
                },
                {
                    "sent": "Like now if you because everything is differentiable in this model, you can simply run gradient descent on all the parameters of this model.",
                    "label": 0
                },
                {
                    "sent": "So in the end the model will figure out how to use.",
                    "label": 0
                },
                {
                    "sent": "The graph structure and the features to optimize this specific loss directly without going through two step pipeline.",
                    "label": 0
                },
                {
                    "sent": "And you can then, essentially, if you peek into the embedding said you get these hidden layers.",
                    "label": 0
                },
                {
                    "sent": "You see that these embeddings are optimized for specific tasks like embeddings.",
                    "label": 0
                },
                {
                    "sent": "For classification we have embeddings for link prediction.",
                    "label": 0
                },
                {
                    "sent": "And just to visualize this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We had a toy experiment from some earlier work that I want to show you here that if you take an untrained model with just random parameters before you train it and feed in some graph like he's just a toy graph where some communities are marked with colors and if you just feed in this type of graph in this model and you inspect the output features you get, then even though the model is just randomly initialized and not trained yet, you get some some type of separation in this output embedding space.",
                    "label": 0
                },
                {
                    "sent": "But this is without training yet, and this is of course depending on the initialization of many factors.",
                    "label": 0
                },
                {
                    "sent": "But now if you want to optimize this to specif.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We go like we want to classify certain nodes in this graph or entities.",
                    "label": 0
                },
                {
                    "sent": "Then let's say you give it for each of these four different communities.",
                    "label": 0
                },
                {
                    "sent": "Here you give it one label, one labeled node, and then you ask it to optimize embedding so that it can classify the unlabeled nodes.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when we look at what's happening now, we zoomed out of this initial embedding space, because these are very small randomized values and we start training with gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And we optimize for these embeddings to be good for a classification task, and now for each color is only one labeled.",
                    "label": 0
                },
                {
                    "sent": "Note that it gets like gradient information from and the embeddings are now pulled towards the edges, so embeddings are.",
                    "label": 0
                },
                {
                    "sent": "So the embedding so very easy to use for the classifier.",
                    "label": 0
                },
                {
                    "sent": "Another classifier can essentially just draw 2 lines in that as a good classification in this case.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the case for Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do entity classification, we have the input graph.",
                    "label": 1
                },
                {
                    "sent": "We put our encoder on top and at the end we have a node specific loss.",
                    "label": 0
                },
                {
                    "sent": "And we evaluate this loss on labeled notes only and the loss here is a cross entropy error.",
                    "label": 1
                },
                {
                    "sent": "You could also use a mean squared error whatever you like in this case.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Similarly, you can build a model for link prediction, which we did in this paper where you take the input.",
                    "label": 0
                },
                {
                    "sent": "You put this encoder model on top, so this is only to be differentiable models and then you can have a decoder model.",
                    "label": 0
                },
                {
                    "sent": "So very simple decoder in this case that predicts, so the decoder's job is to take two node embeddings and compare them in such a way that it can give you a score.",
                    "label": 0
                },
                {
                    "sent": "As of how they are related.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This mode is in a knowledge completion and knowledge graph completion literature often use model for this case and here.",
                    "label": 0
                },
                {
                    "sent": "Essentially just take a bilinear product of two embeddings and transformative.",
                    "label": 0
                },
                {
                    "sent": "Learned weight matrix in between and in this small dissuade metrics to be taken diagonal.",
                    "label": 0
                },
                {
                    "sent": "But you can also think of taking a non diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "And this can be trained by again a cross entropy error.",
                    "label": 0
                },
                {
                    "sent": "Now this is binary.",
                    "label": 0
                },
                {
                    "sent": "Therefore it looks a bit more complicated and a normalization now over all the edges.",
                    "label": 0
                },
                {
                    "sent": "And to train this you also need to provide negative examples.",
                    "label": 0
                },
                {
                    "sent": "So what we do in this case is we corrupt either an object or a subject in the relation just randomly corrupted and this entry is a negative example.",
                    "label": 0
                },
                {
                    "sent": "So for each positive example we construct a certain number of negative examples and then we can train this.",
                    "label": 0
                },
                {
                    "sent": "So in experiments we see.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For entity classification, we tested this on a number of benchmark datasets that go from very small graphs with like 30,000 edges to big graphs with up to six 6 million edges.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In this case, up to 133 relation types.",
                    "label": 0
                },
                {
                    "sent": "And we compare this against some classical baselines like.",
                    "label": 0
                },
                {
                    "sent": "First of all, Henderson feature, baseline, kernel based baseline, and a embedding based baseline.",
                    "label": 0
                },
                {
                    "sent": "And what we see in the end is that so the results here.",
                    "label": 0
                },
                {
                    "sent": "The results here a little bit mixed.",
                    "label": 0
                },
                {
                    "sent": "So we see that on some of the datasets we get like, especially on FB we get a very big improvement.",
                    "label": 0
                },
                {
                    "sent": "Where is another datasets.",
                    "label": 0
                },
                {
                    "sent": "The improvements at the small or even.",
                    "label": 0
                },
                {
                    "sent": "Losing compared to the feature based baseline on mutek.",
                    "label": 0
                },
                {
                    "sent": "So we try to understand what's going wrong here and how it can be improved in future work.",
                    "label": 0
                },
                {
                    "sent": "And the issue with the model we as we formulated right now is that it doesn't really understand the difference between a regular node in a graph or end like an entity and a type node, and so the type nodes live.",
                    "label": 0
                },
                {
                    "sent": "So mu tect BGS is to datasets have very many type notes, and these have a very high node degree.",
                    "label": 1
                },
                {
                    "sent": "And since we average all these messages, there's a lot of.",
                    "label": 0
                },
                {
                    "sent": "Information being lost in this process, and so we think that.",
                    "label": 0
                },
                {
                    "sent": "Recently there has been some.",
                    "label": 0
                },
                {
                    "sent": "Work on attention mechanisms in these types of models, and we think that this can to some degree alleviate this issue.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now for link prediction experiments, the task is now to recover missing relations in a graph, so we assume that our knowledge graph is incomplete and we want to predict missing relations in this graph.",
                    "label": 1
                },
                {
                    "sent": "And we optimize this encoder decoder architecture with a dismal decoder.",
                    "label": 0
                },
                {
                    "sent": "And in this mode you would directly optimize these embeddings ES and go here and in our model we provide these with encoder model.",
                    "label": 0
                },
                {
                    "sent": "So the encoder in this case propagates measure across the graph gets the embedding and then runs the decoder which allows it to accumulate evidence from multiple steps of reasoning.",
                    "label": 0
                },
                {
                    "sent": "For this graph you could think of it in a way.",
                    "label": 0
                },
                {
                    "sent": "And we also run on Sambol method which is not showing this slide but in later slide and what we see here is on.",
                    "label": 0
                },
                {
                    "sent": "Freebase data said where inverse relations have been removed from the test set.",
                    "label": 0
                },
                {
                    "sent": "So there's an issue with the benchmark datasets that has recently been discovered in link prediction at like a large fraction of the test set data is actually in the training set.",
                    "label": 0
                },
                {
                    "sent": "If you just reverse the link so these are removed here in this data set and then we see that the model gets quite a substantial improvement over all this decoder baselines that only use single step.",
                    "label": 0
                },
                {
                    "sent": "So they basically only directly optimize these embeddings and run a single.",
                    "label": 0
                },
                {
                    "sent": "Step 4 in prediction versus we accumulate evidence of a multiple steps of propagation in this graph.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to only very briefly touch.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Upon details like we train this model also with Edge drop out so to regularize it, we randomly drop edges in the encoder so it's similar to a denoising autoencoder way.",
                    "label": 0
                },
                {
                    "sent": "We trained it in the end.",
                    "label": 0
                },
                {
                    "sent": "And we also ran this model on the original data set where these inverse edges have not been removed and there our original model.",
                    "label": 0
                },
                {
                    "sent": "What is comperable to this modern?",
                    "label": 0
                },
                {
                    "sent": "The performance, but we found that if you combine the two in sort of an assemble model, we get.",
                    "label": 0
                },
                {
                    "sent": "Improved results but.",
                    "label": 0
                },
                {
                    "sent": "This mode has a very does a very good job at inferring these inverse links here in this case, but as you see, even the link field is a baseline here that just takes features of a relation and does a very good job at inferring these inverse relations.",
                    "label": 0
                },
                {
                    "sent": "It's a hand assigned feature baseline, and it does.",
                    "label": 0
                },
                {
                    "sent": "It gives you the best performance here across all these.",
                    "label": 0
                },
                {
                    "sent": "Obviously learning based methods.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'm going to conclude here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I can summarize this that end to end learning or deep learning on this type of data on relational data is a competitive approach compared to what we call classical approaches where you have a multi step pipeline and for link prediction we see that.",
                    "label": 1
                },
                {
                    "sent": "Enriching these triplet scoring methods that are sort of the state of the art in the field.",
                    "label": 0
                },
                {
                    "sent": "If an encoder can even improve their performance, and it also enables to.",
                    "label": 1
                },
                {
                    "sent": "Aggregate evidence over multiple steps of information propagation graph.",
                    "label": 0
                },
                {
                    "sent": "So for future work, it would be interesting to look into attention mechanisms to get rid of these issues of high degree nodes that just average all kinds of information together and then you lose this information effectively.",
                    "label": 0
                },
                {
                    "sent": "And at the same time, scalability something we have only managed to scale this to something like 3 million 6 million edges.",
                    "label": 0
                },
                {
                    "sent": "I think in this model.",
                    "label": 0
                },
                {
                    "sent": "But if you want to go to really big databases then or near big knowledge basis then you would have to use some kind of sub sampling techniques and there have been a number of papers this year like Fast GC and in this case it does some type of important sampling to reduce the variance.",
                    "label": 0
                },
                {
                    "sent": "In September 17 notes.",
                    "label": 0
                },
                {
                    "sent": "And if you want to learn.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or about this work, then have a look at our paper.",
                    "label": 0
                },
                {
                    "sent": "This is a preprint version, which is a little bit longer on the archive, and we released the code for both the entity classification and the link prediction setting.",
                    "label": 1
                },
                {
                    "sent": "And before I go to questions session just one very big brief mention.",
                    "label": 0
                },
                {
                    "sent": "So there is a new review article on this graph networks or graph neural network section that just came out today from Deep Mind, and if you're interested in this field and want to learn about the latest developments, I think this is a very good overview, so there's a lot of people from deep mind on there and other.",
                    "label": 0
                },
                {
                    "sent": "Places, and I think this could also be interesting for the Knowledge Graph community in general.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "I so very, very interesting applications.",
                    "label": 0
                },
                {
                    "sent": "Also very interesting theory.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "One question that I have is the unsupervised setting that you have, so if you're not optimizing on some specific task.",
                    "label": 0
                },
                {
                    "sent": "If I understand correctly, all you do is just propagate once through the network, right?",
                    "label": 0
                },
                {
                    "sent": "And this kind of updates the random weights that you assigned in the beginning.",
                    "label": 0
                },
                {
                    "sent": "Is this correct?",
                    "label": 0
                },
                {
                    "sent": "Or is this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I wouldn't smarter way to also optimize this.",
                    "label": 0
                },
                {
                    "sent": "Also, like in a standard backpropagation way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, some kind of loss function that come up with.",
                    "label": 0
                },
                {
                    "sent": "That's a very good question.",
                    "label": 0
                },
                {
                    "sent": "So in this is delicious.",
                    "label": 0
                },
                {
                    "sent": "For example, in a sense, so here I would not recommend using this unsupervised setting for any practical purposes.",
                    "label": 0
                },
                {
                    "sent": "So if you care about just having embeddings of your graph with this kind of method and unsupervised way, what it can do is remove edges in the input and just try to predict them back in the decoder, which is like link prediction model and then you get fairly usable general embeddings which accumulate evidence or multiple steps in the graph.",
                    "label": 0
                },
                {
                    "sent": "So this is something we call graph out enclosures in the field.",
                    "label": 0
                },
                {
                    "sent": "OK, Frank.",
                    "label": 0
                },
                {
                    "sent": "So you said you suffer from 2 problems, one of which he said.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe this attention mechanisms will help.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the other one is district that you you introduce these base relations now to scale down a number of large matrices that you have to manipulate exactly.",
                    "label": 0
                },
                {
                    "sent": "Oh, how do you choose these or how do they get chosen?",
                    "label": 0
                },
                {
                    "sent": "This set of base relations.",
                    "label": 0
                },
                {
                    "sent": "So we treat us a hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "So we have two hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "One is whether to choose the block decomposition or the basis decomposition.",
                    "label": 0
                },
                {
                    "sent": "And then the number of blocks or the number of basis functions we do, we just do a hyperparameter sweep on.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the number of base relations, the number of base relations, and then how do you choose which base relations with relations?",
                    "label": 0
                },
                {
                    "sent": "Oh, this is optimizable via gradient descent.",
                    "label": 0
                },
                {
                    "sent": "OK, so this directly trained.",
                    "label": 0
                },
                {
                    "sent": "OK, more questions.",
                    "label": 0
                },
                {
                    "sent": "Somewhere.",
                    "label": 0
                },
                {
                    "sent": "I would have a small question.",
                    "label": 0
                },
                {
                    "sent": "I'm not familiar with this graph convolutional neural networks, but I was wondering, did anybody tried to include a larger neighborhood instead of only the neighbor node?",
                    "label": 0
                },
                {
                    "sent": "So let's say neighbors that are indirectly related so you step tools that we.",
                    "label": 0
                },
                {
                    "sent": "Yeah, their number of variations, so you could just, for example, redefine a knowledge graph is having like second order neighbor relations.",
                    "label": 0
                },
                {
                    "sent": "Then this model would give you effectively a larger receptive field in each layer.",
                    "label": 0
                },
                {
                    "sent": "Then there was a model that explicitly take into higher order neighborhoods, not for knowledge graph, so that's so there hasn't been much work on knowledge graphs yet.",
                    "label": 0
                },
                {
                    "sent": "But for undirected graphs or molecules, people have tried including higher order neighborhoods, and in practice it's always a tradeoff like you always want a bigger receptive field, so you have two ways of going to getting this.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, you can.",
                    "label": 0
                },
                {
                    "sent": "Increase the depth of your model.",
                    "label": 0
                },
                {
                    "sent": "Just do more of these layers.",
                    "label": 0
                },
                {
                    "sent": "Each layer will add one neighborhood, or you can have more neighborhoods per layer.",
                    "label": 0
                },
                {
                    "sent": "By defining these kinds of 2nd order neighbor relations and typically you will need to tune these hyperparameters and just find out.",
                    "label": 0
                },
                {
                    "sent": "In practice it's useful.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So whole front is again one.",
                    "label": 0
                },
                {
                    "sent": "So, so one thing that was very noticeable in the keynote this morning was that this work really took into account the schema types, right?",
                    "label": 0
                },
                {
                    "sent": "So they were treated differently and they have a particular meaning and ethical influence on the graph and on the conclusions you could draw from the graph, and algorithms were aware of the of that schema.",
                    "label": 0
                },
                {
                    "sent": "In your approach, you don't do this.",
                    "label": 0
                },
                {
                    "sent": "You just treat all neighbors equally whether they are schema links or not exactly, and then your attention mechanism, which kind of a fix for some of the problems that causes, is still agnostic towards the schema.",
                    "label": 0
                },
                {
                    "sent": "So what would you think about actually admitting that you are manipulating knowledge graphs and actually take take the schema of treat ischemic links differently?",
                    "label": 0
                },
                {
                    "sent": "I think that's a very good idea, so we tried initially for this work also to.",
                    "label": 0
                },
                {
                    "sent": "Include tree type nodes not as neighbors, but treat them as features of a node and that gave us comperable results, but it led to some issues that you sometimes have a hierarchy of types and you can.",
                    "label": 0
                },
                {
                    "sent": "It wasn't really clear to us how to how to feature eyes these in this case, so essentially it was a bit messy that model so we didn't put it in the paper, but you get similar results in this case, and because this graph is in smaller, it actually is faster optimized.",
                    "label": 0
                },
                {
                    "sent": "So I think it's a good idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's thank the speaker again.",
                    "label": 0
                },
                {
                    "sent": "So we have to.",
                    "label": 0
                }
            ]
        }
    }
}