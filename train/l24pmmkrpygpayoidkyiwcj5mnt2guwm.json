{
    "id": "l24pmmkrpygpayoidkyiwcj5mnt2guwm",
    "title": "Policy Search for RL",
    "info": {
        "author": [
            "Pieter Abbeel, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_abbeel_policy_search/",
    "segmentation": [
        [
            "So I gotta couple affiliations.",
            "It's just three research scientists at open AI.",
            "Where do AI research professor at Berkeley?",
            "Where do AI research and teach AI and do some administration, but not while I'm on leave right now and then.",
            "I'm also one of the Co founders of Great Scope.",
            "If you ever need to do any grading, this can save you a lot of time.",
            "We got tired of spending a lot of time on grading, homework and exams and build a tool for it.",
            "Now we've got a lot of data of people having graded.",
            "Now we build AI to learn from that data to great automatically, and you can get close to zero time spent on grading in the near future.",
            "OK, reinforcement learning, let's see."
        ],
        [
            "Alright, so Joe alright already introduced this formalism earlier today in reinforcement learning.",
            "We have an agent and we want somehow to discover the right software for this agent, and this software will take actions as a consequence of the action, the environment, and maybe the robot or something else will change.",
            "State reward will be admitted that indicates how good or bad the resulting situation is, and then the agent is faced with the consequences of the action and is to act in this new situation.",
            "And this repeats.",
            "And that's fundamentally what makes it different from.",
            "Let's say supervised learning where you have a one off thing.",
            "You make a decision and then things reset and you get a new one off thing and it repeats here where you see next depends on what you did before."
        ],
        [
            "The topic within reinforcement that we're going to cover in this lecture is policy optimization impulse optimization.",
            "The goal is to directly find a policy Pi Theta that, given the current state, emits a distribution over possible actions.",
            "And then sample from that distribution, and hopefully that distribution is a good distribution that leads to good performance.",
            "Underneath in everything I'll cover here we can pretty much assume that it's going to be a deep neural net representing the policy, but in principle it could be any other function that's parameterized with some vector Theta, and as you change the entries in Theta, you'll change the policy.",
            "The process that goes from current state to action being taken."
        ],
        [
            "So, formally, what it looks like, then we're trying to solve, is it trying to solve an optimization problem where we maximize expected sum of rewards, accumulated overtime expected?",
            "Because, well, the environment might be stochastic, so it might not be deterministic.",
            "What we encounter and 2nd off and the policy will be stochastic.",
            "It is the case that the optimal policy in NDPS tends to be deterministic.",
            "But for learning it can be convenient to consider a broader class of policy that is also considered stochastic policies, because that actually tends to smooth out the optimization surface.",
            "So we'll be optimizing over stochastic policies and try to find one that performs well on expectation, even though it converges.",
            "Maybe it'll end up being deterministic.",
            "To make this a little specific for robotics, for example, the reward could be the quality of a meal that a robot prepares, so robots busy cooking in a kitchen after half an hour, robot comes out gives you a meal.",
            "There's been zero word all the way throughout.",
            "But then when the meal comes out, you eat it.",
            "You say how good it is?",
            "Maybe it read it from one to five stars and the robot will get a reward from one through 5.",
            "As you can see how this can be really hard problem because the robots busy for half an hour.",
            "And only gets one piece of feedback at the end of the whole session about how good the cooking was, and so to actually learn from that is really difficult.",
            "Might have to do this again.",
            "Maybe it gets a different rating.",
            "I can tease apart what was different between the different sessions and from that somehow understand what it should do more of what it should do less often.",
            "That's exactly what policy optimization will try to do."
        ],
        [
            "Now politicians are the only way to tackle the problem solving methods too.",
            "The other methods are important too.",
            "We're focusing policy limitation here, so let me give you a little picture of why Paula Civilization might be the method of choice in your practical situations.",
            "Often the policy is simply to represent them, the Q function or the value function.",
            "Imagine a robot wanted to grasp an object to try to find the policy we trying to find is a path for the gripper to go around the object and close.",
            "He's trying to find a value function is trying to learn to predict exactly how much expected reward you have as a function of the state that you're in, and that's a much harder problem to precisely solve, and so it might be that just learning the policy is simpler.",
            "Now learning the value function.",
            "If you learn the value function, you actually don't yet know how to take actions.",
            "Once you have a value function to then decide how to act, you need to have a dynamics model that allows you to look ahead to one step.",
            "Look ahead, say OK.",
            "If my value function for all the actions I have currently available to me, what would happen if we take that action, then what?",
            "What do I get for the transition an what's the value at the next state?",
            "And so you need to actually learn two things to be able to act.",
            "You can resolve this by learning AQ function instead.",
            "That's even more complicated.",
            "Learning value function in many ways, because now you need to learn a function that.",
            "Understands how much reward you're going to get as a function of current state in action, which can be a pretty complicated function to learn even for simple task were often the policy can be much easier to represent."
        ],
        [
            "Here are some success stories of policy optimization that might help motivate why you want to do it.",
            "The top row, our success is from a little while ago and they don't use any deep learning underneath.",
            "They are similar policies underneath, but still parameterized policies where the policy optimization reinforcement algorithm allowed to fine tune parameters in a way that humans couldn't find.",
            "Tune them by hand and as a consequence could get better performance than hand tuning of these systems.",
            "So you see, at the top is.",
            "From UT Austin, the robot dogs that play Robo Cup soccer used reinforcement learning to learn to walk or run faster than the other robots that they're playing against and hence be able to outcompete them in soccer.",
            "Then next one will see in a little more detail.",
            "Later is inverted helicopter flight by Entering's Group at Stanford.",
            "I was part of that team then next one is two legged locomotion Rust Hendricks, PhD thesis at MIT.",
            "This is a biped.",
            "Walker learn to walk from scratch.",
            "With reinforcement learning in the real world, and then the last one is one that will also see a little bit about later.",
            "It's a task where you have a Cup and a little string, and then a bowl attached to the end of the string is supposed to swing up the ball and catch it in the Cup.",
            "The bottom row is some more recent success stories where there are deep neural Nets underneath what's being learned.",
            "The policy isn't network that will map for most of them, not all of them, though from raw pixels to round motor commands on the left you see some Atari games as well as Labyrinth which 3D navigation environment that you navigate with these solutions from first person vision, then control of two dimensional robots in Majokko Control 3 dimensional robots.",
            "Image Coco control of actual robots and then Alpha Go has a few stars here because there's a lot of pieces to Alpha go and only one piece of it is policy optimization.",
            "But it is also in there as part of how you restrict the branching factor, so your policy can help you reduce the number of things to consider as you search in the game tree through plausible futures that you might encounter."
        ],
        [
            "Of the overall landscape of reinforcement learning policy optimization on the left.",
            "Here dynamic program on the right dynamic program will see a lot more of later today.",
            "Sure, Rich suddenly in the next session will cover a lot of dynamic programming.",
            "Dynamic programming relies on a self consistency equation that the value at the current time is equal to reward.",
            "You get in the first transition plus value at the next time.",
            "Policy implementation directly optimizes the objective that you care about, which is expected.",
            "Expected reward and of course then with actor critic methods you kind of get in the middle between the two and this session will cover these three.",
            "So we'll cover the remaining free optimization will cover policy gradients, and then we'll see how we can use value functions inside policy gradients.",
            "So that we get actor critic methods, which gets us pretty close to the value function methods.",
            "It's not that the other ones aren't important, it's just only so much we can cover in one lecture."
        ],
        [
            "In terms of outline for this lecture will look at.",
            "Model based methods first.",
            "It's often not what people think of when they say policy optimization.",
            "Methods, methods, and policy grade methods, but we'll see them first.",
            "An admittedly, people have had a hard time getting these to work, as well as the other ones, but there's also strong belief that model based methods in the long run could be more sample efficient an in a session like this is good to know what's out there rather than just the things that have worked the best of all things, and then we'll look at model free methods.",
            "And at the end will do something that kind of brings them both together through the stochastic computation graphs framework.",
            "OK, starting with model based where you see here is also split into three types of derivatives that will study, will study path derivatives will study derivative free methods and then will study likely ratio gradients.",
            "Three different ways of computing the derivative.",
            "If you have a derivative, you know which direction to step to improve your policy.",
            "Different sets of assumptions will start out with some pretty strong assumptions.",
            "Here will assume that F is evident Dynamics model that describes next date given current state and action.",
            "For now, we'll assume it's known.",
            "Differentiable will assume the reward function is known.",
            "Differentiable will assume that the policy is known, but that's not really an assumption 'cause we're designing the policy ourselves.",
            "We always know the policy and then also assume that it's a differentiable policy."
        ],
        [
            "OK, so let's assume we have an MVP with only two transitions that happen so very short horizon just for making it concrete and fitting it on the slide.",
            "Top row what do we have stayed at?",
            "Time Zero Dynamics model an action we take as prescribed by the policy, potentially stochastic policy, then results in next state then policy size.",
            "What to do next?",
            "State time 2.",
            "And then we might have another transition and this would continue.",
            "But we could also stop at two time steps and then we have rewards at the bottom that decide how much we got.",
            "How good this current trajectory was.",
            "But we try to optimize a sum of all rewards.",
            "The first one here doesn't matter too much, because we can't really influence it because we start in whatever state we started.",
            "But I'm just showing it for symmetry of the figure.",
            "So the notation reward function R Maps from state to reward policy Pi Theta Maps from state to action.",
            "For now let's assume deterministic and then the dynamics model Maps from state and action to the next state."
        ],
        [
            "Then what we'd like to do is optimize expected sum of rewards.",
            "So we'll call you have data utility of a current policy Pi, Theta, and so we're trying to maximize you have data.",
            "So in this case, what that means is maximizing a sum of R0R 1R2.",
            "Now we can actually compute aggrandisement along this graph.",
            "Think of this me a lot of you have seen a whole week of deep neural Nets last week.",
            "Think of the thing on the left has just a neural net.",
            "That is kind of special structure.",
            "It's a neural net with three layers with some connections within the layer.",
            "But it's very much like a neural net, and so in principle you could run backpropagation through this to find the derivative of utility, which is a sum of the bottom things, which would be some of the bottom things would be a loss function.",
            "Make it as high as possible.",
            "In this case you can just take derivatives through this, so you'd say, well, I care bout derivative of utility, respect to all the parameter parameters, Theta, which are the weights on the policy connection.",
            "Then what is this?",
            "It has derivatives of reward.",
            "Expect each of the rewards respect to state the dirt of state respect to the policy parameters.",
            "We can expand the last one.",
            "This way the state depends on the policy parameters through how you ended up in that state, which is dependent on what state you were in at the previous time, which it also depends on the policy parameters and depends on the dynamics model directly.",
            "So this should be a you hear them all directly depending on the controls you actually took and then how to control action depends on the parameter vector Theta or in this case just entry Theta I.",
            "And then.",
            "The third of the control action respect to the parameter Theta depends on the policy, but then also depends on what state you were in, and just so there is another recursion here with respect to state and the state respect to the policy parameters.",
            "That's just backpropagation spelled out in a recursive way, and so you could either do that by hand, or you could just feed this computation graph that you see at the top left feed into an automatic differentiation package where you feed in F. You feed in R, you feed in a parameterized policy Pi Theta, which is the variables that you're trying to optimize over, and then just ask it for a derivative, and you're good to go.",
            "So to get derivatives policy gradients, we just do a rollout switch is executing the policy, then do back prop which gives a gradient an.",
            "Maybe we do multiple rollouts and multiple initial states to get a lower variance in our gradient and we can do updates so that gives us the simplest version of a policy gradient algorithm for domestic dynamics domestic policy."
        ],
        [
            "Sticker step further.",
            "What if we have a stochastic dynamics model so next date is function of Kirsten Action plus some noise.",
            "Actually, you can simply consider these noise variables constant once the rollout has happened.",
            "So once the rollout has happened, you just freeze the noise and you're back to the terministic model 'cause the noise is froze is nothing stochastic about it anymore, and you can still apply back propagation just like you could do before.",
            "Of course, you gotta realize that you might have to do multiple rollouts to see multiple instantiations of these noise variables and then compute multiple gradients and average those gradients to get a lower variance estimate.",
            "But mathematically speaking, you just do one, roll out an get estimate of the gradient in exactly the same way."
        ],
        [
            "So more generally, if you had a stochastic dynamics model, next day depends to Castle and current state in action you can re parameterized this by saying it's going to be a domestic function that depends on current state and action as well as some noise variable WT.",
            "And the function encodes the stochasticity by depending on that noise variable.",
            "For example, in the case of a Gaussian distribution, maybe the next day is a Gaussian distribution as a function of G of ST&UT, you can just split it out as we did on the previous slide, but it doesn't have to be a Gaussian.",
            "Anything where you have a continuous noise variable, you can do this."
        ],
        [
            "And then what you have is a compilation graph that looks like this.",
            "You have you roll out as you rollout happens, your noise happens.",
            "You can determine what that noise is, put it in there, and then do a back propagation to the graph in exactly the same way.",
            "The same is true when you're Paul."
        ],
        [
            "She becomes stochastic.",
            "You can when you execute your policy see what the noise variables are that you sampled, insert them into the graph, freeze them for your back propagation and same for the reward function.",
            "Even though often the right functions domestic.",
            "If in some situations you have a stochastic reward function, you can do the same thing."
        ],
        [
            "So at this point, we're able to deal with stochastic reward dynamics, an policy, and just use a factorization pass to get the gradient out.",
            "Of course, it still assumes that we know the dynamics model, and we know the reward function.",
            "We just have to fill in the noise to be able to do this."
        ],
        [
            "Let's look at a full algorithm so we know how to compute gradients.",
            "Welcome.",
            "Now get is complete policy gradient algorithm.",
            "We can iterate going from 1, two over iterations.",
            "Then we do multiple rollouts.",
            "In each iteration, during the rollout, means sampling at initial state and sampling all the noise that will experience noise in the dynamics noise in the policy execution and noise in the reward function faintings deterministic.",
            "You just skip the noise sampling for that.",
            "Based on that, you can do your 4th pass.",
            "Then you can do a backward pass average older grand estimates and take a step in the direction or do something fancier.",
            "That's a higher order to optimize this.",
            "If you have a.",
            "Real-world system you might not have access to the noise yourself.",
            "The real world will sample the noise for you, at least for the dynamics and possibly for their reward function.",
            "Then you just back solve for it as you have had an experience.",
            "He said well S T + 1 should be equal to F of SD, UTI plus some noise.",
            "I've seen S T + 1 SD and UTI.",
            "I can back solve for the noise and that's just the environment provides it and you solve for it.",
            "Yes.",
            "One reason to average it is to get a lower variance estimate.",
            "An alternative would be just take one grand estimate, take a step and repeat.",
            "Either way can work.",
            "That's kind of up to you how you want to implement this.",
            "If you don't average it, you might have to take smaller steps.",
            "Might still be advantages.",
            "It could be that your average is because it's easier to simulate many things in parallel and the same rate as you could simulate one thing, and then you'd rather average it rather than doing one at a time.",
            "But both can work.",
            "Yes.",
            "Good question.",
            "So is this for continuous States and actions for the differentiability assumption to be true, we need continuous States and actions, otherwise you won't be able to differentiate through dysfunctions.",
            "Ann will later see what to do when we don't have that assumption, but for now, that's where we're assuming, correct?",
            "Yes?",
            "Better.",
            "I don't think anybody has a real answer for that yet in that.",
            "It's still a little bit of a challenge to get model based or help to work well when you.",
            "Typical so typically this coming next on the slide here.",
            "Typically you would not have a known dynamics model F and you would have to estimated from the data that you're collecting.",
            "So you have an unknown dynamics model F, which you're learning as well as learning the policy at the same time.",
            "An there's a lot being learned at the same time, and people still have a hard time getting this all to work.",
            "Reliably and when you're trying to get something to work, it's often easier to work with larger batches 'cause you know if you take an infinitely large batch, you know what the behavior is going to be, whereas you take very small batches or only one roll out.",
            "It's harder to understand the behavior in hard to have invariants that you know are guaranteed to be true, but I do think in the long run people will figure out ways that you go just one roll out at a time update and go again.",
            "In fact, I suspect people will do things, and people have tried some things like that where you actually.",
            "During the rollout you would update your Dynamics Model F as you're rolling out.",
            "Read re optimize your policy during the rollout based on the latest update of F An adjust your policy during a single rollout, even to be even more sample efficient.",
            "Other questions.",
            "So the full model based algorithm would be.",
            "You do what's shown on the slide as well as in every iteration you estimate the dynamics Model F from the data.",
            "So it's been hard to get these things to work, but."
        ],
        [
            "In a few successes, for example, the SVG algorithms from Nicholas Hayes and collaborators at DeepMind have shown quite some success with this model based oral approach.",
            "So let's expand this to a full horizon here, so the full computation graph I dropped the noise parameters.",
            "The noise in the dynamics noise in the policy noise in reward function just to keep it a little more self contained.",
            "US will play around with this quite a bit, but this is the computation graph that will be considering will and what happens is that will compute a gradient starting from every time step T, so it won't just do one gradient path starting from time 0, but we'll see from every time step.",
            "What is the gradient that we can compute relative to the policy parameters at that time based on the entire future?",
            "OK, so we'll have many copies of this one for each time step T. SVG Infinity does what I just described.",
            "You layout the accommodation graph from each time step and then do the back prop.",
            "Based on what I just said back solving for the noise backdrop through it and get a gradient.",
            "They do it for each time then add it together from all times and that's your great investment."
        ],
        [
            "Now you can have other variants of this issue with this SVG Infinity is that it can be high variance if the horizon is long.",
            "There can be a lot of stochasticity in the process, and the estimate that you get could be high variance estimate and there are ways to reduce the variance.",
            "One way to do this is introducing discounting, so you could have discounted rewards for the future rather than actual rewards.",
            "That's a pretty natural thing to do.",
            "You can do more, you can just shorten the horizon.",
            "You can say I'm only going to look case steps ahead.",
            "And then to account for what happens after K steps ahead, you introduce a value function.",
            "We haven't covered in this lecture how we're going to estimate value function, But let's assume you can estimate the value function.",
            "You can have a value function that is at the very last node, so over here instead of R we have V V-55.",
            "Because we're parameterising the value function and we're going to have to estimate these parameters 5 and then once you put your value function here, you can do the exact same thing as if it was a reward.",
            "Still optimizing some of all of these.",
            "Maybe this cannot sum of all of these.",
            "That's our computation graph.",
            "Backpropagation and we get it.",
            "Policy gradient out.",
            "And we do it again for all time steps.",
            "T and more extreme scenario.",
            "SG-1.",
            "We stop after one time step going to look one time step ahead, cap it off with the value function there backward through this small combination graph but do it for all time slices.",
            "T added altogether.",
            "Get your policy gradient.",
            "Even more extreme.",
            "You could not even have the dynamics modeling this anymore.",
            "You could just say I have a current state current action results in a Q value.",
            "That's your computation graph back propagate through that which is very short back propagation and you get a gradient of your policy this way.",
            "Now you estimated Q function Q5 that's FG0 or DG depending on the exact details of how you collect data and represents some things.",
            "And the last one you actually don't need to learn a dynamics model anymore, so some advantage there that you don't need to.",
            "Learning dynamics model.",
            "Of course, by learning it, maybe you internalize something about the environment that's useful and you can learn more efficiently.",
            "But if you have trouble learning at dynamic, small, don't want to do it, SVG zero avoids you having to learn a dynamics model.",
            "Let's"
        ],
        [
            "Make this concrete so you can do this for all the versions that I showed on the previous slide symbol.",
            "OK, good question.",
            "What does it stand for?",
            "DPG stands for deep deterministic policy gradient.",
            "And SVG I should spacing out on what it stands for.",
            "Stochastic value gradient.",
            "OK, thank you, so SVG is stochastic value gradient.",
            "So let's make this into a full algorithm.",
            "I picked SG-1 as a SVG 0 as a SVG 0.",
            "Yeah, I think this is easier.",
            "As a concrete example, but you can do the same thing for the other ones.",
            "Actually I skipped one, then let's go back to.",
            "SG-1.",
            "I picked this one as the first concrete example.",
            "SG-1 this is the computation graph.",
            "This is what's associated with that, and then on the right we see the full algorithm.",
            "We do rollouts.",
            "For each roll out, what happens is a forward pass.",
            "We collect the data from that forward pass.",
            "We store any noise encountered during the 4th pass, either buyback solving if it's in the real world for what the noise was, or by just knowing what the noises were simulating, and certainly the noise ourselves.",
            "Then we have three things to do an you can choose when you do each one of them.",
            "There's a bit of choice there, but you have to learn three things.",
            "You have to learn a policy, a value function, a dynamics model.",
            "And so maybe you first policy update.",
            "We don't have to a natural thing could be to 1st Dynamics model update on it, then solve for the noise, then do a policy update and do a value update.",
            "The policy update is requires the noise in the dynamics and then the back propagation pass, which in this case is equation shown over here.",
            "That gives you a great investment for the policy.",
            "Your value function estimate could be something as simple as you have a premises value function which would least queries have fit to a estimated value which is estimated based on one step.",
            "Look ahead, reward your experience plus gamma times value at the next state based on the value function you currently have typically.",
            "This is considered a constant once you get it out of there, and only this Phi is differentiated through when taking the gradient here, and this is considered constant computed based on what is the previous iterations Phi.",
            "And then we need a gradient for the dynamics model.",
            "Incoming update D dynamics model.",
            "So that's a full argument.",
            "You can implement an run POS agreements with."
        ],
        [
            "Yes, can you go back to yeah, is it actually necessary to solve for WT there?",
            "Because then the the bullet point below that you're just going to use on F of SU&W, which I guess.",
            "Would be the just as T + 1.",
            "So yes, good point.",
            "So let me clarify this.",
            "So what happens here?",
            "When I say F of STUTWT.",
            "F is a function that we have available 'cause we're learning it.",
            "This function F will depend both on South U&W and you want to make sure to evaluate your F at the noise that you experienced in the real world or in your rollout.",
            "You don't want to assume zero noise here.",
            "You could, but it would not be as precise a gradient as the average over multiple rollouts.",
            "Then if you use the actual noise that you experienced, so this thing here is actually differentiated through.",
            "So do we actually have two dynamics models?",
            "'cause there's one on the line above that is just a function venue, and then there's just one.",
            "Oh yeah, so notation wise.",
            "What yeah OK, so for this dynamics model here, the way this could be written as you have STUT plus WT that would just be a plus WT living here if you had a more complicated dynamics model where the noise comes in a more complicated way they have to solve in a different way for WT and then inserted in here correct?",
            "Yeah?",
            "So think of this as just a plus WT.",
            "And you do want to keep the plus WT you don't want to lose it.",
            "This this on the second line here is just going to evaluate S T + 1, so we don't.",
            "I don't see WT being used anywhere else, so the reason it will not evaluate the S T + 1 is because we actually substituted this thing over here.",
            "The policy Pi Pi Theta.",
            "Will go into here for UTI's.",
            "Rash is differentiating through this whole thing.",
            "And but it has the same value, but it doesn't have the same same value but not same that not the same gradient necessarily.",
            "It depends on how WT plays into it.",
            "If it plays into it, the simplest way I believe it actually will have the same gradient even more complicated dependence on WT.",
            "It's not guaranteed that the gradient is the same.",
            "Thank you, thanks for pointing that out.",
            "FG0.",
            "We don't need to learn a dynamic smaller 'cause the competition graph doesn't have the dynamics model in it.",
            "We just have a Q function there.",
            "So we have a backdrop to compute the gradient of the policy and then a Q function update which could be just like with the value function we had before.",
            "Just some kind of squared error minimization where this estimate here is usually frozen to be whatever the previous parameter vector predicted for it to be fixed.",
            "Here, derivative here only with respect to the first Phi appearance.",
            "The first term."
        ],
        [
            "OK, so we have.",
            "A few policy and organs that we've seen or whole family of them is actually been used to solve some pretty interesting tasks.",
            "2 dimensional robotic control in majokko the different versions seem to learn about equally fast."
        ],
        [
            "If you want to turn this into domestic policy gradients.",
            "And for it to work well, you need to do something little extra.",
            "If you naively train SVG zero, you end up very quickly with a deterministic policy that optimizes the Q values.",
            "'cause when you look at the policy update, quit trying to find the action that maximizes Q and that will naturally make it deterministic.",
            "So you need to do something to keep it stochastic.",
            "So add noise to the policy.",
            "Explicitly force your policy be stochastic when you collect data.",
            "Then your Q values learned our learned off policy, but that's fine with Q values.",
            "You can learn them off policy if you do TD0.",
            "So with TD0 you can learn your values off policy and have a stochastic policy collected data."
        ],
        [
            "Then you get this to work stably with a deep neural net.",
            "You need to play a few extra tricks, which were proposed in JDG paper by Tim Lillicrap and collaborators.",
            "For learning the Q function, you just don't.",
            "Don't just take the last rollouts, you actually keep around the replay buffer against the replay buffer, do updates on your Q function, then when you do this, when you compute your target Q values as reward plus next Q value under the current policy.",
            "These primes here 5 prime and Theta prime.",
            "What they mean is that they are not the current setting of your policy parameters in Q function parameters.",
            "Instead, there the Polyak averaged, which is exponentially average past weights.",
            "That you've encountered during learning so far, and so these average parameters evolve much more slowly than the parameters of your current policy and current queue function and will stabilize your target values, which allow your argument to be more stable and hopefully converge."
        ],
        [
            "And this actually has been applied to some more complicated robotic simulation tasks and including vision based control on driving.",
            "Question.",
            "Functions have a have a meeting or is it like is it function correctly in itself?",
            "So I'm not an expert on the meaning of of what?",
            "Is associated with it, but a crude meaning that I've heard people associate with it is it's like keeping track of the posterior over possible values that your neural net should take on, and it's keeping track of some mode of a posterior based on everything you've seen in the past.",
            "In practice, what it means is that you can take larger step sizes and more more aggressive step sizes and at the same time still have a stable representation.",
            "They keep around that is the result of your learning.",
            "This effectively kind of slow.",
            "That average version is slower moving than the thing that's actually moving ahead, but that way you can explore, you can have more aggressive exploration, more aggressive optimization in the landscape of that you're working in while keeping around something that's quite stable.",
            "Yes.",
            "Play for the policy updates or just value book chapters.",
            "I believe just for the Q value updates, but I'm not 100% sure."
        ],
        [
            "OK, that's our first type of gradients which you can use inside model based reinforcement learning.",
            "Or if you do the SG RDP version, even within model free settings.",
            "But assuming a differentiable policy and or dynamics model.",
            "If you have discrete actions, what I just described to you is actually not going to workout.",
            "If you don't have the ability to learn a dynamics model, if discrete state space is also not going to workout as describe, let's look at."
        ],
        [
            "Some other methods that are applicable there.",
            "Simplest way to compute branches just to do finite differences, right?",
            "If you have an optimal optimization objective utility which depends on your policy parameter vectors, Theta, just perturb the policy parameter vector Theta one coordinate at a time, positive perturbation negative perturbation.",
            "See what the differences divide by two epsilon and there you have your derivative.",
            "Estimate some trickiness there."
        ],
        [
            "If your function is stochastic, so at any point along the X axis which is here, optimizing a function of X anywhere along the way when you ask for a while value distribution over Y values, then you need to be careful because you might end up with samples that are somehow inconsistent in some way.",
            "You sample.",
            "Here you get this point, you sample here you get this point, and now derivative looks the exact opposite way down what you want, so you need to be a little careful about this.",
            "If you have control over the noise that's present.",
            "Well, if you don't control you just average over many samples.",
            "If you do have control, you can make sure that the noise present in the sampling is the same for the plus and a negative epsilon perturbation.",
            "And now we can get out a reliable derivative estimate.",
            "So fixed."
        ],
        [
            "Random seed can help to get those derivatives can actually also help.",
            "In general whenever you're doing policy optimization.",
            "This way when the dynamics model is being executed, if you control, the noise is consistent.",
            "That's the same as intuitively something like Oh my helicopters trying to fly in pretty windy conditions, but when I compare performance of two policies actually compare those policies under the same windy conditions, not each time.",
            "Randomly sample windy conditions and see which one does better.",
            "This one might have just been luckier.",
            "With the wind conditions than the other one."
        ],
        [
            "With this kind of approach, just finite perturbations find a different derivatives.",
            "Actually, the helicopter project at Stanford was able to learn to control this helicopter here.",
            "So you see, here is a helicopter.",
            "Being stable control is actually very hard control problem, and in fact being stable, controlled to fly inverted.",
            "Now.",
            "What happened here is that."
        ],
        [
            "To make this work.",
            "A.",
            "Policy classes handcrafted with careful consideration of how to four controls that you have for a helicopter depends on the state.",
            "Resulting in only 12 parameters, I need to be optimized over and we have only a very small number of parameters.",
            "Actually, this simple methods can be made to work and very successfully so.",
            "To do this kind of delivery method, if you want to optimize over policy with 100,000 parameters might be a lot harder.",
            "Might take a lot longer, but if you do have a lot of prior knowledge then this simple methods can actually work quite well."
        ],
        [
            "Now there are other gradient free methods beyond just find out difference perturbation, something called cross entropy method that covariance matrix adaptation and variance.",
            "There also stick a look at those.",
            "Cross entropy method is still optimizing the same objective.",
            "Will view you the utility as a black box.",
            "So in everything we've seen in the first half of the lecture, we've taken derivatives through the dynamics and so forth understood that there was a temporal process happening here, we're going to ignore that.",
            "There's a temporal process.",
            "Just say we have a policy, see what happens just like in the finite differences."
        ],
        [
            "So that means we're probably not going to be optimal in terms of sample efficiency, but we do have something very simple to work with.",
            "And so cross entropy is an example of revolutionary method an.",
            "What characterizes those methods that you keep around the population of whatever you're optimizing over.",
            "In this case of population of policies.",
            "So we don't have just one parameter vector Theta with a distribution over parameters Theta.",
            "For example a Gaussian distribution with a mean mu and maybe a fixed covariance.",
            "Or maybe learn covariance.",
            "In this case, let's assume a fixed covariance, so we don't have to explicitly talk about it, and we have a mean mu I, which is that for our current population, our current.",
            "Additional policies neways the mean and if we want to put in that population and sample from a Gaussian with that mean and that gives me a parameter vector Theta CM, then iterates.",
            "Samples population members, which is samples policies from the Gaussian distribution over policies.",
            "Once it's sampled, a policy, it executes robots.",
            "Under that policy it stores what parameter vector is sampled, as well as the utility achieved under that policy.",
            "Once it's collected those and now checks.",
            "Some subset of the policies that it sampled, let's say the top 10% and then computes anew mean for the Gaussian which maximizes the log probability of the policies sampled that were in the top 10%.",
            "So you're shifting your Gaussian mean to where the top 10% was, and then repeat again and again and again.",
            "Very simple."
        ],
        [
            "Seizure can actually work embarrassingly well, so for example, for the game of Tetris, which for a long time was a benchmark and reinforcement was used quite a bit.",
            "The numbers were such that cross entropy was actually outperforming other reinforcement methods by quite a bit for a long time, and it took till.",
            "And it's 2013 paper.",
            "For a dynamic parameter method to finally perform as well as cross entropy method in the game of Tetris.",
            "Even the dynamic program methods use a lot more information about the process than this.",
            "Cost cross entropy method."
        ],
        [
            "There's a whole family of these, so I'm collecting these on the slide.",
            "The intent is not for understanding each one of these just right now when we showed him too, but later we look back at the slides.",
            "You can see the relation close relationship reward.",
            "Weighted regression says instead of taking the top let's say 10% for each policy that was sampled, we take some function of the utility achieved and the probability of sampling that particular policy put that in front of the log probability and maximize this cross entropy would be a special case where you make that function.",
            "One minor in the top 10% zero when you're not in the top 10%.",
            "But there are other functions you can take prescribed by this paper over here.",
            "Path integral method.",
            "Replaces the top ten percent 10 scoring by exponential weighting of utility.",
            "Achieve so it's a soft version of cross entropy where the best ones get waited more and then if you're not as good you get weighted less in the finding of the new mean.",
            "You can also find the covariance, not just the mean of that Gaussian.",
            "Then you get a CMA method so and then you can also do play some tricks with how you weight each one of them as a function of their utility.",
            "And then in the power method there is actually some additional M like ideas and important sampling ideas that extract a little bit of 2nd order information in the update, which might make it more efficient.",
            "So all of these effectively look at maximizing log probability of.",
            "Parameter vectors of policy that you sampled before based on how well these parameter vectors ended up performing in their rollouts."
        ],
        [
            "Some example success stories often used in graphics for animation of humanoid fly characters.",
            "We see here on the right is learn to control for the ball in a Cup game.",
            "It started with a demonstration of policy was learned to match the demonstration.",
            "That was not a good policy.",
            "We're seeing that policy being executed initially, then the power method, which is a cross entropy method is being run to optimize the policy.",
            "And in just about 100 iterations it's able to learn to swing up the ball catches in the Cup.",
            "This particular case it's working based on a motion capture system that tracks the location of the ball, so it's working in state space and knows location of the ball knows the state of the robot and then based on the state information provides feedback control to swing up and catch the ball."
        ],
        [
            "The main caveat, the main advantage of this man is super simple.",
            "Always nice to introduce to do as your first Test when you try something in reinforced learning.",
            "Just run cross entropy like method or just cross entropy itself.",
            "See how it works, gives you a measure of how feasible it might be to solve the problem, not super sample efficient.",
            "That's the main caveat.",
            "Especially, we have a high dimensional space that you're optimizing over.",
            "It could be.",
            "Sample inefficient, but the flip side of all of that is."
        ],
        [
            "But maybe these days you don't care as much about sample efficiency.",
            "If you're working in simulation.",
            "If you can parallelize over enough machines.",
            "This method is extremely easy to parallelize and scales extremely well with parallel with parallelization, where you see here is a graph showing as a function of the number of cores.",
            "The median time to solve in this case, I believe, is a 3D humanoid problem where you see here is a linear curve, which means that as you introduce more course, it literally speeds of the method by a factor of how many more cores introduced into double number.",
            "Of course you go twice as fast finding a solution, and so that's really nice.",
            "So actually you want to find a solution as quickly as possible, even though these methods are not the most sample efficient, they might actually be the fastest if you have a large compute budget, yes.",
            "So will you only looking at the results of the last iteration when you're trying to take the top 10%?",
            "Or do you look at all of the history?",
            "Everything laceration?",
            "Why?",
            "OK, so in these methods it's actually extremely simple.",
            "What happens you have your current distribution over policies you sample from that you execute each one of them.",
            "Maybe one episode for each one of them might be a few episodes if you're in a very stochastic environment to get a better measure of how good they are, and then you just you don't do any optimization on those executions, you just execute them as is.",
            "So there is no the first execution is not any different from the last one at that point.",
            "What I'm asking is like when you when you find the top 10% like because you're going to be iterating this right.",
            "Yes, you look at the whole history and everything OK. From last version.",
            "Typical simple implementation just looks at the ones from the last iteration.",
            "Now, if in the last iteration you ended up being somehow confused 'cause something weird happen, then you found the spot where things look good and you're concentrated all there and things all of a sudden are pretty bad.",
            "Then you want to exactly think about what you're thinking about this.",
            "Well, should I really reset from scratch, or should I actually go back to previous iterations, see where things were pretty good?",
            "An restart from there?",
            "Nothing.",
            "There's a lot of interesting questions about how to what you might want to keep around.",
            "From past iterations, I haven't seen anything people might have done it.",
            "Where to keep around more?",
            "You do a lot of rollouts here, so probably not going to keep around everything, but it might be that there's a scheme where you keep around most salient past executions to them more effectively update in the future.",
            "Sorry.",
            "Yeah."
        ],
        [
            "Yes.",
            "Later.",
            "Data.",
            "We assume that.",
            "Algorithm.",
            "But normally no more questions.",
            "Yeah, so yeah, you're right.",
            "So the assumption and what I described is that the posterior keep around is a Gaussian over parameter vectors.",
            "It's very simple assumption.",
            "It's not necessarily the right fit for most situations, it's quite effective and simple to go with.",
            "It would be interesting to try other distributions that might better fit where your posterior looks like and re sample from those simple things could be let's say a mixture of Gaussians which is also very simple to compute them to sample from.",
            "And investigate how that does, maybe even more complicated distributions.",
            "The results have shown to you are all just keeping a simple Gaussian distribution around them.",
            "Yeah, if you used one of the latest methods in inference for representing distributions, maybe use a personal auto encoder to represent your distribution.",
            "Or maybe you use one of the auto regressive models to represent the distribution.",
            "There's a tradeoff, because there would be some compute involved in keeping that distribution around.",
            "Then if you're doing everything in simulation, then ultimately compute is what you're constrained by.",
            "If you're doing things in the real world, then maybe more complicated distributions, even if they require a lot of compute, are easier to justify, because, well, you want to reduce real world rollouts, and you can do it by doing more compute.",
            "Yeah, be very interesting to play around with.",
            "I don't know how well it would work, but there is probably a tradeoff point there where being a little smarter about the distribution while not expecting too much compute somehow somehow will achieve the best performance.",
            "Yes, this is.",
            "They're environment constantly.",
            "Changes are driven experiments where you vary the environment.",
            "End times.",
            "Why are you running cross entropy method?",
            "So if the environment is 2, is the environment can change one way it changes that you get dropped in a new environment, but there's a distribution over environments, in which case you can think of that randomness and which you might not have to do.",
            "Multiple rollouts per policy to get a good estimate of how good that policy parameter vector is, so that would be a pretty good fit then.",
            "Another way the environment could change is that your environment changes during the rollout, so there is.",
            "Change overtime.",
            "In that case, if you had a simple feedforward neural net policy, let's say that you're parameterising, then it would not be a good fit for your environment, and so if you want to apply this approach, you want to do it the way I just described, you would have to pick a recurrent neural net.",
            "Recurrent neural net would then somehow store information about how his environment would change it with somehow track how the environment is changing and so then what you would learn as you would learn the weights of the recurrent neural net.",
            "But in its activations it would be able to keep track of changes to the environment and so then you would train a.",
            "Policy that is good at adapting to a changing environment which is not a lot.",
            "Not a lot of people are testing on this, but it is in the real world.",
            "It is what it would be like.",
            "You need something that's good at adapting to a changing environment, not just a good policy for stationary environment.",
            "Because the big Pro is fast as you do that you end up with the same time.",
            "So that's a good question.",
            "So.",
            "One reason it's fast is also because you don't have to do a back propagation pass.",
            "OK, so that's nice because that means you don't have to store any information here forward pass, and so if you had a recurrent neural net.",
            "Policy you would have to store a lot of information you forward pass and then do a backward pass and so for kernel nuts.",
            "It would also be extra fast in some sense that you don't have to do a backward pass, so there might still be benefits in that regard, the parallelization.",
            "One thing I forgot to mention, the reason the position works out so well is that.",
            "All you need in terms of communication between nodes is communicating the random seed of the perturbation.",
            "So even if you have a, let's say million dimensional.",
            "Parameter vector.",
            "If I'm collecting the data from all the other workers, all I need to know is what random seed did they use and what was the average performance under the policy they have.",
            "'cause if I know the average performance which is a scalar, another random seed which is a scalar that can locally be construct, what their perturbation was, and I can do the cross entropy update locally.",
            "So communication is essentially.",
            "Well, not zero, but it's extremely close to 0 and that's why you see the linear scaling, and that would still be true if you had a recurrent neural net policy is still would only need to know the random seed that was used for the perturbation at each of the worker nodes to understand what updates to make locally.",
            "I think the real question in some sense, what you're asking about is.",
            "What does it mean to be acting in an environment that continuously changes?",
            "And I think it's a question a lot of people are very interested in, but we don't have really good benchmarks for makes it harder to work on.",
            "So you can.",
            "Somehow you need a setting.",
            "You need settings where things are continually changing on you so you can test those ideas, and I think right now we don't have a good set of.",
            "Environments in which to test them, even though we know that for the real world, clearly it is what we need.",
            "Now we're going to look at for the last 1/3 at what are the classical policy grading methods?",
            "So we're going to see now is if somebody tells you policy gradients.",
            "And don't say anything more.",
            "They'll likely refer to what we're going to cover now, even though we've already seen two types of policy gradient methods, namely the backwardation ones, namely path derivatives and we've seen the kind of cross entropy evolutionary style updates.",
            "Compared to evolutionary strategies will have the same set of assumptions.",
            "That is, no assumptions on the dynamics model, no assumptions on the reward function, we just need to be able to execute things and know how much reward was collected.",
            "We can work with.",
            "Discrete state action spaces, but we're going to want to do now is look at the details of what happens during the rollout to make the method more sample efficient than evolutionary method."
        ],
        [
            "So.",
            "Will change notation a little bit temporarily to reduce clutter and will introduce something called Tao.",
            "Tao stands for an entire trajectory.",
            "OK, so utility under policy Pi Theta is expected sum of rewards and will write it as some overall possible trajectories Tau probability of that trajectory towel under the current policy parameter vector Theta times the reward accumulated along that trajectory Tau.",
            "Same problem as we had before, just changing notation to make it more compact and so our goal is still to find the policy parameter Theta that maximizes expected some of her words.",
            "Now written this way.",
            "Yes.",
            "The same as the lobby with me.",
            "Yeah, so trajectory rollout are the same thing.",
            "So Tao stands for S0U0F1 U One South, two U2.",
            "The entire sequence of States and actions."
        ],
        [
            "So we'd like to do is computed gradient of this utility, you Theta.",
            "Anne.",
            "So.",
            "Nabla Theta some overall trajectory's of the quantity we just looked at.",
            "An to derive this will place some tricks that only in hindsight makes sense, but it's a very short derivation.",
            "I think it's worth being aware of this trick and understanding how this is derived.",
            "So we bring in the gradient into the submission.",
            "That can pretty much always be done when you have a submission.",
            "It can also be an integral.",
            "Can always almost always be done too.",
            "Then now we want to somehow compute that gradient.",
            "We know that the distribution depends on our parameter Theta, but we're actually going to play a little trick first.",
            "When multiplying divide by probability of trajectories under Theta.",
            "Then we.",
            "Move the denominator off to the right.",
            "Then we used identity that the derivative of the log of X is the same as DX over X.",
            "So we replace it with.",
            "Gradient of log probability of trajectory under Theta.",
            "So this thing here.",
            "It's just.",
            "Applying derivative log in the opposite direction, then it's usually used.",
            "At this point, we actually are in a very interesting situation.",
            "There is this interesting is because we have an expectation.",
            "Again, when you see there is an expected value of under distribution over trajectories of grad log probability of trajectory times reward.",
            "We have an expectation we can approximate it from samples and so we can say, well, we're going to compute this grander approximated by this quantity over here, which is same thing as we have over here, but approximated from samples.",
            "So that means that we can execute our policy.",
            "And for the rollout structures that we obtain, we can average that quantity for each roll out to get a great investment.",
            "Now we still have to compute that can't even told you how to compute the ground log.",
            "Probably have a trajectory, but if we know how to compute that, then we can do this based on sample estimate.",
            "You can drive this in a different way too, so here it's very much a hindsight type derivation where you want it to end up with an expectation.",
            "So you multiply the probability divide by it, and then work the other thing in the back so you end up with an expectation so you can compute this from samples."
        ],
        [
            "Another way to derive it is to look at important sampling.",
            "What is important sampling?",
            "It's a tool to sample from a distribution.",
            "Well, to generate sample some distribution that you can sample from directly.",
            "So you want to sample from a distribution.",
            "Let's say you want to sample from a distribution under parameter Theta, but all you can sample from a distribution under parameter vector Theta old.",
            "And you want to still know what the expectation is under the new parameter vector, Theta?",
            "What you can do is you can sample from third old and then re weight your samples by the ratio of.",
            "This probability under Theta divided by probability under Theta old if you re weight your samples by this ratio you get a good estimate of the expected value under the distribution under Theta even though you sampled under Theta old.",
            "So this tells you that in principle you could sample under current policy.",
            "They are old.",
            "An evaluate how good another policy is with new parameter vector Theta and then maybe optimize this quantity to find a better parameter vector Theta.",
            "You can actually do this, but often it's not that great an estimate.",
            "If you move far away.",
            "So actually the 1st order approximation of this might be the most meaningful thing to use, so see what the 1st order approximation looks like.",
            "First order, approximate this important sampling estimate.",
            "Work through it.",
            "This expectation over here take the gradient to see what it is.",
            "Then we evaluate it at the current.",
            "Theta, which is Theta old.",
            "We get the same thing we had before and so we get the same quantity as we had on the previous slide.",
            "So we see here is that from important sampling.",
            "Seeing what the important sample that looks like what the derivative looks like in the important sample estimate, we end up with the exact same.",
            "Checked it or exact same brand as we had on the previous slide.",
            "So two ways of deriving the same thing."
        ],
        [
            "Very interesting what we have here is a great an estimate.",
            "That works even if the reward function is discontinuous.",
            "So if your rewards is 01 reward.",
            "You can still use this.",
            "'cause we actually don't take a derivative of the reward function even though we're trying to optimize expected reward.",
            "We never take a derivative through it.",
            "And a lot of problems are more easily specified with 01 reward function, success, failure and so for those problems this can actually work.",
            "Just really nice.",
            "Intuitively, what?"
        ],
        [
            "Happening with these kind of gradient estimates is that.",
            "What you're doing is you increase if you take a step in the interaction, you increase the probability of paths that have high reward and decrease the probability of path to have negative reward.",
            "Or if their older roots are always positive, you increase a lot the probability of paths that have high reward and increase a little bit the path of the probability of passing have low reward.",
            "Now I can't increase all probabilities, of course is to sum to 1 S. This renormalization happening, and so we've all rewards are positive, will still end up being that the ones that don't achieve as high reward only will see their probability drop to be able to increase the probability of the ones with higher reward.",
            "So it's happening here.",
            "If you look at three rollouts.",
            "If you have three rollouts, this update which shift probability mass today rather has better reward is very different from the past derivatives we saw before with the path derivatives from the very first stretch, those derivatives will look at those trajectories and look at how should I change my parameter vector to locally perturbed my trajectory to get a better trajectory.",
            "And then essentially update the path for each of the rollouts rather than shifting probability mass from the not so good ones to the better ones.",
            "It's a very different way of optimizing the beauty of this one is that you actually don't need derivatives through the reward function.",
            "In fact, will soon see you don't have any derivatives through the dynamics model to compute the grad log probability of path under parameter vector.",
            "Let's take a look at that."
        ],
        [
            "So what does this gridlock probability of path under parameter vector?",
            "Well, the probability of a path is the problem of initial state, which I left out here.",
            "Then times probability of next state given current state and action and probability of action given state multiplied.",
            "Overall times this is the policy we have access to that this is the dynamics model.",
            "We might not have access to that, but look there's no Theta here and that's going to help us a lot.",
            "So if you work this out grad log for product is grad.",
            "Of the sum of the logs then.",
            "Since there is no Theta in here, the grad respect to Theta just disappears and we end up with just grad Theta.",
            "Some log probabilities of actions given state.",
            "So we see here that we can compute the derivative without having access to a dynamics model.",
            "OK.",
            "So we can do now is we can do rollouts.",
            "For each of the rollouts, we can compute the ground log probability of path under the parameter Victor.",
            "This way, multiply with reward along that path, and that gives the contribution of that path to the gradient."
        ],
        [
            "Spell this out more fully.",
            "This is how we're going to make gradients.",
            "And this quantity.",
            "Over here we're going to estimate this way.",
            "And an expectation this gives us the right grade.",
            "You can do infinitely many rollouts.",
            "This will be the correct gradient if you do.",
            "Finally many it will be an estimate of the gradient."
        ],
        [
            "As formulated so far, there are some issues.",
            "It's a very high high variance estimate, so I do need a ton of robots or you need to play some tricks to reduce the variance of this estimate so we look at some tricks to do that.",
            "Baseline temple structure exploitation and then also look at what we need to do to make this stable in terms of step sizing slash trust regions."
        ],
        [
            "Yes.",
            "Why's it high variance?",
            "Intuitively the reason is high variance.",
            "Imagine all your words are positive, then every path will try to increase its own probability.",
            "When you when you roll out and you have a path.",
            "This gradient says we should make this path more likely, even the not so good ones.",
            "So that's not great.",
            "That means that you have all these contributions fighting each other, all trying to increase their probability instead of the bad ones understanding their bad and trying to decrease their probability.",
            "That's exactly we're going to exploit, but in the current equation right now that's not exploited.",
            "So.",
            "When the roads are always positive, we try to increase proposal passed.",
            "That's not great, so it really should do is introduce the notion of ranking or understanding what you're better than average or not.",
            "That's called the baseline.",
            "So the gradient becomes this over here with subtracting a baseline from their award along a path.",
            "Now if you're better than average, or the baseline is the average you've got along all executions, then you increase your probability.",
            "If you're worse than average, you decrease your probability.",
            "OK, there's some math you can do to show that this will still give you a unbiased estimate of the gradient.",
            "In fact, you can even do some math to find a optimal that is minimal variance estimate, which in practice people don't use that much, but it is what it is.",
            "You can principle use it simple thing to do is to just use the average of what you got among the roles that you did and just use that as your baseline."
        ],
        [
            "So.",
            "Another thing you can do is exploit temporal structure.",
            "Right now what we have is.",
            "This equation over here.",
            "The grad log probably have a lot of temple structure, but the reward has temporal structure too.",
            "An where you can expose the fact that Future Past rewards don't depend on current actions or otherwise phrased current action only influences future rewards.",
            "But in the equation over here, current action probabilities or grad law, current action probabilities multiply with all rewards past or future.",
            "We should get rid of the past and so then we get an estimate that looks like this where you only multiply with everything that's in the future here rather than also what's in the past.",
            "A good choice for B would then be expected future return.",
            "That's actually a value function.",
            "You can use a value function if you can estimate one, or you can just use an average of future returns from that time over your multiple rollouts.",
            "This gives us a full."
        ],
        [
            "Algorithm, so there's a vanilla policy gradient algorithm.",
            "You have some initial parameter vector Theta some initial baseline, maybe zero.",
            "What you think of us average performance.",
            "Then you iterate, you collect the set of trajectories Mexican current policy in each trajectory you compute the returns from that time onwards you compute what we call an advantage estimate, which is the reward you got from that time onwards, minus we get on average from that time or state onwards.",
            "That's the thing we have.",
            "Multiplied with the grad log probabilities.",
            "You might also refit this average estimate based on your current rollouts, and then you use this and your grad log probably actually gives state times advantage how much you're better than average to do an update to the policy.",
            "So the basic algorithm, if you draw enough samples, this will actually work pretty well."
        ],
        [
            "Now what remains I want to give you a quick overview on extra things you can do to make this even more effective."
        ],
        [
            "First thing is that you need to worry about step sizing.",
            "Step sizing always matters because he stepped too far.",
            "The grand is only local approximation step too far.",
            "You actually don't improve on their objective.",
            "You might do worse.",
            "In reinforcing"
        ],
        [
            "Learning, however, it's even a bigger issue than another optimization problems 'cause I'm reinforcement learning.",
            "If you take a step that's too big.",
            "The data you're going to collect under the new policy is going to be non interesting data.",
            "It's not good data to compute grades from.",
            "Imagine that you have a policy where you sometimes get nonsi rewards.",
            "Sometimes you don't, but sometimes do get nonzero rewards.",
            "You have some gradient signal.",
            "Take a step that's too big.",
            "You're back in the room where you never get any reward.",
            "You get no signal.",
            "So you want to be extra careful NRL about your step sizes in supervised learning, why?",
            "Why is it not as big a concern?",
            "Well, if you step too far the next update the data is sitting there for you, waiting for you to tell you to go back in the other direction.",
            "Maybe it's a little less efficient, but the data will tell you what to do RL you're too far have a terrible policy.",
            "You don't get good data, you don't get any signal, and now you actually don't get to optimize in a meaningful way."
        ],
        [
            "Simple step sizing would be just to do a line search.",
            "You have a great interaction along the great interaction.",
            "You tried different step size, see how well the rollouts perform and then just take the best one and call it done.",
            "It's all naive, but because it ignores more information you can exploit from your robots.",
            "It turns out that just looks at 1st order information, but you can exploit a little more information to understand where your first approximation is.",
            "Good words bad, and then within a region of where it's good, find the best spot.",
            "So we're going to define a trust region is the region where you trust your first order approximation."
        ],
        [
            "And then once we have the trust region, will find the best point within the trust region and we repeat what would be a good trust region.",
            "We could say, well, we want to stay in a region that's close in terms of in terms of policy parameter vector want resulting distribution after we make a change over directories to be close to the regional distribution.",
            "So we want to measure in terms of distribution over trajectories, not measure in terms of Euclidean distance in policy vector space.",
            "Now we need to."
        ],
        [
            "Should the scale Divergance turns out we can expand the probability of paths, compute those Cal divergences, go little faster, but you can look at the slides more slowly.",
            "Later.",
            "It turns out it simplifies and that all that shows up ultimately is probabilities under your policy.",
            "The Dynamics model cancels out again, so you can compute this trust region based on just having access to the policy.",
            "That's nice, so we have kill Cal based on the policy which you can approximate by looking at along the rollouts.",
            "At the state you visit, what is the KL divergent at those states?",
            "Between the policy that you used to have in the policy have after your update."
        ],
        [
            "So with that we have a constraint optimization problem.",
            "We want to keep the scale small enough to be in our trust region while optimizing the 1st order approximation of our objective.",
            "We can 2nd order approximate the scale to make this whole thing a little more efficient.",
            "What you get is the Fisher information metric shows up and what you actually then get this something very similar to the natural gradient exit with a constraint rather than just a natural gradient step.",
            "Question somewhere yes.",
            "Sorry, say it again.",
            "So G had here is the estimate of the gradient.",
            "So we compute an estimate of the gradient based on the likelihood ratio policy gradient that's living there, and that's what that is.",
            "Yes.",
            "Policy.",
            "So why do we measure the calendar direction that we're measuring it?",
            "It's the computationally easier one to measure is the main reason.",
            "Yes.",
            "So why we use the device rather than a distance metric?",
            "The math works out really nicely that I would say is the main justification.",
            "I mean the killer version in general is seen as a pretty good measure of how far distributions are apart, but among the measures you can use for how far distributions are apart.",
            "This is the one where the math works out very cleanly and is easy to compute with."
        ],
        [
            "OK, so we have a constraint optimization problem which can be solved quite easily, it turns out.",
            "It is very similar to the natural policy gradient.",
            "But by having a region rather than a stepsize, which is a natural policy gradient would, do you actually have a more stable algorithm?",
            "Define the actual update.",
            "Here you can form the Lagrangian and do dual descent on the Lagrangian LaGrange multiplier to find LaGrange multiplier that achieves the right epsilon and find the correct step that you want."
        ],
        [
            "OK, are we done when you do a little bit of extra work here.",
            "Turns out if Theta is high dimensional, inverting that Fisher information matrix can be expensive.",
            "An not practical.",
            "So there are some tricks you can play to make this practical which is detailed in the 2015 I smell paper by John Schulman and collaborators that can speed this up quite a bit.",
            "Another thing you can do to make this even more efficient is instead of using jihad here, you can replace this by something else.",
            "So instead of using the.",
            "Actual policy gradient.",
            "The 1st order approximation.",
            "You can actually go back to the important sampling.",
            "Best estimate that I showed you that allowed us to derive this first order approximation.",
            "You can actually plug back in the important sampling element of the objective aclocal estimate, which will also be valid within this trust region.",
            "If you make a trust region the right size and that way you can optimize something much closer to the original objective than just using the 1st order approximation.",
            "If you look at the arlab TRP implementation, that's what it does.",
            "As the optimization objective is the important sampling based estimate within the KL Trust region.",
            "And we'll look a little more at this later, so here are some experiments that."
        ],
        [
            "We actually did with this what we're going to see here is.",
            "The final gates obtained with Trust region policy optimization, which what we just covered.",
            "In this case, for 2D locomotion experiments in majokko.",
            "The reward function is actually pretty similar.",
            "Word function is just about forward progress and impact with the ground, so you don't need to encode anything about what walking looks like.",
            "The way this works is that initially this robot will just fall over.",
            "But overtime.",
            "Two thanks to different random executions.",
            "It figures out that some ways of falling over take longer to fall over that other sense or better puts more weight on those trajectories and then drifts towards finding a good gate that maximizes reward."
        ],
        [
            "Here are some comparison graphs with.",
            "Related methods, but that don't have all the machinery that we just talked about and actually tends to learn quite a bit quicker.",
            "Even on the simple problems, but especially."
        ],
        [
            "On the harder problems such as Hopper, Ann Walker and apply the exact same method to attack."
        ],
        [
            "Organs actually you can use trusted policy optimization to find good policies on Atari original results.",
            "Of course, work with Q learning DQM and it turns out that typically decline remains the more sample efficient method.",
            "But in terms of Wall Clock time and ease of use, often the policy implementation methods are easier and the current state of yard is often achieved with something called a 3C, which will cover in a few slides an that might not be as sample efficient document, but is wall Clock time more efficient?",
            "Then the one and very similar to what I just showed you.",
            "So stay."
        ],
        [
            "Look at that in the last couple of minutes we have here with 10 minutes for connecting with actor critic.",
            "We can do even better than what I just described by bringing in value functions."
        ],
        [
            "So our policy grand estimates.",
            "As Summer future awards minus a baseline and baseline, a natural baseline would be the value function.",
            "If we have a value function 'cause that tells us from this state on average, how well are you going to do and then we can understand whether the random action you took was better or worse than average, and then you need to increase or decrease the probability of that action.",
            "So how do we estimate the value of a state under current policy while there's the Bellman equation which tells you how the value of current state S is a function of policy probability of actions given state, then probably affecting the Christian action reward associated with the transition plus gamma times the value at the next state.",
            "Self consistent set of equations that you can solve for the value of the policy.",
            "In effect, you can fill in your current estimate on the right hand side and then compute from that the left hand side.",
            "And repeat until this converges."
        ],
        [
            "If you have a large state space, you can't really do it by enumerating over all States and repeating this, so you need to approximate things.",
            "You might have a big neural net that represents your value function with parameter vector 50.",
            "Initially you collect your data on your data, you set up the objective above here based on samples, and so you want to move.",
            "Want to find a new value function V5 that is correspond to their left hand side here and then it should match this thing over here, which is a sampling of the right hand side.",
            "And then of course, you don't want to move too far from your current estimate, otherwise you might be overfitting to the last set of samples, and so this way you get a new estimate value function, and then you might repeat this until you have convergence.",
            "We might just do 1 update and do a policy grand update again."
        ],
        [
            "So we can then fill that in over here an estimate of the value function.",
            "What's nice about this is that we have generalization.",
            "Now across states that we're exploiting, rather than just a simple baseline.",
            "Might just depend on average.",
            "How much do you get?",
            "We have another estimate that's high variance sum of rewards.",
            "When you do a rollout again from the same state, likely different things will happen.",
            "So clearly this is going to be a noisy estimate.",
            "What it really is is an estimate of the Q value.",
            "What you would like to have there is to know what the Q value is so that state in action and then use that compared with the value in.",
            "That tells you how much better the action was.",
            "Then other actions are worse than other actions.",
            "That tells you how to update your policy.",
            "So.",
            "Yes, it's a reasonable estimate to just take some of rewards.",
            "It's an unbiased estimate, but it's not a low variance estimate and you might need a lot of samples for it to be good.",
            "It doesn't exploit any notion of generalization that we might be able to exploit from having seen other things.",
            "So we can first thing we can do is reduce variance by discounting very easy thing to do, and then the next thing we can do is reduce variance by function approximation.",
            "So maybe that in reality we care about actual sum of rewards.",
            "But we use discounting nevertheless to reduce variance."
        ],
        [
            "OK, so.",
            "Gamma here, then, becomes a hyperparameter that we optimize over that maybe initially is.",
            "Far away from one, maybe 0.5 something very small so you don't look far ahead.",
            "You have low variance, but then as you do more learning it might become higher.",
            "You're able to look further ahead."
        ],
        [
            "Inducing function approximation where you can do is well, you can say just like we saw very early on you can see current reward plus value at the next time.",
            "That could be what you're using as your Q estimate, or you could use value at the next next time or reward the road without the next next next time and so forth.",
            "A 3C is policy grade method that follows the things I've described, minus the trust region.",
            "But then use instead of sum of rewards.",
            "Uses some estimate with some value.",
            "There may be 5 steps ahead and then the value function of 10 steps ahead.",
            "Then the value function instead of just the sum of rewards."
        ],
        [
            "You can also wait all those estimates and if you take an exponentially weighted average, it turns out that the way you calculate things, you do it cleverly.",
            "It's pretty much as efficient as using just one of them, very related to TD Lambda, and that's what generalized advantage estimation uses.",
            "And then you have a instead of choosing a horizon at which you cap it off, you choose the Lambda, which effectively says in a weighted way.",
            "How far are you willing to look ahead in your rollouts.",
            "And so that's essentially using Elegibility traces, which I imagine rich might talk about this afternoon."
        ],
        [
            "So what do we have then?",
            "We now have an actor critic algorithm.",
            "We're going to learn both a policy in a value function.",
            "We're going to collect rollouts.",
            "An estimates of the value from each state action.",
            "The simplest estimate here is just sum of rewards experienced, but more complicated estimates will be the ACC estimate of some of the words followed by value function or the generalized advantage estimate, which is the exponentially weighted combination of rewards and value functions at all times.",
            "Once you've collected those, you can do an update to the value function by saying, well, the value at that state should be brought closer to what this quantity is.",
            "And of course some regularization and we can get a policy grand update on the policy based on the Advantage Testament over here.",
            "Whatever you estimated as sum of rewards which might be using value functions in here minus the value of the state that you were in times grad liability action gives state so pretty easy to implement, and that's going to be a very effective policy gradient method.",
            "There are many variations you can have here.",
            "You can imagine that for the targets here.",
            "That instead of using.",
            "Case step look ahead.",
            "You only use one step.",
            "Look ahead which would be TD0 matching.",
            "You can use full roll out.",
            "There's a lot of tweaking.",
            "You can do different choices of Lambda and gamma that you can use over there, and then you actually don't have to use the same over here as we use over here principle that could be different.",
            "In fact, often you might do something where here when you're initially debugging you still just use a sum of rewards because the sum of rewards is unbiased.",
            "And once you do value functions it becomes biased so often.",
            "Initially you'll use some of her words over here.",
            "Ann still something with a value function of TD style estimate over here and then once things are working well, maybe you start using value functions in this estimate over here too.",
            "OK."
        ],
        [
            "We see async Advantage actor critic.",
            "Actually this is on a bunch of Atari games, so you see here is that a 3C outperforms Q learning methods decurion style methods.",
            "The paper from leaf about year ago experiments from here in 1/2 ago.",
            "It shows that ATC outperforms the current methods in terms of training time.",
            "A through C was all."
        ],
        [
            "Applied then she do a pretty complicated task where you see here is a learned policy that learn to map from first person vision.",
            "Two actions and how to navigate.",
            "Amazed to find the things that are high reward like apples and cherries and so forth.",
            "So this is to learn a vision system.",
            "3D vision system.",
            "Effectively an actions all in one big neural net and was possible with this approach."
        ],
        [
            "We can also study the effect of the choices of Lambda and gamma.",
            "It turns out that for Lambda here and then gamma here it's good to choose something that's not all the way at the extremes.",
            "So Lambda 0.96 gamma 0.98 seems to be best here.",
            "What that means is that there is a little bit of a tradeoff between how far you want to look ahead gamma.",
            "0.98 means that your effective looking at about 5200 time steps and that that's in these environments a good look ahead while reducing variance by not looking too far ahead.",
            "Lambda is determining how much you're waiting the.",
            "The pure reward based estimate versus the value function contribution when you top things off with value functions and it shows that you actually want to mostly rely on the pure reward based estimate, which is the unbiased estimate.",
            "But bringing a little bit of the value function to reduce variance."
        ],
        [
            "With this able to learn things like this, what we're seeing here is a humanoid robot in 3D.",
            "Now learning to control itself.",
            "The reward function here is just the further North, the better, the less impact with the ground the better.",
            "And this is a neural net with about 100,000 parameters going from mapping from the Joint angles, joint velocities and center of mass coordinates and velocity to torques at each of the Motors.",
            "So it's completely from raw sensory inputs, in this case at the at the joints to Ross rock controls at each of the Motors.",
            "And we're of course with reinforcement is that it's not specific to the environment in which would deploy it.",
            "Additionally, if you want to, you know to walk.",
            "Maybe you would have spent a lot of time thinking about humanoids and how you should control them.",
            "But with this kind of approach, you just.",
            "Diploid, our algorithm on the robot of choice.",
            "You switch your robot of choice and you can run the exact same code.",
            "This is also the Majorca environment is a simulator build by a motor at University of Washington Ann.",
            "This robot is now learn to control yourself with the same objective.",
            "Same algorithm and control itself to run actually quite fast.",
            "You can also put in different tasks.",
            "It doesn't need to be running.",
            "Here there what function is about how closed ahead is to standing head height.",
            "Setting is better than standing, but it's even better to stand.",
            "And what you see here is it invents standing up.",
            "There's nothing in there that has told it about what you need to do to stand up.",
            "It's just measuring distance to standing head as the reward function, and it learns against that."
        ],
        [
            "OK, So what do we have left we've seen here is actually different ways of computing derivatives here.",
            "Pop one level up.",
            "We've seen it all in the context of reinforcement learning, but actually this is applicable.",
            "Whether it's reinforced learning or any other setting in which you need derivatives."
        ],
        [
            "As in the slides would have.",
            "Here is a few examples of how you can compute gradients.",
            "For stochastic neural Nets using the same methodology we just covered.",
            "So it turns out that in this notation, a square node is a deterministic node.",
            "A circular node is a stochastic node.",
            "And now you can compute gradients through this stochastic neural Nets by using the likelihood ratio trick, the same way we computed it for policy gradients.",
            "Well, it's really beautiful here with Miss is so interesting is that you don't have to do the work by hand.",
            "What you can do is, once you have your stochastic computation graph, you can actually define a loss function.",
            "So instead of saying oh as a stochastic node that's being sampled, I need to do this grad log trick.",
            "You can actually define a loss function on that graph whenever this to Kasich node, you hang off a loss function as the log probability of this so castec node times what comes after the node.",
            "Once you do that, you can plug this into Tensorflow and Tensorflow.",
            "Compute is likely racial grains for you with back propagation, just like you could do with a deterministic network, so you can automatically get these gradients for stochastic neural Nets by just computing the combination graph corresponding to this, which means hanging log loss functions, log probabilities, times what comes after off of these graphs."
        ],
        [
            "Some food for thought.",
            "We've seen very different ways of computing derivatives, including in this general setting here.",
            "Even when you have a stochastic notice being sampled, sometimes you can re parameterise and bring it out.",
            "Bring out the noise and you might have a question.",
            "What is the more effective way to compute derivatives?",
            "It's not always obvious.",
            "What the best way is to do it?",
            "And I think there's a lot of thought that can still go into how to compute derivatives when you have different ways of computing derivatives.",
            "There's empirical things you could do.",
            "You can compute them both ways.",
            "Look at the variance, see which one has lower variance empirically.",
            "Maybe go with that, but there might be other things you can do.",
            "I think we're out of time, So what I'm going to do is flush by you, the five remaining slide."
        ],
        [
            "Instead, you can look at when the slides are put online and made us set of slides.",
            "I show you current frontiers directions where you might want to do research in deep reinforcement learning, as well as pointers to recent papers that relate to this."
        ],
        [
            "This one slide here is another slide of current directions that I think are quite important, so a lot of opportunity to do research in this field, and a lot of starting points.",
            "In terms of coding or more."
        ],
        [
            "Materials there are a few classes that you can check out also be linked in the slides there is."
        ],
        [
            "Code bases that you can refer to to get started, and there's a lot of."
        ],
        [
            "Domains which can often be the bottleneck.",
            "If you have any domains to test in that you might want to try out to work with.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I gotta couple affiliations.",
                    "label": 0
                },
                {
                    "sent": "It's just three research scientists at open AI.",
                    "label": 0
                },
                {
                    "sent": "Where do AI research professor at Berkeley?",
                    "label": 0
                },
                {
                    "sent": "Where do AI research and teach AI and do some administration, but not while I'm on leave right now and then.",
                    "label": 0
                },
                {
                    "sent": "I'm also one of the Co founders of Great Scope.",
                    "label": 0
                },
                {
                    "sent": "If you ever need to do any grading, this can save you a lot of time.",
                    "label": 0
                },
                {
                    "sent": "We got tired of spending a lot of time on grading, homework and exams and build a tool for it.",
                    "label": 0
                },
                {
                    "sent": "Now we've got a lot of data of people having graded.",
                    "label": 0
                },
                {
                    "sent": "Now we build AI to learn from that data to great automatically, and you can get close to zero time spent on grading in the near future.",
                    "label": 0
                },
                {
                    "sent": "OK, reinforcement learning, let's see.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so Joe alright already introduced this formalism earlier today in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We have an agent and we want somehow to discover the right software for this agent, and this software will take actions as a consequence of the action, the environment, and maybe the robot or something else will change.",
                    "label": 0
                },
                {
                    "sent": "State reward will be admitted that indicates how good or bad the resulting situation is, and then the agent is faced with the consequences of the action and is to act in this new situation.",
                    "label": 0
                },
                {
                    "sent": "And this repeats.",
                    "label": 0
                },
                {
                    "sent": "And that's fundamentally what makes it different from.",
                    "label": 0
                },
                {
                    "sent": "Let's say supervised learning where you have a one off thing.",
                    "label": 0
                },
                {
                    "sent": "You make a decision and then things reset and you get a new one off thing and it repeats here where you see next depends on what you did before.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The topic within reinforcement that we're going to cover in this lecture is policy optimization impulse optimization.",
                    "label": 0
                },
                {
                    "sent": "The goal is to directly find a policy Pi Theta that, given the current state, emits a distribution over possible actions.",
                    "label": 0
                },
                {
                    "sent": "And then sample from that distribution, and hopefully that distribution is a good distribution that leads to good performance.",
                    "label": 0
                },
                {
                    "sent": "Underneath in everything I'll cover here we can pretty much assume that it's going to be a deep neural net representing the policy, but in principle it could be any other function that's parameterized with some vector Theta, and as you change the entries in Theta, you'll change the policy.",
                    "label": 0
                },
                {
                    "sent": "The process that goes from current state to action being taken.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, formally, what it looks like, then we're trying to solve, is it trying to solve an optimization problem where we maximize expected sum of rewards, accumulated overtime expected?",
                    "label": 0
                },
                {
                    "sent": "Because, well, the environment might be stochastic, so it might not be deterministic.",
                    "label": 0
                },
                {
                    "sent": "What we encounter and 2nd off and the policy will be stochastic.",
                    "label": 0
                },
                {
                    "sent": "It is the case that the optimal policy in NDPS tends to be deterministic.",
                    "label": 0
                },
                {
                    "sent": "But for learning it can be convenient to consider a broader class of policy that is also considered stochastic policies, because that actually tends to smooth out the optimization surface.",
                    "label": 0
                },
                {
                    "sent": "So we'll be optimizing over stochastic policies and try to find one that performs well on expectation, even though it converges.",
                    "label": 0
                },
                {
                    "sent": "Maybe it'll end up being deterministic.",
                    "label": 0
                },
                {
                    "sent": "To make this a little specific for robotics, for example, the reward could be the quality of a meal that a robot prepares, so robots busy cooking in a kitchen after half an hour, robot comes out gives you a meal.",
                    "label": 0
                },
                {
                    "sent": "There's been zero word all the way throughout.",
                    "label": 0
                },
                {
                    "sent": "But then when the meal comes out, you eat it.",
                    "label": 0
                },
                {
                    "sent": "You say how good it is?",
                    "label": 0
                },
                {
                    "sent": "Maybe it read it from one to five stars and the robot will get a reward from one through 5.",
                    "label": 0
                },
                {
                    "sent": "As you can see how this can be really hard problem because the robots busy for half an hour.",
                    "label": 0
                },
                {
                    "sent": "And only gets one piece of feedback at the end of the whole session about how good the cooking was, and so to actually learn from that is really difficult.",
                    "label": 0
                },
                {
                    "sent": "Might have to do this again.",
                    "label": 0
                },
                {
                    "sent": "Maybe it gets a different rating.",
                    "label": 0
                },
                {
                    "sent": "I can tease apart what was different between the different sessions and from that somehow understand what it should do more of what it should do less often.",
                    "label": 0
                },
                {
                    "sent": "That's exactly what policy optimization will try to do.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now politicians are the only way to tackle the problem solving methods too.",
                    "label": 0
                },
                {
                    "sent": "The other methods are important too.",
                    "label": 0
                },
                {
                    "sent": "We're focusing policy limitation here, so let me give you a little picture of why Paula Civilization might be the method of choice in your practical situations.",
                    "label": 0
                },
                {
                    "sent": "Often the policy is simply to represent them, the Q function or the value function.",
                    "label": 0
                },
                {
                    "sent": "Imagine a robot wanted to grasp an object to try to find the policy we trying to find is a path for the gripper to go around the object and close.",
                    "label": 0
                },
                {
                    "sent": "He's trying to find a value function is trying to learn to predict exactly how much expected reward you have as a function of the state that you're in, and that's a much harder problem to precisely solve, and so it might be that just learning the policy is simpler.",
                    "label": 0
                },
                {
                    "sent": "Now learning the value function.",
                    "label": 0
                },
                {
                    "sent": "If you learn the value function, you actually don't yet know how to take actions.",
                    "label": 0
                },
                {
                    "sent": "Once you have a value function to then decide how to act, you need to have a dynamics model that allows you to look ahead to one step.",
                    "label": 0
                },
                {
                    "sent": "Look ahead, say OK.",
                    "label": 0
                },
                {
                    "sent": "If my value function for all the actions I have currently available to me, what would happen if we take that action, then what?",
                    "label": 0
                },
                {
                    "sent": "What do I get for the transition an what's the value at the next state?",
                    "label": 0
                },
                {
                    "sent": "And so you need to actually learn two things to be able to act.",
                    "label": 0
                },
                {
                    "sent": "You can resolve this by learning AQ function instead.",
                    "label": 0
                },
                {
                    "sent": "That's even more complicated.",
                    "label": 0
                },
                {
                    "sent": "Learning value function in many ways, because now you need to learn a function that.",
                    "label": 0
                },
                {
                    "sent": "Understands how much reward you're going to get as a function of current state in action, which can be a pretty complicated function to learn even for simple task were often the policy can be much easier to represent.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some success stories of policy optimization that might help motivate why you want to do it.",
                    "label": 0
                },
                {
                    "sent": "The top row, our success is from a little while ago and they don't use any deep learning underneath.",
                    "label": 0
                },
                {
                    "sent": "They are similar policies underneath, but still parameterized policies where the policy optimization reinforcement algorithm allowed to fine tune parameters in a way that humans couldn't find.",
                    "label": 0
                },
                {
                    "sent": "Tune them by hand and as a consequence could get better performance than hand tuning of these systems.",
                    "label": 0
                },
                {
                    "sent": "So you see, at the top is.",
                    "label": 0
                },
                {
                    "sent": "From UT Austin, the robot dogs that play Robo Cup soccer used reinforcement learning to learn to walk or run faster than the other robots that they're playing against and hence be able to outcompete them in soccer.",
                    "label": 0
                },
                {
                    "sent": "Then next one will see in a little more detail.",
                    "label": 0
                },
                {
                    "sent": "Later is inverted helicopter flight by Entering's Group at Stanford.",
                    "label": 0
                },
                {
                    "sent": "I was part of that team then next one is two legged locomotion Rust Hendricks, PhD thesis at MIT.",
                    "label": 0
                },
                {
                    "sent": "This is a biped.",
                    "label": 0
                },
                {
                    "sent": "Walker learn to walk from scratch.",
                    "label": 0
                },
                {
                    "sent": "With reinforcement learning in the real world, and then the last one is one that will also see a little bit about later.",
                    "label": 0
                },
                {
                    "sent": "It's a task where you have a Cup and a little string, and then a bowl attached to the end of the string is supposed to swing up the ball and catch it in the Cup.",
                    "label": 0
                },
                {
                    "sent": "The bottom row is some more recent success stories where there are deep neural Nets underneath what's being learned.",
                    "label": 0
                },
                {
                    "sent": "The policy isn't network that will map for most of them, not all of them, though from raw pixels to round motor commands on the left you see some Atari games as well as Labyrinth which 3D navigation environment that you navigate with these solutions from first person vision, then control of two dimensional robots in Majokko Control 3 dimensional robots.",
                    "label": 0
                },
                {
                    "sent": "Image Coco control of actual robots and then Alpha Go has a few stars here because there's a lot of pieces to Alpha go and only one piece of it is policy optimization.",
                    "label": 0
                },
                {
                    "sent": "But it is also in there as part of how you restrict the branching factor, so your policy can help you reduce the number of things to consider as you search in the game tree through plausible futures that you might encounter.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the overall landscape of reinforcement learning policy optimization on the left.",
                    "label": 0
                },
                {
                    "sent": "Here dynamic program on the right dynamic program will see a lot more of later today.",
                    "label": 0
                },
                {
                    "sent": "Sure, Rich suddenly in the next session will cover a lot of dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "Dynamic programming relies on a self consistency equation that the value at the current time is equal to reward.",
                    "label": 0
                },
                {
                    "sent": "You get in the first transition plus value at the next time.",
                    "label": 0
                },
                {
                    "sent": "Policy implementation directly optimizes the objective that you care about, which is expected.",
                    "label": 0
                },
                {
                    "sent": "Expected reward and of course then with actor critic methods you kind of get in the middle between the two and this session will cover these three.",
                    "label": 0
                },
                {
                    "sent": "So we'll cover the remaining free optimization will cover policy gradients, and then we'll see how we can use value functions inside policy gradients.",
                    "label": 0
                },
                {
                    "sent": "So that we get actor critic methods, which gets us pretty close to the value function methods.",
                    "label": 0
                },
                {
                    "sent": "It's not that the other ones aren't important, it's just only so much we can cover in one lecture.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of outline for this lecture will look at.",
                    "label": 0
                },
                {
                    "sent": "Model based methods first.",
                    "label": 0
                },
                {
                    "sent": "It's often not what people think of when they say policy optimization.",
                    "label": 0
                },
                {
                    "sent": "Methods, methods, and policy grade methods, but we'll see them first.",
                    "label": 0
                },
                {
                    "sent": "An admittedly, people have had a hard time getting these to work, as well as the other ones, but there's also strong belief that model based methods in the long run could be more sample efficient an in a session like this is good to know what's out there rather than just the things that have worked the best of all things, and then we'll look at model free methods.",
                    "label": 0
                },
                {
                    "sent": "And at the end will do something that kind of brings them both together through the stochastic computation graphs framework.",
                    "label": 0
                },
                {
                    "sent": "OK, starting with model based where you see here is also split into three types of derivatives that will study, will study path derivatives will study derivative free methods and then will study likely ratio gradients.",
                    "label": 0
                },
                {
                    "sent": "Three different ways of computing the derivative.",
                    "label": 0
                },
                {
                    "sent": "If you have a derivative, you know which direction to step to improve your policy.",
                    "label": 0
                },
                {
                    "sent": "Different sets of assumptions will start out with some pretty strong assumptions.",
                    "label": 0
                },
                {
                    "sent": "Here will assume that F is evident Dynamics model that describes next date given current state and action.",
                    "label": 0
                },
                {
                    "sent": "For now, we'll assume it's known.",
                    "label": 0
                },
                {
                    "sent": "Differentiable will assume the reward function is known.",
                    "label": 0
                },
                {
                    "sent": "Differentiable will assume that the policy is known, but that's not really an assumption 'cause we're designing the policy ourselves.",
                    "label": 0
                },
                {
                    "sent": "We always know the policy and then also assume that it's a differentiable policy.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's assume we have an MVP with only two transitions that happen so very short horizon just for making it concrete and fitting it on the slide.",
                    "label": 0
                },
                {
                    "sent": "Top row what do we have stayed at?",
                    "label": 0
                },
                {
                    "sent": "Time Zero Dynamics model an action we take as prescribed by the policy, potentially stochastic policy, then results in next state then policy size.",
                    "label": 0
                },
                {
                    "sent": "What to do next?",
                    "label": 0
                },
                {
                    "sent": "State time 2.",
                    "label": 0
                },
                {
                    "sent": "And then we might have another transition and this would continue.",
                    "label": 0
                },
                {
                    "sent": "But we could also stop at two time steps and then we have rewards at the bottom that decide how much we got.",
                    "label": 0
                },
                {
                    "sent": "How good this current trajectory was.",
                    "label": 0
                },
                {
                    "sent": "But we try to optimize a sum of all rewards.",
                    "label": 0
                },
                {
                    "sent": "The first one here doesn't matter too much, because we can't really influence it because we start in whatever state we started.",
                    "label": 0
                },
                {
                    "sent": "But I'm just showing it for symmetry of the figure.",
                    "label": 0
                },
                {
                    "sent": "So the notation reward function R Maps from state to reward policy Pi Theta Maps from state to action.",
                    "label": 0
                },
                {
                    "sent": "For now let's assume deterministic and then the dynamics model Maps from state and action to the next state.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then what we'd like to do is optimize expected sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "So we'll call you have data utility of a current policy Pi, Theta, and so we're trying to maximize you have data.",
                    "label": 0
                },
                {
                    "sent": "So in this case, what that means is maximizing a sum of R0R 1R2.",
                    "label": 0
                },
                {
                    "sent": "Now we can actually compute aggrandisement along this graph.",
                    "label": 0
                },
                {
                    "sent": "Think of this me a lot of you have seen a whole week of deep neural Nets last week.",
                    "label": 0
                },
                {
                    "sent": "Think of the thing on the left has just a neural net.",
                    "label": 0
                },
                {
                    "sent": "That is kind of special structure.",
                    "label": 0
                },
                {
                    "sent": "It's a neural net with three layers with some connections within the layer.",
                    "label": 0
                },
                {
                    "sent": "But it's very much like a neural net, and so in principle you could run backpropagation through this to find the derivative of utility, which is a sum of the bottom things, which would be some of the bottom things would be a loss function.",
                    "label": 0
                },
                {
                    "sent": "Make it as high as possible.",
                    "label": 0
                },
                {
                    "sent": "In this case you can just take derivatives through this, so you'd say, well, I care bout derivative of utility, respect to all the parameter parameters, Theta, which are the weights on the policy connection.",
                    "label": 0
                },
                {
                    "sent": "Then what is this?",
                    "label": 0
                },
                {
                    "sent": "It has derivatives of reward.",
                    "label": 0
                },
                {
                    "sent": "Expect each of the rewards respect to state the dirt of state respect to the policy parameters.",
                    "label": 0
                },
                {
                    "sent": "We can expand the last one.",
                    "label": 0
                },
                {
                    "sent": "This way the state depends on the policy parameters through how you ended up in that state, which is dependent on what state you were in at the previous time, which it also depends on the policy parameters and depends on the dynamics model directly.",
                    "label": 0
                },
                {
                    "sent": "So this should be a you hear them all directly depending on the controls you actually took and then how to control action depends on the parameter vector Theta or in this case just entry Theta I.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "The third of the control action respect to the parameter Theta depends on the policy, but then also depends on what state you were in, and just so there is another recursion here with respect to state and the state respect to the policy parameters.",
                    "label": 0
                },
                {
                    "sent": "That's just backpropagation spelled out in a recursive way, and so you could either do that by hand, or you could just feed this computation graph that you see at the top left feed into an automatic differentiation package where you feed in F. You feed in R, you feed in a parameterized policy Pi Theta, which is the variables that you're trying to optimize over, and then just ask it for a derivative, and you're good to go.",
                    "label": 0
                },
                {
                    "sent": "So to get derivatives policy gradients, we just do a rollout switch is executing the policy, then do back prop which gives a gradient an.",
                    "label": 0
                },
                {
                    "sent": "Maybe we do multiple rollouts and multiple initial states to get a lower variance in our gradient and we can do updates so that gives us the simplest version of a policy gradient algorithm for domestic dynamics domestic policy.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sticker step further.",
                    "label": 0
                },
                {
                    "sent": "What if we have a stochastic dynamics model so next date is function of Kirsten Action plus some noise.",
                    "label": 0
                },
                {
                    "sent": "Actually, you can simply consider these noise variables constant once the rollout has happened.",
                    "label": 0
                },
                {
                    "sent": "So once the rollout has happened, you just freeze the noise and you're back to the terministic model 'cause the noise is froze is nothing stochastic about it anymore, and you can still apply back propagation just like you could do before.",
                    "label": 0
                },
                {
                    "sent": "Of course, you gotta realize that you might have to do multiple rollouts to see multiple instantiations of these noise variables and then compute multiple gradients and average those gradients to get a lower variance estimate.",
                    "label": 0
                },
                {
                    "sent": "But mathematically speaking, you just do one, roll out an get estimate of the gradient in exactly the same way.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So more generally, if you had a stochastic dynamics model, next day depends to Castle and current state in action you can re parameterized this by saying it's going to be a domestic function that depends on current state and action as well as some noise variable WT.",
                    "label": 0
                },
                {
                    "sent": "And the function encodes the stochasticity by depending on that noise variable.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case of a Gaussian distribution, maybe the next day is a Gaussian distribution as a function of G of ST&UT, you can just split it out as we did on the previous slide, but it doesn't have to be a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Anything where you have a continuous noise variable, you can do this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then what you have is a compilation graph that looks like this.",
                    "label": 0
                },
                {
                    "sent": "You have you roll out as you rollout happens, your noise happens.",
                    "label": 0
                },
                {
                    "sent": "You can determine what that noise is, put it in there, and then do a back propagation to the graph in exactly the same way.",
                    "label": 0
                },
                {
                    "sent": "The same is true when you're Paul.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "She becomes stochastic.",
                    "label": 0
                },
                {
                    "sent": "You can when you execute your policy see what the noise variables are that you sampled, insert them into the graph, freeze them for your back propagation and same for the reward function.",
                    "label": 0
                },
                {
                    "sent": "Even though often the right functions domestic.",
                    "label": 0
                },
                {
                    "sent": "If in some situations you have a stochastic reward function, you can do the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at this point, we're able to deal with stochastic reward dynamics, an policy, and just use a factorization pass to get the gradient out.",
                    "label": 0
                },
                {
                    "sent": "Of course, it still assumes that we know the dynamics model, and we know the reward function.",
                    "label": 0
                },
                {
                    "sent": "We just have to fill in the noise to be able to do this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at a full algorithm so we know how to compute gradients.",
                    "label": 0
                },
                {
                    "sent": "Welcome.",
                    "label": 0
                },
                {
                    "sent": "Now get is complete policy gradient algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can iterate going from 1, two over iterations.",
                    "label": 0
                },
                {
                    "sent": "Then we do multiple rollouts.",
                    "label": 0
                },
                {
                    "sent": "In each iteration, during the rollout, means sampling at initial state and sampling all the noise that will experience noise in the dynamics noise in the policy execution and noise in the reward function faintings deterministic.",
                    "label": 0
                },
                {
                    "sent": "You just skip the noise sampling for that.",
                    "label": 0
                },
                {
                    "sent": "Based on that, you can do your 4th pass.",
                    "label": 0
                },
                {
                    "sent": "Then you can do a backward pass average older grand estimates and take a step in the direction or do something fancier.",
                    "label": 0
                },
                {
                    "sent": "That's a higher order to optimize this.",
                    "label": 0
                },
                {
                    "sent": "If you have a.",
                    "label": 0
                },
                {
                    "sent": "Real-world system you might not have access to the noise yourself.",
                    "label": 0
                },
                {
                    "sent": "The real world will sample the noise for you, at least for the dynamics and possibly for their reward function.",
                    "label": 0
                },
                {
                    "sent": "Then you just back solve for it as you have had an experience.",
                    "label": 0
                },
                {
                    "sent": "He said well S T + 1 should be equal to F of SD, UTI plus some noise.",
                    "label": 0
                },
                {
                    "sent": "I've seen S T + 1 SD and UTI.",
                    "label": 0
                },
                {
                    "sent": "I can back solve for the noise and that's just the environment provides it and you solve for it.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "One reason to average it is to get a lower variance estimate.",
                    "label": 0
                },
                {
                    "sent": "An alternative would be just take one grand estimate, take a step and repeat.",
                    "label": 0
                },
                {
                    "sent": "Either way can work.",
                    "label": 0
                },
                {
                    "sent": "That's kind of up to you how you want to implement this.",
                    "label": 0
                },
                {
                    "sent": "If you don't average it, you might have to take smaller steps.",
                    "label": 0
                },
                {
                    "sent": "Might still be advantages.",
                    "label": 0
                },
                {
                    "sent": "It could be that your average is because it's easier to simulate many things in parallel and the same rate as you could simulate one thing, and then you'd rather average it rather than doing one at a time.",
                    "label": 0
                },
                {
                    "sent": "But both can work.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Good question.",
                    "label": 0
                },
                {
                    "sent": "So is this for continuous States and actions for the differentiability assumption to be true, we need continuous States and actions, otherwise you won't be able to differentiate through dysfunctions.",
                    "label": 0
                },
                {
                    "sent": "Ann will later see what to do when we don't have that assumption, but for now, that's where we're assuming, correct?",
                    "label": 0
                },
                {
                    "sent": "Yes?",
                    "label": 0
                },
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "I don't think anybody has a real answer for that yet in that.",
                    "label": 0
                },
                {
                    "sent": "It's still a little bit of a challenge to get model based or help to work well when you.",
                    "label": 0
                },
                {
                    "sent": "Typical so typically this coming next on the slide here.",
                    "label": 0
                },
                {
                    "sent": "Typically you would not have a known dynamics model F and you would have to estimated from the data that you're collecting.",
                    "label": 0
                },
                {
                    "sent": "So you have an unknown dynamics model F, which you're learning as well as learning the policy at the same time.",
                    "label": 0
                },
                {
                    "sent": "An there's a lot being learned at the same time, and people still have a hard time getting this all to work.",
                    "label": 0
                },
                {
                    "sent": "Reliably and when you're trying to get something to work, it's often easier to work with larger batches 'cause you know if you take an infinitely large batch, you know what the behavior is going to be, whereas you take very small batches or only one roll out.",
                    "label": 0
                },
                {
                    "sent": "It's harder to understand the behavior in hard to have invariants that you know are guaranteed to be true, but I do think in the long run people will figure out ways that you go just one roll out at a time update and go again.",
                    "label": 0
                },
                {
                    "sent": "In fact, I suspect people will do things, and people have tried some things like that where you actually.",
                    "label": 0
                },
                {
                    "sent": "During the rollout you would update your Dynamics Model F as you're rolling out.",
                    "label": 0
                },
                {
                    "sent": "Read re optimize your policy during the rollout based on the latest update of F An adjust your policy during a single rollout, even to be even more sample efficient.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "So the full model based algorithm would be.",
                    "label": 0
                },
                {
                    "sent": "You do what's shown on the slide as well as in every iteration you estimate the dynamics Model F from the data.",
                    "label": 0
                },
                {
                    "sent": "So it's been hard to get these things to work, but.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a few successes, for example, the SVG algorithms from Nicholas Hayes and collaborators at DeepMind have shown quite some success with this model based oral approach.",
                    "label": 0
                },
                {
                    "sent": "So let's expand this to a full horizon here, so the full computation graph I dropped the noise parameters.",
                    "label": 0
                },
                {
                    "sent": "The noise in the dynamics noise in the policy noise in reward function just to keep it a little more self contained.",
                    "label": 0
                },
                {
                    "sent": "US will play around with this quite a bit, but this is the computation graph that will be considering will and what happens is that will compute a gradient starting from every time step T, so it won't just do one gradient path starting from time 0, but we'll see from every time step.",
                    "label": 0
                },
                {
                    "sent": "What is the gradient that we can compute relative to the policy parameters at that time based on the entire future?",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll have many copies of this one for each time step T. SVG Infinity does what I just described.",
                    "label": 0
                },
                {
                    "sent": "You layout the accommodation graph from each time step and then do the back prop.",
                    "label": 0
                },
                {
                    "sent": "Based on what I just said back solving for the noise backdrop through it and get a gradient.",
                    "label": 0
                },
                {
                    "sent": "They do it for each time then add it together from all times and that's your great investment.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you can have other variants of this issue with this SVG Infinity is that it can be high variance if the horizon is long.",
                    "label": 0
                },
                {
                    "sent": "There can be a lot of stochasticity in the process, and the estimate that you get could be high variance estimate and there are ways to reduce the variance.",
                    "label": 0
                },
                {
                    "sent": "One way to do this is introducing discounting, so you could have discounted rewards for the future rather than actual rewards.",
                    "label": 0
                },
                {
                    "sent": "That's a pretty natural thing to do.",
                    "label": 0
                },
                {
                    "sent": "You can do more, you can just shorten the horizon.",
                    "label": 0
                },
                {
                    "sent": "You can say I'm only going to look case steps ahead.",
                    "label": 0
                },
                {
                    "sent": "And then to account for what happens after K steps ahead, you introduce a value function.",
                    "label": 0
                },
                {
                    "sent": "We haven't covered in this lecture how we're going to estimate value function, But let's assume you can estimate the value function.",
                    "label": 0
                },
                {
                    "sent": "You can have a value function that is at the very last node, so over here instead of R we have V V-55.",
                    "label": 0
                },
                {
                    "sent": "Because we're parameterising the value function and we're going to have to estimate these parameters 5 and then once you put your value function here, you can do the exact same thing as if it was a reward.",
                    "label": 0
                },
                {
                    "sent": "Still optimizing some of all of these.",
                    "label": 0
                },
                {
                    "sent": "Maybe this cannot sum of all of these.",
                    "label": 0
                },
                {
                    "sent": "That's our computation graph.",
                    "label": 0
                },
                {
                    "sent": "Backpropagation and we get it.",
                    "label": 0
                },
                {
                    "sent": "Policy gradient out.",
                    "label": 0
                },
                {
                    "sent": "And we do it again for all time steps.",
                    "label": 0
                },
                {
                    "sent": "T and more extreme scenario.",
                    "label": 0
                },
                {
                    "sent": "SG-1.",
                    "label": 0
                },
                {
                    "sent": "We stop after one time step going to look one time step ahead, cap it off with the value function there backward through this small combination graph but do it for all time slices.",
                    "label": 0
                },
                {
                    "sent": "T added altogether.",
                    "label": 0
                },
                {
                    "sent": "Get your policy gradient.",
                    "label": 0
                },
                {
                    "sent": "Even more extreme.",
                    "label": 0
                },
                {
                    "sent": "You could not even have the dynamics modeling this anymore.",
                    "label": 0
                },
                {
                    "sent": "You could just say I have a current state current action results in a Q value.",
                    "label": 0
                },
                {
                    "sent": "That's your computation graph back propagate through that which is very short back propagation and you get a gradient of your policy this way.",
                    "label": 0
                },
                {
                    "sent": "Now you estimated Q function Q5 that's FG0 or DG depending on the exact details of how you collect data and represents some things.",
                    "label": 0
                },
                {
                    "sent": "And the last one you actually don't need to learn a dynamics model anymore, so some advantage there that you don't need to.",
                    "label": 0
                },
                {
                    "sent": "Learning dynamics model.",
                    "label": 0
                },
                {
                    "sent": "Of course, by learning it, maybe you internalize something about the environment that's useful and you can learn more efficiently.",
                    "label": 0
                },
                {
                    "sent": "But if you have trouble learning at dynamic, small, don't want to do it, SVG zero avoids you having to learn a dynamics model.",
                    "label": 0
                },
                {
                    "sent": "Let's",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make this concrete so you can do this for all the versions that I showed on the previous slide symbol.",
                    "label": 0
                },
                {
                    "sent": "OK, good question.",
                    "label": 0
                },
                {
                    "sent": "What does it stand for?",
                    "label": 0
                },
                {
                    "sent": "DPG stands for deep deterministic policy gradient.",
                    "label": 1
                },
                {
                    "sent": "And SVG I should spacing out on what it stands for.",
                    "label": 1
                },
                {
                    "sent": "Stochastic value gradient.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you, so SVG is stochastic value gradient.",
                    "label": 0
                },
                {
                    "sent": "So let's make this into a full algorithm.",
                    "label": 0
                },
                {
                    "sent": "I picked SG-1 as a SVG 0 as a SVG 0.",
                    "label": 1
                },
                {
                    "sent": "Yeah, I think this is easier.",
                    "label": 0
                },
                {
                    "sent": "As a concrete example, but you can do the same thing for the other ones.",
                    "label": 0
                },
                {
                    "sent": "Actually I skipped one, then let's go back to.",
                    "label": 0
                },
                {
                    "sent": "SG-1.",
                    "label": 0
                },
                {
                    "sent": "I picked this one as the first concrete example.",
                    "label": 0
                },
                {
                    "sent": "SG-1 this is the computation graph.",
                    "label": 0
                },
                {
                    "sent": "This is what's associated with that, and then on the right we see the full algorithm.",
                    "label": 0
                },
                {
                    "sent": "We do rollouts.",
                    "label": 0
                },
                {
                    "sent": "For each roll out, what happens is a forward pass.",
                    "label": 0
                },
                {
                    "sent": "We collect the data from that forward pass.",
                    "label": 0
                },
                {
                    "sent": "We store any noise encountered during the 4th pass, either buyback solving if it's in the real world for what the noise was, or by just knowing what the noises were simulating, and certainly the noise ourselves.",
                    "label": 0
                },
                {
                    "sent": "Then we have three things to do an you can choose when you do each one of them.",
                    "label": 0
                },
                {
                    "sent": "There's a bit of choice there, but you have to learn three things.",
                    "label": 0
                },
                {
                    "sent": "You have to learn a policy, a value function, a dynamics model.",
                    "label": 0
                },
                {
                    "sent": "And so maybe you first policy update.",
                    "label": 0
                },
                {
                    "sent": "We don't have to a natural thing could be to 1st Dynamics model update on it, then solve for the noise, then do a policy update and do a value update.",
                    "label": 0
                },
                {
                    "sent": "The policy update is requires the noise in the dynamics and then the back propagation pass, which in this case is equation shown over here.",
                    "label": 0
                },
                {
                    "sent": "That gives you a great investment for the policy.",
                    "label": 0
                },
                {
                    "sent": "Your value function estimate could be something as simple as you have a premises value function which would least queries have fit to a estimated value which is estimated based on one step.",
                    "label": 0
                },
                {
                    "sent": "Look ahead, reward your experience plus gamma times value at the next state based on the value function you currently have typically.",
                    "label": 0
                },
                {
                    "sent": "This is considered a constant once you get it out of there, and only this Phi is differentiated through when taking the gradient here, and this is considered constant computed based on what is the previous iterations Phi.",
                    "label": 0
                },
                {
                    "sent": "And then we need a gradient for the dynamics model.",
                    "label": 0
                },
                {
                    "sent": "Incoming update D dynamics model.",
                    "label": 0
                },
                {
                    "sent": "So that's a full argument.",
                    "label": 0
                },
                {
                    "sent": "You can implement an run POS agreements with.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, can you go back to yeah, is it actually necessary to solve for WT there?",
                    "label": 1
                },
                {
                    "sent": "Because then the the bullet point below that you're just going to use on F of SU&W, which I guess.",
                    "label": 1
                },
                {
                    "sent": "Would be the just as T + 1.",
                    "label": 0
                },
                {
                    "sent": "So yes, good point.",
                    "label": 0
                },
                {
                    "sent": "So let me clarify this.",
                    "label": 0
                },
                {
                    "sent": "So what happens here?",
                    "label": 1
                },
                {
                    "sent": "When I say F of STUTWT.",
                    "label": 0
                },
                {
                    "sent": "F is a function that we have available 'cause we're learning it.",
                    "label": 0
                },
                {
                    "sent": "This function F will depend both on South U&W and you want to make sure to evaluate your F at the noise that you experienced in the real world or in your rollout.",
                    "label": 0
                },
                {
                    "sent": "You don't want to assume zero noise here.",
                    "label": 0
                },
                {
                    "sent": "You could, but it would not be as precise a gradient as the average over multiple rollouts.",
                    "label": 0
                },
                {
                    "sent": "Then if you use the actual noise that you experienced, so this thing here is actually differentiated through.",
                    "label": 0
                },
                {
                    "sent": "So do we actually have two dynamics models?",
                    "label": 0
                },
                {
                    "sent": "'cause there's one on the line above that is just a function venue, and then there's just one.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so notation wise.",
                    "label": 0
                },
                {
                    "sent": "What yeah OK, so for this dynamics model here, the way this could be written as you have STUT plus WT that would just be a plus WT living here if you had a more complicated dynamics model where the noise comes in a more complicated way they have to solve in a different way for WT and then inserted in here correct?",
                    "label": 1
                },
                {
                    "sent": "Yeah?",
                    "label": 0
                },
                {
                    "sent": "So think of this as just a plus WT.",
                    "label": 1
                },
                {
                    "sent": "And you do want to keep the plus WT you don't want to lose it.",
                    "label": 0
                },
                {
                    "sent": "This this on the second line here is just going to evaluate S T + 1, so we don't.",
                    "label": 0
                },
                {
                    "sent": "I don't see WT being used anywhere else, so the reason it will not evaluate the S T + 1 is because we actually substituted this thing over here.",
                    "label": 0
                },
                {
                    "sent": "The policy Pi Pi Theta.",
                    "label": 0
                },
                {
                    "sent": "Will go into here for UTI's.",
                    "label": 0
                },
                {
                    "sent": "Rash is differentiating through this whole thing.",
                    "label": 0
                },
                {
                    "sent": "And but it has the same value, but it doesn't have the same same value but not same that not the same gradient necessarily.",
                    "label": 0
                },
                {
                    "sent": "It depends on how WT plays into it.",
                    "label": 0
                },
                {
                    "sent": "If it plays into it, the simplest way I believe it actually will have the same gradient even more complicated dependence on WT.",
                    "label": 0
                },
                {
                    "sent": "It's not guaranteed that the gradient is the same.",
                    "label": 1
                },
                {
                    "sent": "Thank you, thanks for pointing that out.",
                    "label": 1
                },
                {
                    "sent": "FG0.",
                    "label": 0
                },
                {
                    "sent": "We don't need to learn a dynamic smaller 'cause the competition graph doesn't have the dynamics model in it.",
                    "label": 0
                },
                {
                    "sent": "We just have a Q function there.",
                    "label": 0
                },
                {
                    "sent": "So we have a backdrop to compute the gradient of the policy and then a Q function update which could be just like with the value function we had before.",
                    "label": 0
                },
                {
                    "sent": "Just some kind of squared error minimization where this estimate here is usually frozen to be whatever the previous parameter vector predicted for it to be fixed.",
                    "label": 0
                },
                {
                    "sent": "Here, derivative here only with respect to the first Phi appearance.",
                    "label": 0
                },
                {
                    "sent": "The first term.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have.",
                    "label": 0
                },
                {
                    "sent": "A few policy and organs that we've seen or whole family of them is actually been used to solve some pretty interesting tasks.",
                    "label": 0
                },
                {
                    "sent": "2 dimensional robotic control in majokko the different versions seem to learn about equally fast.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you want to turn this into domestic policy gradients.",
                    "label": 0
                },
                {
                    "sent": "And for it to work well, you need to do something little extra.",
                    "label": 0
                },
                {
                    "sent": "If you naively train SVG zero, you end up very quickly with a deterministic policy that optimizes the Q values.",
                    "label": 0
                },
                {
                    "sent": "'cause when you look at the policy update, quit trying to find the action that maximizes Q and that will naturally make it deterministic.",
                    "label": 0
                },
                {
                    "sent": "So you need to do something to keep it stochastic.",
                    "label": 0
                },
                {
                    "sent": "So add noise to the policy.",
                    "label": 0
                },
                {
                    "sent": "Explicitly force your policy be stochastic when you collect data.",
                    "label": 0
                },
                {
                    "sent": "Then your Q values learned our learned off policy, but that's fine with Q values.",
                    "label": 0
                },
                {
                    "sent": "You can learn them off policy if you do TD0.",
                    "label": 0
                },
                {
                    "sent": "So with TD0 you can learn your values off policy and have a stochastic policy collected data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you get this to work stably with a deep neural net.",
                    "label": 0
                },
                {
                    "sent": "You need to play a few extra tricks, which were proposed in JDG paper by Tim Lillicrap and collaborators.",
                    "label": 0
                },
                {
                    "sent": "For learning the Q function, you just don't.",
                    "label": 0
                },
                {
                    "sent": "Don't just take the last rollouts, you actually keep around the replay buffer against the replay buffer, do updates on your Q function, then when you do this, when you compute your target Q values as reward plus next Q value under the current policy.",
                    "label": 0
                },
                {
                    "sent": "These primes here 5 prime and Theta prime.",
                    "label": 0
                },
                {
                    "sent": "What they mean is that they are not the current setting of your policy parameters in Q function parameters.",
                    "label": 0
                },
                {
                    "sent": "Instead, there the Polyak averaged, which is exponentially average past weights.",
                    "label": 0
                },
                {
                    "sent": "That you've encountered during learning so far, and so these average parameters evolve much more slowly than the parameters of your current policy and current queue function and will stabilize your target values, which allow your argument to be more stable and hopefully converge.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this actually has been applied to some more complicated robotic simulation tasks and including vision based control on driving.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Functions have a have a meeting or is it like is it function correctly in itself?",
                    "label": 0
                },
                {
                    "sent": "So I'm not an expert on the meaning of of what?",
                    "label": 0
                },
                {
                    "sent": "Is associated with it, but a crude meaning that I've heard people associate with it is it's like keeping track of the posterior over possible values that your neural net should take on, and it's keeping track of some mode of a posterior based on everything you've seen in the past.",
                    "label": 0
                },
                {
                    "sent": "In practice, what it means is that you can take larger step sizes and more more aggressive step sizes and at the same time still have a stable representation.",
                    "label": 0
                },
                {
                    "sent": "They keep around that is the result of your learning.",
                    "label": 0
                },
                {
                    "sent": "This effectively kind of slow.",
                    "label": 0
                },
                {
                    "sent": "That average version is slower moving than the thing that's actually moving ahead, but that way you can explore, you can have more aggressive exploration, more aggressive optimization in the landscape of that you're working in while keeping around something that's quite stable.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Play for the policy updates or just value book chapters.",
                    "label": 0
                },
                {
                    "sent": "I believe just for the Q value updates, but I'm not 100% sure.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's our first type of gradients which you can use inside model based reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Or if you do the SG RDP version, even within model free settings.",
                    "label": 0
                },
                {
                    "sent": "But assuming a differentiable policy and or dynamics model.",
                    "label": 0
                },
                {
                    "sent": "If you have discrete actions, what I just described to you is actually not going to workout.",
                    "label": 0
                },
                {
                    "sent": "If you don't have the ability to learn a dynamics model, if discrete state space is also not going to workout as describe, let's look at.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some other methods that are applicable there.",
                    "label": 0
                },
                {
                    "sent": "Simplest way to compute branches just to do finite differences, right?",
                    "label": 0
                },
                {
                    "sent": "If you have an optimal optimization objective utility which depends on your policy parameter vectors, Theta, just perturb the policy parameter vector Theta one coordinate at a time, positive perturbation negative perturbation.",
                    "label": 0
                },
                {
                    "sent": "See what the differences divide by two epsilon and there you have your derivative.",
                    "label": 0
                },
                {
                    "sent": "Estimate some trickiness there.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If your function is stochastic, so at any point along the X axis which is here, optimizing a function of X anywhere along the way when you ask for a while value distribution over Y values, then you need to be careful because you might end up with samples that are somehow inconsistent in some way.",
                    "label": 0
                },
                {
                    "sent": "You sample.",
                    "label": 0
                },
                {
                    "sent": "Here you get this point, you sample here you get this point, and now derivative looks the exact opposite way down what you want, so you need to be a little careful about this.",
                    "label": 0
                },
                {
                    "sent": "If you have control over the noise that's present.",
                    "label": 0
                },
                {
                    "sent": "Well, if you don't control you just average over many samples.",
                    "label": 0
                },
                {
                    "sent": "If you do have control, you can make sure that the noise present in the sampling is the same for the plus and a negative epsilon perturbation.",
                    "label": 0
                },
                {
                    "sent": "And now we can get out a reliable derivative estimate.",
                    "label": 0
                },
                {
                    "sent": "So fixed.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Random seed can help to get those derivatives can actually also help.",
                    "label": 0
                },
                {
                    "sent": "In general whenever you're doing policy optimization.",
                    "label": 0
                },
                {
                    "sent": "This way when the dynamics model is being executed, if you control, the noise is consistent.",
                    "label": 0
                },
                {
                    "sent": "That's the same as intuitively something like Oh my helicopters trying to fly in pretty windy conditions, but when I compare performance of two policies actually compare those policies under the same windy conditions, not each time.",
                    "label": 0
                },
                {
                    "sent": "Randomly sample windy conditions and see which one does better.",
                    "label": 0
                },
                {
                    "sent": "This one might have just been luckier.",
                    "label": 0
                },
                {
                    "sent": "With the wind conditions than the other one.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this kind of approach, just finite perturbations find a different derivatives.",
                    "label": 0
                },
                {
                    "sent": "Actually, the helicopter project at Stanford was able to learn to control this helicopter here.",
                    "label": 0
                },
                {
                    "sent": "So you see, here is a helicopter.",
                    "label": 0
                },
                {
                    "sent": "Being stable control is actually very hard control problem, and in fact being stable, controlled to fly inverted.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What happened here is that.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To make this work.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Policy classes handcrafted with careful consideration of how to four controls that you have for a helicopter depends on the state.",
                    "label": 0
                },
                {
                    "sent": "Resulting in only 12 parameters, I need to be optimized over and we have only a very small number of parameters.",
                    "label": 0
                },
                {
                    "sent": "Actually, this simple methods can be made to work and very successfully so.",
                    "label": 0
                },
                {
                    "sent": "To do this kind of delivery method, if you want to optimize over policy with 100,000 parameters might be a lot harder.",
                    "label": 0
                },
                {
                    "sent": "Might take a lot longer, but if you do have a lot of prior knowledge then this simple methods can actually work quite well.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there are other gradient free methods beyond just find out difference perturbation, something called cross entropy method that covariance matrix adaptation and variance.",
                    "label": 0
                },
                {
                    "sent": "There also stick a look at those.",
                    "label": 0
                },
                {
                    "sent": "Cross entropy method is still optimizing the same objective.",
                    "label": 0
                },
                {
                    "sent": "Will view you the utility as a black box.",
                    "label": 0
                },
                {
                    "sent": "So in everything we've seen in the first half of the lecture, we've taken derivatives through the dynamics and so forth understood that there was a temporal process happening here, we're going to ignore that.",
                    "label": 0
                },
                {
                    "sent": "There's a temporal process.",
                    "label": 0
                },
                {
                    "sent": "Just say we have a policy, see what happens just like in the finite differences.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that means we're probably not going to be optimal in terms of sample efficiency, but we do have something very simple to work with.",
                    "label": 0
                },
                {
                    "sent": "And so cross entropy is an example of revolutionary method an.",
                    "label": 0
                },
                {
                    "sent": "What characterizes those methods that you keep around the population of whatever you're optimizing over.",
                    "label": 0
                },
                {
                    "sent": "In this case of population of policies.",
                    "label": 0
                },
                {
                    "sent": "So we don't have just one parameter vector Theta with a distribution over parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "For example a Gaussian distribution with a mean mu and maybe a fixed covariance.",
                    "label": 0
                },
                {
                    "sent": "Or maybe learn covariance.",
                    "label": 0
                },
                {
                    "sent": "In this case, let's assume a fixed covariance, so we don't have to explicitly talk about it, and we have a mean mu I, which is that for our current population, our current.",
                    "label": 0
                },
                {
                    "sent": "Additional policies neways the mean and if we want to put in that population and sample from a Gaussian with that mean and that gives me a parameter vector Theta CM, then iterates.",
                    "label": 0
                },
                {
                    "sent": "Samples population members, which is samples policies from the Gaussian distribution over policies.",
                    "label": 0
                },
                {
                    "sent": "Once it's sampled, a policy, it executes robots.",
                    "label": 0
                },
                {
                    "sent": "Under that policy it stores what parameter vector is sampled, as well as the utility achieved under that policy.",
                    "label": 0
                },
                {
                    "sent": "Once it's collected those and now checks.",
                    "label": 0
                },
                {
                    "sent": "Some subset of the policies that it sampled, let's say the top 10% and then computes anew mean for the Gaussian which maximizes the log probability of the policies sampled that were in the top 10%.",
                    "label": 0
                },
                {
                    "sent": "So you're shifting your Gaussian mean to where the top 10% was, and then repeat again and again and again.",
                    "label": 0
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seizure can actually work embarrassingly well, so for example, for the game of Tetris, which for a long time was a benchmark and reinforcement was used quite a bit.",
                    "label": 0
                },
                {
                    "sent": "The numbers were such that cross entropy was actually outperforming other reinforcement methods by quite a bit for a long time, and it took till.",
                    "label": 0
                },
                {
                    "sent": "And it's 2013 paper.",
                    "label": 0
                },
                {
                    "sent": "For a dynamic parameter method to finally perform as well as cross entropy method in the game of Tetris.",
                    "label": 0
                },
                {
                    "sent": "Even the dynamic program methods use a lot more information about the process than this.",
                    "label": 0
                },
                {
                    "sent": "Cost cross entropy method.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a whole family of these, so I'm collecting these on the slide.",
                    "label": 0
                },
                {
                    "sent": "The intent is not for understanding each one of these just right now when we showed him too, but later we look back at the slides.",
                    "label": 0
                },
                {
                    "sent": "You can see the relation close relationship reward.",
                    "label": 0
                },
                {
                    "sent": "Weighted regression says instead of taking the top let's say 10% for each policy that was sampled, we take some function of the utility achieved and the probability of sampling that particular policy put that in front of the log probability and maximize this cross entropy would be a special case where you make that function.",
                    "label": 0
                },
                {
                    "sent": "One minor in the top 10% zero when you're not in the top 10%.",
                    "label": 0
                },
                {
                    "sent": "But there are other functions you can take prescribed by this paper over here.",
                    "label": 0
                },
                {
                    "sent": "Path integral method.",
                    "label": 0
                },
                {
                    "sent": "Replaces the top ten percent 10 scoring by exponential weighting of utility.",
                    "label": 0
                },
                {
                    "sent": "Achieve so it's a soft version of cross entropy where the best ones get waited more and then if you're not as good you get weighted less in the finding of the new mean.",
                    "label": 0
                },
                {
                    "sent": "You can also find the covariance, not just the mean of that Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then you get a CMA method so and then you can also do play some tricks with how you weight each one of them as a function of their utility.",
                    "label": 0
                },
                {
                    "sent": "And then in the power method there is actually some additional M like ideas and important sampling ideas that extract a little bit of 2nd order information in the update, which might make it more efficient.",
                    "label": 0
                },
                {
                    "sent": "So all of these effectively look at maximizing log probability of.",
                    "label": 0
                },
                {
                    "sent": "Parameter vectors of policy that you sampled before based on how well these parameter vectors ended up performing in their rollouts.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some example success stories often used in graphics for animation of humanoid fly characters.",
                    "label": 0
                },
                {
                    "sent": "We see here on the right is learn to control for the ball in a Cup game.",
                    "label": 0
                },
                {
                    "sent": "It started with a demonstration of policy was learned to match the demonstration.",
                    "label": 0
                },
                {
                    "sent": "That was not a good policy.",
                    "label": 0
                },
                {
                    "sent": "We're seeing that policy being executed initially, then the power method, which is a cross entropy method is being run to optimize the policy.",
                    "label": 0
                },
                {
                    "sent": "And in just about 100 iterations it's able to learn to swing up the ball catches in the Cup.",
                    "label": 0
                },
                {
                    "sent": "This particular case it's working based on a motion capture system that tracks the location of the ball, so it's working in state space and knows location of the ball knows the state of the robot and then based on the state information provides feedback control to swing up and catch the ball.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main caveat, the main advantage of this man is super simple.",
                    "label": 0
                },
                {
                    "sent": "Always nice to introduce to do as your first Test when you try something in reinforced learning.",
                    "label": 0
                },
                {
                    "sent": "Just run cross entropy like method or just cross entropy itself.",
                    "label": 0
                },
                {
                    "sent": "See how it works, gives you a measure of how feasible it might be to solve the problem, not super sample efficient.",
                    "label": 0
                },
                {
                    "sent": "That's the main caveat.",
                    "label": 0
                },
                {
                    "sent": "Especially, we have a high dimensional space that you're optimizing over.",
                    "label": 0
                },
                {
                    "sent": "It could be.",
                    "label": 0
                },
                {
                    "sent": "Sample inefficient, but the flip side of all of that is.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But maybe these days you don't care as much about sample efficiency.",
                    "label": 0
                },
                {
                    "sent": "If you're working in simulation.",
                    "label": 0
                },
                {
                    "sent": "If you can parallelize over enough machines.",
                    "label": 0
                },
                {
                    "sent": "This method is extremely easy to parallelize and scales extremely well with parallel with parallelization, where you see here is a graph showing as a function of the number of cores.",
                    "label": 0
                },
                {
                    "sent": "The median time to solve in this case, I believe, is a 3D humanoid problem where you see here is a linear curve, which means that as you introduce more course, it literally speeds of the method by a factor of how many more cores introduced into double number.",
                    "label": 0
                },
                {
                    "sent": "Of course you go twice as fast finding a solution, and so that's really nice.",
                    "label": 0
                },
                {
                    "sent": "So actually you want to find a solution as quickly as possible, even though these methods are not the most sample efficient, they might actually be the fastest if you have a large compute budget, yes.",
                    "label": 0
                },
                {
                    "sent": "So will you only looking at the results of the last iteration when you're trying to take the top 10%?",
                    "label": 0
                },
                {
                    "sent": "Or do you look at all of the history?",
                    "label": 0
                },
                {
                    "sent": "Everything laceration?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "OK, so in these methods it's actually extremely simple.",
                    "label": 0
                },
                {
                    "sent": "What happens you have your current distribution over policies you sample from that you execute each one of them.",
                    "label": 0
                },
                {
                    "sent": "Maybe one episode for each one of them might be a few episodes if you're in a very stochastic environment to get a better measure of how good they are, and then you just you don't do any optimization on those executions, you just execute them as is.",
                    "label": 0
                },
                {
                    "sent": "So there is no the first execution is not any different from the last one at that point.",
                    "label": 0
                },
                {
                    "sent": "What I'm asking is like when you when you find the top 10% like because you're going to be iterating this right.",
                    "label": 0
                },
                {
                    "sent": "Yes, you look at the whole history and everything OK. From last version.",
                    "label": 0
                },
                {
                    "sent": "Typical simple implementation just looks at the ones from the last iteration.",
                    "label": 0
                },
                {
                    "sent": "Now, if in the last iteration you ended up being somehow confused 'cause something weird happen, then you found the spot where things look good and you're concentrated all there and things all of a sudden are pretty bad.",
                    "label": 0
                },
                {
                    "sent": "Then you want to exactly think about what you're thinking about this.",
                    "label": 0
                },
                {
                    "sent": "Well, should I really reset from scratch, or should I actually go back to previous iterations, see where things were pretty good?",
                    "label": 0
                },
                {
                    "sent": "An restart from there?",
                    "label": 0
                },
                {
                    "sent": "Nothing.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of interesting questions about how to what you might want to keep around.",
                    "label": 0
                },
                {
                    "sent": "From past iterations, I haven't seen anything people might have done it.",
                    "label": 0
                },
                {
                    "sent": "Where to keep around more?",
                    "label": 0
                },
                {
                    "sent": "You do a lot of rollouts here, so probably not going to keep around everything, but it might be that there's a scheme where you keep around most salient past executions to them more effectively update in the future.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "We assume that.",
                    "label": 0
                },
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "But normally no more questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah, you're right.",
                    "label": 0
                },
                {
                    "sent": "So the assumption and what I described is that the posterior keep around is a Gaussian over parameter vectors.",
                    "label": 0
                },
                {
                    "sent": "It's very simple assumption.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily the right fit for most situations, it's quite effective and simple to go with.",
                    "label": 0
                },
                {
                    "sent": "It would be interesting to try other distributions that might better fit where your posterior looks like and re sample from those simple things could be let's say a mixture of Gaussians which is also very simple to compute them to sample from.",
                    "label": 0
                },
                {
                    "sent": "And investigate how that does, maybe even more complicated distributions.",
                    "label": 0
                },
                {
                    "sent": "The results have shown to you are all just keeping a simple Gaussian distribution around them.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you used one of the latest methods in inference for representing distributions, maybe use a personal auto encoder to represent your distribution.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you use one of the auto regressive models to represent the distribution.",
                    "label": 0
                },
                {
                    "sent": "There's a tradeoff, because there would be some compute involved in keeping that distribution around.",
                    "label": 0
                },
                {
                    "sent": "Then if you're doing everything in simulation, then ultimately compute is what you're constrained by.",
                    "label": 0
                },
                {
                    "sent": "If you're doing things in the real world, then maybe more complicated distributions, even if they require a lot of compute, are easier to justify, because, well, you want to reduce real world rollouts, and you can do it by doing more compute.",
                    "label": 0
                },
                {
                    "sent": "Yeah, be very interesting to play around with.",
                    "label": 0
                },
                {
                    "sent": "I don't know how well it would work, but there is probably a tradeoff point there where being a little smarter about the distribution while not expecting too much compute somehow somehow will achieve the best performance.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is.",
                    "label": 0
                },
                {
                    "sent": "They're environment constantly.",
                    "label": 0
                },
                {
                    "sent": "Changes are driven experiments where you vary the environment.",
                    "label": 0
                },
                {
                    "sent": "End times.",
                    "label": 0
                },
                {
                    "sent": "Why are you running cross entropy method?",
                    "label": 0
                },
                {
                    "sent": "So if the environment is 2, is the environment can change one way it changes that you get dropped in a new environment, but there's a distribution over environments, in which case you can think of that randomness and which you might not have to do.",
                    "label": 0
                },
                {
                    "sent": "Multiple rollouts per policy to get a good estimate of how good that policy parameter vector is, so that would be a pretty good fit then.",
                    "label": 0
                },
                {
                    "sent": "Another way the environment could change is that your environment changes during the rollout, so there is.",
                    "label": 0
                },
                {
                    "sent": "Change overtime.",
                    "label": 0
                },
                {
                    "sent": "In that case, if you had a simple feedforward neural net policy, let's say that you're parameterising, then it would not be a good fit for your environment, and so if you want to apply this approach, you want to do it the way I just described, you would have to pick a recurrent neural net.",
                    "label": 0
                },
                {
                    "sent": "Recurrent neural net would then somehow store information about how his environment would change it with somehow track how the environment is changing and so then what you would learn as you would learn the weights of the recurrent neural net.",
                    "label": 0
                },
                {
                    "sent": "But in its activations it would be able to keep track of changes to the environment and so then you would train a.",
                    "label": 0
                },
                {
                    "sent": "Policy that is good at adapting to a changing environment which is not a lot.",
                    "label": 0
                },
                {
                    "sent": "Not a lot of people are testing on this, but it is in the real world.",
                    "label": 0
                },
                {
                    "sent": "It is what it would be like.",
                    "label": 0
                },
                {
                    "sent": "You need something that's good at adapting to a changing environment, not just a good policy for stationary environment.",
                    "label": 0
                },
                {
                    "sent": "Because the big Pro is fast as you do that you end up with the same time.",
                    "label": 0
                },
                {
                    "sent": "So that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One reason it's fast is also because you don't have to do a back propagation pass.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's nice because that means you don't have to store any information here forward pass, and so if you had a recurrent neural net.",
                    "label": 0
                },
                {
                    "sent": "Policy you would have to store a lot of information you forward pass and then do a backward pass and so for kernel nuts.",
                    "label": 0
                },
                {
                    "sent": "It would also be extra fast in some sense that you don't have to do a backward pass, so there might still be benefits in that regard, the parallelization.",
                    "label": 0
                },
                {
                    "sent": "One thing I forgot to mention, the reason the position works out so well is that.",
                    "label": 0
                },
                {
                    "sent": "All you need in terms of communication between nodes is communicating the random seed of the perturbation.",
                    "label": 0
                },
                {
                    "sent": "So even if you have a, let's say million dimensional.",
                    "label": 0
                },
                {
                    "sent": "Parameter vector.",
                    "label": 0
                },
                {
                    "sent": "If I'm collecting the data from all the other workers, all I need to know is what random seed did they use and what was the average performance under the policy they have.",
                    "label": 0
                },
                {
                    "sent": "'cause if I know the average performance which is a scalar, another random seed which is a scalar that can locally be construct, what their perturbation was, and I can do the cross entropy update locally.",
                    "label": 0
                },
                {
                    "sent": "So communication is essentially.",
                    "label": 0
                },
                {
                    "sent": "Well, not zero, but it's extremely close to 0 and that's why you see the linear scaling, and that would still be true if you had a recurrent neural net policy is still would only need to know the random seed that was used for the perturbation at each of the worker nodes to understand what updates to make locally.",
                    "label": 0
                },
                {
                    "sent": "I think the real question in some sense, what you're asking about is.",
                    "label": 0
                },
                {
                    "sent": "What does it mean to be acting in an environment that continuously changes?",
                    "label": 0
                },
                {
                    "sent": "And I think it's a question a lot of people are very interested in, but we don't have really good benchmarks for makes it harder to work on.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "Somehow you need a setting.",
                    "label": 0
                },
                {
                    "sent": "You need settings where things are continually changing on you so you can test those ideas, and I think right now we don't have a good set of.",
                    "label": 0
                },
                {
                    "sent": "Environments in which to test them, even though we know that for the real world, clearly it is what we need.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to look at for the last 1/3 at what are the classical policy grading methods?",
                    "label": 0
                },
                {
                    "sent": "So we're going to see now is if somebody tells you policy gradients.",
                    "label": 0
                },
                {
                    "sent": "And don't say anything more.",
                    "label": 0
                },
                {
                    "sent": "They'll likely refer to what we're going to cover now, even though we've already seen two types of policy gradient methods, namely the backwardation ones, namely path derivatives and we've seen the kind of cross entropy evolutionary style updates.",
                    "label": 0
                },
                {
                    "sent": "Compared to evolutionary strategies will have the same set of assumptions.",
                    "label": 0
                },
                {
                    "sent": "That is, no assumptions on the dynamics model, no assumptions on the reward function, we just need to be able to execute things and know how much reward was collected.",
                    "label": 0
                },
                {
                    "sent": "We can work with.",
                    "label": 0
                },
                {
                    "sent": "Discrete state action spaces, but we're going to want to do now is look at the details of what happens during the rollout to make the method more sample efficient than evolutionary method.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Will change notation a little bit temporarily to reduce clutter and will introduce something called Tao.",
                    "label": 0
                },
                {
                    "sent": "Tao stands for an entire trajectory.",
                    "label": 0
                },
                {
                    "sent": "OK, so utility under policy Pi Theta is expected sum of rewards and will write it as some overall possible trajectories Tau probability of that trajectory towel under the current policy parameter vector Theta times the reward accumulated along that trajectory Tau.",
                    "label": 0
                },
                {
                    "sent": "Same problem as we had before, just changing notation to make it more compact and so our goal is still to find the policy parameter Theta that maximizes expected some of her words.",
                    "label": 0
                },
                {
                    "sent": "Now written this way.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The same as the lobby with me.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so trajectory rollout are the same thing.",
                    "label": 0
                },
                {
                    "sent": "So Tao stands for S0U0F1 U One South, two U2.",
                    "label": 0
                },
                {
                    "sent": "The entire sequence of States and actions.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we'd like to do is computed gradient of this utility, you Theta.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Nabla Theta some overall trajectory's of the quantity we just looked at.",
                    "label": 0
                },
                {
                    "sent": "An to derive this will place some tricks that only in hindsight makes sense, but it's a very short derivation.",
                    "label": 0
                },
                {
                    "sent": "I think it's worth being aware of this trick and understanding how this is derived.",
                    "label": 0
                },
                {
                    "sent": "So we bring in the gradient into the submission.",
                    "label": 0
                },
                {
                    "sent": "That can pretty much always be done when you have a submission.",
                    "label": 0
                },
                {
                    "sent": "It can also be an integral.",
                    "label": 0
                },
                {
                    "sent": "Can always almost always be done too.",
                    "label": 0
                },
                {
                    "sent": "Then now we want to somehow compute that gradient.",
                    "label": 0
                },
                {
                    "sent": "We know that the distribution depends on our parameter Theta, but we're actually going to play a little trick first.",
                    "label": 0
                },
                {
                    "sent": "When multiplying divide by probability of trajectories under Theta.",
                    "label": 0
                },
                {
                    "sent": "Then we.",
                    "label": 0
                },
                {
                    "sent": "Move the denominator off to the right.",
                    "label": 0
                },
                {
                    "sent": "Then we used identity that the derivative of the log of X is the same as DX over X.",
                    "label": 0
                },
                {
                    "sent": "So we replace it with.",
                    "label": 0
                },
                {
                    "sent": "Gradient of log probability of trajectory under Theta.",
                    "label": 0
                },
                {
                    "sent": "So this thing here.",
                    "label": 0
                },
                {
                    "sent": "It's just.",
                    "label": 0
                },
                {
                    "sent": "Applying derivative log in the opposite direction, then it's usually used.",
                    "label": 0
                },
                {
                    "sent": "At this point, we actually are in a very interesting situation.",
                    "label": 0
                },
                {
                    "sent": "There is this interesting is because we have an expectation.",
                    "label": 0
                },
                {
                    "sent": "Again, when you see there is an expected value of under distribution over trajectories of grad log probability of trajectory times reward.",
                    "label": 0
                },
                {
                    "sent": "We have an expectation we can approximate it from samples and so we can say, well, we're going to compute this grander approximated by this quantity over here, which is same thing as we have over here, but approximated from samples.",
                    "label": 0
                },
                {
                    "sent": "So that means that we can execute our policy.",
                    "label": 0
                },
                {
                    "sent": "And for the rollout structures that we obtain, we can average that quantity for each roll out to get a great investment.",
                    "label": 0
                },
                {
                    "sent": "Now we still have to compute that can't even told you how to compute the ground log.",
                    "label": 0
                },
                {
                    "sent": "Probably have a trajectory, but if we know how to compute that, then we can do this based on sample estimate.",
                    "label": 0
                },
                {
                    "sent": "You can drive this in a different way too, so here it's very much a hindsight type derivation where you want it to end up with an expectation.",
                    "label": 0
                },
                {
                    "sent": "So you multiply the probability divide by it, and then work the other thing in the back so you end up with an expectation so you can compute this from samples.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way to derive it is to look at important sampling.",
                    "label": 0
                },
                {
                    "sent": "What is important sampling?",
                    "label": 0
                },
                {
                    "sent": "It's a tool to sample from a distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, to generate sample some distribution that you can sample from directly.",
                    "label": 0
                },
                {
                    "sent": "So you want to sample from a distribution.",
                    "label": 0
                },
                {
                    "sent": "Let's say you want to sample from a distribution under parameter Theta, but all you can sample from a distribution under parameter vector Theta old.",
                    "label": 0
                },
                {
                    "sent": "And you want to still know what the expectation is under the new parameter vector, Theta?",
                    "label": 0
                },
                {
                    "sent": "What you can do is you can sample from third old and then re weight your samples by the ratio of.",
                    "label": 0
                },
                {
                    "sent": "This probability under Theta divided by probability under Theta old if you re weight your samples by this ratio you get a good estimate of the expected value under the distribution under Theta even though you sampled under Theta old.",
                    "label": 0
                },
                {
                    "sent": "So this tells you that in principle you could sample under current policy.",
                    "label": 0
                },
                {
                    "sent": "They are old.",
                    "label": 0
                },
                {
                    "sent": "An evaluate how good another policy is with new parameter vector Theta and then maybe optimize this quantity to find a better parameter vector Theta.",
                    "label": 0
                },
                {
                    "sent": "You can actually do this, but often it's not that great an estimate.",
                    "label": 0
                },
                {
                    "sent": "If you move far away.",
                    "label": 0
                },
                {
                    "sent": "So actually the 1st order approximation of this might be the most meaningful thing to use, so see what the 1st order approximation looks like.",
                    "label": 0
                },
                {
                    "sent": "First order, approximate this important sampling estimate.",
                    "label": 0
                },
                {
                    "sent": "Work through it.",
                    "label": 0
                },
                {
                    "sent": "This expectation over here take the gradient to see what it is.",
                    "label": 0
                },
                {
                    "sent": "Then we evaluate it at the current.",
                    "label": 0
                },
                {
                    "sent": "Theta, which is Theta old.",
                    "label": 0
                },
                {
                    "sent": "We get the same thing we had before and so we get the same quantity as we had on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So we see here is that from important sampling.",
                    "label": 0
                },
                {
                    "sent": "Seeing what the important sample that looks like what the derivative looks like in the important sample estimate, we end up with the exact same.",
                    "label": 0
                },
                {
                    "sent": "Checked it or exact same brand as we had on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So two ways of deriving the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very interesting what we have here is a great an estimate.",
                    "label": 0
                },
                {
                    "sent": "That works even if the reward function is discontinuous.",
                    "label": 0
                },
                {
                    "sent": "So if your rewards is 01 reward.",
                    "label": 0
                },
                {
                    "sent": "You can still use this.",
                    "label": 0
                },
                {
                    "sent": "'cause we actually don't take a derivative of the reward function even though we're trying to optimize expected reward.",
                    "label": 0
                },
                {
                    "sent": "We never take a derivative through it.",
                    "label": 0
                },
                {
                    "sent": "And a lot of problems are more easily specified with 01 reward function, success, failure and so for those problems this can actually work.",
                    "label": 0
                },
                {
                    "sent": "Just really nice.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, what?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Happening with these kind of gradient estimates is that.",
                    "label": 0
                },
                {
                    "sent": "What you're doing is you increase if you take a step in the interaction, you increase the probability of paths that have high reward and decrease the probability of path to have negative reward.",
                    "label": 0
                },
                {
                    "sent": "Or if their older roots are always positive, you increase a lot the probability of paths that have high reward and increase a little bit the path of the probability of passing have low reward.",
                    "label": 0
                },
                {
                    "sent": "Now I can't increase all probabilities, of course is to sum to 1 S. This renormalization happening, and so we've all rewards are positive, will still end up being that the ones that don't achieve as high reward only will see their probability drop to be able to increase the probability of the ones with higher reward.",
                    "label": 0
                },
                {
                    "sent": "So it's happening here.",
                    "label": 0
                },
                {
                    "sent": "If you look at three rollouts.",
                    "label": 0
                },
                {
                    "sent": "If you have three rollouts, this update which shift probability mass today rather has better reward is very different from the past derivatives we saw before with the path derivatives from the very first stretch, those derivatives will look at those trajectories and look at how should I change my parameter vector to locally perturbed my trajectory to get a better trajectory.",
                    "label": 0
                },
                {
                    "sent": "And then essentially update the path for each of the rollouts rather than shifting probability mass from the not so good ones to the better ones.",
                    "label": 0
                },
                {
                    "sent": "It's a very different way of optimizing the beauty of this one is that you actually don't need derivatives through the reward function.",
                    "label": 0
                },
                {
                    "sent": "In fact, will soon see you don't have any derivatives through the dynamics model to compute the grad log probability of path under parameter vector.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look at that.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does this gridlock probability of path under parameter vector?",
                    "label": 0
                },
                {
                    "sent": "Well, the probability of a path is the problem of initial state, which I left out here.",
                    "label": 0
                },
                {
                    "sent": "Then times probability of next state given current state and action and probability of action given state multiplied.",
                    "label": 0
                },
                {
                    "sent": "Overall times this is the policy we have access to that this is the dynamics model.",
                    "label": 0
                },
                {
                    "sent": "We might not have access to that, but look there's no Theta here and that's going to help us a lot.",
                    "label": 0
                },
                {
                    "sent": "So if you work this out grad log for product is grad.",
                    "label": 0
                },
                {
                    "sent": "Of the sum of the logs then.",
                    "label": 0
                },
                {
                    "sent": "Since there is no Theta in here, the grad respect to Theta just disappears and we end up with just grad Theta.",
                    "label": 0
                },
                {
                    "sent": "Some log probabilities of actions given state.",
                    "label": 0
                },
                {
                    "sent": "So we see here that we can compute the derivative without having access to a dynamics model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we can do now is we can do rollouts.",
                    "label": 0
                },
                {
                    "sent": "For each of the rollouts, we can compute the ground log probability of path under the parameter Victor.",
                    "label": 0
                },
                {
                    "sent": "This way, multiply with reward along that path, and that gives the contribution of that path to the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spell this out more fully.",
                    "label": 0
                },
                {
                    "sent": "This is how we're going to make gradients.",
                    "label": 0
                },
                {
                    "sent": "And this quantity.",
                    "label": 0
                },
                {
                    "sent": "Over here we're going to estimate this way.",
                    "label": 0
                },
                {
                    "sent": "And an expectation this gives us the right grade.",
                    "label": 0
                },
                {
                    "sent": "You can do infinitely many rollouts.",
                    "label": 0
                },
                {
                    "sent": "This will be the correct gradient if you do.",
                    "label": 0
                },
                {
                    "sent": "Finally many it will be an estimate of the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As formulated so far, there are some issues.",
                    "label": 0
                },
                {
                    "sent": "It's a very high high variance estimate, so I do need a ton of robots or you need to play some tricks to reduce the variance of this estimate so we look at some tricks to do that.",
                    "label": 0
                },
                {
                    "sent": "Baseline temple structure exploitation and then also look at what we need to do to make this stable in terms of step sizing slash trust regions.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Why's it high variance?",
                    "label": 0
                },
                {
                    "sent": "Intuitively the reason is high variance.",
                    "label": 0
                },
                {
                    "sent": "Imagine all your words are positive, then every path will try to increase its own probability.",
                    "label": 0
                },
                {
                    "sent": "When you when you roll out and you have a path.",
                    "label": 0
                },
                {
                    "sent": "This gradient says we should make this path more likely, even the not so good ones.",
                    "label": 0
                },
                {
                    "sent": "So that's not great.",
                    "label": 0
                },
                {
                    "sent": "That means that you have all these contributions fighting each other, all trying to increase their probability instead of the bad ones understanding their bad and trying to decrease their probability.",
                    "label": 0
                },
                {
                    "sent": "That's exactly we're going to exploit, but in the current equation right now that's not exploited.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When the roads are always positive, we try to increase proposal passed.",
                    "label": 0
                },
                {
                    "sent": "That's not great, so it really should do is introduce the notion of ranking or understanding what you're better than average or not.",
                    "label": 0
                },
                {
                    "sent": "That's called the baseline.",
                    "label": 0
                },
                {
                    "sent": "So the gradient becomes this over here with subtracting a baseline from their award along a path.",
                    "label": 0
                },
                {
                    "sent": "Now if you're better than average, or the baseline is the average you've got along all executions, then you increase your probability.",
                    "label": 0
                },
                {
                    "sent": "If you're worse than average, you decrease your probability.",
                    "label": 0
                },
                {
                    "sent": "OK, there's some math you can do to show that this will still give you a unbiased estimate of the gradient.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can even do some math to find a optimal that is minimal variance estimate, which in practice people don't use that much, but it is what it is.",
                    "label": 0
                },
                {
                    "sent": "You can principle use it simple thing to do is to just use the average of what you got among the roles that you did and just use that as your baseline.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another thing you can do is exploit temporal structure.",
                    "label": 0
                },
                {
                    "sent": "Right now what we have is.",
                    "label": 0
                },
                {
                    "sent": "This equation over here.",
                    "label": 0
                },
                {
                    "sent": "The grad log probably have a lot of temple structure, but the reward has temporal structure too.",
                    "label": 0
                },
                {
                    "sent": "An where you can expose the fact that Future Past rewards don't depend on current actions or otherwise phrased current action only influences future rewards.",
                    "label": 0
                },
                {
                    "sent": "But in the equation over here, current action probabilities or grad law, current action probabilities multiply with all rewards past or future.",
                    "label": 0
                },
                {
                    "sent": "We should get rid of the past and so then we get an estimate that looks like this where you only multiply with everything that's in the future here rather than also what's in the past.",
                    "label": 0
                },
                {
                    "sent": "A good choice for B would then be expected future return.",
                    "label": 0
                },
                {
                    "sent": "That's actually a value function.",
                    "label": 0
                },
                {
                    "sent": "You can use a value function if you can estimate one, or you can just use an average of future returns from that time over your multiple rollouts.",
                    "label": 0
                },
                {
                    "sent": "This gives us a full.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm, so there's a vanilla policy gradient algorithm.",
                    "label": 0
                },
                {
                    "sent": "You have some initial parameter vector Theta some initial baseline, maybe zero.",
                    "label": 0
                },
                {
                    "sent": "What you think of us average performance.",
                    "label": 0
                },
                {
                    "sent": "Then you iterate, you collect the set of trajectories Mexican current policy in each trajectory you compute the returns from that time onwards you compute what we call an advantage estimate, which is the reward you got from that time onwards, minus we get on average from that time or state onwards.",
                    "label": 0
                },
                {
                    "sent": "That's the thing we have.",
                    "label": 0
                },
                {
                    "sent": "Multiplied with the grad log probabilities.",
                    "label": 0
                },
                {
                    "sent": "You might also refit this average estimate based on your current rollouts, and then you use this and your grad log probably actually gives state times advantage how much you're better than average to do an update to the policy.",
                    "label": 0
                },
                {
                    "sent": "So the basic algorithm, if you draw enough samples, this will actually work pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what remains I want to give you a quick overview on extra things you can do to make this even more effective.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First thing is that you need to worry about step sizing.",
                    "label": 0
                },
                {
                    "sent": "Step sizing always matters because he stepped too far.",
                    "label": 0
                },
                {
                    "sent": "The grand is only local approximation step too far.",
                    "label": 0
                },
                {
                    "sent": "You actually don't improve on their objective.",
                    "label": 0
                },
                {
                    "sent": "You might do worse.",
                    "label": 0
                },
                {
                    "sent": "In reinforcing",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning, however, it's even a bigger issue than another optimization problems 'cause I'm reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "If you take a step that's too big.",
                    "label": 0
                },
                {
                    "sent": "The data you're going to collect under the new policy is going to be non interesting data.",
                    "label": 0
                },
                {
                    "sent": "It's not good data to compute grades from.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you have a policy where you sometimes get nonsi rewards.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you don't, but sometimes do get nonzero rewards.",
                    "label": 0
                },
                {
                    "sent": "You have some gradient signal.",
                    "label": 0
                },
                {
                    "sent": "Take a step that's too big.",
                    "label": 0
                },
                {
                    "sent": "You're back in the room where you never get any reward.",
                    "label": 0
                },
                {
                    "sent": "You get no signal.",
                    "label": 0
                },
                {
                    "sent": "So you want to be extra careful NRL about your step sizes in supervised learning, why?",
                    "label": 0
                },
                {
                    "sent": "Why is it not as big a concern?",
                    "label": 0
                },
                {
                    "sent": "Well, if you step too far the next update the data is sitting there for you, waiting for you to tell you to go back in the other direction.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a little less efficient, but the data will tell you what to do RL you're too far have a terrible policy.",
                    "label": 0
                },
                {
                    "sent": "You don't get good data, you don't get any signal, and now you actually don't get to optimize in a meaningful way.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple step sizing would be just to do a line search.",
                    "label": 1
                },
                {
                    "sent": "You have a great interaction along the great interaction.",
                    "label": 0
                },
                {
                    "sent": "You tried different step size, see how well the rollouts perform and then just take the best one and call it done.",
                    "label": 0
                },
                {
                    "sent": "It's all naive, but because it ignores more information you can exploit from your robots.",
                    "label": 0
                },
                {
                    "sent": "It turns out that just looks at 1st order information, but you can exploit a little more information to understand where your first approximation is.",
                    "label": 0
                },
                {
                    "sent": "Good words bad, and then within a region of where it's good, find the best spot.",
                    "label": 0
                },
                {
                    "sent": "So we're going to define a trust region is the region where you trust your first order approximation.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then once we have the trust region, will find the best point within the trust region and we repeat what would be a good trust region.",
                    "label": 0
                },
                {
                    "sent": "We could say, well, we want to stay in a region that's close in terms of in terms of policy parameter vector want resulting distribution after we make a change over directories to be close to the regional distribution.",
                    "label": 0
                },
                {
                    "sent": "So we want to measure in terms of distribution over trajectories, not measure in terms of Euclidean distance in policy vector space.",
                    "label": 0
                },
                {
                    "sent": "Now we need to.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Should the scale Divergance turns out we can expand the probability of paths, compute those Cal divergences, go little faster, but you can look at the slides more slowly.",
                    "label": 0
                },
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "It turns out it simplifies and that all that shows up ultimately is probabilities under your policy.",
                    "label": 0
                },
                {
                    "sent": "The Dynamics model cancels out again, so you can compute this trust region based on just having access to the policy.",
                    "label": 0
                },
                {
                    "sent": "That's nice, so we have kill Cal based on the policy which you can approximate by looking at along the rollouts.",
                    "label": 0
                },
                {
                    "sent": "At the state you visit, what is the KL divergent at those states?",
                    "label": 0
                },
                {
                    "sent": "Between the policy that you used to have in the policy have after your update.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with that we have a constraint optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We want to keep the scale small enough to be in our trust region while optimizing the 1st order approximation of our objective.",
                    "label": 0
                },
                {
                    "sent": "We can 2nd order approximate the scale to make this whole thing a little more efficient.",
                    "label": 0
                },
                {
                    "sent": "What you get is the Fisher information metric shows up and what you actually then get this something very similar to the natural gradient exit with a constraint rather than just a natural gradient step.",
                    "label": 0
                },
                {
                    "sent": "Question somewhere yes.",
                    "label": 0
                },
                {
                    "sent": "Sorry, say it again.",
                    "label": 0
                },
                {
                    "sent": "So G had here is the estimate of the gradient.",
                    "label": 0
                },
                {
                    "sent": "So we compute an estimate of the gradient based on the likelihood ratio policy gradient that's living there, and that's what that is.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Policy.",
                    "label": 0
                },
                {
                    "sent": "So why do we measure the calendar direction that we're measuring it?",
                    "label": 0
                },
                {
                    "sent": "It's the computationally easier one to measure is the main reason.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So why we use the device rather than a distance metric?",
                    "label": 0
                },
                {
                    "sent": "The math works out really nicely that I would say is the main justification.",
                    "label": 0
                },
                {
                    "sent": "I mean the killer version in general is seen as a pretty good measure of how far distributions are apart, but among the measures you can use for how far distributions are apart.",
                    "label": 0
                },
                {
                    "sent": "This is the one where the math works out very cleanly and is easy to compute with.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have a constraint optimization problem which can be solved quite easily, it turns out.",
                    "label": 0
                },
                {
                    "sent": "It is very similar to the natural policy gradient.",
                    "label": 0
                },
                {
                    "sent": "But by having a region rather than a stepsize, which is a natural policy gradient would, do you actually have a more stable algorithm?",
                    "label": 0
                },
                {
                    "sent": "Define the actual update.",
                    "label": 0
                },
                {
                    "sent": "Here you can form the Lagrangian and do dual descent on the Lagrangian LaGrange multiplier to find LaGrange multiplier that achieves the right epsilon and find the correct step that you want.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, are we done when you do a little bit of extra work here.",
                    "label": 0
                },
                {
                    "sent": "Turns out if Theta is high dimensional, inverting that Fisher information matrix can be expensive.",
                    "label": 0
                },
                {
                    "sent": "An not practical.",
                    "label": 0
                },
                {
                    "sent": "So there are some tricks you can play to make this practical which is detailed in the 2015 I smell paper by John Schulman and collaborators that can speed this up quite a bit.",
                    "label": 0
                },
                {
                    "sent": "Another thing you can do to make this even more efficient is instead of using jihad here, you can replace this by something else.",
                    "label": 0
                },
                {
                    "sent": "So instead of using the.",
                    "label": 0
                },
                {
                    "sent": "Actual policy gradient.",
                    "label": 0
                },
                {
                    "sent": "The 1st order approximation.",
                    "label": 0
                },
                {
                    "sent": "You can actually go back to the important sampling.",
                    "label": 0
                },
                {
                    "sent": "Best estimate that I showed you that allowed us to derive this first order approximation.",
                    "label": 0
                },
                {
                    "sent": "You can actually plug back in the important sampling element of the objective aclocal estimate, which will also be valid within this trust region.",
                    "label": 0
                },
                {
                    "sent": "If you make a trust region the right size and that way you can optimize something much closer to the original objective than just using the 1st order approximation.",
                    "label": 0
                },
                {
                    "sent": "If you look at the arlab TRP implementation, that's what it does.",
                    "label": 0
                },
                {
                    "sent": "As the optimization objective is the important sampling based estimate within the KL Trust region.",
                    "label": 0
                },
                {
                    "sent": "And we'll look a little more at this later, so here are some experiments that.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually did with this what we're going to see here is.",
                    "label": 0
                },
                {
                    "sent": "The final gates obtained with Trust region policy optimization, which what we just covered.",
                    "label": 0
                },
                {
                    "sent": "In this case, for 2D locomotion experiments in majokko.",
                    "label": 0
                },
                {
                    "sent": "The reward function is actually pretty similar.",
                    "label": 0
                },
                {
                    "sent": "Word function is just about forward progress and impact with the ground, so you don't need to encode anything about what walking looks like.",
                    "label": 0
                },
                {
                    "sent": "The way this works is that initially this robot will just fall over.",
                    "label": 0
                },
                {
                    "sent": "But overtime.",
                    "label": 0
                },
                {
                    "sent": "Two thanks to different random executions.",
                    "label": 0
                },
                {
                    "sent": "It figures out that some ways of falling over take longer to fall over that other sense or better puts more weight on those trajectories and then drifts towards finding a good gate that maximizes reward.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some comparison graphs with.",
                    "label": 0
                },
                {
                    "sent": "Related methods, but that don't have all the machinery that we just talked about and actually tends to learn quite a bit quicker.",
                    "label": 0
                },
                {
                    "sent": "Even on the simple problems, but especially.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the harder problems such as Hopper, Ann Walker and apply the exact same method to attack.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Organs actually you can use trusted policy optimization to find good policies on Atari original results.",
                    "label": 0
                },
                {
                    "sent": "Of course, work with Q learning DQM and it turns out that typically decline remains the more sample efficient method.",
                    "label": 0
                },
                {
                    "sent": "But in terms of Wall Clock time and ease of use, often the policy implementation methods are easier and the current state of yard is often achieved with something called a 3C, which will cover in a few slides an that might not be as sample efficient document, but is wall Clock time more efficient?",
                    "label": 0
                },
                {
                    "sent": "Then the one and very similar to what I just showed you.",
                    "label": 0
                },
                {
                    "sent": "So stay.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at that in the last couple of minutes we have here with 10 minutes for connecting with actor critic.",
                    "label": 0
                },
                {
                    "sent": "We can do even better than what I just described by bringing in value functions.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our policy grand estimates.",
                    "label": 0
                },
                {
                    "sent": "As Summer future awards minus a baseline and baseline, a natural baseline would be the value function.",
                    "label": 0
                },
                {
                    "sent": "If we have a value function 'cause that tells us from this state on average, how well are you going to do and then we can understand whether the random action you took was better or worse than average, and then you need to increase or decrease the probability of that action.",
                    "label": 0
                },
                {
                    "sent": "So how do we estimate the value of a state under current policy while there's the Bellman equation which tells you how the value of current state S is a function of policy probability of actions given state, then probably affecting the Christian action reward associated with the transition plus gamma times the value at the next state.",
                    "label": 0
                },
                {
                    "sent": "Self consistent set of equations that you can solve for the value of the policy.",
                    "label": 0
                },
                {
                    "sent": "In effect, you can fill in your current estimate on the right hand side and then compute from that the left hand side.",
                    "label": 0
                },
                {
                    "sent": "And repeat until this converges.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you have a large state space, you can't really do it by enumerating over all States and repeating this, so you need to approximate things.",
                    "label": 0
                },
                {
                    "sent": "You might have a big neural net that represents your value function with parameter vector 50.",
                    "label": 0
                },
                {
                    "sent": "Initially you collect your data on your data, you set up the objective above here based on samples, and so you want to move.",
                    "label": 0
                },
                {
                    "sent": "Want to find a new value function V5 that is correspond to their left hand side here and then it should match this thing over here, which is a sampling of the right hand side.",
                    "label": 0
                },
                {
                    "sent": "And then of course, you don't want to move too far from your current estimate, otherwise you might be overfitting to the last set of samples, and so this way you get a new estimate value function, and then you might repeat this until you have convergence.",
                    "label": 0
                },
                {
                    "sent": "We might just do 1 update and do a policy grand update again.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can then fill that in over here an estimate of the value function.",
                    "label": 0
                },
                {
                    "sent": "What's nice about this is that we have generalization.",
                    "label": 0
                },
                {
                    "sent": "Now across states that we're exploiting, rather than just a simple baseline.",
                    "label": 0
                },
                {
                    "sent": "Might just depend on average.",
                    "label": 0
                },
                {
                    "sent": "How much do you get?",
                    "label": 0
                },
                {
                    "sent": "We have another estimate that's high variance sum of rewards.",
                    "label": 1
                },
                {
                    "sent": "When you do a rollout again from the same state, likely different things will happen.",
                    "label": 0
                },
                {
                    "sent": "So clearly this is going to be a noisy estimate.",
                    "label": 0
                },
                {
                    "sent": "What it really is is an estimate of the Q value.",
                    "label": 0
                },
                {
                    "sent": "What you would like to have there is to know what the Q value is so that state in action and then use that compared with the value in.",
                    "label": 0
                },
                {
                    "sent": "That tells you how much better the action was.",
                    "label": 0
                },
                {
                    "sent": "Then other actions are worse than other actions.",
                    "label": 0
                },
                {
                    "sent": "That tells you how to update your policy.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's a reasonable estimate to just take some of rewards.",
                    "label": 0
                },
                {
                    "sent": "It's an unbiased estimate, but it's not a low variance estimate and you might need a lot of samples for it to be good.",
                    "label": 0
                },
                {
                    "sent": "It doesn't exploit any notion of generalization that we might be able to exploit from having seen other things.",
                    "label": 0
                },
                {
                    "sent": "So we can first thing we can do is reduce variance by discounting very easy thing to do, and then the next thing we can do is reduce variance by function approximation.",
                    "label": 1
                },
                {
                    "sent": "So maybe that in reality we care about actual sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "But we use discounting nevertheless to reduce variance.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Gamma here, then, becomes a hyperparameter that we optimize over that maybe initially is.",
                    "label": 0
                },
                {
                    "sent": "Far away from one, maybe 0.5 something very small so you don't look far ahead.",
                    "label": 0
                },
                {
                    "sent": "You have low variance, but then as you do more learning it might become higher.",
                    "label": 0
                },
                {
                    "sent": "You're able to look further ahead.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inducing function approximation where you can do is well, you can say just like we saw very early on you can see current reward plus value at the next time.",
                    "label": 0
                },
                {
                    "sent": "That could be what you're using as your Q estimate, or you could use value at the next next time or reward the road without the next next next time and so forth.",
                    "label": 0
                },
                {
                    "sent": "A 3C is policy grade method that follows the things I've described, minus the trust region.",
                    "label": 0
                },
                {
                    "sent": "But then use instead of sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "Uses some estimate with some value.",
                    "label": 0
                },
                {
                    "sent": "There may be 5 steps ahead and then the value function of 10 steps ahead.",
                    "label": 0
                },
                {
                    "sent": "Then the value function instead of just the sum of rewards.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also wait all those estimates and if you take an exponentially weighted average, it turns out that the way you calculate things, you do it cleverly.",
                    "label": 0
                },
                {
                    "sent": "It's pretty much as efficient as using just one of them, very related to TD Lambda, and that's what generalized advantage estimation uses.",
                    "label": 0
                },
                {
                    "sent": "And then you have a instead of choosing a horizon at which you cap it off, you choose the Lambda, which effectively says in a weighted way.",
                    "label": 0
                },
                {
                    "sent": "How far are you willing to look ahead in your rollouts.",
                    "label": 0
                },
                {
                    "sent": "And so that's essentially using Elegibility traces, which I imagine rich might talk about this afternoon.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do we have then?",
                    "label": 0
                },
                {
                    "sent": "We now have an actor critic algorithm.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn both a policy in a value function.",
                    "label": 0
                },
                {
                    "sent": "We're going to collect rollouts.",
                    "label": 0
                },
                {
                    "sent": "An estimates of the value from each state action.",
                    "label": 0
                },
                {
                    "sent": "The simplest estimate here is just sum of rewards experienced, but more complicated estimates will be the ACC estimate of some of the words followed by value function or the generalized advantage estimate, which is the exponentially weighted combination of rewards and value functions at all times.",
                    "label": 0
                },
                {
                    "sent": "Once you've collected those, you can do an update to the value function by saying, well, the value at that state should be brought closer to what this quantity is.",
                    "label": 0
                },
                {
                    "sent": "And of course some regularization and we can get a policy grand update on the policy based on the Advantage Testament over here.",
                    "label": 0
                },
                {
                    "sent": "Whatever you estimated as sum of rewards which might be using value functions in here minus the value of the state that you were in times grad liability action gives state so pretty easy to implement, and that's going to be a very effective policy gradient method.",
                    "label": 0
                },
                {
                    "sent": "There are many variations you can have here.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that for the targets here.",
                    "label": 0
                },
                {
                    "sent": "That instead of using.",
                    "label": 0
                },
                {
                    "sent": "Case step look ahead.",
                    "label": 0
                },
                {
                    "sent": "You only use one step.",
                    "label": 0
                },
                {
                    "sent": "Look ahead which would be TD0 matching.",
                    "label": 0
                },
                {
                    "sent": "You can use full roll out.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of tweaking.",
                    "label": 0
                },
                {
                    "sent": "You can do different choices of Lambda and gamma that you can use over there, and then you actually don't have to use the same over here as we use over here principle that could be different.",
                    "label": 0
                },
                {
                    "sent": "In fact, often you might do something where here when you're initially debugging you still just use a sum of rewards because the sum of rewards is unbiased.",
                    "label": 0
                },
                {
                    "sent": "And once you do value functions it becomes biased so often.",
                    "label": 0
                },
                {
                    "sent": "Initially you'll use some of her words over here.",
                    "label": 0
                },
                {
                    "sent": "Ann still something with a value function of TD style estimate over here and then once things are working well, maybe you start using value functions in this estimate over here too.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see async Advantage actor critic.",
                    "label": 0
                },
                {
                    "sent": "Actually this is on a bunch of Atari games, so you see here is that a 3C outperforms Q learning methods decurion style methods.",
                    "label": 0
                },
                {
                    "sent": "The paper from leaf about year ago experiments from here in 1/2 ago.",
                    "label": 0
                },
                {
                    "sent": "It shows that ATC outperforms the current methods in terms of training time.",
                    "label": 0
                },
                {
                    "sent": "A through C was all.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applied then she do a pretty complicated task where you see here is a learned policy that learn to map from first person vision.",
                    "label": 0
                },
                {
                    "sent": "Two actions and how to navigate.",
                    "label": 0
                },
                {
                    "sent": "Amazed to find the things that are high reward like apples and cherries and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this is to learn a vision system.",
                    "label": 0
                },
                {
                    "sent": "3D vision system.",
                    "label": 0
                },
                {
                    "sent": "Effectively an actions all in one big neural net and was possible with this approach.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also study the effect of the choices of Lambda and gamma.",
                    "label": 0
                },
                {
                    "sent": "It turns out that for Lambda here and then gamma here it's good to choose something that's not all the way at the extremes.",
                    "label": 0
                },
                {
                    "sent": "So Lambda 0.96 gamma 0.98 seems to be best here.",
                    "label": 0
                },
                {
                    "sent": "What that means is that there is a little bit of a tradeoff between how far you want to look ahead gamma.",
                    "label": 0
                },
                {
                    "sent": "0.98 means that your effective looking at about 5200 time steps and that that's in these environments a good look ahead while reducing variance by not looking too far ahead.",
                    "label": 0
                },
                {
                    "sent": "Lambda is determining how much you're waiting the.",
                    "label": 0
                },
                {
                    "sent": "The pure reward based estimate versus the value function contribution when you top things off with value functions and it shows that you actually want to mostly rely on the pure reward based estimate, which is the unbiased estimate.",
                    "label": 0
                },
                {
                    "sent": "But bringing a little bit of the value function to reduce variance.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this able to learn things like this, what we're seeing here is a humanoid robot in 3D.",
                    "label": 0
                },
                {
                    "sent": "Now learning to control itself.",
                    "label": 0
                },
                {
                    "sent": "The reward function here is just the further North, the better, the less impact with the ground the better.",
                    "label": 0
                },
                {
                    "sent": "And this is a neural net with about 100,000 parameters going from mapping from the Joint angles, joint velocities and center of mass coordinates and velocity to torques at each of the Motors.",
                    "label": 0
                },
                {
                    "sent": "So it's completely from raw sensory inputs, in this case at the at the joints to Ross rock controls at each of the Motors.",
                    "label": 0
                },
                {
                    "sent": "And we're of course with reinforcement is that it's not specific to the environment in which would deploy it.",
                    "label": 0
                },
                {
                    "sent": "Additionally, if you want to, you know to walk.",
                    "label": 0
                },
                {
                    "sent": "Maybe you would have spent a lot of time thinking about humanoids and how you should control them.",
                    "label": 0
                },
                {
                    "sent": "But with this kind of approach, you just.",
                    "label": 0
                },
                {
                    "sent": "Diploid, our algorithm on the robot of choice.",
                    "label": 0
                },
                {
                    "sent": "You switch your robot of choice and you can run the exact same code.",
                    "label": 0
                },
                {
                    "sent": "This is also the Majorca environment is a simulator build by a motor at University of Washington Ann.",
                    "label": 0
                },
                {
                    "sent": "This robot is now learn to control yourself with the same objective.",
                    "label": 0
                },
                {
                    "sent": "Same algorithm and control itself to run actually quite fast.",
                    "label": 0
                },
                {
                    "sent": "You can also put in different tasks.",
                    "label": 0
                },
                {
                    "sent": "It doesn't need to be running.",
                    "label": 0
                },
                {
                    "sent": "Here there what function is about how closed ahead is to standing head height.",
                    "label": 0
                },
                {
                    "sent": "Setting is better than standing, but it's even better to stand.",
                    "label": 0
                },
                {
                    "sent": "And what you see here is it invents standing up.",
                    "label": 0
                },
                {
                    "sent": "There's nothing in there that has told it about what you need to do to stand up.",
                    "label": 0
                },
                {
                    "sent": "It's just measuring distance to standing head as the reward function, and it learns against that.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what do we have left we've seen here is actually different ways of computing derivatives here.",
                    "label": 0
                },
                {
                    "sent": "Pop one level up.",
                    "label": 0
                },
                {
                    "sent": "We've seen it all in the context of reinforcement learning, but actually this is applicable.",
                    "label": 0
                },
                {
                    "sent": "Whether it's reinforced learning or any other setting in which you need derivatives.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As in the slides would have.",
                    "label": 0
                },
                {
                    "sent": "Here is a few examples of how you can compute gradients.",
                    "label": 0
                },
                {
                    "sent": "For stochastic neural Nets using the same methodology we just covered.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that in this notation, a square node is a deterministic node.",
                    "label": 0
                },
                {
                    "sent": "A circular node is a stochastic node.",
                    "label": 0
                },
                {
                    "sent": "And now you can compute gradients through this stochastic neural Nets by using the likelihood ratio trick, the same way we computed it for policy gradients.",
                    "label": 0
                },
                {
                    "sent": "Well, it's really beautiful here with Miss is so interesting is that you don't have to do the work by hand.",
                    "label": 0
                },
                {
                    "sent": "What you can do is, once you have your stochastic computation graph, you can actually define a loss function.",
                    "label": 0
                },
                {
                    "sent": "So instead of saying oh as a stochastic node that's being sampled, I need to do this grad log trick.",
                    "label": 0
                },
                {
                    "sent": "You can actually define a loss function on that graph whenever this to Kasich node, you hang off a loss function as the log probability of this so castec node times what comes after the node.",
                    "label": 0
                },
                {
                    "sent": "Once you do that, you can plug this into Tensorflow and Tensorflow.",
                    "label": 0
                },
                {
                    "sent": "Compute is likely racial grains for you with back propagation, just like you could do with a deterministic network, so you can automatically get these gradients for stochastic neural Nets by just computing the combination graph corresponding to this, which means hanging log loss functions, log probabilities, times what comes after off of these graphs.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some food for thought.",
                    "label": 0
                },
                {
                    "sent": "We've seen very different ways of computing derivatives, including in this general setting here.",
                    "label": 0
                },
                {
                    "sent": "Even when you have a stochastic notice being sampled, sometimes you can re parameterise and bring it out.",
                    "label": 0
                },
                {
                    "sent": "Bring out the noise and you might have a question.",
                    "label": 0
                },
                {
                    "sent": "What is the more effective way to compute derivatives?",
                    "label": 0
                },
                {
                    "sent": "It's not always obvious.",
                    "label": 0
                },
                {
                    "sent": "What the best way is to do it?",
                    "label": 0
                },
                {
                    "sent": "And I think there's a lot of thought that can still go into how to compute derivatives when you have different ways of computing derivatives.",
                    "label": 0
                },
                {
                    "sent": "There's empirical things you could do.",
                    "label": 0
                },
                {
                    "sent": "You can compute them both ways.",
                    "label": 0
                },
                {
                    "sent": "Look at the variance, see which one has lower variance empirically.",
                    "label": 0
                },
                {
                    "sent": "Maybe go with that, but there might be other things you can do.",
                    "label": 0
                },
                {
                    "sent": "I think we're out of time, So what I'm going to do is flush by you, the five remaining slide.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead, you can look at when the slides are put online and made us set of slides.",
                    "label": 0
                },
                {
                    "sent": "I show you current frontiers directions where you might want to do research in deep reinforcement learning, as well as pointers to recent papers that relate to this.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one slide here is another slide of current directions that I think are quite important, so a lot of opportunity to do research in this field, and a lot of starting points.",
                    "label": 0
                },
                {
                    "sent": "In terms of coding or more.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Materials there are a few classes that you can check out also be linked in the slides there is.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Code bases that you can refer to to get started, and there's a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Domains which can often be the bottleneck.",
                    "label": 0
                },
                {
                    "sent": "If you have any domains to test in that you might want to try out to work with.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}