{
    "id": "7cweluwrzryhijgguiph6tlec7qxb53d",
    "title": "When causality matters for prediction:Investigating the practical tradeoffs",
    "info": {
        "author": [
            "Robert E. Tillman, Carnegie Mellon University",
            "Peter Spirtes, Carnegie Mellon University"
        ],
        "published": "Dec. 22, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/coa08_tillman_wcmfp/",
    "segmentation": [
        [
            "Yes."
        ],
        [
            "So in the standard setting we've already talked about you, so there's some data generating process that you can observe, But you can get an idea."
        ],
        [
            "Sample so then the objective is you want."
        ],
        [
            "Learning the structure from the data in the causal discovery case, the way that you assess how well you do typically is you have some ground truth for what this structure is supposed to look like, so these are see there through like simulations or experimental studies, and you can look at like how many edges you miss in these."
        ],
        [
            "The things but the focus then, is you want to learn these models that accurately depict the detr."
        ],
        [
            "Data generating system.",
            "Now in the just prediction scenario, the focus is different because you just have some target you're interested in and some predict."
        ],
        [
            "Variables that were related, but you're not positing anything about actually how they are related so."
        ],
        [
            "Just want to predict the values and the way you assess in this case as you have some known values for the target variable given predictions and you apply your prediction function and then you."
        ],
        [
            "Use some loss function and you want to minimize your loss and pick the model that gives good predictions.",
            "But the key thing to emphasize with this is the model that you pick doesn't have to necessarily generate, doesn't have to necessarily resemble the true data generating process.",
            "So like examples like Naive Bayes where we know that we're not going to actually resemble it.",
            "But maybe it works good in practice, so that helps with overfitting."
        ],
        [
            "So use anyway.",
            "Now, the way we've talked about combining the two typically is like what Pearl is talking about.",
            "Previously when we talk about so, we get data from an unmanipulated distribution and we want to predict the effect of an intervention.",
            "If we were going to.",
            "So we're asking the question, we don't have the manipulated data, but we're asking the question, if we did make such an intervention using the unmanipulated data, what would our distribution look like when we talk about counterfactuals, etc."
        ],
        [
            "Yeah, so the key key point is we don't have data from the manipulated popule."
        ],
        [
            "Now, in the causation and prediction challenge, so the setting was different because data was given first from the unmanipulated population and this was used as training data for people to learn."
        ],
        [
            "Some prediction function and then the structure used to generate the unmanipulated data.",
            "There were some interventions were performed and then they waited for the system to stabilize or you waited for the interventions to propagate down and then."
        ],
        [
            "Then once that happened, another ID sample was drawn from the manipulated data and you predicted your target from that.",
            "So the point here is you actually have the data and you have the data after the system is stabilized from the prediction."
        ],
        [
            "R. OK, so the results were that in some instances methods which didn't use causality in any way actually outperformed the causal methods.",
            "Particulated some people used like support vector machines just directly on the data and didn't use them.",
            "Any type of calls away and they did better than some people who you."
        ],
        [
            "Star causal methods.",
            "So you want to ask then if causality is really useful just when you're in this standard prediction scenario, maybe theoretically you can give conditions that where it is, but you also are interested in whether it's actually."
        ],
        [
            "To be useful in practice, and then we can all."
        ],
        [
            "To ask just for the purposes of this workshop.",
            "But this is a realistic scenario where we're going to have the data from the two different populations and we want to.",
            "This is how we want to actually assess our causal discovery methods."
        ],
        [
            "OK, so some possible explanations as to what might have been going on in this case is, well, we know we're going to get some error just due to like sampling error overfitting 'cause we know the causal methods don't do particularly well in this category.",
            "The other thing is that we know, so the calls will make methods make some pretty significant parametric assumptions, which are oftentimes not going to hold linearity in gas can be particularly our most of the methods, at least we've seen some that happened, but the other thing that we want to talk about, which is.",
            "Important for this particular task is that in some cases the prediction function for the target is going to be invariant under the manipulation.",
            "Now what we mean by invariant is that in training using the manipulated distribution for Ticular predicts that particular target variable, and then you train in the OR you switch to the manipulated distribution.",
            "Then the prediction function that you learned from the unmanipulated data is going to be there as well, like you wouldn't have learned something differently.",
            "Using the manipulated data."
        ],
        [
            "So just like a simple example as to how this would happen, so say this is your structure and you're just going to predict Y.",
            "So you want your Bayes optimal prediction to be."
        ],
        [
            "Probably why given X?",
            "So something really X, so that's not going to change distribution.",
            "The probability of Y given X.",
            "And this is still Bayes optimal.",
            "So if you have the data at once, the.",
            "The manipulation has been performed an you get the effect to why as opposed to just trying to predict what would happen if you did manipulate why given only unmanipulated data then this problem becomes a lot easier.",
            "Now this is obviously of course not the case if human."
        ],
        [
            "Why?",
            "Because it does change the distribution and this is no longer a Bayes optimal, so you're going to have to make errors if you use.",
            "If you used a function that you learned on the unmanipulated data after an intervention was performed on Y. OK, so we can generalize this notion to just OK. We can generalize this notion too.",
            "The more complicated cases using the Markov blanket.",
            "OK, so due to time does everyone know?",
            "Do I need to explain the Markov blankets?"
        ],
        [
            "To people."
        ],
        [
            "OK."
        ],
        [
            "So.",
            "This is a.",
            "This is how we're going to model interventions here, so we.",
            "If this is the variable we're going to intervene and we're just going to policy node and then when we set the policy node equal to 1, that means we delete the."
        ],
        [
            "Just here.",
            "OK, So what this theorem says is that if we.",
            "If the ceiling damage if this is the variable that we're interested, this is our target that we're interested in predicting, and we train a prediction function using any super set of the Markov blanket here.",
            "Then as long as.",
            "And we manipulate some set of variables as long as the set of variables we manipulate are not children of this target variable, even if there are other variables in Markov blanket like, even if their parents or Co parents here, then our prediction function is going to be invariant.",
            "So this is important because so, like in the challenge, particularly when you had like 1000 variables.",
            "So it's going to be unlikely that if you're just predicting one variable and you actually have this data, that the variable that you're going to predict is going to be.",
            "You know children or you're going to make enough errors just because of children of that particular target."
        ],
        [
            "So OK."
        ],
        [
            "And it's easy to."
        ],
        [
            "To show."
        ],
        [
            "Why this business is pretty trivial?",
            "Now if we do have the causal information we can fix.",
            "Fix this so that even if even if children are manipulated, will condition on the right variables and so basically what this says this theorem says is that if we're going to manipulate breathing disorder here, then we rather than using any subset either any super set with the Markov blanket we're going to use the Markov blanket for the manipulated distribution."
        ],
        [
            "So if we perform the manipulation, then this becomes our new Markov blanket here these.",
            "Darkow darker red nodes.",
            "So if we condition on these variables, then we'll have.",
            "We will not will account for the proper shift in the distribution, so will make the correct predictions.",
            "If we use these variables as our predictor variables and this is true even if manipulate children."
        ],
        [
            "Manipulated, unless we have a path from manipulated child to an unmanipulated child.",
            "So like in this case we're going to manipulate lung capacity."
        ],
        [
            "And when we do that.",
            "You see, we have this red arrow right here, so this manipulation is going to affect this variable here, which is in the Markov blanket in the manipulated structure.",
            "So we want to.",
            "We need to make a correction to account for that, but the point is so we can.",
            "We know how to make the correction, so this theorem, the theorem basically gives us all the information that we need to correct for the shift in the distribution when children are manipulated, in addition to anything else.",
            "Anything else is not going to matter, so this is the only instance where the causal methods would actually outperform the non causal methods in just these standard prediction tasks when you actually have data.",
            "From this table."
        ],
        [
            "Eyes manipulated distribution.",
            "OK, so we did some experiments just to confirm these conditions, so our hypothesis were that non causal methods will be equivalent or better than.",
            "The methods which take advantage of causality when no children."
        ],
        [
            "Related and then the causal methods will do increasingly better as.",
            "Children are manipulated."
        ],
        [
            "OK, so this was the structure that we started with when we did the experiments, so we have a number of.",
            "Parents and children and we have some of these paths like I was talking bout from children to other children.",
            "So if we manipulate this we're going to get that effect like I was talking about here since this is an ancestor of."
        ],
        [
            "Their child.",
            "So the method that we used is.",
            "We used a variety of prediction methods, some of which were causal, some of which are non causal and."
        ],
        [
            "We we generated data from this unmanipulated distribution.",
            "We use linear Gaussians.",
            "In this case just we wanted an optimal scenario just to make sure we're giving causality.",
            "The benefit of the doubt when it would we would expect it to do better.",
            "We manipulated.",
            "After that we manipulated 05 and 10 random non children of the target and this include."
        ],
        [
            "Variables in the Markov blanket and then for each of these situations we would manipulate from zero to 9 chill."
        ],
        [
            "Of the target.",
            "And then we predict T after we generate."
        ],
        [
            "Sample from the manipulated distribution.",
            "OK, so running at a time so I'm not going to explain this particular one, But this this is before we actually look at our real results.",
            "This is just showing the shift in the distribution and so we're showing if we're just predicting one target variable the these three lines represent the non children and the X axis represents the children.",
            "So this is how artists are.",
            "Distribution isn't shifting based on the non children."
        ],
        [
            "Word manipulating here.",
            "So the non causal methods that we used, we used just basic linear regression on using all the predictors.",
            "We use linear regression using only the Markov blanket.",
            "We use the lasso, which is just the standard L1 penalty regression.",
            "And then we used support vector regression and relevance vector regression."
        ],
        [
            "Both of the radial kernels.",
            "The and then the causal methods that we use.",
            "We just use basic linear regression on the corrected Markov blanket.",
            "We did two instances, in one of which we corrected for this error where we have paths from manipulated children to non children."
        ],
        [
            "And so these are the results when we manipulate no non children.",
            "So we see when nothing is manipulated the causal methods actually do worse, which is what we might expect since they're not particularly good at handling overfitting.",
            "And these types of things.",
            "But all of the causal methods stay down here and we see a rise in the non causal methods as more children."
        ],
        [
            "Simulator and then we shift to five manipulated non children.",
            "The results say the same.",
            "So this is what we're expecting again because these these additional variables that were manipulating are not affecting our prediction function are not are not affecting the non causal map."
        ],
        [
            "And then the call.",
            "Let's say the same.",
            "You see the same results for when you manipulate 10 non children and this."
        ],
        [
            "Generalizes more.",
            "OK."
        ],
        [
            "Our yeah we."
        ],
        [
            "Right with nonlinear data, but I'm not going to go into that.",
            "So.",
            "Back to just the original questions they were asking, so is causality relevant for prediction?",
            "Well, so we can show this theoretical case where if we put."
        ],
        [
            "In the best scenario possible that it is doing well and we can show how it increases, but a lot of the times we think we're going to have this in variance between the non causal and acausal methods, especially if you have like lots of variables because it's unlikely that you're going to be manipulating lots of children all the time, or it may even be the case that if you're manipulating a few children."
        ],
        [
            "That's the."
        ],
        [
            "That the children that you manipulate are not going to make up for as much error as you make when you assume these parametric assumptions, that the causality methods assume.",
            "And if you're using a method like a support vector machine or something like that that's non parametric, it's going to be more robust to these."
        ],
        [
            "Types of things.",
            "So there's a tradeoff between these that are going on in practice.",
            "And so I mean, this is these are basically the."
        ],
        [
            "Cultivar analysis OK. Training data.",
            "Are we did buy?",
            "I mean, I mean, you saw, that's all the methods started to do a little bit better.",
            "Like you didn't see much of a difference between the causal methods or the non causal methods based on the amount of training data that you were getting.",
            "I mean, it was just kind of a.",
            "Like you still you still see the effect the same pattern of effect even when you vary the training data.",
            "At least that was what we saw.",
            "Yeah.",
            "Number of children.",
            "When does this become visible?",
            "The amount of frame data.",
            "Um?",
            "Like I mean, I would have to go back to the original results, but we tended to see pretty much about the same pattern along all along.",
            "It was basically like they all.",
            "All the methods would do fairly poorly, or they would or you would start to see the trend start to happen so.",
            "Well, so like we don't know the ground truth so we don't know like we couldn't try and start manipulating more and more children.",
            "Well, yeah, 'cause we don't basically know the ground truth like we tried to.",
            "We did a discovery on that data like I think red datasets.",
            "The structure that we generated it was to try and resemble sort of what we were getting.",
            "In that case like.",
            "I don't know.",
            "I don't know if we resembled it that close, but that was we were we were aiming for something to sort of like that.",
            "Maybe one comment on your question is if the assumptions are wrong.",
            "For instance, if you assume multivariant causality and you don't have that, then it doesn't help that even when you collect, collect more and more data.",
            "So maybe that's also the problem that it's too much parameterized.",
            "Well, the first challenge.",
            "Yes."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the standard setting we've already talked about you, so there's some data generating process that you can observe, But you can get an idea.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample so then the objective is you want.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning the structure from the data in the causal discovery case, the way that you assess how well you do typically is you have some ground truth for what this structure is supposed to look like, so these are see there through like simulations or experimental studies, and you can look at like how many edges you miss in these.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The things but the focus then, is you want to learn these models that accurately depict the detr.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data generating system.",
                    "label": 0
                },
                {
                    "sent": "Now in the just prediction scenario, the focus is different because you just have some target you're interested in and some predict.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variables that were related, but you're not positing anything about actually how they are related so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just want to predict the values and the way you assess in this case as you have some known values for the target variable given predictions and you apply your prediction function and then you.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use some loss function and you want to minimize your loss and pick the model that gives good predictions.",
                    "label": 0
                },
                {
                    "sent": "But the key thing to emphasize with this is the model that you pick doesn't have to necessarily generate, doesn't have to necessarily resemble the true data generating process.",
                    "label": 1
                },
                {
                    "sent": "So like examples like Naive Bayes where we know that we're not going to actually resemble it.",
                    "label": 0
                },
                {
                    "sent": "But maybe it works good in practice, so that helps with overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So use anyway.",
                    "label": 0
                },
                {
                    "sent": "Now, the way we've talked about combining the two typically is like what Pearl is talking about.",
                    "label": 0
                },
                {
                    "sent": "Previously when we talk about so, we get data from an unmanipulated distribution and we want to predict the effect of an intervention.",
                    "label": 0
                },
                {
                    "sent": "If we were going to.",
                    "label": 0
                },
                {
                    "sent": "So we're asking the question, we don't have the manipulated data, but we're asking the question, if we did make such an intervention using the unmanipulated data, what would our distribution look like when we talk about counterfactuals, etc.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so the key key point is we don't have data from the manipulated popule.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in the causation and prediction challenge, so the setting was different because data was given first from the unmanipulated population and this was used as training data for people to learn.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some prediction function and then the structure used to generate the unmanipulated data.",
                    "label": 0
                },
                {
                    "sent": "There were some interventions were performed and then they waited for the system to stabilize or you waited for the interventions to propagate down and then.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then once that happened, another ID sample was drawn from the manipulated data and you predicted your target from that.",
                    "label": 0
                },
                {
                    "sent": "So the point here is you actually have the data and you have the data after the system is stabilized from the prediction.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "R. OK, so the results were that in some instances methods which didn't use causality in any way actually outperformed the causal methods.",
                    "label": 1
                },
                {
                    "sent": "Particulated some people used like support vector machines just directly on the data and didn't use them.",
                    "label": 0
                },
                {
                    "sent": "Any type of calls away and they did better than some people who you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Star causal methods.",
                    "label": 0
                },
                {
                    "sent": "So you want to ask then if causality is really useful just when you're in this standard prediction scenario, maybe theoretically you can give conditions that where it is, but you also are interested in whether it's actually.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be useful in practice, and then we can all.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To ask just for the purposes of this workshop.",
                    "label": 0
                },
                {
                    "sent": "But this is a realistic scenario where we're going to have the data from the two different populations and we want to.",
                    "label": 1
                },
                {
                    "sent": "This is how we want to actually assess our causal discovery methods.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so some possible explanations as to what might have been going on in this case is, well, we know we're going to get some error just due to like sampling error overfitting 'cause we know the causal methods don't do particularly well in this category.",
                    "label": 1
                },
                {
                    "sent": "The other thing is that we know, so the calls will make methods make some pretty significant parametric assumptions, which are oftentimes not going to hold linearity in gas can be particularly our most of the methods, at least we've seen some that happened, but the other thing that we want to talk about, which is.",
                    "label": 0
                },
                {
                    "sent": "Important for this particular task is that in some cases the prediction function for the target is going to be invariant under the manipulation.",
                    "label": 1
                },
                {
                    "sent": "Now what we mean by invariant is that in training using the manipulated distribution for Ticular predicts that particular target variable, and then you train in the OR you switch to the manipulated distribution.",
                    "label": 0
                },
                {
                    "sent": "Then the prediction function that you learned from the unmanipulated data is going to be there as well, like you wouldn't have learned something differently.",
                    "label": 0
                },
                {
                    "sent": "Using the manipulated data.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just like a simple example as to how this would happen, so say this is your structure and you're just going to predict Y.",
                    "label": 0
                },
                {
                    "sent": "So you want your Bayes optimal prediction to be.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probably why given X?",
                    "label": 0
                },
                {
                    "sent": "So something really X, so that's not going to change distribution.",
                    "label": 0
                },
                {
                    "sent": "The probability of Y given X.",
                    "label": 0
                },
                {
                    "sent": "And this is still Bayes optimal.",
                    "label": 1
                },
                {
                    "sent": "So if you have the data at once, the.",
                    "label": 0
                },
                {
                    "sent": "The manipulation has been performed an you get the effect to why as opposed to just trying to predict what would happen if you did manipulate why given only unmanipulated data then this problem becomes a lot easier.",
                    "label": 0
                },
                {
                    "sent": "Now this is obviously of course not the case if human.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because it does change the distribution and this is no longer a Bayes optimal, so you're going to have to make errors if you use.",
                    "label": 1
                },
                {
                    "sent": "If you used a function that you learned on the unmanipulated data after an intervention was performed on Y. OK, so we can generalize this notion to just OK. We can generalize this notion too.",
                    "label": 0
                },
                {
                    "sent": "The more complicated cases using the Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "OK, so due to time does everyone know?",
                    "label": 0
                },
                {
                    "sent": "Do I need to explain the Markov blankets?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To people.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "This is how we're going to model interventions here, so we.",
                    "label": 0
                },
                {
                    "sent": "If this is the variable we're going to intervene and we're just going to policy node and then when we set the policy node equal to 1, that means we delete the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just here.",
                    "label": 0
                },
                {
                    "sent": "OK, So what this theorem says is that if we.",
                    "label": 0
                },
                {
                    "sent": "If the ceiling damage if this is the variable that we're interested, this is our target that we're interested in predicting, and we train a prediction function using any super set of the Markov blanket here.",
                    "label": 0
                },
                {
                    "sent": "Then as long as.",
                    "label": 0
                },
                {
                    "sent": "And we manipulate some set of variables as long as the set of variables we manipulate are not children of this target variable, even if there are other variables in Markov blanket like, even if their parents or Co parents here, then our prediction function is going to be invariant.",
                    "label": 0
                },
                {
                    "sent": "So this is important because so, like in the challenge, particularly when you had like 1000 variables.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be unlikely that if you're just predicting one variable and you actually have this data, that the variable that you're going to predict is going to be.",
                    "label": 0
                },
                {
                    "sent": "You know children or you're going to make enough errors just because of children of that particular target.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's easy to.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To show.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why this business is pretty trivial?",
                    "label": 0
                },
                {
                    "sent": "Now if we do have the causal information we can fix.",
                    "label": 0
                },
                {
                    "sent": "Fix this so that even if even if children are manipulated, will condition on the right variables and so basically what this says this theorem says is that if we're going to manipulate breathing disorder here, then we rather than using any subset either any super set with the Markov blanket we're going to use the Markov blanket for the manipulated distribution.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we perform the manipulation, then this becomes our new Markov blanket here these.",
                    "label": 0
                },
                {
                    "sent": "Darkow darker red nodes.",
                    "label": 0
                },
                {
                    "sent": "So if we condition on these variables, then we'll have.",
                    "label": 0
                },
                {
                    "sent": "We will not will account for the proper shift in the distribution, so will make the correct predictions.",
                    "label": 0
                },
                {
                    "sent": "If we use these variables as our predictor variables and this is true even if manipulate children.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Manipulated, unless we have a path from manipulated child to an unmanipulated child.",
                    "label": 0
                },
                {
                    "sent": "So like in this case we're going to manipulate lung capacity.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when we do that.",
                    "label": 0
                },
                {
                    "sent": "You see, we have this red arrow right here, so this manipulation is going to affect this variable here, which is in the Markov blanket in the manipulated structure.",
                    "label": 0
                },
                {
                    "sent": "So we want to.",
                    "label": 0
                },
                {
                    "sent": "We need to make a correction to account for that, but the point is so we can.",
                    "label": 0
                },
                {
                    "sent": "We know how to make the correction, so this theorem, the theorem basically gives us all the information that we need to correct for the shift in the distribution when children are manipulated, in addition to anything else.",
                    "label": 0
                },
                {
                    "sent": "Anything else is not going to matter, so this is the only instance where the causal methods would actually outperform the non causal methods in just these standard prediction tasks when you actually have data.",
                    "label": 0
                },
                {
                    "sent": "From this table.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eyes manipulated distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so we did some experiments just to confirm these conditions, so our hypothesis were that non causal methods will be equivalent or better than.",
                    "label": 0
                },
                {
                    "sent": "The methods which take advantage of causality when no children.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Related and then the causal methods will do increasingly better as.",
                    "label": 0
                },
                {
                    "sent": "Children are manipulated.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this was the structure that we started with when we did the experiments, so we have a number of.",
                    "label": 0
                },
                {
                    "sent": "Parents and children and we have some of these paths like I was talking bout from children to other children.",
                    "label": 0
                },
                {
                    "sent": "So if we manipulate this we're going to get that effect like I was talking about here since this is an ancestor of.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their child.",
                    "label": 0
                },
                {
                    "sent": "So the method that we used is.",
                    "label": 0
                },
                {
                    "sent": "We used a variety of prediction methods, some of which were causal, some of which are non causal and.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We we generated data from this unmanipulated distribution.",
                    "label": 0
                },
                {
                    "sent": "We use linear Gaussians.",
                    "label": 0
                },
                {
                    "sent": "In this case just we wanted an optimal scenario just to make sure we're giving causality.",
                    "label": 0
                },
                {
                    "sent": "The benefit of the doubt when it would we would expect it to do better.",
                    "label": 0
                },
                {
                    "sent": "We manipulated.",
                    "label": 0
                },
                {
                    "sent": "After that we manipulated 05 and 10 random non children of the target and this include.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variables in the Markov blanket and then for each of these situations we would manipulate from zero to 9 chill.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the target.",
                    "label": 0
                },
                {
                    "sent": "And then we predict T after we generate.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample from the manipulated distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so running at a time so I'm not going to explain this particular one, But this this is before we actually look at our real results.",
                    "label": 0
                },
                {
                    "sent": "This is just showing the shift in the distribution and so we're showing if we're just predicting one target variable the these three lines represent the non children and the X axis represents the children.",
                    "label": 0
                },
                {
                    "sent": "So this is how artists are.",
                    "label": 0
                },
                {
                    "sent": "Distribution isn't shifting based on the non children.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Word manipulating here.",
                    "label": 0
                },
                {
                    "sent": "So the non causal methods that we used, we used just basic linear regression on using all the predictors.",
                    "label": 0
                },
                {
                    "sent": "We use linear regression using only the Markov blanket.",
                    "label": 0
                },
                {
                    "sent": "We use the lasso, which is just the standard L1 penalty regression.",
                    "label": 0
                },
                {
                    "sent": "And then we used support vector regression and relevance vector regression.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Both of the radial kernels.",
                    "label": 0
                },
                {
                    "sent": "The and then the causal methods that we use.",
                    "label": 0
                },
                {
                    "sent": "We just use basic linear regression on the corrected Markov blanket.",
                    "label": 1
                },
                {
                    "sent": "We did two instances, in one of which we corrected for this error where we have paths from manipulated children to non children.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so these are the results when we manipulate no non children.",
                    "label": 0
                },
                {
                    "sent": "So we see when nothing is manipulated the causal methods actually do worse, which is what we might expect since they're not particularly good at handling overfitting.",
                    "label": 0
                },
                {
                    "sent": "And these types of things.",
                    "label": 0
                },
                {
                    "sent": "But all of the causal methods stay down here and we see a rise in the non causal methods as more children.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simulator and then we shift to five manipulated non children.",
                    "label": 0
                },
                {
                    "sent": "The results say the same.",
                    "label": 0
                },
                {
                    "sent": "So this is what we're expecting again because these these additional variables that were manipulating are not affecting our prediction function are not are not affecting the non causal map.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the call.",
                    "label": 0
                },
                {
                    "sent": "Let's say the same.",
                    "label": 0
                },
                {
                    "sent": "You see the same results for when you manipulate 10 non children and this.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generalizes more.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our yeah we.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right with nonlinear data, but I'm not going to go into that.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Back to just the original questions they were asking, so is causality relevant for prediction?",
                    "label": 0
                },
                {
                    "sent": "Well, so we can show this theoretical case where if we put.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the best scenario possible that it is doing well and we can show how it increases, but a lot of the times we think we're going to have this in variance between the non causal and acausal methods, especially if you have like lots of variables because it's unlikely that you're going to be manipulating lots of children all the time, or it may even be the case that if you're manipulating a few children.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That the children that you manipulate are not going to make up for as much error as you make when you assume these parametric assumptions, that the causality methods assume.",
                    "label": 0
                },
                {
                    "sent": "And if you're using a method like a support vector machine or something like that that's non parametric, it's going to be more robust to these.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Types of things.",
                    "label": 0
                },
                {
                    "sent": "So there's a tradeoff between these that are going on in practice.",
                    "label": 1
                },
                {
                    "sent": "And so I mean, this is these are basically the.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cultivar analysis OK. Training data.",
                    "label": 0
                },
                {
                    "sent": "Are we did buy?",
                    "label": 0
                },
                {
                    "sent": "I mean, I mean, you saw, that's all the methods started to do a little bit better.",
                    "label": 0
                },
                {
                    "sent": "Like you didn't see much of a difference between the causal methods or the non causal methods based on the amount of training data that you were getting.",
                    "label": 0
                },
                {
                    "sent": "I mean, it was just kind of a.",
                    "label": 0
                },
                {
                    "sent": "Like you still you still see the effect the same pattern of effect even when you vary the training data.",
                    "label": 0
                },
                {
                    "sent": "At least that was what we saw.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Number of children.",
                    "label": 0
                },
                {
                    "sent": "When does this become visible?",
                    "label": 0
                },
                {
                    "sent": "The amount of frame data.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Like I mean, I would have to go back to the original results, but we tended to see pretty much about the same pattern along all along.",
                    "label": 0
                },
                {
                    "sent": "It was basically like they all.",
                    "label": 0
                },
                {
                    "sent": "All the methods would do fairly poorly, or they would or you would start to see the trend start to happen so.",
                    "label": 0
                },
                {
                    "sent": "Well, so like we don't know the ground truth so we don't know like we couldn't try and start manipulating more and more children.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, 'cause we don't basically know the ground truth like we tried to.",
                    "label": 0
                },
                {
                    "sent": "We did a discovery on that data like I think red datasets.",
                    "label": 0
                },
                {
                    "sent": "The structure that we generated it was to try and resemble sort of what we were getting.",
                    "label": 0
                },
                {
                    "sent": "In that case like.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know if we resembled it that close, but that was we were we were aiming for something to sort of like that.",
                    "label": 0
                },
                {
                    "sent": "Maybe one comment on your question is if the assumptions are wrong.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you assume multivariant causality and you don't have that, then it doesn't help that even when you collect, collect more and more data.",
                    "label": 0
                },
                {
                    "sent": "So maybe that's also the problem that it's too much parameterized.",
                    "label": 0
                },
                {
                    "sent": "Well, the first challenge.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        }
    }
}