{
    "id": "w56t6qgh2jlp2xf2ltutdyau5ajsirva",
    "title": "Spectral Graph Theory, Linear Solvers and Applications",
    "info": {
        "author": [
            "Gary L Miller, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/mlss09us_miller_sgtlsa/",
    "segmentation": [
        [
            "OK, now it's my pleasure to introduce Gary Miller from Carnegie Mellon.",
            "Thanks very much, Gary.",
            "Thank you, thank you for inviting me.",
            "Some pleasure to give a talk.",
            "So hopefully this will be understandable.",
            "So again, please ask questions.",
            "This is a workshop, not a. Pre programmed talk.",
            "So today what I'd like to talk about is this area, which is called now spectral graph theory, right?",
            "This is the idea that you think of a graph as a matrix and look at its eigenvectors and you think of a matrix as a graph and do graph theory to the matrix OK, and so in particular we're going to look at questions about how do you quick."
        ],
        [
            "We solve linear systems and again that's going to reduce to graph graph questions, and then we'll also show applications where we use spectral graph theory to solve some of these questions.",
            "So of course, the simplest of all constraint systems that have been around for hundreds of years is the notion of a linear system, and this is very fundamental in all the workers.",
            "Everyone is every talk I've seen so far under the hood somewhere they're solving linear systems, and so it's critical to be able to solve these systems quickly."
        ],
        [
            "So of course obvious thing to do is write it as a matrix.",
            "I guess it's interesting point, I believe that Gauss did not understand this notation, so that's one of the reasons I've never been able to read of a lot of his original work is that he didn't know about matrices with much later it was discovered, but a very beautiful way to think of the world rise matrices."
        ],
        [
            "So what's interesting is it is theoretically anyway, if you want to solve dense matrices, you can do that in Ncube time.",
            "Strassen then showed how to do that in end of the two point 8 and then times been improved.",
            "Most of these then, are of course way too large for the kind of systems we'd like to solve today, and so it's still a very big number, right?"
        ],
        [
            "So what's interesting is that it's not if it's just a general sparse system.",
            "We don't have a better theory.",
            "Worst case analysis to say much better about."
        ],
        [
            "So there are easy cases for the simplest of all easy cases.",
            "The upper triangular or lower triangular we can do back substitution, anwen."
        ],
        [
            "We do this, then we get to solve it in a number of non zeros in right?",
            "So the goal one goal then would be to find a bunch of other."
        ],
        [
            "For linear systems, all of which we can then solve as fast as we can solve an upper triangular system OK, and so that's their goal."
        ],
        [
            "So the first obvious constraint that people put on with is that it's symmetric, so throughout the rest of this talk.",
            "Then we'll assume our matrix is symmetric, is symmetric, and of course is a graph that means our graph is undirected.",
            "It would be nice to get rid of this constraint.",
            "I don't know how, but tonight, but anyway."
        ],
        [
            "I will use that.",
            "So another assumption throughout this talk is is going to be to start with.",
            "That is positive definite, right?",
            "Namely that all the eigenvalue values are strictly positive, or that Rayleigh quotient is always positive, right?"
        ],
        [
            "So, so the first approach that people started doing.",
            "I guess I just decided to do a history of some of the standard computer science literature and solving systems, and that is to look at direct methods, Gaussian elimination.",
            "And this is still the method to beat if only if you only have a few thousand variables, and so that's where we're going to start, 'cause in fact, what happens in all modern systems is what you're going to do then is reduce the problem to smaller size and as soon as this fall size goes below a certain level, you're going to go to whatever the method that's best for that size.",
            "So in particular, let's look at direct methods."
        ],
        [
            "OK, so in the 70s graph theoretic interpretation of Gaussian elimination was popularized probably was known before that, but that is what you do is view every non zero off diagonal edge value is undirected edge and you view."
        ],
        [
            "Pivoting is a graph operation.",
            "OK, so in particular, what is pivoting for?",
            "And from a graph theoretic point of view, as you have some graph for your original."
        ],
        [
            "And you have a vertex and you'd like to pivot on that vertex.",
            "So what you end up doing is you make a click out of the neighborhood of that vertex.",
            "You would then remove that vertex and its edges from the graph and you repeat until you've eliminate."
        ],
        [
            "The graph OK, so throughout this."
        ],
        [
            "Process the new edges are formed by forming cliques.",
            "Whenever the edge is not already there.",
            "An also work is done independent of whether an edge is there or not.",
            "You end up touching all those edges by changing their value, and so the work ends up being all the edges touch throughout the life of the algorithm."
        ],
        [
            "OK, so this is the classic work from the 70s of Lipton, Rose, and Tarjan said that you can solve any planar linear system, in other words a system for which the undirected the nonzero structure is a planar graph, so 2D.",
            "So for instance this is natural for like 2D image processing would come up.",
            "You can solve those and end of the three halves work and then log filler space.",
            "It's actually interesting that N log N ends up being important and we'll see."
        ],
        [
            "So in 3D, unfortunately the numbers are much worse, so if you have a 3D system like a 3D medical image, you want to do, or if there's somehow intrinsic manifold of three dimensions, then in general the work is going to be N squared work and it's going to be end of the three halves filler space.",
            "And again, this will see that this is bad.",
            "OK, so in this is."
        ],
        [
            "Very problematic, we can show you very simple examples for which it is the kiss of death into the three halves space, so you can't afford to do that, OK?",
            "So the next method that people proposed."
        ],
        [
            "This is iterative methods, so iterative methods are the simplest of all.",
            "Litter method is the following recurrence relation right?",
            "I want to solve the system ax equals B and it's the case that I consider the following recurrence.",
            "I take my guess at time I X super I and I multiply it by the identity minus A and I add B and then it's an easy exercise to see that if I ever find a fixed point for this recurrence, I have a solution."
        ],
        [
            "In my system.",
            "And at the rate of convergence, then, is related to the norm of the matrix I -- A, which we'll get back to later on OK?",
            "So and then."
        ],
        [
            "Of course, if you if you want to do fancier methods, those are all exists Chevy Chev iteration or conjugate gradients are all generalizations of this method.",
            "Here we won't go into those, but there are slight but they improve the rate of convergence slightly and."
        ],
        [
            "People do those all the time OK, but even again what we'll see is the conjugate gradient still is not fast enough for large systems, OK?",
            "So so that."
        ],
        [
            "First idea that came along as the idea of preconditioned iterative method and so let me just explain this so that the idea now is is what we're going to do is we again want to solve the system ax equals B, but will multiply both sides of the system by a matrix B inverse.",
            "This gives us a new linear system to solve, which is the same 1B inverse A X = B prime.",
            "OK, so if we just write out that same recurrence again, we get the equations we had it before, but the crucial point here is is that.",
            "You should not multiply out the matrix I -- B inverse A into a big messy matrix and then just iterate.",
            "What you should do is the following four."
        ],
        [
            "And what you should do now is first, let's just worry about computing this terms equals.",
            "I think this might work actually.",
            "So let's just multiple compute the term Z, which is B inverse AX super I.",
            "Well what will first do is we'll do a forward multiply to compute Y which is a * X.",
            "And then rather than do then the last step here is we'll do a solve for Z instead of computing the inverse, right?",
            "So we think of.",
            "This recurrence really is a hybrid between two methods, right?",
            "So it's a simple iterative method, but inside the inner loop is 2 methods of forward multiply, which we talked about before plus plus I back solve OK."
        ],
        [
            "So, So what the trouble now is, is with this kind of method is is that we have two costs right?",
            "One is to minimize the number of iterations that where are our methods going to take and the second problem here is to minimize the cost of the solve these equals Y.",
            "So we want to somehow be able to balance those things to those off so that we get a faster rate of convergence.",
            "OK, so just in case.",
            "You you think?"
        ],
        [
            "I haven't seen this before.",
            "Anyone that's taken a standard miracle analysis class on a solver you've seen preconditioned iterative systems all along.",
            "The simplest is Jacobi B is just simply the diagonal of a.",
            "Another"
        ],
        [
            "Classic example is is that Gauss Seidel B is simply just the upper triangular part because again, we now know that solving upper triangular matrices can be done quickly, so therefore that's the obvious thing to put in there, up to the fact that it's easy to solve for the upper triangular part of a. Numerically, it doesn't really make a lot of sense to do that, but people do it anyway, right?",
            "It means a very natural thing to put in 'cause what else you're going to do, right?",
            "But anyway, so that's a class."
        ],
        [
            "Thing to do there is also symmetric successive over relaxation, which is you do every exotic combination of upper and lower triangular diagonal parts of your original matrix, and you put it into a big messy thing, and then you back solve that.",
            "OK, so that's another classic thing that people do.",
            "OK, so very classic methods."
        ],
        [
            "It can be thought of nothing more than preconditioned iterative methods, right?",
            "OK.",
            "So these things still or are too slow and too unreliable in the sense of Gauss Seidel.",
            "If it converges, great, and if it doesn't, it's not so clear what you do right?",
            "So the problem is is that you know.",
            "Our goal is to generate solvers for which they work systematically, right.",
            "What I would like to be able to do is generate a solver for which I can put it into a program, roll it into a doctors office, and have them use it 24/7 and not phone me up in the middle of the night and say the solver didn't work.",
            "I mean, there's other kinds of solvers, right?",
            "So the big distinction here to make sure the reliability right?",
            "So I would imagine if all we want is a solver for which you're going to go home and use it yourself.",
            "Personally, if it fails one in 200 times about the failure rate of the shuttle.",
            "That may be just fine, right?",
            "I mean the spatial that's all you need.",
            "If it's the case, you want to send this out and sell it as a product, then failure rates one 200 is a little bit as too high.",
            "You need a lower failure rate.",
            "So what we'd like to do is actually have algorithms that always work, so that's what I mean by unreliable.",
            "Here most of the stuff works nice most of the time, but it will fail.",
            "There's a problem, OK?",
            "So."
        ],
        [
            "So in order to be able to come up with a theory for which we can claim that the algorithm works more systematically.",
            "In other words, we can give guarantees.",
            "Unfortunately, we have to start restricting the number of kinds of matrices that come up.",
            "Fortunately, if you look at most people in this room, they think this is a very natural restriction, and so they're not unhappy at all, or you're not unhappy, but but nonetheless it is a major restriction on the types of matrices were going to be able to solve.",
            "So let's talk about the graph Laplacian, which you all know."
        ],
        [
            "Namely, you take a weighted undirected graph.",
            "You assume all the edge weights are.",
            "Strictly positive you define the incidence matrix."
        ],
        [
            "You define the degree matrix, which is just simply the sum of the weights.",
            "Giving you a diagonal matrix and from that then the Laplacian is simply D -- A."
        ],
        [
            "And So what we're going to do now, for the."
        ],
        [
            "While then is we're going to restrict ourselves to solving linear systems where the underlying system is as a graph Laplacian."
        ],
        [
            "OK, so so the graph Laplacian comes up in many places, probably every every other talk here.",
            "But let me just make this just review some of the classic places that comes up.",
            "One of the reasons it's useful to do that is is that even if you don't care about these applications and you're still using a graph Laplacian, it's very useful to know that the system you're solving has very other multitude of other meanings.",
            "So you're solving the system for some other meeting.",
            "You know what's some marketing problem or something, but it may have some very natural meaning in terms of resistors or in terms of spring mass systems and other things, right?",
            "So let's just go through some."
        ],
        [
            "OK, so one idea is your view.",
            "Each edge is a conductor.",
            "With conductance, WIJ.",
            "So if you let V be a call."
        ],
        [
            "Vector of voltages than L V = C. Then C will be the residual current needed to maintain the given voltage.",
            "Good OK, so I thought I had more or less like maybe not OK, so in particular what this says then is is that in order to maintain the voltage at those nodes, you're going to have to either inject current or remove current and the amount you have to inject are removed is precisely see.",
            "You can also turn this on its head if you would know how much current you want to inject, then what voltages will it float too in order to maintain to have those currents OK, and so there's a classic way to view Laplacian, some very useful."
        ],
        [
            "OK. Good so another place this comes up that's basically the heat equation, and so that's a standard."
        ],
        [
            "Place this comes up another place this comes up as in random walks again here the transition matrix ends up being simply D inverse are diagonal matrix we had before times the Laplacian.",
            "An interesting question.",
            "Then it is to compute the fundamental eigenvectors of these kinds of systems and one of the amazing things here is is that there's fast solvers which will talk about the Spielman Tang solver is very theoretical, but it's interesting in the sense that says that the problem won't get any harder.",
            "Someone won't be able to prove that you could do it, that it takes more time.",
            "So what ends up happening is, well, maybe this is the wrong place for it in the talk, but.",
            "If you're interested in eigenvectors of graph laplacian's, so the standard theory would say you would try to do forward powers as a way to find those eigenvectors.",
            "So we found both experimentally and theoretically actually doing inverse powering is the trick that works much better if you in fact simply do take your initial guess for an eigenvector, and you simply then take the inverse power of that guess and repeat that process.",
            "After login iterations, you'll converge to something with very small Rayleigh quotient, or in practice something that looks like an eigenvector.",
            "OK, so that's a very powerful trick.",
            "We should maybe go over that, but you know that that's very nice."
        ],
        [
            "Yeah, OK.",
            "So so the other place that laplacian's come up is in spring mass systems.",
            "So again, suppose we have a weighted graph and suppose we view instead the constants as spring constants.",
            "The WIJ is a spring constant.",
            "Um?",
            "And suppose we let M. Villa."
        ],
        [
            "Diagonal matrix of mass constants.",
            "Then a standard result is."
        ],
        [
            "That if you look at the eigen pairs to the system here.",
            "That you look at the eigenvectors X to this linear system.",
            "This generalized systems.",
            "Those are precisely the fundamental modes of your spring mass system.",
            "Um?",
            "And.",
            "So this somehow didn't work, so let's see here.",
            "So there's some way I'm supposed to go back to get that picture to work.",
            "So.",
            "Let's see."
        ],
        [
            "So there was Sam.",
            "Sorry here, let me see what went wrong here.",
            "Shout it before.",
            "Some silly question here.",
            "So, so in particular, here's a simple example.",
            "So what this example is, is a.",
            "At the picture of a artificial image.",
            "And don't did it once here.",
            "Let me do it once more.",
            "So I image where we have now masses at the nodes we have heavier weight Springs.",
            "Blue Springs are stronger and the green Springs are weaker.",
            "I can see I haven't worked this out here.",
            "And you can see that in the fundamental mode, which interesting that happens here, is that these green Springs are being stretched a lot where the blued Springs are not being stretched much right?",
            "And those are just a simple mode of vibration of the screen system.",
            "OK, so anyway?",
            "OK so.",
            "Oh, now it's showing anyway.",
            "OK, so this is anywhere nice picture, right?",
            "So so you can think of and what we'll talk about later is these ideas of like she and Malik to compute eigenvectors for images as pictures of this form, right?",
            "And this is very valuable way to view Shi and Malik.",
            "Those who use it.",
            "No, it will talk about it later as as simply taking the image and shaking it and look at the modes of vibration.",
            "I see I have my choice.",
            "I can either show the movie is indefinitely.",
            "So now how do I get it to go up on?",
            "That's interesting.",
            "OK, sorry.",
            "So maybe I'll just.",
            "OK."
        ],
        [
            "So, so that's so.",
            "Those are sort of simple applications of it, so let's go on now to understanding.",
            "So assuming we want to still go back and solve laplacian's, how should we go about coming up with interesting preconditioners, right?",
            "So from a numeric POV, people took things that look geometrically nice in the matrix, right?",
            "So, so Jacobi, propose a diagonal Gauss Seidel proposed the upper triangular part, so now is a graph theorist.",
            "What are some natural subgraphs of a graph that may be of use to us right?",
            "Well?",
            "And 93 Vaidya proposed a very interesting idea.",
            "Basically, it's just that all he really needed to do was do the sound bite, and that was enough, but he did much more and that is he said, well.",
            "I'm a graph theorist and I want to find a graph that's very similar to my original graph, which I want to solve for my Laplacian.",
            "What's a graph that has that property for which direct methods will be fast, but yet somehow it's close in some sense to my original graph and the particular thing he proposed is let's pick a Max weight spanning tree.",
            "OK, so it's clear that once he says this idea and you go, that's a clever idea, right?",
            "And then from then there's been a whole line of research of how to extend this idea on.",
            "But anyway, so the idea here is, you know, we're saying we want to solve a linear system.",
            "The non zero structure of the linear system here is shown with red and green dotted edges an what Vaidya proposes then is what we should do is take a Max weight spanning tree for this and use that as our preconditioner causes recalls that we said that.",
            "If we.",
            "If we take a spanning tree and we do direct methods will be no fill.",
            "An linear work.",
            "Actually we should have gone through that example, but in particular, if you want to do Gaussian elimination on only the red graph, all you need to do is remove the degree one nodes 'cause that says.",
            "When you remove this node, it says form a clique out of the neighborhood where the neighborhood single.",
            "So it means you do nothing.",
            "So you simply remove the degree one node and then therefore now this is of degree one.",
            "So you remove that.",
            "You remove this, then you can remove this and you can keep going and with it if it's a spanning tree you can remove all the nodes of the graph without causing any fill.",
            "And in linear work OK, so in particular that's why spanning tree is unnatural.",
            "First cut at a good preconditioner, right?",
            "Because remember it.",
            "Hopefully it has good.",
            "It speeds up the iteration and at the same time it has a nice property that the solve using the spanning tree will be fast.",
            "OK, so that's his idea.",
            "It still works.",
            "Something here.",
            "Not this crazy thing."
        ],
        [
            "I.",
            "So right, so let's just quickly write some advantages down, so simple advantage.",
            "Then this is that.",
            "Of course, it's easy to find an easy to solve, right?",
            "We know how to find spanning trees in linear time, and we know how to solve them for them quickly, right?",
            "So one unfortunate problem is is that if all the edges in the graph actually have very similar edge weights, then any perturbation in the edge weights is going to force."
        ],
        [
            "To pick a very different spanning tree.",
            "So, and that's not going to be good in terms of the condition number.",
            "Not only that, actually it's not too hard."
        ],
        [
            "So what happened then is is that?",
            "In 2005, also another spanning tree was proposed, called the low stretch spanning trees, which get around this problem won't go into the definition here, but there's another definition of spanning tree which then removes this.",
            "This this problem of having lightweight edges perturbing the."
        ],
        [
            "Spanning tree you get.",
            "And the advantage with these is that they have much better condition."
        ],
        [
            "Number one problem is is that we don't know how to find them in linear times with their super."
        ],
        [
            "Linear define another problem is that they're still not really very good if you take even at the simple square mesh.",
            "As the underlying system you want to solve that there is no good spanning tree for that, right?",
            "That's one of these results."
        ],
        [
            "From 2004.",
            "So grim bomb and I in 94 proposed that what we really should do is just take a Steiner tree.",
            "So the idea here is instead of taking this graph and requiring ourselves to pick a sub graph, why don't we just start allowing ourselves to have Steiner nodes?",
            "So in particular, what we did here is is these nodes then are the original variables and we've just introduced new variables to form a tree."
        ],
        [
            "Good.",
            "So so in fact, for the square mesh, this gives dramatically better condition number and you."
        ],
        [
            "And solve them much faster.",
            "And also experimentally, this works quite well substantially better than any."
        ],
        [
            "The other spanning trees.",
            "Unfortunately, we never really figured out how to find a good way to compute these trees.",
            "In general.",
            "I mean, it's a nice hard problem, so."
        ],
        [
            "So we haven't made any headway there in 2007.",
            "Could assume myself proposed using Steiner forest.",
            "So the idea now is all we're going to do is we're going to take this underlying graph, and we're going to find highly connected pieces in this graph.",
            "I think in this case it probably doesn't actually exist that way, but the idea now is what we're going to do now is just take small pieces of this graph, and we're going to make a signing Steiner tree for each one of those pieces and use that as our preconditioner.",
            "Well, actually more precisely.",
            "So the advantage of this is that it's easy to find these things they work well with recursive solve."
        ],
        [
            "So we haven't got to that yet.",
            "So the next thing to do is is is show how to recur."
        ],
        [
            "Simply use these ideas.",
            "And also they give good XP."
        ],
        [
            "Elemental results will show those OK.",
            "Unfortunately, at this point we still have only been able to analyze these for the planar system.",
            "So for those we can show good runtimes."
        ],
        [
            "Linear time OK, so so the second idea is advised.",
            "You had is is that what you should do?",
            "But we're now applying this to our Steiner system.",
            "So the idea now is again, these are original variables.",
            "These are our Steiner variables and these are the edge weights in this new graph.",
            "So the idea now that Vaidya proposed, he said, well, find your preconditioner.",
            "And suppose it's not a spanning tree, but actually has a few more edges like.",
            "I think this has one extra edge in this case, right?",
            "If we remove this edge, then we get a tree.",
            "But suppose in general we may have a lot more other edges.",
            "Let's just remove those edges which are easy using direct method.",
            "So in this case will remove all the original variables OK, and then we'll end up with a much smaller system.",
            "And then what we'll do now."
        ],
        [
            "Is take this system here which is now reduced and instead of trying to solve that directly, why don't we recurse?",
            "Will just now ask for a recursive solver to solve this simple this newer system and hopefully this newer system is much smaller than the old system, so it will get a geometric increase, decrease in problem size, which will give us a good solver."
        ],
        [
            "Um so.",
            "So whether we worry about this so so maybe I should just.",
            "Talk about congestion dilation, right?",
            "So hopefully you all know this stuff.",
            "I'm not sure why this should be here, but if you have a simple recurrence relation of this form right like in the case we started with and you want to analyze how fast these."
        ],
        [
            "Things converge, a standard result says that if you let the air be simply your guess minus there actually."
        ],
        [
            "Answer To the problem that in fact the guest then the error at time I is simply the original error times that the G."
        ],
        [
            "The power of your matrix.",
            "OK, so all you need to then show this is that this matrix here goes to zero as I go."
        ],
        [
            "To Infinity, right?",
            "I got it."
        ],
        [
            "So that gives us then.",
            "So let's let's remember, I guess we already find it before, but if we have a system of equations of this form ax equals Lambda B, then across will call lamb."
        ],
        [
            "Under an eigenvalue and action eigenvector, in our case, here all our eigenvalues.",
            "Then our semi are greater than or equal to 0.",
            "So since it applausi and actually there's a 01, and then there's a bunch of other ones, right?"
        ],
        [
            "Suppose we sort these so the condition number is simply defined to be of this matrix.",
            "The largest eigenvalue divided by the smallest eigen."
        ],
        [
            "OK.",
            "So then standard results says that the convergence rate for the basic iterative method is one over the condition number, so this ends up being very."
        ],
        [
            "Or if you then go to conjugate gradient, standard result says that you get to take this."
        ],
        [
            "Square root of the condition number.",
            "So with this roughly says for simple examples is that the rate of convergence.",
            "In other words, if you want one bit of accuracy, the number of iterations has to be something like the diameter of the graph, right?",
            "So if in other words you take the square root of N by square root of N grid, and you do conjugant gradient, then you need square root of N iterations to get one bit of accuracy.",
            "So, so this is class."
        ],
        [
            "Analysis.",
            "So what we want to do now is bound the condition number of B inverse A because that's what we're iterating on instead of simply."
        ],
        [
            "OK right good.",
            "Well, another way to write that is is simply if we're interested in the eigenvalues of this system, is that's equivalent to the generalized eigenvalues of the form."
        ],
        [
            "X equals Lambda BX OK.",
            "So Lam just called the generalized eigenvalue OK, so in general, then what we're interested in is the condition number again, so we're interested in."
        ],
        [
            "The eigenvalues of this system here, so the largest divided by the smallest and that will determine our rate of convergence in the enorme anyway."
        ],
        [
            "OK. Good.",
            "So one thing that happens in this case, then is as we are look at.",
            "So our matrix are semidefinite, right positive semidefinite definition simply says that for all X not equal to 0, or I guess I don't need that here X transpose axes."
        ],
        [
            "Great and equal to zero.",
            "OK, so a standard definition that makes all the analysis works here is is that we're interested in the support of A by B and this is just simply I want to minimize.",
            "I want the minimum Lambda such that Lambda B -- A a semi positive definite.",
            "So what this means in terms of electrical circuits, it says that I have two electrical circuits, 1A and 1B, and what I want to know is how many copies of the electrical circuit be do I need in order to be able to carry as much current for all experiments as a."
        ],
        [
            "Good and then the condition number here is just the product of these things, so it ends up happening now which is from an electrical point of view.",
            "What we're going to do if you are two graphs is electrical circuits and what we're going to want to know is how do we then support.",
            "A by B and support be by A and that's going to determine our runtime OK."
        ],
        [
            "And so how do you do that?",
            "So the classic thing here is using what are called path embeddings.",
            "So I suppose that we have two graphs.",
            "With vertices VG and VH, then a path embedding is simply a map from the edges of G. To pass in, not then, but pass in H such that the past begins at VI and ends at VJ.",
            "OK, so let's see."
        ],
        [
            "Do a small example here.",
            "So suppose in this example, now we have two graphs.",
            "One is the complete graph and the other graph is the four cycle.",
            "So suppose we have our four cycle here.",
            "Anne Green and then the red graph here is actually the K4, and so there's a natural path embedding, namely, this red edge will simply math map to this green edge.",
            "This red edge will map to this screen edge.",
            "This one here just here, but this edge here doesn't exist.",
            "In the original 4 cycle.",
            "So why don't we just map that to the path that uses this green edge?",
            "Then the screen edge and at the same time why don't we map this red path here to these two green edges so the net effect now is.",
            "So I guess it's in the bottom of the slide here.",
            "So so in this example then what happens is a critical thing that makes all this analysis work.",
            "Then is the following an that is in this example?",
            "There's going to be congestion.",
            "Not sure what happened here, so the congestion in this case then is going to be what well?",
            "This edge then is used by three pass, right?",
            "Namely this edge.",
            "This edge in this edge right in the original K4, so the congestion here, and that seems to be the worst, right?",
            "This has congestion too.",
            "One and two, so that also the worst congestion is 3.",
            "The dilation here.",
            "In this case, well, this red edges map to an edge the same link, so it's not dilated at all right?",
            "This edge here.",
            "This red one is mapped to a path of length two, so the dilation is 2.",
            "So a natural thing then is is that the congestion times the dilation in this case is 6.",
            "OK, So what I want to claim then and there is that here in the slide.",
            "Let's just check and see.",
            "Um?",
            "So now let's go back.",
            "Go back here.",
            "So if I go back, so I guess that's missing here.",
            "So on the other slide.",
            "So this is so in this case here, then a standard theorem, then it's not too hard to show is that the support then of K4 by the cycle of size 4 then is.",
            "Is bounded by 6.",
            "Good.",
            "And so so therefore, and on the other hand, clearly the support of C4 by K4 since it's the sub graph.",
            "You just do the path embedding as a unit 1 is less than or equal to 1.",
            "So this says that the condition number of the Laplacian for K4 and the Laplacian of C4 then is less than or equal to 6.",
            "Shut.",
            "C as a cycle of length four.",
            "OK, thank you.",
            "OK, So what that says then is that in this particular case that if we preconditioned K4 with a four cycle.",
            "We would expect to have to do about 6 iterations per bit.",
            "If we did the naive method an.",
            "If we did conjugate gradient, then we'd need to do sqrt 6 iterations, But of course there's big constants in there, right?",
            "And so this is the technology.",
            "Then that's used to be able to analyze one quality.",
            "Precondition to another good OK.",
            "So I guess what I'd like to do actually is just to."
        ],
        [
            "Talk about historically what's happened in terms of solvers for the.",
            "The planar case.",
            "There's still a classic place for which at least the theoreticians have cut their teeth, and so if I go back and you sort of say well prior to the 50s, nothing was known, and so you could say that the runtime was MN cubed.",
            "Even for planar laplacian's in the 50s conjugate gradient came around.",
            "This gave a a runtime of N ^2.",
            "Bye."
        ],
        [
            "The analysis I guess we didn't finish that, but because of convergence rates and then, Lipton, Rose and Tarjan proved that any planar graph could be done in end of the 1.5."
        ],
        [
            "We've talked about Vaidya, then gave an algorithm using preconditioned iterative methods, which is end of the."
        ],
        [
            ".2.",
            "Then using low stretch spanning trees, Spielman and Tang showed how to get this down to and logs."
        ],
        [
            "Where Dan and more recently we've shown that you can actually get this down to linear time, so it's known that any planar linear system now can be solved in linear time.",
            "So of course the punchline is is that it says that if you want to do a forward multiplier and inverse power up to constants, those are all the same, so you should then and let me show you some examples to actually quantify the exact cost of doing an inverse power over forward power, but it says in these cases that.",
            "There they cost about the same, right?"
        ],
        [
            "So, but I should point out that there's very famous result in this area and this is due to Spielman and Tang and what they were able to show is that if you take any Laplacian, you can solve this in time M + N + M. Remember ends the number of variables M, the number of nonzeros in the matrix, and the twiddle here means that we're ignoring logs, and I think it's building pointed out in his talk yesterday.",
            "Number of logs as somewhere bigger than 29, right?",
            "So so it's not exactly practically yet, but it's some huge number of logs, but at least theoretically this is very interesting because it says that with more work chances are the right answer is is that it's in log in with some very few logs, right?",
            "And I'll show you some experiments on that we've done with other other solvers, OK?",
            "Good."
        ],
        [
            "So let's go back and actually look at this solver.",
            "Youngest Curtis is actually developed a solver at CMU, using what we call common troll multigrid, and we've compared this to the MATLAB solver.",
            "And what's interesting here is is that the map and this is for 2D images, which will talk about more, and you can see that this is a slightly better method than the others.",
            "What's interesting here is that what happens in the Matlab solver, at least for the machines we have.",
            "So is very quickly we end up running out of.",
            "Too bad I can't see the numbers here, right?",
            "So I think this is about somewhere around .7 million variables.",
            "This thing starts to breakdown, right?",
            "We can no longer solve these things.",
            "OK, because we run out of memory in Matlab.",
            "Good, and it seems like the direct methods of anything.",
            "That's the one downside is they quickly run out of memory, so in 3D."
        ],
        [
            "Again, in millions of variables here.",
            "So what happens is that we again get this nice linear runtime.",
            "So this is in seconds to solve one of these systems and you can see that Matlab quickly just runs out of memory, 'cause again we said that if you're doing direct methods that Phil is going what's going to cost.",
            "That's end of the three halves.",
            "And so these things run out of memory and crash, right?",
            "So?",
            "Is there?",
            "See."
        ],
        [
            "So, so I guess what I'd like to do and then anyway, so that's our solver stuff.",
            "And so the hope was to eventually try to get solvers out.",
            "'cause it seems like this is a natural community that would should have access to good solvers.",
            "What I'd like to do though, we spend a few minutes on how some one generalization that we have Laplacian.",
            "And unfortunately I don't know any interesting applications, and one of the reasons I want to do this here is hopefully in showing you this reduction.",
            "One of you are many of you will see how to apply this to machine learning, so let's assume that that instead of being a graph Laplacian, this is simply a symmetric diagonally dominant system, right?",
            "This simply means that the diagonal then is greater than or equal to the sum of its off diagonal.",
            "The absolute value of its off diagonals.",
            "So the goal of the rest of this talk then, is to show that these systems can be solved can be reduced to simple regular laplacian's.",
            "OK, good."
        ],
        [
            "So, so in our case now the other way to think of these these symmetric diagonally dominant systems.",
            "Now is we can still take every off diagonal edge.",
            "And what we can do now is simply say that we have an edge from I to J, then the function WIJ is not zero for that value, right?",
            "'cause it's so now will just allow WIG to J to be."
        ],
        [
            "Good, so now this is just a weighted let's let a be the weighted incidence matrix as we defined it before.",
            "Now let."
        ],
        [
            "To find the degree of the system to simply be.",
            "Some of the absolute values of the edge weights."
        ],
        [
            "And then we can now get, let's call this a generalized Laplacian.",
            "Things of the form LD equals.",
            "I also notice that of course this system here is a symmetric, diagonally dominant.",
            "The only thing that's missing here is is that we are requiring here that the diagonal not be arbitrarily big with respect to a.",
            "That really doesn't matter.",
            "You can actually make the diagonal bigger, but let's just worry bout how would we solve these kind of linear systems where we allow.",
            "The elements in here at the off diagonals to be positive.",
            "Now they need not be negative."
        ],
        [
            "I.",
            "So this is just a simple generalization of regular glass repossi.",
            "OK, good, so the."
        ],
        [
            "First thing I want to point out is is that if we have one of these matrices.",
            "That we can write out this quotient.",
            "The Rayleigh quotient or the numerator of the Rayleigh quotient and the following form, right?",
            "So this is all the same thing as usual.",
            "I don't think I'll go through derivation.",
            "What happens is you get two kinds of terms.",
            "Now you get terms when the WIJ is positive.",
            "That looks like.",
            "Right so.",
            "Start word.",
            "Great now, so this is just the standard term we get for the regular Laplacian, right?",
            "We get things of the form WIJ times I exhibi, minus X sub J squared.",
            "But now what happens is for the terms for where the edge weights in the original graph are negative.",
            "What happens then?",
            "Is is we just take the absolute value or you put a - here and you get another sum of positive terms.",
            "OK, good.",
            "So the."
        ],
        [
            "Says that it notice that these generalife laplacians are still positive semidefinite.",
            "They may not be positive definite because they include clearly the regular Laplacian, so we can't expect that in general right?",
            "So.",
            "Crazy.",
            "Chrome.",
            "OK.",
            "Sorry.",
            "Good.",
            "Um?"
        ],
        [
            "So, but I claim that it's not too hard to see that that if they rank is still an minus one if G is connected, right?",
            "So we have that."
        ],
        [
            "So suppose we want to solve these systems using Laplace iiams.",
            "Let's consider the following very simple idea and that is let's do a change of variables to see if we can't get it into the form of a regular Laplacian.",
            "OK, so so suppose we take one of these things then."
        ],
        [
            "You multiply one of these matrices, the I TH column and the I throw by minus one.",
            "Then I claim if you work this out because we're multiplying the same column and row by minus one, that means the diagonal won't change sign.",
            "It will still be positive and all we've done is flip the sign on the off diagonal elements, which does not change the diagonal dominance of the system, right?",
            "So what this says is that."
        ],
        [
            "Is.",
            "This is equivalent to doing a change of variables where we flip the sign on the variable XI to minus XI in the variable and DVI to minus BI in the system.",
            "We want to solve.",
            "OK, so let's just do a small example to make sure we understand.",
            "So suppose now we take a generalized graph, it has edge weights.",
            "Some of these are negative and some are positive, like a Reg."
        ],
        [
            "Laplacian so we make up the matrix.",
            "What is it going to look like?",
            "Well, we again take the sum of the absolute value of the edge weights out.",
            "So in this case we have two minus ones or two for the century 2, so they're all twos on the diagonal, just 'cause it's a trivial example.",
            "And then of course we take minus the sign.",
            "So for V1 it has two edges out, one to V2, which is negative.",
            "So we change that to a plus it has another edge, which is plus we change to a minus, right?",
            "So this then is a generalized symmetric diagonally dominant system.",
            "Corresponding to this graph, we can see that good, and so we'd like to solve such a system, and so one idea is is why don't we try doing a change of variable?",
            "Let's multiply, say the first row by minus one in the first column by minus one.",
            "OK, so if we do that, what happens that means we change?"
        ],
        [
            "Ames assign on everyone on the 1st row except for the diagonal.",
            "We change the elements on the 1st Column by that, but I claim that we can now make up a new linear system where we put a - on X1 and a - on the B1.",
            "And if you work it out then.",
            "This works out correctly right in the sense that now what's going to happen here is is we're going to get this term here.",
            "This term is negated.",
            "This term is negated, so therefore when we multiply this vector times this row, we're just going to get a -- B one, just like we want.",
            "And when we do the dot product with any other row, we're going to get the same old value back.",
            "OK, So what I claim then is if you want to solve this system.",
            "You could try multiplying a given column in a given row by minus one and seeing if that helped, and then when you got done you would just have a change of variables.",
            "Here you would change the buys an you would change the excise accordingly, right?",
            "So at anytime you can do that and it doesn't change the underlying system you're solving."
        ],
        [
            "So let's make a definition.",
            "Let's say that that an underlying graph is orientable if there exists a sequence of flips such that all the edge weights go positive, OK.",
            "So did we define that?",
            "Let's just go back here.",
            "OK. OK.",
            "So we should have have so, so in general, what are we doing here when we're doing a flip at a vertex?",
            "So what are we doing?",
            "Graph theoretically has a very similar meaning is simply says.",
            "What we do is we're taking a graph here.",
            "Write an we've picked out a vertex VI and what we're going to simply do is multiply all the weights of the edges here by minus one.",
            "OK and so at anytime we can do a change of variables which simply graph theoretically means you change all the edges to have the opposite signs that they had before.",
            "OK, good an.",
            "So suppose you have a graph.",
            "One possibility is it you're very lucky and what you can do is change all.",
            "All the do flips until you eventually get a regular old vanilla Laplacian.",
            "Remember this vanilla Laplacian and then you go solve it and just remember what flips you did to figure out what your answer is up to.",
            "A change of sign.",
            "And do that OK. OK, so."
        ],
        [
            "Of course, this is just a."
        ],
        [
            "So.",
            "So notice now will make another definition, will say the matrix is orientable.",
            "Of course, if the graph is right.",
            "So I claim that Orientability is very similar to what's called 2.",
            "Coloring in graph theory, and it's very easy to figure out.",
            "You just do a simple greedy algorithm.",
            "OK, so of course we could then figure out quickly whether we have an orientable matrix, and if we do, then we just solve that using Spielman Tang or the state of the art method, right?",
            "If not, and it's not orientable, then we have to go figure out how we're going to solve these.",
            "So in general, of course they shouldn't be orientable, but.",
            "We wanted to."
        ],
        [
            "Eliminate those from our list OK?",
            "So let's let's prove something simple here.",
            "Suppose that G is connected and not orientable.",
            "Then I want to claim its Laplacian, then is now symmetric positive definite is no longer semidefinite."
        ],
        [
            "And let's do a quick proof.",
            "So let's do it by contradiction.",
            "Suppose that LXX transpose I'll ax equals zero an X is not equal to the all zero vector.",
            "OK, then we can pick a spanning."
        ],
        [
            "Free for G and we can Oriente by flipping.",
            "All the edges, in other words, make all the edges in some spanning tree positive OK.",
            "So therefore, since all the."
        ],
        [
            "Edges are positive in terms of formula.",
            "The value at all those vertices in X has to be equal, otherwise we're going to get a non 0 value here.",
            "So this implies that X has to be, you know up to normalization the all ones vector, which is a contradiction because she still has some negative edge weights.",
            "So it's not too hard to prove.",
            "I think it's almost easier to work it out yourself, but I claim that now we have a positive definite system where that's interesting, OK?"
        ],
        [
            "Here's how we're going to solve these kinds of systems.",
            "We're going to do what we call a two fold cover.",
            "So what I'm going to do is I'm going to take the original linear system and I'm going to double the number of variables for you.",
            "You don't have to do this in practice, but this is a nice way to think about it.",
            "An in that twofold cover.",
            "It'll be a regular Laplacian.",
            "OK, here's the idea.",
            "So what we'll do now is."
        ],
        [
            "I'm in the middle of defining A2 fold cover OK. And so it's going to have two N variables.",
            "It's going to have a regular variable.",
            "Ann is going to have a complementary variable, so there's a variable VI and VI compliment, and so we're going to do that, OK?"
        ],
        [
            "And So what we'll do now is if it's the case that the original edge WI J is positive.",
            "In other words, it's a regular edge.",
            "Then we'll simply connect together VI and VJ and Connect together VI Baran Vijay Bar with an edge with this weight on it.",
            "If it's on the other hand."
        ],
        [
            "It's a negative weight edge.",
            "What we'll do then is will connect together VI with Vijay Bar and VI bar with Vijay OK.",
            "In other words, we put a twist in it.",
            "If you're into electrical engineering, this is just a double rail logic.",
            "We just put a twist in to negate and then will negate the edge weight OK?",
            "And so this is called A2 fold cover so."
        ],
        [
            "Just in case we want to do a small example.",
            "So the idea with this example here is we take our original graph, the two.",
            "This has three vertices, so the two fold cover should have six vertices.",
            "They should come in pairs, so in this case here we have V3 and VN and V3 compliment.",
            "We have V1 and V2 complement V2 and V2 complement.",
            "These OK, so somehow this is not the right graph.",
            "Unfortunately.",
            "So so can I draw this graph for you over here instead, 'cause it looks like there's a bug in this example.",
            "OK, so and then I can get.",
            "So avert our graph, then look something like this, right?",
            "So this is this is 1.",
            "This is V1 bar.",
            "This is V2V2 Bar V1V3V3 bar.",
            "There is 1 positive edge so that says you should hook these together in the usual way.",
            "Right?",
            "There says these these two here have the property that.",
            "That there are negative edge weight.",
            "So we should put a twist in here.",
            "The bottom edge also has a negative edge weight, so this should also have A twist.",
            "Right?",
            "And so this now is called A2 fold cover of that original graph, OK?",
            "Ann, you should both ignore the fact that there's a meeting I'm missing, and the other example that's there.",
            "So let me quickly remove the meeting.",
            "OK, so everyone understand the construction is very simple construction.",
            "What we've done is an hopefully save some use.",
            "We've taken a general graph with positive and negative weight edges, and we've got a new graph with just twice as many variables.",
            "But this graph has a nice property that it's a regular vanilla Laplacian.",
            "All the edge weights are non negative or positive, right good, and so I claim that that's useful.",
            "And the reason?",
            "It just follows."
        ],
        [
            "So let's let's let's let our original graph BG and let US Laplacian BL.",
            "Let's let G Barbie our new double cover an L Barbie the Laplacian of that OK?",
            "OK, so notice that L bar then up to a factor of two in the variable count is now a vanilla Laplacian so we can solve."
        ],
        [
            "All that using regular methods.",
            "Of course we shouldn't really do that, but at least theoretically we could just do that OK."
        ],
        [
            "And note the following thing that suppose we had a solution to this system.",
            "Then you should convince yourself that if we order the variables so that we have X1 to XN X1 bar to XN bar Ann B1 through BN, and then B1 through BN bar that all we did is introduce 2 copies.",
            "So what happens now?",
            "Is that we take our solution to this equation and we make 2 copies, one with X and one with minus X.",
            "We take B and minus B&I claim.",
            "We have a solution to this system.",
            "You should work that out.",
            "It's not too hard to see that what happens now is that since these are all, if we negate the value that we have for X in here with one in here that it does the right thing because of these twists, you should convince yourself that it works.",
            "So that says that.",
            "One Direction which is not the interesting direction is true, but it's not."
        ],
        [
            "Hard to see that.",
            "On the other hand.",
            "This matrix here is a regular Laplacian right, and it's a connected graph.",
            "We should approve that because the two fold covered is connected.",
            "Now matter fact it's an orientable graph if and only if when you make the two fold cover you end up with a disconnected graph.",
            "So this ends up being connected so we know the rank of this system.",
            "So suppose you had a solution to this linear system.",
            "Then this this is not a full rank, right?",
            "So this has rank in this case 2 N minus one, so you could do simple.",
            "You could take one solution and you could add copies of the all one vectors, but in particular that says that if you take the average of X&X&Y this is a solution to the original system.",
            "OK, so why is this interesting?",
            "It says that what did we do?",
            "We generated a new matrix.",
            "We send this off to a solver.",
            "It gives us back an answer of the form XY.",
            "We simply take the the.",
            "The average of those, the difference, half the difference between those two solutions.",
            "And magically that's a solution to our original system.",
            "OK, good.",
            "So let me just need to quit here.",
            "So let me just."
        ],
        [
            "So you can do that without the fat."
        ],
        [
            "After 2.",
            "So somehow and we had never figured out and recommendation problems like Netflix.",
            "This should be of some value, but we haven't figured out how to do that.",
            "People have looked at that OK."
        ],
        [
            "It doesn't seem to do exactly the right thing, so maybe I should quit here.",
            "I guess I took more time, so let me just go."
        ],
        [
            "On and so I was going to talk about spectral graph theory, but I see that I don't have time for that.",
            "So let me just.",
            "Go over here."
        ],
        [
            "And just talk about 'cause I should.",
            "That's a nice topic, but that's a whole another talk.",
            "So one of the questions now.",
            "So it would be nice to have methods that work for any symmetric positive definite system, right?",
            "We've looked at a few when they're diagonally dominant.",
            "We know how to solve these things, but in general unfortunately this everything breaks down as soon as you have.",
            "It's not diagonally dominant.",
            "So the other question is which I didn't get a chance to talk about his spectral graph theory, so again, that's very nice.",
            "These one of my favorite questions of all is how to use more than one eigenvector for a graph in order to do good cuts.",
            "That's a very beautiful question.",
            "What it also like to do is find solvers at work in the L2 norm.",
            "Notice the solutions we're coming up with actually in the enorme.",
            "So it also be nice to take something like the Spielman Tang algorithm and get those kind of guarantees with something that actually works in practice, right?",
            "And maybe I should quit at that.",
            "OK, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now it's my pleasure to introduce Gary Miller from Carnegie Mellon.",
                    "label": 1
                },
                {
                    "sent": "Thanks very much, Gary.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you for inviting me.",
                    "label": 0
                },
                {
                    "sent": "Some pleasure to give a talk.",
                    "label": 0
                },
                {
                    "sent": "So hopefully this will be understandable.",
                    "label": 0
                },
                {
                    "sent": "So again, please ask questions.",
                    "label": 0
                },
                {
                    "sent": "This is a workshop, not a. Pre programmed talk.",
                    "label": 0
                },
                {
                    "sent": "So today what I'd like to talk about is this area, which is called now spectral graph theory, right?",
                    "label": 0
                },
                {
                    "sent": "This is the idea that you think of a graph as a matrix and look at its eigenvectors and you think of a matrix as a graph and do graph theory to the matrix OK, and so in particular we're going to look at questions about how do you quick.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We solve linear systems and again that's going to reduce to graph graph questions, and then we'll also show applications where we use spectral graph theory to solve some of these questions.",
                    "label": 1
                },
                {
                    "sent": "So of course, the simplest of all constraint systems that have been around for hundreds of years is the notion of a linear system, and this is very fundamental in all the workers.",
                    "label": 0
                },
                {
                    "sent": "Everyone is every talk I've seen so far under the hood somewhere they're solving linear systems, and so it's critical to be able to solve these systems quickly.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course obvious thing to do is write it as a matrix.",
                    "label": 0
                },
                {
                    "sent": "I guess it's interesting point, I believe that Gauss did not understand this notation, so that's one of the reasons I've never been able to read of a lot of his original work is that he didn't know about matrices with much later it was discovered, but a very beautiful way to think of the world rise matrices.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's interesting is it is theoretically anyway, if you want to solve dense matrices, you can do that in Ncube time.",
                    "label": 0
                },
                {
                    "sent": "Strassen then showed how to do that in end of the two point 8 and then times been improved.",
                    "label": 0
                },
                {
                    "sent": "Most of these then, are of course way too large for the kind of systems we'd like to solve today, and so it's still a very big number, right?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's interesting is that it's not if it's just a general sparse system.",
                    "label": 0
                },
                {
                    "sent": "We don't have a better theory.",
                    "label": 0
                },
                {
                    "sent": "Worst case analysis to say much better about.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are easy cases for the simplest of all easy cases.",
                    "label": 0
                },
                {
                    "sent": "The upper triangular or lower triangular we can do back substitution, anwen.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do this, then we get to solve it in a number of non zeros in right?",
                    "label": 0
                },
                {
                    "sent": "So the goal one goal then would be to find a bunch of other.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For linear systems, all of which we can then solve as fast as we can solve an upper triangular system OK, and so that's their goal.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first obvious constraint that people put on with is that it's symmetric, so throughout the rest of this talk.",
                    "label": 0
                },
                {
                    "sent": "Then we'll assume our matrix is symmetric, is symmetric, and of course is a graph that means our graph is undirected.",
                    "label": 0
                },
                {
                    "sent": "It would be nice to get rid of this constraint.",
                    "label": 0
                },
                {
                    "sent": "I don't know how, but tonight, but anyway.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will use that.",
                    "label": 0
                },
                {
                    "sent": "So another assumption throughout this talk is is going to be to start with.",
                    "label": 0
                },
                {
                    "sent": "That is positive definite, right?",
                    "label": 1
                },
                {
                    "sent": "Namely that all the eigenvalue values are strictly positive, or that Rayleigh quotient is always positive, right?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so the first approach that people started doing.",
                    "label": 0
                },
                {
                    "sent": "I guess I just decided to do a history of some of the standard computer science literature and solving systems, and that is to look at direct methods, Gaussian elimination.",
                    "label": 1
                },
                {
                    "sent": "And this is still the method to beat if only if you only have a few thousand variables, and so that's where we're going to start, 'cause in fact, what happens in all modern systems is what you're going to do then is reduce the problem to smaller size and as soon as this fall size goes below a certain level, you're going to go to whatever the method that's best for that size.",
                    "label": 0
                },
                {
                    "sent": "So in particular, let's look at direct methods.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in the 70s graph theoretic interpretation of Gaussian elimination was popularized probably was known before that, but that is what you do is view every non zero off diagonal edge value is undirected edge and you view.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pivoting is a graph operation.",
                    "label": 1
                },
                {
                    "sent": "OK, so in particular, what is pivoting for?",
                    "label": 1
                },
                {
                    "sent": "And from a graph theoretic point of view, as you have some graph for your original.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you have a vertex and you'd like to pivot on that vertex.",
                    "label": 1
                },
                {
                    "sent": "So what you end up doing is you make a click out of the neighborhood of that vertex.",
                    "label": 1
                },
                {
                    "sent": "You would then remove that vertex and its edges from the graph and you repeat until you've eliminate.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The graph OK, so throughout this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process the new edges are formed by forming cliques.",
                    "label": 0
                },
                {
                    "sent": "Whenever the edge is not already there.",
                    "label": 0
                },
                {
                    "sent": "An also work is done independent of whether an edge is there or not.",
                    "label": 0
                },
                {
                    "sent": "You end up touching all those edges by changing their value, and so the work ends up being all the edges touch throughout the life of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the classic work from the 70s of Lipton, Rose, and Tarjan said that you can solve any planar linear system, in other words a system for which the undirected the nonzero structure is a planar graph, so 2D.",
                    "label": 0
                },
                {
                    "sent": "So for instance this is natural for like 2D image processing would come up.",
                    "label": 0
                },
                {
                    "sent": "You can solve those and end of the three halves work and then log filler space.",
                    "label": 0
                },
                {
                    "sent": "It's actually interesting that N log N ends up being important and we'll see.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in 3D, unfortunately the numbers are much worse, so if you have a 3D system like a 3D medical image, you want to do, or if there's somehow intrinsic manifold of three dimensions, then in general the work is going to be N squared work and it's going to be end of the three halves filler space.",
                    "label": 0
                },
                {
                    "sent": "And again, this will see that this is bad.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very problematic, we can show you very simple examples for which it is the kiss of death into the three halves space, so you can't afford to do that, OK?",
                    "label": 0
                },
                {
                    "sent": "So the next method that people proposed.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is iterative methods, so iterative methods are the simplest of all.",
                    "label": 1
                },
                {
                    "sent": "Litter method is the following recurrence relation right?",
                    "label": 0
                },
                {
                    "sent": "I want to solve the system ax equals B and it's the case that I consider the following recurrence.",
                    "label": 0
                },
                {
                    "sent": "I take my guess at time I X super I and I multiply it by the identity minus A and I add B and then it's an easy exercise to see that if I ever find a fixed point for this recurrence, I have a solution.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In my system.",
                    "label": 0
                },
                {
                    "sent": "And at the rate of convergence, then, is related to the norm of the matrix I -- A, which we'll get back to later on OK?",
                    "label": 0
                },
                {
                    "sent": "So and then.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, if you if you want to do fancier methods, those are all exists Chevy Chev iteration or conjugate gradients are all generalizations of this method.",
                    "label": 0
                },
                {
                    "sent": "Here we won't go into those, but there are slight but they improve the rate of convergence slightly and.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People do those all the time OK, but even again what we'll see is the conjugate gradient still is not fast enough for large systems, OK?",
                    "label": 0
                },
                {
                    "sent": "So so that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First idea that came along as the idea of preconditioned iterative method and so let me just explain this so that the idea now is is what we're going to do is we again want to solve the system ax equals B, but will multiply both sides of the system by a matrix B inverse.",
                    "label": 0
                },
                {
                    "sent": "This gives us a new linear system to solve, which is the same 1B inverse A X = B prime.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we just write out that same recurrence again, we get the equations we had it before, but the crucial point here is is that.",
                    "label": 0
                },
                {
                    "sent": "You should not multiply out the matrix I -- B inverse A into a big messy matrix and then just iterate.",
                    "label": 0
                },
                {
                    "sent": "What you should do is the following four.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what you should do now is first, let's just worry about computing this terms equals.",
                    "label": 0
                },
                {
                    "sent": "I think this might work actually.",
                    "label": 0
                },
                {
                    "sent": "So let's just multiple compute the term Z, which is B inverse AX super I.",
                    "label": 1
                },
                {
                    "sent": "Well what will first do is we'll do a forward multiply to compute Y which is a * X.",
                    "label": 0
                },
                {
                    "sent": "And then rather than do then the last step here is we'll do a solve for Z instead of computing the inverse, right?",
                    "label": 0
                },
                {
                    "sent": "So we think of.",
                    "label": 0
                },
                {
                    "sent": "This recurrence really is a hybrid between two methods, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a simple iterative method, but inside the inner loop is 2 methods of forward multiply, which we talked about before plus plus I back solve OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, So what the trouble now is, is with this kind of method is is that we have two costs right?",
                    "label": 0
                },
                {
                    "sent": "One is to minimize the number of iterations that where are our methods going to take and the second problem here is to minimize the cost of the solve these equals Y.",
                    "label": 1
                },
                {
                    "sent": "So we want to somehow be able to balance those things to those off so that we get a faster rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "OK, so just in case.",
                    "label": 0
                },
                {
                    "sent": "You you think?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I haven't seen this before.",
                    "label": 0
                },
                {
                    "sent": "Anyone that's taken a standard miracle analysis class on a solver you've seen preconditioned iterative systems all along.",
                    "label": 0
                },
                {
                    "sent": "The simplest is Jacobi B is just simply the diagonal of a.",
                    "label": 0
                },
                {
                    "sent": "Another",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classic example is is that Gauss Seidel B is simply just the upper triangular part because again, we now know that solving upper triangular matrices can be done quickly, so therefore that's the obvious thing to put in there, up to the fact that it's easy to solve for the upper triangular part of a. Numerically, it doesn't really make a lot of sense to do that, but people do it anyway, right?",
                    "label": 0
                },
                {
                    "sent": "It means a very natural thing to put in 'cause what else you're going to do, right?",
                    "label": 0
                },
                {
                    "sent": "But anyway, so that's a class.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing to do there is also symmetric successive over relaxation, which is you do every exotic combination of upper and lower triangular diagonal parts of your original matrix, and you put it into a big messy thing, and then you back solve that.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's another classic thing that people do.",
                    "label": 0
                },
                {
                    "sent": "OK, so very classic methods.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It can be thought of nothing more than preconditioned iterative methods, right?",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So these things still or are too slow and too unreliable in the sense of Gauss Seidel.",
                    "label": 1
                },
                {
                    "sent": "If it converges, great, and if it doesn't, it's not so clear what you do right?",
                    "label": 0
                },
                {
                    "sent": "So the problem is is that you know.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to generate solvers for which they work systematically, right.",
                    "label": 0
                },
                {
                    "sent": "What I would like to be able to do is generate a solver for which I can put it into a program, roll it into a doctors office, and have them use it 24/7 and not phone me up in the middle of the night and say the solver didn't work.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's other kinds of solvers, right?",
                    "label": 0
                },
                {
                    "sent": "So the big distinction here to make sure the reliability right?",
                    "label": 0
                },
                {
                    "sent": "So I would imagine if all we want is a solver for which you're going to go home and use it yourself.",
                    "label": 0
                },
                {
                    "sent": "Personally, if it fails one in 200 times about the failure rate of the shuttle.",
                    "label": 0
                },
                {
                    "sent": "That may be just fine, right?",
                    "label": 0
                },
                {
                    "sent": "I mean the spatial that's all you need.",
                    "label": 0
                },
                {
                    "sent": "If it's the case, you want to send this out and sell it as a product, then failure rates one 200 is a little bit as too high.",
                    "label": 0
                },
                {
                    "sent": "You need a lower failure rate.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to do is actually have algorithms that always work, so that's what I mean by unreliable.",
                    "label": 0
                },
                {
                    "sent": "Here most of the stuff works nice most of the time, but it will fail.",
                    "label": 0
                },
                {
                    "sent": "There's a problem, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in order to be able to come up with a theory for which we can claim that the algorithm works more systematically.",
                    "label": 0
                },
                {
                    "sent": "In other words, we can give guarantees.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, we have to start restricting the number of kinds of matrices that come up.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, if you look at most people in this room, they think this is a very natural restriction, and so they're not unhappy at all, or you're not unhappy, but but nonetheless it is a major restriction on the types of matrices were going to be able to solve.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about the graph Laplacian, which you all know.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Namely, you take a weighted undirected graph.",
                    "label": 1
                },
                {
                    "sent": "You assume all the edge weights are.",
                    "label": 1
                },
                {
                    "sent": "Strictly positive you define the incidence matrix.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You define the degree matrix, which is just simply the sum of the weights.",
                    "label": 0
                },
                {
                    "sent": "Giving you a diagonal matrix and from that then the Laplacian is simply D -- A.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we're going to do now, for the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "While then is we're going to restrict ourselves to solving linear systems where the underlying system is as a graph Laplacian.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so the graph Laplacian comes up in many places, probably every every other talk here.",
                    "label": 0
                },
                {
                    "sent": "But let me just make this just review some of the classic places that comes up.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons it's useful to do that is is that even if you don't care about these applications and you're still using a graph Laplacian, it's very useful to know that the system you're solving has very other multitude of other meanings.",
                    "label": 0
                },
                {
                    "sent": "So you're solving the system for some other meeting.",
                    "label": 0
                },
                {
                    "sent": "You know what's some marketing problem or something, but it may have some very natural meaning in terms of resistors or in terms of spring mass systems and other things, right?",
                    "label": 0
                },
                {
                    "sent": "So let's just go through some.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so one idea is your view.",
                    "label": 0
                },
                {
                    "sent": "Each edge is a conductor.",
                    "label": 0
                },
                {
                    "sent": "With conductance, WIJ.",
                    "label": 0
                },
                {
                    "sent": "So if you let V be a call.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vector of voltages than L V = C. Then C will be the residual current needed to maintain the given voltage.",
                    "label": 1
                },
                {
                    "sent": "Good OK, so I thought I had more or less like maybe not OK, so in particular what this says then is is that in order to maintain the voltage at those nodes, you're going to have to either inject current or remove current and the amount you have to inject are removed is precisely see.",
                    "label": 0
                },
                {
                    "sent": "You can also turn this on its head if you would know how much current you want to inject, then what voltages will it float too in order to maintain to have those currents OK, and so there's a classic way to view Laplacian, some very useful.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Good so another place this comes up that's basically the heat equation, and so that's a standard.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Place this comes up another place this comes up as in random walks again here the transition matrix ends up being simply D inverse are diagonal matrix we had before times the Laplacian.",
                    "label": 1
                },
                {
                    "sent": "An interesting question.",
                    "label": 0
                },
                {
                    "sent": "Then it is to compute the fundamental eigenvectors of these kinds of systems and one of the amazing things here is is that there's fast solvers which will talk about the Spielman Tang solver is very theoretical, but it's interesting in the sense that says that the problem won't get any harder.",
                    "label": 0
                },
                {
                    "sent": "Someone won't be able to prove that you could do it, that it takes more time.",
                    "label": 0
                },
                {
                    "sent": "So what ends up happening is, well, maybe this is the wrong place for it in the talk, but.",
                    "label": 1
                },
                {
                    "sent": "If you're interested in eigenvectors of graph laplacian's, so the standard theory would say you would try to do forward powers as a way to find those eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "So we found both experimentally and theoretically actually doing inverse powering is the trick that works much better if you in fact simply do take your initial guess for an eigenvector, and you simply then take the inverse power of that guess and repeat that process.",
                    "label": 0
                },
                {
                    "sent": "After login iterations, you'll converge to something with very small Rayleigh quotient, or in practice something that looks like an eigenvector.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a very powerful trick.",
                    "label": 0
                },
                {
                    "sent": "We should maybe go over that, but you know that that's very nice.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "So so the other place that laplacian's come up is in spring mass systems.",
                    "label": 1
                },
                {
                    "sent": "So again, suppose we have a weighted graph and suppose we view instead the constants as spring constants.",
                    "label": 0
                },
                {
                    "sent": "The WIJ is a spring constant.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And suppose we let M. Villa.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diagonal matrix of mass constants.",
                    "label": 0
                },
                {
                    "sent": "Then a standard result is.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That if you look at the eigen pairs to the system here.",
                    "label": 0
                },
                {
                    "sent": "That you look at the eigenvectors X to this linear system.",
                    "label": 0
                },
                {
                    "sent": "This generalized systems.",
                    "label": 0
                },
                {
                    "sent": "Those are precisely the fundamental modes of your spring mass system.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this somehow didn't work, so let's see here.",
                    "label": 0
                },
                {
                    "sent": "So there's some way I'm supposed to go back to get that picture to work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there was Sam.",
                    "label": 0
                },
                {
                    "sent": "Sorry here, let me see what went wrong here.",
                    "label": 0
                },
                {
                    "sent": "Shout it before.",
                    "label": 0
                },
                {
                    "sent": "Some silly question here.",
                    "label": 0
                },
                {
                    "sent": "So, so in particular, here's a simple example.",
                    "label": 0
                },
                {
                    "sent": "So what this example is, is a.",
                    "label": 0
                },
                {
                    "sent": "At the picture of a artificial image.",
                    "label": 0
                },
                {
                    "sent": "And don't did it once here.",
                    "label": 0
                },
                {
                    "sent": "Let me do it once more.",
                    "label": 0
                },
                {
                    "sent": "So I image where we have now masses at the nodes we have heavier weight Springs.",
                    "label": 0
                },
                {
                    "sent": "Blue Springs are stronger and the green Springs are weaker.",
                    "label": 0
                },
                {
                    "sent": "I can see I haven't worked this out here.",
                    "label": 0
                },
                {
                    "sent": "And you can see that in the fundamental mode, which interesting that happens here, is that these green Springs are being stretched a lot where the blued Springs are not being stretched much right?",
                    "label": 0
                },
                {
                    "sent": "And those are just a simple mode of vibration of the screen system.",
                    "label": 0
                },
                {
                    "sent": "OK, so anyway?",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Oh, now it's showing anyway.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is anywhere nice picture, right?",
                    "label": 0
                },
                {
                    "sent": "So so you can think of and what we'll talk about later is these ideas of like she and Malik to compute eigenvectors for images as pictures of this form, right?",
                    "label": 0
                },
                {
                    "sent": "And this is very valuable way to view Shi and Malik.",
                    "label": 0
                },
                {
                    "sent": "Those who use it.",
                    "label": 0
                },
                {
                    "sent": "No, it will talk about it later as as simply taking the image and shaking it and look at the modes of vibration.",
                    "label": 0
                },
                {
                    "sent": "I see I have my choice.",
                    "label": 0
                },
                {
                    "sent": "I can either show the movie is indefinitely.",
                    "label": 0
                },
                {
                    "sent": "So now how do I get it to go up on?",
                    "label": 0
                },
                {
                    "sent": "That's interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry.",
                    "label": 0
                },
                {
                    "sent": "So maybe I'll just.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so that's so.",
                    "label": 0
                },
                {
                    "sent": "Those are sort of simple applications of it, so let's go on now to understanding.",
                    "label": 0
                },
                {
                    "sent": "So assuming we want to still go back and solve laplacian's, how should we go about coming up with interesting preconditioners, right?",
                    "label": 0
                },
                {
                    "sent": "So from a numeric POV, people took things that look geometrically nice in the matrix, right?",
                    "label": 0
                },
                {
                    "sent": "So, so Jacobi, propose a diagonal Gauss Seidel proposed the upper triangular part, so now is a graph theorist.",
                    "label": 0
                },
                {
                    "sent": "What are some natural subgraphs of a graph that may be of use to us right?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "And 93 Vaidya proposed a very interesting idea.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's just that all he really needed to do was do the sound bite, and that was enough, but he did much more and that is he said, well.",
                    "label": 0
                },
                {
                    "sent": "I'm a graph theorist and I want to find a graph that's very similar to my original graph, which I want to solve for my Laplacian.",
                    "label": 0
                },
                {
                    "sent": "What's a graph that has that property for which direct methods will be fast, but yet somehow it's close in some sense to my original graph and the particular thing he proposed is let's pick a Max weight spanning tree.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's clear that once he says this idea and you go, that's a clever idea, right?",
                    "label": 0
                },
                {
                    "sent": "And then from then there's been a whole line of research of how to extend this idea on.",
                    "label": 0
                },
                {
                    "sent": "But anyway, so the idea here is, you know, we're saying we want to solve a linear system.",
                    "label": 0
                },
                {
                    "sent": "The non zero structure of the linear system here is shown with red and green dotted edges an what Vaidya proposes then is what we should do is take a Max weight spanning tree for this and use that as our preconditioner causes recalls that we said that.",
                    "label": 0
                },
                {
                    "sent": "If we.",
                    "label": 0
                },
                {
                    "sent": "If we take a spanning tree and we do direct methods will be no fill.",
                    "label": 0
                },
                {
                    "sent": "An linear work.",
                    "label": 0
                },
                {
                    "sent": "Actually we should have gone through that example, but in particular, if you want to do Gaussian elimination on only the red graph, all you need to do is remove the degree one nodes 'cause that says.",
                    "label": 0
                },
                {
                    "sent": "When you remove this node, it says form a clique out of the neighborhood where the neighborhood single.",
                    "label": 0
                },
                {
                    "sent": "So it means you do nothing.",
                    "label": 0
                },
                {
                    "sent": "So you simply remove the degree one node and then therefore now this is of degree one.",
                    "label": 0
                },
                {
                    "sent": "So you remove that.",
                    "label": 0
                },
                {
                    "sent": "You remove this, then you can remove this and you can keep going and with it if it's a spanning tree you can remove all the nodes of the graph without causing any fill.",
                    "label": 1
                },
                {
                    "sent": "And in linear work OK, so in particular that's why spanning tree is unnatural.",
                    "label": 0
                },
                {
                    "sent": "First cut at a good preconditioner, right?",
                    "label": 0
                },
                {
                    "sent": "Because remember it.",
                    "label": 0
                },
                {
                    "sent": "Hopefully it has good.",
                    "label": 0
                },
                {
                    "sent": "It speeds up the iteration and at the same time it has a nice property that the solve using the spanning tree will be fast.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's his idea.",
                    "label": 0
                },
                {
                    "sent": "It still works.",
                    "label": 0
                },
                {
                    "sent": "Something here.",
                    "label": 0
                },
                {
                    "sent": "Not this crazy thing.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So right, so let's just quickly write some advantages down, so simple advantage.",
                    "label": 0
                },
                {
                    "sent": "Then this is that.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's easy to find an easy to solve, right?",
                    "label": 1
                },
                {
                    "sent": "We know how to find spanning trees in linear time, and we know how to solve them for them quickly, right?",
                    "label": 0
                },
                {
                    "sent": "So one unfortunate problem is is that if all the edges in the graph actually have very similar edge weights, then any perturbation in the edge weights is going to force.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To pick a very different spanning tree.",
                    "label": 0
                },
                {
                    "sent": "So, and that's not going to be good in terms of the condition number.",
                    "label": 0
                },
                {
                    "sent": "Not only that, actually it's not too hard.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what happened then is is that?",
                    "label": 0
                },
                {
                    "sent": "In 2005, also another spanning tree was proposed, called the low stretch spanning trees, which get around this problem won't go into the definition here, but there's another definition of spanning tree which then removes this.",
                    "label": 1
                },
                {
                    "sent": "This this problem of having lightweight edges perturbing the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spanning tree you get.",
                    "label": 0
                },
                {
                    "sent": "And the advantage with these is that they have much better condition.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Number one problem is is that we don't know how to find them in linear times with their super.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Linear define another problem is that they're still not really very good if you take even at the simple square mesh.",
                    "label": 1
                },
                {
                    "sent": "As the underlying system you want to solve that there is no good spanning tree for that, right?",
                    "label": 1
                },
                {
                    "sent": "That's one of these results.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From 2004.",
                    "label": 0
                },
                {
                    "sent": "So grim bomb and I in 94 proposed that what we really should do is just take a Steiner tree.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is instead of taking this graph and requiring ourselves to pick a sub graph, why don't we just start allowing ourselves to have Steiner nodes?",
                    "label": 0
                },
                {
                    "sent": "So in particular, what we did here is is these nodes then are the original variables and we've just introduced new variables to form a tree.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "So so in fact, for the square mesh, this gives dramatically better condition number and you.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And solve them much faster.",
                    "label": 0
                },
                {
                    "sent": "And also experimentally, this works quite well substantially better than any.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other spanning trees.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, we never really figured out how to find a good way to compute these trees.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a nice hard problem, so.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we haven't made any headway there in 2007.",
                    "label": 0
                },
                {
                    "sent": "Could assume myself proposed using Steiner forest.",
                    "label": 0
                },
                {
                    "sent": "So the idea now is all we're going to do is we're going to take this underlying graph, and we're going to find highly connected pieces in this graph.",
                    "label": 0
                },
                {
                    "sent": "I think in this case it probably doesn't actually exist that way, but the idea now is what we're going to do now is just take small pieces of this graph, and we're going to make a signing Steiner tree for each one of those pieces and use that as our preconditioner.",
                    "label": 0
                },
                {
                    "sent": "Well, actually more precisely.",
                    "label": 0
                },
                {
                    "sent": "So the advantage of this is that it's easy to find these things they work well with recursive solve.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we haven't got to that yet.",
                    "label": 0
                },
                {
                    "sent": "So the next thing to do is is is show how to recur.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simply use these ideas.",
                    "label": 0
                },
                {
                    "sent": "And also they give good XP.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Elemental results will show those OK.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, at this point we still have only been able to analyze these for the planar system.",
                    "label": 0
                },
                {
                    "sent": "So for those we can show good runtimes.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Linear time OK, so so the second idea is advised.",
                    "label": 0
                },
                {
                    "sent": "You had is is that what you should do?",
                    "label": 0
                },
                {
                    "sent": "But we're now applying this to our Steiner system.",
                    "label": 0
                },
                {
                    "sent": "So the idea now is again, these are original variables.",
                    "label": 0
                },
                {
                    "sent": "These are our Steiner variables and these are the edge weights in this new graph.",
                    "label": 0
                },
                {
                    "sent": "So the idea now that Vaidya proposed, he said, well, find your preconditioner.",
                    "label": 0
                },
                {
                    "sent": "And suppose it's not a spanning tree, but actually has a few more edges like.",
                    "label": 0
                },
                {
                    "sent": "I think this has one extra edge in this case, right?",
                    "label": 0
                },
                {
                    "sent": "If we remove this edge, then we get a tree.",
                    "label": 0
                },
                {
                    "sent": "But suppose in general we may have a lot more other edges.",
                    "label": 1
                },
                {
                    "sent": "Let's just remove those edges which are easy using direct method.",
                    "label": 0
                },
                {
                    "sent": "So in this case will remove all the original variables OK, and then we'll end up with a much smaller system.",
                    "label": 1
                },
                {
                    "sent": "And then what we'll do now.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is take this system here which is now reduced and instead of trying to solve that directly, why don't we recurse?",
                    "label": 0
                },
                {
                    "sent": "Will just now ask for a recursive solver to solve this simple this newer system and hopefully this newer system is much smaller than the old system, so it will get a geometric increase, decrease in problem size, which will give us a good solver.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "So whether we worry about this so so maybe I should just.",
                    "label": 0
                },
                {
                    "sent": "Talk about congestion dilation, right?",
                    "label": 0
                },
                {
                    "sent": "So hopefully you all know this stuff.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure why this should be here, but if you have a simple recurrence relation of this form right like in the case we started with and you want to analyze how fast these.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things converge, a standard result says that if you let the air be simply your guess minus there actually.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answer To the problem that in fact the guest then the error at time I is simply the original error times that the G.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The power of your matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so all you need to then show this is that this matrix here goes to zero as I go.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To Infinity, right?",
                    "label": 0
                },
                {
                    "sent": "I got it.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that gives us then.",
                    "label": 0
                },
                {
                    "sent": "So let's let's remember, I guess we already find it before, but if we have a system of equations of this form ax equals Lambda B, then across will call lamb.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Under an eigenvalue and action eigenvector, in our case, here all our eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Then our semi are greater than or equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So since it applausi and actually there's a 01, and then there's a bunch of other ones, right?",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose we sort these so the condition number is simply defined to be of this matrix.",
                    "label": 0
                },
                {
                    "sent": "The largest eigenvalue divided by the smallest eigen.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So then standard results says that the convergence rate for the basic iterative method is one over the condition number, so this ends up being very.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or if you then go to conjugate gradient, standard result says that you get to take this.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Square root of the condition number.",
                    "label": 0
                },
                {
                    "sent": "So with this roughly says for simple examples is that the rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "In other words, if you want one bit of accuracy, the number of iterations has to be something like the diameter of the graph, right?",
                    "label": 0
                },
                {
                    "sent": "So if in other words you take the square root of N by square root of N grid, and you do conjugant gradient, then you need square root of N iterations to get one bit of accuracy.",
                    "label": 0
                },
                {
                    "sent": "So, so this is class.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Analysis.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do now is bound the condition number of B inverse A because that's what we're iterating on instead of simply.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK right good.",
                    "label": 0
                },
                {
                    "sent": "Well, another way to write that is is simply if we're interested in the eigenvalues of this system, is that's equivalent to the generalized eigenvalues of the form.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "X equals Lambda BX OK.",
                    "label": 0
                },
                {
                    "sent": "So Lam just called the generalized eigenvalue OK, so in general, then what we're interested in is the condition number again, so we're interested in.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The eigenvalues of this system here, so the largest divided by the smallest and that will determine our rate of convergence in the enorme anyway.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Good.",
                    "label": 0
                },
                {
                    "sent": "So one thing that happens in this case, then is as we are look at.",
                    "label": 0
                },
                {
                    "sent": "So our matrix are semidefinite, right positive semidefinite definition simply says that for all X not equal to 0, or I guess I don't need that here X transpose axes.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great and equal to zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so a standard definition that makes all the analysis works here is is that we're interested in the support of A by B and this is just simply I want to minimize.",
                    "label": 1
                },
                {
                    "sent": "I want the minimum Lambda such that Lambda B -- A a semi positive definite.",
                    "label": 0
                },
                {
                    "sent": "So what this means in terms of electrical circuits, it says that I have two electrical circuits, 1A and 1B, and what I want to know is how many copies of the electrical circuit be do I need in order to be able to carry as much current for all experiments as a.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good and then the condition number here is just the product of these things, so it ends up happening now which is from an electrical point of view.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do if you are two graphs is electrical circuits and what we're going to want to know is how do we then support.",
                    "label": 0
                },
                {
                    "sent": "A by B and support be by A and that's going to determine our runtime OK.",
                    "label": 1
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so how do you do that?",
                    "label": 0
                },
                {
                    "sent": "So the classic thing here is using what are called path embeddings.",
                    "label": 0
                },
                {
                    "sent": "So I suppose that we have two graphs.",
                    "label": 0
                },
                {
                    "sent": "With vertices VG and VH, then a path embedding is simply a map from the edges of G. To pass in, not then, but pass in H such that the past begins at VI and ends at VJ.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do a small example here.",
                    "label": 0
                },
                {
                    "sent": "So suppose in this example, now we have two graphs.",
                    "label": 0
                },
                {
                    "sent": "One is the complete graph and the other graph is the four cycle.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have our four cycle here.",
                    "label": 0
                },
                {
                    "sent": "Anne Green and then the red graph here is actually the K4, and so there's a natural path embedding, namely, this red edge will simply math map to this green edge.",
                    "label": 0
                },
                {
                    "sent": "This red edge will map to this screen edge.",
                    "label": 0
                },
                {
                    "sent": "This one here just here, but this edge here doesn't exist.",
                    "label": 0
                },
                {
                    "sent": "In the original 4 cycle.",
                    "label": 0
                },
                {
                    "sent": "So why don't we just map that to the path that uses this green edge?",
                    "label": 0
                },
                {
                    "sent": "Then the screen edge and at the same time why don't we map this red path here to these two green edges so the net effect now is.",
                    "label": 0
                },
                {
                    "sent": "So I guess it's in the bottom of the slide here.",
                    "label": 0
                },
                {
                    "sent": "So so in this example then what happens is a critical thing that makes all this analysis work.",
                    "label": 0
                },
                {
                    "sent": "Then is the following an that is in this example?",
                    "label": 0
                },
                {
                    "sent": "There's going to be congestion.",
                    "label": 0
                },
                {
                    "sent": "Not sure what happened here, so the congestion in this case then is going to be what well?",
                    "label": 0
                },
                {
                    "sent": "This edge then is used by three pass, right?",
                    "label": 0
                },
                {
                    "sent": "Namely this edge.",
                    "label": 0
                },
                {
                    "sent": "This edge in this edge right in the original K4, so the congestion here, and that seems to be the worst, right?",
                    "label": 0
                },
                {
                    "sent": "This has congestion too.",
                    "label": 0
                },
                {
                    "sent": "One and two, so that also the worst congestion is 3.",
                    "label": 0
                },
                {
                    "sent": "The dilation here.",
                    "label": 0
                },
                {
                    "sent": "In this case, well, this red edges map to an edge the same link, so it's not dilated at all right?",
                    "label": 0
                },
                {
                    "sent": "This edge here.",
                    "label": 0
                },
                {
                    "sent": "This red one is mapped to a path of length two, so the dilation is 2.",
                    "label": 0
                },
                {
                    "sent": "So a natural thing then is is that the congestion times the dilation in this case is 6.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I want to claim then and there is that here in the slide.",
                    "label": 0
                },
                {
                    "sent": "Let's just check and see.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So now let's go back.",
                    "label": 0
                },
                {
                    "sent": "Go back here.",
                    "label": 0
                },
                {
                    "sent": "So if I go back, so I guess that's missing here.",
                    "label": 0
                },
                {
                    "sent": "So on the other slide.",
                    "label": 0
                },
                {
                    "sent": "So this is so in this case here, then a standard theorem, then it's not too hard to show is that the support then of K4 by the cycle of size 4 then is.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by 6.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "And so so therefore, and on the other hand, clearly the support of C4 by K4 since it's the sub graph.",
                    "label": 0
                },
                {
                    "sent": "You just do the path embedding as a unit 1 is less than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So this says that the condition number of the Laplacian for K4 and the Laplacian of C4 then is less than or equal to 6.",
                    "label": 0
                },
                {
                    "sent": "Shut.",
                    "label": 0
                },
                {
                    "sent": "C as a cycle of length four.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, So what that says then is that in this particular case that if we preconditioned K4 with a four cycle.",
                    "label": 0
                },
                {
                    "sent": "We would expect to have to do about 6 iterations per bit.",
                    "label": 0
                },
                {
                    "sent": "If we did the naive method an.",
                    "label": 0
                },
                {
                    "sent": "If we did conjugate gradient, then we'd need to do sqrt 6 iterations, But of course there's big constants in there, right?",
                    "label": 0
                },
                {
                    "sent": "And so this is the technology.",
                    "label": 0
                },
                {
                    "sent": "Then that's used to be able to analyze one quality.",
                    "label": 0
                },
                {
                    "sent": "Precondition to another good OK.",
                    "label": 0
                },
                {
                    "sent": "So I guess what I'd like to do actually is just to.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about historically what's happened in terms of solvers for the.",
                    "label": 0
                },
                {
                    "sent": "The planar case.",
                    "label": 0
                },
                {
                    "sent": "There's still a classic place for which at least the theoreticians have cut their teeth, and so if I go back and you sort of say well prior to the 50s, nothing was known, and so you could say that the runtime was MN cubed.",
                    "label": 0
                },
                {
                    "sent": "Even for planar laplacian's in the 50s conjugate gradient came around.",
                    "label": 0
                },
                {
                    "sent": "This gave a a runtime of N ^2.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The analysis I guess we didn't finish that, but because of convergence rates and then, Lipton, Rose and Tarjan proved that any planar graph could be done in end of the 1.5.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've talked about Vaidya, then gave an algorithm using preconditioned iterative methods, which is end of the.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": ".2.",
                    "label": 0
                },
                {
                    "sent": "Then using low stretch spanning trees, Spielman and Tang showed how to get this down to and logs.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where Dan and more recently we've shown that you can actually get this down to linear time, so it's known that any planar linear system now can be solved in linear time.",
                    "label": 0
                },
                {
                    "sent": "So of course the punchline is is that it says that if you want to do a forward multiplier and inverse power up to constants, those are all the same, so you should then and let me show you some examples to actually quantify the exact cost of doing an inverse power over forward power, but it says in these cases that.",
                    "label": 0
                },
                {
                    "sent": "There they cost about the same, right?",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, but I should point out that there's very famous result in this area and this is due to Spielman and Tang and what they were able to show is that if you take any Laplacian, you can solve this in time M + N + M. Remember ends the number of variables M, the number of nonzeros in the matrix, and the twiddle here means that we're ignoring logs, and I think it's building pointed out in his talk yesterday.",
                    "label": 0
                },
                {
                    "sent": "Number of logs as somewhere bigger than 29, right?",
                    "label": 0
                },
                {
                    "sent": "So so it's not exactly practically yet, but it's some huge number of logs, but at least theoretically this is very interesting because it says that with more work chances are the right answer is is that it's in log in with some very few logs, right?",
                    "label": 0
                },
                {
                    "sent": "And I'll show you some experiments on that we've done with other other solvers, OK?",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go back and actually look at this solver.",
                    "label": 0
                },
                {
                    "sent": "Youngest Curtis is actually developed a solver at CMU, using what we call common troll multigrid, and we've compared this to the MATLAB solver.",
                    "label": 0
                },
                {
                    "sent": "And what's interesting here is is that the map and this is for 2D images, which will talk about more, and you can see that this is a slightly better method than the others.",
                    "label": 0
                },
                {
                    "sent": "What's interesting here is that what happens in the Matlab solver, at least for the machines we have.",
                    "label": 0
                },
                {
                    "sent": "So is very quickly we end up running out of.",
                    "label": 0
                },
                {
                    "sent": "Too bad I can't see the numbers here, right?",
                    "label": 0
                },
                {
                    "sent": "So I think this is about somewhere around .7 million variables.",
                    "label": 0
                },
                {
                    "sent": "This thing starts to breakdown, right?",
                    "label": 0
                },
                {
                    "sent": "We can no longer solve these things.",
                    "label": 0
                },
                {
                    "sent": "OK, because we run out of memory in Matlab.",
                    "label": 0
                },
                {
                    "sent": "Good, and it seems like the direct methods of anything.",
                    "label": 0
                },
                {
                    "sent": "That's the one downside is they quickly run out of memory, so in 3D.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, in millions of variables here.",
                    "label": 1
                },
                {
                    "sent": "So what happens is that we again get this nice linear runtime.",
                    "label": 0
                },
                {
                    "sent": "So this is in seconds to solve one of these systems and you can see that Matlab quickly just runs out of memory, 'cause again we said that if you're doing direct methods that Phil is going what's going to cost.",
                    "label": 0
                },
                {
                    "sent": "That's end of the three halves.",
                    "label": 0
                },
                {
                    "sent": "And so these things run out of memory and crash, right?",
                    "label": 1
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "Is there?",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so I guess what I'd like to do and then anyway, so that's our solver stuff.",
                    "label": 0
                },
                {
                    "sent": "And so the hope was to eventually try to get solvers out.",
                    "label": 0
                },
                {
                    "sent": "'cause it seems like this is a natural community that would should have access to good solvers.",
                    "label": 0
                },
                {
                    "sent": "What I'd like to do though, we spend a few minutes on how some one generalization that we have Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately I don't know any interesting applications, and one of the reasons I want to do this here is hopefully in showing you this reduction.",
                    "label": 0
                },
                {
                    "sent": "One of you are many of you will see how to apply this to machine learning, so let's assume that that instead of being a graph Laplacian, this is simply a symmetric diagonally dominant system, right?",
                    "label": 1
                },
                {
                    "sent": "This simply means that the diagonal then is greater than or equal to the sum of its off diagonal.",
                    "label": 0
                },
                {
                    "sent": "The absolute value of its off diagonals.",
                    "label": 1
                },
                {
                    "sent": "So the goal of the rest of this talk then, is to show that these systems can be solved can be reduced to simple regular laplacian's.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so in our case now the other way to think of these these symmetric diagonally dominant systems.",
                    "label": 0
                },
                {
                    "sent": "Now is we can still take every off diagonal edge.",
                    "label": 0
                },
                {
                    "sent": "And what we can do now is simply say that we have an edge from I to J, then the function WIJ is not zero for that value, right?",
                    "label": 0
                },
                {
                    "sent": "'cause it's so now will just allow WIG to J to be.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, so now this is just a weighted let's let a be the weighted incidence matrix as we defined it before.",
                    "label": 0
                },
                {
                    "sent": "Now let.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To find the degree of the system to simply be.",
                    "label": 0
                },
                {
                    "sent": "Some of the absolute values of the edge weights.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can now get, let's call this a generalized Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Things of the form LD equals.",
                    "label": 0
                },
                {
                    "sent": "I also notice that of course this system here is a symmetric, diagonally dominant.",
                    "label": 0
                },
                {
                    "sent": "The only thing that's missing here is is that we are requiring here that the diagonal not be arbitrarily big with respect to a.",
                    "label": 0
                },
                {
                    "sent": "That really doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You can actually make the diagonal bigger, but let's just worry bout how would we solve these kind of linear systems where we allow.",
                    "label": 0
                },
                {
                    "sent": "The elements in here at the off diagonals to be positive.",
                    "label": 0
                },
                {
                    "sent": "Now they need not be negative.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So this is just a simple generalization of regular glass repossi.",
                    "label": 0
                },
                {
                    "sent": "OK, good, so the.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First thing I want to point out is is that if we have one of these matrices.",
                    "label": 0
                },
                {
                    "sent": "That we can write out this quotient.",
                    "label": 0
                },
                {
                    "sent": "The Rayleigh quotient or the numerator of the Rayleigh quotient and the following form, right?",
                    "label": 0
                },
                {
                    "sent": "So this is all the same thing as usual.",
                    "label": 0
                },
                {
                    "sent": "I don't think I'll go through derivation.",
                    "label": 0
                },
                {
                    "sent": "What happens is you get two kinds of terms.",
                    "label": 0
                },
                {
                    "sent": "Now you get terms when the WIJ is positive.",
                    "label": 0
                },
                {
                    "sent": "That looks like.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Start word.",
                    "label": 0
                },
                {
                    "sent": "Great now, so this is just the standard term we get for the regular Laplacian, right?",
                    "label": 0
                },
                {
                    "sent": "We get things of the form WIJ times I exhibi, minus X sub J squared.",
                    "label": 0
                },
                {
                    "sent": "But now what happens is for the terms for where the edge weights in the original graph are negative.",
                    "label": 0
                },
                {
                    "sent": "What happens then?",
                    "label": 0
                },
                {
                    "sent": "Is is we just take the absolute value or you put a - here and you get another sum of positive terms.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Says that it notice that these generalife laplacians are still positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "They may not be positive definite because they include clearly the regular Laplacian, so we can't expect that in general right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Crazy.",
                    "label": 0
                },
                {
                    "sent": "Chrome.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, but I claim that it's not too hard to see that that if they rank is still an minus one if G is connected, right?",
                    "label": 0
                },
                {
                    "sent": "So we have that.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So suppose we want to solve these systems using Laplace iiams.",
                    "label": 0
                },
                {
                    "sent": "Let's consider the following very simple idea and that is let's do a change of variables to see if we can't get it into the form of a regular Laplacian.",
                    "label": 1
                },
                {
                    "sent": "OK, so so suppose we take one of these things then.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You multiply one of these matrices, the I TH column and the I throw by minus one.",
                    "label": 0
                },
                {
                    "sent": "Then I claim if you work this out because we're multiplying the same column and row by minus one, that means the diagonal won't change sign.",
                    "label": 1
                },
                {
                    "sent": "It will still be positive and all we've done is flip the sign on the off diagonal elements, which does not change the diagonal dominance of the system, right?",
                    "label": 0
                },
                {
                    "sent": "So what this says is that.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to doing a change of variables where we flip the sign on the variable XI to minus XI in the variable and DVI to minus BI in the system.",
                    "label": 1
                },
                {
                    "sent": "We want to solve.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just do a small example to make sure we understand.",
                    "label": 0
                },
                {
                    "sent": "So suppose now we take a generalized graph, it has edge weights.",
                    "label": 0
                },
                {
                    "sent": "Some of these are negative and some are positive, like a Reg.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Laplacian so we make up the matrix.",
                    "label": 0
                },
                {
                    "sent": "What is it going to look like?",
                    "label": 0
                },
                {
                    "sent": "Well, we again take the sum of the absolute value of the edge weights out.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have two minus ones or two for the century 2, so they're all twos on the diagonal, just 'cause it's a trivial example.",
                    "label": 0
                },
                {
                    "sent": "And then of course we take minus the sign.",
                    "label": 0
                },
                {
                    "sent": "So for V1 it has two edges out, one to V2, which is negative.",
                    "label": 0
                },
                {
                    "sent": "So we change that to a plus it has another edge, which is plus we change to a minus, right?",
                    "label": 0
                },
                {
                    "sent": "So this then is a generalized symmetric diagonally dominant system.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to this graph, we can see that good, and so we'd like to solve such a system, and so one idea is is why don't we try doing a change of variable?",
                    "label": 0
                },
                {
                    "sent": "Let's multiply, say the first row by minus one in the first column by minus one.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we do that, what happens that means we change?",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ames assign on everyone on the 1st row except for the diagonal.",
                    "label": 0
                },
                {
                    "sent": "We change the elements on the 1st Column by that, but I claim that we can now make up a new linear system where we put a - on X1 and a - on the B1.",
                    "label": 0
                },
                {
                    "sent": "And if you work it out then.",
                    "label": 0
                },
                {
                    "sent": "This works out correctly right in the sense that now what's going to happen here is is we're going to get this term here.",
                    "label": 0
                },
                {
                    "sent": "This term is negated.",
                    "label": 0
                },
                {
                    "sent": "This term is negated, so therefore when we multiply this vector times this row, we're just going to get a -- B one, just like we want.",
                    "label": 0
                },
                {
                    "sent": "And when we do the dot product with any other row, we're going to get the same old value back.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I claim then is if you want to solve this system.",
                    "label": 0
                },
                {
                    "sent": "You could try multiplying a given column in a given row by minus one and seeing if that helped, and then when you got done you would just have a change of variables.",
                    "label": 0
                },
                {
                    "sent": "Here you would change the buys an you would change the excise accordingly, right?",
                    "label": 0
                },
                {
                    "sent": "So at anytime you can do that and it doesn't change the underlying system you're solving.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's make a definition.",
                    "label": 0
                },
                {
                    "sent": "Let's say that that an underlying graph is orientable if there exists a sequence of flips such that all the edge weights go positive, OK.",
                    "label": 1
                },
                {
                    "sent": "So did we define that?",
                    "label": 0
                },
                {
                    "sent": "Let's just go back here.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                },
                {
                    "sent": "So we should have have so, so in general, what are we doing here when we're doing a flip at a vertex?",
                    "label": 0
                },
                {
                    "sent": "So what are we doing?",
                    "label": 0
                },
                {
                    "sent": "Graph theoretically has a very similar meaning is simply says.",
                    "label": 0
                },
                {
                    "sent": "What we do is we're taking a graph here.",
                    "label": 0
                },
                {
                    "sent": "Write an we've picked out a vertex VI and what we're going to simply do is multiply all the weights of the edges here by minus one.",
                    "label": 0
                },
                {
                    "sent": "OK and so at anytime we can do a change of variables which simply graph theoretically means you change all the edges to have the opposite signs that they had before.",
                    "label": 0
                },
                {
                    "sent": "OK, good an.",
                    "label": 0
                },
                {
                    "sent": "So suppose you have a graph.",
                    "label": 0
                },
                {
                    "sent": "One possibility is it you're very lucky and what you can do is change all.",
                    "label": 0
                },
                {
                    "sent": "All the do flips until you eventually get a regular old vanilla Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Remember this vanilla Laplacian and then you go solve it and just remember what flips you did to figure out what your answer is up to.",
                    "label": 0
                },
                {
                    "sent": "A change of sign.",
                    "label": 0
                },
                {
                    "sent": "And do that OK. OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, this is just a.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So notice now will make another definition, will say the matrix is orientable.",
                    "label": 0
                },
                {
                    "sent": "Of course, if the graph is right.",
                    "label": 0
                },
                {
                    "sent": "So I claim that Orientability is very similar to what's called 2.",
                    "label": 1
                },
                {
                    "sent": "Coloring in graph theory, and it's very easy to figure out.",
                    "label": 1
                },
                {
                    "sent": "You just do a simple greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course we could then figure out quickly whether we have an orientable matrix, and if we do, then we just solve that using Spielman Tang or the state of the art method, right?",
                    "label": 0
                },
                {
                    "sent": "If not, and it's not orientable, then we have to go figure out how we're going to solve these.",
                    "label": 0
                },
                {
                    "sent": "So in general, of course they shouldn't be orientable, but.",
                    "label": 0
                },
                {
                    "sent": "We wanted to.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eliminate those from our list OK?",
                    "label": 0
                },
                {
                    "sent": "So let's let's prove something simple here.",
                    "label": 0
                },
                {
                    "sent": "Suppose that G is connected and not orientable.",
                    "label": 1
                },
                {
                    "sent": "Then I want to claim its Laplacian, then is now symmetric positive definite is no longer semidefinite.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's do a quick proof.",
                    "label": 0
                },
                {
                    "sent": "So let's do it by contradiction.",
                    "label": 0
                },
                {
                    "sent": "Suppose that LXX transpose I'll ax equals zero an X is not equal to the all zero vector.",
                    "label": 0
                },
                {
                    "sent": "OK, then we can pick a spanning.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free for G and we can Oriente by flipping.",
                    "label": 0
                },
                {
                    "sent": "All the edges, in other words, make all the edges in some spanning tree positive OK.",
                    "label": 0
                },
                {
                    "sent": "So therefore, since all the.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edges are positive in terms of formula.",
                    "label": 0
                },
                {
                    "sent": "The value at all those vertices in X has to be equal, otherwise we're going to get a non 0 value here.",
                    "label": 0
                },
                {
                    "sent": "So this implies that X has to be, you know up to normalization the all ones vector, which is a contradiction because she still has some negative edge weights.",
                    "label": 0
                },
                {
                    "sent": "So it's not too hard to prove.",
                    "label": 0
                },
                {
                    "sent": "I think it's almost easier to work it out yourself, but I claim that now we have a positive definite system where that's interesting, OK?",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's how we're going to solve these kinds of systems.",
                    "label": 0
                },
                {
                    "sent": "We're going to do what we call a two fold cover.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is I'm going to take the original linear system and I'm going to double the number of variables for you.",
                    "label": 0
                },
                {
                    "sent": "You don't have to do this in practice, but this is a nice way to think about it.",
                    "label": 0
                },
                {
                    "sent": "An in that twofold cover.",
                    "label": 0
                },
                {
                    "sent": "It'll be a regular Laplacian.",
                    "label": 0
                },
                {
                    "sent": "OK, here's the idea.",
                    "label": 0
                },
                {
                    "sent": "So what we'll do now is.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm in the middle of defining A2 fold cover OK. And so it's going to have two N variables.",
                    "label": 0
                },
                {
                    "sent": "It's going to have a regular variable.",
                    "label": 0
                },
                {
                    "sent": "Ann is going to have a complementary variable, so there's a variable VI and VI compliment, and so we're going to do that, OK?",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we'll do now is if it's the case that the original edge WI J is positive.",
                    "label": 0
                },
                {
                    "sent": "In other words, it's a regular edge.",
                    "label": 0
                },
                {
                    "sent": "Then we'll simply connect together VI and VJ and Connect together VI Baran Vijay Bar with an edge with this weight on it.",
                    "label": 0
                },
                {
                    "sent": "If it's on the other hand.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a negative weight edge.",
                    "label": 0
                },
                {
                    "sent": "What we'll do then is will connect together VI with Vijay Bar and VI bar with Vijay OK.",
                    "label": 0
                },
                {
                    "sent": "In other words, we put a twist in it.",
                    "label": 0
                },
                {
                    "sent": "If you're into electrical engineering, this is just a double rail logic.",
                    "label": 0
                },
                {
                    "sent": "We just put a twist in to negate and then will negate the edge weight OK?",
                    "label": 0
                },
                {
                    "sent": "And so this is called A2 fold cover so.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just in case we want to do a small example.",
                    "label": 0
                },
                {
                    "sent": "So the idea with this example here is we take our original graph, the two.",
                    "label": 0
                },
                {
                    "sent": "This has three vertices, so the two fold cover should have six vertices.",
                    "label": 0
                },
                {
                    "sent": "They should come in pairs, so in this case here we have V3 and VN and V3 compliment.",
                    "label": 0
                },
                {
                    "sent": "We have V1 and V2 complement V2 and V2 complement.",
                    "label": 0
                },
                {
                    "sent": "These OK, so somehow this is not the right graph.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately.",
                    "label": 0
                },
                {
                    "sent": "So so can I draw this graph for you over here instead, 'cause it looks like there's a bug in this example.",
                    "label": 0
                },
                {
                    "sent": "OK, so and then I can get.",
                    "label": 0
                },
                {
                    "sent": "So avert our graph, then look something like this, right?",
                    "label": 0
                },
                {
                    "sent": "So this is this is 1.",
                    "label": 0
                },
                {
                    "sent": "This is V1 bar.",
                    "label": 0
                },
                {
                    "sent": "This is V2V2 Bar V1V3V3 bar.",
                    "label": 0
                },
                {
                    "sent": "There is 1 positive edge so that says you should hook these together in the usual way.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "There says these these two here have the property that.",
                    "label": 0
                },
                {
                    "sent": "That there are negative edge weight.",
                    "label": 0
                },
                {
                    "sent": "So we should put a twist in here.",
                    "label": 0
                },
                {
                    "sent": "The bottom edge also has a negative edge weight, so this should also have A twist.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And so this now is called A2 fold cover of that original graph, OK?",
                    "label": 0
                },
                {
                    "sent": "Ann, you should both ignore the fact that there's a meeting I'm missing, and the other example that's there.",
                    "label": 0
                },
                {
                    "sent": "So let me quickly remove the meeting.",
                    "label": 0
                },
                {
                    "sent": "OK, so everyone understand the construction is very simple construction.",
                    "label": 0
                },
                {
                    "sent": "What we've done is an hopefully save some use.",
                    "label": 0
                },
                {
                    "sent": "We've taken a general graph with positive and negative weight edges, and we've got a new graph with just twice as many variables.",
                    "label": 0
                },
                {
                    "sent": "But this graph has a nice property that it's a regular vanilla Laplacian.",
                    "label": 0
                },
                {
                    "sent": "All the edge weights are non negative or positive, right good, and so I claim that that's useful.",
                    "label": 0
                },
                {
                    "sent": "And the reason?",
                    "label": 0
                },
                {
                    "sent": "It just follows.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's let's let's let our original graph BG and let US Laplacian BL.",
                    "label": 0
                },
                {
                    "sent": "Let's let G Barbie our new double cover an L Barbie the Laplacian of that OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so notice that L bar then up to a factor of two in the variable count is now a vanilla Laplacian so we can solve.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All that using regular methods.",
                    "label": 0
                },
                {
                    "sent": "Of course we shouldn't really do that, but at least theoretically we could just do that OK.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And note the following thing that suppose we had a solution to this system.",
                    "label": 0
                },
                {
                    "sent": "Then you should convince yourself that if we order the variables so that we have X1 to XN X1 bar to XN bar Ann B1 through BN, and then B1 through BN bar that all we did is introduce 2 copies.",
                    "label": 0
                },
                {
                    "sent": "So what happens now?",
                    "label": 0
                },
                {
                    "sent": "Is that we take our solution to this equation and we make 2 copies, one with X and one with minus X.",
                    "label": 0
                },
                {
                    "sent": "We take B and minus B&I claim.",
                    "label": 0
                },
                {
                    "sent": "We have a solution to this system.",
                    "label": 0
                },
                {
                    "sent": "You should work that out.",
                    "label": 0
                },
                {
                    "sent": "It's not too hard to see that what happens now is that since these are all, if we negate the value that we have for X in here with one in here that it does the right thing because of these twists, you should convince yourself that it works.",
                    "label": 0
                },
                {
                    "sent": "So that says that.",
                    "label": 0
                },
                {
                    "sent": "One Direction which is not the interesting direction is true, but it's not.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hard to see that.",
                    "label": 0
                },
                {
                    "sent": "On the other hand.",
                    "label": 0
                },
                {
                    "sent": "This matrix here is a regular Laplacian right, and it's a connected graph.",
                    "label": 0
                },
                {
                    "sent": "We should approve that because the two fold covered is connected.",
                    "label": 0
                },
                {
                    "sent": "Now matter fact it's an orientable graph if and only if when you make the two fold cover you end up with a disconnected graph.",
                    "label": 0
                },
                {
                    "sent": "So this ends up being connected so we know the rank of this system.",
                    "label": 0
                },
                {
                    "sent": "So suppose you had a solution to this linear system.",
                    "label": 0
                },
                {
                    "sent": "Then this this is not a full rank, right?",
                    "label": 0
                },
                {
                    "sent": "So this has rank in this case 2 N minus one, so you could do simple.",
                    "label": 0
                },
                {
                    "sent": "You could take one solution and you could add copies of the all one vectors, but in particular that says that if you take the average of X&X&Y this is a solution to the original system.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is this interesting?",
                    "label": 0
                },
                {
                    "sent": "It says that what did we do?",
                    "label": 0
                },
                {
                    "sent": "We generated a new matrix.",
                    "label": 0
                },
                {
                    "sent": "We send this off to a solver.",
                    "label": 0
                },
                {
                    "sent": "It gives us back an answer of the form XY.",
                    "label": 0
                },
                {
                    "sent": "We simply take the the.",
                    "label": 0
                },
                {
                    "sent": "The average of those, the difference, half the difference between those two solutions.",
                    "label": 0
                },
                {
                    "sent": "And magically that's a solution to our original system.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "So let me just need to quit here.",
                    "label": 0
                },
                {
                    "sent": "So let me just.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can do that without the fat.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After 2.",
                    "label": 0
                },
                {
                    "sent": "So somehow and we had never figured out and recommendation problems like Netflix.",
                    "label": 1
                },
                {
                    "sent": "This should be of some value, but we haven't figured out how to do that.",
                    "label": 0
                },
                {
                    "sent": "People have looked at that OK.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It doesn't seem to do exactly the right thing, so maybe I should quit here.",
                    "label": 0
                },
                {
                    "sent": "I guess I took more time, so let me just go.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On and so I was going to talk about spectral graph theory, but I see that I don't have time for that.",
                    "label": 1
                },
                {
                    "sent": "So let me just.",
                    "label": 0
                },
                {
                    "sent": "Go over here.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just talk about 'cause I should.",
                    "label": 0
                },
                {
                    "sent": "That's a nice topic, but that's a whole another talk.",
                    "label": 0
                },
                {
                    "sent": "So one of the questions now.",
                    "label": 0
                },
                {
                    "sent": "So it would be nice to have methods that work for any symmetric positive definite system, right?",
                    "label": 0
                },
                {
                    "sent": "We've looked at a few when they're diagonally dominant.",
                    "label": 0
                },
                {
                    "sent": "We know how to solve these things, but in general unfortunately this everything breaks down as soon as you have.",
                    "label": 0
                },
                {
                    "sent": "It's not diagonally dominant.",
                    "label": 0
                },
                {
                    "sent": "So the other question is which I didn't get a chance to talk about his spectral graph theory, so again, that's very nice.",
                    "label": 0
                },
                {
                    "sent": "These one of my favorite questions of all is how to use more than one eigenvector for a graph in order to do good cuts.",
                    "label": 0
                },
                {
                    "sent": "That's a very beautiful question.",
                    "label": 0
                },
                {
                    "sent": "What it also like to do is find solvers at work in the L2 norm.",
                    "label": 1
                },
                {
                    "sent": "Notice the solutions we're coming up with actually in the enorme.",
                    "label": 0
                },
                {
                    "sent": "So it also be nice to take something like the Spielman Tang algorithm and get those kind of guarantees with something that actually works in practice, right?",
                    "label": 0
                },
                {
                    "sent": "And maybe I should quit at that.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                }
            ]
        }
    }
}