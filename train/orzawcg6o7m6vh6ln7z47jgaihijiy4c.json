{
    "id": "orzawcg6o7m6vh6ln7z47jgaihijiy4c",
    "title": "Introduction to Kernel Methods",
    "info": {
        "author": [
            "Bernhard Sch\u00f6lkopf, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "September 2004",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlss04_scholkopf_ikm/",
    "segmentation": [
        [
            "Which one is it?",
            "Is this one?",
            "If you want, you can take it.",
            "I don't need it.",
            "Maybe turn it down a little bit because there's some.",
            "You should keep it somewhere that it changed my face because he that's his problem with me.",
            "Here.",
            "And you have a laptop.",
            "Specially if it has MATLAB installed, please bring it along to the practical session.",
            "So I've seen that a lot of people have laptops here, so we shouldn't be short on laptops.",
            "Thing is, of course you should feel free to interrupt the lecturers at anytime.",
            "It's a summer school and no question is stupid enough for us, so please feel free also to ask similar questions, specially in the morning courses.",
            "And.",
            "Yes, so there's one more technical thing.",
            "Yeah, but so we have someone who's recording the lectures, and it's quite difficult to do the sound so so this is an announcement to the lecturers.",
            "Just imagine that little imaginary area here that you shouldn't step outside.",
            "Also, don't Walker around all the way, which you can't anyway, because this cable is limited to try to stay somewhere around the center.",
            "OK. You should sign into that.",
            "How is it like this for the camera, man?",
            "It's OK here.",
            "OK, so the subject of my lecture is the introduction to kernel methods, but since it's the first lecture in the series, maybe it should also be a little bit of an introduction to machine learning in general, so it would be interesting to know how many here do not know machine learning.",
            "So who comes with a background that doesn't include machine learning?",
            "So.",
            "Looks like it's maybe 10%.",
            "Louder is it OK like this?",
            "So 10% of the people don't have a machine learning background.",
            "Maybe not, it's too loud.",
            "So, and how many of you would say they?",
            "No, a little bit, but not really much.",
            "Yeah, maybe that's.",
            "Another 20%.",
            "And how many know already a lot?",
            "I mean, how many are writing papers in machine learning journals of the machine learning conferences?",
            "So I think you can reach it.",
            "Yeah, OK, so yeah about another 20% and the rest is somewhere in the middle I guess.",
            "So there will be a lot of advanced lectures that cater to the requirements of these advanced guys and but I will try to start a little bit more basic.",
            "So let me start with a little demo of what machine learning can do.",
            "Let's see whether it works, so that's always a bit of a risk.",
            "So what I have here is face detection system, and we can also use it so that people can get get to know each other a bit better, or the quality is not very good and.",
            "I can install this here.",
            "OK, I'll not use the microphone from signal.",
            "And I have to focus this.",
            "Actually I tried it in the break.",
            "I tried it in the break but the light was still on.",
            "OK.",
            "Suggest for a minute?",
            "Excuse me, I'll switch on the light.",
            "'cause maybe it will make things easier.",
            "And then I go like this.",
            "OK, now you see quite a few face detections lighting up.",
            "Some of them are faces, some of them are not faces.",
            "But of course it's a.",
            "It's a pretty poor quality camera in the fairly cluttered scene.",
            "Anne.",
            "We switch it off again, So what you are seeing here is a.",
            "So called support vector machine that some of you some of you may.",
            "Some of you may know and others maybe not.",
            "It's A kind of classifier that's trained on data.",
            "In this case the data are positive and negative examples.",
            "So there are examples of faces in examples of non faces.",
            "So for instance background clutter in the classifiers trained on a few thousand such examples and afterwards can automatically recognize faces from non faces.",
            "And so this is a problem of binary pattern recognition or binary classification.",
            "We have two different classes of objects and we would like to find a general rule which will assign new inputs to one of these two classes face online phase.",
            "So.",
            "So let me start with."
        ],
        [
            "Some informal thoughts on machine learning.",
            "In my lecture I will be fairly non theoretical because there are other lectures lectures.",
            "For instance on statistical learning theory that will afterwards provide you with the statistical basis for what I'm talking about.",
            "So I will not talk much about statistics.",
            "I will talk about some informal ideas and machine learning a little bit about functional analysis, but no statistics because other lecturers will supply you with that later on.",
            "So for my purposes, motivation for learning is quite simple, so we have some data taken from input and output domains X&Y.",
            "So for instance X could be classified images of a certain size that contains faces or non faces.",
            "And why could be plus or minus 1 + 1 meaning face minus one meaning on face.",
            "So it's just a set containing two elements and when I train a classifier, I'm given a training set that consists of pairs X&Y.",
            "So some of them faces some of them, not faces and machine learning.",
            "People often talk about something called generalization.",
            "So given a number of examples you would like to make a prediction on new examples.",
            "So you would like to find a general rule that will generalize to new examples that you haven't seen before.",
            "So in particular, given a previously unseen X.",
            "So given a new face or non face Patch you would like to assign it to the right class.",
            "So you would find to predict the you would like to predict the right output Y for it.",
            "And one way to think about this would be to say, well in some sense, my prediction.",
            "Why would be reasonable if the new pair X&Y.",
            "So I'm given an X and I want to find the corresponding why is somehow similar to the ones in the training set, so this is the training set here, but then of course the question comes up how do we measure similarity and outputs?",
            "People usually use something which is called a loss function to measure similarity.",
            "So for instance, if we have labels plus or minus one, we could just say our losses zero whenever the label is correct.",
            "So whenever we say face and it's really a phase or losses zero.",
            "Whenever we say non phase and it's really a non face our losses zero likewise.",
            "But for the inputs it's a little bit less trivial how to measure the similarity between different inputs and that's where a concept comes up which is called a kernel which is basic for a number of different learning algorithms nowadays."
        ],
        [
            "So I would like to spend some time on kernels.",
            "So what I will consider as possible kernels are symmetric functions so defined on X * X.",
            "So I have two inputs, X&X prime.",
            "So there could be 2 phases, or there could be 2 non phases, one phase, one phase and I would like to assign kernel value K of X, X prime to these two inputs.",
            "And one example of such a kernel.",
            "So this this function K is called the kernel and this function now captures somehow our notion of similarity for the moment, and one example that we could use if our input domain is a vector space to the end.",
            "We could just consider what's called the Canonical dot product between the two vectors X&X prime.",
            "So this is some overripe coordinates of X times the Earth coordinate of X prime.",
            "Now, if X is not a dot product space and it's not obvious why it should be a dot product space.",
            "I mean, even in the case of images, you first have to represent them as vectors before you can take DOT products.",
            "But if it's not about product space then we will assume that at least K has a representation as a dot product space as a dot product in some other space, which is a dot product space.",
            "So we're assuming that if X&X prime are not living it or product space, then at least.",
            "That can be mapped into some dot product space using a mapping file, which we can think of as our representation.",
            "Sometimes it's also called the feature map.",
            "So we assume we have a mapping into some other space which is a dot product space.",
            "In there we have a dot product, so the K corresponds to a dot product in some other space at least.",
            "So in that case we can think of our inputs or sometimes they called patterns.",
            "So the excess of inputs we can think of them as actually 5X5 X prime and then that other space that's not our product space.",
            "We can do whatever we can do based on the products we can carry out to your metric algorithms."
        ],
        [
            "And now I'll show you a simple example of such an algorithm.",
            "Which could be used, for instance for face detection, although it won't be the very best algorithm that you can use for that.",
            "It's not such a bad one.",
            "It's very simple.",
            "OK, so suppose we have two classes of points, so we have some points 5 XI which belong which form the positive class.",
            "So the ones that have a label plus one and we have some points over here, the circles 5 XI that belong to the negative class, and in addition we have some test points that I'll just write SX.",
            "So BF X is a shorthand for.",
            "The image under fire of some point X, so the input points.",
            "They come from some set cartographic X on the last slide and we map them into a dot product space and then what we're going to do is we will compute the mean of all positive points over here.",
            "So M M1 is a number of positive points.",
            "These are the positive points and compute the mean of all of them.",
            "I'll do the same for the negative points over here.",
            "And then given a test point that could be anywhere, but after only down here.",
            "I will assign this test point onto the class whose mean is closer.",
            "OK, so.",
            "If you think about it for a second, what kind of decision rule does this?",
            "What kind of decision boundary does this rule imply?",
            "Then it's clear that the set of all points that are closer to see minors then there are to see plus forms what's called 1/2 space.",
            "So all the points on the right hand side of this line, or in general hyperplane will be closer to see minus than they are to see plus.",
            "And we can workout this decision rule and write it.",
            "As you will see, as a kernel expansion which looks quite similar to a number of other classifiers, such as the support vector machine.",
            "And I'll show you how you can do this.",
            "One way of doing this and this as follows.",
            "Remember that we know that our kernel is the same as the DOT product between patterns map by FI.",
            "So we can compute DOT product in this representation.",
            "So we have to somehow express this decision rule in terms of products and one way to do this is the following.",
            "We can compute this vector W, which is the vector connecting the two class means.",
            "Moreover, we could compute this vector C which is halfway in between.",
            "The two class means and we compute the vector connecting C2X and that's the vector X -- Y.",
            "And now we only have to check whether the angle between these two BF vectors is larger than 90 degrees or smaller than 90 degrees.",
            "And depending on that, X has to be on this side of the hyperplane or on the other side.",
            "OK, so feel free to interrupt at any point.",
            "And one way to compute this angle is to compute the whether the angle is smaller or larger than 90 degrees is simply to check the algebraic sign of the cosine enclosed by the two vectors.",
            "Now the cosine enclosed by the two vectors is essentially the same as the dot product between the two vectors up to a scaling factor.",
            "So if you Remember Remember that product from school there's a correspondence between DOT products and angles by the cosine dot product computes the cosine up to scaling factor.",
            "So if we're interested in whether the angle is smaller or larger than 90 degrees, we just have to check whether the product is positive or negative.",
            "So what we're going to do is we compute the dot product between this vector and this one.",
            "I've already told you how these vectors look, so now it's just a matter of plugging it in."
        ],
        [
            "And if you do this.",
            "You actually get this decision were here, so you get an expansion over the positive points dot products with the test point normalized by the number of positive points.",
            "Likewise, you here get an expansion over the negative points, normalized over the number of negative points and back.",
            "Here you have a constant, by which I mean all terms that don't depend on X don't depend on the test point.",
            "Now since we have this in terms of products, we can substitute kernel functions for their products.",
            "So then what you get is this solution here, and that's the decision rule.",
            "So that's a function or decision function."
        ],
        [
            "That's a function that will take the value plus one on this half space and minus one on this half space."
        ],
        [
            "And.",
            "The derivation is pretty straightforward and I haven't gone into detail on it, but.",
            "Actually I would like to have some involvement of the audience now because I find especially in the morning if I just listen to a lecture, I'm getting tired rapidly, so I'll take a few minutes off and let you derive this.",
            "But I would like to like you to derive it's in a slightly different way, so 'cause what I did was actually."
        ],
        [
            "Complicated right?",
            "I had to compute this vector then this vector take the dot product.",
            "There's a more direct way of doing it, which is simply to compute the distance between C plus and X and to compute the distance between C minus and X.",
            "And then to take the difference of these two distances.",
            "It depends on whether one distance is larger or the other one is.",
            "Difference is positive or negative.",
            "So if we plug this this this."
        ],
        [
            "Difference into the sine function, we should actually get the same solution.",
            "So so."
        ],
        [
            "Tell you again, So what you can do and I'll give you 5 minutes and everybody can try it.",
            "And then we can do it together on the blackboard.",
            "So what you do is you take this vector, take this vector, compute the difference distances between C plus in X and the distance between C minus in X, and then you take the difference between these two distances.",
            "And as one more additional hint, if you use the you should be using the squared distance.",
            "It's easier because the squared distance.",
            "Can be computed using the dot product easily.",
            "OK, so any more questions before you.",
            "Start doing your algebra with pen and paper.",
            "OK, so I'll give you 5 minutes.",
            "So would now be a good time for the solution.",
            "Or are most people still working it?",
            "How many of the people have solved the problem already?",
            "OK, well then maybe we'll give it another two minutes.",
            "I'm waiting too long.",
            "It took you one minute to solve it.",
            "I'm not just people are not used to it there.",
            "And.",
            "So I can do the solution here in the whiteboards, but.",
            "Maybe someone is confident he has found the right solution or she has found the right solution and would like to present it.",
            "I should say we will have a price for the people who find the most solutions.",
            "And it's it's a very rare item that might one day in the future command high prices if you sell it on eBay.",
            "'cause we notice when we printed these T shirts, you'll have this little logo builder here in one of the T shirts.",
            "The logo is upside down.",
            "We don't know why.",
            "But we thought you mean something, so we have to do something with this T shirt.",
            "So I will have occasional little exercise like this in my lectures and whoever solves most of them will get the T shirts.",
            "Maybe also some problems in the other lectures, but.",
            "Of course you have to prove that you solved it.",
            "It's not enough if you tell us you would have to do it here.",
            "And I'll help you if you want.",
            "So does that convince anyone?",
            "Maybe not in the very first lecture.",
            "I see so many signs that I don't know whom to choose.",
            "OK, I'll do it.",
            "So.",
            "Fun.",
            "OK.",
            "So we have a courageous volunteer here.",
            "Also talking.",
            "I formulated in terms of the point that you fixed it.",
            "And rather than.",
            "Directly write out everything in the way that you have it on the right side.",
            "I just left in terms of the C plus and the axis OK, and the particular way I get like it's just the opposite sign of the one that you have on the right side.",
            "So you get something like this.",
            "But with the opposite sign, yes.",
            "It's exactly that only with the opposite sign.",
            "Yeah, and instead of writing it out in terms of the sums, I'm just leaving in terms of the dot product between the points as you've got them up there.",
            "OK, just show us OK right here.",
            "Yeah, do you want the microphone?",
            "Can you hear him without microphone?",
            "Maybe you better use it too OK.",
            "So.",
            "1st, I'll write down the particular thing that he has, and then I'll start using the distance formulation that he suggested.",
            "So can you back it up one slide, OK?",
            "So here we have.",
            "That we want the sign of the dot product between X -- C. And.",
            "C + -- C minus.",
            "And see it's just one half C plus.",
            "Minus later on I'll just put that in, but right now I'm just going to write things just a couple lines before I get to that point.",
            "OK.",
            "So this is X dot C plus.",
            "Minus X dot C minus.",
            "Minus C dot C + + C dot C minus.",
            "I guess I could have just grouped these to begin with, but we have X dot C plus and then I'm not grouping these together minus T minus and then on the other side we're going to have minus C dot.",
            "I'm sorry I'm just going to write.",
            "This is a plus.",
            "C and then it will have a C -- -- C Plus.",
            "And remember this C is just one half of the sum C + + C minus.",
            "So we got this dot product.",
            "I'm sorry you're right.",
            "That's what I was saying, but just wrote it OK. And C -- -- C plus.",
            "OK, so this is the formulation that you get going from the dot product right here, and I'm going to show that you get something that's.",
            "Basically, minus this expression times two when you're going through the distance formulation in just a moment.",
            "OK so I have.",
            "Basically, X -- Y plus distance squared X -- Y minus.",
            "Let him let him right on the line and then we'll discuss it.",
            "If I've gotten it, just run by the sign.",
            "X -- Y minus end road.",
            "He said that that's incredible.",
            "So let me let me rephrase it so so So what he's saying is that if you use if you X -- Y plus and you subtract X -- Y minus yes if you think about when is this function positive, so it will be a function.",
            "If this part is bigger than the other one, which means if the point is further away from the positive mean, then from the negative.",
            "OK, so maybe you start the other way around, sort of the other way around.",
            "My mistake, you're exactly right, but as I said, it's just going to be a difference of a minus, so.",
            "Minus.",
            "Again, X X -- Y minus.",
            "And these are just.",
            "Top products.",
            "Marcus show new slide.",
            "Up to.",
            "I won't talk with this section, I'll just talk about it once I've written it up.",
            "OK. And on this record.",
            "You have some cancellations and what's left will be again with the way I've written it.",
            "Just assign change away from what I had previously.",
            "Close.",
            "So cancellation.",
            "Two ads.",
            "Dot.",
            "This is minus.",
            "This is plus C minus.",
            "My C plus.",
            "We have the difference between.",
            "C plus dot C plus.",
            "Minus C minus dot C minus.",
            "OK, OK. Finance is still acceptable.",
            "I'm reading the X as the transformed version, so think of this as a BF X that he's using.",
            "Which, like at the top of the slide is used as the shorthand, but it's a little bit confusing, so all these vectors are already living in the feature space that he's using here.",
            "OK, and I didn't finish writing well.",
            "First off X dot C + -- T minus and we have two X and.",
            "The opposite of what I have here and here.",
            "We have a C + C + -- C minus dot C minus.",
            "And this has the C plus dot C minus term as a plus and then as a minus, so it cancels and all you're left with is the C + C + * 1/2 an C minus dot C minus time to happen and assigns again just opposite to what?",
            "Human relations.",
            "Right, so thank you very much, so it's.",
            "Different from what I expected, but it also shows that the two formulations are the same, so it counts as a solution.",
            "So yeah, so he's using some linear algebra just to show that the one formulation that I was suggesting originally here with the DOT product in the distance formulations are the same.",
            "Of course I guess some of you may have just use this formula as a starting point and directly plugging things and then directly get to."
        ],
        [
            "This solution here.",
            "So maybe maybe I'll just briefly do it so that those people can also double check that they did things correctly, but.",
            "Well then maybe write down who solved the problem.",
            "So I forgot your name.",
            "Patrick OK, so one point for Patrick.",
            "And maybe also one point for the guy who notice the sign to change.",
            "No Lawrence.",
            "OK, you'll be wearing the yellow Jersey tonight.",
            "And OK, so let's just quickly do it the other way.",
            "So if we if we directly use this formulation here.",
            "So just ignore all this in between.",
            "And directly plug it in.",
            "So this is the dot product between this vector and itself.",
            "OK, so.",
            "So C minus is.",
            "Sum over all my negative points.",
            "Sorry in 1 / M Two.",
            "OK, so you have.",
            "This is the first time I'm skipping the second, which is sort of the same right to the second term, which is the same, only that you sum over the positive terms.",
            "OK, so let me just.",
            "Ignore this for now.",
            "So this is a difference between those terms and take the dot product with this difference.",
            "So I have four terms in this dot product right one is this times this this times this and then the two cross terms and two cross terms are actually the same so.",
            "What you get is a dot product between 5X and 5X.",
            "Mr First term, then you get the mixed term which is 2 / M two.",
            "Times the sum and then the dot product between 5X and 5X I.",
            "So this is a bit small and then you have the last term which is 1 / M two squared.",
            "And then we have a double sum.",
            "Over Inj and here we have 5 XI and five XJ.",
            "OK so 5X5 X in the mix turn 2 / M two and here is the sum over 5X, 5 XI and then the last term is 1 / M M 2 squared.",
            "And here we have a double sum 5 X I5 XJ and then we have three more terms of this form that correspond with the second part.",
            "So we've done the first part now OK.",
            "So if you then.",
            "How do we best do this?",
            "So if you continue with this, what you get is here you get 5X5 X, but actually we will have the same thing again in the second term, so that's going to cancel.",
            "OK, so this one we can ignore.",
            "It cancels with the corresponding path in the second term.",
            "However, what we have left is.",
            "This term here.",
            "This is some of our negative points and I'll write this as K of X, XI.",
            "We know how to compute Dot products, we just substitute the kernel so this is K of X, XI.",
            "Then we have the same term coming from the second distance, right the distance to the positive class.",
            "However, with a different sign.",
            "OK, we will have another term like this.",
            "Move the plus end with M1 and with some being over the positive points.",
            "OK, and then here we have the same thing K of X, XI.",
            "And now we have this term here.",
            "Again, I'll write this in terms of the kernel, so this is sum of inj ranging overall negative points.",
            "So here I have K of XIXJ and then I'll have the same thing again for the other class, again with a different sign.",
            "So I have one over M 1 ^2.",
            "Another some overall positive pairs of points XIXJ.",
            "OK, now let's see whether this is correct.",
            "Yeah, so if you compare this to the formulation here on the blackboard.",
            "You can see that.",
            "OK, so here we have minus 2 / M two times this kernel expansion, so up to a factor of two we have this term here.",
            "Here we have plus 2 / M one times the kernel expansion so again up to a factor of two.",
            "We have this thing here.",
            "Then down here we have 1 / M two squared times negative one.",
            "So that's this one.",
            "And we have minus one over and one squared times the sum over the positive.",
            "And again we have this factor of two which is different.",
            "So up to a factor of two in the argument of the decision function we have the same.",
            "And of course this decision function is invariant to scaling with a positive number.",
            "So if I multiply in here with a factor of two this thing the the sine function will not notice it.",
            "So we have the same solutions.",
            "OK, so.",
            "Let's actually see.",
            "So now that we've spent quite a bit of time of this, let's proceed.",
            "Let's see how this kind of classifier actually does, so it's hope you will agree.",
            "It's a very simple type of classifier.",
            "We're just mapping our points into some other space, and then we compute the mean of the two classes and check whether the point is close to the window to the other one, yes?",
            "OK, I'll tell you, I'll tell you in a second.",
            "It's a good question.",
            "Yeah.",
            "Why is it called passing windows?",
            "So.",
            "OK, so also a little demo where I'm using a kernel that I haven't told you about yet, so I'm using some specific function K. I'll get to that later.",
            "Don't worry for it now.",
            "But what I'm doing at generating points of the two classes.",
            "And actually I would start with this setting here.",
            "You see what happens?",
            "I haven't used this in a while.",
            "OK, so here you see the decision boundary, and it's pretty much a straight line.",
            "Let's say it's a straight line for now and you can see that it's sort of in between or it separates orthogonal to the vector.",
            "Connecting the class means.",
            "Now let's say I use a slightly different kernel.",
            "I'm changing a parameter here in my similarity measure.",
            "Now make the whole thing a little bit more nonlinear, and also I'll make the.",
            "Problem a little bit more difficult maybe.",
            "See what happens.",
            "OK, this is already too difficult.",
            "I have to make my problem.",
            "I have to make my kernel norm nonlinear and see what happens then.",
            "OK, here I get the solution which separates the problem.",
            "Again, this is still a hyperplane in the feature space.",
            "I've only changed my similarity measure.",
            "In other words, I've changed.",
            "I think I've changed key, which is equivalent to saying I've changed."
        ],
        [
            "This mapping, or which is almost the same as saying I've changed this mapping fine, so I've changed the way I'm embedding my points in the feature space.",
            "So why is this called parsing Windows?",
            "There is a?",
            "Yeah, classical algorithm from statistics used for density estimation, which is called 1000 Windows estimator, and it's very simple.",
            "What it does, it just puts a function a kernel function on each data point.",
            "For instance a Gaussian.",
            "Let's say you use a Gaussian which is normalized to have integral one.",
            "You put one of these Gaussians on all points of the positive class and then you normalize by the number of points in the positive class, which means that the resulting.",
            "Expansion will still have integral one, so this is a model of a density wherein away the density is modeled by just smearing out each point that you have seen each observation.",
            "And likewise here we have a density model of the negative class.",
            "So here we have a constant, but let's ignore this for now.",
            "So we have a model of the positive class model elective class and basically we're just roughly speaking, we're just checking whether it's more likely that the point.",
            "Comes from the positive class or from the negative class.",
            "So which of the two density models makes the point more more likely?",
            "So we've recovered something that's known in classical statistics.",
            "Some of you may know it or may not know it.",
            "Apartment windows based classifier.",
            "However, in our formulation this classifier has a very simple geometric interpretation as a classifier or.",
            "Come to the vector connecting the class mean.",
            "So classifier that checks with the points it closer to the positive or negative class.",
            "So geometrically this is very simple and we will actually exploit dormitory all the time.",
            "When we talk about kernel methods.",
            "So we always try to construct algorithms that come with some geometric intuition."
        ],
        [
            "OK.",
            "So now that I've talked about kernels already in about these mappings into other spaces, let's give an example of why such a mapping could actually be useful.",
            "You've already seen in the demo that I showed you I was.",
            "I was mucking around with some parameter of the kernel and suddenly the problem became solvable.",
            "So what is all this about?",
            "Let's assume we have a problem which is again the two class problem.",
            "So the problem is to separate these blue crosses from the red circles.",
            "And let's assume that true decision boundary, which we don't know is an ellipse, so we only given these data points.",
            "So again, you can think of these as faces and the blue ones have non faces.",
            "We don't know this decision boundary.",
            "We would like to estimate it from the data that we're given.",
            "So in this case, one way to do it would be to consider a map into a feature space which is now higher dimensional space.",
            "3 dimensions which is constructed such that the new coordinates are products of order two in the old coordinates, so called monomials of degree two.",
            "We have two coordinates in input domain, so each point is a 2 dimensional vector and we compute a 3 dimensional representation by taking all possible products of two input.",
            "The coordinates and they're just three such products X 1 ^2 X two squared and X 1 * X Two.",
            "And please ignore this factor sqrt 2.",
            "From now it's just scaling of one of these axis.",
            "Strange.",
            "OK, I think this thing is gradually dying.",
            "I might use the mouse pointer so it's just a scaling of the axis.",
            "Now if you think about this ellipse for a second, this is a simple axis aligned ellipse, and if you were to write down the equation for this ellipse exactly, a linear equation in X1 squared and X2 squared the ellipse equation for such an ellipse.",
            "Which means that if you write it in the new coordinates in that new space, it's a linear equation in or enough in equation in Z1 and Z3.",
            "Geometrically this means the set of all points satisfying the equation.",
            "The ellipse equation in the new space will be simply a linear hyperplane.",
            "So the separation between the inside and the outside of the ellipse in this 3 dimensional space is just a hyperplane separation.",
            "So in a way you might have wondered before."
        ],
        [
            "White."
        ],
        [
            "We consider classifiers that have such a simple decision boundary, which is just a hyperplane.",
            "The reason is that."
        ],
        [
            "We make the decision boundary more complete."
        ],
        [
            "By considering other representations of the data.",
            "So in this case, for instance, using this kind of mapping, we get decision boundaries that are..."
        ],
        [
            "But of course, we're not interested in 2 dimensional data, otherwise we wouldn't be doing machine learning.",
            "We would be doing statistics, so if there any statisticians in the room, excuse me.",
            "It's not true anymore.",
            "Statistics has also changed.",
            "So we are interested in dimensional 310 points.",
            "Yes, sorry.",
            "It's a difficult question and in a way it's a question you can still ask at the end of the summer school and you still won't really have an answer.",
            "So that's how do you choose the mapping?",
            "Or how do you choose the kernel?",
            "It's difficult question then I'll come back to this and probably other lecture results are.",
            "So suppose we have in dimensional input, so patterns and we want to consider product not of order two but of order D. Obviously if you wanted to do the same thing here, so compute all products of order D."
        ],
        [
            "In any input variables, there's a lot of these products.",
            "It's a community Tauriel number.",
            "Yeah, actually."
        ],
        [
            "Rose like into the power of the so even in this fairly low dimensional examples.",
            "Here we have inputs of size 16 by 16 deck and smaller than that faces that I showed you before.",
            "I think the faces were 20 by 20 or something like that.",
            "So even in this low dimensional example, if we wanted to consider the space spanned by all products of all the five, we would already be a space of dimension around 10 to the power of 10.",
            "So we don't really want to do this in practice."
        ],
        [
            "However.",
            "The nice thing is we don't have to do it and we can still work in this representation and to see this, let me go back again to this mapping from 2 slides ago.",
            "So I'm going to consider this mapping here."
        ],
        [
            "Do it, I'll take two points.",
            "X&X Prime met them both into that representation and then take the drug product."
        ],
        [
            "So here we have X&X prime.",
            "And this is the image of X.",
            "This is the image of X prime and if you think about this for a second, I could make this another exercise, but I think we'll move on now.",
            "If the dot product, this gives you three terms gives you this.",
            "Times this this times this and these times this and then you sum of all three and if you do this you will notice it's a complete binomial formula which you can actually write as the square of this blood product here.",
            "Maybe I'll just do it briefly here.",
            "OK, so this is just the dot product between the two vectors in the first row.",
            "OK, and.",
            "I can rewrite this.",
            "There's this thing.",
            "OK, so here I've just swapped the order of the terms.",
            "OK, and this probably looks familiar to all of you.",
            "I've got this is the same as.",
            "Is my writing lot enough for people in the back?",
            "OK or less, but the people in the back are probably smarter.",
            "That's why they're sitting so far away, so they managed to derive it themselves, so it's the square of this quantity here at this point in the year, of course, is the dot product taken in the input space, so it's just the dot product and I'm always using these angular brackets for that product taken in the input space raised to the power of two.",
            "So it's a simple function of X&X prime that we can evaluate directly in the input space.",
            "It's an example of a kernel.",
            "The kernel, because as I said before, it kernels are symmetric functions of X&X prime which have the property that they correspond to DOT products in some other representation that here the other representation is simply the representation computing or products of order 2.",
            "So."
        ],
        [
            "Of course, we're not interested in this 2 dimensional case again, but the nice thing is the same trick also works in the in dimensional case.",
            "Now, in the end dimensional case, what we're going to do is we will actually go backwards."
        ],
        [
            "So I'll just start from like a generalization of this expression here and workout what."
        ],
        [
            "Means so.",
            "I'll take 2 N dimensional vectors and some D, which is any number, any natural number.",
            "So I'll take the dot product raised to the power of D. The product is this formula here raised to the power of D. If I multiply it out, I get these sums in the front and back.",
            "Here I get the corresponding products and.",
            "If you look at this, it's just a big sum and here we have a function of X same function of X prime.",
            "So this is actually a dot product after some transformation where the elements of the transformation are spent by these kinds of functions.",
            "And what are these functions?",
            "They're just mono meals of degree D, so they are products of the input variables of these vectors X&X prime.",
            "So actually now we've gone backwards, but we've shown that.",
            "This kernel here is actually nothing else but that product in this representation, and this representation is simply the space spanned by all monomials of degree D. And just as a little aside.",
            "These are all ordered products, so X1 and X2 I have like X1 and X2 but I also have X2 and X1 so some of the terms appear several times and that's why in this slide I have this factor sqrt 2 here.",
            "But don't worry bout this for now.",
            "So when is the coffee break?",
            "Is anyone know?",
            "Ben 1045 OK so we have 5 more minutes."
        ],
        [
            "So maybe I'll.",
            "Take three more slides and then we make a break.",
            "So here's a theorem from functional analysis that people in the early days of kernel machines research.",
            "Have used to characterize the class of kernels that corresponds to mappings into other spaces.",
            "It's called versus theorem.",
            "It's from the beginning of the last century.",
            "As follows, if we have a continuous term of a positive, difficult definite integral operator on some Hilbert space that I'm not going into detail on.",
            "So by this I mean we have a function symmetric function of X&X prime.",
            "And by kernel of positive definite integral operator, I mean that whenever I take 2 functions for the same function F of XF of X prime here from this space and compute this integral here I get a non negative number.",
            "So suppose I have such a continuous symmetric function which has this property over this function space.",
            "If that's the case, then this function can be expanded in terms of its eigen values and eigen functions.",
            "Just like and I can, I can back to expansion in matrix theory only in the dimensional case.",
            "So something like this, where all eigenvalues are non negative, so that's due to the fact that it's a positive definite integral operators.",
            "So this inequality here implies this inequality.",
            "OK, so once we have this expansion we can actually use this expansion to construct a map into the feature space such that K computes the dot product in that feature space."
        ],
        [
            "How do we do this?",
            "Let's define the map as follows.",
            "The first coordinate is sqrt 1 to 1 * 5, One upside 1, first eigenfunction evaluated at X and so on.",
            "So if the."
        ],
        [
            "If the operator has infinitely many non zero eigenvalues then this means this mapping here is perfect."
        ],
        [
            "We are mapping into an infinite dimensional space.",
            "And if I define this mapping and if you know that, take the dot products or 5X and 5X prime.",
            "Using this mapping then you will recover the kernel.",
            "Why is that?",
            "Well, I just plug it in so this is the image of the point X.",
            "That's the image of X prime.",
            "If you take this out product, you just multiply corresponding term and some over all of them.",
            "So you get this expansion, which of course is the item function expansion from the last slide.",
            "So you recover the kernel.",
            "So let me sum up what we've seen so far, and then we may we go for the coffee break.",
            "We think of the kernel function as some kind of similarity measure."
        ],
        [
            "Which may or may not be nonlinear.",
            "It may even be a similarity measure based on a space which is not a vector space, in which case it doesn't make sense to talk about whether it's linear or not.",
            "So we have some kind of general similarity measure.",
            "So we can define it also on data that's not vectorial to begin with, because whenever we have such a similarity measure, we get a vector space representation for free.",
            "We get a representation in the feature space implicitly whenever we have an algorithm that only depends on the products, we can actually substitute kernel functions for the products.",
            "So if we have some geometric algorithm that can be done in terms of products.",
            "We can substitute kernel functions wherever we had to compute a dot product, and by this we implicitly carry out the algorithm in the feature space.",
            "So implicitly we carry out the algorithm in the representation induced by the mapping file.",
            "Here are some examples of kernels that people are using polynomial kernel option you before I've shown you the case where C is equal to 0, it turns out also for positive C This is a valid kernel.",
            "This one I'm not going to discuss because it's a bit of a tricky story, whether it's a kernel or not, so I'm not saying that this is positive definite.",
            "I'm just saying people are using it because it somehow reminds them of neural networks.",
            "This one is a positive definite kernel that people are using a lot and that's the one I've been using in the.",
            "Simulation on this little for example on the screen.",
            "And there's also a lot of work I should mention on kernels in the Gaussian process prediction community.",
            "Maybe we will hear something about this in some of the other lectures where clones play the role of covariance functions.",
            "So in the next lecture I will tell you a bit more about kernels.",
            "In particular, I give you a different definition of kernel functions, which is probably better than this.",
            "Definition in terms of the Mercer theorem, because it's slightly more general and also it gives you a nice intuition for how the feature space looks for different kernel functions.",
            "And also I will give you.",
            "Pretty much complete proof for how you construct the feature space associated with the kernel.",
            "So far I've just shown you the Mercer theorem and you had to believe it or not in the next lecture I'll tell you a bit more details about this, so we'll have the coffee break now, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which one is it?",
                    "label": 0
                },
                {
                    "sent": "Is this one?",
                    "label": 0
                },
                {
                    "sent": "If you want, you can take it.",
                    "label": 0
                },
                {
                    "sent": "I don't need it.",
                    "label": 0
                },
                {
                    "sent": "Maybe turn it down a little bit because there's some.",
                    "label": 0
                },
                {
                    "sent": "You should keep it somewhere that it changed my face because he that's his problem with me.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "And you have a laptop.",
                    "label": 0
                },
                {
                    "sent": "Specially if it has MATLAB installed, please bring it along to the practical session.",
                    "label": 0
                },
                {
                    "sent": "So I've seen that a lot of people have laptops here, so we shouldn't be short on laptops.",
                    "label": 0
                },
                {
                    "sent": "Thing is, of course you should feel free to interrupt the lecturers at anytime.",
                    "label": 0
                },
                {
                    "sent": "It's a summer school and no question is stupid enough for us, so please feel free also to ask similar questions, specially in the morning courses.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yes, so there's one more technical thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but so we have someone who's recording the lectures, and it's quite difficult to do the sound so so this is an announcement to the lecturers.",
                    "label": 0
                },
                {
                    "sent": "Just imagine that little imaginary area here that you shouldn't step outside.",
                    "label": 0
                },
                {
                    "sent": "Also, don't Walker around all the way, which you can't anyway, because this cable is limited to try to stay somewhere around the center.",
                    "label": 0
                },
                {
                    "sent": "OK. You should sign into that.",
                    "label": 0
                },
                {
                    "sent": "How is it like this for the camera, man?",
                    "label": 0
                },
                {
                    "sent": "It's OK here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the subject of my lecture is the introduction to kernel methods, but since it's the first lecture in the series, maybe it should also be a little bit of an introduction to machine learning in general, so it would be interesting to know how many here do not know machine learning.",
                    "label": 0
                },
                {
                    "sent": "So who comes with a background that doesn't include machine learning?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Looks like it's maybe 10%.",
                    "label": 0
                },
                {
                    "sent": "Louder is it OK like this?",
                    "label": 0
                },
                {
                    "sent": "So 10% of the people don't have a machine learning background.",
                    "label": 0
                },
                {
                    "sent": "Maybe not, it's too loud.",
                    "label": 0
                },
                {
                    "sent": "So, and how many of you would say they?",
                    "label": 0
                },
                {
                    "sent": "No, a little bit, but not really much.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe that's.",
                    "label": 0
                },
                {
                    "sent": "Another 20%.",
                    "label": 0
                },
                {
                    "sent": "And how many know already a lot?",
                    "label": 0
                },
                {
                    "sent": "I mean, how many are writing papers in machine learning journals of the machine learning conferences?",
                    "label": 0
                },
                {
                    "sent": "So I think you can reach it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so yeah about another 20% and the rest is somewhere in the middle I guess.",
                    "label": 0
                },
                {
                    "sent": "So there will be a lot of advanced lectures that cater to the requirements of these advanced guys and but I will try to start a little bit more basic.",
                    "label": 0
                },
                {
                    "sent": "So let me start with a little demo of what machine learning can do.",
                    "label": 0
                },
                {
                    "sent": "Let's see whether it works, so that's always a bit of a risk.",
                    "label": 0
                },
                {
                    "sent": "So what I have here is face detection system, and we can also use it so that people can get get to know each other a bit better, or the quality is not very good and.",
                    "label": 0
                },
                {
                    "sent": "I can install this here.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll not use the microphone from signal.",
                    "label": 0
                },
                {
                    "sent": "And I have to focus this.",
                    "label": 0
                },
                {
                    "sent": "Actually I tried it in the break.",
                    "label": 0
                },
                {
                    "sent": "I tried it in the break but the light was still on.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Suggest for a minute?",
                    "label": 0
                },
                {
                    "sent": "Excuse me, I'll switch on the light.",
                    "label": 0
                },
                {
                    "sent": "'cause maybe it will make things easier.",
                    "label": 0
                },
                {
                    "sent": "And then I go like this.",
                    "label": 0
                },
                {
                    "sent": "OK, now you see quite a few face detections lighting up.",
                    "label": 0
                },
                {
                    "sent": "Some of them are faces, some of them are not faces.",
                    "label": 0
                },
                {
                    "sent": "But of course it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a pretty poor quality camera in the fairly cluttered scene.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We switch it off again, So what you are seeing here is a.",
                    "label": 0
                },
                {
                    "sent": "So called support vector machine that some of you some of you may.",
                    "label": 0
                },
                {
                    "sent": "Some of you may know and others maybe not.",
                    "label": 0
                },
                {
                    "sent": "It's A kind of classifier that's trained on data.",
                    "label": 0
                },
                {
                    "sent": "In this case the data are positive and negative examples.",
                    "label": 0
                },
                {
                    "sent": "So there are examples of faces in examples of non faces.",
                    "label": 0
                },
                {
                    "sent": "So for instance background clutter in the classifiers trained on a few thousand such examples and afterwards can automatically recognize faces from non faces.",
                    "label": 0
                },
                {
                    "sent": "And so this is a problem of binary pattern recognition or binary classification.",
                    "label": 0
                },
                {
                    "sent": "We have two different classes of objects and we would like to find a general rule which will assign new inputs to one of these two classes face online phase.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So let me start with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some informal thoughts on machine learning.",
                    "label": 0
                },
                {
                    "sent": "In my lecture I will be fairly non theoretical because there are other lectures lectures.",
                    "label": 0
                },
                {
                    "sent": "For instance on statistical learning theory that will afterwards provide you with the statistical basis for what I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "So I will not talk much about statistics.",
                    "label": 0
                },
                {
                    "sent": "I will talk about some informal ideas and machine learning a little bit about functional analysis, but no statistics because other lecturers will supply you with that later on.",
                    "label": 0
                },
                {
                    "sent": "So for my purposes, motivation for learning is quite simple, so we have some data taken from input and output domains X&Y.",
                    "label": 0
                },
                {
                    "sent": "So for instance X could be classified images of a certain size that contains faces or non faces.",
                    "label": 0
                },
                {
                    "sent": "And why could be plus or minus 1 + 1 meaning face minus one meaning on face.",
                    "label": 0
                },
                {
                    "sent": "So it's just a set containing two elements and when I train a classifier, I'm given a training set that consists of pairs X&Y.",
                    "label": 0
                },
                {
                    "sent": "So some of them faces some of them, not faces and machine learning.",
                    "label": 0
                },
                {
                    "sent": "People often talk about something called generalization.",
                    "label": 0
                },
                {
                    "sent": "So given a number of examples you would like to make a prediction on new examples.",
                    "label": 0
                },
                {
                    "sent": "So you would like to find a general rule that will generalize to new examples that you haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "So in particular, given a previously unseen X.",
                    "label": 0
                },
                {
                    "sent": "So given a new face or non face Patch you would like to assign it to the right class.",
                    "label": 0
                },
                {
                    "sent": "So you would find to predict the you would like to predict the right output Y for it.",
                    "label": 0
                },
                {
                    "sent": "And one way to think about this would be to say, well in some sense, my prediction.",
                    "label": 0
                },
                {
                    "sent": "Why would be reasonable if the new pair X&Y.",
                    "label": 0
                },
                {
                    "sent": "So I'm given an X and I want to find the corresponding why is somehow similar to the ones in the training set, so this is the training set here, but then of course the question comes up how do we measure similarity and outputs?",
                    "label": 0
                },
                {
                    "sent": "People usually use something which is called a loss function to measure similarity.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if we have labels plus or minus one, we could just say our losses zero whenever the label is correct.",
                    "label": 0
                },
                {
                    "sent": "So whenever we say face and it's really a phase or losses zero.",
                    "label": 0
                },
                {
                    "sent": "Whenever we say non phase and it's really a non face our losses zero likewise.",
                    "label": 0
                },
                {
                    "sent": "But for the inputs it's a little bit less trivial how to measure the similarity between different inputs and that's where a concept comes up which is called a kernel which is basic for a number of different learning algorithms nowadays.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I would like to spend some time on kernels.",
                    "label": 0
                },
                {
                    "sent": "So what I will consider as possible kernels are symmetric functions so defined on X * X.",
                    "label": 0
                },
                {
                    "sent": "So I have two inputs, X&X prime.",
                    "label": 0
                },
                {
                    "sent": "So there could be 2 phases, or there could be 2 non phases, one phase, one phase and I would like to assign kernel value K of X, X prime to these two inputs.",
                    "label": 0
                },
                {
                    "sent": "And one example of such a kernel.",
                    "label": 0
                },
                {
                    "sent": "So this this function K is called the kernel and this function now captures somehow our notion of similarity for the moment, and one example that we could use if our input domain is a vector space to the end.",
                    "label": 0
                },
                {
                    "sent": "We could just consider what's called the Canonical dot product between the two vectors X&X prime.",
                    "label": 0
                },
                {
                    "sent": "So this is some overripe coordinates of X times the Earth coordinate of X prime.",
                    "label": 0
                },
                {
                    "sent": "Now, if X is not a dot product space and it's not obvious why it should be a dot product space.",
                    "label": 0
                },
                {
                    "sent": "I mean, even in the case of images, you first have to represent them as vectors before you can take DOT products.",
                    "label": 0
                },
                {
                    "sent": "But if it's not about product space then we will assume that at least K has a representation as a dot product space as a dot product in some other space, which is a dot product space.",
                    "label": 0
                },
                {
                    "sent": "So we're assuming that if X&X prime are not living it or product space, then at least.",
                    "label": 0
                },
                {
                    "sent": "That can be mapped into some dot product space using a mapping file, which we can think of as our representation.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's also called the feature map.",
                    "label": 0
                },
                {
                    "sent": "So we assume we have a mapping into some other space which is a dot product space.",
                    "label": 0
                },
                {
                    "sent": "In there we have a dot product, so the K corresponds to a dot product in some other space at least.",
                    "label": 0
                },
                {
                    "sent": "So in that case we can think of our inputs or sometimes they called patterns.",
                    "label": 0
                },
                {
                    "sent": "So the excess of inputs we can think of them as actually 5X5 X prime and then that other space that's not our product space.",
                    "label": 0
                },
                {
                    "sent": "We can do whatever we can do based on the products we can carry out to your metric algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now I'll show you a simple example of such an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which could be used, for instance for face detection, although it won't be the very best algorithm that you can use for that.",
                    "label": 0
                },
                {
                    "sent": "It's not such a bad one.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose we have two classes of points, so we have some points 5 XI which belong which form the positive class.",
                    "label": 0
                },
                {
                    "sent": "So the ones that have a label plus one and we have some points over here, the circles 5 XI that belong to the negative class, and in addition we have some test points that I'll just write SX.",
                    "label": 0
                },
                {
                    "sent": "So BF X is a shorthand for.",
                    "label": 0
                },
                {
                    "sent": "The image under fire of some point X, so the input points.",
                    "label": 0
                },
                {
                    "sent": "They come from some set cartographic X on the last slide and we map them into a dot product space and then what we're going to do is we will compute the mean of all positive points over here.",
                    "label": 0
                },
                {
                    "sent": "So M M1 is a number of positive points.",
                    "label": 0
                },
                {
                    "sent": "These are the positive points and compute the mean of all of them.",
                    "label": 0
                },
                {
                    "sent": "I'll do the same for the negative points over here.",
                    "label": 0
                },
                {
                    "sent": "And then given a test point that could be anywhere, but after only down here.",
                    "label": 0
                },
                {
                    "sent": "I will assign this test point onto the class whose mean is closer.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If you think about it for a second, what kind of decision rule does this?",
                    "label": 0
                },
                {
                    "sent": "What kind of decision boundary does this rule imply?",
                    "label": 0
                },
                {
                    "sent": "Then it's clear that the set of all points that are closer to see minors then there are to see plus forms what's called 1/2 space.",
                    "label": 0
                },
                {
                    "sent": "So all the points on the right hand side of this line, or in general hyperplane will be closer to see minus than they are to see plus.",
                    "label": 0
                },
                {
                    "sent": "And we can workout this decision rule and write it.",
                    "label": 0
                },
                {
                    "sent": "As you will see, as a kernel expansion which looks quite similar to a number of other classifiers, such as the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you how you can do this.",
                    "label": 0
                },
                {
                    "sent": "One way of doing this and this as follows.",
                    "label": 0
                },
                {
                    "sent": "Remember that we know that our kernel is the same as the DOT product between patterns map by FI.",
                    "label": 0
                },
                {
                    "sent": "So we can compute DOT product in this representation.",
                    "label": 0
                },
                {
                    "sent": "So we have to somehow express this decision rule in terms of products and one way to do this is the following.",
                    "label": 0
                },
                {
                    "sent": "We can compute this vector W, which is the vector connecting the two class means.",
                    "label": 0
                },
                {
                    "sent": "Moreover, we could compute this vector C which is halfway in between.",
                    "label": 0
                },
                {
                    "sent": "The two class means and we compute the vector connecting C2X and that's the vector X -- Y.",
                    "label": 0
                },
                {
                    "sent": "And now we only have to check whether the angle between these two BF vectors is larger than 90 degrees or smaller than 90 degrees.",
                    "label": 0
                },
                {
                    "sent": "And depending on that, X has to be on this side of the hyperplane or on the other side.",
                    "label": 0
                },
                {
                    "sent": "OK, so feel free to interrupt at any point.",
                    "label": 0
                },
                {
                    "sent": "And one way to compute this angle is to compute the whether the angle is smaller or larger than 90 degrees is simply to check the algebraic sign of the cosine enclosed by the two vectors.",
                    "label": 0
                },
                {
                    "sent": "Now the cosine enclosed by the two vectors is essentially the same as the dot product between the two vectors up to a scaling factor.",
                    "label": 0
                },
                {
                    "sent": "So if you Remember Remember that product from school there's a correspondence between DOT products and angles by the cosine dot product computes the cosine up to scaling factor.",
                    "label": 0
                },
                {
                    "sent": "So if we're interested in whether the angle is smaller or larger than 90 degrees, we just have to check whether the product is positive or negative.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we compute the dot product between this vector and this one.",
                    "label": 0
                },
                {
                    "sent": "I've already told you how these vectors look, so now it's just a matter of plugging it in.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you do this.",
                    "label": 0
                },
                {
                    "sent": "You actually get this decision were here, so you get an expansion over the positive points dot products with the test point normalized by the number of positive points.",
                    "label": 0
                },
                {
                    "sent": "Likewise, you here get an expansion over the negative points, normalized over the number of negative points and back.",
                    "label": 0
                },
                {
                    "sent": "Here you have a constant, by which I mean all terms that don't depend on X don't depend on the test point.",
                    "label": 0
                },
                {
                    "sent": "Now since we have this in terms of products, we can substitute kernel functions for their products.",
                    "label": 0
                },
                {
                    "sent": "So then what you get is this solution here, and that's the decision rule.",
                    "label": 0
                },
                {
                    "sent": "So that's a function or decision function.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a function that will take the value plus one on this half space and minus one on this half space.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The derivation is pretty straightforward and I haven't gone into detail on it, but.",
                    "label": 0
                },
                {
                    "sent": "Actually I would like to have some involvement of the audience now because I find especially in the morning if I just listen to a lecture, I'm getting tired rapidly, so I'll take a few minutes off and let you derive this.",
                    "label": 0
                },
                {
                    "sent": "But I would like to like you to derive it's in a slightly different way, so 'cause what I did was actually.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complicated right?",
                    "label": 0
                },
                {
                    "sent": "I had to compute this vector then this vector take the dot product.",
                    "label": 0
                },
                {
                    "sent": "There's a more direct way of doing it, which is simply to compute the distance between C plus and X and to compute the distance between C minus and X.",
                    "label": 0
                },
                {
                    "sent": "And then to take the difference of these two distances.",
                    "label": 0
                },
                {
                    "sent": "It depends on whether one distance is larger or the other one is.",
                    "label": 0
                },
                {
                    "sent": "Difference is positive or negative.",
                    "label": 0
                },
                {
                    "sent": "So if we plug this this this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difference into the sine function, we should actually get the same solution.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tell you again, So what you can do and I'll give you 5 minutes and everybody can try it.",
                    "label": 0
                },
                {
                    "sent": "And then we can do it together on the blackboard.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you take this vector, take this vector, compute the difference distances between C plus in X and the distance between C minus in X, and then you take the difference between these two distances.",
                    "label": 0
                },
                {
                    "sent": "And as one more additional hint, if you use the you should be using the squared distance.",
                    "label": 0
                },
                {
                    "sent": "It's easier because the squared distance.",
                    "label": 0
                },
                {
                    "sent": "Can be computed using the dot product easily.",
                    "label": 0
                },
                {
                    "sent": "OK, so any more questions before you.",
                    "label": 0
                },
                {
                    "sent": "Start doing your algebra with pen and paper.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll give you 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "So would now be a good time for the solution.",
                    "label": 0
                },
                {
                    "sent": "Or are most people still working it?",
                    "label": 0
                },
                {
                    "sent": "How many of the people have solved the problem already?",
                    "label": 0
                },
                {
                    "sent": "OK, well then maybe we'll give it another two minutes.",
                    "label": 0
                },
                {
                    "sent": "I'm waiting too long.",
                    "label": 0
                },
                {
                    "sent": "It took you one minute to solve it.",
                    "label": 0
                },
                {
                    "sent": "I'm not just people are not used to it there.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So I can do the solution here in the whiteboards, but.",
                    "label": 0
                },
                {
                    "sent": "Maybe someone is confident he has found the right solution or she has found the right solution and would like to present it.",
                    "label": 0
                },
                {
                    "sent": "I should say we will have a price for the people who find the most solutions.",
                    "label": 0
                },
                {
                    "sent": "And it's it's a very rare item that might one day in the future command high prices if you sell it on eBay.",
                    "label": 0
                },
                {
                    "sent": "'cause we notice when we printed these T shirts, you'll have this little logo builder here in one of the T shirts.",
                    "label": 0
                },
                {
                    "sent": "The logo is upside down.",
                    "label": 0
                },
                {
                    "sent": "We don't know why.",
                    "label": 0
                },
                {
                    "sent": "But we thought you mean something, so we have to do something with this T shirt.",
                    "label": 0
                },
                {
                    "sent": "So I will have occasional little exercise like this in my lectures and whoever solves most of them will get the T shirts.",
                    "label": 0
                },
                {
                    "sent": "Maybe also some problems in the other lectures, but.",
                    "label": 0
                },
                {
                    "sent": "Of course you have to prove that you solved it.",
                    "label": 0
                },
                {
                    "sent": "It's not enough if you tell us you would have to do it here.",
                    "label": 0
                },
                {
                    "sent": "And I'll help you if you want.",
                    "label": 0
                },
                {
                    "sent": "So does that convince anyone?",
                    "label": 0
                },
                {
                    "sent": "Maybe not in the very first lecture.",
                    "label": 0
                },
                {
                    "sent": "I see so many signs that I don't know whom to choose.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll do it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Fun.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have a courageous volunteer here.",
                    "label": 0
                },
                {
                    "sent": "Also talking.",
                    "label": 0
                },
                {
                    "sent": "I formulated in terms of the point that you fixed it.",
                    "label": 0
                },
                {
                    "sent": "And rather than.",
                    "label": 0
                },
                {
                    "sent": "Directly write out everything in the way that you have it on the right side.",
                    "label": 0
                },
                {
                    "sent": "I just left in terms of the C plus and the axis OK, and the particular way I get like it's just the opposite sign of the one that you have on the right side.",
                    "label": 0
                },
                {
                    "sent": "So you get something like this.",
                    "label": 0
                },
                {
                    "sent": "But with the opposite sign, yes.",
                    "label": 0
                },
                {
                    "sent": "It's exactly that only with the opposite sign.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and instead of writing it out in terms of the sums, I'm just leaving in terms of the dot product between the points as you've got them up there.",
                    "label": 0
                },
                {
                    "sent": "OK, just show us OK right here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, do you want the microphone?",
                    "label": 0
                },
                {
                    "sent": "Can you hear him without microphone?",
                    "label": 0
                },
                {
                    "sent": "Maybe you better use it too OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "1st, I'll write down the particular thing that he has, and then I'll start using the distance formulation that he suggested.",
                    "label": 0
                },
                {
                    "sent": "So can you back it up one slide, OK?",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "That we want the sign of the dot product between X -- C. And.",
                    "label": 0
                },
                {
                    "sent": "C + -- C minus.",
                    "label": 0
                },
                {
                    "sent": "And see it's just one half C plus.",
                    "label": 0
                },
                {
                    "sent": "Minus later on I'll just put that in, but right now I'm just going to write things just a couple lines before I get to that point.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is X dot C plus.",
                    "label": 0
                },
                {
                    "sent": "Minus X dot C minus.",
                    "label": 0
                },
                {
                    "sent": "Minus C dot C + + C dot C minus.",
                    "label": 1
                },
                {
                    "sent": "I guess I could have just grouped these to begin with, but we have X dot C plus and then I'm not grouping these together minus T minus and then on the other side we're going to have minus C dot.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I'm just going to write.",
                    "label": 0
                },
                {
                    "sent": "This is a plus.",
                    "label": 0
                },
                {
                    "sent": "C and then it will have a C -- -- C Plus.",
                    "label": 0
                },
                {
                    "sent": "And remember this C is just one half of the sum C + + C minus.",
                    "label": 0
                },
                {
                    "sent": "So we got this dot product.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry you're right.",
                    "label": 0
                },
                {
                    "sent": "That's what I was saying, but just wrote it OK. And C -- -- C plus.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the formulation that you get going from the dot product right here, and I'm going to show that you get something that's.",
                    "label": 0
                },
                {
                    "sent": "Basically, minus this expression times two when you're going through the distance formulation in just a moment.",
                    "label": 0
                },
                {
                    "sent": "OK so I have.",
                    "label": 0
                },
                {
                    "sent": "Basically, X -- Y plus distance squared X -- Y minus.",
                    "label": 0
                },
                {
                    "sent": "Let him let him right on the line and then we'll discuss it.",
                    "label": 0
                },
                {
                    "sent": "If I've gotten it, just run by the sign.",
                    "label": 0
                },
                {
                    "sent": "X -- Y minus end road.",
                    "label": 0
                },
                {
                    "sent": "He said that that's incredible.",
                    "label": 0
                },
                {
                    "sent": "So let me let me rephrase it so so So what he's saying is that if you use if you X -- Y plus and you subtract X -- Y minus yes if you think about when is this function positive, so it will be a function.",
                    "label": 0
                },
                {
                    "sent": "If this part is bigger than the other one, which means if the point is further away from the positive mean, then from the negative.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe you start the other way around, sort of the other way around.",
                    "label": 0
                },
                {
                    "sent": "My mistake, you're exactly right, but as I said, it's just going to be a difference of a minus, so.",
                    "label": 0
                },
                {
                    "sent": "Minus.",
                    "label": 0
                },
                {
                    "sent": "Again, X X -- Y minus.",
                    "label": 0
                },
                {
                    "sent": "And these are just.",
                    "label": 0
                },
                {
                    "sent": "Top products.",
                    "label": 0
                },
                {
                    "sent": "Marcus show new slide.",
                    "label": 0
                },
                {
                    "sent": "Up to.",
                    "label": 0
                },
                {
                    "sent": "I won't talk with this section, I'll just talk about it once I've written it up.",
                    "label": 0
                },
                {
                    "sent": "OK. And on this record.",
                    "label": 0
                },
                {
                    "sent": "You have some cancellations and what's left will be again with the way I've written it.",
                    "label": 0
                },
                {
                    "sent": "Just assign change away from what I had previously.",
                    "label": 0
                },
                {
                    "sent": "Close.",
                    "label": 0
                },
                {
                    "sent": "So cancellation.",
                    "label": 0
                },
                {
                    "sent": "Two ads.",
                    "label": 0
                },
                {
                    "sent": "Dot.",
                    "label": 0
                },
                {
                    "sent": "This is minus.",
                    "label": 0
                },
                {
                    "sent": "This is plus C minus.",
                    "label": 0
                },
                {
                    "sent": "My C plus.",
                    "label": 0
                },
                {
                    "sent": "We have the difference between.",
                    "label": 0
                },
                {
                    "sent": "C plus dot C plus.",
                    "label": 0
                },
                {
                    "sent": "Minus C minus dot C minus.",
                    "label": 0
                },
                {
                    "sent": "OK, OK. Finance is still acceptable.",
                    "label": 0
                },
                {
                    "sent": "I'm reading the X as the transformed version, so think of this as a BF X that he's using.",
                    "label": 0
                },
                {
                    "sent": "Which, like at the top of the slide is used as the shorthand, but it's a little bit confusing, so all these vectors are already living in the feature space that he's using here.",
                    "label": 0
                },
                {
                    "sent": "OK, and I didn't finish writing well.",
                    "label": 0
                },
                {
                    "sent": "First off X dot C + -- T minus and we have two X and.",
                    "label": 0
                },
                {
                    "sent": "The opposite of what I have here and here.",
                    "label": 0
                },
                {
                    "sent": "We have a C + C + -- C minus dot C minus.",
                    "label": 0
                },
                {
                    "sent": "And this has the C plus dot C minus term as a plus and then as a minus, so it cancels and all you're left with is the C + C + * 1/2 an C minus dot C minus time to happen and assigns again just opposite to what?",
                    "label": 0
                },
                {
                    "sent": "Human relations.",
                    "label": 0
                },
                {
                    "sent": "Right, so thank you very much, so it's.",
                    "label": 0
                },
                {
                    "sent": "Different from what I expected, but it also shows that the two formulations are the same, so it counts as a solution.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so he's using some linear algebra just to show that the one formulation that I was suggesting originally here with the DOT product in the distance formulations are the same.",
                    "label": 0
                },
                {
                    "sent": "Of course I guess some of you may have just use this formula as a starting point and directly plugging things and then directly get to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This solution here.",
                    "label": 0
                },
                {
                    "sent": "So maybe maybe I'll just briefly do it so that those people can also double check that they did things correctly, but.",
                    "label": 0
                },
                {
                    "sent": "Well then maybe write down who solved the problem.",
                    "label": 0
                },
                {
                    "sent": "So I forgot your name.",
                    "label": 0
                },
                {
                    "sent": "Patrick OK, so one point for Patrick.",
                    "label": 0
                },
                {
                    "sent": "And maybe also one point for the guy who notice the sign to change.",
                    "label": 0
                },
                {
                    "sent": "No Lawrence.",
                    "label": 0
                },
                {
                    "sent": "OK, you'll be wearing the yellow Jersey tonight.",
                    "label": 0
                },
                {
                    "sent": "And OK, so let's just quickly do it the other way.",
                    "label": 0
                },
                {
                    "sent": "So if we if we directly use this formulation here.",
                    "label": 0
                },
                {
                    "sent": "So just ignore all this in between.",
                    "label": 0
                },
                {
                    "sent": "And directly plug it in.",
                    "label": 0
                },
                {
                    "sent": "So this is the dot product between this vector and itself.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So C minus is.",
                    "label": 0
                },
                {
                    "sent": "Sum over all my negative points.",
                    "label": 0
                },
                {
                    "sent": "Sorry in 1 / M Two.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have.",
                    "label": 0
                },
                {
                    "sent": "This is the first time I'm skipping the second, which is sort of the same right to the second term, which is the same, only that you sum over the positive terms.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me just.",
                    "label": 0
                },
                {
                    "sent": "Ignore this for now.",
                    "label": 0
                },
                {
                    "sent": "So this is a difference between those terms and take the dot product with this difference.",
                    "label": 0
                },
                {
                    "sent": "So I have four terms in this dot product right one is this times this this times this and then the two cross terms and two cross terms are actually the same so.",
                    "label": 0
                },
                {
                    "sent": "What you get is a dot product between 5X and 5X.",
                    "label": 0
                },
                {
                    "sent": "Mr First term, then you get the mixed term which is 2 / M two.",
                    "label": 0
                },
                {
                    "sent": "Times the sum and then the dot product between 5X and 5X I.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit small and then you have the last term which is 1 / M two squared.",
                    "label": 0
                },
                {
                    "sent": "And then we have a double sum.",
                    "label": 0
                },
                {
                    "sent": "Over Inj and here we have 5 XI and five XJ.",
                    "label": 0
                },
                {
                    "sent": "OK so 5X5 X in the mix turn 2 / M two and here is the sum over 5X, 5 XI and then the last term is 1 / M M 2 squared.",
                    "label": 0
                },
                {
                    "sent": "And here we have a double sum 5 X I5 XJ and then we have three more terms of this form that correspond with the second part.",
                    "label": 0
                },
                {
                    "sent": "So we've done the first part now OK.",
                    "label": 0
                },
                {
                    "sent": "So if you then.",
                    "label": 0
                },
                {
                    "sent": "How do we best do this?",
                    "label": 0
                },
                {
                    "sent": "So if you continue with this, what you get is here you get 5X5 X, but actually we will have the same thing again in the second term, so that's going to cancel.",
                    "label": 0
                },
                {
                    "sent": "OK, so this one we can ignore.",
                    "label": 0
                },
                {
                    "sent": "It cancels with the corresponding path in the second term.",
                    "label": 0
                },
                {
                    "sent": "However, what we have left is.",
                    "label": 0
                },
                {
                    "sent": "This term here.",
                    "label": 0
                },
                {
                    "sent": "This is some of our negative points and I'll write this as K of X, XI.",
                    "label": 0
                },
                {
                    "sent": "We know how to compute Dot products, we just substitute the kernel so this is K of X, XI.",
                    "label": 0
                },
                {
                    "sent": "Then we have the same term coming from the second distance, right the distance to the positive class.",
                    "label": 0
                },
                {
                    "sent": "However, with a different sign.",
                    "label": 0
                },
                {
                    "sent": "OK, we will have another term like this.",
                    "label": 0
                },
                {
                    "sent": "Move the plus end with M1 and with some being over the positive points.",
                    "label": 0
                },
                {
                    "sent": "OK, and then here we have the same thing K of X, XI.",
                    "label": 0
                },
                {
                    "sent": "And now we have this term here.",
                    "label": 0
                },
                {
                    "sent": "Again, I'll write this in terms of the kernel, so this is sum of inj ranging overall negative points.",
                    "label": 0
                },
                {
                    "sent": "So here I have K of XIXJ and then I'll have the same thing again for the other class, again with a different sign.",
                    "label": 0
                },
                {
                    "sent": "So I have one over M 1 ^2.",
                    "label": 0
                },
                {
                    "sent": "Another some overall positive pairs of points XIXJ.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's see whether this is correct.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you compare this to the formulation here on the blackboard.",
                    "label": 0
                },
                {
                    "sent": "You can see that.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we have minus 2 / M two times this kernel expansion, so up to a factor of two we have this term here.",
                    "label": 0
                },
                {
                    "sent": "Here we have plus 2 / M one times the kernel expansion so again up to a factor of two.",
                    "label": 0
                },
                {
                    "sent": "We have this thing here.",
                    "label": 0
                },
                {
                    "sent": "Then down here we have 1 / M two squared times negative one.",
                    "label": 0
                },
                {
                    "sent": "So that's this one.",
                    "label": 0
                },
                {
                    "sent": "And we have minus one over and one squared times the sum over the positive.",
                    "label": 0
                },
                {
                    "sent": "And again we have this factor of two which is different.",
                    "label": 0
                },
                {
                    "sent": "So up to a factor of two in the argument of the decision function we have the same.",
                    "label": 0
                },
                {
                    "sent": "And of course this decision function is invariant to scaling with a positive number.",
                    "label": 0
                },
                {
                    "sent": "So if I multiply in here with a factor of two this thing the the sine function will not notice it.",
                    "label": 0
                },
                {
                    "sent": "So we have the same solutions.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's actually see.",
                    "label": 0
                },
                {
                    "sent": "So now that we've spent quite a bit of time of this, let's proceed.",
                    "label": 0
                },
                {
                    "sent": "Let's see how this kind of classifier actually does, so it's hope you will agree.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple type of classifier.",
                    "label": 0
                },
                {
                    "sent": "We're just mapping our points into some other space, and then we compute the mean of the two classes and check whether the point is close to the window to the other one, yes?",
                    "label": 0
                },
                {
                    "sent": "OK, I'll tell you, I'll tell you in a second.",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Why is it called passing windows?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so also a little demo where I'm using a kernel that I haven't told you about yet, so I'm using some specific function K. I'll get to that later.",
                    "label": 0
                },
                {
                    "sent": "Don't worry for it now.",
                    "label": 0
                },
                {
                    "sent": "But what I'm doing at generating points of the two classes.",
                    "label": 0
                },
                {
                    "sent": "And actually I would start with this setting here.",
                    "label": 0
                },
                {
                    "sent": "You see what happens?",
                    "label": 0
                },
                {
                    "sent": "I haven't used this in a while.",
                    "label": 0
                },
                {
                    "sent": "OK, so here you see the decision boundary, and it's pretty much a straight line.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's a straight line for now and you can see that it's sort of in between or it separates orthogonal to the vector.",
                    "label": 0
                },
                {
                    "sent": "Connecting the class means.",
                    "label": 0
                },
                {
                    "sent": "Now let's say I use a slightly different kernel.",
                    "label": 0
                },
                {
                    "sent": "I'm changing a parameter here in my similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Now make the whole thing a little bit more nonlinear, and also I'll make the.",
                    "label": 0
                },
                {
                    "sent": "Problem a little bit more difficult maybe.",
                    "label": 0
                },
                {
                    "sent": "See what happens.",
                    "label": 0
                },
                {
                    "sent": "OK, this is already too difficult.",
                    "label": 0
                },
                {
                    "sent": "I have to make my problem.",
                    "label": 0
                },
                {
                    "sent": "I have to make my kernel norm nonlinear and see what happens then.",
                    "label": 0
                },
                {
                    "sent": "OK, here I get the solution which separates the problem.",
                    "label": 0
                },
                {
                    "sent": "Again, this is still a hyperplane in the feature space.",
                    "label": 0
                },
                {
                    "sent": "I've only changed my similarity measure.",
                    "label": 0
                },
                {
                    "sent": "In other words, I've changed.",
                    "label": 0
                },
                {
                    "sent": "I think I've changed key, which is equivalent to saying I've changed.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This mapping, or which is almost the same as saying I've changed this mapping fine, so I've changed the way I'm embedding my points in the feature space.",
                    "label": 0
                },
                {
                    "sent": "So why is this called parsing Windows?",
                    "label": 0
                },
                {
                    "sent": "There is a?",
                    "label": 0
                },
                {
                    "sent": "Yeah, classical algorithm from statistics used for density estimation, which is called 1000 Windows estimator, and it's very simple.",
                    "label": 0
                },
                {
                    "sent": "What it does, it just puts a function a kernel function on each data point.",
                    "label": 0
                },
                {
                    "sent": "For instance a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Let's say you use a Gaussian which is normalized to have integral one.",
                    "label": 0
                },
                {
                    "sent": "You put one of these Gaussians on all points of the positive class and then you normalize by the number of points in the positive class, which means that the resulting.",
                    "label": 0
                },
                {
                    "sent": "Expansion will still have integral one, so this is a model of a density wherein away the density is modeled by just smearing out each point that you have seen each observation.",
                    "label": 0
                },
                {
                    "sent": "And likewise here we have a density model of the negative class.",
                    "label": 0
                },
                {
                    "sent": "So here we have a constant, but let's ignore this for now.",
                    "label": 0
                },
                {
                    "sent": "So we have a model of the positive class model elective class and basically we're just roughly speaking, we're just checking whether it's more likely that the point.",
                    "label": 0
                },
                {
                    "sent": "Comes from the positive class or from the negative class.",
                    "label": 0
                },
                {
                    "sent": "So which of the two density models makes the point more more likely?",
                    "label": 0
                },
                {
                    "sent": "So we've recovered something that's known in classical statistics.",
                    "label": 0
                },
                {
                    "sent": "Some of you may know it or may not know it.",
                    "label": 0
                },
                {
                    "sent": "Apartment windows based classifier.",
                    "label": 0
                },
                {
                    "sent": "However, in our formulation this classifier has a very simple geometric interpretation as a classifier or.",
                    "label": 0
                },
                {
                    "sent": "Come to the vector connecting the class mean.",
                    "label": 0
                },
                {
                    "sent": "So classifier that checks with the points it closer to the positive or negative class.",
                    "label": 0
                },
                {
                    "sent": "So geometrically this is very simple and we will actually exploit dormitory all the time.",
                    "label": 0
                },
                {
                    "sent": "When we talk about kernel methods.",
                    "label": 0
                },
                {
                    "sent": "So we always try to construct algorithms that come with some geometric intuition.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now that I've talked about kernels already in about these mappings into other spaces, let's give an example of why such a mapping could actually be useful.",
                    "label": 0
                },
                {
                    "sent": "You've already seen in the demo that I showed you I was.",
                    "label": 0
                },
                {
                    "sent": "I was mucking around with some parameter of the kernel and suddenly the problem became solvable.",
                    "label": 0
                },
                {
                    "sent": "So what is all this about?",
                    "label": 0
                },
                {
                    "sent": "Let's assume we have a problem which is again the two class problem.",
                    "label": 0
                },
                {
                    "sent": "So the problem is to separate these blue crosses from the red circles.",
                    "label": 0
                },
                {
                    "sent": "And let's assume that true decision boundary, which we don't know is an ellipse, so we only given these data points.",
                    "label": 0
                },
                {
                    "sent": "So again, you can think of these as faces and the blue ones have non faces.",
                    "label": 0
                },
                {
                    "sent": "We don't know this decision boundary.",
                    "label": 0
                },
                {
                    "sent": "We would like to estimate it from the data that we're given.",
                    "label": 0
                },
                {
                    "sent": "So in this case, one way to do it would be to consider a map into a feature space which is now higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "3 dimensions which is constructed such that the new coordinates are products of order two in the old coordinates, so called monomials of degree two.",
                    "label": 0
                },
                {
                    "sent": "We have two coordinates in input domain, so each point is a 2 dimensional vector and we compute a 3 dimensional representation by taking all possible products of two input.",
                    "label": 0
                },
                {
                    "sent": "The coordinates and they're just three such products X 1 ^2 X two squared and X 1 * X Two.",
                    "label": 0
                },
                {
                    "sent": "And please ignore this factor sqrt 2.",
                    "label": 0
                },
                {
                    "sent": "From now it's just scaling of one of these axis.",
                    "label": 0
                },
                {
                    "sent": "Strange.",
                    "label": 0
                },
                {
                    "sent": "OK, I think this thing is gradually dying.",
                    "label": 0
                },
                {
                    "sent": "I might use the mouse pointer so it's just a scaling of the axis.",
                    "label": 0
                },
                {
                    "sent": "Now if you think about this ellipse for a second, this is a simple axis aligned ellipse, and if you were to write down the equation for this ellipse exactly, a linear equation in X1 squared and X2 squared the ellipse equation for such an ellipse.",
                    "label": 0
                },
                {
                    "sent": "Which means that if you write it in the new coordinates in that new space, it's a linear equation in or enough in equation in Z1 and Z3.",
                    "label": 0
                },
                {
                    "sent": "Geometrically this means the set of all points satisfying the equation.",
                    "label": 0
                },
                {
                    "sent": "The ellipse equation in the new space will be simply a linear hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So the separation between the inside and the outside of the ellipse in this 3 dimensional space is just a hyperplane separation.",
                    "label": 0
                },
                {
                    "sent": "So in a way you might have wondered before.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "White.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We consider classifiers that have such a simple decision boundary, which is just a hyperplane.",
                    "label": 0
                },
                {
                    "sent": "The reason is that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We make the decision boundary more complete.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By considering other representations of the data.",
                    "label": 0
                },
                {
                    "sent": "So in this case, for instance, using this kind of mapping, we get decision boundaries that are...",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But of course, we're not interested in 2 dimensional data, otherwise we wouldn't be doing machine learning.",
                    "label": 0
                },
                {
                    "sent": "We would be doing statistics, so if there any statisticians in the room, excuse me.",
                    "label": 0
                },
                {
                    "sent": "It's not true anymore.",
                    "label": 0
                },
                {
                    "sent": "Statistics has also changed.",
                    "label": 0
                },
                {
                    "sent": "So we are interested in dimensional 310 points.",
                    "label": 0
                },
                {
                    "sent": "Yes, sorry.",
                    "label": 0
                },
                {
                    "sent": "It's a difficult question and in a way it's a question you can still ask at the end of the summer school and you still won't really have an answer.",
                    "label": 0
                },
                {
                    "sent": "So that's how do you choose the mapping?",
                    "label": 0
                },
                {
                    "sent": "Or how do you choose the kernel?",
                    "label": 0
                },
                {
                    "sent": "It's difficult question then I'll come back to this and probably other lecture results are.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have in dimensional input, so patterns and we want to consider product not of order two but of order D. Obviously if you wanted to do the same thing here, so compute all products of order D.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In any input variables, there's a lot of these products.",
                    "label": 0
                },
                {
                    "sent": "It's a community Tauriel number.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rose like into the power of the so even in this fairly low dimensional examples.",
                    "label": 0
                },
                {
                    "sent": "Here we have inputs of size 16 by 16 deck and smaller than that faces that I showed you before.",
                    "label": 0
                },
                {
                    "sent": "I think the faces were 20 by 20 or something like that.",
                    "label": 0
                },
                {
                    "sent": "So even in this low dimensional example, if we wanted to consider the space spanned by all products of all the five, we would already be a space of dimension around 10 to the power of 10.",
                    "label": 0
                },
                {
                    "sent": "So we don't really want to do this in practice.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "The nice thing is we don't have to do it and we can still work in this representation and to see this, let me go back again to this mapping from 2 slides ago.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to consider this mapping here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do it, I'll take two points.",
                    "label": 0
                },
                {
                    "sent": "X&X Prime met them both into that representation and then take the drug product.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have X&X prime.",
                    "label": 0
                },
                {
                    "sent": "And this is the image of X.",
                    "label": 0
                },
                {
                    "sent": "This is the image of X prime and if you think about this for a second, I could make this another exercise, but I think we'll move on now.",
                    "label": 0
                },
                {
                    "sent": "If the dot product, this gives you three terms gives you this.",
                    "label": 0
                },
                {
                    "sent": "Times this this times this and these times this and then you sum of all three and if you do this you will notice it's a complete binomial formula which you can actually write as the square of this blood product here.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll just do it briefly here.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just the dot product between the two vectors in the first row.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "I can rewrite this.",
                    "label": 0
                },
                {
                    "sent": "There's this thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I've just swapped the order of the terms.",
                    "label": 0
                },
                {
                    "sent": "OK, and this probably looks familiar to all of you.",
                    "label": 0
                },
                {
                    "sent": "I've got this is the same as.",
                    "label": 0
                },
                {
                    "sent": "Is my writing lot enough for people in the back?",
                    "label": 0
                },
                {
                    "sent": "OK or less, but the people in the back are probably smarter.",
                    "label": 0
                },
                {
                    "sent": "That's why they're sitting so far away, so they managed to derive it themselves, so it's the square of this quantity here at this point in the year, of course, is the dot product taken in the input space, so it's just the dot product and I'm always using these angular brackets for that product taken in the input space raised to the power of two.",
                    "label": 0
                },
                {
                    "sent": "So it's a simple function of X&X prime that we can evaluate directly in the input space.",
                    "label": 0
                },
                {
                    "sent": "It's an example of a kernel.",
                    "label": 0
                },
                {
                    "sent": "The kernel, because as I said before, it kernels are symmetric functions of X&X prime which have the property that they correspond to DOT products in some other representation that here the other representation is simply the representation computing or products of order 2.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, we're not interested in this 2 dimensional case again, but the nice thing is the same trick also works in the in dimensional case.",
                    "label": 0
                },
                {
                    "sent": "Now, in the end dimensional case, what we're going to do is we will actually go backwards.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll just start from like a generalization of this expression here and workout what.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means so.",
                    "label": 0
                },
                {
                    "sent": "I'll take 2 N dimensional vectors and some D, which is any number, any natural number.",
                    "label": 0
                },
                {
                    "sent": "So I'll take the dot product raised to the power of D. The product is this formula here raised to the power of D. If I multiply it out, I get these sums in the front and back.",
                    "label": 0
                },
                {
                    "sent": "Here I get the corresponding products and.",
                    "label": 0
                },
                {
                    "sent": "If you look at this, it's just a big sum and here we have a function of X same function of X prime.",
                    "label": 0
                },
                {
                    "sent": "So this is actually a dot product after some transformation where the elements of the transformation are spent by these kinds of functions.",
                    "label": 0
                },
                {
                    "sent": "And what are these functions?",
                    "label": 0
                },
                {
                    "sent": "They're just mono meals of degree D, so they are products of the input variables of these vectors X&X prime.",
                    "label": 0
                },
                {
                    "sent": "So actually now we've gone backwards, but we've shown that.",
                    "label": 0
                },
                {
                    "sent": "This kernel here is actually nothing else but that product in this representation, and this representation is simply the space spanned by all monomials of degree D. And just as a little aside.",
                    "label": 0
                },
                {
                    "sent": "These are all ordered products, so X1 and X2 I have like X1 and X2 but I also have X2 and X1 so some of the terms appear several times and that's why in this slide I have this factor sqrt 2 here.",
                    "label": 0
                },
                {
                    "sent": "But don't worry bout this for now.",
                    "label": 0
                },
                {
                    "sent": "So when is the coffee break?",
                    "label": 0
                },
                {
                    "sent": "Is anyone know?",
                    "label": 0
                },
                {
                    "sent": "Ben 1045 OK so we have 5 more minutes.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So maybe I'll.",
                    "label": 0
                },
                {
                    "sent": "Take three more slides and then we make a break.",
                    "label": 0
                },
                {
                    "sent": "So here's a theorem from functional analysis that people in the early days of kernel machines research.",
                    "label": 0
                },
                {
                    "sent": "Have used to characterize the class of kernels that corresponds to mappings into other spaces.",
                    "label": 0
                },
                {
                    "sent": "It's called versus theorem.",
                    "label": 0
                },
                {
                    "sent": "It's from the beginning of the last century.",
                    "label": 0
                },
                {
                    "sent": "As follows, if we have a continuous term of a positive, difficult definite integral operator on some Hilbert space that I'm not going into detail on.",
                    "label": 0
                },
                {
                    "sent": "So by this I mean we have a function symmetric function of X&X prime.",
                    "label": 0
                },
                {
                    "sent": "And by kernel of positive definite integral operator, I mean that whenever I take 2 functions for the same function F of XF of X prime here from this space and compute this integral here I get a non negative number.",
                    "label": 0
                },
                {
                    "sent": "So suppose I have such a continuous symmetric function which has this property over this function space.",
                    "label": 0
                },
                {
                    "sent": "If that's the case, then this function can be expanded in terms of its eigen values and eigen functions.",
                    "label": 0
                },
                {
                    "sent": "Just like and I can, I can back to expansion in matrix theory only in the dimensional case.",
                    "label": 0
                },
                {
                    "sent": "So something like this, where all eigenvalues are non negative, so that's due to the fact that it's a positive definite integral operators.",
                    "label": 0
                },
                {
                    "sent": "So this inequality here implies this inequality.",
                    "label": 0
                },
                {
                    "sent": "OK, so once we have this expansion we can actually use this expansion to construct a map into the feature space such that K computes the dot product in that feature space.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we do this?",
                    "label": 0
                },
                {
                    "sent": "Let's define the map as follows.",
                    "label": 0
                },
                {
                    "sent": "The first coordinate is sqrt 1 to 1 * 5, One upside 1, first eigenfunction evaluated at X and so on.",
                    "label": 0
                },
                {
                    "sent": "So if the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If the operator has infinitely many non zero eigenvalues then this means this mapping here is perfect.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are mapping into an infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And if I define this mapping and if you know that, take the dot products or 5X and 5X prime.",
                    "label": 0
                },
                {
                    "sent": "Using this mapping then you will recover the kernel.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, I just plug it in so this is the image of the point X.",
                    "label": 0
                },
                {
                    "sent": "That's the image of X prime.",
                    "label": 0
                },
                {
                    "sent": "If you take this out product, you just multiply corresponding term and some over all of them.",
                    "label": 0
                },
                {
                    "sent": "So you get this expansion, which of course is the item function expansion from the last slide.",
                    "label": 0
                },
                {
                    "sent": "So you recover the kernel.",
                    "label": 0
                },
                {
                    "sent": "So let me sum up what we've seen so far, and then we may we go for the coffee break.",
                    "label": 0
                },
                {
                    "sent": "We think of the kernel function as some kind of similarity measure.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which may or may not be nonlinear.",
                    "label": 0
                },
                {
                    "sent": "It may even be a similarity measure based on a space which is not a vector space, in which case it doesn't make sense to talk about whether it's linear or not.",
                    "label": 0
                },
                {
                    "sent": "So we have some kind of general similarity measure.",
                    "label": 0
                },
                {
                    "sent": "So we can define it also on data that's not vectorial to begin with, because whenever we have such a similarity measure, we get a vector space representation for free.",
                    "label": 0
                },
                {
                    "sent": "We get a representation in the feature space implicitly whenever we have an algorithm that only depends on the products, we can actually substitute kernel functions for the products.",
                    "label": 0
                },
                {
                    "sent": "So if we have some geometric algorithm that can be done in terms of products.",
                    "label": 0
                },
                {
                    "sent": "We can substitute kernel functions wherever we had to compute a dot product, and by this we implicitly carry out the algorithm in the feature space.",
                    "label": 0
                },
                {
                    "sent": "So implicitly we carry out the algorithm in the representation induced by the mapping file.",
                    "label": 0
                },
                {
                    "sent": "Here are some examples of kernels that people are using polynomial kernel option you before I've shown you the case where C is equal to 0, it turns out also for positive C This is a valid kernel.",
                    "label": 0
                },
                {
                    "sent": "This one I'm not going to discuss because it's a bit of a tricky story, whether it's a kernel or not, so I'm not saying that this is positive definite.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying people are using it because it somehow reminds them of neural networks.",
                    "label": 0
                },
                {
                    "sent": "This one is a positive definite kernel that people are using a lot and that's the one I've been using in the.",
                    "label": 0
                },
                {
                    "sent": "Simulation on this little for example on the screen.",
                    "label": 0
                },
                {
                    "sent": "And there's also a lot of work I should mention on kernels in the Gaussian process prediction community.",
                    "label": 0
                },
                {
                    "sent": "Maybe we will hear something about this in some of the other lectures where clones play the role of covariance functions.",
                    "label": 0
                },
                {
                    "sent": "So in the next lecture I will tell you a bit more about kernels.",
                    "label": 0
                },
                {
                    "sent": "In particular, I give you a different definition of kernel functions, which is probably better than this.",
                    "label": 0
                },
                {
                    "sent": "Definition in terms of the Mercer theorem, because it's slightly more general and also it gives you a nice intuition for how the feature space looks for different kernel functions.",
                    "label": 0
                },
                {
                    "sent": "And also I will give you.",
                    "label": 0
                },
                {
                    "sent": "Pretty much complete proof for how you construct the feature space associated with the kernel.",
                    "label": 0
                },
                {
                    "sent": "So far I've just shown you the Mercer theorem and you had to believe it or not in the next lecture I'll tell you a bit more details about this, so we'll have the coffee break now, thank you.",
                    "label": 0
                }
            ]
        }
    }
}