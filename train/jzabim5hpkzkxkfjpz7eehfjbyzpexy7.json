{
    "id": "jzabim5hpkzkxkfjpz7eehfjbyzpexy7",
    "title": "Learning Mixture of Discrete Distributions over Product Spaces",
    "info": {
        "author": [
            "Prateek Jain, Nuance Communications, Inc."
        ],
        "published": "July 15, 2014",
        "recorded": "June 2014",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2014_jain_discrete_distributions/",
    "segmentation": [
        [
            "Thanks a lot for staying for the last talk of the day.",
            "This is a joint work with seven Goo from UIUC.",
            "So in this talk I will discuss our recent results for learning mixture of discrete distributions, so let."
        ],
        [
            "We briefly describe the problem, so we are given a bunch of discrete data points, and we want to cluster these data points and we assume that each data point is sampled IID from a generative model and the model is the following."
        ],
        [
            "But let's say we have K clusters and we pick let's secure cluster with cruelty WQ.",
            "And then for each cluster we have a distribution DQ associated.",
            "So this distribution is a product distribution over N dimensions and each coordinate can take one of L values."
        ],
        [
            "So that is we are given this end dimensional vector and each of the coordinate can have a different probability distribution Pi.",
            "So let's say for the jail coordinate of the cluster, the distribution is by GQ, which itself can be thought of as a L dimensional vector.",
            "And we are pretty well OK. And now the goal is parameter estimation.",
            "That is, we want to estimate all these probability vector spies as well as the weights of the clusters.",
            "So."
        ],
        [
            "Problem was introduced in 1994 by Michael Kearns and Co.",
            "Authors, and since then several specific cases of the problem has been solved, but the general case is still sort of open and over the years the problem is found.",
            "Applications in a variety of domains like learning theory, crowdsourcing, popular population stratification.",
            "So let me."
        ],
        [
            "Briefly describe an application in crowdsourcing domain which is sort of motivated this work.",
            "So let's say we are in standard crowdsourcing setting where we have N workers and each of the worker is presented a question.",
            "And let's say this question is multiple choice questions.",
            "So after looking at this question, each of the worker will give you know one of let's say L possible answers.",
            "And now each of the worker can have different expertise for different type of tasks.",
            "Maybe they are, they have different expertise and our goal is to assess quality of each worker for a particular question type.",
            "And also to figure out the correct answer by combining answers of all these workers.",
            "So let me now tell you how we model this problem as a problem of learning mixtures of discrete distributions, and for this we use a popular dividend scheme model for crowdsourcing.",
            "So let's say we have K type of tasks.",
            "OK type of questions.",
            "And we pick up one of the cluster with certain cruelty.",
            "And then once picking up the cluster, we will generate a question from this cluster or from this particular task type uniformly at random and will present."
        ],
        [
            "Question to each of the end workers, and now each of the workers will give their answers, which can be one of possible L choices and evidence key models.",
            "Is that given a question, type each of the workers response is independent of each other.",
            "And that is we are opening and dimensional discrete vector whose each coordinate is independent of each other given at particular Question Time.",
            "OK, and our goal is now to estimate all the probability parameters as well as weight of each."
        ],
        [
            "Alright, so there has been a lot of work for this problem over the years and these words can be divided into 2 broad categories.",
            "So first category is where we assume something about distribution to make the distribution easier to work with.",
            "So in a very nice work by Comerica, Charlie and satisfy they showed that if the distribution has means that are far away, then it's simple spectral algorithm will solve the problem efficiently.",
            "And then there's another line of work where there are no assumptions on the distribution, but most of the existing methods.",
            "Either assume that the parameters key in there, which is the number of clusters, or number of choices are either constant or the running time is exponential in.",
            "You know either K or M. So."
        ],
        [
            "For this problem, our contribution has been to come up with the algorithm that is.",
            "That is, polynomial time in all the major parameters which are N, the number of coordinates, key, the number of clusters, and L the number of choices, and we do not need to make any additional spreader mission about for our method and we can handle this small amount of noise as well.",
            "But there are some caveats over results, so one is that our sample complexity depends on the smallest.",
            "The weight of the smallest mean and that is natural because we are working in executive every second you want to recover each of the parameter exactly.",
            "And also our sample complexity depends on condition, number of a particular distribution.",
            "And that's why because of this second caveat, we cannot apply our method to a popular problem that we want to apply, which is Kalief decision tree problem.",
            "OK, and in addition of these two assumptions, we also need to assume that a particular probability matrix is in current.",
            "So I'll come to that in a couple of slides.",
            "So."
        ],
        [
            "So our approach is based on this popular moment making based approach that has recently gained a lot of popularity because of some of the results by anymore.",
            "Daniel too.",
            "Notice, so the key idea behind these approaches is estimate.",
            "Let's a second moment, third moment, or even fourth moment of the data distribution, and then reduce the problem into some sort of tensor decomposition problem.",
            "OK, and our approach is also follows very similar lines, but we cannot estimate the required second moment, third moment type of matrices.",
            "We have some data missing there, so we need to fill in the blanks some, so I'll come to that."
        ],
        [
            "So for.",
            "Simplification of presentation.",
            "I'll first change the representation of our data, so let's say if we have N dimensional vector.",
            "I will represent this end dimensional vector as a end times L dimensional vector.",
            "Where each coordinate now translates into a block.",
            "So for example, this block X1 corresponds to 1st coordinate and it's a 3 dimensional binary vector where this coordinate.",
            "This block is one in the third coordinate and zero in the first 2 coordinates.",
            "A simple binary representation of the data and note that these blocks X1X2.",
            "I don't know if you can see them.",
            "These blocks.",
            "Different blocks are independent of each other because the coordinates are independent of each other, but within each block we can have dependencies, because if we know that the third coordinate of this first block is 1, then the other two coordinates have to deal.",
            "So we have some dependencies of between blocks which create some issues.",
            "So now let me describe the general program that we want to follow to recover the parameter of this mixture, distrib."
        ],
        [
            "Ocean.",
            "So the approach is the following.",
            "We want to estimate this by matrix which is now enabled by K dimensional matrix.",
            "So each column of this matrix represents one cluster and given one column that secures column.",
            "The GF block of this vector is the probability distribution corresponding to JF coordinate of the cluster.",
            "Talking.",
            "So each block itself is a L dimensional vector.",
            "And the."
        ],
        [
            "Approach that we want to follow in this moment matching approach.",
            "So let's say we want to estimate \u03c0 and we write it singular value decomposition.",
            "So it's given by U Sigma we transferred and the goal is to determine U Sigma NV individually and by uniqueness of SVD will be able to recover the exact parameters of the distribution.",
            "So by the way here now in the remaining part of the talk I'll assume that all the weights are equal to each other for simplicity presentation.",
            "So now one can say that OK if I want to estimate this pile, suppose I conform this matrix M2, which is given by 5 * 5, transpose it into to level.",
            "This matrix can typically can be computed using second moment of the distribution.",
            "But in our case it's not exactly that so.",
            "But let's say we can estimate M2 then by eigenvalue decomposition of M2 will be able to recover you and Sigma.",
            "So the only missing part will be this meet this week and for that, let's say we have to go to 3rd order tensor.",
            "So we compute let's say M3, which is by cross 5 + 5 and will whiten this tensor.",
            "That is, multiply this tensor on each mode with this matrix like Sigma inverse U and that will give us we cross V plus Vita Intel which is an automaton because we is a set of orthonormal.",
            "And for this tensor we can use this popular power or tensor power method by Anima and Co.",
            "Authors that can recover a orthonormal tensor.",
            "Exactly.",
            "That is, we can recover back these.",
            "So once we have U Sigma we have record that are parameters."
        ],
        [
            "Alright, so we want to follow this program, but there are some issues.",
            "So first of all, how do you estimate MTN?",
            "So for M2 like a typical guess would be to estimate second moment of the distribution.",
            "So let's look at how that looks so.",
            "And if you compute second moment of the distribution, so let's say data is given by these blocks 1X2 and so on, so the distribution second moment, or the Co variance covariance matrix will look something like this where the off diagonal parts are correct.",
            "'cause I know that block extrudes independent of Edison, so I will get this by 2 * 5 on transfers is good, but the diagonal parts are sort of missing becausw of dependencies because I know that.",
            "X1 if, let's say third coordinate of X one is 1, then other two coordinates have to be zero X one X1.",
            "Transpose will always be a diagonal matrix.",
            "In necessary need not be by 1 * 5 and transfer, so we cannot estimate the second moment or like the required \u03c0 * 5 transmitted from the second moment directly.",
            "And in fact, let's say if we just have one coordinate that is just one worker in the crowdsourcing application and that says that we cannot.",
            "We cannot obtain from the second moment at all.",
            "However, if the number of workers or the number of coordinates in our distribution is much larger than the number of clusters, then the number of observations that we have, which is the off diagonal parts, is much larger than the degrees of freedom, which is sort of just.",
            "Times key and the reason for that is because this matrix has rank here, where K is the number of clusters.",
            "So ultimately, what it reduces to is a low rank matrix completion kind of problem.",
            "That is, we have a rank key matrix, but it's blocked diagonal parts are missing and we need to fill in those block diagonal parts to be able to estimate this matrix by 10 file transfers and use it for the moment making it work.",
            "OK, so now because it's a matrix completion type of problem, we can apply some of the standard tricks there."
        ],
        [
            "And for this particular task we use this alternating minimization approach to fill in the blanks oranges block diagram parts.",
            "So this alternative minimization approach is very simple.",
            "All it says is that on the known entry that is on the non over off diagonal blocks will just iterate over the two components of M2.",
            "That is, let's see if M2 is Ranke.",
            "Then we can represent it as a product of U&V where both you and we have at most key columns and we'll just iterate over both U&V.",
            "To solve the problem.",
            "OK, and we can show that this ordering innovation approach will actually converge to the global optimum, and that's why it will be able to fill in all these blanks in our improvement.",
            "So we'll be able to obtain this file testfile transpose matrix and a similar thing can be done for estimating third moment that is this Picross Picross 5 matrix, But then we don't need to do alternate minimizing.",
            "The simple least squares work here.",
            "OK, so now let me put."
        ],
        [
            "All these things together.",
            "So let's say we have by images using mobile transfers.",
            "Then we form M2 which is using my square.",
            "You transpose plus some error and this error will be there because of finite number of samples and also because alternating imagine will also give you some finite test.",
            "OK, and similarly we can estimate M3 and again we'll have some error because of the finite number of samples as well as the estimation procedure.",
            "And then we whiten this entry using this noisy version of M2 and will get this orthonormal tensor but with some small noise.",
            "And for this problem will have to use a robust version of the power method, which is again proposed by an emergency encoders and what we can show is that by combining all these three methods or all these three steps, we won't increase the error to be too much.",
            "Will have will be able to give reasonable error guarantees so."
        ],
        [
            "I'm getting to the following, so if we assume that the number of samples is growing as something like NQ by accident, squared times some other factors.",
            "Independence Day script in a minute then and if the number of workers is large enough.",
            "So in our original submission we had this N greater than K2 or four.",
            "Now we can just get it to be intense.",
            "So, so we can bring down the number of workers that we need to be significantly low.",
            "So under these two assumptions, we can guarantee that each of the probability vector is estimated up to epsilon.",
            "So here note we have this dependence of the condition number of the distribution matrix.",
            "And at least for crowdsourcing application, this seems to be So what it sort of says is that jointly all the workers combine to ensure that each of the options is can be designated properly.",
            "OK, that is.",
            "So let's say we have three workers and three type of options.",
            "So let's say worker one can differentiate between option A&B but not BNC.",
            "And worker two kind of differentiate between option B&C but not written envy and then work on three can differentiate between AC but not busy.",
            "OK, so each of the individual worker is not good enough and condition number of each of the individual matrix won't be good enough.",
            "But what we can show is that by combining or by concatenating these three probability matrices, the overall condition number becomes reasonable.",
            "That is jointly all these workers will be able to differentiate between the different options and give us the correct answer.",
            "OK, so that's sort of the interpretation in crowdsourcing."
        ],
        [
            "Let me summarize briefly, so we consider this problem of learning mixture of discrete distribution, where each component is a product distribution and for this task we provide a moment matching this method and we provide an algorithm that has sample as well as time complexity is polynomial in all the key parameters.",
            "One the Hue or one drawback of a method is that we have a dependence on condition number of the matrix which does not allow us to apply this result to several important learning problems like K binary decision tree problem.",
            "So we realized that it might not be possible to do it in exact recovery mode.",
            "So we want to study our methods, impact learning model and see if we can do better and also for these.",
            "Problems, especially in the crowd sourcing domain E methods have been known to be very successful, but so far we do not have any rigorous analysis of these matters.",
            "So one question is that can be analyzed.",
            "Easier methods, underseen protocol, Sumption and show that they have to do something.",
            "So."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks a lot for staying for the last talk of the day.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with seven Goo from UIUC.",
                    "label": 1
                },
                {
                    "sent": "So in this talk I will discuss our recent results for learning mixture of discrete distributions, so let.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We briefly describe the problem, so we are given a bunch of discrete data points, and we want to cluster these data points and we assume that each data point is sampled IID from a generative model and the model is the following.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But let's say we have K clusters and we pick let's secure cluster with cruelty WQ.",
                    "label": 0
                },
                {
                    "sent": "And then for each cluster we have a distribution DQ associated.",
                    "label": 0
                },
                {
                    "sent": "So this distribution is a product distribution over N dimensions and each coordinate can take one of L values.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that is we are given this end dimensional vector and each of the coordinate can have a different probability distribution Pi.",
                    "label": 0
                },
                {
                    "sent": "So let's say for the jail coordinate of the cluster, the distribution is by GQ, which itself can be thought of as a L dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "And we are pretty well OK. And now the goal is parameter estimation.",
                    "label": 0
                },
                {
                    "sent": "That is, we want to estimate all these probability vector spies as well as the weights of the clusters.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem was introduced in 1994 by Michael Kearns and Co.",
                    "label": 1
                },
                {
                    "sent": "Authors, and since then several specific cases of the problem has been solved, but the general case is still sort of open and over the years the problem is found.",
                    "label": 1
                },
                {
                    "sent": "Applications in a variety of domains like learning theory, crowdsourcing, popular population stratification.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Briefly describe an application in crowdsourcing domain which is sort of motivated this work.",
                    "label": 0
                },
                {
                    "sent": "So let's say we are in standard crowdsourcing setting where we have N workers and each of the worker is presented a question.",
                    "label": 0
                },
                {
                    "sent": "And let's say this question is multiple choice questions.",
                    "label": 0
                },
                {
                    "sent": "So after looking at this question, each of the worker will give you know one of let's say L possible answers.",
                    "label": 0
                },
                {
                    "sent": "And now each of the worker can have different expertise for different type of tasks.",
                    "label": 0
                },
                {
                    "sent": "Maybe they are, they have different expertise and our goal is to assess quality of each worker for a particular question type.",
                    "label": 1
                },
                {
                    "sent": "And also to figure out the correct answer by combining answers of all these workers.",
                    "label": 0
                },
                {
                    "sent": "So let me now tell you how we model this problem as a problem of learning mixtures of discrete distributions, and for this we use a popular dividend scheme model for crowdsourcing.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have K type of tasks.",
                    "label": 0
                },
                {
                    "sent": "OK type of questions.",
                    "label": 0
                },
                {
                    "sent": "And we pick up one of the cluster with certain cruelty.",
                    "label": 0
                },
                {
                    "sent": "And then once picking up the cluster, we will generate a question from this cluster or from this particular task type uniformly at random and will present.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question to each of the end workers, and now each of the workers will give their answers, which can be one of possible L choices and evidence key models.",
                    "label": 0
                },
                {
                    "sent": "Is that given a question, type each of the workers response is independent of each other.",
                    "label": 0
                },
                {
                    "sent": "And that is we are opening and dimensional discrete vector whose each coordinate is independent of each other given at particular Question Time.",
                    "label": 0
                },
                {
                    "sent": "OK, and our goal is now to estimate all the probability parameters as well as weight of each.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so there has been a lot of work for this problem over the years and these words can be divided into 2 broad categories.",
                    "label": 0
                },
                {
                    "sent": "So first category is where we assume something about distribution to make the distribution easier to work with.",
                    "label": 0
                },
                {
                    "sent": "So in a very nice work by Comerica, Charlie and satisfy they showed that if the distribution has means that are far away, then it's simple spectral algorithm will solve the problem efficiently.",
                    "label": 1
                },
                {
                    "sent": "And then there's another line of work where there are no assumptions on the distribution, but most of the existing methods.",
                    "label": 0
                },
                {
                    "sent": "Either assume that the parameters key in there, which is the number of clusters, or number of choices are either constant or the running time is exponential in.",
                    "label": 1
                },
                {
                    "sent": "You know either K or M. So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this problem, our contribution has been to come up with the algorithm that is.",
                    "label": 0
                },
                {
                    "sent": "That is, polynomial time in all the major parameters which are N, the number of coordinates, key, the number of clusters, and L the number of choices, and we do not need to make any additional spreader mission about for our method and we can handle this small amount of noise as well.",
                    "label": 1
                },
                {
                    "sent": "But there are some caveats over results, so one is that our sample complexity depends on the smallest.",
                    "label": 0
                },
                {
                    "sent": "The weight of the smallest mean and that is natural because we are working in executive every second you want to recover each of the parameter exactly.",
                    "label": 0
                },
                {
                    "sent": "And also our sample complexity depends on condition, number of a particular distribution.",
                    "label": 1
                },
                {
                    "sent": "And that's why because of this second caveat, we cannot apply our method to a popular problem that we want to apply, which is Kalief decision tree problem.",
                    "label": 0
                },
                {
                    "sent": "OK, and in addition of these two assumptions, we also need to assume that a particular probability matrix is in current.",
                    "label": 0
                },
                {
                    "sent": "So I'll come to that in a couple of slides.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach is based on this popular moment making based approach that has recently gained a lot of popularity because of some of the results by anymore.",
                    "label": 0
                },
                {
                    "sent": "Daniel too.",
                    "label": 0
                },
                {
                    "sent": "Notice, so the key idea behind these approaches is estimate.",
                    "label": 1
                },
                {
                    "sent": "Let's a second moment, third moment, or even fourth moment of the data distribution, and then reduce the problem into some sort of tensor decomposition problem.",
                    "label": 1
                },
                {
                    "sent": "OK, and our approach is also follows very similar lines, but we cannot estimate the required second moment, third moment type of matrices.",
                    "label": 0
                },
                {
                    "sent": "We have some data missing there, so we need to fill in the blanks some, so I'll come to that.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for.",
                    "label": 0
                },
                {
                    "sent": "Simplification of presentation.",
                    "label": 0
                },
                {
                    "sent": "I'll first change the representation of our data, so let's say if we have N dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "I will represent this end dimensional vector as a end times L dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "Where each coordinate now translates into a block.",
                    "label": 0
                },
                {
                    "sent": "So for example, this block X1 corresponds to 1st coordinate and it's a 3 dimensional binary vector where this coordinate.",
                    "label": 0
                },
                {
                    "sent": "This block is one in the third coordinate and zero in the first 2 coordinates.",
                    "label": 0
                },
                {
                    "sent": "A simple binary representation of the data and note that these blocks X1X2.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can see them.",
                    "label": 0
                },
                {
                    "sent": "These blocks.",
                    "label": 0
                },
                {
                    "sent": "Different blocks are independent of each other because the coordinates are independent of each other, but within each block we can have dependencies, because if we know that the third coordinate of this first block is 1, then the other two coordinates have to deal.",
                    "label": 0
                },
                {
                    "sent": "So we have some dependencies of between blocks which create some issues.",
                    "label": 0
                },
                {
                    "sent": "So now let me describe the general program that we want to follow to recover the parameter of this mixture, distrib.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "So the approach is the following.",
                    "label": 0
                },
                {
                    "sent": "We want to estimate this by matrix which is now enabled by K dimensional matrix.",
                    "label": 0
                },
                {
                    "sent": "So each column of this matrix represents one cluster and given one column that secures column.",
                    "label": 0
                },
                {
                    "sent": "The GF block of this vector is the probability distribution corresponding to JF coordinate of the cluster.",
                    "label": 0
                },
                {
                    "sent": "Talking.",
                    "label": 0
                },
                {
                    "sent": "So each block itself is a L dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach that we want to follow in this moment matching approach.",
                    "label": 1
                },
                {
                    "sent": "So let's say we want to estimate \u03c0 and we write it singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "So it's given by U Sigma we transferred and the goal is to determine U Sigma NV individually and by uniqueness of SVD will be able to recover the exact parameters of the distribution.",
                    "label": 0
                },
                {
                    "sent": "So by the way here now in the remaining part of the talk I'll assume that all the weights are equal to each other for simplicity presentation.",
                    "label": 0
                },
                {
                    "sent": "So now one can say that OK if I want to estimate this pile, suppose I conform this matrix M2, which is given by 5 * 5, transpose it into to level.",
                    "label": 0
                },
                {
                    "sent": "This matrix can typically can be computed using second moment of the distribution.",
                    "label": 0
                },
                {
                    "sent": "But in our case it's not exactly that so.",
                    "label": 0
                },
                {
                    "sent": "But let's say we can estimate M2 then by eigenvalue decomposition of M2 will be able to recover you and Sigma.",
                    "label": 0
                },
                {
                    "sent": "So the only missing part will be this meet this week and for that, let's say we have to go to 3rd order tensor.",
                    "label": 0
                },
                {
                    "sent": "So we compute let's say M3, which is by cross 5 + 5 and will whiten this tensor.",
                    "label": 0
                },
                {
                    "sent": "That is, multiply this tensor on each mode with this matrix like Sigma inverse U and that will give us we cross V plus Vita Intel which is an automaton because we is a set of orthonormal.",
                    "label": 1
                },
                {
                    "sent": "And for this tensor we can use this popular power or tensor power method by Anima and Co.",
                    "label": 0
                },
                {
                    "sent": "Authors that can recover a orthonormal tensor.",
                    "label": 0
                },
                {
                    "sent": "Exactly.",
                    "label": 0
                },
                {
                    "sent": "That is, we can recover back these.",
                    "label": 0
                },
                {
                    "sent": "So once we have U Sigma we have record that are parameters.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so we want to follow this program, but there are some issues.",
                    "label": 0
                },
                {
                    "sent": "So first of all, how do you estimate MTN?",
                    "label": 0
                },
                {
                    "sent": "So for M2 like a typical guess would be to estimate second moment of the distribution.",
                    "label": 0
                },
                {
                    "sent": "So let's look at how that looks so.",
                    "label": 0
                },
                {
                    "sent": "And if you compute second moment of the distribution, so let's say data is given by these blocks 1X2 and so on, so the distribution second moment, or the Co variance covariance matrix will look something like this where the off diagonal parts are correct.",
                    "label": 0
                },
                {
                    "sent": "'cause I know that block extrudes independent of Edison, so I will get this by 2 * 5 on transfers is good, but the diagonal parts are sort of missing becausw of dependencies because I know that.",
                    "label": 0
                },
                {
                    "sent": "X1 if, let's say third coordinate of X one is 1, then other two coordinates have to be zero X one X1.",
                    "label": 0
                },
                {
                    "sent": "Transpose will always be a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "In necessary need not be by 1 * 5 and transfer, so we cannot estimate the second moment or like the required \u03c0 * 5 transmitted from the second moment directly.",
                    "label": 0
                },
                {
                    "sent": "And in fact, let's say if we just have one coordinate that is just one worker in the crowdsourcing application and that says that we cannot.",
                    "label": 0
                },
                {
                    "sent": "We cannot obtain from the second moment at all.",
                    "label": 0
                },
                {
                    "sent": "However, if the number of workers or the number of coordinates in our distribution is much larger than the number of clusters, then the number of observations that we have, which is the off diagonal parts, is much larger than the degrees of freedom, which is sort of just.",
                    "label": 0
                },
                {
                    "sent": "Times key and the reason for that is because this matrix has rank here, where K is the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "So ultimately, what it reduces to is a low rank matrix completion kind of problem.",
                    "label": 0
                },
                {
                    "sent": "That is, we have a rank key matrix, but it's blocked diagonal parts are missing and we need to fill in those block diagonal parts to be able to estimate this matrix by 10 file transfers and use it for the moment making it work.",
                    "label": 0
                },
                {
                    "sent": "OK, so now because it's a matrix completion type of problem, we can apply some of the standard tricks there.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for this particular task we use this alternating minimization approach to fill in the blanks oranges block diagram parts.",
                    "label": 1
                },
                {
                    "sent": "So this alternative minimization approach is very simple.",
                    "label": 0
                },
                {
                    "sent": "All it says is that on the known entry that is on the non over off diagonal blocks will just iterate over the two components of M2.",
                    "label": 0
                },
                {
                    "sent": "That is, let's see if M2 is Ranke.",
                    "label": 0
                },
                {
                    "sent": "Then we can represent it as a product of U&V where both you and we have at most key columns and we'll just iterate over both U&V.",
                    "label": 0
                },
                {
                    "sent": "To solve the problem.",
                    "label": 0
                },
                {
                    "sent": "OK, and we can show that this ordering innovation approach will actually converge to the global optimum, and that's why it will be able to fill in all these blanks in our improvement.",
                    "label": 1
                },
                {
                    "sent": "So we'll be able to obtain this file testfile transpose matrix and a similar thing can be done for estimating third moment that is this Picross Picross 5 matrix, But then we don't need to do alternate minimizing.",
                    "label": 0
                },
                {
                    "sent": "The simple least squares work here.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let me put.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these things together.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have by images using mobile transfers.",
                    "label": 0
                },
                {
                    "sent": "Then we form M2 which is using my square.",
                    "label": 0
                },
                {
                    "sent": "You transpose plus some error and this error will be there because of finite number of samples and also because alternating imagine will also give you some finite test.",
                    "label": 0
                },
                {
                    "sent": "OK, and similarly we can estimate M3 and again we'll have some error because of the finite number of samples as well as the estimation procedure.",
                    "label": 0
                },
                {
                    "sent": "And then we whiten this entry using this noisy version of M2 and will get this orthonormal tensor but with some small noise.",
                    "label": 0
                },
                {
                    "sent": "And for this problem will have to use a robust version of the power method, which is again proposed by an emergency encoders and what we can show is that by combining all these three methods or all these three steps, we won't increase the error to be too much.",
                    "label": 0
                },
                {
                    "sent": "Will have will be able to give reasonable error guarantees so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm getting to the following, so if we assume that the number of samples is growing as something like NQ by accident, squared times some other factors.",
                    "label": 0
                },
                {
                    "sent": "Independence Day script in a minute then and if the number of workers is large enough.",
                    "label": 1
                },
                {
                    "sent": "So in our original submission we had this N greater than K2 or four.",
                    "label": 0
                },
                {
                    "sent": "Now we can just get it to be intense.",
                    "label": 0
                },
                {
                    "sent": "So, so we can bring down the number of workers that we need to be significantly low.",
                    "label": 0
                },
                {
                    "sent": "So under these two assumptions, we can guarantee that each of the probability vector is estimated up to epsilon.",
                    "label": 0
                },
                {
                    "sent": "So here note we have this dependence of the condition number of the distribution matrix.",
                    "label": 1
                },
                {
                    "sent": "And at least for crowdsourcing application, this seems to be So what it sort of says is that jointly all the workers combine to ensure that each of the options is can be designated properly.",
                    "label": 0
                },
                {
                    "sent": "OK, that is.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have three workers and three type of options.",
                    "label": 0
                },
                {
                    "sent": "So let's say worker one can differentiate between option A&B but not BNC.",
                    "label": 0
                },
                {
                    "sent": "And worker two kind of differentiate between option B&C but not written envy and then work on three can differentiate between AC but not busy.",
                    "label": 0
                },
                {
                    "sent": "OK, so each of the individual worker is not good enough and condition number of each of the individual matrix won't be good enough.",
                    "label": 0
                },
                {
                    "sent": "But what we can show is that by combining or by concatenating these three probability matrices, the overall condition number becomes reasonable.",
                    "label": 0
                },
                {
                    "sent": "That is jointly all these workers will be able to differentiate between the different options and give us the correct answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's sort of the interpretation in crowdsourcing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me summarize briefly, so we consider this problem of learning mixture of discrete distribution, where each component is a product distribution and for this task we provide a moment matching this method and we provide an algorithm that has sample as well as time complexity is polynomial in all the key parameters.",
                    "label": 1
                },
                {
                    "sent": "One the Hue or one drawback of a method is that we have a dependence on condition number of the matrix which does not allow us to apply this result to several important learning problems like K binary decision tree problem.",
                    "label": 0
                },
                {
                    "sent": "So we realized that it might not be possible to do it in exact recovery mode.",
                    "label": 0
                },
                {
                    "sent": "So we want to study our methods, impact learning model and see if we can do better and also for these.",
                    "label": 0
                },
                {
                    "sent": "Problems, especially in the crowd sourcing domain E methods have been known to be very successful, but so far we do not have any rigorous analysis of these matters.",
                    "label": 0
                },
                {
                    "sent": "So one question is that can be analyzed.",
                    "label": 0
                },
                {
                    "sent": "Easier methods, underseen protocol, Sumption and show that they have to do something.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}