{
    "id": "c35aqz4hya32yci52rwqxrjwo3y4uydl",
    "title": "Grammatical Inference: news from the Machine Translation front",
    "info": {
        "author": [
            "Fran\u00e7ois Yvon, Computer Sciences Laboratory for Mechanics and Engineering Sciences (LIMSI), National Center for Scientific Research (CNRS)"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/icgi08_yvon_gi/",
    "segmentation": [
        [
            "Pleasure to introduce.",
            "Horse Horse World is a professor at the University and he has a long history with a a CGI conference because it was an author in 86.",
            "I wasn't born in anything.",
            "Yes.",
            "Attended the another conference.",
            "So is the interested basically in language processing that for language processing and now he's also interested in the machine learning course.",
            "I know he's the head of a team in the machine translation, so thank you for sweat for accepting.",
            "First of all, I would like to thank the organizer for inviting me to this conference.",
            "This is really an honor to be here today and I must say it is an unexpected one because even though I've.",
            "I'm doing work that is not so far away from what you do in this community.",
            "I consider myself more as a natural language processing person with very little knowledge of the theoretical aspects of grammatical inference, and in fact, reading the proceeding of ICJ many times has provided me nice insights about the objects that I was manipulating.",
            "So it's really a great honor to be with you today.",
            "Anne.",
            "My talk is entitled grammatical inference news from the machine translation, and, as I have told you, I don't know much about grammatical inference, and in fact I don't know much about machine translation either.",
            "So I'm almost embarrassed to have chosen this title for my talk.",
            "In fact, I'm a newcomer to the field of machine translation, and it's only since one year that I've started really to work on machine translation systems.",
            "But since then I've been reading quite a lot and I have been discussing with more.",
            "Knowledgeable people than I was.",
            "And here are the name of a few people at Lindsey who are doing a machine translation with me.",
            "And I have even tried a few things here and there, but nothing substantial enough to make the flash of the full talk.",
            "So what I'm going to do today is.",
            "Present my vision of machine translation today, mostly insisting on what is going on machine to non statistical machine translation.",
            "And I hope I will spice this talk with a few comments.",
            "My own comments on what I have, what I consider important or working well or not working well in the film, and I'm very free to do so because none of the work I'm going to present is mine, so."
        ],
        [
            "So this is the outline of my talk machine translation.",
            "Statistical machine translation is really about taking large corpora and using machine learning algorithm to train statistical machine statistical machine translation system an.",
            "I will first go on detailing the anatomy of state of the art systems an along the way.",
            "I will try to convince you that paradoxically, and some even say, depressingly, the recent progress."
        ],
        [
            "In the field are mainly due to the ability of systems to use very large corpora, and in fact quite simple machine learning techniques.",
            "And of course, this is not completely right, and even though great successes have been accomplished using this kind of techniques, I will try to convince you that there are some open issues, difficult open issues, and that we can't go really much further if we don't change the way this systems are."
        ],
        [
            "Working because nowadays I will.",
            "I will show you that statistical machine translation works well for restricted domains and for local like Langg."
        ],
        [
            "Edges and what we'd like really to have is General Assembly systems that would need, in my opinion, linguistic, linguistically analyzed corpora and structure where machine learning."
        ],
        [
            "Before starting as the audience may not know the problems of machine translation, even though it's a fairly easy problem to understand, at least I will just.",
            "Outline some of the problems with machine translation, so machine translation is really making machine able to go from there.",
            "A German sentence to, for instance, there the English translation and in some cases this is quite easy and we will see some easy case, but this is an example of what I consider difficult language pairs.",
            "Just because these two languages they don't use the same linguistic process to convey.",
            "The meaning of the sentence through the way the phrases inside the sentences are organized.",
            "And here we have an example of what is called a free order language where basically case markers like this is dative.",
            "This is accusative.",
            "Will help the speaker understand the relative role of the reference of the various groups that are involved in this sentence.",
            "These groups can move around quite freely.",
            "I mean within certain constraints, whereas here you don't have such a rich morphology, but you have a strong word order that will help the speaker understand again the role of the different constituents of the sentence and really what is difficult today is to model the way in three other languages constituents are allowed to move.",
            "Within the sentence where you don't have the same order of the words from both in both languages."
        ],
        [
            "OK, So what is mainstream statistical machine translate?"
        ],
        [
            "Chen basically to build a machine translation system.",
            "The first thing you have to do is to collect data and data.",
            "In this case takes the form of parallel sentences where is called in the community be text.",
            "And not not only do we need parallel sentences, but we need to align them word for word for word.",
            "So this is the first."
        ],
        [
            "Work we have to do when this is done, we can start training what is called the.",
            "Translation model and I will refer to the translation model as the phrase table.",
            "Because the translation model in such systems is based on variable length units that are to my way, to my view, a bit inappropriately called phrases, even though they don't have a linguistic content.",
            "Or linguistic in."
        ],
        [
            "Rotation.",
            "The second step of training is to build a target statistical language."
        ],
        [
            "All using monolingual texts, and this is fairly common and I will go briefly."
        ],
        [
            "On that topic.",
            "The next step is to make sure that your system."
        ],
        [
            "Is tuned.",
            "Because translating then will take the form of solving a complex maximization problem involving here a combination of various scores.",
            "So three tuning is making sure you give the right weights to the various components of the models."
        ],
        [
            "Once you've done that, you can get some numbers."
        ],
        [
            "And if you're not happy with them, you start again.",
            "So I will now detail each of these steps in more depth."
        ],
        [
            "Starting with the corporate acquisition acquisition step.",
            "So what we need here is a billing world corpus align on the person sentence per sentence.",
            "There are ways to do it.",
            "I won't talk about them in this in this talk, but basically we know how to take parallel corpora and to get down to such a pair of sentences."
        ],
        [
            "Where do we find the data?"
        ],
        [
            "Well, the main sources have been historically documents from the Institute for Multilingual institutions, such as the Canadian Parliament, the European Parliament, the United Nations, and this institutions have helped build corpora that contains several millions of sentence pairs and several hundreds of millions of words.",
            "So these are pretty large corpora.",
            "People people have also tried to use literature, touristic guides, technical documentations.",
            "There is every kind of sources that was at the time."
        ],
        [
            "I'll with the.",
            "With the.",
            "Widening of the potential applications of machine translation, people have started consider started to consider different jobs of text such as news websites, blogs and speech speech transcripts, also featuring scripts.",
            "Some again are fairly easy to collect.",
            "For instance, you can find websites in, you can find multilingual websites, so you will get a parallel website, but it's important to realize that for some applications people are really starting to develop.",
            "Opera four training statistical machine translation system and this is.",
            "This makes a big difference because not every text or not every translation of every text is suitable to train a statistical machine translation system.",
            "And it is clear that it is easier to work with translation that are very literal than to work with more literary.",
            "I would say translations.",
            "So I mean.",
            "The fact that we are building corpora for especially well, sorry that we're designing corpora for the pure.",
            "The simple aim of building statistical machine translation is a novelty, and it provides us with a new kind of."
        ],
        [
            "Of data.",
            "Of course, this is never enough so."
        ],
        [
            "People have trained to investigate ways to mine not only parallel corpora but also comparable corpora."
        ],
        [
            "I won't detail that today.",
            "So basically regarding corpora, my opinion is that we have large corpora.",
            "And they pose specific problem because we have to be able to fare with millions of parallel sentences and very large models etc.",
            "But.",
            "At the same time this this corpora are quite restricted restricted in scope, especially the European Parliament.",
            "Debates, for instance, is really boring in some ways.",
            "And so data scarcity is still an issue and still hampers the development of machine translation system for many domains.",
            "For many languages and still impairs the development of general domain machine statistical machine translation systems."
        ],
        [
            "So once the data data has been collected, the first step as I said is to build a word alignment.",
            "So here you got an example of what we'd like to have an alignment between an English sentence here on the French sentence here.",
            "This is an isometric alignment.",
            "This is not obvious in this case, but the alignments were aiming at in this step or asymmetric.",
            "Each French word is linked with exactly 1 English word, but the reverse is not true here you have an English word that is linked with several French word, so the alignment realignments we're building here asymmetric during this first step.",
            "So this is a nice alignment.",
            "I mean, the sentences are almost parallel."
        ],
        [
            "Except for a few cases where the we have local reorderings, because French and English don't agree in the way, they, the adjective, is located with respect to its sorry head noun."
        ],
        [
            "We have one example here, one exam."
        ],
        [
            "Here on the last example here were English uses a noun phrase were French uses a prepositional phrase.",
            "So this makes another slight change in the words or order."
        ],
        [
            "So this is a nice case.",
            "Of course we have to we have to deal with messier cases like this one."
        ],
        [
            "Where we have a long distance rearrangement here.",
            "Well, the English group, it seems absolutely disgraceful, is moved to the end of the sentence of the French sentence.",
            "And we even have."
        ],
        [
            "Errors in this alignment, like the French porno.",
            "Pronoun E is aligned with the verb, whereas the English pronoun it is aligned with an adverb."
        ],
        [
            "So I have a case where the two French pronouns new our line with the same reflexive pronouns in English ourselves.",
            "And that's, again is wrong."
        ],
        [
            "So the way to do it is to train a symmetric alignment model."
        ],
        [
            "And these models are basically generative joint model of an alignment.",
            "Under French sentence, given the English sentence and as the alignment is an hidden variable, you have to do something like EM training or something of the like."
        ],
        [
            "So once you've trained your models.",
            "You can align the words.",
            "You can build your sorry your word alignment by solving this this this decoding problem."
        ],
        [
            "And you can even translate.",
            "And this this in fact was the reason why this models were first proposed in the litter in the in the 90s by the people at IBM who proposed a series of models from IBM, one to IBM Five which were aimed at.",
            "Providing statistical, the first statistical word based translation models."
        ],
        [
            "So we have public domains implementation for this models.",
            "They are fairly efficient there are.",
            "Even we have even parallel implementations of this model, so I guess the general point of view.",
            "But word word alignment is that people are happy with these tools and they don't on they stop here basically."
        ],
        [
            "However, recent recent research has shown that you can do a bit better than that using discriminative training techniques.",
            "And if you do discriminative training and if you add many more features in your model, such as you can in a discriminative model, you can enforce the fact the constraint that cognates should align or words with the same frequency should align or words having the same length should align.",
            "At least preferably so you can.",
            "You can have.",
            "You can add sorry, many more features in your model and that will help.",
            "That will help you.",
            "At least that will provide you better."
        ],
        [
            "Alignment.",
            "To be able to do that, what you need is training data and training data in that task is manually aligned word.",
            "Sorry sentences, manually aligned at the world level and this is a resource that is that it's really difficult to build.",
            "I mean, as you've seen, it's not easy to decide how the words should be aligned together, so.",
            "Basic."
        ],
        [
            "Before."
        ],
        [
            "At least for a symmetric model, this is more or less considered as a solved issue in the community, and people simply take the existing public domain implementation and stay."
        ],
        [
            "From there.",
            "The next step in training the translation model consists in extracting what I called phrase, even though there are more correctly designed as clumps or chunks of words.",
            "So what you do here is you take you run the alignment model in both directions so you have one French to English alignment and you have one English to French alignment, and somehow you symmetrize.",
            "The alignment, for instance, taking the intersection of all the links that you have and maybe doing something more until you get something like that.",
            "And this is a symmetric alignment between two sentences.",
            "Again, French and English.",
            "This one rates from bottom to top.",
            "On this one, sorry it's not French.",
            "Is German and English.",
            "This one reads from bottom to top on this run with this way.",
            "So how do we extract this multi word alignments?",
            "It's fairly simple.",
            "What we do is we go from left."
        ],
        [
            "Writer Anne we will build.",
            "Blocks rectangular blocks, with the constraints that whenever a row word in the row appears within a block, then it's it's made in.",
            "The column should be within the block as well.",
            "OK, so if I do that I can have one one.",
            "Best like Michael Michael.",
            "OK, this is a small rectum."
        ],
        [
            "But this rectangle can be extended again this respect the constraints, the constraints I have stated.",
            "Michael assumes with Michael Mikkel get the phone house."
        ],
        [
            "I can again extend this rectangle and the point here.",
            "The important point here is that I'm including some word that is not aligned.",
            "That is, a common German has no English counterpart, but still it's included within the phrases within the phrase and it's important in that German construct that we have the coma here."
        ],
        [
            "This is an example of pair that you can't extract because in that case, for instance, Houser in German is aligned with House, so if you take that rectangle it should contain Hauser, but it's made his house is outside the rectangle, so you don't extract this one.",
            "Ugh."
        ],
        [
            "Going along on the on the sentence you get also this one."
        ],
        [
            "Many others.",
            "So once you've done that for every sentence in your corpus, you have your translation model almost ready.",
            "What we have collected here is pairs.",
            "Of English French chunks, together with some statistics, for instance, the count of how many times we see this pair in the in the corpus."
        ],
        [
            "So this is a very heuristic and very noisy process.",
            "I just want to show you.",
            "Real war."
        ],
        [
            "Old free stable.",
            "That they have extracted from the European data.",
            "So in this very stable we have about 500 different translations for European Community 500.",
            "And this is because I mean we can see European Commission with the correct translation.",
            "But also here because of a misalignment we have the article and here we have not only the article but also a preposition etc.",
            "And here again, because of misalignment we have the comma within the group.",
            "So we have many many translation like that that is our model, our system when you will have to translate European Commission into French, we have to choose among.",
            "That many numbers of alternatives."
        ],
        [
            "After you have chosen some more examples for culture which is polysemic both in French an English, it can mean on both.",
            "You can have both the agricultural meaning at all, so the artistic meaning, for instance culture has about 100 translations."
        ],
        [
            "And worse, the exclamation mark is about 700 different translations just because of alignment problems.",
            "So this is really a very noisy process."
        ],
        [
            "Of course, not all these phrases are equally reliable, so we have to find ways to assess the reliability.",
            "The reliability of 1 French translation for each.",
            "Of each possible translation of an English sentence.",
            "And if you just take the usual maximum likelihood estimate of the conditional probability you run into problems because of the city.",
            "So you have to do something about data paucity.",
            "For instance, taking several estimates of this quantity.",
            "And this really."
        ],
        [
            "Mount your system.",
            "Surprisingly, trying to extract linguistic phrases doesn't help, and it seems.",
            "Even to hurt the performance of the system."
        ],
        [
            "If size is an issue, I mean if you if you don't want to work with that with a very big with a very large phrase table, what you can do is prune the phrase table based on statistical grounds.",
            "That is, get away with phrases that have.",
            "Conditional probability that is below the chance."
        ],
        [
            "For instance, to appear.",
            "If size is not an issue, then there are ways to have even larger, larger tables.",
            "Larger phrase tables using gapi phrases, and I may."
        ],
        [
            "Come back."
        ],
        [
            "This issue a bit later.",
            "But basically everyone seems to agree on the fact that the largest your phrase table, the better performance.",
            "So people are trying all possible tricks to get more and more phrases, even if the resulting translation model is very noisy, like for instance combining phrase tables from different corpora.",
            "So you learn the phrase table with the European data, you learn a different phrase table with the national.",
            "With the nation with the United Nation data and you combine them or even you take translation dictionaries and you see them as a phrase table and you put them into your system.",
            "So basically the goal is to have the biggest possible phrase table."
        ],
        [
            "The next step in training is fairly easy.",
            "It's just a matter of training."
        ],
        [
            "Target language."
        ],
        [
            "Also I will."
        ],
        [
            "I'll go briefly on this one because I mean, I guess you're all familiar with the training statistical language model.",
            "Just know that in empty we're using the same kinds of model that we use for speech recognition.",
            "With some small you know we have small differences.",
            "For instance, it has been shown that using very large funds model was helping for translation, so people again are trained to use all possible kinds of data to train their model.",
            "And it's it's common to have systems where the language model is trained on more than a billion tokens.",
            "So you have again.",
            "You have to be capable of accommodating, accommodating very large textual corpora to train."
        ],
        [
            "This model.",
            "I won't detail them here, although I mean if you look at the slides afterwards, you have the exact numbers, but in a recent experiment Google the Teal.",
            "Sorry, Google's team has tried to use all the data they were capable of processing and of course.",
            "That wasn't doable with current study with current estimation techniques for language models, because when you train language models, you have to do some kind of smoothing and smoothing was no longer possible given the size of data they were using, so they decided not to smooth sorta smooth in a very simple way, and in fact it proved that it was more helpful to put as many data they could with the wrong model, then less data.",
            "With the correct model.",
            "So basically the."
        ],
        [
            "Focus in the area of language modeling is more on scalability than modeling, and I mean I'm citing her recent work on you know people explaining how to scale up the language model allowed to train with gigantic opera or two feet gigantic language models within the memory."
        ],
        [
            "Next you have to tune your score function.",
            "So basically what we're going to do is decode by finding the sentence here East that maximizes some score where the score is defined as a combination of many models.",
            "OK, your best translation as to be a good translation of all the phrases, so it has to have a high score for the translation model.",
            "But also it has to have a high score for the language model and he has to have a high score for many other models that you can put into your system like distortion, model links, model segmentation models etc.",
            "So you have to.",
            "You have to go to put the right weights for each model here Lambda.",
            "So basically this step amounts to take some additional set of text, although data and try to optimize the weights in this in this combination, sorry.",
            "So this amounts to solving some kind of optimization."
        ],
        [
            "Problem here were you try to find the lambdas that will minimize your loss over the L dot set.",
            "This is not as easy as it may seem."
        ],
        [
            "Because the loss is not differentiable in Lambda because the losses related to the lambdas in in a very indirect manners, because the lambdas will influence your translation, but your translations are discrete by nature.",
            "Well sets of words.",
            "So basically you have to do something to work with optimization with non differential with non differentiable functions.",
            "And I mean this can be done by using conventional optimization techniques like line.",
            "Linesearch or the simplex algorithm or many other things."
        ],
        [
            "The main point here is that you do you have to do it right.",
            "Because it makes a big difference in your final results.",
            "So no matter what you do, try to do it right."
        ],
        [
            "Once the system is trained, decoding is really conceptually simple.",
            "We just."
        ],
        [
            "Need to solve, sorry this problem here.",
            "So we find the sentence that maximizes this score.",
            "And of course, you can't find the exact solution.",
            "I mean this problem cannot be solved because the large base, the search base salary is too big to be search exhaustively.",
            "You have to consider all the segmentations of the source phrase of the source sentence an all and for also on for every segment.",
            "You have to."
        ],
        [
            "Consider all possible translations, and as you've seen, you can.",
            "You can have several hundreds of possible translation for each phrase in the source."
        ],
        [
            "Bridge and then worst, you have to consider every permutation because of word order difference.",
            "You have to consider every possible permutation of the source phrases and this makes the the problem really."
        ],
        [
            "Intractable.",
            "So what people do usually there is sort of some kind of heuristic search.",
            "And the results you have are really a function of the cleverness of the pruning.",
            "You implement the pruning strategy you use within the."
        ],
        [
            "During the search.",
            "But nevertheless people state of the art systems are achieving high performances at a reasonable decoding rate.",
            "To give you an order of magnitude.",
            "The.",
            "It's going to have systems that can translate more than 100 words per second, so this is much more than we need for laboratory systems.",
            "I mean, I'm not talking here about prediction system, but I mean in the labs.",
            "This is more than this is faster enough for us."
        ],
        [
            "Again, I can in at the end of the talk I can come back if you'd like to see me because I have a slide that describes exactly how decoding works for this kind of."
        ],
        [
            "A problem, but I I'd better."
        ],
        [
            "Move on.",
            "Machine to machine translation evaluation is a difficult problem, even even the evaluation of the translation by humans is difficult.",
            "But for machine it's much worse, and it's even in fact it's almost as difficult to have good objective.",
            "Good.",
            "Machine evaluation as to have good machine translation, I mean both problems are really as difficult.",
            "Nevertheless it was it was felt."
        ],
        [
            "In the in the community that it was necessary to have.",
            "Metric to evaluate automatically translation because in the neighborhood, neighboring field of speech recognition having metrics really pushed the progress."
        ],
        [
            "In the field so people have came up with what I call a fragile consensus, which is a metric that has been proposed by the people at IBM in 2001.",
            "It's a very crude metric.",
            "It's a very simple metric, but nonetheless it constitutes nowadays and exited ref."
        ],
        [
            "Once.",
            "And what it does it just measure the surface similarity between your hypothesis an the reference translation."
        ],
        [
            "On the surface, similarity is a simple function of the N gram precision, so it's a very simple score."
        ],
        [
            "So I have an example here.",
            "Assume that you want to assess the quality of this.",
            "I put this with respect to these two translations.",
            "Ewww"
        ],
        [
            "Just compute the unigram match so."
        ],
        [
            "All the unigram.",
            "If you in your hypothesis are OK, so your unigram matches one, because I is here is involves filling is here and good is here.",
            "So all the unigrams now OK. 2/3 of the bigrams are OK. OK."
        ],
        [
            "And when you have these four numbers, then you take the geometric mean and basically this is a blue score.",
            "In practice, you don't do exactly that, but you average.",
            "This calls over the whole test corpus because you know in some cases you have zeros, and I mean taking the geometric mean will give you very bad numbers if you don't do anything."
        ],
        [
            "Evaluation is still an active research topic.",
            "I mean, people are.",
            "Not so happy with Blur because I mean blue eyes as allowed to measure progress in the five last years, but it's felt that blue is not fair, is not entirely fair, especially to systems that are trained to do more complicated things than what I'm presenting now.",
            "So there is still active debates within the community and many proposals of new metrics are on."
        ],
        [
            "Discussions.",
            "So this is it.",
            "At this stage, I mean it must be pretty obvious, at least to anybody in this room that what I've presented is entirely finite state, because the phrase table look up is a finite state."
        ],
        [
            "Assess and because ngram language models are also finite state."
        ],
        [
            "And so if you want to implement a system, it's simply a matter of simple combination of finite state operations where you take your input.",
            "Sorry, you take your input F. You look through composition in the phrase table to get a lattice containing all the possible translations of your input sentence.",
            "You project it onto the target side, uncombined with the language model and this will score.",
            "This will re score the passing this lattice with the language model scores and finding the best path is exactly what you do when you do monotonic decoding.",
            "So basically, I mean this has been.",
            "Acknowledge long ago, all these techniques are finite state.",
            "As I have tried to.",
            "Show in the beginning of my talk translation is not a matter of monotonic decoding because you have word order differences between language."
        ],
        [
            "So you need to do something about reordering right, and it's fairly easy to do.",
            "Just changing this from this to this where I've included some kind of function that will compute for me permutations.",
            "Of the input sentence, I mean I know there are a finite number of permutations, so I know that them of F is itself a finite state machine or can be represented as a finite state machine, albeit a very large one.",
            "So decoding reordering is again a simple matter of solving this finite state.",
            "Affirming implementing these finite state operations."
        ],
        [
            "And this point of view is great because we have efficient implementations of finite state tools.",
            "We know that these tools can scale up, at least if you use lazy on the fly implementations of composition.",
            "We have training procedures.",
            "We have ways we know how to integrate more knowledge sources within the system.",
            "For instance, if we if we want to add some morphological analysis, we know we know we could do that.",
            "By changing, I think some kind of morphological.",
            "Transducer here etc etc.",
            "So when you adopt that view.",
            "Basically, you have only one problem.",
            "And the problem?"
        ],
        [
            "Is.",
            "How to implement perm?"
        ],
        [
            "I mean, a lot of research has been put.",
            "A lot of effort has been put into the implementation of the reordering, and there are many proposals again that are on the table."
        ],
        [
            "The most I mean the most popular system basically tries a brute force approach, so basically considers all possible permutations.",
            "But as this is intractable, it will highly penalize permutations that are far from the monotonic alignment.",
            "OK, so in principle every permutation is possible is explored during the search except that.",
            "The closer you stay to the monotonic alignment, the higher the score basically."
        ],
        [
            "The alternative is to restrict a priori the set of possible."
        ],
        [
            "Stations.",
            "Anne again."
        ],
        [
            "I mean, there are various ways to do it.",
            "You can do it with finite state techniques, for instance, trying to define the transducer T which will allow you to compute a subset of this of the set of all permutation through a rational transaction, like here.",
            "Or you can use context free models.",
            "Where you will try to find some grammar.",
            "Some synchronous grammar here that will help you define the set of the set of permutations you will consider as a set of string F prime that are jointly derived from South together with F. OK.",
            "Instead of using a priori models, what you can try to."
        ],
        [
            "Who is to look at the data and learn such?"
        ],
        [
            "User."
        ],
        [
            "Or such grammar.",
            "And again, this has been done.",
            "In the past, so both approaches have example and you can find example of both approaches in the literature."
        ],
        [
            "Or you can try to use your linguistic knowledge.",
            "By defining reordering rules that will carefully restrict the set of possible permutations of F that you want to consider."
        ],
        [
            "In the search on, in fact, any combination thereof is possible.",
            "That is, you could define a priori."
        ],
        [
            "Gramma sorry."
        ],
        [
            "You could define a priori transducer here and try to learn the weights on this one user based on what you see in your data.",
            "Or you can try to combine.",
            "I don't know linguistic information with a priori restrictions etc and this has been tried also."
        ],
        [
            "So in the literature.",
            "And I mean I can.",
            "Illustrate any of this with.",
            "So if for inst."
        ],
        [
            "Since.",
            "IBM style constraint that you will that you often find in the literature are typically within the in that Category 4 full typically in that categories the IBM style constraints are constraints or other possible permutation that you can consider and the rule is very simple.",
            "You try to produce permutations here permutations on a set of numbers OK from left to right and at each step you have the choice between.",
            "The K remaining items, that is the K remaining numbers that you haven't already chosen, right?",
            "So assuming that after four throw, the situation is like that.",
            "So zero is taken to his taken 3 steak and five is taken, but you have.",
            "Forgotten somehow one and four so they are free.",
            "Assuming that you have the choice between 4 numbers.",
            "At the step at the next step, what you can do is either choose one OK. And then you next choice will be between 3:00.",
            "5 six and seven.",
            "Or you can choose four and your next choice will be between 1656 and seven.",
            "Or you can choose six on your next choice will be between.",
            "146 and seven OK.",
            "So."
        ],
        [
            "This is very, very simple.",
            "You can add additional constraint like restricted number of moves to take place between within.",
            "Sorry fixed size window for instance impose the constraint at zero.",
            "For instance shall not be selected after the five draw.",
            "The 5th row for instance.",
            "OK, so you limit your somehow limitating the movement the possible movement of zero OK?",
            "Or you can restrict the number of simultaneous gaps.",
            "So you've seen gaps in my previous representation.",
            "You can limit the number of gaps to one or two.",
            "2 for instance, and that will yield more restrictions of the other possible permutation that you will consider during searching."
        ],
        [
            "And of course, this is what you get.",
            "But the point here I'm not even to define the transducer, but you can compute this this with a finite state transducer."
        ],
        [
            "Shackleton, OK, so an example of what context?",
            "Free models?",
            "How?",
            "Context free model sorry are used to define the set of possible alignments is given."
        ],
        [
            "And by the model of invention construction grammars ITG's that were introduced in the field of machine translation by dekiru, so these are synchronous grammars.",
            "So it just like context free grammars, except that you're producing two derivations in parallel.",
            "OK, so we're working with two alphabets, Sigma and gamma OK on each.",
            "Each rewrite rules as two left hands as two right inside.",
            "OK, the specific constraint within inversion transaction grammars is that the right hand side.",
            "Are limited to be either equal the two right inside of a rule are either equal or a mirror of one another.",
            "OK, so the only thing you can find is either twice the same right hand side or one is a mirror of the other."
        ],
        [
            "So how can you use such devices to generate permutations?",
            "Just consider this very simple it G. You have only two productions here.",
            "Plus one terminating production for each vocabulary word.",
            "OK, then you can define the set of permutations of W1W N to be all the words.",
            "All the sequences of words that can be jointly derived."
        ],
        [
            "From X, so if this is a pass for W1W four."
        ],
        [
            "You will have this permutation OK, and flipping this subtree."
        ],
        [
            "This one."
        ],
        [
            "This one.",
            "And many others.",
            "OK."
        ],
        [
            "So more about the these kinds of constraints that define the strict strict subset of all permutations.",
            "It is proven that you cannot get all the possible permutations with such devices.",
            "Still, the number of permutation remains.",
            "Caminha tauriel.",
            "It's a.",
            "In the order of K to the power N, word K is around six, so it's still growing very fast, but still it can be searched in polynomial time using parsing techniques basically."
        ],
        [
            "I will give you last."
        ],
        [
            "Example, which is an instance.",
            "I mean this is an example of of.",
            "Trying to infer in user as a finite state transducer from the data and this has been promoted by Hughes at Craig, who is now with Lindsey and this is something we are working on actively at the Lindsay.",
            "So what?"
        ],
        [
            "Do here is during the training procedure."
        ],
        [
            "Sorry.",
            "You do the training procedure as usual, except that.",
            "You will extract within phrase reordering rule that are based on part of speech tags.",
            "So you have to 1st tag your corpus with morphological syntactical information and learn local reordering rules.",
            "Each rule is.",
            "It is right.",
            "Role is compiled as a finite state transducer on your reordering transducer is simply the composition of all this individual transducer inducer.",
            "Sorry."
        ],
        [
            "So friends for example.",
            "If this is your alignment in the data, so you hear you have Spanish and here you have English and again you have an instance of movement.",
            "Between court movement of the noun at the end of the non phrase in English, whereas it is at the beginning of the non fray.",
            "In Spanish.",
            "So."
        ],
        [
            "The rule you will extract is this one.",
            "You would expect the rule that in Spanish, when you have a sequence noun, adjective coordination, now you have to rewrite it as adjective coordination, adjective, noun OK.",
            "So you take."
        ],
        [
            "Other possible rules you can find in your corpus and build this machine."
        ],
        [
            "Then, during decoding, the set of permutation is simply computed by composing the tag.",
            "The input with this transducer and projecting it on the input side to get rid of the attacks.",
            "And this is one of the way you can do it.",
            "I'm sure that you have many other ideas of how you could learn such that machine, but this is just an illustration of what people have have come up with.",
            "Trying to solve this problem."
        ],
        [
            "My impression in that I mean no matter what ingenuity people have tried to put into this machinery's.",
            "The gains you get are quite small, are quite small compared with monotonic translation, and in fact for languages such as French and English, for instance, it's almost better to do decoding without reordering.",
            "And at the same time, so it doesn't help you when the language that you have to translate are quite similar.",
            "It doesn't help you much and still you have a huge gap between performances you get when you translate easy language pairs and difficult language pairs.",
            "To give you just the order of magnitude the scores we have for say French to English is in the area of 30 or 40.",
            "What is the score we get for French?",
            "German is more like 10 or 15.",
            "OK, so there is a huge gap between easy pairs and difficult pairs, and this is for me the sign that this models are not good enough when we.",
            "When we have to translate such different language."
        ],
        [
            "OK to summary, I will try to give some reasons why machine learning statistical models for machine translation work.",
            "At least they work better."
        ],
        [
            "Then word based model and they work better than than work based model because they provide an easy and cheap way to model non competition non compositional translations like videos or terms or multi word units in general.",
            "So I mean this is really easy with phrase base."
        ],
        [
            "Machine translation they also give you almost for free local reordering."
        ],
        [
            "I mean, I've discussed already the case of objective non in.",
            "In in English that have to be turned into non adjective in French and if you learn the phrase directly, you don't have to worry about reordering because reordering is given you for free."
        ],
        [
            "Something else that comes for free is what I call local context decisiones, local context."
        ],
        [
            "An agreement.",
            "I mean, if you have to translate the International Conference into French word word, you would have to have some mechanism to ensure that the gender and number agreement in French is respected right?",
            "Because the data miner than an adjective to be feminine on singular, all of them.",
            "But if you translate the phrase you have to worry about agreement, I mean, agreement is free.",
            "It's within the phrase.",
            "So you have anything to do."
        ],
        [
            "So it relies simplicity, speed and raw."
        ],
        [
            "Business.",
            "And on top of that, there are two 2 reasons that explain the success of these techniques.",
            "One is that basically what you're doing is you're matching large phrases in your in your in your training data, and it's exactly what the score the metrics have talked about his rewarding.",
            "So I mean, there's no wonder that these systems are highly ranked that are really difficult to outperform.",
            "They are, they do exactly what is needed to please the metrics.",
            "OK, and there is yet another reason, but I haven't dared written it on my slide.",
            "Is that the vast majority of the work that has been going on in machine translation has been to translate.",
            "Foreigner.",
            "Into English for ending anything, and for many reasons, I mean, English lends itself well to the kind of finite state based techniques that we have.",
            "Very large corpora for building the target language model, for instance, and we.",
            "We always go from languages with rich morphology to the language with the poor morphology, and this is much easier than doing doing it the other way around.",
            "For instance, going to English from English to German is more difficult, much more difficult because you have to somehow predict the right case marker and plural an etc that you don't have in English.",
            "So you really have to do something to predict them correctly, right?"
        ],
        [
            "Some reasons why it's it still fails.",
            "I mean first, I mean, for me it was even the wonder that this kind of, you know techniques could work at all.",
            "I mean, it's purely so fast it has no morphology at all, right?",
            "The only thing it considers our strings of characters or even characters so it's it has no morphology at all.",
            "And I mean, it's it's clear that adding some morphology could help if we could find a way to."
        ],
        [
            "Do it efficiently.",
            "In these models we only consider contiguous phrases OK and contiguous phrases.",
            "They miss important generalization, like for instance just in French you have the the way you built negation in French is you put an ad Werner the verb and then power.",
            "And if you want to and then the adverb.",
            "So you have you have to embed a verb between none power right?",
            "So and if you only consider contiguous fragments, while you will have to learn.",
            "All the possible verbs that have to appear within their own path to get the right generalization to English.",
            "OK.",
            "Whereas the generalization is fairly simple, I mean you know, basically you you could extract know something, but then you would have, you know almost everything that is needed."
        ],
        [
            "These models only use local syntax on the target side, and I mean this is this is known to be.",
            "Not good enough to predict the long distance agreements or such phenomena that you can find."
        ],
        [
            "In many languages.",
            "An important drawback of this model is as is that phrase waiting and phrase selection is context free.",
            "I mean you in the architecture I've I've detailed, the assessment of the quality of the translation of for a given phrase is done once and for all.",
            "It's always the same score you will use to translate European Community no matter what.",
            "OK, you don't look at the context of European Community to decide how reliable this translation is.",
            "So I mean again, this is not a big issue, but I mean improving on all these dimensions should buy us some millage in improving statistical translation systems."
        ],
        [
            "The main weakness again is this.",
            "This models lack a global reordering model, and this is this is a shame for many reasons.",
            "This is a shame because it costs you at decoding because we are considering many reorderings many permutations that are very, very unlikely.",
            "That are very bad.",
            "It's also a shame because I mean.",
            "The due to the.",
            "Due to the fact that we are learning very imperfect model, we can't easily transfer what we've learned from one domain to another domain.",
            "Because I mean what we learn is really two attached to the kind of corpus we're processing.",
            "So I mean using this poor this simple models is hampering the possibility to domain adaptation.",
            "And there are many other reasons why it's it's a shame."
        ],
        [
            "We don't have a."
        ],
        [
            "The models."
        ],
        [
            "So these are my temporary conclusions.",
            "I guess the most of the recent progress of SMT of SMT can be attributed to the use of simpler models and logic opera.",
            "Plus many small things, small details, paying attention to details, doing the optimization right, etc.",
            "It produces acceptable translations for many pairs.",
            "I mean, you can train trait on the web.",
            "I mean Google system on the web is using statistical machine translation.",
            "I guess Microsoft System on the web is now using statistical machine translation etc.",
            "So you can see it's acceptable for many pairs.",
            "There is 1 issue.",
            "One important issue which is modeling world order and for that we need more linguistically informed systems that.",
            "And that's a big restriction that nonetheless are able to provide us an acceptable level of robustness and speed.",
            "Because as I said, you have to be able to process very large databases to build good systems."
        ],
        [
            "I stop and I can take some questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pleasure to introduce.",
                    "label": 0
                },
                {
                    "sent": "Horse Horse World is a professor at the University and he has a long history with a a CGI conference because it was an author in 86.",
                    "label": 0
                },
                {
                    "sent": "I wasn't born in anything.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Attended the another conference.",
                    "label": 0
                },
                {
                    "sent": "So is the interested basically in language processing that for language processing and now he's also interested in the machine learning course.",
                    "label": 0
                },
                {
                    "sent": "I know he's the head of a team in the machine translation, so thank you for sweat for accepting.",
                    "label": 0
                },
                {
                    "sent": "First of all, I would like to thank the organizer for inviting me to this conference.",
                    "label": 0
                },
                {
                    "sent": "This is really an honor to be here today and I must say it is an unexpected one because even though I've.",
                    "label": 0
                },
                {
                    "sent": "I'm doing work that is not so far away from what you do in this community.",
                    "label": 0
                },
                {
                    "sent": "I consider myself more as a natural language processing person with very little knowledge of the theoretical aspects of grammatical inference, and in fact, reading the proceeding of ICJ many times has provided me nice insights about the objects that I was manipulating.",
                    "label": 0
                },
                {
                    "sent": "So it's really a great honor to be with you today.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "My talk is entitled grammatical inference news from the machine translation, and, as I have told you, I don't know much about grammatical inference, and in fact I don't know much about machine translation either.",
                    "label": 1
                },
                {
                    "sent": "So I'm almost embarrassed to have chosen this title for my talk.",
                    "label": 0
                },
                {
                    "sent": "In fact, I'm a newcomer to the field of machine translation, and it's only since one year that I've started really to work on machine translation systems.",
                    "label": 0
                },
                {
                    "sent": "But since then I've been reading quite a lot and I have been discussing with more.",
                    "label": 0
                },
                {
                    "sent": "Knowledgeable people than I was.",
                    "label": 0
                },
                {
                    "sent": "And here are the name of a few people at Lindsey who are doing a machine translation with me.",
                    "label": 0
                },
                {
                    "sent": "And I have even tried a few things here and there, but nothing substantial enough to make the flash of the full talk.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do today is.",
                    "label": 0
                },
                {
                    "sent": "Present my vision of machine translation today, mostly insisting on what is going on machine to non statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "And I hope I will spice this talk with a few comments.",
                    "label": 0
                },
                {
                    "sent": "My own comments on what I have, what I consider important or working well or not working well in the film, and I'm very free to do so because none of the work I'm going to present is mine, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the outline of my talk machine translation.",
                    "label": 0
                },
                {
                    "sent": "Statistical machine translation is really about taking large corpora and using machine learning algorithm to train statistical machine statistical machine translation system an.",
                    "label": 1
                },
                {
                    "sent": "I will first go on detailing the anatomy of state of the art systems an along the way.",
                    "label": 0
                },
                {
                    "sent": "I will try to convince you that paradoxically, and some even say, depressingly, the recent progress.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the field are mainly due to the ability of systems to use very large corpora, and in fact quite simple machine learning techniques.",
                    "label": 0
                },
                {
                    "sent": "And of course, this is not completely right, and even though great successes have been accomplished using this kind of techniques, I will try to convince you that there are some open issues, difficult open issues, and that we can't go really much further if we don't change the way this systems are.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Working because nowadays I will.",
                    "label": 0
                },
                {
                    "sent": "I will show you that statistical machine translation works well for restricted domains and for local like Langg.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edges and what we'd like really to have is General Assembly systems that would need, in my opinion, linguistic, linguistically analyzed corpora and structure where machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before starting as the audience may not know the problems of machine translation, even though it's a fairly easy problem to understand, at least I will just.",
                    "label": 0
                },
                {
                    "sent": "Outline some of the problems with machine translation, so machine translation is really making machine able to go from there.",
                    "label": 1
                },
                {
                    "sent": "A German sentence to, for instance, there the English translation and in some cases this is quite easy and we will see some easy case, but this is an example of what I consider difficult language pairs.",
                    "label": 0
                },
                {
                    "sent": "Just because these two languages they don't use the same linguistic process to convey.",
                    "label": 0
                },
                {
                    "sent": "The meaning of the sentence through the way the phrases inside the sentences are organized.",
                    "label": 0
                },
                {
                    "sent": "And here we have an example of what is called a free order language where basically case markers like this is dative.",
                    "label": 0
                },
                {
                    "sent": "This is accusative.",
                    "label": 0
                },
                {
                    "sent": "Will help the speaker understand the relative role of the reference of the various groups that are involved in this sentence.",
                    "label": 0
                },
                {
                    "sent": "These groups can move around quite freely.",
                    "label": 0
                },
                {
                    "sent": "I mean within certain constraints, whereas here you don't have such a rich morphology, but you have a strong word order that will help the speaker understand again the role of the different constituents of the sentence and really what is difficult today is to model the way in three other languages constituents are allowed to move.",
                    "label": 0
                },
                {
                    "sent": "Within the sentence where you don't have the same order of the words from both in both languages.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what is mainstream statistical machine translate?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Chen basically to build a machine translation system.",
                    "label": 1
                },
                {
                    "sent": "The first thing you have to do is to collect data and data.",
                    "label": 1
                },
                {
                    "sent": "In this case takes the form of parallel sentences where is called in the community be text.",
                    "label": 0
                },
                {
                    "sent": "And not not only do we need parallel sentences, but we need to align them word for word for word.",
                    "label": 1
                },
                {
                    "sent": "So this is the first.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work we have to do when this is done, we can start training what is called the.",
                    "label": 0
                },
                {
                    "sent": "Translation model and I will refer to the translation model as the phrase table.",
                    "label": 1
                },
                {
                    "sent": "Because the translation model in such systems is based on variable length units that are to my way, to my view, a bit inappropriately called phrases, even though they don't have a linguistic content.",
                    "label": 0
                },
                {
                    "sent": "Or linguistic in.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rotation.",
                    "label": 0
                },
                {
                    "sent": "The second step of training is to build a target statistical language.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All using monolingual texts, and this is fairly common and I will go briefly.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On that topic.",
                    "label": 0
                },
                {
                    "sent": "The next step is to make sure that your system.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is tuned.",
                    "label": 0
                },
                {
                    "sent": "Because translating then will take the form of solving a complex maximization problem involving here a combination of various scores.",
                    "label": 0
                },
                {
                    "sent": "So three tuning is making sure you give the right weights to the various components of the models.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once you've done that, you can get some numbers.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you're not happy with them, you start again.",
                    "label": 0
                },
                {
                    "sent": "So I will now detail each of these steps in more depth.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starting with the corporate acquisition acquisition step.",
                    "label": 0
                },
                {
                    "sent": "So what we need here is a billing world corpus align on the person sentence per sentence.",
                    "label": 0
                },
                {
                    "sent": "There are ways to do it.",
                    "label": 0
                },
                {
                    "sent": "I won't talk about them in this in this talk, but basically we know how to take parallel corpora and to get down to such a pair of sentences.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where do we find the data?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, the main sources have been historically documents from the Institute for Multilingual institutions, such as the Canadian Parliament, the European Parliament, the United Nations, and this institutions have helped build corpora that contains several millions of sentence pairs and several hundreds of millions of words.",
                    "label": 0
                },
                {
                    "sent": "So these are pretty large corpora.",
                    "label": 1
                },
                {
                    "sent": "People people have also tried to use literature, touristic guides, technical documentations.",
                    "label": 1
                },
                {
                    "sent": "There is every kind of sources that was at the time.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll with the.",
                    "label": 0
                },
                {
                    "sent": "With the.",
                    "label": 0
                },
                {
                    "sent": "Widening of the potential applications of machine translation, people have started consider started to consider different jobs of text such as news websites, blogs and speech speech transcripts, also featuring scripts.",
                    "label": 0
                },
                {
                    "sent": "Some again are fairly easy to collect.",
                    "label": 0
                },
                {
                    "sent": "For instance, you can find websites in, you can find multilingual websites, so you will get a parallel website, but it's important to realize that for some applications people are really starting to develop.",
                    "label": 0
                },
                {
                    "sent": "Opera four training statistical machine translation system and this is.",
                    "label": 0
                },
                {
                    "sent": "This makes a big difference because not every text or not every translation of every text is suitable to train a statistical machine translation system.",
                    "label": 0
                },
                {
                    "sent": "And it is clear that it is easier to work with translation that are very literal than to work with more literary.",
                    "label": 0
                },
                {
                    "sent": "I would say translations.",
                    "label": 0
                },
                {
                    "sent": "So I mean.",
                    "label": 0
                },
                {
                    "sent": "The fact that we are building corpora for especially well, sorry that we're designing corpora for the pure.",
                    "label": 0
                },
                {
                    "sent": "The simple aim of building statistical machine translation is a novelty, and it provides us with a new kind of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of data.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is never enough so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People have trained to investigate ways to mine not only parallel corpora but also comparable corpora.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I won't detail that today.",
                    "label": 0
                },
                {
                    "sent": "So basically regarding corpora, my opinion is that we have large corpora.",
                    "label": 1
                },
                {
                    "sent": "And they pose specific problem because we have to be able to fare with millions of parallel sentences and very large models etc.",
                    "label": 1
                },
                {
                    "sent": "But.",
                    "label": 0
                },
                {
                    "sent": "At the same time this this corpora are quite restricted restricted in scope, especially the European Parliament.",
                    "label": 1
                },
                {
                    "sent": "Debates, for instance, is really boring in some ways.",
                    "label": 0
                },
                {
                    "sent": "And so data scarcity is still an issue and still hampers the development of machine translation system for many domains.",
                    "label": 0
                },
                {
                    "sent": "For many languages and still impairs the development of general domain machine statistical machine translation systems.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once the data data has been collected, the first step as I said is to build a word alignment.",
                    "label": 0
                },
                {
                    "sent": "So here you got an example of what we'd like to have an alignment between an English sentence here on the French sentence here.",
                    "label": 0
                },
                {
                    "sent": "This is an isometric alignment.",
                    "label": 0
                },
                {
                    "sent": "This is not obvious in this case, but the alignments were aiming at in this step or asymmetric.",
                    "label": 0
                },
                {
                    "sent": "Each French word is linked with exactly 1 English word, but the reverse is not true here you have an English word that is linked with several French word, so the alignment realignments we're building here asymmetric during this first step.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice alignment.",
                    "label": 0
                },
                {
                    "sent": "I mean, the sentences are almost parallel.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Except for a few cases where the we have local reorderings, because French and English don't agree in the way, they, the adjective, is located with respect to its sorry head noun.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have one example here, one exam.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here on the last example here were English uses a noun phrase were French uses a prepositional phrase.",
                    "label": 0
                },
                {
                    "sent": "So this makes another slight change in the words or order.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a nice case.",
                    "label": 0
                },
                {
                    "sent": "Of course we have to we have to deal with messier cases like this one.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where we have a long distance rearrangement here.",
                    "label": 0
                },
                {
                    "sent": "Well, the English group, it seems absolutely disgraceful, is moved to the end of the sentence of the French sentence.",
                    "label": 1
                },
                {
                    "sent": "And we even have.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Errors in this alignment, like the French porno.",
                    "label": 0
                },
                {
                    "sent": "Pronoun E is aligned with the verb, whereas the English pronoun it is aligned with an adverb.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have a case where the two French pronouns new our line with the same reflexive pronouns in English ourselves.",
                    "label": 0
                },
                {
                    "sent": "And that's, again is wrong.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way to do it is to train a symmetric alignment model.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these models are basically generative joint model of an alignment.",
                    "label": 0
                },
                {
                    "sent": "Under French sentence, given the English sentence and as the alignment is an hidden variable, you have to do something like EM training or something of the like.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once you've trained your models.",
                    "label": 0
                },
                {
                    "sent": "You can align the words.",
                    "label": 0
                },
                {
                    "sent": "You can build your sorry your word alignment by solving this this this decoding problem.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can even translate.",
                    "label": 0
                },
                {
                    "sent": "And this this in fact was the reason why this models were first proposed in the litter in the in the 90s by the people at IBM who proposed a series of models from IBM, one to IBM Five which were aimed at.",
                    "label": 0
                },
                {
                    "sent": "Providing statistical, the first statistical word based translation models.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have public domains implementation for this models.",
                    "label": 0
                },
                {
                    "sent": "They are fairly efficient there are.",
                    "label": 0
                },
                {
                    "sent": "Even we have even parallel implementations of this model, so I guess the general point of view.",
                    "label": 0
                },
                {
                    "sent": "But word word alignment is that people are happy with these tools and they don't on they stop here basically.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, recent recent research has shown that you can do a bit better than that using discriminative training techniques.",
                    "label": 1
                },
                {
                    "sent": "And if you do discriminative training and if you add many more features in your model, such as you can in a discriminative model, you can enforce the fact the constraint that cognates should align or words with the same frequency should align or words having the same length should align.",
                    "label": 1
                },
                {
                    "sent": "At least preferably so you can.",
                    "label": 0
                },
                {
                    "sent": "You can have.",
                    "label": 0
                },
                {
                    "sent": "You can add sorry, many more features in your model and that will help.",
                    "label": 0
                },
                {
                    "sent": "That will help you.",
                    "label": 0
                },
                {
                    "sent": "At least that will provide you better.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alignment.",
                    "label": 0
                },
                {
                    "sent": "To be able to do that, what you need is training data and training data in that task is manually aligned word.",
                    "label": 0
                },
                {
                    "sent": "Sorry sentences, manually aligned at the world level and this is a resource that is that it's really difficult to build.",
                    "label": 0
                },
                {
                    "sent": "I mean, as you've seen, it's not easy to decide how the words should be aligned together, so.",
                    "label": 0
                },
                {
                    "sent": "Basic.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least for a symmetric model, this is more or less considered as a solved issue in the community, and people simply take the existing public domain implementation and stay.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From there.",
                    "label": 0
                },
                {
                    "sent": "The next step in training the translation model consists in extracting what I called phrase, even though there are more correctly designed as clumps or chunks of words.",
                    "label": 0
                },
                {
                    "sent": "So what you do here is you take you run the alignment model in both directions so you have one French to English alignment and you have one English to French alignment, and somehow you symmetrize.",
                    "label": 0
                },
                {
                    "sent": "The alignment, for instance, taking the intersection of all the links that you have and maybe doing something more until you get something like that.",
                    "label": 0
                },
                {
                    "sent": "And this is a symmetric alignment between two sentences.",
                    "label": 0
                },
                {
                    "sent": "Again, French and English.",
                    "label": 0
                },
                {
                    "sent": "This one rates from bottom to top.",
                    "label": 0
                },
                {
                    "sent": "On this one, sorry it's not French.",
                    "label": 0
                },
                {
                    "sent": "Is German and English.",
                    "label": 0
                },
                {
                    "sent": "This one reads from bottom to top on this run with this way.",
                    "label": 0
                },
                {
                    "sent": "So how do we extract this multi word alignments?",
                    "label": 0
                },
                {
                    "sent": "It's fairly simple.",
                    "label": 0
                },
                {
                    "sent": "What we do is we go from left.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Writer Anne we will build.",
                    "label": 0
                },
                {
                    "sent": "Blocks rectangular blocks, with the constraints that whenever a row word in the row appears within a block, then it's it's made in.",
                    "label": 0
                },
                {
                    "sent": "The column should be within the block as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I do that I can have one one.",
                    "label": 0
                },
                {
                    "sent": "Best like Michael Michael.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a small rectum.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this rectangle can be extended again this respect the constraints, the constraints I have stated.",
                    "label": 0
                },
                {
                    "sent": "Michael assumes with Michael Mikkel get the phone house.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can again extend this rectangle and the point here.",
                    "label": 0
                },
                {
                    "sent": "The important point here is that I'm including some word that is not aligned.",
                    "label": 0
                },
                {
                    "sent": "That is, a common German has no English counterpart, but still it's included within the phrases within the phrase and it's important in that German construct that we have the coma here.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an example of pair that you can't extract because in that case, for instance, Houser in German is aligned with House, so if you take that rectangle it should contain Hauser, but it's made his house is outside the rectangle, so you don't extract this one.",
                    "label": 0
                },
                {
                    "sent": "Ugh.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going along on the on the sentence you get also this one.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many others.",
                    "label": 0
                },
                {
                    "sent": "So once you've done that for every sentence in your corpus, you have your translation model almost ready.",
                    "label": 0
                },
                {
                    "sent": "What we have collected here is pairs.",
                    "label": 0
                },
                {
                    "sent": "Of English French chunks, together with some statistics, for instance, the count of how many times we see this pair in the in the corpus.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a very heuristic and very noisy process.",
                    "label": 1
                },
                {
                    "sent": "I just want to show you.",
                    "label": 0
                },
                {
                    "sent": "Real war.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Old free stable.",
                    "label": 0
                },
                {
                    "sent": "That they have extracted from the European data.",
                    "label": 0
                },
                {
                    "sent": "So in this very stable we have about 500 different translations for European Community 500.",
                    "label": 0
                },
                {
                    "sent": "And this is because I mean we can see European Commission with the correct translation.",
                    "label": 0
                },
                {
                    "sent": "But also here because of a misalignment we have the article and here we have not only the article but also a preposition etc.",
                    "label": 0
                },
                {
                    "sent": "And here again, because of misalignment we have the comma within the group.",
                    "label": 0
                },
                {
                    "sent": "So we have many many translation like that that is our model, our system when you will have to translate European Commission into French, we have to choose among.",
                    "label": 0
                },
                {
                    "sent": "That many numbers of alternatives.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After you have chosen some more examples for culture which is polysemic both in French an English, it can mean on both.",
                    "label": 0
                },
                {
                    "sent": "You can have both the agricultural meaning at all, so the artistic meaning, for instance culture has about 100 translations.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And worse, the exclamation mark is about 700 different translations just because of alignment problems.",
                    "label": 0
                },
                {
                    "sent": "So this is really a very noisy process.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course, not all these phrases are equally reliable, so we have to find ways to assess the reliability.",
                    "label": 0
                },
                {
                    "sent": "The reliability of 1 French translation for each.",
                    "label": 0
                },
                {
                    "sent": "Of each possible translation of an English sentence.",
                    "label": 0
                },
                {
                    "sent": "And if you just take the usual maximum likelihood estimate of the conditional probability you run into problems because of the city.",
                    "label": 0
                },
                {
                    "sent": "So you have to do something about data paucity.",
                    "label": 0
                },
                {
                    "sent": "For instance, taking several estimates of this quantity.",
                    "label": 0
                },
                {
                    "sent": "And this really.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mount your system.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly, trying to extract linguistic phrases doesn't help, and it seems.",
                    "label": 0
                },
                {
                    "sent": "Even to hurt the performance of the system.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If size is an issue, I mean if you if you don't want to work with that with a very big with a very large phrase table, what you can do is prune the phrase table based on statistical grounds.",
                    "label": 1
                },
                {
                    "sent": "That is, get away with phrases that have.",
                    "label": 0
                },
                {
                    "sent": "Conditional probability that is below the chance.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For instance, to appear.",
                    "label": 0
                },
                {
                    "sent": "If size is not an issue, then there are ways to have even larger, larger tables.",
                    "label": 1
                },
                {
                    "sent": "Larger phrase tables using gapi phrases, and I may.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come back.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This issue a bit later.",
                    "label": 0
                },
                {
                    "sent": "But basically everyone seems to agree on the fact that the largest your phrase table, the better performance.",
                    "label": 1
                },
                {
                    "sent": "So people are trying all possible tricks to get more and more phrases, even if the resulting translation model is very noisy, like for instance combining phrase tables from different corpora.",
                    "label": 1
                },
                {
                    "sent": "So you learn the phrase table with the European data, you learn a different phrase table with the national.",
                    "label": 0
                },
                {
                    "sent": "With the nation with the United Nation data and you combine them or even you take translation dictionaries and you see them as a phrase table and you put them into your system.",
                    "label": 0
                },
                {
                    "sent": "So basically the goal is to have the biggest possible phrase table.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next step in training is fairly easy.",
                    "label": 0
                },
                {
                    "sent": "It's just a matter of training.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Target language.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also I will.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll go briefly on this one because I mean, I guess you're all familiar with the training statistical language model.",
                    "label": 0
                },
                {
                    "sent": "Just know that in empty we're using the same kinds of model that we use for speech recognition.",
                    "label": 1
                },
                {
                    "sent": "With some small you know we have small differences.",
                    "label": 0
                },
                {
                    "sent": "For instance, it has been shown that using very large funds model was helping for translation, so people again are trained to use all possible kinds of data to train their model.",
                    "label": 0
                },
                {
                    "sent": "And it's it's common to have systems where the language model is trained on more than a billion tokens.",
                    "label": 1
                },
                {
                    "sent": "So you have again.",
                    "label": 0
                },
                {
                    "sent": "You have to be capable of accommodating, accommodating very large textual corpora to train.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This model.",
                    "label": 0
                },
                {
                    "sent": "I won't detail them here, although I mean if you look at the slides afterwards, you have the exact numbers, but in a recent experiment Google the Teal.",
                    "label": 0
                },
                {
                    "sent": "Sorry, Google's team has tried to use all the data they were capable of processing and of course.",
                    "label": 0
                },
                {
                    "sent": "That wasn't doable with current study with current estimation techniques for language models, because when you train language models, you have to do some kind of smoothing and smoothing was no longer possible given the size of data they were using, so they decided not to smooth sorta smooth in a very simple way, and in fact it proved that it was more helpful to put as many data they could with the wrong model, then less data.",
                    "label": 0
                },
                {
                    "sent": "With the correct model.",
                    "label": 0
                },
                {
                    "sent": "So basically the.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Focus in the area of language modeling is more on scalability than modeling, and I mean I'm citing her recent work on you know people explaining how to scale up the language model allowed to train with gigantic opera or two feet gigantic language models within the memory.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next you have to tune your score function.",
                    "label": 0
                },
                {
                    "sent": "So basically what we're going to do is decode by finding the sentence here East that maximizes some score where the score is defined as a combination of many models.",
                    "label": 0
                },
                {
                    "sent": "OK, your best translation as to be a good translation of all the phrases, so it has to have a high score for the translation model.",
                    "label": 0
                },
                {
                    "sent": "But also it has to have a high score for the language model and he has to have a high score for many other models that you can put into your system like distortion, model links, model segmentation models etc.",
                    "label": 0
                },
                {
                    "sent": "So you have to.",
                    "label": 0
                },
                {
                    "sent": "You have to go to put the right weights for each model here Lambda.",
                    "label": 0
                },
                {
                    "sent": "So basically this step amounts to take some additional set of text, although data and try to optimize the weights in this in this combination, sorry.",
                    "label": 0
                },
                {
                    "sent": "So this amounts to solving some kind of optimization.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem here were you try to find the lambdas that will minimize your loss over the L dot set.",
                    "label": 0
                },
                {
                    "sent": "This is not as easy as it may seem.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because the loss is not differentiable in Lambda because the losses related to the lambdas in in a very indirect manners, because the lambdas will influence your translation, but your translations are discrete by nature.",
                    "label": 0
                },
                {
                    "sent": "Well sets of words.",
                    "label": 0
                },
                {
                    "sent": "So basically you have to do something to work with optimization with non differential with non differentiable functions.",
                    "label": 0
                },
                {
                    "sent": "And I mean this can be done by using conventional optimization techniques like line.",
                    "label": 0
                },
                {
                    "sent": "Linesearch or the simplex algorithm or many other things.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The main point here is that you do you have to do it right.",
                    "label": 0
                },
                {
                    "sent": "Because it makes a big difference in your final results.",
                    "label": 0
                },
                {
                    "sent": "So no matter what you do, try to do it right.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once the system is trained, decoding is really conceptually simple.",
                    "label": 0
                },
                {
                    "sent": "We just.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need to solve, sorry this problem here.",
                    "label": 0
                },
                {
                    "sent": "So we find the sentence that maximizes this score.",
                    "label": 0
                },
                {
                    "sent": "And of course, you can't find the exact solution.",
                    "label": 0
                },
                {
                    "sent": "I mean this problem cannot be solved because the large base, the search base salary is too big to be search exhaustively.",
                    "label": 0
                },
                {
                    "sent": "You have to consider all the segmentations of the source phrase of the source sentence an all and for also on for every segment.",
                    "label": 0
                },
                {
                    "sent": "You have to.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider all possible translations, and as you've seen, you can.",
                    "label": 0
                },
                {
                    "sent": "You can have several hundreds of possible translation for each phrase in the source.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bridge and then worst, you have to consider every permutation because of word order difference.",
                    "label": 0
                },
                {
                    "sent": "You have to consider every possible permutation of the source phrases and this makes the the problem really.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intractable.",
                    "label": 0
                },
                {
                    "sent": "So what people do usually there is sort of some kind of heuristic search.",
                    "label": 0
                },
                {
                    "sent": "And the results you have are really a function of the cleverness of the pruning.",
                    "label": 0
                },
                {
                    "sent": "You implement the pruning strategy you use within the.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "During the search.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless people state of the art systems are achieving high performances at a reasonable decoding rate.",
                    "label": 1
                },
                {
                    "sent": "To give you an order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "It's going to have systems that can translate more than 100 words per second, so this is much more than we need for laboratory systems.",
                    "label": 1
                },
                {
                    "sent": "I mean, I'm not talking here about prediction system, but I mean in the labs.",
                    "label": 0
                },
                {
                    "sent": "This is more than this is faster enough for us.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, I can in at the end of the talk I can come back if you'd like to see me because I have a slide that describes exactly how decoding works for this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A problem, but I I'd better.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move on.",
                    "label": 0
                },
                {
                    "sent": "Machine to machine translation evaluation is a difficult problem, even even the evaluation of the translation by humans is difficult.",
                    "label": 0
                },
                {
                    "sent": "But for machine it's much worse, and it's even in fact it's almost as difficult to have good objective.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Machine evaluation as to have good machine translation, I mean both problems are really as difficult.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless it was it was felt.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the in the community that it was necessary to have.",
                    "label": 0
                },
                {
                    "sent": "Metric to evaluate automatically translation because in the neighborhood, neighboring field of speech recognition having metrics really pushed the progress.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the field so people have came up with what I call a fragile consensus, which is a metric that has been proposed by the people at IBM in 2001.",
                    "label": 0
                },
                {
                    "sent": "It's a very crude metric.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple metric, but nonetheless it constitutes nowadays and exited ref.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once.",
                    "label": 0
                },
                {
                    "sent": "And what it does it just measure the surface similarity between your hypothesis an the reference translation.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the surface, similarity is a simple function of the N gram precision, so it's a very simple score.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have an example here.",
                    "label": 0
                },
                {
                    "sent": "Assume that you want to assess the quality of this.",
                    "label": 0
                },
                {
                    "sent": "I put this with respect to these two translations.",
                    "label": 0
                },
                {
                    "sent": "Ewww",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just compute the unigram match so.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the unigram.",
                    "label": 0
                },
                {
                    "sent": "If you in your hypothesis are OK, so your unigram matches one, because I is here is involves filling is here and good is here.",
                    "label": 0
                },
                {
                    "sent": "So all the unigrams now OK. 2/3 of the bigrams are OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when you have these four numbers, then you take the geometric mean and basically this is a blue score.",
                    "label": 1
                },
                {
                    "sent": "In practice, you don't do exactly that, but you average.",
                    "label": 0
                },
                {
                    "sent": "This calls over the whole test corpus because you know in some cases you have zeros, and I mean taking the geometric mean will give you very bad numbers if you don't do anything.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluation is still an active research topic.",
                    "label": 0
                },
                {
                    "sent": "I mean, people are.",
                    "label": 0
                },
                {
                    "sent": "Not so happy with Blur because I mean blue eyes as allowed to measure progress in the five last years, but it's felt that blue is not fair, is not entirely fair, especially to systems that are trained to do more complicated things than what I'm presenting now.",
                    "label": 0
                },
                {
                    "sent": "So there is still active debates within the community and many proposals of new metrics are on.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discussions.",
                    "label": 0
                },
                {
                    "sent": "So this is it.",
                    "label": 0
                },
                {
                    "sent": "At this stage, I mean it must be pretty obvious, at least to anybody in this room that what I've presented is entirely finite state, because the phrase table look up is a finite state.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assess and because ngram language models are also finite state.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if you want to implement a system, it's simply a matter of simple combination of finite state operations where you take your input.",
                    "label": 0
                },
                {
                    "sent": "Sorry, you take your input F. You look through composition in the phrase table to get a lattice containing all the possible translations of your input sentence.",
                    "label": 0
                },
                {
                    "sent": "You project it onto the target side, uncombined with the language model and this will score.",
                    "label": 0
                },
                {
                    "sent": "This will re score the passing this lattice with the language model scores and finding the best path is exactly what you do when you do monotonic decoding.",
                    "label": 0
                },
                {
                    "sent": "So basically, I mean this has been.",
                    "label": 0
                },
                {
                    "sent": "Acknowledge long ago, all these techniques are finite state.",
                    "label": 0
                },
                {
                    "sent": "As I have tried to.",
                    "label": 0
                },
                {
                    "sent": "Show in the beginning of my talk translation is not a matter of monotonic decoding because you have word order differences between language.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you need to do something about reordering right, and it's fairly easy to do.",
                    "label": 0
                },
                {
                    "sent": "Just changing this from this to this where I've included some kind of function that will compute for me permutations.",
                    "label": 0
                },
                {
                    "sent": "Of the input sentence, I mean I know there are a finite number of permutations, so I know that them of F is itself a finite state machine or can be represented as a finite state machine, albeit a very large one.",
                    "label": 0
                },
                {
                    "sent": "So decoding reordering is again a simple matter of solving this finite state.",
                    "label": 0
                },
                {
                    "sent": "Affirming implementing these finite state operations.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this point of view is great because we have efficient implementations of finite state tools.",
                    "label": 0
                },
                {
                    "sent": "We know that these tools can scale up, at least if you use lazy on the fly implementations of composition.",
                    "label": 0
                },
                {
                    "sent": "We have training procedures.",
                    "label": 0
                },
                {
                    "sent": "We have ways we know how to integrate more knowledge sources within the system.",
                    "label": 0
                },
                {
                    "sent": "For instance, if we if we want to add some morphological analysis, we know we know we could do that.",
                    "label": 0
                },
                {
                    "sent": "By changing, I think some kind of morphological.",
                    "label": 0
                },
                {
                    "sent": "Transducer here etc etc.",
                    "label": 0
                },
                {
                    "sent": "So when you adopt that view.",
                    "label": 0
                },
                {
                    "sent": "Basically, you have only one problem.",
                    "label": 0
                },
                {
                    "sent": "And the problem?",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "How to implement perm?",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, a lot of research has been put.",
                    "label": 0
                },
                {
                    "sent": "A lot of effort has been put into the implementation of the reordering, and there are many proposals again that are on the table.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The most I mean the most popular system basically tries a brute force approach, so basically considers all possible permutations.",
                    "label": 0
                },
                {
                    "sent": "But as this is intractable, it will highly penalize permutations that are far from the monotonic alignment.",
                    "label": 0
                },
                {
                    "sent": "OK, so in principle every permutation is possible is explored during the search except that.",
                    "label": 0
                },
                {
                    "sent": "The closer you stay to the monotonic alignment, the higher the score basically.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The alternative is to restrict a priori the set of possible.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stations.",
                    "label": 0
                },
                {
                    "sent": "Anne again.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, there are various ways to do it.",
                    "label": 0
                },
                {
                    "sent": "You can do it with finite state techniques, for instance, trying to define the transducer T which will allow you to compute a subset of this of the set of all permutation through a rational transaction, like here.",
                    "label": 0
                },
                {
                    "sent": "Or you can use context free models.",
                    "label": 0
                },
                {
                    "sent": "Where you will try to find some grammar.",
                    "label": 0
                },
                {
                    "sent": "Some synchronous grammar here that will help you define the set of the set of permutations you will consider as a set of string F prime that are jointly derived from South together with F. OK.",
                    "label": 0
                },
                {
                    "sent": "Instead of using a priori models, what you can try to.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who is to look at the data and learn such?",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "User.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or such grammar.",
                    "label": 0
                },
                {
                    "sent": "And again, this has been done.",
                    "label": 0
                },
                {
                    "sent": "In the past, so both approaches have example and you can find example of both approaches in the literature.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or you can try to use your linguistic knowledge.",
                    "label": 0
                },
                {
                    "sent": "By defining reordering rules that will carefully restrict the set of possible permutations of F that you want to consider.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the search on, in fact, any combination thereof is possible.",
                    "label": 0
                },
                {
                    "sent": "That is, you could define a priori.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gramma sorry.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You could define a priori transducer here and try to learn the weights on this one user based on what you see in your data.",
                    "label": 0
                },
                {
                    "sent": "Or you can try to combine.",
                    "label": 0
                },
                {
                    "sent": "I don't know linguistic information with a priori restrictions etc and this has been tried also.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the literature.",
                    "label": 0
                },
                {
                    "sent": "And I mean I can.",
                    "label": 0
                },
                {
                    "sent": "Illustrate any of this with.",
                    "label": 0
                },
                {
                    "sent": "So if for inst.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since.",
                    "label": 0
                },
                {
                    "sent": "IBM style constraint that you will that you often find in the literature are typically within the in that Category 4 full typically in that categories the IBM style constraints are constraints or other possible permutation that you can consider and the rule is very simple.",
                    "label": 0
                },
                {
                    "sent": "You try to produce permutations here permutations on a set of numbers OK from left to right and at each step you have the choice between.",
                    "label": 0
                },
                {
                    "sent": "The K remaining items, that is the K remaining numbers that you haven't already chosen, right?",
                    "label": 0
                },
                {
                    "sent": "So assuming that after four throw, the situation is like that.",
                    "label": 0
                },
                {
                    "sent": "So zero is taken to his taken 3 steak and five is taken, but you have.",
                    "label": 0
                },
                {
                    "sent": "Forgotten somehow one and four so they are free.",
                    "label": 0
                },
                {
                    "sent": "Assuming that you have the choice between 4 numbers.",
                    "label": 0
                },
                {
                    "sent": "At the step at the next step, what you can do is either choose one OK. And then you next choice will be between 3:00.",
                    "label": 0
                },
                {
                    "sent": "5 six and seven.",
                    "label": 0
                },
                {
                    "sent": "Or you can choose four and your next choice will be between 1656 and seven.",
                    "label": 0
                },
                {
                    "sent": "Or you can choose six on your next choice will be between.",
                    "label": 0
                },
                {
                    "sent": "146 and seven OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is very, very simple.",
                    "label": 0
                },
                {
                    "sent": "You can add additional constraint like restricted number of moves to take place between within.",
                    "label": 0
                },
                {
                    "sent": "Sorry fixed size window for instance impose the constraint at zero.",
                    "label": 0
                },
                {
                    "sent": "For instance shall not be selected after the five draw.",
                    "label": 0
                },
                {
                    "sent": "The 5th row for instance.",
                    "label": 0
                },
                {
                    "sent": "OK, so you limit your somehow limitating the movement the possible movement of zero OK?",
                    "label": 0
                },
                {
                    "sent": "Or you can restrict the number of simultaneous gaps.",
                    "label": 0
                },
                {
                    "sent": "So you've seen gaps in my previous representation.",
                    "label": 0
                },
                {
                    "sent": "You can limit the number of gaps to one or two.",
                    "label": 0
                },
                {
                    "sent": "2 for instance, and that will yield more restrictions of the other possible permutation that you will consider during searching.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, this is what you get.",
                    "label": 0
                },
                {
                    "sent": "But the point here I'm not even to define the transducer, but you can compute this this with a finite state transducer.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shackleton, OK, so an example of what context?",
                    "label": 0
                },
                {
                    "sent": "Free models?",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Context free model sorry are used to define the set of possible alignments is given.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And by the model of invention construction grammars ITG's that were introduced in the field of machine translation by dekiru, so these are synchronous grammars.",
                    "label": 0
                },
                {
                    "sent": "So it just like context free grammars, except that you're producing two derivations in parallel.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're working with two alphabets, Sigma and gamma OK on each.",
                    "label": 0
                },
                {
                    "sent": "Each rewrite rules as two left hands as two right inside.",
                    "label": 0
                },
                {
                    "sent": "OK, the specific constraint within inversion transaction grammars is that the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Are limited to be either equal the two right inside of a rule are either equal or a mirror of one another.",
                    "label": 0
                },
                {
                    "sent": "OK, so the only thing you can find is either twice the same right hand side or one is a mirror of the other.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how can you use such devices to generate permutations?",
                    "label": 0
                },
                {
                    "sent": "Just consider this very simple it G. You have only two productions here.",
                    "label": 0
                },
                {
                    "sent": "Plus one terminating production for each vocabulary word.",
                    "label": 0
                },
                {
                    "sent": "OK, then you can define the set of permutations of W1W N to be all the words.",
                    "label": 0
                },
                {
                    "sent": "All the sequences of words that can be jointly derived.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From X, so if this is a pass for W1W four.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You will have this permutation OK, and flipping this subtree.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "And many others.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So more about the these kinds of constraints that define the strict strict subset of all permutations.",
                    "label": 0
                },
                {
                    "sent": "It is proven that you cannot get all the possible permutations with such devices.",
                    "label": 0
                },
                {
                    "sent": "Still, the number of permutation remains.",
                    "label": 0
                },
                {
                    "sent": "Caminha tauriel.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "In the order of K to the power N, word K is around six, so it's still growing very fast, but still it can be searched in polynomial time using parsing techniques basically.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will give you last.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, which is an instance.",
                    "label": 0
                },
                {
                    "sent": "I mean this is an example of of.",
                    "label": 0
                },
                {
                    "sent": "Trying to infer in user as a finite state transducer from the data and this has been promoted by Hughes at Craig, who is now with Lindsey and this is something we are working on actively at the Lindsay.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do here is during the training procedure.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_121": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "You do the training procedure as usual, except that.",
                    "label": 1
                },
                {
                    "sent": "You will extract within phrase reordering rule that are based on part of speech tags.",
                    "label": 1
                },
                {
                    "sent": "So you have to 1st tag your corpus with morphological syntactical information and learn local reordering rules.",
                    "label": 0
                },
                {
                    "sent": "Each rule is.",
                    "label": 0
                },
                {
                    "sent": "It is right.",
                    "label": 0
                },
                {
                    "sent": "Role is compiled as a finite state transducer on your reordering transducer is simply the composition of all this individual transducer inducer.",
                    "label": 1
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So friends for example.",
                    "label": 0
                },
                {
                    "sent": "If this is your alignment in the data, so you hear you have Spanish and here you have English and again you have an instance of movement.",
                    "label": 0
                },
                {
                    "sent": "Between court movement of the noun at the end of the non phrase in English, whereas it is at the beginning of the non fray.",
                    "label": 0
                },
                {
                    "sent": "In Spanish.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The rule you will extract is this one.",
                    "label": 0
                },
                {
                    "sent": "You would expect the rule that in Spanish, when you have a sequence noun, adjective coordination, now you have to rewrite it as adjective coordination, adjective, noun OK.",
                    "label": 0
                },
                {
                    "sent": "So you take.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other possible rules you can find in your corpus and build this machine.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then, during decoding, the set of permutation is simply computed by composing the tag.",
                    "label": 0
                },
                {
                    "sent": "The input with this transducer and projecting it on the input side to get rid of the attacks.",
                    "label": 0
                },
                {
                    "sent": "And this is one of the way you can do it.",
                    "label": 0
                },
                {
                    "sent": "I'm sure that you have many other ideas of how you could learn such that machine, but this is just an illustration of what people have have come up with.",
                    "label": 0
                },
                {
                    "sent": "Trying to solve this problem.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My impression in that I mean no matter what ingenuity people have tried to put into this machinery's.",
                    "label": 0
                },
                {
                    "sent": "The gains you get are quite small, are quite small compared with monotonic translation, and in fact for languages such as French and English, for instance, it's almost better to do decoding without reordering.",
                    "label": 0
                },
                {
                    "sent": "And at the same time, so it doesn't help you when the language that you have to translate are quite similar.",
                    "label": 0
                },
                {
                    "sent": "It doesn't help you much and still you have a huge gap between performances you get when you translate easy language pairs and difficult language pairs.",
                    "label": 0
                },
                {
                    "sent": "To give you just the order of magnitude the scores we have for say French to English is in the area of 30 or 40.",
                    "label": 0
                },
                {
                    "sent": "What is the score we get for French?",
                    "label": 0
                },
                {
                    "sent": "German is more like 10 or 15.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a huge gap between easy pairs and difficult pairs, and this is for me the sign that this models are not good enough when we.",
                    "label": 0
                },
                {
                    "sent": "When we have to translate such different language.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK to summary, I will try to give some reasons why machine learning statistical models for machine translation work.",
                    "label": 0
                },
                {
                    "sent": "At least they work better.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then word based model and they work better than than work based model because they provide an easy and cheap way to model non competition non compositional translations like videos or terms or multi word units in general.",
                    "label": 0
                },
                {
                    "sent": "So I mean this is really easy with phrase base.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Machine translation they also give you almost for free local reordering.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, I've discussed already the case of objective non in.",
                    "label": 0
                },
                {
                    "sent": "In in English that have to be turned into non adjective in French and if you learn the phrase directly, you don't have to worry about reordering because reordering is given you for free.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something else that comes for free is what I call local context decisiones, local context.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An agreement.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you have to translate the International Conference into French word word, you would have to have some mechanism to ensure that the gender and number agreement in French is respected right?",
                    "label": 0
                },
                {
                    "sent": "Because the data miner than an adjective to be feminine on singular, all of them.",
                    "label": 0
                },
                {
                    "sent": "But if you translate the phrase you have to worry about agreement, I mean, agreement is free.",
                    "label": 0
                },
                {
                    "sent": "It's within the phrase.",
                    "label": 0
                },
                {
                    "sent": "So you have anything to do.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it relies simplicity, speed and raw.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Business.",
                    "label": 0
                },
                {
                    "sent": "And on top of that, there are two 2 reasons that explain the success of these techniques.",
                    "label": 0
                },
                {
                    "sent": "One is that basically what you're doing is you're matching large phrases in your in your in your training data, and it's exactly what the score the metrics have talked about his rewarding.",
                    "label": 0
                },
                {
                    "sent": "So I mean, there's no wonder that these systems are highly ranked that are really difficult to outperform.",
                    "label": 0
                },
                {
                    "sent": "They are, they do exactly what is needed to please the metrics.",
                    "label": 0
                },
                {
                    "sent": "OK, and there is yet another reason, but I haven't dared written it on my slide.",
                    "label": 0
                },
                {
                    "sent": "Is that the vast majority of the work that has been going on in machine translation has been to translate.",
                    "label": 0
                },
                {
                    "sent": "Foreigner.",
                    "label": 0
                },
                {
                    "sent": "Into English for ending anything, and for many reasons, I mean, English lends itself well to the kind of finite state based techniques that we have.",
                    "label": 0
                },
                {
                    "sent": "Very large corpora for building the target language model, for instance, and we.",
                    "label": 0
                },
                {
                    "sent": "We always go from languages with rich morphology to the language with the poor morphology, and this is much easier than doing doing it the other way around.",
                    "label": 0
                },
                {
                    "sent": "For instance, going to English from English to German is more difficult, much more difficult because you have to somehow predict the right case marker and plural an etc that you don't have in English.",
                    "label": 0
                },
                {
                    "sent": "So you really have to do something to predict them correctly, right?",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some reasons why it's it still fails.",
                    "label": 0
                },
                {
                    "sent": "I mean first, I mean, for me it was even the wonder that this kind of, you know techniques could work at all.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's purely so fast it has no morphology at all, right?",
                    "label": 0
                },
                {
                    "sent": "The only thing it considers our strings of characters or even characters so it's it has no morphology at all.",
                    "label": 0
                },
                {
                    "sent": "And I mean, it's it's clear that adding some morphology could help if we could find a way to.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do it efficiently.",
                    "label": 0
                },
                {
                    "sent": "In these models we only consider contiguous phrases OK and contiguous phrases.",
                    "label": 0
                },
                {
                    "sent": "They miss important generalization, like for instance just in French you have the the way you built negation in French is you put an ad Werner the verb and then power.",
                    "label": 0
                },
                {
                    "sent": "And if you want to and then the adverb.",
                    "label": 0
                },
                {
                    "sent": "So you have you have to embed a verb between none power right?",
                    "label": 0
                },
                {
                    "sent": "So and if you only consider contiguous fragments, while you will have to learn.",
                    "label": 0
                },
                {
                    "sent": "All the possible verbs that have to appear within their own path to get the right generalization to English.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Whereas the generalization is fairly simple, I mean you know, basically you you could extract know something, but then you would have, you know almost everything that is needed.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These models only use local syntax on the target side, and I mean this is this is known to be.",
                    "label": 0
                },
                {
                    "sent": "Not good enough to predict the long distance agreements or such phenomena that you can find.",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In many languages.",
                    "label": 0
                },
                {
                    "sent": "An important drawback of this model is as is that phrase waiting and phrase selection is context free.",
                    "label": 0
                },
                {
                    "sent": "I mean you in the architecture I've I've detailed, the assessment of the quality of the translation of for a given phrase is done once and for all.",
                    "label": 0
                },
                {
                    "sent": "It's always the same score you will use to translate European Community no matter what.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't look at the context of European Community to decide how reliable this translation is.",
                    "label": 0
                },
                {
                    "sent": "So I mean again, this is not a big issue, but I mean improving on all these dimensions should buy us some millage in improving statistical translation systems.",
                    "label": 0
                }
            ]
        },
        "clip_139": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main weakness again is this.",
                    "label": 0
                },
                {
                    "sent": "This models lack a global reordering model, and this is this is a shame for many reasons.",
                    "label": 1
                },
                {
                    "sent": "This is a shame because it costs you at decoding because we are considering many reorderings many permutations that are very, very unlikely.",
                    "label": 0
                },
                {
                    "sent": "That are very bad.",
                    "label": 0
                },
                {
                    "sent": "It's also a shame because I mean.",
                    "label": 0
                },
                {
                    "sent": "The due to the.",
                    "label": 0
                },
                {
                    "sent": "Due to the fact that we are learning very imperfect model, we can't easily transfer what we've learned from one domain to another domain.",
                    "label": 0
                },
                {
                    "sent": "Because I mean what we learn is really two attached to the kind of corpus we're processing.",
                    "label": 0
                },
                {
                    "sent": "So I mean using this poor this simple models is hampering the possibility to domain adaptation.",
                    "label": 0
                },
                {
                    "sent": "And there are many other reasons why it's it's a shame.",
                    "label": 0
                }
            ]
        },
        "clip_140": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't have a.",
                    "label": 0
                }
            ]
        },
        "clip_141": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The models.",
                    "label": 0
                }
            ]
        },
        "clip_142": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are my temporary conclusions.",
                    "label": 0
                },
                {
                    "sent": "I guess the most of the recent progress of SMT of SMT can be attributed to the use of simpler models and logic opera.",
                    "label": 0
                },
                {
                    "sent": "Plus many small things, small details, paying attention to details, doing the optimization right, etc.",
                    "label": 0
                },
                {
                    "sent": "It produces acceptable translations for many pairs.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can train trait on the web.",
                    "label": 0
                },
                {
                    "sent": "I mean Google system on the web is using statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "I guess Microsoft System on the web is now using statistical machine translation etc.",
                    "label": 0
                },
                {
                    "sent": "So you can see it's acceptable for many pairs.",
                    "label": 0
                },
                {
                    "sent": "There is 1 issue.",
                    "label": 0
                },
                {
                    "sent": "One important issue which is modeling world order and for that we need more linguistically informed systems that.",
                    "label": 0
                },
                {
                    "sent": "And that's a big restriction that nonetheless are able to provide us an acceptable level of robustness and speed.",
                    "label": 0
                },
                {
                    "sent": "Because as I said, you have to be able to process very large databases to build good systems.",
                    "label": 0
                }
            ]
        },
        "clip_143": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I stop and I can take some questions.",
                    "label": 0
                }
            ]
        }
    }
}