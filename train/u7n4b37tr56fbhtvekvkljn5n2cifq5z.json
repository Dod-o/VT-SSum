{
    "id": "u7n4b37tr56fbhtvekvkljn5n2cifq5z",
    "title": "Structural Properties as Proxy for Semantic Relevance in RDF Graph Sampling",
    "info": {
        "author": [
            "Laurens Rietveld, Faculty of Sciences, Vrije Universiteit Amsterdam (VU)"
        ],
        "published": "Dec. 19, 2014",
        "recorded": "October 2014",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2014_rietveld_structural_properties/",
    "segmentation": [
        [
            "And from the few University Amsterdam an Emma talk will be about samples, structural properties as proxy for semantic elements.",
            "For those who want to scroll forward or backward in my presentation, you can find it online as well.",
            "Presentations that louder seats filled up and else less now.",
            "1st of May."
        ],
        [
            "About the low clouds, an industry couple of datasets here in this table.",
            "So, DB Pedia everybody knows it's an link.",
            "Jada links.",
            "Geo data loss of geological data.",
            "And geographic data metalex at Dutch datasets about law.",
            "Open Biome.",
            "It's about RDF and Semantic Web dog foods from the SWC an you see the number of triple series data set but you can also see the number of queries we have so these queries are taken from the useful challenge there taken from the server logs of these datasets and the interesting thing here is the last column sense for the coverage of the data set for these queries.",
            "So how many?",
            "What portion of the data set is actually needed?",
            "Answer These queries and the interesting thing is that transferred me pedia.",
            "We have 640 queries and they only used points oh 3% of the data set.",
            "So we only use a very small portion.",
            "So isn't it possible to extract that part of the data set that we are really interested in?",
            "Anne."
        ],
        [
            "This brings me to the topic of my talk, which is relevance based sampling.",
            "Which is finding the smallest possible area of subgraph that covers the maximum number of potential queries.",
            "Questions which are relevant are how can we determine which triples are relevant and which are not, and how can we do this in a scalable fashion, both for the approach and for the evaluation?"
        ],
        [
            "And two approaches you can think of for creating such sample is one is informed sampling, so you know which queries will be asked and you can simply extract the triples needed to answer these queries and create your sample.",
            "But the problem is you don't always know it's queries will be asked and you don't.",
            "We only have a limited number of queries known, so the useful challenge provides queries for six datasets.",
            "Which is quite limited.",
            "So if we want to have a broader scope for this approach, we would need to do it somewhat differently.",
            "So therefore we come up with the concept of uninformed sampling, where we use the structure and the policy of the graph an.",
            "To determine which triples are relevant, and if we know the relevance of triples, we can easily short these triples by relevance and select the best book K. Let's resulting in a sample."
        ],
        [
            "So our approach for doing this is study the topology of the graph using network analysis tools.",
            "And then evaluate the relevance of these samples against the queries that we do know.",
            "So for evaluation purposes, we are still restricted through the six datasets from the useful challenge.",
            "Ann, this will hopefully answer the question whether a network structure is a good predictor for query answerability."
        ],
        [
            "1st About network analysis.",
            "So it is often used to explain real with phenomenons.",
            "Think about citation networks.",
            "Think about social networks.",
            "Finding central parts in a graph is often an interesting thing here.",
            "For instance, finding that note in a citation network which connects to more or less disconnected reaches areas.",
            "An so between solid, yeah.",
            "Good example for this.",
            "What it does is you calculate all the shortest files for every node in a graph and for each node you counting how many short responses in.",
            "This this is very expensive approach and very difficult to.",
            "To calculate in a distributed fashion, for instance using Hadoop, Google Pagerank, that's it is very scalable, so it's used by Google to rank their search results.",
            "And it uses a system of authoritative notes.",
            "So whenever you know this, authoritative is important, the outgoing edge of that nodes bears more weight."
        ],
        [
            "So we apply in our in our approach is 3 simple but very scalable methods.",
            "One is the in degree for each note we count the number of incoming links when it's out degree, we count the number of outgoing links and the other one is page rank which I just discussed."
        ],
        [
            "But there's a problem here, because these network members algorithms they we cannot just apply them to RDF graph, and RDF graph is in is a graph with labels edges in this network.",
            "Analysis algorithms they work on graph with unlabelled edges, so we need to rewrite these graphs to fit them into the form of suitable for these network members algorithms.",
            "But if you re writes these graphs then we are exactly influencing the thing that we are interested in, namely the structure of the graph and how we can use this to determine relevant triples.",
            "So our approach is to apply five different rebirth methods and see how this influences the sampling methods."
        ],
        [
            "So the first one, abbreviated by S, is the simple rewrite methods where we just remove the predicates an.",
            "This will cause issues.",
            "For instance, Sally has Weight 50, Sally has age 50.",
            "This little will lose all its context this number.",
            "The other approach is using unique little, so in this particular example, with his agent and his weights, these little or 50 will create two separate nodes.",
            "Other approach you can think of is the complex literals appreciated by CL, where we prepends the predicates to the literal value.",
            "And one other approaches without literal, simply removing all the literals from the from the data set altogether and removing the predicate of course.",
            "This will create a.",
            "Quite smaller graph, obviously with less edges and nodes.",
            "The final reward methods is quite different than the previous four.",
            "Here we treat every triple as a single note and there's an edge between two nodes.",
            "When they form a length of path two or in other words, when the object of one trip was the same as the subject of another.",
            "This will create a lot of smaller graph and a more disconnected graph as well.",
            "For instance, in this example you can see that we already have one orphan nodes coming up with no incoming and outgoing links.",
            "So what we have when we have applied this network analysis, we have weights assigned to all these notes that we need to get from node ways with triple weights were interested in the relevance of triples.",
            "So for the first 4 rewrite methods we have aggregation approach where we take the maximum weight of either the subject or object and assign data triple weight.",
            "When we have this weighted list of triples, we can easily rank the triples by relevance.",
            "And then create the simple select the best book game."
        ],
        [
            "Our evaluation.",
            "For each data set we have a query set.",
            "So for each query we actually we can execute it on the data set executed on sample, get the result set and see how many answers we would get on the sample compared to the original data set.",
            "This is calculating Rico.",
            "And we will do this for all the queries for each data set and the average recall for the queries would be the quality of that particular sample.",
            "We do this for sample sizes one till 99 to cover the complete spectrum of sample size is an as there is no related and inform sampling approach for RDF.",
            "We have two simple baselines.",
            "When it's random frequency, we apply this 10 * 2 more alleviates the deterministic this problem and the other one is resource frequency where for each triple we count how often each resource occurs in there in the graph.",
            "And the weight of the triple is the sum of all these frequencies."
        ],
        [
            "But there's a problem here, because when we calculate Rico in the old fashioned way, namely executing this query on the sample and only original 1.",
            "We will need to lose a lot of people so this formula shows calculates her number.",
            "How much triples we would need to evaluate on certain datasets.",
            "As input, the text to the number of samples of the data set, and then for each sample size 1 till 99 it multiplies this with a number of sampling methods, which is very bad.",
            "Methods times the network analysis algorithms, so 15 and the number of baseline methods, random sample and the resource frequency which is 11 times the number of triples in the data set.",
            "So in total for all the datasets we would need over 15,000 datasets including the samples over 1.4 trillion triples.",
            "And this is obviously not easy to analyze.",
            "So if we want to have this done in a fast fashion and we would really need powerful hardware and to load all the triples, execute the queries and calculate the recall.",
            "It's just impossible."
        ],
        [
            "So we need a scalable approach.",
            "So our scalable process retreat, which triples are used by query and let me know which ships are used factory.",
            "We can use outlook cluster to find the weights of these triples and analyze whether these triples would have been included in the symbol.",
            "This is scalable, so we only execute is each query.",
            "Once an we can do most of the parts on the Hadoop cluster.",
            "So an example to illustrate this, we have a query select."
        ],
        [
            "Person born in the capital of country.",
            "We have a data set with five triples and we have for this particular query two rows in a result set."
        ],
        [
            "What we need to do to get that ripples used in this query would first need to rewrite the query to a distinct stars, so we get all the used variables in this query.",
            "This would get you the right part of the table you see here, so we have loud on Samsung, Netherlands, Steven Berlin, Germany.",
            "An if you replace the variables in the query with the values from the result set, you would actually get the triples needed to answer this query.",
            "So here we will see on the left side the four triples needed to enter both rows in result set."
        ],
        [
            "And when we know these triples used by the query, we can easily calculate the recall.",
            "So on the left side we have the data set, again sorted by weight.",
            "If we would want to get a sample of 60%, we would need the top three triples and we can now easily on the right side the triples needed to answer the query.",
            "We can check our these queries include it in a possible sample of 60%.",
            "In this case, one of the roles would not be included.",
            "Then we would get a recall of 50%."
        ],
        [
            "So this evaluation is a better specificity than regular Rico is scalable you we use big instead of sparkle.",
            "There are some special cases or aggregates, groupby, limits distincts, but we want to talk to me about that too.",
            "We should talk.",
            "We should do that offline because."
        ],
        [
            "With short on time, so these are the datasets actually discuss them, before which we would which we evaluate."
        ],
        [
            "These are the results, so if you're looking at the online version, you can click on the forum or interactive version so we would see.",
            "Here are the open Biome.",
            "It's for instance, which has a recall effort Rico for the queries of 69% with a sample size of 6%.",
            "So this does extremely well.",
            "The Pedia has a more of a smooth curve, but we can already see that we have about a 50% recall for a sample size of 7%.",
            "If you look at the small prints for their sets, you see that the best sampling method here difference between a data set.",
            "This because there's an influence of both the creation used to evaluate the properties of the data sets in the sampling methods we use, and sadly, we cannot exhaustively find the correlations between all these features of the query set and data etc, because we just don't have enough courage to do this evaluation with."
        ],
        [
            "Some other small observations.",
            "I'll skip this for for time reasons.",
            "But the in general the message of this license is that again, there is a link between the Patch of queries.",
            "We use the best performing sampling method and certain properties of the data set.",
            "So are lots of literals described.",
            "There are lots of resources instead of rituals described."
        ],
        [
            "So finally the conclusion.",
            "What we did here was represented scalable pipeline for for create examples.",
            "So we apply network Ness algorithms combined with several rewrite methods.",
            "An hour evaluation approach was pretty scalable as well, so we were able to evaluate over 15,000.",
            "Virtual datasets, including the samples, of course, an consisting of 1.4 trillion triples.",
            "But again, the number of queries sets we have is too limited to learn significant correlations between the influence of the query sets, data center sampling method.",
            "But what it does show is that we can use the topology in the structure of the graph to determine good samples, and in essence this means that we can mimic semantic relevance through structural properties and all this without an uproar in notion of relevance.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And from the few University Amsterdam an Emma talk will be about samples, structural properties as proxy for semantic elements.",
                    "label": 1
                },
                {
                    "sent": "For those who want to scroll forward or backward in my presentation, you can find it online as well.",
                    "label": 0
                },
                {
                    "sent": "Presentations that louder seats filled up and else less now.",
                    "label": 0
                },
                {
                    "sent": "1st of May.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About the low clouds, an industry couple of datasets here in this table.",
                    "label": 0
                },
                {
                    "sent": "So, DB Pedia everybody knows it's an link.",
                    "label": 0
                },
                {
                    "sent": "Jada links.",
                    "label": 0
                },
                {
                    "sent": "Geo data loss of geological data.",
                    "label": 1
                },
                {
                    "sent": "And geographic data metalex at Dutch datasets about law.",
                    "label": 0
                },
                {
                    "sent": "Open Biome.",
                    "label": 0
                },
                {
                    "sent": "It's about RDF and Semantic Web dog foods from the SWC an you see the number of triple series data set but you can also see the number of queries we have so these queries are taken from the useful challenge there taken from the server logs of these datasets and the interesting thing here is the last column sense for the coverage of the data set for these queries.",
                    "label": 0
                },
                {
                    "sent": "So how many?",
                    "label": 0
                },
                {
                    "sent": "What portion of the data set is actually needed?",
                    "label": 0
                },
                {
                    "sent": "Answer These queries and the interesting thing is that transferred me pedia.",
                    "label": 0
                },
                {
                    "sent": "We have 640 queries and they only used points oh 3% of the data set.",
                    "label": 0
                },
                {
                    "sent": "So we only use a very small portion.",
                    "label": 0
                },
                {
                    "sent": "So isn't it possible to extract that part of the data set that we are really interested in?",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This brings me to the topic of my talk, which is relevance based sampling.",
                    "label": 0
                },
                {
                    "sent": "Which is finding the smallest possible area of subgraph that covers the maximum number of potential queries.",
                    "label": 0
                },
                {
                    "sent": "Questions which are relevant are how can we determine which triples are relevant and which are not, and how can we do this in a scalable fashion, both for the approach and for the evaluation?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And two approaches you can think of for creating such sample is one is informed sampling, so you know which queries will be asked and you can simply extract the triples needed to answer these queries and create your sample.",
                    "label": 0
                },
                {
                    "sent": "But the problem is you don't always know it's queries will be asked and you don't.",
                    "label": 0
                },
                {
                    "sent": "We only have a limited number of queries known, so the useful challenge provides queries for six datasets.",
                    "label": 0
                },
                {
                    "sent": "Which is quite limited.",
                    "label": 0
                },
                {
                    "sent": "So if we want to have a broader scope for this approach, we would need to do it somewhat differently.",
                    "label": 0
                },
                {
                    "sent": "So therefore we come up with the concept of uninformed sampling, where we use the structure and the policy of the graph an.",
                    "label": 0
                },
                {
                    "sent": "To determine which triples are relevant, and if we know the relevance of triples, we can easily short these triples by relevance and select the best book K. Let's resulting in a sample.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach for doing this is study the topology of the graph using network analysis tools.",
                    "label": 1
                },
                {
                    "sent": "And then evaluate the relevance of these samples against the queries that we do know.",
                    "label": 1
                },
                {
                    "sent": "So for evaluation purposes, we are still restricted through the six datasets from the useful challenge.",
                    "label": 0
                },
                {
                    "sent": "Ann, this will hopefully answer the question whether a network structure is a good predictor for query answerability.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st About network analysis.",
                    "label": 0
                },
                {
                    "sent": "So it is often used to explain real with phenomenons.",
                    "label": 0
                },
                {
                    "sent": "Think about citation networks.",
                    "label": 0
                },
                {
                    "sent": "Think about social networks.",
                    "label": 0
                },
                {
                    "sent": "Finding central parts in a graph is often an interesting thing here.",
                    "label": 0
                },
                {
                    "sent": "For instance, finding that note in a citation network which connects to more or less disconnected reaches areas.",
                    "label": 0
                },
                {
                    "sent": "An so between solid, yeah.",
                    "label": 0
                },
                {
                    "sent": "Good example for this.",
                    "label": 0
                },
                {
                    "sent": "What it does is you calculate all the shortest files for every node in a graph and for each node you counting how many short responses in.",
                    "label": 0
                },
                {
                    "sent": "This this is very expensive approach and very difficult to.",
                    "label": 0
                },
                {
                    "sent": "To calculate in a distributed fashion, for instance using Hadoop, Google Pagerank, that's it is very scalable, so it's used by Google to rank their search results.",
                    "label": 0
                },
                {
                    "sent": "And it uses a system of authoritative notes.",
                    "label": 0
                },
                {
                    "sent": "So whenever you know this, authoritative is important, the outgoing edge of that nodes bears more weight.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we apply in our in our approach is 3 simple but very scalable methods.",
                    "label": 0
                },
                {
                    "sent": "One is the in degree for each note we count the number of incoming links when it's out degree, we count the number of outgoing links and the other one is page rank which I just discussed.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's a problem here, because these network members algorithms they we cannot just apply them to RDF graph, and RDF graph is in is a graph with labels edges in this network.",
                    "label": 0
                },
                {
                    "sent": "Analysis algorithms they work on graph with unlabelled edges, so we need to rewrite these graphs to fit them into the form of suitable for these network members algorithms.",
                    "label": 0
                },
                {
                    "sent": "But if you re writes these graphs then we are exactly influencing the thing that we are interested in, namely the structure of the graph and how we can use this to determine relevant triples.",
                    "label": 0
                },
                {
                    "sent": "So our approach is to apply five different rebirth methods and see how this influences the sampling methods.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first one, abbreviated by S, is the simple rewrite methods where we just remove the predicates an.",
                    "label": 0
                },
                {
                    "sent": "This will cause issues.",
                    "label": 0
                },
                {
                    "sent": "For instance, Sally has Weight 50, Sally has age 50.",
                    "label": 0
                },
                {
                    "sent": "This little will lose all its context this number.",
                    "label": 0
                },
                {
                    "sent": "The other approach is using unique little, so in this particular example, with his agent and his weights, these little or 50 will create two separate nodes.",
                    "label": 0
                },
                {
                    "sent": "Other approach you can think of is the complex literals appreciated by CL, where we prepends the predicates to the literal value.",
                    "label": 0
                },
                {
                    "sent": "And one other approaches without literal, simply removing all the literals from the from the data set altogether and removing the predicate of course.",
                    "label": 0
                },
                {
                    "sent": "This will create a.",
                    "label": 0
                },
                {
                    "sent": "Quite smaller graph, obviously with less edges and nodes.",
                    "label": 0
                },
                {
                    "sent": "The final reward methods is quite different than the previous four.",
                    "label": 0
                },
                {
                    "sent": "Here we treat every triple as a single note and there's an edge between two nodes.",
                    "label": 0
                },
                {
                    "sent": "When they form a length of path two or in other words, when the object of one trip was the same as the subject of another.",
                    "label": 0
                },
                {
                    "sent": "This will create a lot of smaller graph and a more disconnected graph as well.",
                    "label": 0
                },
                {
                    "sent": "For instance, in this example you can see that we already have one orphan nodes coming up with no incoming and outgoing links.",
                    "label": 0
                },
                {
                    "sent": "So what we have when we have applied this network analysis, we have weights assigned to all these notes that we need to get from node ways with triple weights were interested in the relevance of triples.",
                    "label": 0
                },
                {
                    "sent": "So for the first 4 rewrite methods we have aggregation approach where we take the maximum weight of either the subject or object and assign data triple weight.",
                    "label": 0
                },
                {
                    "sent": "When we have this weighted list of triples, we can easily rank the triples by relevance.",
                    "label": 1
                },
                {
                    "sent": "And then create the simple select the best book game.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our evaluation.",
                    "label": 0
                },
                {
                    "sent": "For each data set we have a query set.",
                    "label": 0
                },
                {
                    "sent": "So for each query we actually we can execute it on the data set executed on sample, get the result set and see how many answers we would get on the sample compared to the original data set.",
                    "label": 0
                },
                {
                    "sent": "This is calculating Rico.",
                    "label": 0
                },
                {
                    "sent": "And we will do this for all the queries for each data set and the average recall for the queries would be the quality of that particular sample.",
                    "label": 0
                },
                {
                    "sent": "We do this for sample sizes one till 99 to cover the complete spectrum of sample size is an as there is no related and inform sampling approach for RDF.",
                    "label": 0
                },
                {
                    "sent": "We have two simple baselines.",
                    "label": 0
                },
                {
                    "sent": "When it's random frequency, we apply this 10 * 2 more alleviates the deterministic this problem and the other one is resource frequency where for each triple we count how often each resource occurs in there in the graph.",
                    "label": 0
                },
                {
                    "sent": "And the weight of the triple is the sum of all these frequencies.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's a problem here, because when we calculate Rico in the old fashioned way, namely executing this query on the sample and only original 1.",
                    "label": 0
                },
                {
                    "sent": "We will need to lose a lot of people so this formula shows calculates her number.",
                    "label": 0
                },
                {
                    "sent": "How much triples we would need to evaluate on certain datasets.",
                    "label": 0
                },
                {
                    "sent": "As input, the text to the number of samples of the data set, and then for each sample size 1 till 99 it multiplies this with a number of sampling methods, which is very bad.",
                    "label": 0
                },
                {
                    "sent": "Methods times the network analysis algorithms, so 15 and the number of baseline methods, random sample and the resource frequency which is 11 times the number of triples in the data set.",
                    "label": 0
                },
                {
                    "sent": "So in total for all the datasets we would need over 15,000 datasets including the samples over 1.4 trillion triples.",
                    "label": 0
                },
                {
                    "sent": "And this is obviously not easy to analyze.",
                    "label": 0
                },
                {
                    "sent": "So if we want to have this done in a fast fashion and we would really need powerful hardware and to load all the triples, execute the queries and calculate the recall.",
                    "label": 0
                },
                {
                    "sent": "It's just impossible.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need a scalable approach.",
                    "label": 0
                },
                {
                    "sent": "So our scalable process retreat, which triples are used by query and let me know which ships are used factory.",
                    "label": 0
                },
                {
                    "sent": "We can use outlook cluster to find the weights of these triples and analyze whether these triples would have been included in the symbol.",
                    "label": 0
                },
                {
                    "sent": "This is scalable, so we only execute is each query.",
                    "label": 0
                },
                {
                    "sent": "Once an we can do most of the parts on the Hadoop cluster.",
                    "label": 0
                },
                {
                    "sent": "So an example to illustrate this, we have a query select.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Person born in the capital of country.",
                    "label": 0
                },
                {
                    "sent": "We have a data set with five triples and we have for this particular query two rows in a result set.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we need to do to get that ripples used in this query would first need to rewrite the query to a distinct stars, so we get all the used variables in this query.",
                    "label": 0
                },
                {
                    "sent": "This would get you the right part of the table you see here, so we have loud on Samsung, Netherlands, Steven Berlin, Germany.",
                    "label": 0
                },
                {
                    "sent": "An if you replace the variables in the query with the values from the result set, you would actually get the triples needed to answer this query.",
                    "label": 0
                },
                {
                    "sent": "So here we will see on the left side the four triples needed to enter both rows in result set.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when we know these triples used by the query, we can easily calculate the recall.",
                    "label": 0
                },
                {
                    "sent": "So on the left side we have the data set, again sorted by weight.",
                    "label": 0
                },
                {
                    "sent": "If we would want to get a sample of 60%, we would need the top three triples and we can now easily on the right side the triples needed to answer the query.",
                    "label": 0
                },
                {
                    "sent": "We can check our these queries include it in a possible sample of 60%.",
                    "label": 0
                },
                {
                    "sent": "In this case, one of the roles would not be included.",
                    "label": 0
                },
                {
                    "sent": "Then we would get a recall of 50%.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this evaluation is a better specificity than regular Rico is scalable you we use big instead of sparkle.",
                    "label": 0
                },
                {
                    "sent": "There are some special cases or aggregates, groupby, limits distincts, but we want to talk to me about that too.",
                    "label": 0
                },
                {
                    "sent": "We should talk.",
                    "label": 0
                },
                {
                    "sent": "We should do that offline because.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With short on time, so these are the datasets actually discuss them, before which we would which we evaluate.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the results, so if you're looking at the online version, you can click on the forum or interactive version so we would see.",
                    "label": 0
                },
                {
                    "sent": "Here are the open Biome.",
                    "label": 0
                },
                {
                    "sent": "It's for instance, which has a recall effort Rico for the queries of 69% with a sample size of 6%.",
                    "label": 0
                },
                {
                    "sent": "So this does extremely well.",
                    "label": 0
                },
                {
                    "sent": "The Pedia has a more of a smooth curve, but we can already see that we have about a 50% recall for a sample size of 7%.",
                    "label": 0
                },
                {
                    "sent": "If you look at the small prints for their sets, you see that the best sampling method here difference between a data set.",
                    "label": 0
                },
                {
                    "sent": "This because there's an influence of both the creation used to evaluate the properties of the data sets in the sampling methods we use, and sadly, we cannot exhaustively find the correlations between all these features of the query set and data etc, because we just don't have enough courage to do this evaluation with.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some other small observations.",
                    "label": 0
                },
                {
                    "sent": "I'll skip this for for time reasons.",
                    "label": 0
                },
                {
                    "sent": "But the in general the message of this license is that again, there is a link between the Patch of queries.",
                    "label": 0
                },
                {
                    "sent": "We use the best performing sampling method and certain properties of the data set.",
                    "label": 0
                },
                {
                    "sent": "So are lots of literals described.",
                    "label": 0
                },
                {
                    "sent": "There are lots of resources instead of rituals described.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally the conclusion.",
                    "label": 0
                },
                {
                    "sent": "What we did here was represented scalable pipeline for for create examples.",
                    "label": 0
                },
                {
                    "sent": "So we apply network Ness algorithms combined with several rewrite methods.",
                    "label": 0
                },
                {
                    "sent": "An hour evaluation approach was pretty scalable as well, so we were able to evaluate over 15,000.",
                    "label": 0
                },
                {
                    "sent": "Virtual datasets, including the samples, of course, an consisting of 1.4 trillion triples.",
                    "label": 0
                },
                {
                    "sent": "But again, the number of queries sets we have is too limited to learn significant correlations between the influence of the query sets, data center sampling method.",
                    "label": 1
                },
                {
                    "sent": "But what it does show is that we can use the topology in the structure of the graph to determine good samples, and in essence this means that we can mimic semantic relevance through structural properties and all this without an uproar in notion of relevance.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}