{
    "id": "wwt2gnz4eznxyvofbz4pmwr2m2qplsrl",
    "title": "Fast Solvers and Efficient Implementations for Distance Metric Learning",
    "info": {
        "author": [
            "Kilian Q. Weinberger, Department of Computer Science, Cornell University"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_weinberger_fse/",
    "segmentation": [
        [
            "Thanks.",
            "This joint work me and actually Lawrence all."
        ],
        [
            "I'm I'm talking today about metric learning, and in particular my numbers, my numbers, metric learning.",
            "I think most of you are probably familiar with that is the minimum distance is basically just a generalization of the Euclidean distance where you squeeze in a matrix here into that vector inner product.",
            "And.",
            "That basically means that you take some dimensions and you squeeze them, or you can stretch them.",
            "So a unit circle and Euclidean distance turns into an ellipsoid."
        ],
        [
            "Amount of assistance.",
            "And that matrix here can't spend any matrix has to be a positive semidefinite matrix that ensures that under the square root of non negative value makes it well defined.",
            "And this talk really is about."
        ],
        [
            "How do we learn that matrix M?",
            "And."
        ],
        [
            "Particular, we want to learn a metric for specific task for K nearest neighbor classification.",
            "So in other words you want to try to adapt our metric to the specifics of the data set to get lower classification error.",
            "So here's for example, toy Data Set where we have a test point and we have two different classes of data points, and if you would just use Euclidean distances, this would clearly be.",
            "This will be the nearest neighbors would probably classify this as a soccer ball, but if you look at the at the specifics of the state of the structure and you can really see that this is most probably should be.",
            "Red ball and."
        ],
        [
            "A better metric would be something like this, where this point here is the nearest neighbor, so we try to find a metric and in which, which emphasizes the directions that are important for classification.",
            "And there's been a lot of previous work.",
            "I'm not going to go over this in detail."
        ],
        [
            "We will build on previous work that we published in 2006 that we called large margin nearest neighbor large margin nearest neighbors algorithm that learns a metric for K nearest neighbor classification.",
            "And by minimizing the leave one out classification error."
        ],
        [
            "Does that in the following way.",
            "The first thing you do is for every data point you have Contacts.",
            "I we fix a target neighbor XJ, so we assume that for every data point we know some data points, another data point which we would like to be the nearest neighbor.",
            "So if this guys was the nearest neighbor of that, then this point will be classified correctly and we want to find a metric.",
            "And that's that's the case."
        ],
        [
            "And in particular, we learn metric."
        ],
        [
            "Such that any other data point that is not of the same classes XI, for example here, this Guy XL should be further away than XJ than the target neighbor.",
            "So for every point you fix the target, maybe say that we want that to be the nearest neighbour and every other data point.",
            "That's a different class should be further away, and in fact it further."
        ],
        [
            "Way by a large margin and that gives us better generalization."
        ],
        [
            "And we show that we can take this intuition that phrase it as an optimization problem, in particular the constraint.",
            "We say the distance between Ixion exchanges between the data point is nearest in its target neighbor should be less equal.",
            "The distance to the data point in any other point that have different class.",
            "So this distance should be less than this distance."
        ],
        [
            "And the large margin we can incorporate here by saying this guy should be.",
            "It's going to be closer by at least one unit.",
            "And that's not always Poss."
        ],
        [
            "So we introduce Slack variables that absorb."
        ],
        [
            "The violation and in the objective we minimize the sum of all the violations and we also have a regularization term that basically says if you can't do anything, ultimately minimize the distance between these two points.",
            "That's a limitation."
        ],
        [
            "We have two more constraints.",
            "One just says the slack has to be non negative.",
            "That's a no brainer and we have the constraint that the matrix and has to be positive definite and that I mentioned that earlier on that just makes sure that the metric that we're learning is well defined.",
            "And so we published."
        ],
        [
            "1006 and we could show that this is a semidefinite program is a specific kind of optimization problem that's convex, so it's only global minima and very nicely behaved.",
            "So you solve this problem.",
            "You get a matrix M and that gives you monovisc distance that has nice properties for Canyon Theatre classification."
        ],
        [
            "Today we will go further.",
            "We will extend this work in three different ways and this was really motivated by questions that people asked."
        ],
        [
            "And so I face these questions.",
            "The first question is, well, that's a semidefinite program problem program that you're solving there, and those are not always that easy to solve.",
            "So how can you scale that to larger datasets?"
        ],
        [
            "And that's going to be the first talk.",
            "Second part of the talk will be doing Canius neighbor classification.",
            "Yeah, that's order N during test time.",
            "Can you make that faster?"
        ],
        [
            "And finally, you really care about classification accuracy.",
            "How can we get really, really high accuracy with this?",
            "I hope you get.",
            "How can we get higher accuracy with large numbers of patients so it's the third part of the talk."
        ],
        [
            "And as I don't have that much time, I decided to take one of these parts and make it'll skip over it a little bit and have the details in the post.",
            "And that's this part here, so I will only go over this very quick."
        ],
        [
            "Lee the question is how can we make large margin nearest neighbor fast the bottleneck of course, is the semidefinite program.",
            "Is this thing here?",
            "How can we solve that efficiently?",
            "And if you plug it into a standard, this planet packages that you can just solve the issue.",
            "And if you start plug that into a standard package you can solve datasets up to maybe 600 data points and that takes you maybe an hour, so it's not particularly efficient, but if you use the general purpose server, sorry specific purpose over the develop."
        ],
        [
            "Can do much better and.",
            "I'm not going to go through details here, but the details are on the paper.",
            "Basically discover three speedups that really lets you apply this to much, much larger data sets, and the first one is active set method, like in this particular problem, almost all of the constraints actually inactive anyway, so you don't actually have to monitor them all the time.",
            "Second thing is.",
            "When we put the constraints into your objective function as a standard trick in optimization theory you get a non linear optimization objective function.",
            "If you go down the gradient, you don't really have to recompute the gradient every single time.",
            "It's a very very cheap update that you can do every iteration to get the actual gradient, and finally have a very good initialization point.",
            "'cause I said the details here.",
            "Please come to the poster."
        ],
        [
            "I can only tell you I said earlier on the standard solver packages.",
            "You can go up to six on data points and it takes you like over an hour.",
            "We can do that in a few seconds and we can go even up to six 60,000 data points into that is 3 hours, so this results in the endless handwritten digit classification data set.",
            "And when we take the entire data set, we have 3.2 billion constraints that we have to satisfy, but only 540,000 of those had to be monitored every single iteration.",
            "And as I said, we can do is roughly in three hours and we get a classification error, one point 7%."
        ],
        [
            "So to answer the question that people ask us, how can you solve this?",
            "For larger, how can you apply elemental larger data set?",
            "The answer is really use the special purpose solver and the details are on the poster.",
            "OK, second question.",
            "Is how can we make large margin nearest neighbor classification faster?"
        ],
        [
            "And admin classification really is just K nearest neighbor classification with a different metric.",
            "So once you have the metric, it's just Kenya signification.",
            "So usually what do people do when they want to make a nice neighbor classification faster than you?"
        ],
        [
            "Something called ball trees and that's been studied very extensively in detail by Andrew Moore County and also John Langford.",
            "Melina Bagel Simmer have several papers on this, and the main idea is very simple.",
            "Actually, you take your data set and you build a data structure by just taking it in, dividing it into two, and fitting a sphere around each of the two parts.",
            "And then you keep doing that recursively.",
            "And so at the end of the entire data set."
        ],
        [
            "Small fears packaged up, and that allows you much faster nearest neighbor search because you can do pruning so you once when you do nearest neighbor search it goes through your data set and you try to find the closest neighbor.",
            "Let's say this is that you have some good guy."
        ],
        [
            "It's already, and now you find this.",
            "Some data points are inside of a sphere or ball, and if that sphere is further away than your best guess so far, then you know you don't have to look at any of these data points because they must be further away than this fear, so you."
        ],
        [
            "Just disregard all these points right away and prune it, and that's really where the money comes from.",
            "By about trees you can do very very aggressive pruning and can do much faster nearest neighbor search.",
            "So that sounds great.",
            "So why don't we just do this?",
            "The problem is poultries is they make a lot of sense on these low dimensional spaces but."
        ],
        [
            "You go a little higher.",
            "You answered the curse of dimensionality and it really bites you.",
            "So in two dimensions, for example, amazing them give you speed of factor 80 or something.",
            "If you intend dimensions, they still give you speed up factor 9, but this speed up goes down radically as number of dimensions increases, and if you actually go to something like 200 dimensions actually become smaller than brute and efficient brute force nearest neighbor search this year.",
            "The results, by the way on MNIST datasets will reduce the dimensionality with PCA.",
            "So all trees are great idea, but they only work in low dimensions.",
            "Most of the interesting data sets are usually much higher dimension, like much higher dimensional than 200 or so.",
            "So we can't really use batteries.",
            "We could use them, however, if we had a method to reduce."
        ],
        [
            "Dimensionality, and that's exactly what we're going to do."
        ],
        [
            "So I will now show you two different methods to different ways to reduce the dimensionality with large margin nearest neighbors.",
            "The first one we refer to as Element and SVD, and so very, very simple trick.",
            "Basically you take your data set.",
            "And the first thing you do is you run element and so you solve the SDP semidefinite program of element.",
            "And outcomes the matrix M. That's the map that defines the numbers metric.",
            "And now you take that matrix and then you decompose it is am equals LL squared.",
            "You can always do this because M is positive semidefinite and wise and positive definite because you have a constraint here.",
            "So you take that positive definite matrix M and you decompose it in L ^2.",
            "And now it's very easy way to show that actually the monovisc distance under M is the same thing as the Euclidean distance after the transformation X goes to Alex.",
            "So in other words, really what we're doing here.",
            "Just learning a linear transformation where the linear transformation here is defined by the square root of the matrix M. And the view from that angle we could actually do dimensionality reduction if that matrix L was rectangular, so would map the data into a low dimensional space.",
            "So if you do this decomposition, Alice squared, but we can very easily make it rectangular, just applying SVD.",
            "It's a very simple trick, so we first run element N. We get a matrix, decompose it into linear mapping and we take that matrix L and.",
            "Make it low rank with SVD and now we get a mapping into a low dimensional space.",
            "That's the."
        ],
        [
            "Algorithm.",
            "Second algorithm refer to as element and wrecked.",
            "So for element rectangular need.",
            "The idea is well, I just told you that we will be learning is a linear mapping.",
            "But we then use a cleaning distances, so we might as well reflect that an optimization problem.",
            "So optimization problem here.",
            "That's the element and optimization problem solve over M and we have all the distances everything in terms of M. But I just showed you the distance on the M and the amount of assistance is equivalent to the distance Euclidean distance after the linear transformation.",
            "So I might as well just substitute this and for every occurrence of this."
        ],
        [
            "Might as well do this.",
            "So I take all these occurrences of M and substituting the equivalent term of L and also get rid of the positive."
        ],
        [
            "The constraint and the resulting optimization problem is now basically the same thing, just in terms of L. And there's one other thing, is no longer convex, so we might have some local minima, but the global minimum is the same.",
            "That's great because now we are learning the linear transformation directly and I told you early on, but we really want to reduce the dimensionality is a low rank matrix cells or rectangular matrix L. So we can just inform."
        ],
        [
            "This with additional constraints, so we just add a constraint that says we solve the optimization problem and the linear mapping has to be rectangular.",
            "And.",
            "And the furthest to this element and rectange."
        ],
        [
            "OK, now we can look at how this does on classification task, so here's result of nystagmus is 100 digit classification data set.",
            "And the classification accuracy under the varying number of dimensions.",
            "And we have two baselines here.",
            "The Gray line is the classification accuracy in the original 350 dimensional space, and this is originally 748 dimensional.",
            "But there's a lot of dimensions that actually just zero.",
            "If you do PCA.",
            "So really it's kind of 50 dimensional.",
            "So if you do a nearest neighbor classification on the Euclidean distance, you just download the data set you communication.",
            "That's what you get 2.3% error.",
            "And if you want element N in that 350 dimensional space, would you get?",
            "Is this one point 7% error?",
            "And these lines here.",
            "So the key is never classification in the low dimensional space with we reduce the dimensionality with various methods, have PCA and special version of multicast LDA.",
            "But elements videon element and rectangular.",
            "And it's quite surprising, is element rectangular in 15 dimensions just only slightly worse than Euclidean distance in 350 dimensions, so we reduce the dimensionality of 350, two, 15 and get almost the same classification error is a very small price to pay, and here of course we can use ball trees.",
            "We have significant speedup as information she would actually be slower with boundaries, and if you were willing to sacrifice a little speed and go a little higher, dimensional can go 25 dimensions and you get a classification error of 1.76, that's.",
            "Almost the same as 1.7 billion, only slightly slightly worse than 1.72, so only slightly worse than element in the 350 dimensional space.",
            "But you're now in 25 dmen."
        ],
        [
            "So if you use poultries, it is significant speedup.",
            "In this little space."
        ],
        [
            "So to answer this question, how can we make element really fast?",
            "No test time.",
            "The trick is you learn rectangular matrix use element and wrecked.",
            "But basically now Maps you high dimensional input input data into a low dimensional space and now you can use poultries.",
            "OK, that was the second part of the talk.",
            "Now third part of the talk question."
        ],
        [
            "Make element really accurate.",
            "So lot of people really care about accuracy, and here's.",
            "There's a limitation to element."
        ],
        [
            "And that's that would be learning the man over distance and all this distance is really one global distance over the entire data set and you have to imagine for a lot of data sets as much more local details that might be relevant for the global transformation.",
            "You cannot account.",
            "For example, here's a really nasty data set that I constructed that really breaks any model with distance.",
            "So here I took two circles in sample points from them, two different classes, red points in the points and for every red point the closest point blue point for every blue point there were close points are red point.",
            "So if you do nearest neighbor classification you 100% error.",
            "The bomb and no linear transformation can solve this problem, because if you want to resolve this matter, you have to move the red points away from the blue points.",
            "But at every everywhere you have to move in a different direction.",
            "So here tried element and the errors you're showing this direction that tries to squeeze it, tries to try to stretch it, but doesn't help it on.",
            "You still get 100% error.",
            "But if you could do a simple trick."
        ],
        [
            "You could take this data set and divide it in three different positions and then a different metric in each of these partitions.",
            "You can actually very very easily solve this problem.",
            "Get 100% error.",
            "Because you can.",
            "Actually, you know stretch in different directions in each of these positions.",
            "So it seems like that's a good idea and a lot of data sets will probably have that.",
            "The decision boundary is not just in One Direction."
        ],
        [
            "So we just do this right.",
            "You could just do that.",
            "We take the data set, be somehow cluster the you know the data points, divide them into different positions, and then we just run element locally for each of these partitions on different metrics.",
            "Well, it's it's."
        ],
        [
            "That easy because.",
            "Imagine we have a test point and now we compute the distance to two different data points.",
            "But we have to compare to find the nearest neighbor.",
            "We have to compare these two distances, but these two distances were obtained with two different metrics, right?",
            "Here we use metric M2 and he will use metric M1.",
            "So really what we are comparing is apples and oranges, right?",
            "Two different metrics like Miles and kilometers or something.",
            "So we cannot do this.",
            "You cannot learn the metrics individually.",
            "However, if you're little careful, you can do it."
        ],
        [
            "And the trick really is.",
            "We divide the data set up and we learn different metrics, but we don't learn them independently.",
            "We learn them jointly in one optimization problem, and we constrain it to ensure that we can compare the different metrics."
        ],
        [
            "How do we do this?",
            "Trick is very similar to the element of directive.",
            "I just want you to focus on this constraint here.",
            "That's the same constraint that we had with element and we say this index index J is less equal the distance between X in Excel bexel is any other data point that has a different class.",
            "However, what we."
        ],
        [
            "Do we measure this distance here under the metric of XJ?",
            "And we say."
        ],
        [
            "So this distance here zynex L under the metric of XL.",
            "So we take different metrics in this optimization problem.",
            "This constraint and this makes sure that we can compare them, because it really says that these two points under the under each local metric we have this property that the point of the right class is closer than any other point."
        ],
        [
            "And it turns out that if we do this for the entire problem details on the paper, I don't want to go too much into the gory things.",
            "But this isn't semidefinite program.",
            "Again, slightly more complex semidefinite constraint.",
            "But again, it's a convex optimization problems.",
            "We just go down to the global minimum and we get a metric for every single one of the of the local patches.",
            "And."
        ],
        [
            "Now we don't have that problem anymore.",
            "Now if you take a test point, we still have to compare distances of two different metrics, but our constraints as if XI is the same class Excel the Nextel, it's going to be closer than X Ray even if you compare two different metrics.",
            "So we don't learn one global metric is not well defined metric, but we learn local metrics that can be compared, and that's all we need for Kenny's neighbor classification.",
            "OK, so I told you that element N is overly restricted by learning a global metric and that you should divide up the petition, the input data and learn local metrics and I showed you that we can do this in one joint optimization problem, But then only global minimum which gives us all the metrics in one go."
        ],
        [
            "What I didn't tell you is how to divide up how to divide up the input data.",
            "So we thought about various ways of doing this, and the first thing that came to mind when we just cluster it.",
            "We just use K means to cluster the data and he can see the classification error.",
            "As number of clusters increases, so one class is exactly the same as element and you just learn one.",
            "You have one class reunion, one global metric, as number of classes increases, you can see the classification error goes down is isolated from UCI Irvine, and then this data set only the first 10,000.",
            "The averages of 101 hundred runs because K means has local minima.",
            "So you can see the intuition really works.",
            "It really helps to have local metrics.",
            "But actually we don't even have to do the clustering."
        ],
        [
            "If you just take.",
            "One metric per class.",
            "We divide the data set by class by class.",
            "And so in case if I have 26 classes, we learn 26 metrics into Venice.",
            "You learn exactly 10 metrics, have 100 digits, is 10 different different digits then actually we do much better than any K means clustering.",
            "That makes a lot of sense.",
            "I can visualize this by the way.",
            "That's of course much nicer because you have to run a clustering algorithm like this.",
            "Division is already given to us with the training data."
        ],
        [
            "I can visualize this here.",
            "On the end, this data set here took 2 dimensional 2 dimensional projection of this data set and learned.",
            "Local metrics among the global optimization problem, and you can really see that each of them has a different.",
            "Different metrics like these are basically these are the unit circles under the local metrics and you can see the forms in different directions and that makes a lot of sense that you want to run metric per class because really the points of one class kind of share the same decision boundaries."
        ],
        [
            "So we evaluated this an compared against large margin nearest neighbor with just a single metric and here 5 different datasets like the endless handwritten digit classification data set.",
            "Here's the letters the two type classification datasets, 20 newsgroups, is text classification data set as well as spoken Bauer classification and yell faces.",
            "Face recognition data set.",
            "They are sorted according to size, so this is largest data set that's the smallest one we chose these datasets despite this is.",
            "The largest datasets that we had.",
            "And to compare principal component Alesis and then K nearest neighbors element metric in Kenya's neighbors and multiple metrics K nearest neighbors.",
            "And you can see on every single one of these datasets multiple metrics improved significantly over element.",
            "And so that we are really state of the."
        ],
        [
            "I'd be compared against multiclass SVM by Kramer and Singer 2001, and can see an endless letters and isolate.",
            "We get the same or slightly better classification results.",
            "On your face, as we do significant better.",
            "That's mostly though that's mostly due to the fact that on your face, that data set actually K nearest neighbors is just a good algorithm to run.",
            "The only data set where SVM is really, really much better than US 20 newsgroups.",
            "And that actually we have an excuse for this, at least, and the excuses.",
            "The 20 newsgroups is actually very high dimensional data set is 20,000 dimensional and to run our algorithm we had to reduce it to 200 principal components.",
            "We only use the top two opponents.",
            "This data.",
            "This number here was obtained with 200 dimensional input.",
            "But this number here was obtained with 20,000 dimensional input.",
            "So I assume that some of that difference comes from that."
        ],
        [
            "OK.",
            "So to answer the final question, how can we make element N more accurate?",
            "The answer is used.",
            "The multiple Myrtle metric versions, multiple local linear metrics and I showed you that you can do that in one convex optimization problem."
        ],
        [
            "OK, that concludes my talk.",
            "Summarize in some sense, we have three extensions for linen.",
            "Now scales to large datasets.",
            "I didn't tell you the details of the solver, but please come to the poster and I go into that.",
            "And they can be used in a special way to be very fast during test time.",
            "And if you use it with multiple metrics, this slightly slower during training, but you get state of the art classification accuracy."
        ],
        [
            "That's it, thank you.",
            "Find next to come up well.",
            "Questions.",
            "It's important that ranks.",
            "Sorry.",
            "Yes, you could do that.",
            "It didn't really.",
            "We tried that, of course, and it didn't reduce the rank too much.",
            "And it seems that actually just having a constraint is having a rank hardware constraints based convexity.",
            "Anne.",
            "I definitely mean of course was reliable, reliable in terms of rank, and we didn't really fall into local minima that often, but it seems like the better choice in that case.",
            "But yeah, that's definitely yeah.",
            "Questions.",
            "Like dimensions.",
            "25 years.",
            "Yeah you can.",
            "You can use that as an initialization.",
            "It will probably actually then converge very fast.",
            "Absolutely.",
            "That's a good point.",
            "Dinner.",
            "Yeah.",
            "Yeah.",
            "Yes, yes actually.",
            "So that's basically what that paper does.",
            "Hastings training five they do local LDA.",
            "So at doing test time to look at the local neighborhood.",
            "An iterative at EA and all that significantly worse.",
            "I mean, since then actually that means several papers that raise the bar over that paper.",
            "I mean, that was kind of the paper that started the direction, but yeah, I mean we compared against that and.",
            "All the days, that's much better."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "This joint work me and actually Lawrence all.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm I'm talking today about metric learning, and in particular my numbers, my numbers, metric learning.",
                    "label": 0
                },
                {
                    "sent": "I think most of you are probably familiar with that is the minimum distance is basically just a generalization of the Euclidean distance where you squeeze in a matrix here into that vector inner product.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That basically means that you take some dimensions and you squeeze them, or you can stretch them.",
                    "label": 0
                },
                {
                    "sent": "So a unit circle and Euclidean distance turns into an ellipsoid.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amount of assistance.",
                    "label": 0
                },
                {
                    "sent": "And that matrix here can't spend any matrix has to be a positive semidefinite matrix that ensures that under the square root of non negative value makes it well defined.",
                    "label": 0
                },
                {
                    "sent": "And this talk really is about.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we learn that matrix M?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Particular, we want to learn a metric for specific task for K nearest neighbor classification.",
                    "label": 0
                },
                {
                    "sent": "So in other words you want to try to adapt our metric to the specifics of the data set to get lower classification error.",
                    "label": 1
                },
                {
                    "sent": "So here's for example, toy Data Set where we have a test point and we have two different classes of data points, and if you would just use Euclidean distances, this would clearly be.",
                    "label": 0
                },
                {
                    "sent": "This will be the nearest neighbors would probably classify this as a soccer ball, but if you look at the at the specifics of the state of the structure and you can really see that this is most probably should be.",
                    "label": 0
                },
                {
                    "sent": "Red ball and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A better metric would be something like this, where this point here is the nearest neighbor, so we try to find a metric and in which, which emphasizes the directions that are important for classification.",
                    "label": 0
                },
                {
                    "sent": "And there's been a lot of previous work.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go over this in detail.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will build on previous work that we published in 2006 that we called large margin nearest neighbor large margin nearest neighbors algorithm that learns a metric for K nearest neighbor classification.",
                    "label": 0
                },
                {
                    "sent": "And by minimizing the leave one out classification error.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does that in the following way.",
                    "label": 0
                },
                {
                    "sent": "The first thing you do is for every data point you have Contacts.",
                    "label": 0
                },
                {
                    "sent": "I we fix a target neighbor XJ, so we assume that for every data point we know some data points, another data point which we would like to be the nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "So if this guys was the nearest neighbor of that, then this point will be classified correctly and we want to find a metric.",
                    "label": 0
                },
                {
                    "sent": "And that's that's the case.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in particular, we learn metric.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such that any other data point that is not of the same classes XI, for example here, this Guy XL should be further away than XJ than the target neighbor.",
                    "label": 0
                },
                {
                    "sent": "So for every point you fix the target, maybe say that we want that to be the nearest neighbour and every other data point.",
                    "label": 0
                },
                {
                    "sent": "That's a different class should be further away, and in fact it further.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way by a large margin and that gives us better generalization.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we show that we can take this intuition that phrase it as an optimization problem, in particular the constraint.",
                    "label": 0
                },
                {
                    "sent": "We say the distance between Ixion exchanges between the data point is nearest in its target neighbor should be less equal.",
                    "label": 0
                },
                {
                    "sent": "The distance to the data point in any other point that have different class.",
                    "label": 0
                },
                {
                    "sent": "So this distance should be less than this distance.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the large margin we can incorporate here by saying this guy should be.",
                    "label": 0
                },
                {
                    "sent": "It's going to be closer by at least one unit.",
                    "label": 0
                },
                {
                    "sent": "And that's not always Poss.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we introduce Slack variables that absorb.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The violation and in the objective we minimize the sum of all the violations and we also have a regularization term that basically says if you can't do anything, ultimately minimize the distance between these two points.",
                    "label": 0
                },
                {
                    "sent": "That's a limitation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have two more constraints.",
                    "label": 0
                },
                {
                    "sent": "One just says the slack has to be non negative.",
                    "label": 0
                },
                {
                    "sent": "That's a no brainer and we have the constraint that the matrix and has to be positive definite and that I mentioned that earlier on that just makes sure that the metric that we're learning is well defined.",
                    "label": 0
                },
                {
                    "sent": "And so we published.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1006 and we could show that this is a semidefinite program is a specific kind of optimization problem that's convex, so it's only global minima and very nicely behaved.",
                    "label": 0
                },
                {
                    "sent": "So you solve this problem.",
                    "label": 0
                },
                {
                    "sent": "You get a matrix M and that gives you monovisc distance that has nice properties for Canyon Theatre classification.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today we will go further.",
                    "label": 0
                },
                {
                    "sent": "We will extend this work in three different ways and this was really motivated by questions that people asked.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I face these questions.",
                    "label": 0
                },
                {
                    "sent": "The first question is, well, that's a semidefinite program problem program that you're solving there, and those are not always that easy to solve.",
                    "label": 0
                },
                {
                    "sent": "So how can you scale that to larger datasets?",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's going to be the first talk.",
                    "label": 0
                },
                {
                    "sent": "Second part of the talk will be doing Canius neighbor classification.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's order N during test time.",
                    "label": 0
                },
                {
                    "sent": "Can you make that faster?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, you really care about classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "How can we get really, really high accuracy with this?",
                    "label": 1
                },
                {
                    "sent": "I hope you get.",
                    "label": 0
                },
                {
                    "sent": "How can we get higher accuracy with large numbers of patients so it's the third part of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as I don't have that much time, I decided to take one of these parts and make it'll skip over it a little bit and have the details in the post.",
                    "label": 0
                },
                {
                    "sent": "And that's this part here, so I will only go over this very quick.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lee the question is how can we make large margin nearest neighbor fast the bottleneck of course, is the semidefinite program.",
                    "label": 0
                },
                {
                    "sent": "Is this thing here?",
                    "label": 0
                },
                {
                    "sent": "How can we solve that efficiently?",
                    "label": 0
                },
                {
                    "sent": "And if you plug it into a standard, this planet packages that you can just solve the issue.",
                    "label": 0
                },
                {
                    "sent": "And if you start plug that into a standard package you can solve datasets up to maybe 600 data points and that takes you maybe an hour, so it's not particularly efficient, but if you use the general purpose server, sorry specific purpose over the develop.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can do much better and.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through details here, but the details are on the paper.",
                    "label": 1
                },
                {
                    "sent": "Basically discover three speedups that really lets you apply this to much, much larger data sets, and the first one is active set method, like in this particular problem, almost all of the constraints actually inactive anyway, so you don't actually have to monitor them all the time.",
                    "label": 0
                },
                {
                    "sent": "Second thing is.",
                    "label": 0
                },
                {
                    "sent": "When we put the constraints into your objective function as a standard trick in optimization theory you get a non linear optimization objective function.",
                    "label": 0
                },
                {
                    "sent": "If you go down the gradient, you don't really have to recompute the gradient every single time.",
                    "label": 0
                },
                {
                    "sent": "It's a very very cheap update that you can do every iteration to get the actual gradient, and finally have a very good initialization point.",
                    "label": 0
                },
                {
                    "sent": "'cause I said the details here.",
                    "label": 0
                },
                {
                    "sent": "Please come to the poster.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can only tell you I said earlier on the standard solver packages.",
                    "label": 0
                },
                {
                    "sent": "You can go up to six on data points and it takes you like over an hour.",
                    "label": 0
                },
                {
                    "sent": "We can do that in a few seconds and we can go even up to six 60,000 data points into that is 3 hours, so this results in the endless handwritten digit classification data set.",
                    "label": 0
                },
                {
                    "sent": "And when we take the entire data set, we have 3.2 billion constraints that we have to satisfy, but only 540,000 of those had to be monitored every single iteration.",
                    "label": 0
                },
                {
                    "sent": "And as I said, we can do is roughly in three hours and we get a classification error, one point 7%.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to answer the question that people ask us, how can you solve this?",
                    "label": 0
                },
                {
                    "sent": "For larger, how can you apply elemental larger data set?",
                    "label": 1
                },
                {
                    "sent": "The answer is really use the special purpose solver and the details are on the poster.",
                    "label": 0
                },
                {
                    "sent": "OK, second question.",
                    "label": 1
                },
                {
                    "sent": "Is how can we make large margin nearest neighbor classification faster?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And admin classification really is just K nearest neighbor classification with a different metric.",
                    "label": 0
                },
                {
                    "sent": "So once you have the metric, it's just Kenya signification.",
                    "label": 0
                },
                {
                    "sent": "So usually what do people do when they want to make a nice neighbor classification faster than you?",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something called ball trees and that's been studied very extensively in detail by Andrew Moore County and also John Langford.",
                    "label": 0
                },
                {
                    "sent": "Melina Bagel Simmer have several papers on this, and the main idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "Actually, you take your data set and you build a data structure by just taking it in, dividing it into two, and fitting a sphere around each of the two parts.",
                    "label": 0
                },
                {
                    "sent": "And then you keep doing that recursively.",
                    "label": 0
                },
                {
                    "sent": "And so at the end of the entire data set.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Small fears packaged up, and that allows you much faster nearest neighbor search because you can do pruning so you once when you do nearest neighbor search it goes through your data set and you try to find the closest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Let's say this is that you have some good guy.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's already, and now you find this.",
                    "label": 0
                },
                {
                    "sent": "Some data points are inside of a sphere or ball, and if that sphere is further away than your best guess so far, then you know you don't have to look at any of these data points because they must be further away than this fear, so you.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just disregard all these points right away and prune it, and that's really where the money comes from.",
                    "label": 0
                },
                {
                    "sent": "By about trees you can do very very aggressive pruning and can do much faster nearest neighbor search.",
                    "label": 1
                },
                {
                    "sent": "So that sounds great.",
                    "label": 0
                },
                {
                    "sent": "So why don't we just do this?",
                    "label": 0
                },
                {
                    "sent": "The problem is poultries is they make a lot of sense on these low dimensional spaces but.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You go a little higher.",
                    "label": 0
                },
                {
                    "sent": "You answered the curse of dimensionality and it really bites you.",
                    "label": 0
                },
                {
                    "sent": "So in two dimensions, for example, amazing them give you speed of factor 80 or something.",
                    "label": 0
                },
                {
                    "sent": "If you intend dimensions, they still give you speed up factor 9, but this speed up goes down radically as number of dimensions increases, and if you actually go to something like 200 dimensions actually become smaller than brute and efficient brute force nearest neighbor search this year.",
                    "label": 0
                },
                {
                    "sent": "The results, by the way on MNIST datasets will reduce the dimensionality with PCA.",
                    "label": 0
                },
                {
                    "sent": "So all trees are great idea, but they only work in low dimensions.",
                    "label": 0
                },
                {
                    "sent": "Most of the interesting data sets are usually much higher dimension, like much higher dimensional than 200 or so.",
                    "label": 0
                },
                {
                    "sent": "So we can't really use batteries.",
                    "label": 0
                },
                {
                    "sent": "We could use them, however, if we had a method to reduce.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dimensionality, and that's exactly what we're going to do.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will now show you two different methods to different ways to reduce the dimensionality with large margin nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "The first one we refer to as Element and SVD, and so very, very simple trick.",
                    "label": 0
                },
                {
                    "sent": "Basically you take your data set.",
                    "label": 0
                },
                {
                    "sent": "And the first thing you do is you run element and so you solve the SDP semidefinite program of element.",
                    "label": 0
                },
                {
                    "sent": "And outcomes the matrix M. That's the map that defines the numbers metric.",
                    "label": 0
                },
                {
                    "sent": "And now you take that matrix and then you decompose it is am equals LL squared.",
                    "label": 0
                },
                {
                    "sent": "You can always do this because M is positive semidefinite and wise and positive definite because you have a constraint here.",
                    "label": 0
                },
                {
                    "sent": "So you take that positive definite matrix M and you decompose it in L ^2.",
                    "label": 0
                },
                {
                    "sent": "And now it's very easy way to show that actually the monovisc distance under M is the same thing as the Euclidean distance after the transformation X goes to Alex.",
                    "label": 0
                },
                {
                    "sent": "So in other words, really what we're doing here.",
                    "label": 0
                },
                {
                    "sent": "Just learning a linear transformation where the linear transformation here is defined by the square root of the matrix M. And the view from that angle we could actually do dimensionality reduction if that matrix L was rectangular, so would map the data into a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So if you do this decomposition, Alice squared, but we can very easily make it rectangular, just applying SVD.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple trick, so we first run element N. We get a matrix, decompose it into linear mapping and we take that matrix L and.",
                    "label": 0
                },
                {
                    "sent": "Make it low rank with SVD and now we get a mapping into a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "That's the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "Second algorithm refer to as element and wrecked.",
                    "label": 0
                },
                {
                    "sent": "So for element rectangular need.",
                    "label": 0
                },
                {
                    "sent": "The idea is well, I just told you that we will be learning is a linear mapping.",
                    "label": 0
                },
                {
                    "sent": "But we then use a cleaning distances, so we might as well reflect that an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So optimization problem here.",
                    "label": 0
                },
                {
                    "sent": "That's the element and optimization problem solve over M and we have all the distances everything in terms of M. But I just showed you the distance on the M and the amount of assistance is equivalent to the distance Euclidean distance after the linear transformation.",
                    "label": 0
                },
                {
                    "sent": "So I might as well just substitute this and for every occurrence of this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Might as well do this.",
                    "label": 0
                },
                {
                    "sent": "So I take all these occurrences of M and substituting the equivalent term of L and also get rid of the positive.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The constraint and the resulting optimization problem is now basically the same thing, just in terms of L. And there's one other thing, is no longer convex, so we might have some local minima, but the global minimum is the same.",
                    "label": 0
                },
                {
                    "sent": "That's great because now we are learning the linear transformation directly and I told you early on, but we really want to reduce the dimensionality is a low rank matrix cells or rectangular matrix L. So we can just inform.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This with additional constraints, so we just add a constraint that says we solve the optimization problem and the linear mapping has to be rectangular.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And the furthest to this element and rectange.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we can look at how this does on classification task, so here's result of nystagmus is 100 digit classification data set.",
                    "label": 0
                },
                {
                    "sent": "And the classification accuracy under the varying number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "And we have two baselines here.",
                    "label": 0
                },
                {
                    "sent": "The Gray line is the classification accuracy in the original 350 dimensional space, and this is originally 748 dimensional.",
                    "label": 0
                },
                {
                    "sent": "But there's a lot of dimensions that actually just zero.",
                    "label": 0
                },
                {
                    "sent": "If you do PCA.",
                    "label": 0
                },
                {
                    "sent": "So really it's kind of 50 dimensional.",
                    "label": 0
                },
                {
                    "sent": "So if you do a nearest neighbor classification on the Euclidean distance, you just download the data set you communication.",
                    "label": 0
                },
                {
                    "sent": "That's what you get 2.3% error.",
                    "label": 0
                },
                {
                    "sent": "And if you want element N in that 350 dimensional space, would you get?",
                    "label": 0
                },
                {
                    "sent": "Is this one point 7% error?",
                    "label": 0
                },
                {
                    "sent": "And these lines here.",
                    "label": 0
                },
                {
                    "sent": "So the key is never classification in the low dimensional space with we reduce the dimensionality with various methods, have PCA and special version of multicast LDA.",
                    "label": 0
                },
                {
                    "sent": "But elements videon element and rectangular.",
                    "label": 0
                },
                {
                    "sent": "And it's quite surprising, is element rectangular in 15 dimensions just only slightly worse than Euclidean distance in 350 dimensions, so we reduce the dimensionality of 350, two, 15 and get almost the same classification error is a very small price to pay, and here of course we can use ball trees.",
                    "label": 0
                },
                {
                    "sent": "We have significant speedup as information she would actually be slower with boundaries, and if you were willing to sacrifice a little speed and go a little higher, dimensional can go 25 dimensions and you get a classification error of 1.76, that's.",
                    "label": 0
                },
                {
                    "sent": "Almost the same as 1.7 billion, only slightly slightly worse than 1.72, so only slightly worse than element in the 350 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "But you're now in 25 dmen.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you use poultries, it is significant speedup.",
                    "label": 0
                },
                {
                    "sent": "In this little space.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to answer this question, how can we make element really fast?",
                    "label": 0
                },
                {
                    "sent": "No test time.",
                    "label": 0
                },
                {
                    "sent": "The trick is you learn rectangular matrix use element and wrecked.",
                    "label": 0
                },
                {
                    "sent": "But basically now Maps you high dimensional input input data into a low dimensional space and now you can use poultries.",
                    "label": 0
                },
                {
                    "sent": "OK, that was the second part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Now third part of the talk question.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Make element really accurate.",
                    "label": 0
                },
                {
                    "sent": "So lot of people really care about accuracy, and here's.",
                    "label": 0
                },
                {
                    "sent": "There's a limitation to element.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's that would be learning the man over distance and all this distance is really one global distance over the entire data set and you have to imagine for a lot of data sets as much more local details that might be relevant for the global transformation.",
                    "label": 0
                },
                {
                    "sent": "You cannot account.",
                    "label": 0
                },
                {
                    "sent": "For example, here's a really nasty data set that I constructed that really breaks any model with distance.",
                    "label": 0
                },
                {
                    "sent": "So here I took two circles in sample points from them, two different classes, red points in the points and for every red point the closest point blue point for every blue point there were close points are red point.",
                    "label": 0
                },
                {
                    "sent": "So if you do nearest neighbor classification you 100% error.",
                    "label": 0
                },
                {
                    "sent": "The bomb and no linear transformation can solve this problem, because if you want to resolve this matter, you have to move the red points away from the blue points.",
                    "label": 0
                },
                {
                    "sent": "But at every everywhere you have to move in a different direction.",
                    "label": 0
                },
                {
                    "sent": "So here tried element and the errors you're showing this direction that tries to squeeze it, tries to try to stretch it, but doesn't help it on.",
                    "label": 0
                },
                {
                    "sent": "You still get 100% error.",
                    "label": 0
                },
                {
                    "sent": "But if you could do a simple trick.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You could take this data set and divide it in three different positions and then a different metric in each of these partitions.",
                    "label": 0
                },
                {
                    "sent": "You can actually very very easily solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Get 100% error.",
                    "label": 0
                },
                {
                    "sent": "Because you can.",
                    "label": 0
                },
                {
                    "sent": "Actually, you know stretch in different directions in each of these positions.",
                    "label": 0
                },
                {
                    "sent": "So it seems like that's a good idea and a lot of data sets will probably have that.",
                    "label": 0
                },
                {
                    "sent": "The decision boundary is not just in One Direction.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we just do this right.",
                    "label": 0
                },
                {
                    "sent": "You could just do that.",
                    "label": 0
                },
                {
                    "sent": "We take the data set, be somehow cluster the you know the data points, divide them into different positions, and then we just run element locally for each of these partitions on different metrics.",
                    "label": 0
                },
                {
                    "sent": "Well, it's it's.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That easy because.",
                    "label": 0
                },
                {
                    "sent": "Imagine we have a test point and now we compute the distance to two different data points.",
                    "label": 0
                },
                {
                    "sent": "But we have to compare to find the nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "We have to compare these two distances, but these two distances were obtained with two different metrics, right?",
                    "label": 0
                },
                {
                    "sent": "Here we use metric M2 and he will use metric M1.",
                    "label": 0
                },
                {
                    "sent": "So really what we are comparing is apples and oranges, right?",
                    "label": 0
                },
                {
                    "sent": "Two different metrics like Miles and kilometers or something.",
                    "label": 0
                },
                {
                    "sent": "So we cannot do this.",
                    "label": 0
                },
                {
                    "sent": "You cannot learn the metrics individually.",
                    "label": 0
                },
                {
                    "sent": "However, if you're little careful, you can do it.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the trick really is.",
                    "label": 0
                },
                {
                    "sent": "We divide the data set up and we learn different metrics, but we don't learn them independently.",
                    "label": 0
                },
                {
                    "sent": "We learn them jointly in one optimization problem, and we constrain it to ensure that we can compare the different metrics.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we do this?",
                    "label": 0
                },
                {
                    "sent": "Trick is very similar to the element of directive.",
                    "label": 0
                },
                {
                    "sent": "I just want you to focus on this constraint here.",
                    "label": 0
                },
                {
                    "sent": "That's the same constraint that we had with element and we say this index index J is less equal the distance between X in Excel bexel is any other data point that has a different class.",
                    "label": 0
                },
                {
                    "sent": "However, what we.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do we measure this distance here under the metric of XJ?",
                    "label": 0
                },
                {
                    "sent": "And we say.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this distance here zynex L under the metric of XL.",
                    "label": 0
                },
                {
                    "sent": "So we take different metrics in this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "This constraint and this makes sure that we can compare them, because it really says that these two points under the under each local metric we have this property that the point of the right class is closer than any other point.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out that if we do this for the entire problem details on the paper, I don't want to go too much into the gory things.",
                    "label": 0
                },
                {
                    "sent": "But this isn't semidefinite program.",
                    "label": 0
                },
                {
                    "sent": "Again, slightly more complex semidefinite constraint.",
                    "label": 0
                },
                {
                    "sent": "But again, it's a convex optimization problems.",
                    "label": 0
                },
                {
                    "sent": "We just go down to the global minimum and we get a metric for every single one of the of the local patches.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we don't have that problem anymore.",
                    "label": 0
                },
                {
                    "sent": "Now if you take a test point, we still have to compare distances of two different metrics, but our constraints as if XI is the same class Excel the Nextel, it's going to be closer than X Ray even if you compare two different metrics.",
                    "label": 0
                },
                {
                    "sent": "So we don't learn one global metric is not well defined metric, but we learn local metrics that can be compared, and that's all we need for Kenny's neighbor classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so I told you that element N is overly restricted by learning a global metric and that you should divide up the petition, the input data and learn local metrics and I showed you that we can do this in one joint optimization problem, But then only global minimum which gives us all the metrics in one go.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I didn't tell you is how to divide up how to divide up the input data.",
                    "label": 0
                },
                {
                    "sent": "So we thought about various ways of doing this, and the first thing that came to mind when we just cluster it.",
                    "label": 0
                },
                {
                    "sent": "We just use K means to cluster the data and he can see the classification error.",
                    "label": 0
                },
                {
                    "sent": "As number of clusters increases, so one class is exactly the same as element and you just learn one.",
                    "label": 0
                },
                {
                    "sent": "You have one class reunion, one global metric, as number of classes increases, you can see the classification error goes down is isolated from UCI Irvine, and then this data set only the first 10,000.",
                    "label": 0
                },
                {
                    "sent": "The averages of 101 hundred runs because K means has local minima.",
                    "label": 0
                },
                {
                    "sent": "So you can see the intuition really works.",
                    "label": 0
                },
                {
                    "sent": "It really helps to have local metrics.",
                    "label": 0
                },
                {
                    "sent": "But actually we don't even have to do the clustering.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you just take.",
                    "label": 0
                },
                {
                    "sent": "One metric per class.",
                    "label": 0
                },
                {
                    "sent": "We divide the data set by class by class.",
                    "label": 0
                },
                {
                    "sent": "And so in case if I have 26 classes, we learn 26 metrics into Venice.",
                    "label": 0
                },
                {
                    "sent": "You learn exactly 10 metrics, have 100 digits, is 10 different different digits then actually we do much better than any K means clustering.",
                    "label": 0
                },
                {
                    "sent": "That makes a lot of sense.",
                    "label": 0
                },
                {
                    "sent": "I can visualize this by the way.",
                    "label": 0
                },
                {
                    "sent": "That's of course much nicer because you have to run a clustering algorithm like this.",
                    "label": 0
                },
                {
                    "sent": "Division is already given to us with the training data.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can visualize this here.",
                    "label": 0
                },
                {
                    "sent": "On the end, this data set here took 2 dimensional 2 dimensional projection of this data set and learned.",
                    "label": 0
                },
                {
                    "sent": "Local metrics among the global optimization problem, and you can really see that each of them has a different.",
                    "label": 0
                },
                {
                    "sent": "Different metrics like these are basically these are the unit circles under the local metrics and you can see the forms in different directions and that makes a lot of sense that you want to run metric per class because really the points of one class kind of share the same decision boundaries.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we evaluated this an compared against large margin nearest neighbor with just a single metric and here 5 different datasets like the endless handwritten digit classification data set.",
                    "label": 0
                },
                {
                    "sent": "Here's the letters the two type classification datasets, 20 newsgroups, is text classification data set as well as spoken Bauer classification and yell faces.",
                    "label": 0
                },
                {
                    "sent": "Face recognition data set.",
                    "label": 0
                },
                {
                    "sent": "They are sorted according to size, so this is largest data set that's the smallest one we chose these datasets despite this is.",
                    "label": 0
                },
                {
                    "sent": "The largest datasets that we had.",
                    "label": 0
                },
                {
                    "sent": "And to compare principal component Alesis and then K nearest neighbors element metric in Kenya's neighbors and multiple metrics K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "And you can see on every single one of these datasets multiple metrics improved significantly over element.",
                    "label": 0
                },
                {
                    "sent": "And so that we are really state of the.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd be compared against multiclass SVM by Kramer and Singer 2001, and can see an endless letters and isolate.",
                    "label": 0
                },
                {
                    "sent": "We get the same or slightly better classification results.",
                    "label": 0
                },
                {
                    "sent": "On your face, as we do significant better.",
                    "label": 0
                },
                {
                    "sent": "That's mostly though that's mostly due to the fact that on your face, that data set actually K nearest neighbors is just a good algorithm to run.",
                    "label": 0
                },
                {
                    "sent": "The only data set where SVM is really, really much better than US 20 newsgroups.",
                    "label": 0
                },
                {
                    "sent": "And that actually we have an excuse for this, at least, and the excuses.",
                    "label": 0
                },
                {
                    "sent": "The 20 newsgroups is actually very high dimensional data set is 20,000 dimensional and to run our algorithm we had to reduce it to 200 principal components.",
                    "label": 0
                },
                {
                    "sent": "We only use the top two opponents.",
                    "label": 0
                },
                {
                    "sent": "This data.",
                    "label": 0
                },
                {
                    "sent": "This number here was obtained with 200 dimensional input.",
                    "label": 0
                },
                {
                    "sent": "But this number here was obtained with 20,000 dimensional input.",
                    "label": 0
                },
                {
                    "sent": "So I assume that some of that difference comes from that.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So to answer the final question, how can we make element N more accurate?",
                    "label": 0
                },
                {
                    "sent": "The answer is used.",
                    "label": 0
                },
                {
                    "sent": "The multiple Myrtle metric versions, multiple local linear metrics and I showed you that you can do that in one convex optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "Summarize in some sense, we have three extensions for linen.",
                    "label": 0
                },
                {
                    "sent": "Now scales to large datasets.",
                    "label": 0
                },
                {
                    "sent": "I didn't tell you the details of the solver, but please come to the poster and I go into that.",
                    "label": 0
                },
                {
                    "sent": "And they can be used in a special way to be very fast during test time.",
                    "label": 0
                },
                {
                    "sent": "And if you use it with multiple metrics, this slightly slower during training, but you get state of the art classification accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it, thank you.",
                    "label": 0
                },
                {
                    "sent": "Find next to come up well.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "It's important that ranks.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes, you could do that.",
                    "label": 0
                },
                {
                    "sent": "It didn't really.",
                    "label": 0
                },
                {
                    "sent": "We tried that, of course, and it didn't reduce the rank too much.",
                    "label": 0
                },
                {
                    "sent": "And it seems that actually just having a constraint is having a rank hardware constraints based convexity.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I definitely mean of course was reliable, reliable in terms of rank, and we didn't really fall into local minima that often, but it seems like the better choice in that case.",
                    "label": 0
                },
                {
                    "sent": "But yeah, that's definitely yeah.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Like dimensions.",
                    "label": 0
                },
                {
                    "sent": "25 years.",
                    "label": 0
                },
                {
                    "sent": "Yeah you can.",
                    "label": 0
                },
                {
                    "sent": "You can use that as an initialization.",
                    "label": 0
                },
                {
                    "sent": "It will probably actually then converge very fast.",
                    "label": 0
                },
                {
                    "sent": "Absolutely.",
                    "label": 0
                },
                {
                    "sent": "That's a good point.",
                    "label": 0
                },
                {
                    "sent": "Dinner.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes actually.",
                    "label": 0
                },
                {
                    "sent": "So that's basically what that paper does.",
                    "label": 0
                },
                {
                    "sent": "Hastings training five they do local LDA.",
                    "label": 0
                },
                {
                    "sent": "So at doing test time to look at the local neighborhood.",
                    "label": 0
                },
                {
                    "sent": "An iterative at EA and all that significantly worse.",
                    "label": 0
                },
                {
                    "sent": "I mean, since then actually that means several papers that raise the bar over that paper.",
                    "label": 0
                },
                {
                    "sent": "I mean, that was kind of the paper that started the direction, but yeah, I mean we compared against that and.",
                    "label": 0
                },
                {
                    "sent": "All the days, that's much better.",
                    "label": 0
                }
            ]
        }
    }
}