{
    "id": "pzxblh34whk2khxp2erdnothggcqqx72",
    "title": "Order-Embeddings of Images and Language",
    "info": {
        "author": [
            "Ivan Vendrov, University of Toronto"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_vendrov_order_embeddings/",
    "segmentation": [
        [
            "And I'm going to talk about a new method for learning representations of images and language.",
            "Actually, it's a general method for learning ordered represent for learning hierarchical data, but I'm going to start by describing the specific task which inspired the method in this."
        ],
        [
            "SKZ semantic image search, which is basically given a database of images and a natural language query.",
            "We want to identify which images the query accurately describes and this is something that humans do very easily.",
            "We can readily see that the capture that the query refers to the image on the right, but not to the image on the left."
        ],
        [
            "So there's many approaches to this task going back decades.",
            "I'm going to talk about some of the later.",
            "Right now, I want to focus on this word, describes what exactly do we mean when we say a caption describes an image?",
            "More generally, what exactly is the relationship?"
        ],
        [
            "Between images and the language we use to describe them."
        ],
        [
            "Pandora Contention is that there was.",
            "This relationship is a partial order.",
            "We call this the visual semantic hierarchy.",
            "And the arrows here mean is a special case of.",
            "So woman is a special case of person.",
            "Woman skiing is a special case of woman and the image on the left is a special case of the concept woman skiing, right?",
            "So you have abstract concepts at the top and as you get down you get more and more detail.",
            "You get more and more detailed until you get to images which are incredibly detailed.",
            "An information dense.",
            "You could write a paragraph about any of these images.",
            "I know that most of the arrows here are missing because this relationship is transitive, right?",
            "So if a person is an entity and a woman is a person, then a woman is an entity.",
            "Further, this this view allows us to capture the fact that images have many different correct descriptions depending on the level of abstraction we want and the objects we choose to focus on for a task.",
            "Um Ann.",
            "Finally, this view allows us to Unite some common vision, vision, NLP tasks, right?",
            "So the edges at the top are instances of the hypernym relation, right person is a hypernym of woman.",
            "And the edges in the middle are instances of the entailment relation between phrases, so woman walking her dog entails woman.",
            "Walking entails person walking, etc.",
            "And finally the the edges.",
            "The bottom are instances of the description relation, which is what I asked about initially.",
            "So looking at things in this way, we see that each of these hypergamy entailment and description are partial orders.",
            "And what we're going to do is we're going to describe a general framework for learning partial orders from data that allows us to model each of these.",
            "And we're going to use the standard idea of learned embeddings.",
            "We're going to map each of these concepts, have point in vector space, and reduce hard semantic questions to simple geometry in our embedding space."
        ],
        [
            "So there's been a number of approaches to this, and most of the ones with learned representations fall in one one of two broad categories.",
            "So the first category is.",
            "You assume that the relation is approximately symmetric by using a symmetric similarity like cosine similarity in the embedding space and one prominent line of work.",
            "In this category is the visual semantic embeddings approach and the second category learns a basically unconstrained binary relation between.",
            "So an example of this is the neural tensor network of social.",
            "And so our idea is that since we know that the visual semantic hierarchy is a partial order and partial orders are a very restricted class of binary binary relations that are definitely not symmetric, we should be able to do better than either of these approaches."
        ],
        [
            "An answer what we do is we're going to impose, essentially impose a partial order prior by embedding into an ordered space.",
            "So what do I mean by what kind of order space are we talking about?",
            "So there are many possible choices here, and we basically pick the simplest ordered embedding space that seemed like it might work.",
            "So the space we pick is RN plus the space of all and dimensional vectors with non negative coordinates.",
            "And the order we pick as we see that the vector X will be below the vector Y if all of its coordinates are larger, and you'll see why we do this reversal in a second.",
            "So in two dimensions is looks something like this.",
            "So for X to be below Y in the partial order, it has to be somewhere in.",
            "The green region, right so above or to the right of Y, and the reason why we pick where we do have this reversal is because we want our order to have a single top element, which in this case is going to be the origin.",
            "So a small slice of this order.",
            "The order over this space is this, so we have the origin at the top.",
            "The unit vectors below that and on into Infinity in continuous space as well.",
            "So that's our order.",
            "That's our embedding space.",
            "How do we actually use it?",
            "Anne."
        ],
        [
            "Use it as follows.",
            "So we have our visual semantic hierarchy.",
            "In the left are ordered embedding space on the right and an embedding function F that Maps between them.",
            "And F is going to satisfy one simple property.",
            "It's going to be an order embedding, which means that every ordered pair in the input space is going to also be ordered in the embedding space and vice versa.",
            "So, for instance, person being at the top of this partial order is going to map somewhere near the origin in the embedding space, man and woman being below person in the partial order is going to be above into the right in the embedding space.",
            "And the images being below their respective captions in the partial order.",
            "Are going to be above and to the right in the embedding space.",
            "So now suppose we're given a new word like politician and we want to know what are the what's the relationship between politician and the other objects in our input domain?",
            "And So what we can do is we can use F2 embedded into this order domestic space and then we can look at the relationships inside that space, which are very easy to calculate, right?",
            "So since politician is above and to the right of person, it's below person in our partial order and by the order embedding property by the left arrow going left, we can infer that politician is a special case of person.",
            "And we can do a similar thing with the captions, inferring that.",
            "These two characters are indeed politicians.",
            "So basically, order embeddings allow you to infer semantic relationships in your domain of interest from a simple geometric property in your embedding space.",
            "The only piece we're missing here is, where do we actually get this F?",
            "How do we?",
            "How do we get in order?",
            "And what we're going to do is we're going to learn one using gradient methods.",
            "And so we're not actually getting forced.",
            "The order embedding property as a hard constraint.",
            "Instead, we're going to take a continuous relaxation of it, and we're going to find an approximate order embedding.",
            "So that's going to look something like this so."
        ],
        [
            "Make a trivial partial lawyer like Explo, why?",
            "And suppose embedding space starts off like this, right?",
            "So X should be inside the green region, but it's actually some distance away from it.",
            "And this is an order violation, right?",
            "So it violates the order embedding property.",
            "So what we do is we define an order violation error which is just going to be the length of the red line squared.",
            "And the equation looks like this.",
            "And we can minimize this.",
            "This order violation error with gradient descent, and as we minimize it, the embedding of X will slide to the right and the embedding of Y will slide to the left until."
        ],
        [
            "Until things become correctly ordered and F becomes an order of things.",
            "So this last bit."
        ],
        [
            "The order violation error E is actually the key now."
        ],
        [
            "Of our method, we use it in the exact same way that previous work uses like cosine similarity or by linear comparison operator.",
            "So whenever you're learning a relation that happens to be a partial order, you can just try plugging missing one line of code.",
            "OK, so that's the general method."
        ],
        [
            "And now I'm going to describe some experiments."
        ],
        [
            "So if we go back to this slide right, we apply it to each of these relations intern.",
            "I put entailment in description in this talk.",
            "I'm only going to have time to dive deep into description if you want to find out more about how we did pretty entailment you can come to our poster.",
            "OK, so we're just going to focus on description, which is just a two level partial order between captions and images."
        ],
        [
            "And I'm going to go back to semantic image search.",
            "And going, I'm going to explain a standard state of the art method and show how we can retrofit it to use our order violation error.",
            "So so basically the method is as follows.",
            "We were given a data set of caption image pairs and we're going to learn a caption image similarity S, which measures how well a caption C describes an image I.",
            "And the way we're going to learn is going to minimize the pairwise ranking objective.",
            "Show us something like this and this basically forces the similarity for a ground truth Image caption pair to be higher than the similarity.",
            "If we replace the caption with a random one.",
            "And the second term is if is.",
            "If we replace the image with a random one.",
            "And the similarity is the inner product between the caption embedding and the image embedding the caption embedding is typically just the output of an RNN encoder.",
            "And the image embedding is a pre trained CNN followed by a learned linear projection.",
            "And so this is the kind of the standard visual semantic embedding approach, and the only thing we're going to change here is how we compute the similarity.",
            "So instead of computing it as the inner product of the embeddings.",
            "We're going to compute as the negative order violation error we defined earlier.",
            "So essentially instead of forcing the caption and image embedding to be close in the in the embedding space, we're going to make the image embedding be underneath the caption embedding.",
            "And the other thing we changed since we only use non negative coordinates is we're going to put absolute value signs around the caption image.",
            "OK, so we thought now we're going to see how we're going to evaluate.",
            "How those changes affect things?"
        ],
        [
            "And we're going to use the standard evaluation in the literature.",
            "The Microsoft Coco Ranking benchmark, which has 120,000 images.",
            "Each image has five human captions.",
            "We use the standard splits.",
            "And if you've not familiar with."
        ],
        [
            "So he is just a random example image and find the five human captions and you can see the captions vary fairly widely in, like in their length and what objects they choose to focus on."
        ],
        [
            "So the evaluation we're going to do is image search.",
            "We're going to take each caption from the test set, and we're going to rank all the test images in order of decreasing similarity or increasing order violation error.",
            "And we're going to.",
            "We're going to look at how high the ground truth image is in this order.",
            "And we're going to measure the standard.",
            "Retrieval metrics recall at K and then the mean and median rank of the first ground truth image."
        ],
        [
            "So here are some.",
            "Here are the results.",
            "The results for the state of the art models as of November 2015.",
            "The last model M LM is the most direct comparison because it's exactly our model but with cosine similarity instead of order violation error.",
            "And so how?"
        ],
        [
            "Order meetings do.",
            "They do significantly better than the state of the art at the time, and we can, and we can attribute this improvement almost entirely to the order violation error because it's the only thing we changed."
        ],
        [
            "I should say this is just a small slice of the results.",
            "We evaluate many more metrics and many more baselines to make sure really isolate the effect of the order violation error and.",
            "But the conclusion stayed the same."
        ],
        [
            "So here are a couple of examples.",
            "Of successful cases where we correctly rank the ground truth image first out of 1000.",
            "And here's."
        ],
        [
            "Your case.",
            "Which basically shows that our model can't count because the Captain describes 3 hot dogs at the same time and the all the images involve one or two at most."
        ],
        [
            "The ground truth image by the."
        ],
        [
            "Is this lovely image?",
            "OK, so."
        ],
        [
            "Can we get some idea of what our learned embeddings actually look like?",
            "It's hard to visualize 1000 dimensional space, especially if it's ordered, but we can get some insight by considering what sort of regularity's these embeddings satisfied.",
            "So, famously, embeddings learn using word to back satisfy this sort of regularity, right?",
            "King Minos man Plus Woman is approximately equal to Queen.",
            "We certainly don't expect this to hold in our embedding space, but we do expect two operations to maybe have some interesting semantic properties."
        ],
        [
            "These operations are elementwise Max and Elementwise Min.",
            "And to see why that is, consider the following diagram.",
            "And suppose we have the embeddings of Apple and Banana.",
            "And so where would the min of these two vectors be?",
            "Right there will be somewhere here.",
            "And what would it be would be in semantic meaning when are partial order, the men will be above both Apple and banana.",
            "And so it would have to be something that both Apple and Banana have in common, which is probably the fact that they are fruit.",
            "And similarly, where?",
            "Where does the Max in bed too?",
            "It's probably around there and the since the Max is below both Apple and banana in the partial order is probably some concept that includes both Apple and Banana in it, which is probably just going to be trivial.",
            "Phrase Apple and banana.",
            "OK, so this is just a.",
            "This is just a 2 dimensional image to dimensional theory.",
            "Let's see how this works in practice."
        ],
        [
            "What we're going to do is we're going to embed the word man in the word cat into a using our learned order embedding.",
            "We're going to take the Max and then we're going to look at the nearest images in the Co training set.",
            "Yeah, thanks.",
            "V and the nearest images, and indeed the nearest images here are all men with cats.",
            "And similarly, if we do the Max of a black dog in a park, we all."
        ],
        [
            "And they all the nearest images are all black dogs in parks.",
            "We can do something."
        ],
        [
            "More interesting and apply these operations to images.",
            "Answer As predicted, the Max of Apple and a banana are images of both.",
            "And then the second one is kind of interesting, so it's the minimum of there's a first image on the left has two elephants and a person on the horse and the 2nd.",
            "The second image has a person, a horse in a dog, and what are these two images have in common?",
            "People and horses, right?",
            "So all the images contain people and horses and no dogs and no elephants insight.",
            "And similarly you can blend images and captions.",
            "As you know, we didn't.",
            "We didn't train the model to do any of this sort of reasoning, right.",
            "We trained on raw images and captions.",
            "This is just something you get for free when you use an order demanding space."
        ],
        [
            "So to conclude.",
            "The relationship between images and language forms a partial order.",
            "And whenever we have a partial order, we can efficiently learn them using order preserving mappings, order embeddings between the domain and an ordered vector space.",
            "And if you're looking for a place to start, our code is available on GitHub.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm going to talk about a new method for learning representations of images and language.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's a general method for learning ordered represent for learning hierarchical data, but I'm going to start by describing the specific task which inspired the method in this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "SKZ semantic image search, which is basically given a database of images and a natural language query.",
                    "label": 1
                },
                {
                    "sent": "We want to identify which images the query accurately describes and this is something that humans do very easily.",
                    "label": 0
                },
                {
                    "sent": "We can readily see that the capture that the query refers to the image on the right, but not to the image on the left.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's many approaches to this task going back decades.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about some of the later.",
                    "label": 0
                },
                {
                    "sent": "Right now, I want to focus on this word, describes what exactly do we mean when we say a caption describes an image?",
                    "label": 0
                },
                {
                    "sent": "More generally, what exactly is the relationship?",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between images and the language we use to describe them.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pandora Contention is that there was.",
                    "label": 0
                },
                {
                    "sent": "This relationship is a partial order.",
                    "label": 0
                },
                {
                    "sent": "We call this the visual semantic hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And the arrows here mean is a special case of.",
                    "label": 0
                },
                {
                    "sent": "So woman is a special case of person.",
                    "label": 0
                },
                {
                    "sent": "Woman skiing is a special case of woman and the image on the left is a special case of the concept woman skiing, right?",
                    "label": 0
                },
                {
                    "sent": "So you have abstract concepts at the top and as you get down you get more and more detail.",
                    "label": 0
                },
                {
                    "sent": "You get more and more detailed until you get to images which are incredibly detailed.",
                    "label": 0
                },
                {
                    "sent": "An information dense.",
                    "label": 0
                },
                {
                    "sent": "You could write a paragraph about any of these images.",
                    "label": 0
                },
                {
                    "sent": "I know that most of the arrows here are missing because this relationship is transitive, right?",
                    "label": 0
                },
                {
                    "sent": "So if a person is an entity and a woman is a person, then a woman is an entity.",
                    "label": 0
                },
                {
                    "sent": "Further, this this view allows us to capture the fact that images have many different correct descriptions depending on the level of abstraction we want and the objects we choose to focus on for a task.",
                    "label": 0
                },
                {
                    "sent": "Um Ann.",
                    "label": 0
                },
                {
                    "sent": "Finally, this view allows us to Unite some common vision, vision, NLP tasks, right?",
                    "label": 0
                },
                {
                    "sent": "So the edges at the top are instances of the hypernym relation, right person is a hypernym of woman.",
                    "label": 0
                },
                {
                    "sent": "And the edges in the middle are instances of the entailment relation between phrases, so woman walking her dog entails woman.",
                    "label": 1
                },
                {
                    "sent": "Walking entails person walking, etc.",
                    "label": 0
                },
                {
                    "sent": "And finally the the edges.",
                    "label": 0
                },
                {
                    "sent": "The bottom are instances of the description relation, which is what I asked about initially.",
                    "label": 0
                },
                {
                    "sent": "So looking at things in this way, we see that each of these hypergamy entailment and description are partial orders.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do is we're going to describe a general framework for learning partial orders from data that allows us to model each of these.",
                    "label": 0
                },
                {
                    "sent": "And we're going to use the standard idea of learned embeddings.",
                    "label": 0
                },
                {
                    "sent": "We're going to map each of these concepts, have point in vector space, and reduce hard semantic questions to simple geometry in our embedding space.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's been a number of approaches to this, and most of the ones with learned representations fall in one one of two broad categories.",
                    "label": 0
                },
                {
                    "sent": "So the first category is.",
                    "label": 0
                },
                {
                    "sent": "You assume that the relation is approximately symmetric by using a symmetric similarity like cosine similarity in the embedding space and one prominent line of work.",
                    "label": 1
                },
                {
                    "sent": "In this category is the visual semantic embeddings approach and the second category learns a basically unconstrained binary relation between.",
                    "label": 0
                },
                {
                    "sent": "So an example of this is the neural tensor network of social.",
                    "label": 0
                },
                {
                    "sent": "And so our idea is that since we know that the visual semantic hierarchy is a partial order and partial orders are a very restricted class of binary binary relations that are definitely not symmetric, we should be able to do better than either of these approaches.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An answer what we do is we're going to impose, essentially impose a partial order prior by embedding into an ordered space.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by what kind of order space are we talking about?",
                    "label": 0
                },
                {
                    "sent": "So there are many possible choices here, and we basically pick the simplest ordered embedding space that seemed like it might work.",
                    "label": 1
                },
                {
                    "sent": "So the space we pick is RN plus the space of all and dimensional vectors with non negative coordinates.",
                    "label": 0
                },
                {
                    "sent": "And the order we pick as we see that the vector X will be below the vector Y if all of its coordinates are larger, and you'll see why we do this reversal in a second.",
                    "label": 0
                },
                {
                    "sent": "So in two dimensions is looks something like this.",
                    "label": 0
                },
                {
                    "sent": "So for X to be below Y in the partial order, it has to be somewhere in.",
                    "label": 0
                },
                {
                    "sent": "The green region, right so above or to the right of Y, and the reason why we pick where we do have this reversal is because we want our order to have a single top element, which in this case is going to be the origin.",
                    "label": 0
                },
                {
                    "sent": "So a small slice of this order.",
                    "label": 0
                },
                {
                    "sent": "The order over this space is this, so we have the origin at the top.",
                    "label": 0
                },
                {
                    "sent": "The unit vectors below that and on into Infinity in continuous space as well.",
                    "label": 0
                },
                {
                    "sent": "So that's our order.",
                    "label": 0
                },
                {
                    "sent": "That's our embedding space.",
                    "label": 0
                },
                {
                    "sent": "How do we actually use it?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use it as follows.",
                    "label": 0
                },
                {
                    "sent": "So we have our visual semantic hierarchy.",
                    "label": 0
                },
                {
                    "sent": "In the left are ordered embedding space on the right and an embedding function F that Maps between them.",
                    "label": 0
                },
                {
                    "sent": "And F is going to satisfy one simple property.",
                    "label": 0
                },
                {
                    "sent": "It's going to be an order embedding, which means that every ordered pair in the input space is going to also be ordered in the embedding space and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So, for instance, person being at the top of this partial order is going to map somewhere near the origin in the embedding space, man and woman being below person in the partial order is going to be above into the right in the embedding space.",
                    "label": 0
                },
                {
                    "sent": "And the images being below their respective captions in the partial order.",
                    "label": 0
                },
                {
                    "sent": "Are going to be above and to the right in the embedding space.",
                    "label": 0
                },
                {
                    "sent": "So now suppose we're given a new word like politician and we want to know what are the what's the relationship between politician and the other objects in our input domain?",
                    "label": 0
                },
                {
                    "sent": "And So what we can do is we can use F2 embedded into this order domestic space and then we can look at the relationships inside that space, which are very easy to calculate, right?",
                    "label": 0
                },
                {
                    "sent": "So since politician is above and to the right of person, it's below person in our partial order and by the order embedding property by the left arrow going left, we can infer that politician is a special case of person.",
                    "label": 0
                },
                {
                    "sent": "And we can do a similar thing with the captions, inferring that.",
                    "label": 0
                },
                {
                    "sent": "These two characters are indeed politicians.",
                    "label": 0
                },
                {
                    "sent": "So basically, order embeddings allow you to infer semantic relationships in your domain of interest from a simple geometric property in your embedding space.",
                    "label": 0
                },
                {
                    "sent": "The only piece we're missing here is, where do we actually get this F?",
                    "label": 0
                },
                {
                    "sent": "How do we?",
                    "label": 0
                },
                {
                    "sent": "How do we get in order?",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do is we're going to learn one using gradient methods.",
                    "label": 0
                },
                {
                    "sent": "And so we're not actually getting forced.",
                    "label": 0
                },
                {
                    "sent": "The order embedding property as a hard constraint.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're going to take a continuous relaxation of it, and we're going to find an approximate order embedding.",
                    "label": 0
                },
                {
                    "sent": "So that's going to look something like this so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make a trivial partial lawyer like Explo, why?",
                    "label": 0
                },
                {
                    "sent": "And suppose embedding space starts off like this, right?",
                    "label": 0
                },
                {
                    "sent": "So X should be inside the green region, but it's actually some distance away from it.",
                    "label": 0
                },
                {
                    "sent": "And this is an order violation, right?",
                    "label": 0
                },
                {
                    "sent": "So it violates the order embedding property.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we define an order violation error which is just going to be the length of the red line squared.",
                    "label": 1
                },
                {
                    "sent": "And the equation looks like this.",
                    "label": 0
                },
                {
                    "sent": "And we can minimize this.",
                    "label": 0
                },
                {
                    "sent": "This order violation error with gradient descent, and as we minimize it, the embedding of X will slide to the right and the embedding of Y will slide to the left until.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until things become correctly ordered and F becomes an order of things.",
                    "label": 0
                },
                {
                    "sent": "So this last bit.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The order violation error E is actually the key now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of our method, we use it in the exact same way that previous work uses like cosine similarity or by linear comparison operator.",
                    "label": 0
                },
                {
                    "sent": "So whenever you're learning a relation that happens to be a partial order, you can just try plugging missing one line of code.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the general method.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now I'm going to describe some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we go back to this slide right, we apply it to each of these relations intern.",
                    "label": 0
                },
                {
                    "sent": "I put entailment in description in this talk.",
                    "label": 0
                },
                {
                    "sent": "I'm only going to have time to dive deep into description if you want to find out more about how we did pretty entailment you can come to our poster.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're just going to focus on description, which is just a two level partial order between captions and images.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'm going to go back to semantic image search.",
                    "label": 1
                },
                {
                    "sent": "And going, I'm going to explain a standard state of the art method and show how we can retrofit it to use our order violation error.",
                    "label": 0
                },
                {
                    "sent": "So so basically the method is as follows.",
                    "label": 1
                },
                {
                    "sent": "We were given a data set of caption image pairs and we're going to learn a caption image similarity S, which measures how well a caption C describes an image I.",
                    "label": 0
                },
                {
                    "sent": "And the way we're going to learn is going to minimize the pairwise ranking objective.",
                    "label": 1
                },
                {
                    "sent": "Show us something like this and this basically forces the similarity for a ground truth Image caption pair to be higher than the similarity.",
                    "label": 0
                },
                {
                    "sent": "If we replace the caption with a random one.",
                    "label": 0
                },
                {
                    "sent": "And the second term is if is.",
                    "label": 0
                },
                {
                    "sent": "If we replace the image with a random one.",
                    "label": 0
                },
                {
                    "sent": "And the similarity is the inner product between the caption embedding and the image embedding the caption embedding is typically just the output of an RNN encoder.",
                    "label": 0
                },
                {
                    "sent": "And the image embedding is a pre trained CNN followed by a learned linear projection.",
                    "label": 0
                },
                {
                    "sent": "And so this is the kind of the standard visual semantic embedding approach, and the only thing we're going to change here is how we compute the similarity.",
                    "label": 0
                },
                {
                    "sent": "So instead of computing it as the inner product of the embeddings.",
                    "label": 0
                },
                {
                    "sent": "We're going to compute as the negative order violation error we defined earlier.",
                    "label": 0
                },
                {
                    "sent": "So essentially instead of forcing the caption and image embedding to be close in the in the embedding space, we're going to make the image embedding be underneath the caption embedding.",
                    "label": 0
                },
                {
                    "sent": "And the other thing we changed since we only use non negative coordinates is we're going to put absolute value signs around the caption image.",
                    "label": 0
                },
                {
                    "sent": "OK, so we thought now we're going to see how we're going to evaluate.",
                    "label": 0
                },
                {
                    "sent": "How those changes affect things?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're going to use the standard evaluation in the literature.",
                    "label": 0
                },
                {
                    "sent": "The Microsoft Coco Ranking benchmark, which has 120,000 images.",
                    "label": 0
                },
                {
                    "sent": "Each image has five human captions.",
                    "label": 1
                },
                {
                    "sent": "We use the standard splits.",
                    "label": 0
                },
                {
                    "sent": "And if you've not familiar with.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So he is just a random example image and find the five human captions and you can see the captions vary fairly widely in, like in their length and what objects they choose to focus on.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the evaluation we're going to do is image search.",
                    "label": 0
                },
                {
                    "sent": "We're going to take each caption from the test set, and we're going to rank all the test images in order of decreasing similarity or increasing order violation error.",
                    "label": 1
                },
                {
                    "sent": "And we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at how high the ground truth image is in this order.",
                    "label": 0
                },
                {
                    "sent": "And we're going to measure the standard.",
                    "label": 1
                },
                {
                    "sent": "Retrieval metrics recall at K and then the mean and median rank of the first ground truth image.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some.",
                    "label": 0
                },
                {
                    "sent": "Here are the results.",
                    "label": 0
                },
                {
                    "sent": "The results for the state of the art models as of November 2015.",
                    "label": 0
                },
                {
                    "sent": "The last model M LM is the most direct comparison because it's exactly our model but with cosine similarity instead of order violation error.",
                    "label": 0
                },
                {
                    "sent": "And so how?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Order meetings do.",
                    "label": 0
                },
                {
                    "sent": "They do significantly better than the state of the art at the time, and we can, and we can attribute this improvement almost entirely to the order violation error because it's the only thing we changed.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I should say this is just a small slice of the results.",
                    "label": 0
                },
                {
                    "sent": "We evaluate many more metrics and many more baselines to make sure really isolate the effect of the order violation error and.",
                    "label": 0
                },
                {
                    "sent": "But the conclusion stayed the same.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are a couple of examples.",
                    "label": 0
                },
                {
                    "sent": "Of successful cases where we correctly rank the ground truth image first out of 1000.",
                    "label": 0
                },
                {
                    "sent": "And here's.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your case.",
                    "label": 0
                },
                {
                    "sent": "Which basically shows that our model can't count because the Captain describes 3 hot dogs at the same time and the all the images involve one or two at most.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ground truth image by the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this lovely image?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can we get some idea of what our learned embeddings actually look like?",
                    "label": 0
                },
                {
                    "sent": "It's hard to visualize 1000 dimensional space, especially if it's ordered, but we can get some insight by considering what sort of regularity's these embeddings satisfied.",
                    "label": 0
                },
                {
                    "sent": "So, famously, embeddings learn using word to back satisfy this sort of regularity, right?",
                    "label": 0
                },
                {
                    "sent": "King Minos man Plus Woman is approximately equal to Queen.",
                    "label": 0
                },
                {
                    "sent": "We certainly don't expect this to hold in our embedding space, but we do expect two operations to maybe have some interesting semantic properties.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These operations are elementwise Max and Elementwise Min.",
                    "label": 0
                },
                {
                    "sent": "And to see why that is, consider the following diagram.",
                    "label": 0
                },
                {
                    "sent": "And suppose we have the embeddings of Apple and Banana.",
                    "label": 1
                },
                {
                    "sent": "And so where would the min of these two vectors be?",
                    "label": 0
                },
                {
                    "sent": "Right there will be somewhere here.",
                    "label": 0
                },
                {
                    "sent": "And what would it be would be in semantic meaning when are partial order, the men will be above both Apple and banana.",
                    "label": 0
                },
                {
                    "sent": "And so it would have to be something that both Apple and Banana have in common, which is probably the fact that they are fruit.",
                    "label": 0
                },
                {
                    "sent": "And similarly, where?",
                    "label": 0
                },
                {
                    "sent": "Where does the Max in bed too?",
                    "label": 0
                },
                {
                    "sent": "It's probably around there and the since the Max is below both Apple and banana in the partial order is probably some concept that includes both Apple and Banana in it, which is probably just going to be trivial.",
                    "label": 0
                },
                {
                    "sent": "Phrase Apple and banana.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just a.",
                    "label": 0
                },
                {
                    "sent": "This is just a 2 dimensional image to dimensional theory.",
                    "label": 0
                },
                {
                    "sent": "Let's see how this works in practice.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're going to do is we're going to embed the word man in the word cat into a using our learned order embedding.",
                    "label": 0
                },
                {
                    "sent": "We're going to take the Max and then we're going to look at the nearest images in the Co training set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks.",
                    "label": 0
                },
                {
                    "sent": "V and the nearest images, and indeed the nearest images here are all men with cats.",
                    "label": 0
                },
                {
                    "sent": "And similarly, if we do the Max of a black dog in a park, we all.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they all the nearest images are all black dogs in parks.",
                    "label": 0
                },
                {
                    "sent": "We can do something.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More interesting and apply these operations to images.",
                    "label": 0
                },
                {
                    "sent": "Answer As predicted, the Max of Apple and a banana are images of both.",
                    "label": 0
                },
                {
                    "sent": "And then the second one is kind of interesting, so it's the minimum of there's a first image on the left has two elephants and a person on the horse and the 2nd.",
                    "label": 0
                },
                {
                    "sent": "The second image has a person, a horse in a dog, and what are these two images have in common?",
                    "label": 0
                },
                {
                    "sent": "People and horses, right?",
                    "label": 0
                },
                {
                    "sent": "So all the images contain people and horses and no dogs and no elephants insight.",
                    "label": 0
                },
                {
                    "sent": "And similarly you can blend images and captions.",
                    "label": 0
                },
                {
                    "sent": "As you know, we didn't.",
                    "label": 0
                },
                {
                    "sent": "We didn't train the model to do any of this sort of reasoning, right.",
                    "label": 0
                },
                {
                    "sent": "We trained on raw images and captions.",
                    "label": 0
                },
                {
                    "sent": "This is just something you get for free when you use an order demanding space.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "The relationship between images and language forms a partial order.",
                    "label": 1
                },
                {
                    "sent": "And whenever we have a partial order, we can efficiently learn them using order preserving mappings, order embeddings between the domain and an ordered vector space.",
                    "label": 0
                },
                {
                    "sent": "And if you're looking for a place to start, our code is available on GitHub.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}