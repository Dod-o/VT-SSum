{
    "id": "7gcxcr5d4ghcgisd7s4snqeq64xshlvl",
    "title": "Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss",
    "info": {
        "author": [
            "David McAllester, Toyota Technological Institute at Chicago"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Data",
            "Top->Computer Science->Computational Linguistics->Machine Translation"
        ]
    },
    "url": "http://videolectures.net/nips2011_mcallester_ramploss/",
    "segmentation": [
        [
            "OK, this talk.",
            "Is about learning in the structured setting.",
            "So the first thing I want to do is talk about learning in the structured setting and I like to think about machine translation for this."
        ],
        [
            "So we've got some inputs and I want to take the example where those are maybe English sentence is and some outputs.",
            "Maybe those are French sentences.",
            "And we've got some probability distribution over translation pairs, say.",
            "And we're going to have a translation system.",
            "That takes an English sentence as input and produces a French sentence as output.",
            "And the way it's going to do that is by maximizing the score.",
            "The score is going to score the translation pair, and it's going to find the French sentence that scores best for this given English sentence, and the score is going to be a linear function, a linear weighting of a feature vector.",
            "And that may look.",
            "I'm not sure if everyone in this audience is sophisticated.",
            "You can make this feature vector very complicated and nonlinear so that very sophisticated translation systems could be represented this way.",
            "So what we're going to do is we're going to try to learn this weight vector.",
            "That's what we're interested in.",
            "We're interested in learning a weight vector such that, say, our translation system.",
            "Works well is effective.",
            "We'd really like to find a weight vector such that when we take an arbitrary translation pair and I'm going to assume some probability distribution over translation pairs.",
            "We minimize a loss like the Bleu score between the reference translation and the translation produced by the system.",
            "And what I really want to focus on here is the arbitrary loss function here, like the Bleu score.",
            "Now.",
            "There's an issue here and that is that this translation system is insensitive to scaling W. So I could take the weight vector W and scale it up or down.",
            "More Interestingly, and our translations don't change.",
            "Now what we're going to do with the training our learning algorithm is going to produce a weight vector that minimizes is a typical learning algorithm will minimize some combination of a loss on the training data plus a regularization term.",
            "OK, so the issue is that the loss on the training data if this didn't change, if I were to just use my so called task loss, I'll use the frame.",
            "The phrase task loss for the loss we're ultimately trying to optimize, like the blue score.",
            "If this was literally the Bleu score on the training data, I could scale W down, make it arbitrarily small.",
            "And this would not change, right?",
            "So we can't use the task loss here and all the learning algorithms use some so-called surrogate loss, LS a surrogate loss, and it has to be different from the task loss.",
            "And it has to be scaled sensitive.",
            "So this loss gets larger as W gets smaller, right?",
            "So the question is now."
        ],
        [
            "What surrogate loss functions can we use?",
            "So what I want to do in this talk is talk about sort of this zouave surrogate loss functions.",
            "These are all now scale sensitive loss functions.",
            "And these are standard.",
            "This is log loss.",
            "This is the loss used in a conditional random field.",
            "Now I should say that in this talk I'm going to just be considering the structured case, but in the paper and in recent literature this gets generalized to the case of hidden information as well.",
            "So hidden information just let me or latent information just let me use the language translation analogy.",
            "Might be a parse tree or something.",
            "Some syntactic structure or other information or alignments that aren't in the translation pair, but that are still optimized by the translator.",
            "OK, so.",
            "We're not talking about land information, but this can be generalized latent information.",
            "We can define the problem given a weight vector, it's possible to define the probability of the translation given the input.",
            "As as a ratio of partition functions.",
            "And this is the surrogate loss that's used in a CRF.",
            "This is a structured hinge loss.",
            "So a lot of what this talk is going to be about is making these loss functions transparent.",
            "This is fairly standard at this.",
            "Now this is used in a structural SVM.",
            "It looks at a difference between a loss adjusted inference.",
            "We find a label that has high loss.",
            "We take the Max over the translations of the score plus the loss, so we're intentionally producing a bad translation here, minus the score of the gold standard translation.",
            "OK, and that gives us the this is so called hinge loss.",
            "If this is not clear, I'm going to spend a lot of time trying to elucidate these things.",
            "This is a ramp loss.",
            "OK, the ramp loss differs from the hinge loss only in this second term.",
            "Instead of looking at the gold standard here, the ramp loss looks at the in Ferd at the non loss adjusted in Ferd label.",
            "OK, so we look at the loss adjusted inferred label minus the score of the non loss adjusted inferred label and I'm going to call that the ramp loss.",
            "And finally, I'm going to call this the probate loss.",
            "The probit loss is defined by taking the weight vector and adding Gaussian noise.",
            "So we take the probit loss at a weight vector.",
            "W is defined by taking an expected loss over adding Gaussian noise.",
            "the W of the loss of the resulting translation under the Noise model.",
            "OK, now these all look strange."
        ],
        [
            "So the first thing I want to point out is that these all.",
            "If you take the binary case, is sort of a special case of the structured case.",
            "These losses all mapped to familiar binary case losses.",
            "So this is just the binary case.",
            "We take the binary labels.",
            "We take loss to be 01 loss.",
            "This is a standard mapping between a feature vector on input output pairs to the binary case, and we define the margin in the standard way.",
            "The log loss is the familiar log loss.",
            "The hinge loss is the familiar hinge loss which is 0 for the margin is above 1.",
            "The ramp loss has three linear segments.",
            "The ramp loss is 0 for strongly negative margins.",
            "I'm sorry is 1 for strongly negative margins and zero for margins above 1.",
            "And the probit loss is a sigmoidal smooth sigmoid loss that tends towards loss 0 for high margins and loss, one for low margins.",
            "It's the probability over Gaussian noise that the noise exceeds the margin.",
            "Assuming this is true."
        ],
        [
            "OK. Now.",
            "I just want to review some basic properties of these loss functions.",
            "The log losses and so now I want you to think in the binary case.",
            "OK, just for intuition, just to verify that these are standard properties in the binary case, and then I'm going to mention that they also hold in the structured case.",
            "So both log loss in hinge loss have the property that if you have a wrongly a bad example or wrongly classified training point or a badly classified training point as you scale up W, the loss is unbounded.",
            "The loss will increase without bound as the margin becomes more and more negative.",
            "However, hinge loss, ramp loss and hinge loss are bounded in the interval 01.",
            "If you think of the binary case that should be.",
            "Sorry ramp loss not Angeles ramp loss and profit loss are bounded in 01.",
            "Another property we have is that the hinge loss is that the ramp loss is a strictly tighter upper bound on the task loss then is the hinge loss and this last property I didn't mention."
        ],
        [
            "Let me."
        ],
        [
            "Go back here.",
            "All of these things I should mention, all of these things have a received increasing interest in the machine learning community.",
            "This ramp loss was introduced by Doe at all."
        ],
        [
            "And they used this property that the ramp loss is a tighter upper bound on the task loss, as sort of the motivating property for the introduction of ramp loss."
        ],
        [
            "OK, so let me just say.",
            "A few more things about this.",
            "You can see that these other properties hold these properties."
        ],
        [
            "His other slide."
        ],
        [
            "Like the fact that these are in 01 and this inequality."
        ],
        [
            "Follow essentially immediately by just the fact that this Max, for example, the fact that ramp loss is less than hinge loss follows from the fact that this Max must be larger than this particular score.",
            "Since this is a Max over score, it's larger than that and we get that inequality.",
            "To prove that this is greater than.",
            "Is greater than the task loss.",
            "We simply observe that this Max is greater than sticking.",
            "The maximizer of this in here, in which case the score parts cancel and we just get the task loss.",
            "And similarly another observation involving this mask Max."
        ],
        [
            "Term.",
            "Gives us that the these things that the ramp loss is less than one.",
            "The fact that the probate laws."
        ],
        [
            "Is less than one, just follows from the fact that this is an expectation over noise of a loss which is less than one.",
            "OK, let me back up here."
        ],
        [
            "There's a few more things I want to say about this.",
            "The binary case, I think is a little bit misleading.",
            "Note that this is a difference between 2 two numbers.",
            "A score here of a maximizer of loss adjusted in the gold standard.",
            "So in the binary case, it's natural that we look at two labels because there are only two labels and in the binary case people are used to thinking of the log loss and the hinge loss as being similar.",
            "Here, however, in the structured case, the fact that we're looking at only two labels here.",
            "It seems fundamentally different from taking something that's looking at all the labels.",
            "The partition function underlying this distribution is looking at all the labels.",
            "So there's a suggestion here and I don't have any theorems about what about this idea, but the idea is that in the structured case, the hinge loss and the log loss can be fundamentally different.",
            "The fact that there are many labels means looking at two labels only is throwing away a lot of information and the same observation happens with the ramp loss.",
            "As opposed to the probate loss, so the ramp loss again is looking at just two labels, but the probit loss is looking at is essentially taking into account an exponentially large number of labels.",
            "By looking at all the possible translations you get when you perturb the W. So the intuitions about the binary case are a little dangerous in the structured case."
        ],
        [
            "Um?"
        ],
        [
            "Another thing to observe."
        ],
        [
            "Is that these in the in the simple cases these are convex and these are not convex.",
            "So."
        ],
        [
            "Is sort of a convex?"
        ],
        [
            "City.",
            "Convexity.",
            "Uh."
        ],
        [
            "So let me back up.",
            "Sorry, but we're going to prove in this talk is that these loss functions the learning algorithms based on these loss functions are consistent.",
            "OK, meaning that in the limit are consistent in a predictive sense in the limit of infinite data.",
            "When I take the learning rule over these loss functions, it's going to asymptotically converge on a generalization loss, which is the minimum possible generalization loss overall W."
        ],
        [
            "So this slide is just to point out that.",
            "We this considerable.",
            "There's some case studies that suggest that the use of these consistent loss functions are empirically superior to, say hinge loss, structured hinge loss, so there's some earlier work in machine translation suggesting that of subgradient descent version of the ramp loss is actually a variant of that is superior.",
            "We've done some work in speech recognition and phonetic recognition with indicates that the subgradient descent on the ramp loss.",
            "Works better than hinge loss.",
            "We've also done some work with the probit loss, indicating that it has superior empirical performance to the."
        ],
        [
            "Structure hingeless OK, this is just some quick terminology.",
            "I'm running short on time here.",
            "This what we're interested in achieving is L star is the info over the weight vectors of the loss of the weight vector.",
            "I'm going to talk about this quantity, which is the empirical loss over the first end training points, so this is the empirical loss of the 1st."
        ],
        [
            "Training points, here's our first.",
            "Consistency theorem.",
            "It says if we take this learning rule.",
            "Where this is Luke trained from the 1st?",
            "And training points.",
            "We've got a regularization parameter in here.",
            "As long as this regularization parameter increases without bound as N increases and.",
            "And this property also holds.",
            "This rule is consistent with.",
            "This says this regularization parameter can be chosen to be any power event strictly between zero and one and we get consistency."
        ],
        [
            "OK, I'm going to quickly talk about the pack Bayesian theorem that from which this is proved.",
            "This is sort of a teaser for the technical results if you should read the paper, come to the poster if you really want to see the theorems, this is a version of the Pack Bayesian theorem that I love.",
            "This says that the pack Bayesian theorem says.",
            "That for any prior on a concept class, it's uniformly true for all posteriors that the generalization loss of the posterior is bounded by this quantity.",
            "This is a this particular theorem is recently proved in a couple of papers.",
            "It turns out that simply by instantiating this theorem with Gaussian centered around W to be the posterior we get, we almost immediately get a generalization bound on the probit loss.",
            "So in this generalization bound on the probit loss essentially, essentially very immediately proves the theorem I just showed you, which is."
        ],
        [
            "And see.",
            "Here's the consistency for ramp loss.",
            "It's deceptively similar.",
            "Just a slight log rhythmic adjustment to the conditions.",
            "We also get consistency for ramp loss."
        ],
        [
            "I'm going to have to skip them."
        ],
        [
            "You have ramp loss."
        ],
        [
            "Anne."
        ],
        [
            "The consistency theorem was deceptively similar.",
            "If we workout generalization bounds for ramp loss, we get a significant difference.",
            "We get a 1/3 factor here instead of what is essentially a linear factor that we get for probit loss.",
            "So I apologize that the theorems went through very quickly.",
            "You'll have to read the paper or come to the poster, but the bottom line is ramp loss and probit loss are consistent is a consistency, convexity, trade off.",
            "We found empirically that we get some advantage from the consistent versions and I'll stop there.",
            "So the question is, why does where is the proof break for hinge loss?",
            "Let me go back to this slide since we have a little time for questions."
        ],
        [
            "I should have pointed out there's an intuitive reason that this consistency works.",
            "If you let W go to Infinity.",
            "If you want to go to Infinity, then if you take W and scale it up to Infinity then this score is going to dominate this sum.",
            "So this Max will become realized where the score is maximized, ignoring the loss adjustment.",
            "So as W goes to Infinity.",
            "This Max and this Max will occur at the same point.",
            "These will.",
            "The scores will cancel and you're left with just the loss, so as W goes to Infinity, ramp loss converges to task loss.",
            "And as Doug and the same thing is going to happen with probate law says W goes to Infinity, the the noise becomes irrelevant as Debbie goes to Infinity.",
            "Probate loss converges to task loss.",
            "That's not true of the hinge loss.",
            "The hinge loss can remain large even as W goes to Infinity, and that at least provides the intuition as to why hinge loss is not consistent.",
            "It's subject to outliers.",
            "Another thing that I should have said is it in these properties.",
            "The fact that these are bounded by zero one is related to robustness in learning algorithms, right?",
            "These are robust loss functions.",
            "The fact that hinge and log are unbounded means they're susceptible to outliers.",
            "So.",
            "I hope that answered the question."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this talk.",
                    "label": 0
                },
                {
                    "sent": "Is about learning in the structured setting.",
                    "label": 0
                },
                {
                    "sent": "So the first thing I want to do is talk about learning in the structured setting and I like to think about machine translation for this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've got some inputs and I want to take the example where those are maybe English sentence is and some outputs.",
                    "label": 0
                },
                {
                    "sent": "Maybe those are French sentences.",
                    "label": 0
                },
                {
                    "sent": "And we've got some probability distribution over translation pairs, say.",
                    "label": 0
                },
                {
                    "sent": "And we're going to have a translation system.",
                    "label": 0
                },
                {
                    "sent": "That takes an English sentence as input and produces a French sentence as output.",
                    "label": 0
                },
                {
                    "sent": "And the way it's going to do that is by maximizing the score.",
                    "label": 0
                },
                {
                    "sent": "The score is going to score the translation pair, and it's going to find the French sentence that scores best for this given English sentence, and the score is going to be a linear function, a linear weighting of a feature vector.",
                    "label": 0
                },
                {
                    "sent": "And that may look.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if everyone in this audience is sophisticated.",
                    "label": 0
                },
                {
                    "sent": "You can make this feature vector very complicated and nonlinear so that very sophisticated translation systems could be represented this way.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is we're going to try to learn this weight vector.",
                    "label": 0
                },
                {
                    "sent": "That's what we're interested in.",
                    "label": 0
                },
                {
                    "sent": "We're interested in learning a weight vector such that, say, our translation system.",
                    "label": 0
                },
                {
                    "sent": "Works well is effective.",
                    "label": 0
                },
                {
                    "sent": "We'd really like to find a weight vector such that when we take an arbitrary translation pair and I'm going to assume some probability distribution over translation pairs.",
                    "label": 1
                },
                {
                    "sent": "We minimize a loss like the Bleu score between the reference translation and the translation produced by the system.",
                    "label": 0
                },
                {
                    "sent": "And what I really want to focus on here is the arbitrary loss function here, like the Bleu score.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "There's an issue here and that is that this translation system is insensitive to scaling W. So I could take the weight vector W and scale it up or down.",
                    "label": 0
                },
                {
                    "sent": "More Interestingly, and our translations don't change.",
                    "label": 0
                },
                {
                    "sent": "Now what we're going to do with the training our learning algorithm is going to produce a weight vector that minimizes is a typical learning algorithm will minimize some combination of a loss on the training data plus a regularization term.",
                    "label": 0
                },
                {
                    "sent": "OK, so the issue is that the loss on the training data if this didn't change, if I were to just use my so called task loss, I'll use the frame.",
                    "label": 0
                },
                {
                    "sent": "The phrase task loss for the loss we're ultimately trying to optimize, like the blue score.",
                    "label": 0
                },
                {
                    "sent": "If this was literally the Bleu score on the training data, I could scale W down, make it arbitrarily small.",
                    "label": 0
                },
                {
                    "sent": "And this would not change, right?",
                    "label": 1
                },
                {
                    "sent": "So we can't use the task loss here and all the learning algorithms use some so-called surrogate loss, LS a surrogate loss, and it has to be different from the task loss.",
                    "label": 0
                },
                {
                    "sent": "And it has to be scaled sensitive.",
                    "label": 0
                },
                {
                    "sent": "So this loss gets larger as W gets smaller, right?",
                    "label": 0
                },
                {
                    "sent": "So the question is now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What surrogate loss functions can we use?",
                    "label": 1
                },
                {
                    "sent": "So what I want to do in this talk is talk about sort of this zouave surrogate loss functions.",
                    "label": 0
                },
                {
                    "sent": "These are all now scale sensitive loss functions.",
                    "label": 0
                },
                {
                    "sent": "And these are standard.",
                    "label": 0
                },
                {
                    "sent": "This is log loss.",
                    "label": 0
                },
                {
                    "sent": "This is the loss used in a conditional random field.",
                    "label": 0
                },
                {
                    "sent": "Now I should say that in this talk I'm going to just be considering the structured case, but in the paper and in recent literature this gets generalized to the case of hidden information as well.",
                    "label": 0
                },
                {
                    "sent": "So hidden information just let me or latent information just let me use the language translation analogy.",
                    "label": 0
                },
                {
                    "sent": "Might be a parse tree or something.",
                    "label": 0
                },
                {
                    "sent": "Some syntactic structure or other information or alignments that aren't in the translation pair, but that are still optimized by the translator.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We're not talking about land information, but this can be generalized latent information.",
                    "label": 0
                },
                {
                    "sent": "We can define the problem given a weight vector, it's possible to define the probability of the translation given the input.",
                    "label": 0
                },
                {
                    "sent": "As as a ratio of partition functions.",
                    "label": 0
                },
                {
                    "sent": "And this is the surrogate loss that's used in a CRF.",
                    "label": 0
                },
                {
                    "sent": "This is a structured hinge loss.",
                    "label": 0
                },
                {
                    "sent": "So a lot of what this talk is going to be about is making these loss functions transparent.",
                    "label": 0
                },
                {
                    "sent": "This is fairly standard at this.",
                    "label": 0
                },
                {
                    "sent": "Now this is used in a structural SVM.",
                    "label": 0
                },
                {
                    "sent": "It looks at a difference between a loss adjusted inference.",
                    "label": 0
                },
                {
                    "sent": "We find a label that has high loss.",
                    "label": 0
                },
                {
                    "sent": "We take the Max over the translations of the score plus the loss, so we're intentionally producing a bad translation here, minus the score of the gold standard translation.",
                    "label": 0
                },
                {
                    "sent": "OK, and that gives us the this is so called hinge loss.",
                    "label": 0
                },
                {
                    "sent": "If this is not clear, I'm going to spend a lot of time trying to elucidate these things.",
                    "label": 1
                },
                {
                    "sent": "This is a ramp loss.",
                    "label": 0
                },
                {
                    "sent": "OK, the ramp loss differs from the hinge loss only in this second term.",
                    "label": 0
                },
                {
                    "sent": "Instead of looking at the gold standard here, the ramp loss looks at the in Ferd at the non loss adjusted in Ferd label.",
                    "label": 0
                },
                {
                    "sent": "OK, so we look at the loss adjusted inferred label minus the score of the non loss adjusted inferred label and I'm going to call that the ramp loss.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'm going to call this the probate loss.",
                    "label": 0
                },
                {
                    "sent": "The probit loss is defined by taking the weight vector and adding Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "So we take the probit loss at a weight vector.",
                    "label": 0
                },
                {
                    "sent": "W is defined by taking an expected loss over adding Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "the W of the loss of the resulting translation under the Noise model.",
                    "label": 0
                },
                {
                    "sent": "OK, now these all look strange.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first thing I want to point out is that these all.",
                    "label": 0
                },
                {
                    "sent": "If you take the binary case, is sort of a special case of the structured case.",
                    "label": 0
                },
                {
                    "sent": "These losses all mapped to familiar binary case losses.",
                    "label": 0
                },
                {
                    "sent": "So this is just the binary case.",
                    "label": 0
                },
                {
                    "sent": "We take the binary labels.",
                    "label": 0
                },
                {
                    "sent": "We take loss to be 01 loss.",
                    "label": 0
                },
                {
                    "sent": "This is a standard mapping between a feature vector on input output pairs to the binary case, and we define the margin in the standard way.",
                    "label": 1
                },
                {
                    "sent": "The log loss is the familiar log loss.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss is the familiar hinge loss which is 0 for the margin is above 1.",
                    "label": 1
                },
                {
                    "sent": "The ramp loss has three linear segments.",
                    "label": 0
                },
                {
                    "sent": "The ramp loss is 0 for strongly negative margins.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry is 1 for strongly negative margins and zero for margins above 1.",
                    "label": 0
                },
                {
                    "sent": "And the probit loss is a sigmoidal smooth sigmoid loss that tends towards loss 0 for high margins and loss, one for low margins.",
                    "label": 0
                },
                {
                    "sent": "It's the probability over Gaussian noise that the noise exceeds the margin.",
                    "label": 0
                },
                {
                    "sent": "Assuming this is true.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Now.",
                    "label": 0
                },
                {
                    "sent": "I just want to review some basic properties of these loss functions.",
                    "label": 1
                },
                {
                    "sent": "The log losses and so now I want you to think in the binary case.",
                    "label": 0
                },
                {
                    "sent": "OK, just for intuition, just to verify that these are standard properties in the binary case, and then I'm going to mention that they also hold in the structured case.",
                    "label": 1
                },
                {
                    "sent": "So both log loss in hinge loss have the property that if you have a wrongly a bad example or wrongly classified training point or a badly classified training point as you scale up W, the loss is unbounded.",
                    "label": 1
                },
                {
                    "sent": "The loss will increase without bound as the margin becomes more and more negative.",
                    "label": 0
                },
                {
                    "sent": "However, hinge loss, ramp loss and hinge loss are bounded in the interval 01.",
                    "label": 0
                },
                {
                    "sent": "If you think of the binary case that should be.",
                    "label": 0
                },
                {
                    "sent": "Sorry ramp loss not Angeles ramp loss and profit loss are bounded in 01.",
                    "label": 0
                },
                {
                    "sent": "Another property we have is that the hinge loss is that the ramp loss is a strictly tighter upper bound on the task loss then is the hinge loss and this last property I didn't mention.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back here.",
                    "label": 0
                },
                {
                    "sent": "All of these things I should mention, all of these things have a received increasing interest in the machine learning community.",
                    "label": 0
                },
                {
                    "sent": "This ramp loss was introduced by Doe at all.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they used this property that the ramp loss is a tighter upper bound on the task loss, as sort of the motivating property for the introduction of ramp loss.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me just say.",
                    "label": 0
                },
                {
                    "sent": "A few more things about this.",
                    "label": 0
                },
                {
                    "sent": "You can see that these other properties hold these properties.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His other slide.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like the fact that these are in 01 and this inequality.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Follow essentially immediately by just the fact that this Max, for example, the fact that ramp loss is less than hinge loss follows from the fact that this Max must be larger than this particular score.",
                    "label": 0
                },
                {
                    "sent": "Since this is a Max over score, it's larger than that and we get that inequality.",
                    "label": 0
                },
                {
                    "sent": "To prove that this is greater than.",
                    "label": 0
                },
                {
                    "sent": "Is greater than the task loss.",
                    "label": 0
                },
                {
                    "sent": "We simply observe that this Max is greater than sticking.",
                    "label": 0
                },
                {
                    "sent": "The maximizer of this in here, in which case the score parts cancel and we just get the task loss.",
                    "label": 0
                },
                {
                    "sent": "And similarly another observation involving this mask Max.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Term.",
                    "label": 0
                },
                {
                    "sent": "Gives us that the these things that the ramp loss is less than one.",
                    "label": 0
                },
                {
                    "sent": "The fact that the probate laws.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is less than one, just follows from the fact that this is an expectation over noise of a loss which is less than one.",
                    "label": 0
                },
                {
                    "sent": "OK, let me back up here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a few more things I want to say about this.",
                    "label": 0
                },
                {
                    "sent": "The binary case, I think is a little bit misleading.",
                    "label": 0
                },
                {
                    "sent": "Note that this is a difference between 2 two numbers.",
                    "label": 0
                },
                {
                    "sent": "A score here of a maximizer of loss adjusted in the gold standard.",
                    "label": 0
                },
                {
                    "sent": "So in the binary case, it's natural that we look at two labels because there are only two labels and in the binary case people are used to thinking of the log loss and the hinge loss as being similar.",
                    "label": 0
                },
                {
                    "sent": "Here, however, in the structured case, the fact that we're looking at only two labels here.",
                    "label": 0
                },
                {
                    "sent": "It seems fundamentally different from taking something that's looking at all the labels.",
                    "label": 0
                },
                {
                    "sent": "The partition function underlying this distribution is looking at all the labels.",
                    "label": 0
                },
                {
                    "sent": "So there's a suggestion here and I don't have any theorems about what about this idea, but the idea is that in the structured case, the hinge loss and the log loss can be fundamentally different.",
                    "label": 0
                },
                {
                    "sent": "The fact that there are many labels means looking at two labels only is throwing away a lot of information and the same observation happens with the ramp loss.",
                    "label": 0
                },
                {
                    "sent": "As opposed to the probate loss, so the ramp loss again is looking at just two labels, but the probit loss is looking at is essentially taking into account an exponentially large number of labels.",
                    "label": 0
                },
                {
                    "sent": "By looking at all the possible translations you get when you perturb the W. So the intuitions about the binary case are a little dangerous in the structured case.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing to observe.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that these in the in the simple cases these are convex and these are not convex.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is sort of a convex?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "City.",
                    "label": 0
                },
                {
                    "sent": "Convexity.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me back up.",
                    "label": 0
                },
                {
                    "sent": "Sorry, but we're going to prove in this talk is that these loss functions the learning algorithms based on these loss functions are consistent.",
                    "label": 0
                },
                {
                    "sent": "OK, meaning that in the limit are consistent in a predictive sense in the limit of infinite data.",
                    "label": 0
                },
                {
                    "sent": "When I take the learning rule over these loss functions, it's going to asymptotically converge on a generalization loss, which is the minimum possible generalization loss overall W.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this slide is just to point out that.",
                    "label": 0
                },
                {
                    "sent": "We this considerable.",
                    "label": 0
                },
                {
                    "sent": "There's some case studies that suggest that the use of these consistent loss functions are empirically superior to, say hinge loss, structured hinge loss, so there's some earlier work in machine translation suggesting that of subgradient descent version of the ramp loss is actually a variant of that is superior.",
                    "label": 0
                },
                {
                    "sent": "We've done some work in speech recognition and phonetic recognition with indicates that the subgradient descent on the ramp loss.",
                    "label": 0
                },
                {
                    "sent": "Works better than hinge loss.",
                    "label": 0
                },
                {
                    "sent": "We've also done some work with the probit loss, indicating that it has superior empirical performance to the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Structure hingeless OK, this is just some quick terminology.",
                    "label": 0
                },
                {
                    "sent": "I'm running short on time here.",
                    "label": 0
                },
                {
                    "sent": "This what we're interested in achieving is L star is the info over the weight vectors of the loss of the weight vector.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about this quantity, which is the empirical loss over the first end training points, so this is the empirical loss of the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training points, here's our first.",
                    "label": 0
                },
                {
                    "sent": "Consistency theorem.",
                    "label": 0
                },
                {
                    "sent": "It says if we take this learning rule.",
                    "label": 0
                },
                {
                    "sent": "Where this is Luke trained from the 1st?",
                    "label": 0
                },
                {
                    "sent": "And training points.",
                    "label": 0
                },
                {
                    "sent": "We've got a regularization parameter in here.",
                    "label": 0
                },
                {
                    "sent": "As long as this regularization parameter increases without bound as N increases and.",
                    "label": 0
                },
                {
                    "sent": "And this property also holds.",
                    "label": 0
                },
                {
                    "sent": "This rule is consistent with.",
                    "label": 0
                },
                {
                    "sent": "This says this regularization parameter can be chosen to be any power event strictly between zero and one and we get consistency.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to quickly talk about the pack Bayesian theorem that from which this is proved.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a teaser for the technical results if you should read the paper, come to the poster if you really want to see the theorems, this is a version of the Pack Bayesian theorem that I love.",
                    "label": 0
                },
                {
                    "sent": "This says that the pack Bayesian theorem says.",
                    "label": 0
                },
                {
                    "sent": "That for any prior on a concept class, it's uniformly true for all posteriors that the generalization loss of the posterior is bounded by this quantity.",
                    "label": 0
                },
                {
                    "sent": "This is a this particular theorem is recently proved in a couple of papers.",
                    "label": 0
                },
                {
                    "sent": "It turns out that simply by instantiating this theorem with Gaussian centered around W to be the posterior we get, we almost immediately get a generalization bound on the probit loss.",
                    "label": 0
                },
                {
                    "sent": "So in this generalization bound on the probit loss essentially, essentially very immediately proves the theorem I just showed you, which is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And see.",
                    "label": 0
                },
                {
                    "sent": "Here's the consistency for ramp loss.",
                    "label": 0
                },
                {
                    "sent": "It's deceptively similar.",
                    "label": 0
                },
                {
                    "sent": "Just a slight log rhythmic adjustment to the conditions.",
                    "label": 0
                },
                {
                    "sent": "We also get consistency for ramp loss.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to have to skip them.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have ramp loss.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The consistency theorem was deceptively similar.",
                    "label": 0
                },
                {
                    "sent": "If we workout generalization bounds for ramp loss, we get a significant difference.",
                    "label": 0
                },
                {
                    "sent": "We get a 1/3 factor here instead of what is essentially a linear factor that we get for probit loss.",
                    "label": 0
                },
                {
                    "sent": "So I apologize that the theorems went through very quickly.",
                    "label": 0
                },
                {
                    "sent": "You'll have to read the paper or come to the poster, but the bottom line is ramp loss and probit loss are consistent is a consistency, convexity, trade off.",
                    "label": 0
                },
                {
                    "sent": "We found empirically that we get some advantage from the consistent versions and I'll stop there.",
                    "label": 0
                },
                {
                    "sent": "So the question is, why does where is the proof break for hinge loss?",
                    "label": 0
                },
                {
                    "sent": "Let me go back to this slide since we have a little time for questions.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I should have pointed out there's an intuitive reason that this consistency works.",
                    "label": 0
                },
                {
                    "sent": "If you let W go to Infinity.",
                    "label": 0
                },
                {
                    "sent": "If you want to go to Infinity, then if you take W and scale it up to Infinity then this score is going to dominate this sum.",
                    "label": 0
                },
                {
                    "sent": "So this Max will become realized where the score is maximized, ignoring the loss adjustment.",
                    "label": 0
                },
                {
                    "sent": "So as W goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "This Max and this Max will occur at the same point.",
                    "label": 0
                },
                {
                    "sent": "These will.",
                    "label": 0
                },
                {
                    "sent": "The scores will cancel and you're left with just the loss, so as W goes to Infinity, ramp loss converges to task loss.",
                    "label": 0
                },
                {
                    "sent": "And as Doug and the same thing is going to happen with probate law says W goes to Infinity, the the noise becomes irrelevant as Debbie goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Probate loss converges to task loss.",
                    "label": 0
                },
                {
                    "sent": "That's not true of the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "The hinge loss can remain large even as W goes to Infinity, and that at least provides the intuition as to why hinge loss is not consistent.",
                    "label": 0
                },
                {
                    "sent": "It's subject to outliers.",
                    "label": 0
                },
                {
                    "sent": "Another thing that I should have said is it in these properties.",
                    "label": 0
                },
                {
                    "sent": "The fact that these are bounded by zero one is related to robustness in learning algorithms, right?",
                    "label": 0
                },
                {
                    "sent": "These are robust loss functions.",
                    "label": 0
                },
                {
                    "sent": "The fact that hinge and log are unbounded means they're susceptible to outliers.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I hope that answered the question.",
                    "label": 0
                }
            ]
        }
    }
}