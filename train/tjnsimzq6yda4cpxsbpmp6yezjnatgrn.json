{
    "id": "tjnsimzq6yda4cpxsbpmp6yezjnatgrn",
    "title": "Improving Visual Relationship Detection using Semantic Modeling of Scene Descriptions",
    "info": {
        "author": [
            "Stephan Baier, Ludwig-Maximilians Universit\u00e4t"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_baier_scene_descriptions/",
    "segmentation": [
        [
            "Yeah I want to present today.",
            "I will work on improving visual relationship detection using a semantic modeling of scene descriptions and this is work which I did together with my colleague in Puma and my PhD supervisor, Professor, Doctor for Chris."
        ],
        [
            "So this work is about detecting triples from images.",
            "So if we look at this image for example, we can see multiple objects.",
            "For example, the person.",
            "There's a motorbike, there is a wheel, and so on, and these visual concepts.",
            "Then in some relationship to each other.",
            "So for example, the person is on the motorcycle, the person is wearing a helmet, the motorcycle has a veal.",
            "And the person is wearing a jacket, so it is not only about detecting what is on the image, but also what is the relationship between these concepts in the particular image."
        ],
        [
            "So the goal is to build a system where you input an image and then you output list of triples which describe basically the scene.",
            "And so.",
            "As you can probably, I mean as it is easy to see the relationship between the object that is very important for really capturing the semantics of the scene and there is not has not been so much work on that one major related in prior work is by Lou ET al from the stand for Computer Vision Group from 2016 and they also were the first who proposed a comprehensive data set for that task and we will also use this data set in our experiments.",
            "So there are a number of."
        ],
        [
            "Then just to this problem.",
            "So first of all object detectors.",
            "Work quite well, but so there has been a lot of improvement, but they're still not perfect and they provide us with chest with probabilities of what we see in an image.",
            "And then if you build triples.",
            "You have a combinatorial complexity, so if you have our relationships and E possible visual concepts, then the number of possible troubles becomes our times E * E, which scales really heavily.",
            "If you like, have a high number of visual concepts.",
            "And the more visual concepts you have, the more likely it is that there are triples in your training set which you have never seen, so that you have surf triples in the test set which you have never seen you in the training data.",
            "For example, you might have seen a lot of images where a person is riding a horse.",
            "But you never saw a person riding an elephant, but still you want to classify correctly.",
            "You might have seen persons you might have seen elephants.",
            "You might have seen the relationship right in various contexts, but not the particular triple person ride elephants.",
            "OK."
        ],
        [
            "So our approach to this problem is the following.",
            "Basically, the three components.",
            "The first one is we build a probabilistic model which captures the prior knowledge.",
            "The probabilistic prior knowledge about the semantics, the visual for example, capture that it is more likely that the person is riding a motorbike, then a person flying motorbike or something which wouldn't make sense right?",
            "So there is some world knowledge which we have and this should be captured by this probabilistic prior.",
            "Then the second part is a computer vision pipeline.",
            "Which detects objects and classifieds them so that we know OK there is a person.",
            "There is a motorbike and so on.",
            "An for this second part we basically use the same the same techniques as they did in the previous paper by Louis.",
            "And then finally we have scores from our semantic prior an from our computer vision pipeline and we combine them together in a probabilistic graphical model.",
            "So that's the third step.",
            "So how?"
        ],
        [
            "Does this prior look like so you can think of it as a fully connected graph first, where each node has a probability or each relationship between the between two concepts has a probability of discovering in the real world, and then this graph which we see here only shows the ones which have a certain probabilities.",
            "So above some threshold.",
            "So for example there is some probability that a person is riding a motorcycle, that motorcycle has uvela handlebar, so this doesn't mean it has always.",
            "Properties, but like you discover this often in your data set.",
            "So you maybe discover that the bicycle and motorcycle are pretty similar, so they appear in the same area like urban areas.",
            "They have similar parts like a villain Handlebar and you also see Lofton person riding on motorbikes, but maybe you never saw a person riding a bicycle, so this edge will get a very low prior right.",
            "However, you can infer from the structure that as motorbike and bicycle are very similar, maybe this edge person riding a bicycle should also get a boost and should also have some.",
            "A reasonable price likelihood of being true, and so in order to achieve this, we use methods which have been mainly used for missing link prediction and knowledge graphs and so with this we want to generalize to the these missing links and adjust the probabilities in the whole graph.",
            "So how do we do that?"
        ],
        [
            "That we construct the tensor, which is the three value error, basically, which represents accounts of how often we sorted triple in the training data, and then so many of them are zero.",
            "For example, a person riding a bicycle with zero here, right?",
            "But this is not the right prior which we want to use for our model.",
            "So what we do is we do a low rank tensor decomposition and there are for example here in the slide you see the rest call factorization, which makes the approximation for this tensor.",
            "And captures in it's latent factors kind of the underlying structure of that 3 dimensional tensor.",
            "And there is not only the rest call decompositions, there are other ones such as this multi complex and the multiple neural network which we use in our experiments.",
            "The least methods come from missing link prediction and knowledge graphs that have been successfully used in there and we now apply to this little bit different setting.",
            "So we have to change the cost function a little bit.",
            "We have to go to a post or cost function for modeling discount data because in the knowledge Graph typically of zeros and ones and here now we have commentator as entries in that answer and then we want to transfer the scores which we get out from the approximated tensor.",
            "Get it probability distribution and that is just by normalizing them with the ball spun distribution.",
            "So the output of this semantic model is finally a probability distribution over there.",
            "Suppose so this should now capture that, for example, the probability for person riding motorbike is relatively high person riding bicycle as well.",
            "We generalize to that, but for example, person flying motorbike wouldn't make much sense right?",
            "So this is now in this prior distribution over all the triples."
        ],
        [
            "So second, we need a visual model in order to detect what is actually on the image.",
            "Therefore we use a region, CNN region, convolutional neural network, which is a standard approach to get like salient objects in the image.",
            "Then we take these areas which we find in the objects and classify them with a convolutional neural network.",
            "In order to say OK, this is a surfer and surfboard or a person a surfboard.",
            "And then we always take a combination of two.",
            "The of these boxes take the union of them and build another computer convolutional neural network, which predicate predicts the predicate.",
            "So for example, next tool in this case.",
            "So for a pair of regions we now get a probability distribution over what is the subject.",
            "Or probability distribution over what is the object and the probability distribution of what is the predicate between them?",
            "So this is the output of the visual model and this visual model is basically the same as has been as it has been used in the prior work I mentioned before.",
            "OK, now we have this from the."
        ],
        [
            "Visual model so given given a pair of of regions which we denote by I and we have this prior which we get from the factorization model and we want to combine these probabilities and we do this in a probabilistic graphical network with the following structure.",
            "So here is the structure of the network, so it is.",
            "It implies basically a conditional independence of the areas.",
            "Given this the espoz and what we want to do.",
            "But we wanted to predict is this probability.",
            "So what is the correct triple?",
            "Given a pair of.",
            "Bounding boxes and their union and we can divide that by applying the base rule.",
            "So this is a typical approach being also done in Hmm's often so hidden Markov models and so finally we can combine the score which we get from the prior and the visual score.",
            "Combine it in this prediction."
        ],
        [
            "So we did some experiments on the Stanford visual relationship detection data set.",
            "This is contains 4000 images and there are 100 different visual objects or concepts and 70 relationships between them and the relationships are such As for example, has part or where.",
            "Then there are things like spatial relations like on or next to comparative relations like taller than or smaller than and also actions like riding a bicycle or kicking a ball.",
            "And we do multiple evaluation settings.",
            "In order to like evaluate the whole pipeline and see like where which part works.",
            "English doesn't work so nice."
        ],
        [
            "So we got some results.",
            "We compare against only the visual part, which is the same as in the in this prior work and against the baseline or benchmark model.",
            "Also from the Stanford Computer Vision Group.",
            "They also need use a prior information, but what they use is a language prior so they take word embeddings from the triple and then they look how similar these word embeddings.",
            "So for example person riding a bicycle.",
            "I don't know has similar word embeddings so it get boosted.",
            "Yeah, so at the bird embeddings are derived from a large corpus of text based on Co occurrence.",
            "Basically and then.",
            "So we compare this with our method where we use a prior the rest of the multiple networks are complex decomposition and this multi composition and basically what we see is that except for this mode all the all the priors which we use work better than the language prior used in the original paper and but still there.",
            "Correct object prediction object detection, so the vision part is a big bottleneck there.",
            "Often it fails in this stage.",
            "And finally we can conclude that from all of these methods which we tried, the complex factorization gives the best results on this task."
        ],
        [
            "So here are some examples which have been classified correctly.",
            "So for example there was a person extra person, so it detects both and there is a relationship between them or there is a truck on the road.",
            "It is also correctly predicted, but here you also see that, for example, this detection of the of the objects is not going really well, but still we end up with the right triple.",
            "So we somehow find that it's a truck.",
            "Here it's a Rd here and it has a high prior so we get this triple, but it's not always works that well.",
            "For example in this image.",
            "You see that this green rectangle chest doesn't.",
            "Yeah, it doesn't show any real object, so also we end up with a bad prediction lamp on box, which doesn't make sense and sometimes it's like that.",
            "So we get the right objects.",
            "We find that we find the bicycle and we find the wheel, but we send a OK.",
            "This motorcycle has this wheel and that's not true.",
            "It's a view from the other motorcycle right?",
            "And this also shows another problem.",
            "So also the algorithm correctly detected the two wheels of the red one.",
            "It doesn't know that the motorcycle can only have two wheels.",
            "Another third one, right?",
            "So this kind of prediction is not possible with our current model, so this is an open challenge to bring this into the into a probabilistic factorization model.",
            "Or maybe one can apply like additional rules on top in order to get this kind of behavior."
        ],
        [
            "Then we also looked at so called zero shot learning.",
            "Here we look at triples which we really have never seen in the training set.",
            "So anyways, the test images are new images, right?",
            "So they are different from the training images, but sometimes we have seen already a person extra person, different person maybe, but still that rippled person extra person we have seen.",
            "But there are some we have never seen and hear the we discovered that the this prior this probabilistic prior really brings an advantage by generalizing to this unseen triples.",
            "And we can also conclude that here the multiple neural networks in this case.",
            "The best results and so here also."
        ],
        [
            "2 examples of that.",
            "So in the test set in the training set, there was never a bus.",
            "Next bus, never tree behind a bear, never laptop on the stove, and never a bear riding a motorcycle, and we looked at the different methods.",
            "How do they capture these kind of unseen triples and the visual one captures the right one.",
            "The next bus next bus it it captures, but the other one.",
            "Somehow it makes mistakes.",
            "The language prior works well for the bus next to the bus and the tree behind the bear.",
            "So here you can see a tree bear.",
            "These are like similar concepts.",
            "They often appear like jungle bear, you know.",
            "So a language program might help here to have a high prior, but for example, a laptop on a stove that doesn't match make much sense, right?",
            "So the language prior fails in this case.",
            "Various semantic prior also captures the laptop on the stove.",
            "Why?",
            "Because it learns that laptops are typically on something, so it makes sense they can be on everything on a bed, on the table and that things are typically on the stove, right?",
            "So that it recognizes that on is a very general relationship and that laptop in stove both serve as a subject and object respectively.",
            "So we can classify laptop on the stove, get a high prior.",
            "Even so, we have never seen it in the training set.",
            "And the last one is a pair riding a motorcycle.",
            "So here also our prior obviously fails, right?",
            "Because this is.",
            "To some extent, that's what we want, right?",
            "But this also shows somehow with the weakness of the of the prior.",
            "So to some extent, of course we see what we expect, right?",
            "And we will never expect the bear on a motorcycle, so it fails in these cases.",
            "However, what our system gives us is a person riding a motorcycle, which is also not that bad.",
            "However, it is labeled as a bear riding a motorcycle, right?",
            "So here you have a tradeoff between the probabilities like a classifier gives here high probability for bear, but also high probability for person.",
            "And then it's about probabilistic graphical model.",
            "How to combine it?"
        ],
        [
            "So yeah, with that, let me conclude people.",
            "A model which can extract triples from images that ripples, capture complex semantics of the image which can be used in an image database for search, querying, and so on.",
            "We could show with our approach by including this probabilistic prior where we use methods which have been used for missing link prediction and knowledge graphs.",
            "We use it here.",
            "We can get some significant improvement, however there is still a far way to go.",
            "The detection classification of multiple axis is not perfect and as we saw in some of these examples also the priority is not always.",
            "The best choice, however, in average we got a clear improvement.",
            "Also some on some examples.",
            "Hope I could show that it brings a significant advantage.",
            "And finally for future work there is also the possibility to include richer ontologies, for example from word NET or something.",
            "So one could do that either directly by taking the triples into the tensor, which we decompose, or building a system on top like building type constraints on top.",
            "Or something like that.",
            "So."
        ],
        [
            "Thank you very much for your attention.",
            "Question about how did you construct the ontology in the beginning?",
            "So was it just from the data set?",
            "So yeah, so in this case we wanted to show that we can just take the data sets of the training data set we have there, like I don't know 50 occurrences of person riding a motorbike, right?",
            "So we can take the 50 in the into the 10s alright and we'll just count, right?",
            "But this would not be enough, right?",
            "Otherwise you get a pretty poor prior for triples, which you have never never seen.",
            "So that's why we use this factorization approach to generalized also to the unseen triples, right?",
            "But yes, it's only from the training data.",
            "OK, because I would assume that if you take external ontologies, often those concepts that you need in those predicates that you need, like spatial relations are not covered in ontologies.",
            "OK?",
            "Yeah, that's I think this matching is might be a problem.",
            "Yeah, this side, so I'm very interested in in the kind of using semantics as a prior.",
            "Onto learning can you?",
            "Could you use some more complex semantics such as you know cardinality constraints and other things like that?",
            "Do you think that will be possible?",
            "Like you know a person has two legs or something like that?",
            "Yeah, and so with the current model this is not possible because here basically the triples are independent.",
            "So as we saw in the motorcycle, it basically predicted 3 views right, whereas the third wheel is not is not possible right?",
            "So we thought about concepts of how to improve that, but this would be like.",
            "Really a completely new model which hasn't been there, like in the probabilistic factorization world, hasn't been there.",
            "What we use here are the models from from the Knowledge Graph typically missing link prediction knowledge graphs, right?",
            "So these are the existing models and they work quite well for some of the simpler concepts, but not for the higher more complex concepts.",
            "My question is about the following, so from each image you can possibly extract different triples.",
            "For example what you have shown like 2 persons person next to another person.",
            "You can possibly also extract triple person on snow or something like this.",
            "So in this regard I have two questions.",
            "First of all, is there any control so that I can say that I'm particularly interested in this predicate or in this object and another one about evaluations?",
            "If you ask a person to say OK, what would you extract from this image and she just says some triple and then you compare but with what has been extracted.",
            "Or if you ask a person, OK, this has been extracted.",
            "Does it hold on the image?",
            "The evaluation results would be different, right?",
            "So yeah, yeah, so there is a lot of subtleties involved in the evaluation.",
            "As you correctly stated.",
            "So in this case it was like predicting everything we detect and then comparing to what is labeled as correct, right?",
            "But one problem might also be that the labeled data is not like it is not everything is labeled it right?",
            "So there are some images are very noisy, lot of stuff in there.",
            "And so there is a problem.",
            "So for this evaluation we took the same evaluation method like they did in the previous paper, right?",
            "But yeah, of course there is like a lot of subtleties.",
            "You have to think of and how to interpret the results correctly.",
            "But for for a use case for a real use case, you of course can.",
            "I mean, if you as a human expert want to evaluate, you can like filter the triples however you want them to be filters right so?",
            "Yep."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah I want to present today.",
                    "label": 0
                },
                {
                    "sent": "I will work on improving visual relationship detection using a semantic modeling of scene descriptions and this is work which I did together with my colleague in Puma and my PhD supervisor, Professor, Doctor for Chris.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this work is about detecting triples from images.",
                    "label": 0
                },
                {
                    "sent": "So if we look at this image for example, we can see multiple objects.",
                    "label": 0
                },
                {
                    "sent": "For example, the person.",
                    "label": 0
                },
                {
                    "sent": "There's a motorbike, there is a wheel, and so on, and these visual concepts.",
                    "label": 0
                },
                {
                    "sent": "Then in some relationship to each other.",
                    "label": 0
                },
                {
                    "sent": "So for example, the person is on the motorcycle, the person is wearing a helmet, the motorcycle has a veal.",
                    "label": 0
                },
                {
                    "sent": "And the person is wearing a jacket, so it is not only about detecting what is on the image, but also what is the relationship between these concepts in the particular image.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the goal is to build a system where you input an image and then you output list of triples which describe basically the scene.",
                    "label": 1
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "As you can probably, I mean as it is easy to see the relationship between the object that is very important for really capturing the semantics of the scene and there is not has not been so much work on that one major related in prior work is by Lou ET al from the stand for Computer Vision Group from 2016 and they also were the first who proposed a comprehensive data set for that task and we will also use this data set in our experiments.",
                    "label": 0
                },
                {
                    "sent": "So there are a number of.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then just to this problem.",
                    "label": 0
                },
                {
                    "sent": "So first of all object detectors.",
                    "label": 0
                },
                {
                    "sent": "Work quite well, but so there has been a lot of improvement, but they're still not perfect and they provide us with chest with probabilities of what we see in an image.",
                    "label": 0
                },
                {
                    "sent": "And then if you build triples.",
                    "label": 0
                },
                {
                    "sent": "You have a combinatorial complexity, so if you have our relationships and E possible visual concepts, then the number of possible troubles becomes our times E * E, which scales really heavily.",
                    "label": 1
                },
                {
                    "sent": "If you like, have a high number of visual concepts.",
                    "label": 0
                },
                {
                    "sent": "And the more visual concepts you have, the more likely it is that there are triples in your training set which you have never seen, so that you have surf triples in the test set which you have never seen you in the training data.",
                    "label": 0
                },
                {
                    "sent": "For example, you might have seen a lot of images where a person is riding a horse.",
                    "label": 0
                },
                {
                    "sent": "But you never saw a person riding an elephant, but still you want to classify correctly.",
                    "label": 0
                },
                {
                    "sent": "You might have seen persons you might have seen elephants.",
                    "label": 0
                },
                {
                    "sent": "You might have seen the relationship right in various contexts, but not the particular triple person ride elephants.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach to this problem is the following.",
                    "label": 0
                },
                {
                    "sent": "Basically, the three components.",
                    "label": 0
                },
                {
                    "sent": "The first one is we build a probabilistic model which captures the prior knowledge.",
                    "label": 1
                },
                {
                    "sent": "The probabilistic prior knowledge about the semantics, the visual for example, capture that it is more likely that the person is riding a motorbike, then a person flying motorbike or something which wouldn't make sense right?",
                    "label": 0
                },
                {
                    "sent": "So there is some world knowledge which we have and this should be captured by this probabilistic prior.",
                    "label": 0
                },
                {
                    "sent": "Then the second part is a computer vision pipeline.",
                    "label": 0
                },
                {
                    "sent": "Which detects objects and classifieds them so that we know OK there is a person.",
                    "label": 0
                },
                {
                    "sent": "There is a motorbike and so on.",
                    "label": 0
                },
                {
                    "sent": "An for this second part we basically use the same the same techniques as they did in the previous paper by Louis.",
                    "label": 0
                },
                {
                    "sent": "And then finally we have scores from our semantic prior an from our computer vision pipeline and we combine them together in a probabilistic graphical model.",
                    "label": 1
                },
                {
                    "sent": "So that's the third step.",
                    "label": 0
                },
                {
                    "sent": "So how?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does this prior look like so you can think of it as a fully connected graph first, where each node has a probability or each relationship between the between two concepts has a probability of discovering in the real world, and then this graph which we see here only shows the ones which have a certain probabilities.",
                    "label": 0
                },
                {
                    "sent": "So above some threshold.",
                    "label": 0
                },
                {
                    "sent": "So for example there is some probability that a person is riding a motorcycle, that motorcycle has uvela handlebar, so this doesn't mean it has always.",
                    "label": 0
                },
                {
                    "sent": "Properties, but like you discover this often in your data set.",
                    "label": 0
                },
                {
                    "sent": "So you maybe discover that the bicycle and motorcycle are pretty similar, so they appear in the same area like urban areas.",
                    "label": 0
                },
                {
                    "sent": "They have similar parts like a villain Handlebar and you also see Lofton person riding on motorbikes, but maybe you never saw a person riding a bicycle, so this edge will get a very low prior right.",
                    "label": 0
                },
                {
                    "sent": "However, you can infer from the structure that as motorbike and bicycle are very similar, maybe this edge person riding a bicycle should also get a boost and should also have some.",
                    "label": 0
                },
                {
                    "sent": "A reasonable price likelihood of being true, and so in order to achieve this, we use methods which have been mainly used for missing link prediction and knowledge graphs and so with this we want to generalize to the these missing links and adjust the probabilities in the whole graph.",
                    "label": 0
                },
                {
                    "sent": "So how do we do that?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That we construct the tensor, which is the three value error, basically, which represents accounts of how often we sorted triple in the training data, and then so many of them are zero.",
                    "label": 1
                },
                {
                    "sent": "For example, a person riding a bicycle with zero here, right?",
                    "label": 0
                },
                {
                    "sent": "But this is not the right prior which we want to use for our model.",
                    "label": 1
                },
                {
                    "sent": "So what we do is we do a low rank tensor decomposition and there are for example here in the slide you see the rest call factorization, which makes the approximation for this tensor.",
                    "label": 0
                },
                {
                    "sent": "And captures in it's latent factors kind of the underlying structure of that 3 dimensional tensor.",
                    "label": 0
                },
                {
                    "sent": "And there is not only the rest call decompositions, there are other ones such as this multi complex and the multiple neural network which we use in our experiments.",
                    "label": 0
                },
                {
                    "sent": "The least methods come from missing link prediction and knowledge graphs that have been successfully used in there and we now apply to this little bit different setting.",
                    "label": 0
                },
                {
                    "sent": "So we have to change the cost function a little bit.",
                    "label": 1
                },
                {
                    "sent": "We have to go to a post or cost function for modeling discount data because in the knowledge Graph typically of zeros and ones and here now we have commentator as entries in that answer and then we want to transfer the scores which we get out from the approximated tensor.",
                    "label": 1
                },
                {
                    "sent": "Get it probability distribution and that is just by normalizing them with the ball spun distribution.",
                    "label": 0
                },
                {
                    "sent": "So the output of this semantic model is finally a probability distribution over there.",
                    "label": 0
                },
                {
                    "sent": "Suppose so this should now capture that, for example, the probability for person riding motorbike is relatively high person riding bicycle as well.",
                    "label": 0
                },
                {
                    "sent": "We generalize to that, but for example, person flying motorbike wouldn't make much sense right?",
                    "label": 0
                },
                {
                    "sent": "So this is now in this prior distribution over all the triples.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So second, we need a visual model in order to detect what is actually on the image.",
                    "label": 1
                },
                {
                    "sent": "Therefore we use a region, CNN region, convolutional neural network, which is a standard approach to get like salient objects in the image.",
                    "label": 0
                },
                {
                    "sent": "Then we take these areas which we find in the objects and classify them with a convolutional neural network.",
                    "label": 1
                },
                {
                    "sent": "In order to say OK, this is a surfer and surfboard or a person a surfboard.",
                    "label": 0
                },
                {
                    "sent": "And then we always take a combination of two.",
                    "label": 0
                },
                {
                    "sent": "The of these boxes take the union of them and build another computer convolutional neural network, which predicate predicts the predicate.",
                    "label": 0
                },
                {
                    "sent": "So for example, next tool in this case.",
                    "label": 0
                },
                {
                    "sent": "So for a pair of regions we now get a probability distribution over what is the subject.",
                    "label": 0
                },
                {
                    "sent": "Or probability distribution over what is the object and the probability distribution of what is the predicate between them?",
                    "label": 0
                },
                {
                    "sent": "So this is the output of the visual model and this visual model is basically the same as has been as it has been used in the prior work I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "OK, now we have this from the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Visual model so given given a pair of of regions which we denote by I and we have this prior which we get from the factorization model and we want to combine these probabilities and we do this in a probabilistic graphical network with the following structure.",
                    "label": 0
                },
                {
                    "sent": "So here is the structure of the network, so it is.",
                    "label": 0
                },
                {
                    "sent": "It implies basically a conditional independence of the areas.",
                    "label": 1
                },
                {
                    "sent": "Given this the espoz and what we want to do.",
                    "label": 0
                },
                {
                    "sent": "But we wanted to predict is this probability.",
                    "label": 0
                },
                {
                    "sent": "So what is the correct triple?",
                    "label": 0
                },
                {
                    "sent": "Given a pair of.",
                    "label": 0
                },
                {
                    "sent": "Bounding boxes and their union and we can divide that by applying the base rule.",
                    "label": 0
                },
                {
                    "sent": "So this is a typical approach being also done in Hmm's often so hidden Markov models and so finally we can combine the score which we get from the prior and the visual score.",
                    "label": 1
                },
                {
                    "sent": "Combine it in this prediction.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we did some experiments on the Stanford visual relationship detection data set.",
                    "label": 1
                },
                {
                    "sent": "This is contains 4000 images and there are 100 different visual objects or concepts and 70 relationships between them and the relationships are such As for example, has part or where.",
                    "label": 1
                },
                {
                    "sent": "Then there are things like spatial relations like on or next to comparative relations like taller than or smaller than and also actions like riding a bicycle or kicking a ball.",
                    "label": 0
                },
                {
                    "sent": "And we do multiple evaluation settings.",
                    "label": 0
                },
                {
                    "sent": "In order to like evaluate the whole pipeline and see like where which part works.",
                    "label": 0
                },
                {
                    "sent": "English doesn't work so nice.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we got some results.",
                    "label": 0
                },
                {
                    "sent": "We compare against only the visual part, which is the same as in the in this prior work and against the baseline or benchmark model.",
                    "label": 0
                },
                {
                    "sent": "Also from the Stanford Computer Vision Group.",
                    "label": 0
                },
                {
                    "sent": "They also need use a prior information, but what they use is a language prior so they take word embeddings from the triple and then they look how similar these word embeddings.",
                    "label": 0
                },
                {
                    "sent": "So for example person riding a bicycle.",
                    "label": 0
                },
                {
                    "sent": "I don't know has similar word embeddings so it get boosted.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so at the bird embeddings are derived from a large corpus of text based on Co occurrence.",
                    "label": 0
                },
                {
                    "sent": "Basically and then.",
                    "label": 0
                },
                {
                    "sent": "So we compare this with our method where we use a prior the rest of the multiple networks are complex decomposition and this multi composition and basically what we see is that except for this mode all the all the priors which we use work better than the language prior used in the original paper and but still there.",
                    "label": 1
                },
                {
                    "sent": "Correct object prediction object detection, so the vision part is a big bottleneck there.",
                    "label": 1
                },
                {
                    "sent": "Often it fails in this stage.",
                    "label": 0
                },
                {
                    "sent": "And finally we can conclude that from all of these methods which we tried, the complex factorization gives the best results on this task.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some examples which have been classified correctly.",
                    "label": 0
                },
                {
                    "sent": "So for example there was a person extra person, so it detects both and there is a relationship between them or there is a truck on the road.",
                    "label": 0
                },
                {
                    "sent": "It is also correctly predicted, but here you also see that, for example, this detection of the of the objects is not going really well, but still we end up with the right triple.",
                    "label": 0
                },
                {
                    "sent": "So we somehow find that it's a truck.",
                    "label": 0
                },
                {
                    "sent": "Here it's a Rd here and it has a high prior so we get this triple, but it's not always works that well.",
                    "label": 0
                },
                {
                    "sent": "For example in this image.",
                    "label": 0
                },
                {
                    "sent": "You see that this green rectangle chest doesn't.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it doesn't show any real object, so also we end up with a bad prediction lamp on box, which doesn't make sense and sometimes it's like that.",
                    "label": 0
                },
                {
                    "sent": "So we get the right objects.",
                    "label": 0
                },
                {
                    "sent": "We find that we find the bicycle and we find the wheel, but we send a OK.",
                    "label": 0
                },
                {
                    "sent": "This motorcycle has this wheel and that's not true.",
                    "label": 0
                },
                {
                    "sent": "It's a view from the other motorcycle right?",
                    "label": 0
                },
                {
                    "sent": "And this also shows another problem.",
                    "label": 0
                },
                {
                    "sent": "So also the algorithm correctly detected the two wheels of the red one.",
                    "label": 0
                },
                {
                    "sent": "It doesn't know that the motorcycle can only have two wheels.",
                    "label": 0
                },
                {
                    "sent": "Another third one, right?",
                    "label": 0
                },
                {
                    "sent": "So this kind of prediction is not possible with our current model, so this is an open challenge to bring this into the into a probabilistic factorization model.",
                    "label": 0
                },
                {
                    "sent": "Or maybe one can apply like additional rules on top in order to get this kind of behavior.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we also looked at so called zero shot learning.",
                    "label": 0
                },
                {
                    "sent": "Here we look at triples which we really have never seen in the training set.",
                    "label": 0
                },
                {
                    "sent": "So anyways, the test images are new images, right?",
                    "label": 0
                },
                {
                    "sent": "So they are different from the training images, but sometimes we have seen already a person extra person, different person maybe, but still that rippled person extra person we have seen.",
                    "label": 0
                },
                {
                    "sent": "But there are some we have never seen and hear the we discovered that the this prior this probabilistic prior really brings an advantage by generalizing to this unseen triples.",
                    "label": 0
                },
                {
                    "sent": "And we can also conclude that here the multiple neural networks in this case.",
                    "label": 0
                },
                {
                    "sent": "The best results and so here also.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2 examples of that.",
                    "label": 0
                },
                {
                    "sent": "So in the test set in the training set, there was never a bus.",
                    "label": 0
                },
                {
                    "sent": "Next bus, never tree behind a bear, never laptop on the stove, and never a bear riding a motorcycle, and we looked at the different methods.",
                    "label": 0
                },
                {
                    "sent": "How do they capture these kind of unseen triples and the visual one captures the right one.",
                    "label": 0
                },
                {
                    "sent": "The next bus next bus it it captures, but the other one.",
                    "label": 0
                },
                {
                    "sent": "Somehow it makes mistakes.",
                    "label": 0
                },
                {
                    "sent": "The language prior works well for the bus next to the bus and the tree behind the bear.",
                    "label": 0
                },
                {
                    "sent": "So here you can see a tree bear.",
                    "label": 0
                },
                {
                    "sent": "These are like similar concepts.",
                    "label": 0
                },
                {
                    "sent": "They often appear like jungle bear, you know.",
                    "label": 0
                },
                {
                    "sent": "So a language program might help here to have a high prior, but for example, a laptop on a stove that doesn't match make much sense, right?",
                    "label": 0
                },
                {
                    "sent": "So the language prior fails in this case.",
                    "label": 0
                },
                {
                    "sent": "Various semantic prior also captures the laptop on the stove.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because it learns that laptops are typically on something, so it makes sense they can be on everything on a bed, on the table and that things are typically on the stove, right?",
                    "label": 0
                },
                {
                    "sent": "So that it recognizes that on is a very general relationship and that laptop in stove both serve as a subject and object respectively.",
                    "label": 0
                },
                {
                    "sent": "So we can classify laptop on the stove, get a high prior.",
                    "label": 0
                },
                {
                    "sent": "Even so, we have never seen it in the training set.",
                    "label": 0
                },
                {
                    "sent": "And the last one is a pair riding a motorcycle.",
                    "label": 0
                },
                {
                    "sent": "So here also our prior obviously fails, right?",
                    "label": 0
                },
                {
                    "sent": "Because this is.",
                    "label": 0
                },
                {
                    "sent": "To some extent, that's what we want, right?",
                    "label": 0
                },
                {
                    "sent": "But this also shows somehow with the weakness of the of the prior.",
                    "label": 0
                },
                {
                    "sent": "So to some extent, of course we see what we expect, right?",
                    "label": 0
                },
                {
                    "sent": "And we will never expect the bear on a motorcycle, so it fails in these cases.",
                    "label": 0
                },
                {
                    "sent": "However, what our system gives us is a person riding a motorcycle, which is also not that bad.",
                    "label": 0
                },
                {
                    "sent": "However, it is labeled as a bear riding a motorcycle, right?",
                    "label": 0
                },
                {
                    "sent": "So here you have a tradeoff between the probabilities like a classifier gives here high probability for bear, but also high probability for person.",
                    "label": 0
                },
                {
                    "sent": "And then it's about probabilistic graphical model.",
                    "label": 0
                },
                {
                    "sent": "How to combine it?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, with that, let me conclude people.",
                    "label": 0
                },
                {
                    "sent": "A model which can extract triples from images that ripples, capture complex semantics of the image which can be used in an image database for search, querying, and so on.",
                    "label": 1
                },
                {
                    "sent": "We could show with our approach by including this probabilistic prior where we use methods which have been used for missing link prediction and knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "We use it here.",
                    "label": 1
                },
                {
                    "sent": "We can get some significant improvement, however there is still a far way to go.",
                    "label": 0
                },
                {
                    "sent": "The detection classification of multiple axis is not perfect and as we saw in some of these examples also the priority is not always.",
                    "label": 0
                },
                {
                    "sent": "The best choice, however, in average we got a clear improvement.",
                    "label": 0
                },
                {
                    "sent": "Also some on some examples.",
                    "label": 0
                },
                {
                    "sent": "Hope I could show that it brings a significant advantage.",
                    "label": 0
                },
                {
                    "sent": "And finally for future work there is also the possibility to include richer ontologies, for example from word NET or something.",
                    "label": 0
                },
                {
                    "sent": "So one could do that either directly by taking the triples into the tensor, which we decompose, or building a system on top like building type constraints on top.",
                    "label": 0
                },
                {
                    "sent": "Or something like that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much for your attention.",
                    "label": 1
                },
                {
                    "sent": "Question about how did you construct the ontology in the beginning?",
                    "label": 0
                },
                {
                    "sent": "So was it just from the data set?",
                    "label": 0
                },
                {
                    "sent": "So yeah, so in this case we wanted to show that we can just take the data sets of the training data set we have there, like I don't know 50 occurrences of person riding a motorbike, right?",
                    "label": 0
                },
                {
                    "sent": "So we can take the 50 in the into the 10s alright and we'll just count, right?",
                    "label": 0
                },
                {
                    "sent": "But this would not be enough, right?",
                    "label": 0
                },
                {
                    "sent": "Otherwise you get a pretty poor prior for triples, which you have never never seen.",
                    "label": 0
                },
                {
                    "sent": "So that's why we use this factorization approach to generalized also to the unseen triples, right?",
                    "label": 0
                },
                {
                    "sent": "But yes, it's only from the training data.",
                    "label": 0
                },
                {
                    "sent": "OK, because I would assume that if you take external ontologies, often those concepts that you need in those predicates that you need, like spatial relations are not covered in ontologies.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's I think this matching is might be a problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this side, so I'm very interested in in the kind of using semantics as a prior.",
                    "label": 0
                },
                {
                    "sent": "Onto learning can you?",
                    "label": 0
                },
                {
                    "sent": "Could you use some more complex semantics such as you know cardinality constraints and other things like that?",
                    "label": 0
                },
                {
                    "sent": "Do you think that will be possible?",
                    "label": 0
                },
                {
                    "sent": "Like you know a person has two legs or something like that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, and so with the current model this is not possible because here basically the triples are independent.",
                    "label": 0
                },
                {
                    "sent": "So as we saw in the motorcycle, it basically predicted 3 views right, whereas the third wheel is not is not possible right?",
                    "label": 0
                },
                {
                    "sent": "So we thought about concepts of how to improve that, but this would be like.",
                    "label": 0
                },
                {
                    "sent": "Really a completely new model which hasn't been there, like in the probabilistic factorization world, hasn't been there.",
                    "label": 0
                },
                {
                    "sent": "What we use here are the models from from the Knowledge Graph typically missing link prediction knowledge graphs, right?",
                    "label": 0
                },
                {
                    "sent": "So these are the existing models and they work quite well for some of the simpler concepts, but not for the higher more complex concepts.",
                    "label": 0
                },
                {
                    "sent": "My question is about the following, so from each image you can possibly extract different triples.",
                    "label": 0
                },
                {
                    "sent": "For example what you have shown like 2 persons person next to another person.",
                    "label": 0
                },
                {
                    "sent": "You can possibly also extract triple person on snow or something like this.",
                    "label": 0
                },
                {
                    "sent": "So in this regard I have two questions.",
                    "label": 0
                },
                {
                    "sent": "First of all, is there any control so that I can say that I'm particularly interested in this predicate or in this object and another one about evaluations?",
                    "label": 0
                },
                {
                    "sent": "If you ask a person to say OK, what would you extract from this image and she just says some triple and then you compare but with what has been extracted.",
                    "label": 0
                },
                {
                    "sent": "Or if you ask a person, OK, this has been extracted.",
                    "label": 0
                },
                {
                    "sent": "Does it hold on the image?",
                    "label": 0
                },
                {
                    "sent": "The evaluation results would be different, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, yeah, so there is a lot of subtleties involved in the evaluation.",
                    "label": 0
                },
                {
                    "sent": "As you correctly stated.",
                    "label": 0
                },
                {
                    "sent": "So in this case it was like predicting everything we detect and then comparing to what is labeled as correct, right?",
                    "label": 0
                },
                {
                    "sent": "But one problem might also be that the labeled data is not like it is not everything is labeled it right?",
                    "label": 0
                },
                {
                    "sent": "So there are some images are very noisy, lot of stuff in there.",
                    "label": 0
                },
                {
                    "sent": "And so there is a problem.",
                    "label": 0
                },
                {
                    "sent": "So for this evaluation we took the same evaluation method like they did in the previous paper, right?",
                    "label": 0
                },
                {
                    "sent": "But yeah, of course there is like a lot of subtleties.",
                    "label": 0
                },
                {
                    "sent": "You have to think of and how to interpret the results correctly.",
                    "label": 0
                },
                {
                    "sent": "But for for a use case for a real use case, you of course can.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you as a human expert want to evaluate, you can like filter the triples however you want them to be filters right so?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        }
    }
}