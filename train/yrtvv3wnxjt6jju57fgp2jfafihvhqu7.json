{
    "id": "yrtvv3wnxjt6jju57fgp2jfafihvhqu7",
    "title": "Reinforcement Learning Theory",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_langford_rlt/",
    "segmentation": [
        [
            "Hi.",
            "So we can talk about reinforcement learning.",
            "There's a little bit of backing material available here if you're interested in taking a look at it."
        ],
        [
            "OK, so I guess the first question is what is reinforcement learning?",
            "And.",
            "Reinforcement learning is probably the most general form of learning problem.",
            "Anybody tries to solve.",
            "So yesterday I talked about classification in various forms of supervised learning.",
            "Is also something known as semi supervised learning where people have in addition to bunch of unlabeled data that they tried to learn from and then.",
            "There's the simplest form of reinforcement learning, which is typically reinforcement learning Markov decision process.",
            "And then there's the general form of reinforcement learning which contains everything.",
            "So.",
            "It is a question is why is it important to study reinforcement learning?",
            "Yes, if you face any learning problem in the real world.",
            "You can ask the question, is this a reinforcement learning problem?",
            "And it turns out the answer is always yes.",
            "So asking this question is maybe not so interesting, but the point here is that if we have any kind of theory for reinforcement learning, then it can apply very generally to all kinds of problems, right?",
            "Now it turns out that the kind of things that we can prove for reinforcement learning are not as strong as you might hope, because reinforcement learning is incredibly general.",
            "Nevertheless, you can prove some things and.",
            "Understanding these things can help you understand how to get solutions to individual problems.",
            "So when you face some problem in the real world.",
            "You start out by asking yourself, is this reinforce morning and you know the answer is yes when you start asking yourself is this something that's simpler than the full reinforcement learning problem?",
            "So the goal typically instead of narrowing down what problem you have, is.",
            "You start out with general reinforcement learning, and then often you discover that you know.",
            "Maybe maybe this is some sort of similar problem.",
            "There are other problems as well, which aren't up here.",
            "And then once you discover exactly how simple you can make the problem, then you can try to attack it.",
            "OK."
        ],
        [
            "Um?",
            "By the way, questions are good, so you feel free to ask questions today.",
            "IS.",
            "OK, yeah, these are actually pretty interesting versions of learning.",
            "So in armed bandits what happens is.",
            "You have, it's like you go to the casino and you have a bunch of machines that you can put money into and you can pull an arm right?",
            "And some of the arms pay off more than other arms.",
            "But you don't know which pays off the most in advance, right?",
            "So the goal in our vantage learning it should try to do as well as the best individual arm by exploring in some according to some schedule.",
            "Active learning is a different setting, so in general reinforcement learning what happens is you're going to interact with an environment and the process of interacting with environment changes what we're going to observe in the future.",
            "And active learning, that's not true, but you are actively interacting with the environment, so the idea is maybe you want to do classification.",
            "So you start requesting labels for individual examples.",
            "And then at some point after you get some labels, you produce a classifier.",
            "So active learning is sort of the interactive version of this classification.",
            "Learning, or just supervised learning.",
            "Yeah, yeah.",
            "So this is one of those very basic things.",
            "It seems that.",
            "Interaction, the ability to interact with the world is radically more powerful than just the ability to do batch learning.",
            "As we talked about yesterday.",
            "So what I mean by radically more powerful what I mean is the number of labels that you may need to request can be a log arhythmic in the number that are required to learn.",
            "If you're just using batch learning like we talked about yesterday.",
            "It's not always log rhythmic, example of where it might be log rhythmic is suppose you you just trying to learn to predict a threshold on a line right?",
            "So then?",
            "And let's say there's no noise that would be very easy to.",
            "First query in the middle to see if it's if the threshold is above or below that the query point.",
            "Then that partitions the set and then you can just recursively query just narrowing in very quickly on where the King threshold is.",
            "For that specific problem, it's not always log rhythmic, it is just that sometimes it is and it's very enticing when it's true.",
            "Alright, so I think I'm going to talk about active learning a little bit later.",
            "For the research talk.",
            "But today we're mostly just going to talk about Markov decision process, reinforcement learning and reinforcement learning.",
            "Right, I think I already said this.",
            "That was in our problem.",
            "It's often something simpler than RL and you want to identify how it's simpler than the full RL problem.",
            "Hopefully when you do that, you'll discover that somebody already studied that particular setting, and then you can use whatever approaches."
        ],
        [
            "I've developed.",
            "Alright, so I'll just start by telling you this is this is modern reinforcement learning.",
            "Right, so if you go back to the title is modern reinforcement.",
            "Learning theory is quite a bit of reinforcement learning theory, which is happened before this afternoon.",
            "Talk about here I'm talking about here is things that I think have happened in the last seven years or so.",
            "So the first kind of reinforcing tape that people have been worrying about recently is sample complexity in reinforcement learning.",
            "So sample complexity is how many times do you need to interact with your environment in order to do something interesting something useful."
        ],
        [
            "Who?",
            "Guess what the same place to guarantee?",
            "We're going to think about making theorem statements of the form with high probability given some number of samples.",
            "Some guarantee is going to hold, and I'll fill in what that guarantee is in a minute.",
            "So we're going to be worrying about reinforcement learning, Markov decision process.",
            "Perhaps you should have defined a market position process, but let me define it for you.",
            "So the fundamental quantity in a Markov decision process is next state distribution.",
            "OK, so we're going to have states.",
            "States are something that the world imposes.",
            "We're going to have actions.",
            "We're going to have some distribution over the next state given the action in the state, and then we're going to have a policy which given state.",
            "Produces.",
            "Produces an action.",
            "So I'm kind of curious if we can run a test.",
            "What happens to point the camera at the camera's output?",
            "Oh OK well.",
            "OK, so in Markov decision process we have some state.",
            "The state summarizes all the information we need in order to make a good decision.",
            "So that's kind of the fundamental assumption, right?",
            "You know the state summarizes everything and then based on that state we make some sort of decision.",
            "Are there questions about that?",
            "Like Sam mentioned, this kind of model briefly yesterday.",
            "In general, in the real world, maybe you don't have.",
            "You don't exactly know what your state is.",
            "There are things like.",
            "OK, so when you're sort of physically moving around, if you can't see things because you're blindfolded, then you're not in a Markov decision process.",
            "Right?",
            "But let's pretend that we are in a market position process.",
            "So whatever we see is sufficient to tell us how to move around, which is maybe true.",
            "Some reasonable portion of the time.",
            "OK, so the fundamental quantities are going to be fundamental parameters.",
            "So S will be the number of states.",
            "It will be the number of actions per state.",
            "T is going to be horizon time, so when people are doing reinforcement learning very often.",
            "To we want to be concerned not just with what happens at the next step, but what happens?",
            "T steps into the future.",
            "Sometimes people instead of saying I care about the next steps, they say I care about this step.",
            "Some care for the next step, a little bit less in the next step, a little bit less.",
            "Next step, little bit less, next little bit less where they quantify a little bit less in terms of gamma.",
            "Instead, we're just going to work with T because it's a little bit easier mathematically, but all the results that we talk about today kind of hold if you use gamma discounting.",
            "Sometimes we're going to switch from Markov decision processes to palm DP's so partially observable Markov decision processes like so.",
            "If you're blindfolded and then you're wandering around, you know you can't see anything, but you can still feel and then based on what you feel, you can decide what to do.",
            "And then we're going to have a precision parameter, so I guess.",
            "Having as a goal learning exactly what the optimal policy is, what policy will give you the largest sum of rewards is a little bit implausible, because it just can't be done in a reasonable sense, so our goal is going to be something like we're going to try to find it in a policy which is within epsilon of the optimal policy."
        ],
        [
            "OK, so there's a specific result which is actually quite interesting an.",
            "I'm planning to go through this result in full detail.",
            "This resolves is the EQ guarantee so Y cubed is an algorithm which stands for explicit, explore or exploit.",
            "Uh.",
            "So we're going to there's a question about what kind of information access do we have to the world?",
            "And we're going to imagine that the information that we have to the world is exactly what we have is an agent wandering around in the world.",
            "So as an agent wandering around the world, you know we have some state that we see, and then we can choose some action and then we observe what happens next, right?",
            "And then we can choose some action we observe what happens next, so in particular.",
            "So some people are doing reinforcement learning.",
            "They think about other sorts of interactions, other ways to interact with the world.",
            "So think about generative models where they can say suppose that I'm over here and I take this action.",
            "What happens next?",
            "And then suppose that I'm here and take this action instead.",
            "What happens next?",
            "What we're not doing that instead?",
            "We're just going to be.",
            "It's just as if you are the reinforcement learning agent.",
            "So you can imagine using the EQ dollar Rhythm to to decide how much you're going to explore exploit.",
            "So the way this works is.",
            "We have a current state S that we know we choose some action and then we observe the next state and also a reward.",
            "So the goal and reinforcement learning.",
            "So in addition to having the distribution.",
            "Over the next eight, given the action in the state, we're going to have.",
            "Some kind of reward?",
            "Which may be a function of.",
            "In general, is triple.",
            "Yes.",
            "So, are there questions about this setting?",
            "'cause now is a good time to think about.",
            "Is the setting makes sense?",
            "I don't want to claim that this is the rightmost setting, and actually, if you find a problem with a particular setting in machine learning, you should in general think about how you want to fix it, because it's not clear yet that we have the right settings.",
            "Alright, so.",
            "We had this precision process which has some set of states, some actions per state.",
            "And this distribution over the next state, given the state in the action, we're going to have some horizon T. And then the question is, what is this pie?",
            "What is this thing that's going to hold with high probability, right?",
            "So a sample complexity guarantee has some sort of statement, like after some number of samples, some guarantee is going to hold.",
            "And the question is, what is that guarantee?",
            "So there's a guarantee.",
            "In the original EQ paper.",
            "Which is something like if you.",
            "Additionally, assume that the Markov decision process is a mixing time.",
            "Then after some number of samples related to.",
            "The number of states, number of actions per state, the mixing time in one over epsilon.",
            "You can act within epsilon available policy.",
            "So what is missing time I guess is the question right?",
            "I don't want to tell you so.",
            "Instead I'm going to tell you a simpler.",
            "A theoretical statement which is going when you add in the mixing time assumption will give you back the original.",
            "So the modified form is going to be.",
            "Suppose that you have.",
            "After the bottom part of the theorem is going to be after some number of samples which is polynomial in the number of states, the number of actions per state, the time horizon in one over epsilon.",
            "You can act epsilon optimally 40 time steps.",
            "Roughly speaking, and mixing assumption just means that.",
            "You're assuming that no matter what you do, you end up where you were.",
            "I actually like the second form of this statement, abit more because this applies in a deterministic world.",
            "Well this only applies in the sarcastic world.",
            "Alright."
        ],
        [
            "So here's the theorem statement.",
            "The claim is there is an algorithm E cubed such that for every Markov decision process.",
            "Parameterized like so.",
            "With rewards in 01 with probability 1 minus Delta.",
            "Uh.",
            "For all except a polynomial number of steps.",
            "I think there's.",
            "I think I have failed you.",
            "I need to define a couple more things.",
            "OK, so.",
            "One of the fundamental quantities in reinforcement learning is.",
            "Given that I'm in some particular state, what is my expected future summer rewards?",
            "Right, so the star.",
            "Of S. Oh it's going to be an expectation.",
            "Over state.",
            "Action reward triples.",
            "Drawn from.",
            "The MVP.",
            "In the policy, so we're going to have some policy.",
            "Vanessa markup decision Process market decision process is like so the policy is like so we put the two of them together.",
            "We're going to have a process which generates state action reward triples right?",
            "Then we're going to look at the sum.",
            "Of our.",
            "Summer from I = 1 to T of our sub T. This is the expected sum of rewards.",
            "Given that we're starting in S. Say again.",
            "Subscripts I see.",
            "Yeah, let me see.",
            "OK, so we're doing expectation.",
            "Over state action reward.",
            "Triples T of them.",
            "Drawn according to.",
            "The MVP.",
            "Starting.",
            "In S. And I.",
            "And we had to have the sum.",
            "From I = 1 to T of R sub T. So we have the market decision process.",
            "We're going to assume that we're starting in State S. We're going to act according to the policy.",
            "This is going to induce some distribution over States and rewards the first time step, and that's going to induce some distribution over States and rewards the second time step.",
            "The 3rd and the 4th, and so on.",
            "The goal is to maximize the sum of expected expected sum of rewards.",
            "What does?",
            "Pie stands for the policy, so the pot pie.",
            "\u03a0 Maps a state to an action.",
            "Mother.",
            "Do you need another probability distribution?",
            "So Pi is something that you control your choosing how you're going to respond to what you observe in the world.",
            "You could choose to respond in a randomized way.",
            "And in fact I will talk about that a little bit, but for the Y cubed algorithm, Pi is actually going to be deterministic.",
            "OK, so this is one other thing which is this Q value.",
            "So if I'm in some particular state in a committed taking some particular action.",
            "Then how do we do in the future?",
            "Oh, I forgot to mention something.",
            "So for that definition there so in general in reinforcement learning, if we decorate things with the star, we mean assuming that we act optimally.",
            "So when I say V star.",
            "There's something kind of strange here, because this index here is T minus little T MoD.",
            "T little T is just the iteration, the interaction number that we're at, so this is going to be some quantity varying between zero and capital T -- 1.",
            "Right so.",
            "Maybe I should.",
            "So this.",
            "T here.",
            "He's going to be very in between capital T. Or between zero and capital T -- 1.",
            "This is going to be kind of the horizon that we care about right now.",
            "So what I'm thinking about is sort of an episodic method of evaluating how well we're doing.",
            "So we want to try to do well in next time steps, and then we take an action and we want to do well in XT minus.",
            "One time steps were taken action and we want to do well next T -- 2 time steps and so forth, right?",
            "And until we get down to zero and then we start and we say OK, Now I want her to do well in next time steps T -- 1 and so forth.",
            "It seems a little bit funny, but.",
            "That's all that we know how to prove, actually.",
            "Alright, so.",
            "When you have a star, that means that this, I'm assuming that act optimally.",
            "And then there's also the Q value.",
            "Which in general is going to be dependent on some state in some action this is going to equal the expectation.",
            "Overstate action reward.",
            "Hot sexy.",
            "TV.",
            "During according to.",
            "The MVP.",
            "Assuming that I in state S and take action A and act according to policy \u03c0.",
            "So there's going to be some tea and some pie with parameterized Q.",
            "And then take the same sum.",
            "Over actions, there's some over.",
            "Interactions of reward.",
            "OK, so the V value is if I'm in some state, how well can I do the Q value is if I'm in some state and take some action, how well can I do?",
            "Right, so in general the Q value is going to be less than or equal to the V value for some particular state.",
            "Is the best you can do in some particular.",
            "State.",
            "Is the Max over the actions of the Q value for that state?",
            "Is \u03c0 defined before you start?",
            "No, we're going to be forming.",
            "We're going to be learning Pi as we go.",
            "Yeah, so the pie here is going to be the policy given by E ^3.",
            "So in particular, we're going to have some history of interaction with the world.",
            "That's going to determine how each cube is going to behave, so this is a policy Y cubed of age is going to choose some action given the state and the just given the history of interactions.",
            "And then we're going to assume that we act according to this policy in the future.",
            "And the claim is that when we act according to this learn policy, we're going to do as well as the best we could do.",
            "Minus epsilon.",
            "So we're going to behave within epsilon of optimal.",
            "Every big team.",
            "It's even done so.",
            "The form of this theorem statement says that suppose we have some infinitely long process of interacting with the world according to E. ^3.",
            "Then we're going to mark off some number of these interactions and we'll just.",
            "We'll throw them away, basically.",
            "The claim is that everywhere else will behaving we will be behaving within epsilon of optimal.",
            "Right, so this is going to be some initial boot up phase where we're kind of exploring around the world.",
            "We don't know what we're doing and then.",
            "And then.",
            "And then we're going to be behaving near optimally everywhere else.",
            "So it's worthwhile to think about what this means.",
            "Something kind of strangers can happen if you're.",
            "If you're wandering around in the world trying to explore.",
            "At some point you may be under the impression that you know how to exploit, and then you can just try to maximize your reward.",
            "Or your expected sum of rewards, and it may be that you end up, you know, following through a crack somewhere and we can set another portion of the world which you've never actually explored before.",
            "Then you have to go wander around in that portion of the world.",
            "And then the claim is that the total amount of wandering, no matter which world you are in.",
            "It's just bounded by.",
            "Something like the number of states, the number of actions, number, the time horizon, the precision parameter, and we're going to have a probability that this entire process fails.",
            "OK, so.",
            "What I could do is just go through this, go through with the algorithm is and then go through the proof."
        ],
        [
            "So if we have some market decision process then we have a set of states.",
            "We have a set of actions per state, so maybe we have.",
            "State One state, two State 3 and we have action one and action two and we have action one and action two and we have action one and Action 2.",
            "The outcome of these actions could be deterministic, so if you take some state in some action, you come to some particular state afterwards, well, they could be stochastic, so if he takes in some state, you take some action, maybe with probably .6, you end up here and probably point for you end up here.",
            "This sort of in general to a MDP looks like and then each of these transitions can be decorated with some reward, so maybe the only time you get reward for this MDP so when you're in state S2 and you take action one and then go to S3 you get reward 1, otherwise you get reward 0.",
            "You will find a policy that maximizes the reward, maximizes the sum of rewards to horizon T. From then on or.",
            "From then on, that's a really good point.",
            "Remember that because there's something.",
            "Very good about that point.",
            "OK.",
            "So we're going to.",
            "We have the full MVP, the true MVP, that the world is behaving according to.",
            "And then inside of the Y cubed algorithm, we're going to keep track of something called the known of HM DP and the known of HM.",
            "DP is going to be.",
            "Essentially, the world MDP, except we're only going to keep track of States and actions that we've experienced some number of times.",
            "So initially when we when our history is empty, we."
        ],
        [
            "Oh nothing right.",
            "And then maybe after a while we've transitioned.",
            "In times across this edge and in times across this edge, so this becomes the known of HMD P. And then there's kind of a problem with dangling pointers, right?",
            "'cause it may be that we've gone this way in times, meaning this state is known, but.",
            "This state could still be unknown, so we're going to think of all of the unknown states we're going to lump them all together, so we'll just have some unknown.",
            "And we're going to assume that if we go into the unknown state, then we have reward 0.",
            "So this is the this is internal to the Y cubed algorithm.",
            "They keep that rhythm is defines the known MDP given the history."
        ],
        [
            "So after you interacted bit more than maybe you learn.",
            "S2 this becomes your known MVP, then after you."
        ],
        [
            "Track a bit more.",
            "Maybe you learn everything now.",
            "One thing to note is that when you're trying to.",
            "Compute the probabilities of outcomes.",
            "You just counting according to your observations of what happens, right?",
            "That means that your estimate of the probability of going to state S1 will be different from the truth, because I guess if you haven't even coin, you flip the 100 times.",
            "You don't really expect to get exactly.",
            "50 hits right?",
            "It'll vary a little bit.",
            "OK, so this is an internal data structure to the algorithm, the known MDP."
        ],
        [
            "There's another MVP that we keep track of, which is the unknown in Dupee.",
            "So the unknown MDP is the same as the known indep, except we're going to rearrange the rewards.",
            "So if you go back to the known MDP.",
            "We just observe the rewards and then we compute the expected value of the observations to get these values right.",
            "What we're going to do for the unknown in Dupee you're going to set all the rewards to 0.",
            "Except when we transition into the unknown region.",
            "So let's go back a couple.",
            "It's not so good either try.",
            "This way.",
            "So the unknown.",
            "So for the H which produces this known of HM DP, the unknown of HMDP will be the same, except will assume that we get reward one here and we get reward 0 here.",
            "So in all of the known region in all traditions which transition in all traditions which go to known states we assume reward zero if it goes to an unknown state then we assume reward one.",
            "That's the only difference.",
            "So this is different from what you show.",
            "From known to unknown, you have.",
            "We want zero in your interest.",
            "Write algorithm.",
            "This is another MDP which is a data structure internal to the cubed algorithm.",
            "So the cube Darren is going to keep track of two MVP's.",
            "One MDP is the known of HM DP.",
            "Another MVP is the unknown of HMDP.",
            "The unknown of Asian DP is the same as the known of H MVP.",
            "57 set all rewards to 0.",
            "And then we set the rewards that we set the actions leading to unknown states to have reward one.",
            "This is to figure out the policy or whatever.",
            "What do we need to do the unknown?",
            "Why do we keep track of this?",
            "Brett we're going to use this to define the policy that the equilibrium is going to be taking.",
            "OK. Little bit tricky here because we have all these MDP's running around.",
            "It's important to understand that there's the MDP in the world.",
            "And there's these two MVP's which are internal to the algorithm and then just defined in terms of the history of interactions with the algorithm observes."
        ],
        [
            "OK, so the question is how do we use these MDP's?",
            "There's a fundamental operation that we're going to use them DPS.",
            "The fundamental operation is dynamic programming.",
            "So if we're given any MDP.",
            "And some particular start state some time horizon, then dynamic programming is going to tell us.",
            "What is the maximum expected reward over some time horizon?",
            "So remember we have this episodic notion of how good we're doing.",
            "And what is the action which achieves it?",
            "OK, so we have this routine we can call on any defined internal MDP and it will tell us which action should you take to try to maximize.",
            "Your sum over this length expected reward.",
            "And the question is, how do you actually compute this dynamic program?",
            "So maybe it's easy to just think about what happens if this here is 1, right?",
            "So all we're trying to do is maximize the one step reward right?",
            "So we're just trying to maximize one step reward.",
            "In the markup decision process, each action has some expected reward associated with it, so that will be returned by DP.",
            "Is the action with the maximum effect reward end exactly what that maximum expected reward is?",
            "If we have a two step problem, we're going to.",
            "If we take some action, we have a distribution over the next States and a distribution over our first step rewards, and then conditioned on following it is simpler.",
            "Next state we're going to have for each.",
            "For each action, we're going to have an expected sum of rewards, so.",
            "We can take the maximum over actions of the expected sum of the expected one step reward.",
            "And then we can take an expectation over the state that we might land in.",
            "Of this maximum over the one step reward and then we can take a maximum over the actions of the expected some over the state.",
            "We might end in of the maximum over the next step rewards, so this is a recursively defined copy and we can't get anything.",
            "And.",
            "Earlier, we're going to take the maximum over actions of the expectation over the next states of the reward we might achieve.",
            "Plus DP, so I'm just using the V part of DP here.",
            "The expected sum of rewards that we can get.",
            "Starting from the next state.",
            "So if I know an MDP.",
            "Then sort of existential E. There exists some optimal policy.",
            "Indep is the subroutine which compute with the optimal policy is and what its value is.",
            "It's just a recursively defined subroutine.",
            "So initially you start with.",
            "Start, you have to figure out what the maximum expected sum over T is the whole thing.",
            "Yeah.",
            "Stop that.",
            "No reward yet.",
            "The unknown and yeah.",
            "What do we want, yeah?",
            "And then you.",
            "You take your next step in whatever the news value model the unknown and known model that's going to be about how it works.",
            "Start, so we're starting with the empty history, which means that the known of HMDP is just everything is unknown right now, which means we're in the sub unknowns special state, and we always expect that we serve receive reward 0.",
            "And then we start interacting in the world according to some way which I'll define in a moment.",
            "And at some moment in the future we start to.",
            "Learn what pieces of the world look like.",
            "And once we have these pieces of the world, we can.",
            "We have the structure required to call DP and try to act and some sort of reasonable way.",
            "Young music take the Christian oh so.",
            "Oh, what was the exact question?",
            "We used to use your device in print.",
            "My question was how do you initialize?",
            "Summarize.",
            "The album"
        ],
        [
            "So I'll get to the algorithm in a moment.",
            "Actually, right here, pretty good.",
            "So the keep the algorithm is going to use these internally maintained data structures.",
            "So.",
            "This is going to be 2 cases to start with.",
            "Case one is we're in some state which is not unknown of H. Right, so so we're in some state and we just don't know what's going to happen if we take some action, right?",
            "So then we're going to choose the least previously used action.",
            "Very simple policy.",
            "Otherwise, maybe we're in some portion of known of age, so we were in some state where for every action we've executed at least 10 times.",
            "And then we're going to call DP.",
            "And we're going to check to see.",
            "If DP on the unknown of H is greater than some particular epsilon, some precision parameter, and if it is, we're going to act according to DP of unknown.",
            "We're going to do this until either the state is unknown or T the actual.",
            "The current number of interactions MoD Capital T = 0, and we're going to go back to one.",
            "OK, so this is what this line here is saying is.",
            "If I believe is a good chance.",
            "That I can actually reach an unknown state.",
            "I will try to reach that unknown state.",
            "And then otherwise I'm going to act according to the known of HMDP.",
            "We go back to the definition of unknown of H is kind of a curious fact.",
            "So we're going to receive reward one.",
            "When we execute an action leading to an unknown state, right?",
            "And what that means is that the output of DP unknown of H is going to be our probability that we believe we can reach an unknown state.",
            "The probability in the unknown of HM DP that you can reach the unknown state.",
            "Kind of carefully arranged things just so this is true.",
            "OK, so this is this is the cubed algorithm.",
            "It's going to explicitly choose to explore or exploit.",
            "OK, so are there questions about this algorithm?",
            "Can you explain the timotea cozero?",
            "Write team Marty equals 0.",
            "So when we're doing exploration, it could be that we have some reasonable probability of reaching an unknown state.",
            "But it's that probably just not realized because we have stochastic actions in the system or actions with the skeptic outcome.",
            "So if the stochastic outcome is that we don't reach an unknown state, then we need to.",
            "We need to cease trying to explore at some point in the future.",
            "Because it could be that we just we lose our chance and then we're in a deterministic world where we can't even reach an unknown state, right?",
            "And then continuing to execute the exploration policy would cause us to violate the sample complexity guarantee.",
            "So this is just a timeout.",
            "It says that if you haven't reached, I don't know Satan.",
            "Some reasonable time horizon then just give up and try to do something more sane."
        ],
        [
            "Alright.",
            "Now I want to go through the proof.",
            "The proof is even more complex.",
            "So we have that room DP in the world.",
            "We have a known of ATP which is in the Y cubed algorithm.",
            "We have the unknown of H&P which is in the Y cubed algorithm and then inside the proof there's going to be two other MDP's.",
            "So.",
            "MDP sub KH is going to be like the known of HMD peaks if we're going to.",
            "So for each state and none of H corresponds to some state in in the real MVP, right?",
            "So let's take the real MVP.",
            "And use the real probability of next date given action state and the real expected reward.",
            "And let's define a new P which just uses the states in the Nanavati MD NPI.",
            "So this is kind of halfway in between none of H&M DP.",
            "Right, so MDP of sub KH has all the states of none of H, but it uses the exact transition transitions of the real MVP in the world.",
            "An MDP sub U of H is the same as cave excepting between unknown of H&M DP.",
            "OK, so we have 5 mini peas.",
            "There's the MDP in the real world.",
            "There's the MDP's.",
            "There's two peas in the algorithm.",
            "There's two extra MP's which are only in the proof.",
            "Not the kind of nice intermediate things to think about."
        ],
        [
            "So this is a sketch of how the proof goes.",
            "It's all here, so there's going to be some sort of simulation lemma.",
            "The simulations lemma says that when you set this in parameter to be large enough.",
            "Then the difference between dynamic programs on the MVP of known of H and a dynamic program on none of it's going to be less than.",
            "Some small quantity.",
            "This kind of makes sense so so that in parameter was how many times do you transition over a particular action in a particular state, right?",
            "We turn this many times.",
            "Then your estimate of the probability of the next state is going to converge to the truth, and if these estimates converge very closely, then any dynamic program run on these two MVP's will be almost the same.",
            "In terms of the value of the maximum expected sum of rewards in return.",
            "When is the explore exploit lemma?",
            "The Explore exploit lemma says that.",
            "The value of dynamic programming.",
            "On known MVP, said none of each.",
            "Plus T times.",
            "The value dinner programming on MDP Simone of age is greater than greater than the best you can do in the real world.",
            "OK, so these are the two fundamental lemmas.",
            "One of them says.",
            "The internal data structure of the cubed algorithm is going to converge to the truth.",
            "It's going to converge well enough that we can estimate things well.",
            "And then the Explore exploit lemma says something about.",
            "Uh.",
            "Our belief.",
            "About how well we can explore.",
            "Remember, this is the probability that you can actually reach an unknown state.",
            "Because of the way we define things, and this is what our maximum expected sum of rewards is in known of HMDP in the real world.",
            "So this sum is greater than.",
            "The best that the optimal policy can do in the real world.",
            "OK, so the way we use these two limits is we say.",
            "Going to choose an end.",
            "Just that the simulation lemon bites pretty hard, so in particular.",
            "This one over Poly is very small.",
            "And then.",
            "The Explore exploit lemma.",
            "It's going to imply the difference between how well we can do in the real world and how well we can do in the known of HM DP.",
            "Is.",
            "Right, so this is going to imply that if this difference is large.",
            "If how well we can do in the real world.",
            "And how well we can do in the known of H MDP remember?",
            "Because of the simulation lemma, this is almost the same as DP of none of age, right?",
            "So the difference in how well we can do in the real world, and how well we can do in the known of HMDP, is greater than epsilon.",
            "That's going to imply that.",
            "The probability of good exploration is going to be greater than some reasonable quantity.",
            "So this is sort of two possibilities either.",
            "The inequality goes this way, in which case we're done because acting according to our Nanavati MD is going to be near optimal or the inequality goes this way, in which case the probability of exploration has to be recently large.",
            "And then of course, if the probability of good exploration is reasonably large.",
            "Because we can only successfully explore something like this many times.",
            "So the definition of good exploration is we encounter some state where there is some action we have executed less than 10 times.",
            "Right so.",
            "We encounter some state.",
            "So the number of times that we can encounter a state where there's an action that we've executed lesson in times.",
            "Something like in times essay.",
            "And then we're going to have some reasonable probability of achieving this, which is going to be.",
            "Well, it's epsilon over T, so the expected number of times that we can actually.",
            "Try to explore.",
            "It's going to be like in S80 over epsilon.",
            "And then we have that that time out that condition, which means that every attempt to explore just uses up at most T steps.",
            "And then you can apply some.",
            "Some turn off bound, which is the truth is close to the expectation, and the proof goes through.",
            "Are there questions about this?",
            "OK, so in the notes written out how to do the simulation lemma in the Explore exploit lemma.",
            "But I think that I'll skip them at the moment.",
            "And go on so.",
            "This is this is.",
            "Is it for the cubed algorithm?",
            "So let me just go back to the theorem statement.",
            "It's kind of appealing.",
            "OK, so we have this algorithm E ^3.",
            "The claim is that for all MDP's with high probability.",
            "For all except some.",
            "Polynomial number of steps we can behave near optimally.",
            "So that's kind of encouraging, right?",
            "I mean, we can behave near optimally.",
            "There's some issues, of course, with what exactly this Poly is, but you know, news."
        ],
        [
            "So there's a few modifications to this basic statement which are worth knowing about.",
            "So, there's this armax modification.",
            "So what they are Max modification is is.",
            "You have a slightly different algorithm.",
            "So first step is the same.",
            "The second step is.",
            "Otherwise, act according to this thing.",
            "So what is?",
            "How do I add to MDP together, right in the way that since these MVP's have the same structure, the only difference is the rewards?",
            "What I mean here is I sum up the rewards right?",
            "So if it's this?",
            "Added in DP has reward one if there's reward.",
            "Wanna neither this in new Peor that MVP.",
            "So plus it adds reward on each edge.",
            "OK, so this is a simpler algorithm.",
            "It's kind of intuitive algorithm.",
            "It says it's kind of interesting says that.",
            "You can be acting according to this combined MDP.",
            "And you don't know if you're exploring or you're exploiting.",
            "You're just behaving as seems natural to you.",
            "You're optimizing one particular criteria, which is a combination of your desire to explore your desire to exploit.",
            "And it turns out that when you do that, you can still prove the same guarantee.",
            "When you add the two MVP's, does it become unknown?",
            "Because the only difference between the two is the transition out from the.",
            "From the known to the Unknown, one is 1 and the other is 0.",
            "So you add the reward.",
            "You get one so.",
            "Sum of two ICP.",
            "The one with the.",
            "With that one.",
            "So we this is the known of ATP at some point in our exploration process, if we add the.",
            "The unknown unknown happened 0 from management as to the SNL right.",
            "The unknown I see, so the unknown would actually have a one here.",
            "So the unknown, whenever you transition from a known state to an unknown state has a reward of 1.",
            "And we should go back to here.",
            "Right, so this is this is this is a known.",
            "And the unknown will be the same, except we'll have a reward of 1 here.",
            "So when you add that up, when you get unknown, you get unknown, but you wouldn't necessarily to know 'cause it might be there.",
            "This reward one here.",
            "So let's say paint, let's say that this is 1.",
            "Then this would also be one.",
            "And then when you add in unknown, you get a one and a one which is not the same as what you.",
            "Have here right?",
            "All the rewards for the unknown MVP is zero except for traditions from.",
            "The known state to the unknown state.",
            "So in unknown of age you get reward for just transitioning from a known state to an unknown state, and at no other time.",
            "OK, little bit.",
            "We are.",
            "Can you can you push your button?",
            "What is lies says unequal too.",
            "No, except for the reward.",
            "Going from the note states, the unknown state.",
            "Play.",
            "Right, this is what you're talking about, right?",
            "Yeah, so.",
            "After that transition from the known to the unknown.",
            "So if you add those two up, once you get.",
            "So this says that if you're in a known state.",
            "Any transition to an unknown state then you get reward 1, otherwise you get reward 0.",
            "Oh all the others.",
            "We want to say yes.",
            "OK, so this is kind of a nice appealing modification because it says that.",
            "You can just be optimizing one thing.",
            "And then.",
            "Another modification."
        ],
        [
            "So there's this delayed Q learning algorithm and the claim is that we can reduce this polynomial to be.",
            "Oh~ of essay using a particular algorithm, so I'm not going to say because it's a little bit complex, but it's interesting that you can reduce it.",
            "You can reduce the dependence upon States and actions to just be.",
            "Number of states similar actions per state.",
            "This is like the total number of actions in the entire process, right?",
            "So in particular, maybe this is interesting because let's think for a moment if we have.",
            "This P of S prime given A&S.",
            "How many bits does it take to specify this?",
            "So for any particular action, and in particular state, any particular next state, you have some particular conditional probability, right?",
            "So let's say we want to specify to some precision.",
            "The number of bits required to specify this.",
            "He's going to be.",
            "Oh of S ^2 a.",
            "Right, so the number of bits required to specify the actual Markov decision process.",
            "Is order X squared a an amount of interaction we need with the world in order to behave near optimally?",
            "Is only OSA?",
            "To the~ here means that there's some some terms depend upon log of essay, so this says that we can.",
            "Interact with the world and an amount which is less than the description length of the world.",
            "In order to behave near optimally.",
            "Kind of interesting.",
            "It was just mostly interesting to me because there have been debates in reinforcement learning about whether not it was necessary to have an algorithm which internally built a model of the world.",
            "This this algorithm doesn't, and it can't because the amount of experience that it uses is less than the amount required to actually build an accurate internal model.",
            "How do you qualify as a?",
            "This is a state.",
            "Oh, right, so there's a little bit overloading here, so lower case S is a state.",
            "Upper Case S is either the number of states like here, or is the set of states.",
            "So this is the number of States and the number of actions.",
            "So this is another station times the number of actions per state.",
            "So I guess another thing that you can realize that this algorithm is significantly different from the cubed algorithm because.",
            "This strategy of trying to build up a model the world can't work with that sample complexity, right?"
        ],
        [
            "OK, so.",
            "These are simplicity results, and then I want to go through a short section where I sort of talk about.",
            "The limitations of the sample complexity approach to reinforcement learning and what people have done to try to address these limitations."
        ],
        [
            "So the first limitation is given to you by lower bound.",
            "So the claim is that any algorithm A which satisfies the cube statement must use at least.",
            "TSA actions to explore.",
            "Right, so we have to allow ourselves.",
            "To mark off, at least TSA actions in order to to get any cubelike statement.",
            "The somewhat stronger lower bounds, which actually involve the epsilon, but we won't worry about that because this is."
        ],
        [
            "Patient and the proof is actually very simple.",
            "The proof is just.",
            "Here's an MDP here.",
            "Here's the family of MDP's, and the claim is you need TSA actions to learn to behave near optimally.",
            "So what we're going to do is I have a great way of demonstrating this, so here we have some sort of pathway.",
            "The pathway kind of goes up right?",
            "So if I behave if I behave.",
            "Uh.",
            "Well, maybe I should try to do that.",
            "OK, so we have some sort of pathway.",
            "And we have maybe two actions.",
            "Per state, right?",
            "So we have a one and we have a 2.",
            "Oh, to have a 3?",
            "Oh yes, very good.",
            "Three actions per state.",
            "Have a 18283 and I'm going to think about a family of Markov decision processes.",
            "Where the action which leads to the next state in the path is randomized, right, just independently.",
            "She learning how to behave well here tells you nothing about how to behave well here, so maybe section A2 to get you the next date here.",
            "Section A1, then Section 8, three and then it Section 8 two.",
            "But for any Markov decision process in this in this family.",
            "Knowing how to behave here tells you nothing about how to behave here and then all of their actions just take you back to the beginning.",
            "So let's think about what we have to do in order to figure out how to get to the end of the path and receive the reward, right?",
            "In every state you have to execute about half the actions in order to figure out which ones take it to the next state.",
            "And every time you execute the wrong action, you go back to the beginning.",
            "Right so.",
            "In every state you have to execute half the action, so it's S times A and you go back to the beginning, which is where the T comes in.",
            "Yeah so.",
            "You can't do better than TSA, and this is distressing.",
            "The reason why this is distressing is because in the real world, the number of actions.",
            "If you're going to talk to a physicist.",
            "And you ask him how many states are there right?",
            "Until, well, you know, maybe you can.",
            "Maybe you can kind of alias things which are the same as to, like the Planck scale, which is like 10 to the minus 34 meters.",
            "So the number of states.",
            "Which specify your location within this room.",
            "Is larger than the memory of any computer that we can ever hope to make.",
            "And this says that.",
            "You know for general MVP.",
            "You're going to depend upon the number of states.",
            "If it's a bit of a problem, right?"
        ],
        [
            "So lower bounds is really big.",
            "Problems are not going to be solvable.",
            "By the cubed algorithm.",
            "And then we have a sort of conundrum because we know that the problems that we want to solve are solvable because reinforcement learning is kind of just a mathematical description of life.",
            "You are an agent wandering in a world.",
            "And you solve your exploration and exploitation problems every day, right?",
            "So somehow the math is failing to capture.",
            "What is actually doable?",
            "So what that means mathematically is that we need to either make more or different assumptions and have been several attempts to make more assumptions stronger assumptions in order to capture what's going to happen in the real world."
        ],
        [
            "So one of these variants is called factor D cubed.",
            "The basic idea infected cubed is.",
            "OK, so there's a lot of different people in this room and I think none of you are actually touching.",
            "What that means is that.",
            "Around you, each of you can move independently of the other person, right?",
            "So.",
            "The things you can do individually.",
            "Factor with respect to things that somebody else can do individually.",
            "So we could imagine some sort of system where.",
            "You decide to rotate your chair and maybe that increases your reward in some way and somebody else can decide to rotate their chair in some other way and they can increase the reward that way, right?",
            "So in a factored MDP.",
            "There's a set of bits which may be individual people, and the claim is that the transition probability.",
            "Just depends upon some other small subset of bits.",
            "In particular, might just depend upon a panyu, so it could be just B1, depends upon.",
            "This capital B Subiya which is contains B1.",
            "So, so this is the notion of factoring.",
            "Factoring is extremely powerful.",
            "I mean, factoring is used all the time in science, it's kind of.",
            "Refactoring if you think about it, is sort of.",
            "How do you think about?",
            "The fall of an Apple being equivalent to.",
            "Oh my mikes not working interesting.",
            "Hello.",
            "Hello, excellent, so factoring is very strong.",
            "It's how in science we can think about.",
            "Discussion of gravity which affects the movement of planets in the fall of apples.",
            "But OK, so we can make some sort of statement.",
            "Yeah, if you know the factoring in advance.",
            "So you know which subset of the bits is critical.",
            "In specifying the probability of outcome of any individual bit.",
            "Then we can make the factory Cube statement, which is that.",
            "For all factored MDP's.",
            "With high probability.",
            "In polynomial this description link to the factory MVP so that factored in deciding to have a description link which is much smaller than the description length of the full MVP.",
            "And for this reason, is falling to pieces is going to be.",
            "Squared a.",
            "When when is this strong factoring so the set Capital B survives is much smaller than the set of all bits.",
            "So.",
            "For all except some number of actions, we can make any cube statement.",
            "We can behave near optimally.",
            "Everywhere else.",
            "So I guess the first claim here is this isn't quite enough.",
            "If you just if you think about what happens if everybody tries to go to lunch at the same time, there's going to be some collisions.",
            "That means that this is going to be not much as much factoring as you might hope.",
            "There's another kind of severe difficulty in here, which is where we're something hidden in the factory cubed algorithm, which is the ability to do dynamic programming on a factored MDP.",
            "And turned up.",
            "This is a very difficult thing to do.",
            "So, and it's sort of.",
            "It's not even like NP hard.",
            "It's kind of representation.",
            "Lee hard to even specify how you would do it."
        ],
        [
            "This is another approach people use is called magic metric cubed.",
            "So if you go back to the factoring.",
            "If you just think about.",
            "Even if we believe that the factoring is is simple an even if we manage to overcome this computational difficulty, which may be able to do in practice.",
            "There's still an issue because even for you, individually in this room the number of states for you, let alone the cross product of all of you.",
            "Is is just too large?",
            "The number of possible positions poses you could have in this room is.",
            "More than his representable on any computer.",
            "So we want some approach which allows us to avoid this.",
            "So we're going to assume.",
            "That there exists some modeling function, right?",
            "The modeling function is going to take a history of interactions with the world and a state.",
            "And we're going to assume this modeling function.",
            "What's going on with lots of succeed in the real world is we can somehow generalize past experiences to predict future experiences without actually having experienced precisely the same.",
            "Experience.",
            "Right, so we're not somehow in the real world.",
            "We don't have to experience the state in action in order to figure out what's going to happen next.",
            "Who want to quantify that somehow that's quantified according to this assumption, we're going to say, suppose they're just a modeling algorithm, something which let's generalize, such that if there are any experiences which are nearby according to some metric.",
            "Then the model is going to output in next date in a reward from almost the same distribution as the real world.",
            "Is one way to capture the notion of ability to generalize?",
            "And the claim is there is an algorithm metric y ^3.",
            "Which uses this modeling.",
            "Operation.",
            "Just that for all metric NDPS.",
            "So Magic MDP is like an MDP, except.",
            "This modeling assumption holds where using this metric.",
            "Probably 1 minus Delta.",
            "For all except.",
            "Some number of samples, the EQ statements going to hold.",
            "And the thing which is new here is we have a covering number on the MDP according to this metric.",
            "The covering number is sort of how many States and actions do you have to experience in the real world in MDP such that every state in action is near to something that you've experienced.",
            "The covering number for an MVP for metric MVP is the number of transitions that you have to experience.",
            "So that.",
            "Every sidaction is near according to the metric to one of your experiences.",
            "Covers is a very intuitive notion, but let me try to make it more intuitive.",
            "So let's think about just covering this room right?",
            "So let's say we wanted to cover this room to within 10 meters, right?",
            "So then maybe we could think about going there and there and there and there and there and there and there.",
            "And it and then so I pointed out a bunch of points in the room.",
            "The payments, these points cover all locations in this room to within 10 meters.",
            "Right, so it's a set such that everything in a larger set is near to something in the smaller set.",
            "Sorry cover.",
            "How do you cover?",
            "BP.",
            "When you cover Andy, what do you mean right?",
            "So what I mean is?",
            "So I'm covering a metric MDP.",
            "So there's an MDP, and there's a metric on this MDP, so we have this metric D, right?",
            "The metric is just going to measure how far apart I stayed.",
            "The action is from another state action.",
            "And so the cover on the metric MDP is going to be.",
            "A set of states.",
            "An action such that every state in action is near something in that set.",
            "So there's still some issues here.",
            "Clear that we can reasonably expect model to be this accurate, is saying that.",
            "And if you think about it a little bit, it's actually pretty hard too.",
            "Model The world so well that you know the distribution of our next states.",
            "Precisely enough is required here, and I think it's the most significant drawback."
        ],
        [
            "Alright, and then there's a really fundamental problem.",
            "Which this cartoon explains.",
            "If you think about what these this EQ statement is saying.",
            "It's saying that wherever you end up.",
            "You're going to behave in your optimally, right?",
            "But the E ^3.",
            "It's not me.",
            "OK.",
            "The cubed algorithm is.",
            "He's kind of a very eager exploration algorithm, so if it finds a Cliff, it'll very happily jump off a Cliff.",
            "And then if it lands, the bottom is agent leaves, the bottom has a broken neck, so it can't move at all.",
            "Then the cube statement holds.",
            "It will behave near optimally given where it is right?",
            "Just going to be like.",
            "It's the best you can do when you have a broken neck.",
            "So there's some failure.",
            "To capture what we actually want in the cube statement.",
            "There's a little bit of discussion, so I guess these guys have worried about trying to make a helicopter fly.",
            "If you imagine trying to use the cube to fly helicopter, you're going to have a lot of wrecked helicopters, so some people have been doing some work trying to figure out how to avoid this, but it's not that easy.",
            "To me, this is the natural statement and.",
            "We just need a little more insight to figure out what the right statement is.",
            "OK, so I think we should take a break now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "So we can talk about reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of backing material available here if you're interested in taking a look at it.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I guess the first question is what is reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning is probably the most general form of learning problem.",
                    "label": 0
                },
                {
                    "sent": "Anybody tries to solve.",
                    "label": 0
                },
                {
                    "sent": "So yesterday I talked about classification in various forms of supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Is also something known as semi supervised learning where people have in addition to bunch of unlabeled data that they tried to learn from and then.",
                    "label": 0
                },
                {
                    "sent": "There's the simplest form of reinforcement learning, which is typically reinforcement learning Markov decision process.",
                    "label": 1
                },
                {
                    "sent": "And then there's the general form of reinforcement learning which contains everything.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It is a question is why is it important to study reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "Yes, if you face any learning problem in the real world.",
                    "label": 0
                },
                {
                    "sent": "You can ask the question, is this a reinforcement learning problem?",
                    "label": 0
                },
                {
                    "sent": "And it turns out the answer is always yes.",
                    "label": 0
                },
                {
                    "sent": "So asking this question is maybe not so interesting, but the point here is that if we have any kind of theory for reinforcement learning, then it can apply very generally to all kinds of problems, right?",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that the kind of things that we can prove for reinforcement learning are not as strong as you might hope, because reinforcement learning is incredibly general.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, you can prove some things and.",
                    "label": 0
                },
                {
                    "sent": "Understanding these things can help you understand how to get solutions to individual problems.",
                    "label": 0
                },
                {
                    "sent": "So when you face some problem in the real world.",
                    "label": 0
                },
                {
                    "sent": "You start out by asking yourself, is this reinforce morning and you know the answer is yes when you start asking yourself is this something that's simpler than the full reinforcement learning problem?",
                    "label": 0
                },
                {
                    "sent": "So the goal typically instead of narrowing down what problem you have, is.",
                    "label": 0
                },
                {
                    "sent": "You start out with general reinforcement learning, and then often you discover that you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe this is some sort of similar problem.",
                    "label": 0
                },
                {
                    "sent": "There are other problems as well, which aren't up here.",
                    "label": 0
                },
                {
                    "sent": "And then once you discover exactly how simple you can make the problem, then you can try to attack it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "By the way, questions are good, so you feel free to ask questions today.",
                    "label": 0
                },
                {
                    "sent": "IS.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, these are actually pretty interesting versions of learning.",
                    "label": 0
                },
                {
                    "sent": "So in armed bandits what happens is.",
                    "label": 0
                },
                {
                    "sent": "You have, it's like you go to the casino and you have a bunch of machines that you can put money into and you can pull an arm right?",
                    "label": 0
                },
                {
                    "sent": "And some of the arms pay off more than other arms.",
                    "label": 0
                },
                {
                    "sent": "But you don't know which pays off the most in advance, right?",
                    "label": 0
                },
                {
                    "sent": "So the goal in our vantage learning it should try to do as well as the best individual arm by exploring in some according to some schedule.",
                    "label": 0
                },
                {
                    "sent": "Active learning is a different setting, so in general reinforcement learning what happens is you're going to interact with an environment and the process of interacting with environment changes what we're going to observe in the future.",
                    "label": 0
                },
                {
                    "sent": "And active learning, that's not true, but you are actively interacting with the environment, so the idea is maybe you want to do classification.",
                    "label": 0
                },
                {
                    "sent": "So you start requesting labels for individual examples.",
                    "label": 0
                },
                {
                    "sent": "And then at some point after you get some labels, you produce a classifier.",
                    "label": 0
                },
                {
                    "sent": "So active learning is sort of the interactive version of this classification.",
                    "label": 0
                },
                {
                    "sent": "Learning, or just supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is one of those very basic things.",
                    "label": 0
                },
                {
                    "sent": "It seems that.",
                    "label": 0
                },
                {
                    "sent": "Interaction, the ability to interact with the world is radically more powerful than just the ability to do batch learning.",
                    "label": 0
                },
                {
                    "sent": "As we talked about yesterday.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by radically more powerful what I mean is the number of labels that you may need to request can be a log arhythmic in the number that are required to learn.",
                    "label": 0
                },
                {
                    "sent": "If you're just using batch learning like we talked about yesterday.",
                    "label": 0
                },
                {
                    "sent": "It's not always log rhythmic, example of where it might be log rhythmic is suppose you you just trying to learn to predict a threshold on a line right?",
                    "label": 0
                },
                {
                    "sent": "So then?",
                    "label": 0
                },
                {
                    "sent": "And let's say there's no noise that would be very easy to.",
                    "label": 0
                },
                {
                    "sent": "First query in the middle to see if it's if the threshold is above or below that the query point.",
                    "label": 0
                },
                {
                    "sent": "Then that partitions the set and then you can just recursively query just narrowing in very quickly on where the King threshold is.",
                    "label": 0
                },
                {
                    "sent": "For that specific problem, it's not always log rhythmic, it is just that sometimes it is and it's very enticing when it's true.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I think I'm going to talk about active learning a little bit later.",
                    "label": 0
                },
                {
                    "sent": "For the research talk.",
                    "label": 0
                },
                {
                    "sent": "But today we're mostly just going to talk about Markov decision process, reinforcement learning and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Right, I think I already said this.",
                    "label": 0
                },
                {
                    "sent": "That was in our problem.",
                    "label": 0
                },
                {
                    "sent": "It's often something simpler than RL and you want to identify how it's simpler than the full RL problem.",
                    "label": 0
                },
                {
                    "sent": "Hopefully when you do that, you'll discover that somebody already studied that particular setting, and then you can use whatever approaches.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've developed.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'll just start by telling you this is this is modern reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you go back to the title is modern reinforcement.",
                    "label": 0
                },
                {
                    "sent": "Learning theory is quite a bit of reinforcement learning theory, which is happened before this afternoon.",
                    "label": 0
                },
                {
                    "sent": "Talk about here I'm talking about here is things that I think have happened in the last seven years or so.",
                    "label": 0
                },
                {
                    "sent": "So the first kind of reinforcing tape that people have been worrying about recently is sample complexity in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So sample complexity is how many times do you need to interact with your environment in order to do something interesting something useful.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "Guess what the same place to guarantee?",
                    "label": 0
                },
                {
                    "sent": "We're going to think about making theorem statements of the form with high probability given some number of samples.",
                    "label": 0
                },
                {
                    "sent": "Some guarantee is going to hold, and I'll fill in what that guarantee is in a minute.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be worrying about reinforcement learning, Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you should have defined a market position process, but let me define it for you.",
                    "label": 0
                },
                {
                    "sent": "So the fundamental quantity in a Markov decision process is next state distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to have states.",
                    "label": 0
                },
                {
                    "sent": "States are something that the world imposes.",
                    "label": 0
                },
                {
                    "sent": "We're going to have actions.",
                    "label": 0
                },
                {
                    "sent": "We're going to have some distribution over the next state given the action in the state, and then we're going to have a policy which given state.",
                    "label": 0
                },
                {
                    "sent": "Produces.",
                    "label": 0
                },
                {
                    "sent": "Produces an action.",
                    "label": 0
                },
                {
                    "sent": "So I'm kind of curious if we can run a test.",
                    "label": 0
                },
                {
                    "sent": "What happens to point the camera at the camera's output?",
                    "label": 0
                },
                {
                    "sent": "Oh OK well.",
                    "label": 0
                },
                {
                    "sent": "OK, so in Markov decision process we have some state.",
                    "label": 0
                },
                {
                    "sent": "The state summarizes all the information we need in order to make a good decision.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of the fundamental assumption, right?",
                    "label": 0
                },
                {
                    "sent": "You know the state summarizes everything and then based on that state we make some sort of decision.",
                    "label": 0
                },
                {
                    "sent": "Are there questions about that?",
                    "label": 0
                },
                {
                    "sent": "Like Sam mentioned, this kind of model briefly yesterday.",
                    "label": 0
                },
                {
                    "sent": "In general, in the real world, maybe you don't have.",
                    "label": 0
                },
                {
                    "sent": "You don't exactly know what your state is.",
                    "label": 0
                },
                {
                    "sent": "There are things like.",
                    "label": 0
                },
                {
                    "sent": "OK, so when you're sort of physically moving around, if you can't see things because you're blindfolded, then you're not in a Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "But let's pretend that we are in a market position process.",
                    "label": 0
                },
                {
                    "sent": "So whatever we see is sufficient to tell us how to move around, which is maybe true.",
                    "label": 0
                },
                {
                    "sent": "Some reasonable portion of the time.",
                    "label": 0
                },
                {
                    "sent": "OK, so the fundamental quantities are going to be fundamental parameters.",
                    "label": 0
                },
                {
                    "sent": "So S will be the number of states.",
                    "label": 0
                },
                {
                    "sent": "It will be the number of actions per state.",
                    "label": 0
                },
                {
                    "sent": "T is going to be horizon time, so when people are doing reinforcement learning very often.",
                    "label": 0
                },
                {
                    "sent": "To we want to be concerned not just with what happens at the next step, but what happens?",
                    "label": 0
                },
                {
                    "sent": "T steps into the future.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people instead of saying I care about the next steps, they say I care about this step.",
                    "label": 0
                },
                {
                    "sent": "Some care for the next step, a little bit less in the next step, a little bit less.",
                    "label": 0
                },
                {
                    "sent": "Next step, little bit less, next little bit less where they quantify a little bit less in terms of gamma.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're just going to work with T because it's a little bit easier mathematically, but all the results that we talk about today kind of hold if you use gamma discounting.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we're going to switch from Markov decision processes to palm DP's so partially observable Markov decision processes like so.",
                    "label": 0
                },
                {
                    "sent": "If you're blindfolded and then you're wandering around, you know you can't see anything, but you can still feel and then based on what you feel, you can decide what to do.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have a precision parameter, so I guess.",
                    "label": 0
                },
                {
                    "sent": "Having as a goal learning exactly what the optimal policy is, what policy will give you the largest sum of rewards is a little bit implausible, because it just can't be done in a reasonable sense, so our goal is going to be something like we're going to try to find it in a policy which is within epsilon of the optimal policy.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there's a specific result which is actually quite interesting an.",
                    "label": 0
                },
                {
                    "sent": "I'm planning to go through this result in full detail.",
                    "label": 0
                },
                {
                    "sent": "This resolves is the EQ guarantee so Y cubed is an algorithm which stands for explicit, explore or exploit.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So we're going to there's a question about what kind of information access do we have to the world?",
                    "label": 0
                },
                {
                    "sent": "And we're going to imagine that the information that we have to the world is exactly what we have is an agent wandering around in the world.",
                    "label": 0
                },
                {
                    "sent": "So as an agent wandering around the world, you know we have some state that we see, and then we can choose some action and then we observe what happens next, right?",
                    "label": 0
                },
                {
                    "sent": "And then we can choose some action we observe what happens next, so in particular.",
                    "label": 0
                },
                {
                    "sent": "So some people are doing reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "They think about other sorts of interactions, other ways to interact with the world.",
                    "label": 0
                },
                {
                    "sent": "So think about generative models where they can say suppose that I'm over here and I take this action.",
                    "label": 0
                },
                {
                    "sent": "What happens next?",
                    "label": 0
                },
                {
                    "sent": "And then suppose that I'm here and take this action instead.",
                    "label": 0
                },
                {
                    "sent": "What happens next?",
                    "label": 0
                },
                {
                    "sent": "What we're not doing that instead?",
                    "label": 0
                },
                {
                    "sent": "We're just going to be.",
                    "label": 0
                },
                {
                    "sent": "It's just as if you are the reinforcement learning agent.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine using the EQ dollar Rhythm to to decide how much you're going to explore exploit.",
                    "label": 0
                },
                {
                    "sent": "So the way this works is.",
                    "label": 0
                },
                {
                    "sent": "We have a current state S that we know we choose some action and then we observe the next state and also a reward.",
                    "label": 0
                },
                {
                    "sent": "So the goal and reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So in addition to having the distribution.",
                    "label": 0
                },
                {
                    "sent": "Over the next eight, given the action in the state, we're going to have.",
                    "label": 0
                },
                {
                    "sent": "Some kind of reward?",
                    "label": 0
                },
                {
                    "sent": "Which may be a function of.",
                    "label": 0
                },
                {
                    "sent": "In general, is triple.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So, are there questions about this setting?",
                    "label": 0
                },
                {
                    "sent": "'cause now is a good time to think about.",
                    "label": 0
                },
                {
                    "sent": "Is the setting makes sense?",
                    "label": 0
                },
                {
                    "sent": "I don't want to claim that this is the rightmost setting, and actually, if you find a problem with a particular setting in machine learning, you should in general think about how you want to fix it, because it's not clear yet that we have the right settings.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "We had this precision process which has some set of states, some actions per state.",
                    "label": 0
                },
                {
                    "sent": "And this distribution over the next state, given the state in the action, we're going to have some horizon T. And then the question is, what is this pie?",
                    "label": 0
                },
                {
                    "sent": "What is this thing that's going to hold with high probability, right?",
                    "label": 0
                },
                {
                    "sent": "So a sample complexity guarantee has some sort of statement, like after some number of samples, some guarantee is going to hold.",
                    "label": 0
                },
                {
                    "sent": "And the question is, what is that guarantee?",
                    "label": 0
                },
                {
                    "sent": "So there's a guarantee.",
                    "label": 0
                },
                {
                    "sent": "In the original EQ paper.",
                    "label": 0
                },
                {
                    "sent": "Which is something like if you.",
                    "label": 0
                },
                {
                    "sent": "Additionally, assume that the Markov decision process is a mixing time.",
                    "label": 0
                },
                {
                    "sent": "Then after some number of samples related to.",
                    "label": 0
                },
                {
                    "sent": "The number of states, number of actions per state, the mixing time in one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "You can act within epsilon available policy.",
                    "label": 0
                },
                {
                    "sent": "So what is missing time I guess is the question right?",
                    "label": 0
                },
                {
                    "sent": "I don't want to tell you so.",
                    "label": 0
                },
                {
                    "sent": "Instead I'm going to tell you a simpler.",
                    "label": 0
                },
                {
                    "sent": "A theoretical statement which is going when you add in the mixing time assumption will give you back the original.",
                    "label": 0
                },
                {
                    "sent": "So the modified form is going to be.",
                    "label": 0
                },
                {
                    "sent": "Suppose that you have.",
                    "label": 0
                },
                {
                    "sent": "After the bottom part of the theorem is going to be after some number of samples which is polynomial in the number of states, the number of actions per state, the time horizon in one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "You can act epsilon optimally 40 time steps.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, and mixing assumption just means that.",
                    "label": 0
                },
                {
                    "sent": "You're assuming that no matter what you do, you end up where you were.",
                    "label": 0
                },
                {
                    "sent": "I actually like the second form of this statement, abit more because this applies in a deterministic world.",
                    "label": 0
                },
                {
                    "sent": "Well this only applies in the sarcastic world.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the theorem statement.",
                    "label": 0
                },
                {
                    "sent": "The claim is there is an algorithm E cubed such that for every Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "Parameterized like so.",
                    "label": 0
                },
                {
                    "sent": "With rewards in 01 with probability 1 minus Delta.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "For all except a polynomial number of steps.",
                    "label": 0
                },
                {
                    "sent": "I think there's.",
                    "label": 0
                },
                {
                    "sent": "I think I have failed you.",
                    "label": 0
                },
                {
                    "sent": "I need to define a couple more things.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "One of the fundamental quantities in reinforcement learning is.",
                    "label": 0
                },
                {
                    "sent": "Given that I'm in some particular state, what is my expected future summer rewards?",
                    "label": 0
                },
                {
                    "sent": "Right, so the star.",
                    "label": 0
                },
                {
                    "sent": "Of S. Oh it's going to be an expectation.",
                    "label": 0
                },
                {
                    "sent": "Over state.",
                    "label": 0
                },
                {
                    "sent": "Action reward triples.",
                    "label": 0
                },
                {
                    "sent": "Drawn from.",
                    "label": 0
                },
                {
                    "sent": "The MVP.",
                    "label": 0
                },
                {
                    "sent": "In the policy, so we're going to have some policy.",
                    "label": 0
                },
                {
                    "sent": "Vanessa markup decision Process market decision process is like so the policy is like so we put the two of them together.",
                    "label": 0
                },
                {
                    "sent": "We're going to have a process which generates state action reward triples right?",
                    "label": 0
                },
                {
                    "sent": "Then we're going to look at the sum.",
                    "label": 0
                },
                {
                    "sent": "Of our.",
                    "label": 0
                },
                {
                    "sent": "Summer from I = 1 to T of our sub T. This is the expected sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "Given that we're starting in S. Say again.",
                    "label": 0
                },
                {
                    "sent": "Subscripts I see.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let me see.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're doing expectation.",
                    "label": 0
                },
                {
                    "sent": "Over state action reward.",
                    "label": 0
                },
                {
                    "sent": "Triples T of them.",
                    "label": 0
                },
                {
                    "sent": "Drawn according to.",
                    "label": 0
                },
                {
                    "sent": "The MVP.",
                    "label": 0
                },
                {
                    "sent": "Starting.",
                    "label": 0
                },
                {
                    "sent": "In S. And I.",
                    "label": 0
                },
                {
                    "sent": "And we had to have the sum.",
                    "label": 0
                },
                {
                    "sent": "From I = 1 to T of R sub T. So we have the market decision process.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that we're starting in State S. We're going to act according to the policy.",
                    "label": 0
                },
                {
                    "sent": "This is going to induce some distribution over States and rewards the first time step, and that's going to induce some distribution over States and rewards the second time step.",
                    "label": 0
                },
                {
                    "sent": "The 3rd and the 4th, and so on.",
                    "label": 0
                },
                {
                    "sent": "The goal is to maximize the sum of expected expected sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "What does?",
                    "label": 0
                },
                {
                    "sent": "Pie stands for the policy, so the pot pie.",
                    "label": 0
                },
                {
                    "sent": "\u03a0 Maps a state to an action.",
                    "label": 0
                },
                {
                    "sent": "Mother.",
                    "label": 0
                },
                {
                    "sent": "Do you need another probability distribution?",
                    "label": 0
                },
                {
                    "sent": "So Pi is something that you control your choosing how you're going to respond to what you observe in the world.",
                    "label": 0
                },
                {
                    "sent": "You could choose to respond in a randomized way.",
                    "label": 0
                },
                {
                    "sent": "And in fact I will talk about that a little bit, but for the Y cubed algorithm, Pi is actually going to be deterministic.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is one other thing which is this Q value.",
                    "label": 0
                },
                {
                    "sent": "So if I'm in some particular state in a committed taking some particular action.",
                    "label": 0
                },
                {
                    "sent": "Then how do we do in the future?",
                    "label": 0
                },
                {
                    "sent": "Oh, I forgot to mention something.",
                    "label": 0
                },
                {
                    "sent": "So for that definition there so in general in reinforcement learning, if we decorate things with the star, we mean assuming that we act optimally.",
                    "label": 0
                },
                {
                    "sent": "So when I say V star.",
                    "label": 0
                },
                {
                    "sent": "There's something kind of strange here, because this index here is T minus little T MoD.",
                    "label": 0
                },
                {
                    "sent": "T little T is just the iteration, the interaction number that we're at, so this is going to be some quantity varying between zero and capital T -- 1.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "T here.",
                    "label": 0
                },
                {
                    "sent": "He's going to be very in between capital T. Or between zero and capital T -- 1.",
                    "label": 0
                },
                {
                    "sent": "This is going to be kind of the horizon that we care about right now.",
                    "label": 0
                },
                {
                    "sent": "So what I'm thinking about is sort of an episodic method of evaluating how well we're doing.",
                    "label": 0
                },
                {
                    "sent": "So we want to try to do well in next time steps, and then we take an action and we want to do well in XT minus.",
                    "label": 0
                },
                {
                    "sent": "One time steps were taken action and we want to do well next T -- 2 time steps and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "And until we get down to zero and then we start and we say OK, Now I want her to do well in next time steps T -- 1 and so forth.",
                    "label": 0
                },
                {
                    "sent": "It seems a little bit funny, but.",
                    "label": 0
                },
                {
                    "sent": "That's all that we know how to prove, actually.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "When you have a star, that means that this, I'm assuming that act optimally.",
                    "label": 0
                },
                {
                    "sent": "And then there's also the Q value.",
                    "label": 0
                },
                {
                    "sent": "Which in general is going to be dependent on some state in some action this is going to equal the expectation.",
                    "label": 0
                },
                {
                    "sent": "Overstate action reward.",
                    "label": 0
                },
                {
                    "sent": "Hot sexy.",
                    "label": 0
                },
                {
                    "sent": "TV.",
                    "label": 0
                },
                {
                    "sent": "During according to.",
                    "label": 0
                },
                {
                    "sent": "The MVP.",
                    "label": 0
                },
                {
                    "sent": "Assuming that I in state S and take action A and act according to policy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So there's going to be some tea and some pie with parameterized Q.",
                    "label": 0
                },
                {
                    "sent": "And then take the same sum.",
                    "label": 0
                },
                {
                    "sent": "Over actions, there's some over.",
                    "label": 0
                },
                {
                    "sent": "Interactions of reward.",
                    "label": 0
                },
                {
                    "sent": "OK, so the V value is if I'm in some state, how well can I do the Q value is if I'm in some state and take some action, how well can I do?",
                    "label": 0
                },
                {
                    "sent": "Right, so in general the Q value is going to be less than or equal to the V value for some particular state.",
                    "label": 0
                },
                {
                    "sent": "Is the best you can do in some particular.",
                    "label": 0
                },
                {
                    "sent": "State.",
                    "label": 0
                },
                {
                    "sent": "Is the Max over the actions of the Q value for that state?",
                    "label": 0
                },
                {
                    "sent": "Is \u03c0 defined before you start?",
                    "label": 0
                },
                {
                    "sent": "No, we're going to be forming.",
                    "label": 0
                },
                {
                    "sent": "We're going to be learning Pi as we go.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the pie here is going to be the policy given by E ^3.",
                    "label": 0
                },
                {
                    "sent": "So in particular, we're going to have some history of interaction with the world.",
                    "label": 0
                },
                {
                    "sent": "That's going to determine how each cube is going to behave, so this is a policy Y cubed of age is going to choose some action given the state and the just given the history of interactions.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to assume that we act according to this policy in the future.",
                    "label": 0
                },
                {
                    "sent": "And the claim is that when we act according to this learn policy, we're going to do as well as the best we could do.",
                    "label": 0
                },
                {
                    "sent": "Minus epsilon.",
                    "label": 0
                },
                {
                    "sent": "So we're going to behave within epsilon of optimal.",
                    "label": 0
                },
                {
                    "sent": "Every big team.",
                    "label": 0
                },
                {
                    "sent": "It's even done so.",
                    "label": 0
                },
                {
                    "sent": "The form of this theorem statement says that suppose we have some infinitely long process of interacting with the world according to E. ^3.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to mark off some number of these interactions and we'll just.",
                    "label": 0
                },
                {
                    "sent": "We'll throw them away, basically.",
                    "label": 0
                },
                {
                    "sent": "The claim is that everywhere else will behaving we will be behaving within epsilon of optimal.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is going to be some initial boot up phase where we're kind of exploring around the world.",
                    "label": 0
                },
                {
                    "sent": "We don't know what we're doing and then.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to be behaving near optimally everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So it's worthwhile to think about what this means.",
                    "label": 0
                },
                {
                    "sent": "Something kind of strangers can happen if you're.",
                    "label": 0
                },
                {
                    "sent": "If you're wandering around in the world trying to explore.",
                    "label": 0
                },
                {
                    "sent": "At some point you may be under the impression that you know how to exploit, and then you can just try to maximize your reward.",
                    "label": 0
                },
                {
                    "sent": "Or your expected sum of rewards, and it may be that you end up, you know, following through a crack somewhere and we can set another portion of the world which you've never actually explored before.",
                    "label": 0
                },
                {
                    "sent": "Then you have to go wander around in that portion of the world.",
                    "label": 0
                },
                {
                    "sent": "And then the claim is that the total amount of wandering, no matter which world you are in.",
                    "label": 0
                },
                {
                    "sent": "It's just bounded by.",
                    "label": 0
                },
                {
                    "sent": "Something like the number of states, the number of actions, number, the time horizon, the precision parameter, and we're going to have a probability that this entire process fails.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What I could do is just go through this, go through with the algorithm is and then go through the proof.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we have some market decision process then we have a set of states.",
                    "label": 0
                },
                {
                    "sent": "We have a set of actions per state, so maybe we have.",
                    "label": 0
                },
                {
                    "sent": "State One state, two State 3 and we have action one and action two and we have action one and action two and we have action one and Action 2.",
                    "label": 0
                },
                {
                    "sent": "The outcome of these actions could be deterministic, so if you take some state in some action, you come to some particular state afterwards, well, they could be stochastic, so if he takes in some state, you take some action, maybe with probably .6, you end up here and probably point for you end up here.",
                    "label": 0
                },
                {
                    "sent": "This sort of in general to a MDP looks like and then each of these transitions can be decorated with some reward, so maybe the only time you get reward for this MDP so when you're in state S2 and you take action one and then go to S3 you get reward 1, otherwise you get reward 0.",
                    "label": 0
                },
                {
                    "sent": "You will find a policy that maximizes the reward, maximizes the sum of rewards to horizon T. From then on or.",
                    "label": 0
                },
                {
                    "sent": "From then on, that's a really good point.",
                    "label": 0
                },
                {
                    "sent": "Remember that because there's something.",
                    "label": 0
                },
                {
                    "sent": "Very good about that point.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we're going to.",
                    "label": 0
                },
                {
                    "sent": "We have the full MVP, the true MVP, that the world is behaving according to.",
                    "label": 0
                },
                {
                    "sent": "And then inside of the Y cubed algorithm, we're going to keep track of something called the known of HM DP and the known of HM.",
                    "label": 0
                },
                {
                    "sent": "DP is going to be.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the world MDP, except we're only going to keep track of States and actions that we've experienced some number of times.",
                    "label": 0
                },
                {
                    "sent": "So initially when we when our history is empty, we.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh nothing right.",
                    "label": 0
                },
                {
                    "sent": "And then maybe after a while we've transitioned.",
                    "label": 0
                },
                {
                    "sent": "In times across this edge and in times across this edge, so this becomes the known of HMD P. And then there's kind of a problem with dangling pointers, right?",
                    "label": 0
                },
                {
                    "sent": "'cause it may be that we've gone this way in times, meaning this state is known, but.",
                    "label": 0
                },
                {
                    "sent": "This state could still be unknown, so we're going to think of all of the unknown states we're going to lump them all together, so we'll just have some unknown.",
                    "label": 0
                },
                {
                    "sent": "And we're going to assume that if we go into the unknown state, then we have reward 0.",
                    "label": 0
                },
                {
                    "sent": "So this is the this is internal to the Y cubed algorithm.",
                    "label": 0
                },
                {
                    "sent": "They keep that rhythm is defines the known MDP given the history.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So after you interacted bit more than maybe you learn.",
                    "label": 0
                },
                {
                    "sent": "S2 this becomes your known MVP, then after you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Track a bit more.",
                    "label": 0
                },
                {
                    "sent": "Maybe you learn everything now.",
                    "label": 0
                },
                {
                    "sent": "One thing to note is that when you're trying to.",
                    "label": 0
                },
                {
                    "sent": "Compute the probabilities of outcomes.",
                    "label": 0
                },
                {
                    "sent": "You just counting according to your observations of what happens, right?",
                    "label": 0
                },
                {
                    "sent": "That means that your estimate of the probability of going to state S1 will be different from the truth, because I guess if you haven't even coin, you flip the 100 times.",
                    "label": 0
                },
                {
                    "sent": "You don't really expect to get exactly.",
                    "label": 0
                },
                {
                    "sent": "50 hits right?",
                    "label": 0
                },
                {
                    "sent": "It'll vary a little bit.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an internal data structure to the algorithm, the known MDP.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's another MVP that we keep track of, which is the unknown in Dupee.",
                    "label": 0
                },
                {
                    "sent": "So the unknown MDP is the same as the known indep, except we're going to rearrange the rewards.",
                    "label": 0
                },
                {
                    "sent": "So if you go back to the known MDP.",
                    "label": 0
                },
                {
                    "sent": "We just observe the rewards and then we compute the expected value of the observations to get these values right.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do for the unknown in Dupee you're going to set all the rewards to 0.",
                    "label": 0
                },
                {
                    "sent": "Except when we transition into the unknown region.",
                    "label": 0
                },
                {
                    "sent": "So let's go back a couple.",
                    "label": 0
                },
                {
                    "sent": "It's not so good either try.",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                },
                {
                    "sent": "So the unknown.",
                    "label": 0
                },
                {
                    "sent": "So for the H which produces this known of HM DP, the unknown of HMDP will be the same, except will assume that we get reward one here and we get reward 0 here.",
                    "label": 0
                },
                {
                    "sent": "So in all of the known region in all traditions which transition in all traditions which go to known states we assume reward zero if it goes to an unknown state then we assume reward one.",
                    "label": 0
                },
                {
                    "sent": "That's the only difference.",
                    "label": 0
                },
                {
                    "sent": "So this is different from what you show.",
                    "label": 0
                },
                {
                    "sent": "From known to unknown, you have.",
                    "label": 0
                },
                {
                    "sent": "We want zero in your interest.",
                    "label": 0
                },
                {
                    "sent": "Write algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is another MDP which is a data structure internal to the cubed algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the cube Darren is going to keep track of two MVP's.",
                    "label": 0
                },
                {
                    "sent": "One MDP is the known of HM DP.",
                    "label": 0
                },
                {
                    "sent": "Another MVP is the unknown of HMDP.",
                    "label": 0
                },
                {
                    "sent": "The unknown of Asian DP is the same as the known of H MVP.",
                    "label": 0
                },
                {
                    "sent": "57 set all rewards to 0.",
                    "label": 0
                },
                {
                    "sent": "And then we set the rewards that we set the actions leading to unknown states to have reward one.",
                    "label": 0
                },
                {
                    "sent": "This is to figure out the policy or whatever.",
                    "label": 0
                },
                {
                    "sent": "What do we need to do the unknown?",
                    "label": 0
                },
                {
                    "sent": "Why do we keep track of this?",
                    "label": 0
                },
                {
                    "sent": "Brett we're going to use this to define the policy that the equilibrium is going to be taking.",
                    "label": 0
                },
                {
                    "sent": "OK. Little bit tricky here because we have all these MDP's running around.",
                    "label": 0
                },
                {
                    "sent": "It's important to understand that there's the MDP in the world.",
                    "label": 0
                },
                {
                    "sent": "And there's these two MVP's which are internal to the algorithm and then just defined in terms of the history of interactions with the algorithm observes.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the question is how do we use these MDP's?",
                    "label": 0
                },
                {
                    "sent": "There's a fundamental operation that we're going to use them DPS.",
                    "label": 0
                },
                {
                    "sent": "The fundamental operation is dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "So if we're given any MDP.",
                    "label": 0
                },
                {
                    "sent": "And some particular start state some time horizon, then dynamic programming is going to tell us.",
                    "label": 0
                },
                {
                    "sent": "What is the maximum expected reward over some time horizon?",
                    "label": 0
                },
                {
                    "sent": "So remember we have this episodic notion of how good we're doing.",
                    "label": 0
                },
                {
                    "sent": "And what is the action which achieves it?",
                    "label": 0
                },
                {
                    "sent": "OK, so we have this routine we can call on any defined internal MDP and it will tell us which action should you take to try to maximize.",
                    "label": 0
                },
                {
                    "sent": "Your sum over this length expected reward.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how do you actually compute this dynamic program?",
                    "label": 0
                },
                {
                    "sent": "So maybe it's easy to just think about what happens if this here is 1, right?",
                    "label": 0
                },
                {
                    "sent": "So all we're trying to do is maximize the one step reward right?",
                    "label": 0
                },
                {
                    "sent": "So we're just trying to maximize one step reward.",
                    "label": 0
                },
                {
                    "sent": "In the markup decision process, each action has some expected reward associated with it, so that will be returned by DP.",
                    "label": 0
                },
                {
                    "sent": "Is the action with the maximum effect reward end exactly what that maximum expected reward is?",
                    "label": 0
                },
                {
                    "sent": "If we have a two step problem, we're going to.",
                    "label": 0
                },
                {
                    "sent": "If we take some action, we have a distribution over the next States and a distribution over our first step rewards, and then conditioned on following it is simpler.",
                    "label": 0
                },
                {
                    "sent": "Next state we're going to have for each.",
                    "label": 0
                },
                {
                    "sent": "For each action, we're going to have an expected sum of rewards, so.",
                    "label": 0
                },
                {
                    "sent": "We can take the maximum over actions of the expected sum of the expected one step reward.",
                    "label": 0
                },
                {
                    "sent": "And then we can take an expectation over the state that we might land in.",
                    "label": 0
                },
                {
                    "sent": "Of this maximum over the one step reward and then we can take a maximum over the actions of the expected some over the state.",
                    "label": 0
                },
                {
                    "sent": "We might end in of the maximum over the next step rewards, so this is a recursively defined copy and we can't get anything.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Earlier, we're going to take the maximum over actions of the expectation over the next states of the reward we might achieve.",
                    "label": 0
                },
                {
                    "sent": "Plus DP, so I'm just using the V part of DP here.",
                    "label": 0
                },
                {
                    "sent": "The expected sum of rewards that we can get.",
                    "label": 0
                },
                {
                    "sent": "Starting from the next state.",
                    "label": 0
                },
                {
                    "sent": "So if I know an MDP.",
                    "label": 0
                },
                {
                    "sent": "Then sort of existential E. There exists some optimal policy.",
                    "label": 0
                },
                {
                    "sent": "Indep is the subroutine which compute with the optimal policy is and what its value is.",
                    "label": 0
                },
                {
                    "sent": "It's just a recursively defined subroutine.",
                    "label": 0
                },
                {
                    "sent": "So initially you start with.",
                    "label": 0
                },
                {
                    "sent": "Start, you have to figure out what the maximum expected sum over T is the whole thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Stop that.",
                    "label": 0
                },
                {
                    "sent": "No reward yet.",
                    "label": 0
                },
                {
                    "sent": "The unknown and yeah.",
                    "label": 0
                },
                {
                    "sent": "What do we want, yeah?",
                    "label": 0
                },
                {
                    "sent": "And then you.",
                    "label": 0
                },
                {
                    "sent": "You take your next step in whatever the news value model the unknown and known model that's going to be about how it works.",
                    "label": 0
                },
                {
                    "sent": "Start, so we're starting with the empty history, which means that the known of HMDP is just everything is unknown right now, which means we're in the sub unknowns special state, and we always expect that we serve receive reward 0.",
                    "label": 0
                },
                {
                    "sent": "And then we start interacting in the world according to some way which I'll define in a moment.",
                    "label": 0
                },
                {
                    "sent": "And at some moment in the future we start to.",
                    "label": 0
                },
                {
                    "sent": "Learn what pieces of the world look like.",
                    "label": 0
                },
                {
                    "sent": "And once we have these pieces of the world, we can.",
                    "label": 0
                },
                {
                    "sent": "We have the structure required to call DP and try to act and some sort of reasonable way.",
                    "label": 0
                },
                {
                    "sent": "Young music take the Christian oh so.",
                    "label": 0
                },
                {
                    "sent": "Oh, what was the exact question?",
                    "label": 0
                },
                {
                    "sent": "We used to use your device in print.",
                    "label": 0
                },
                {
                    "sent": "My question was how do you initialize?",
                    "label": 0
                },
                {
                    "sent": "Summarize.",
                    "label": 0
                },
                {
                    "sent": "The album",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll get to the algorithm in a moment.",
                    "label": 0
                },
                {
                    "sent": "Actually, right here, pretty good.",
                    "label": 0
                },
                {
                    "sent": "So the keep the algorithm is going to use these internally maintained data structures.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is going to be 2 cases to start with.",
                    "label": 0
                },
                {
                    "sent": "Case one is we're in some state which is not unknown of H. Right, so so we're in some state and we just don't know what's going to happen if we take some action, right?",
                    "label": 0
                },
                {
                    "sent": "So then we're going to choose the least previously used action.",
                    "label": 0
                },
                {
                    "sent": "Very simple policy.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, maybe we're in some portion of known of age, so we were in some state where for every action we've executed at least 10 times.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to call DP.",
                    "label": 0
                },
                {
                    "sent": "And we're going to check to see.",
                    "label": 0
                },
                {
                    "sent": "If DP on the unknown of H is greater than some particular epsilon, some precision parameter, and if it is, we're going to act according to DP of unknown.",
                    "label": 0
                },
                {
                    "sent": "We're going to do this until either the state is unknown or T the actual.",
                    "label": 0
                },
                {
                    "sent": "The current number of interactions MoD Capital T = 0, and we're going to go back to one.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is what this line here is saying is.",
                    "label": 0
                },
                {
                    "sent": "If I believe is a good chance.",
                    "label": 0
                },
                {
                    "sent": "That I can actually reach an unknown state.",
                    "label": 0
                },
                {
                    "sent": "I will try to reach that unknown state.",
                    "label": 0
                },
                {
                    "sent": "And then otherwise I'm going to act according to the known of HMDP.",
                    "label": 0
                },
                {
                    "sent": "We go back to the definition of unknown of H is kind of a curious fact.",
                    "label": 0
                },
                {
                    "sent": "So we're going to receive reward one.",
                    "label": 0
                },
                {
                    "sent": "When we execute an action leading to an unknown state, right?",
                    "label": 0
                },
                {
                    "sent": "And what that means is that the output of DP unknown of H is going to be our probability that we believe we can reach an unknown state.",
                    "label": 0
                },
                {
                    "sent": "The probability in the unknown of HM DP that you can reach the unknown state.",
                    "label": 0
                },
                {
                    "sent": "Kind of carefully arranged things just so this is true.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is the cubed algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's going to explicitly choose to explore or exploit.",
                    "label": 0
                },
                {
                    "sent": "OK, so are there questions about this algorithm?",
                    "label": 0
                },
                {
                    "sent": "Can you explain the timotea cozero?",
                    "label": 0
                },
                {
                    "sent": "Write team Marty equals 0.",
                    "label": 0
                },
                {
                    "sent": "So when we're doing exploration, it could be that we have some reasonable probability of reaching an unknown state.",
                    "label": 0
                },
                {
                    "sent": "But it's that probably just not realized because we have stochastic actions in the system or actions with the skeptic outcome.",
                    "label": 0
                },
                {
                    "sent": "So if the stochastic outcome is that we don't reach an unknown state, then we need to.",
                    "label": 0
                },
                {
                    "sent": "We need to cease trying to explore at some point in the future.",
                    "label": 0
                },
                {
                    "sent": "Because it could be that we just we lose our chance and then we're in a deterministic world where we can't even reach an unknown state, right?",
                    "label": 0
                },
                {
                    "sent": "And then continuing to execute the exploration policy would cause us to violate the sample complexity guarantee.",
                    "label": 0
                },
                {
                    "sent": "So this is just a timeout.",
                    "label": 0
                },
                {
                    "sent": "It says that if you haven't reached, I don't know Satan.",
                    "label": 0
                },
                {
                    "sent": "Some reasonable time horizon then just give up and try to do something more sane.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Now I want to go through the proof.",
                    "label": 0
                },
                {
                    "sent": "The proof is even more complex.",
                    "label": 0
                },
                {
                    "sent": "So we have that room DP in the world.",
                    "label": 0
                },
                {
                    "sent": "We have a known of ATP which is in the Y cubed algorithm.",
                    "label": 0
                },
                {
                    "sent": "We have the unknown of H&P which is in the Y cubed algorithm and then inside the proof there's going to be two other MDP's.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "MDP sub KH is going to be like the known of HMD peaks if we're going to.",
                    "label": 0
                },
                {
                    "sent": "So for each state and none of H corresponds to some state in in the real MVP, right?",
                    "label": 0
                },
                {
                    "sent": "So let's take the real MVP.",
                    "label": 0
                },
                {
                    "sent": "And use the real probability of next date given action state and the real expected reward.",
                    "label": 0
                },
                {
                    "sent": "And let's define a new P which just uses the states in the Nanavati MD NPI.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of halfway in between none of H&M DP.",
                    "label": 0
                },
                {
                    "sent": "Right, so MDP of sub KH has all the states of none of H, but it uses the exact transition transitions of the real MVP in the world.",
                    "label": 0
                },
                {
                    "sent": "An MDP sub U of H is the same as cave excepting between unknown of H&M DP.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have 5 mini peas.",
                    "label": 0
                },
                {
                    "sent": "There's the MDP in the real world.",
                    "label": 0
                },
                {
                    "sent": "There's the MDP's.",
                    "label": 0
                },
                {
                    "sent": "There's two peas in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "There's two extra MP's which are only in the proof.",
                    "label": 0
                },
                {
                    "sent": "Not the kind of nice intermediate things to think about.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a sketch of how the proof goes.",
                    "label": 0
                },
                {
                    "sent": "It's all here, so there's going to be some sort of simulation lemma.",
                    "label": 0
                },
                {
                    "sent": "The simulations lemma says that when you set this in parameter to be large enough.",
                    "label": 0
                },
                {
                    "sent": "Then the difference between dynamic programs on the MVP of known of H and a dynamic program on none of it's going to be less than.",
                    "label": 0
                },
                {
                    "sent": "Some small quantity.",
                    "label": 0
                },
                {
                    "sent": "This kind of makes sense so so that in parameter was how many times do you transition over a particular action in a particular state, right?",
                    "label": 0
                },
                {
                    "sent": "We turn this many times.",
                    "label": 0
                },
                {
                    "sent": "Then your estimate of the probability of the next state is going to converge to the truth, and if these estimates converge very closely, then any dynamic program run on these two MVP's will be almost the same.",
                    "label": 0
                },
                {
                    "sent": "In terms of the value of the maximum expected sum of rewards in return.",
                    "label": 0
                },
                {
                    "sent": "When is the explore exploit lemma?",
                    "label": 0
                },
                {
                    "sent": "The Explore exploit lemma says that.",
                    "label": 0
                },
                {
                    "sent": "The value of dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "On known MVP, said none of each.",
                    "label": 0
                },
                {
                    "sent": "Plus T times.",
                    "label": 0
                },
                {
                    "sent": "The value dinner programming on MDP Simone of age is greater than greater than the best you can do in the real world.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the two fundamental lemmas.",
                    "label": 0
                },
                {
                    "sent": "One of them says.",
                    "label": 0
                },
                {
                    "sent": "The internal data structure of the cubed algorithm is going to converge to the truth.",
                    "label": 0
                },
                {
                    "sent": "It's going to converge well enough that we can estimate things well.",
                    "label": 0
                },
                {
                    "sent": "And then the Explore exploit lemma says something about.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Our belief.",
                    "label": 0
                },
                {
                    "sent": "About how well we can explore.",
                    "label": 0
                },
                {
                    "sent": "Remember, this is the probability that you can actually reach an unknown state.",
                    "label": 0
                },
                {
                    "sent": "Because of the way we define things, and this is what our maximum expected sum of rewards is in known of HMDP in the real world.",
                    "label": 0
                },
                {
                    "sent": "So this sum is greater than.",
                    "label": 0
                },
                {
                    "sent": "The best that the optimal policy can do in the real world.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way we use these two limits is we say.",
                    "label": 0
                },
                {
                    "sent": "Going to choose an end.",
                    "label": 0
                },
                {
                    "sent": "Just that the simulation lemon bites pretty hard, so in particular.",
                    "label": 0
                },
                {
                    "sent": "This one over Poly is very small.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "The Explore exploit lemma.",
                    "label": 0
                },
                {
                    "sent": "It's going to imply the difference between how well we can do in the real world and how well we can do in the known of HM DP.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is going to imply that if this difference is large.",
                    "label": 0
                },
                {
                    "sent": "If how well we can do in the real world.",
                    "label": 0
                },
                {
                    "sent": "And how well we can do in the known of H MDP remember?",
                    "label": 0
                },
                {
                    "sent": "Because of the simulation lemma, this is almost the same as DP of none of age, right?",
                    "label": 0
                },
                {
                    "sent": "So the difference in how well we can do in the real world, and how well we can do in the known of HMDP, is greater than epsilon.",
                    "label": 0
                },
                {
                    "sent": "That's going to imply that.",
                    "label": 0
                },
                {
                    "sent": "The probability of good exploration is going to be greater than some reasonable quantity.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of two possibilities either.",
                    "label": 0
                },
                {
                    "sent": "The inequality goes this way, in which case we're done because acting according to our Nanavati MD is going to be near optimal or the inequality goes this way, in which case the probability of exploration has to be recently large.",
                    "label": 0
                },
                {
                    "sent": "And then of course, if the probability of good exploration is reasonably large.",
                    "label": 0
                },
                {
                    "sent": "Because we can only successfully explore something like this many times.",
                    "label": 0
                },
                {
                    "sent": "So the definition of good exploration is we encounter some state where there is some action we have executed less than 10 times.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "We encounter some state.",
                    "label": 0
                },
                {
                    "sent": "So the number of times that we can encounter a state where there's an action that we've executed lesson in times.",
                    "label": 0
                },
                {
                    "sent": "Something like in times essay.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have some reasonable probability of achieving this, which is going to be.",
                    "label": 0
                },
                {
                    "sent": "Well, it's epsilon over T, so the expected number of times that we can actually.",
                    "label": 0
                },
                {
                    "sent": "Try to explore.",
                    "label": 0
                },
                {
                    "sent": "It's going to be like in S80 over epsilon.",
                    "label": 0
                },
                {
                    "sent": "And then we have that that time out that condition, which means that every attempt to explore just uses up at most T steps.",
                    "label": 0
                },
                {
                    "sent": "And then you can apply some.",
                    "label": 0
                },
                {
                    "sent": "Some turn off bound, which is the truth is close to the expectation, and the proof goes through.",
                    "label": 0
                },
                {
                    "sent": "Are there questions about this?",
                    "label": 0
                },
                {
                    "sent": "OK, so in the notes written out how to do the simulation lemma in the Explore exploit lemma.",
                    "label": 0
                },
                {
                    "sent": "But I think that I'll skip them at the moment.",
                    "label": 0
                },
                {
                    "sent": "And go on so.",
                    "label": 0
                },
                {
                    "sent": "This is this is.",
                    "label": 0
                },
                {
                    "sent": "Is it for the cubed algorithm?",
                    "label": 0
                },
                {
                    "sent": "So let me just go back to the theorem statement.",
                    "label": 0
                },
                {
                    "sent": "It's kind of appealing.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have this algorithm E ^3.",
                    "label": 0
                },
                {
                    "sent": "The claim is that for all MDP's with high probability.",
                    "label": 0
                },
                {
                    "sent": "For all except some.",
                    "label": 0
                },
                {
                    "sent": "Polynomial number of steps we can behave near optimally.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of encouraging, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, we can behave near optimally.",
                    "label": 0
                },
                {
                    "sent": "There's some issues, of course, with what exactly this Poly is, but you know, news.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a few modifications to this basic statement which are worth knowing about.",
                    "label": 0
                },
                {
                    "sent": "So, there's this armax modification.",
                    "label": 0
                },
                {
                    "sent": "So what they are Max modification is is.",
                    "label": 0
                },
                {
                    "sent": "You have a slightly different algorithm.",
                    "label": 0
                },
                {
                    "sent": "So first step is the same.",
                    "label": 0
                },
                {
                    "sent": "The second step is.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, act according to this thing.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                },
                {
                    "sent": "How do I add to MDP together, right in the way that since these MVP's have the same structure, the only difference is the rewards?",
                    "label": 0
                },
                {
                    "sent": "What I mean here is I sum up the rewards right?",
                    "label": 0
                },
                {
                    "sent": "So if it's this?",
                    "label": 0
                },
                {
                    "sent": "Added in DP has reward one if there's reward.",
                    "label": 0
                },
                {
                    "sent": "Wanna neither this in new Peor that MVP.",
                    "label": 0
                },
                {
                    "sent": "So plus it adds reward on each edge.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a simpler algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's kind of intuitive algorithm.",
                    "label": 0
                },
                {
                    "sent": "It says it's kind of interesting says that.",
                    "label": 0
                },
                {
                    "sent": "You can be acting according to this combined MDP.",
                    "label": 0
                },
                {
                    "sent": "And you don't know if you're exploring or you're exploiting.",
                    "label": 0
                },
                {
                    "sent": "You're just behaving as seems natural to you.",
                    "label": 0
                },
                {
                    "sent": "You're optimizing one particular criteria, which is a combination of your desire to explore your desire to exploit.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that when you do that, you can still prove the same guarantee.",
                    "label": 0
                },
                {
                    "sent": "When you add the two MVP's, does it become unknown?",
                    "label": 0
                },
                {
                    "sent": "Because the only difference between the two is the transition out from the.",
                    "label": 0
                },
                {
                    "sent": "From the known to the Unknown, one is 1 and the other is 0.",
                    "label": 0
                },
                {
                    "sent": "So you add the reward.",
                    "label": 0
                },
                {
                    "sent": "You get one so.",
                    "label": 0
                },
                {
                    "sent": "Sum of two ICP.",
                    "label": 0
                },
                {
                    "sent": "The one with the.",
                    "label": 0
                },
                {
                    "sent": "With that one.",
                    "label": 0
                },
                {
                    "sent": "So we this is the known of ATP at some point in our exploration process, if we add the.",
                    "label": 0
                },
                {
                    "sent": "The unknown unknown happened 0 from management as to the SNL right.",
                    "label": 0
                },
                {
                    "sent": "The unknown I see, so the unknown would actually have a one here.",
                    "label": 0
                },
                {
                    "sent": "So the unknown, whenever you transition from a known state to an unknown state has a reward of 1.",
                    "label": 0
                },
                {
                    "sent": "And we should go back to here.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is this is this is a known.",
                    "label": 0
                },
                {
                    "sent": "And the unknown will be the same, except we'll have a reward of 1 here.",
                    "label": 0
                },
                {
                    "sent": "So when you add that up, when you get unknown, you get unknown, but you wouldn't necessarily to know 'cause it might be there.",
                    "label": 0
                },
                {
                    "sent": "This reward one here.",
                    "label": 0
                },
                {
                    "sent": "So let's say paint, let's say that this is 1.",
                    "label": 0
                },
                {
                    "sent": "Then this would also be one.",
                    "label": 0
                },
                {
                    "sent": "And then when you add in unknown, you get a one and a one which is not the same as what you.",
                    "label": 0
                },
                {
                    "sent": "Have here right?",
                    "label": 0
                },
                {
                    "sent": "All the rewards for the unknown MVP is zero except for traditions from.",
                    "label": 0
                },
                {
                    "sent": "The known state to the unknown state.",
                    "label": 0
                },
                {
                    "sent": "So in unknown of age you get reward for just transitioning from a known state to an unknown state, and at no other time.",
                    "label": 0
                },
                {
                    "sent": "OK, little bit.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                },
                {
                    "sent": "Can you can you push your button?",
                    "label": 0
                },
                {
                    "sent": "What is lies says unequal too.",
                    "label": 0
                },
                {
                    "sent": "No, except for the reward.",
                    "label": 0
                },
                {
                    "sent": "Going from the note states, the unknown state.",
                    "label": 0
                },
                {
                    "sent": "Play.",
                    "label": 0
                },
                {
                    "sent": "Right, this is what you're talking about, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                },
                {
                    "sent": "After that transition from the known to the unknown.",
                    "label": 0
                },
                {
                    "sent": "So if you add those two up, once you get.",
                    "label": 0
                },
                {
                    "sent": "So this says that if you're in a known state.",
                    "label": 0
                },
                {
                    "sent": "Any transition to an unknown state then you get reward 1, otherwise you get reward 0.",
                    "label": 0
                },
                {
                    "sent": "Oh all the others.",
                    "label": 0
                },
                {
                    "sent": "We want to say yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of a nice appealing modification because it says that.",
                    "label": 0
                },
                {
                    "sent": "You can just be optimizing one thing.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Another modification.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's this delayed Q learning algorithm and the claim is that we can reduce this polynomial to be.",
                    "label": 0
                },
                {
                    "sent": "Oh~ of essay using a particular algorithm, so I'm not going to say because it's a little bit complex, but it's interesting that you can reduce it.",
                    "label": 0
                },
                {
                    "sent": "You can reduce the dependence upon States and actions to just be.",
                    "label": 0
                },
                {
                    "sent": "Number of states similar actions per state.",
                    "label": 0
                },
                {
                    "sent": "This is like the total number of actions in the entire process, right?",
                    "label": 0
                },
                {
                    "sent": "So in particular, maybe this is interesting because let's think for a moment if we have.",
                    "label": 0
                },
                {
                    "sent": "This P of S prime given A&S.",
                    "label": 0
                },
                {
                    "sent": "How many bits does it take to specify this?",
                    "label": 0
                },
                {
                    "sent": "So for any particular action, and in particular state, any particular next state, you have some particular conditional probability, right?",
                    "label": 0
                },
                {
                    "sent": "So let's say we want to specify to some precision.",
                    "label": 0
                },
                {
                    "sent": "The number of bits required to specify this.",
                    "label": 0
                },
                {
                    "sent": "He's going to be.",
                    "label": 0
                },
                {
                    "sent": "Oh of S ^2 a.",
                    "label": 0
                },
                {
                    "sent": "Right, so the number of bits required to specify the actual Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "Is order X squared a an amount of interaction we need with the world in order to behave near optimally?",
                    "label": 0
                },
                {
                    "sent": "Is only OSA?",
                    "label": 0
                },
                {
                    "sent": "To the~ here means that there's some some terms depend upon log of essay, so this says that we can.",
                    "label": 0
                },
                {
                    "sent": "Interact with the world and an amount which is less than the description length of the world.",
                    "label": 0
                },
                {
                    "sent": "In order to behave near optimally.",
                    "label": 0
                },
                {
                    "sent": "Kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "It was just mostly interesting to me because there have been debates in reinforcement learning about whether not it was necessary to have an algorithm which internally built a model of the world.",
                    "label": 0
                },
                {
                    "sent": "This this algorithm doesn't, and it can't because the amount of experience that it uses is less than the amount required to actually build an accurate internal model.",
                    "label": 0
                },
                {
                    "sent": "How do you qualify as a?",
                    "label": 0
                },
                {
                    "sent": "This is a state.",
                    "label": 0
                },
                {
                    "sent": "Oh, right, so there's a little bit overloading here, so lower case S is a state.",
                    "label": 0
                },
                {
                    "sent": "Upper Case S is either the number of states like here, or is the set of states.",
                    "label": 0
                },
                {
                    "sent": "So this is the number of States and the number of actions.",
                    "label": 0
                },
                {
                    "sent": "So this is another station times the number of actions per state.",
                    "label": 0
                },
                {
                    "sent": "So I guess another thing that you can realize that this algorithm is significantly different from the cubed algorithm because.",
                    "label": 0
                },
                {
                    "sent": "This strategy of trying to build up a model the world can't work with that sample complexity, right?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "These are simplicity results, and then I want to go through a short section where I sort of talk about.",
                    "label": 0
                },
                {
                    "sent": "The limitations of the sample complexity approach to reinforcement learning and what people have done to try to address these limitations.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first limitation is given to you by lower bound.",
                    "label": 0
                },
                {
                    "sent": "So the claim is that any algorithm A which satisfies the cube statement must use at least.",
                    "label": 0
                },
                {
                    "sent": "TSA actions to explore.",
                    "label": 0
                },
                {
                    "sent": "Right, so we have to allow ourselves.",
                    "label": 0
                },
                {
                    "sent": "To mark off, at least TSA actions in order to to get any cubelike statement.",
                    "label": 0
                },
                {
                    "sent": "The somewhat stronger lower bounds, which actually involve the epsilon, but we won't worry about that because this is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient and the proof is actually very simple.",
                    "label": 0
                },
                {
                    "sent": "The proof is just.",
                    "label": 0
                },
                {
                    "sent": "Here's an MDP here.",
                    "label": 0
                },
                {
                    "sent": "Here's the family of MDP's, and the claim is you need TSA actions to learn to behave near optimally.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is I have a great way of demonstrating this, so here we have some sort of pathway.",
                    "label": 0
                },
                {
                    "sent": "The pathway kind of goes up right?",
                    "label": 0
                },
                {
                    "sent": "So if I behave if I behave.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe I should try to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have some sort of pathway.",
                    "label": 0
                },
                {
                    "sent": "And we have maybe two actions.",
                    "label": 0
                },
                {
                    "sent": "Per state, right?",
                    "label": 0
                },
                {
                    "sent": "So we have a one and we have a 2.",
                    "label": 0
                },
                {
                    "sent": "Oh, to have a 3?",
                    "label": 0
                },
                {
                    "sent": "Oh yes, very good.",
                    "label": 0
                },
                {
                    "sent": "Three actions per state.",
                    "label": 0
                },
                {
                    "sent": "Have a 18283 and I'm going to think about a family of Markov decision processes.",
                    "label": 0
                },
                {
                    "sent": "Where the action which leads to the next state in the path is randomized, right, just independently.",
                    "label": 0
                },
                {
                    "sent": "She learning how to behave well here tells you nothing about how to behave well here, so maybe section A2 to get you the next date here.",
                    "label": 0
                },
                {
                    "sent": "Section A1, then Section 8, three and then it Section 8 two.",
                    "label": 0
                },
                {
                    "sent": "But for any Markov decision process in this in this family.",
                    "label": 0
                },
                {
                    "sent": "Knowing how to behave here tells you nothing about how to behave here and then all of their actions just take you back to the beginning.",
                    "label": 0
                },
                {
                    "sent": "So let's think about what we have to do in order to figure out how to get to the end of the path and receive the reward, right?",
                    "label": 0
                },
                {
                    "sent": "In every state you have to execute about half the actions in order to figure out which ones take it to the next state.",
                    "label": 0
                },
                {
                    "sent": "And every time you execute the wrong action, you go back to the beginning.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "In every state you have to execute half the action, so it's S times A and you go back to the beginning, which is where the T comes in.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "You can't do better than TSA, and this is distressing.",
                    "label": 0
                },
                {
                    "sent": "The reason why this is distressing is because in the real world, the number of actions.",
                    "label": 0
                },
                {
                    "sent": "If you're going to talk to a physicist.",
                    "label": 0
                },
                {
                    "sent": "And you ask him how many states are there right?",
                    "label": 0
                },
                {
                    "sent": "Until, well, you know, maybe you can.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can kind of alias things which are the same as to, like the Planck scale, which is like 10 to the minus 34 meters.",
                    "label": 0
                },
                {
                    "sent": "So the number of states.",
                    "label": 0
                },
                {
                    "sent": "Which specify your location within this room.",
                    "label": 0
                },
                {
                    "sent": "Is larger than the memory of any computer that we can ever hope to make.",
                    "label": 0
                },
                {
                    "sent": "And this says that.",
                    "label": 0
                },
                {
                    "sent": "You know for general MVP.",
                    "label": 0
                },
                {
                    "sent": "You're going to depend upon the number of states.",
                    "label": 0
                },
                {
                    "sent": "If it's a bit of a problem, right?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So lower bounds is really big.",
                    "label": 0
                },
                {
                    "sent": "Problems are not going to be solvable.",
                    "label": 0
                },
                {
                    "sent": "By the cubed algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then we have a sort of conundrum because we know that the problems that we want to solve are solvable because reinforcement learning is kind of just a mathematical description of life.",
                    "label": 0
                },
                {
                    "sent": "You are an agent wandering in a world.",
                    "label": 0
                },
                {
                    "sent": "And you solve your exploration and exploitation problems every day, right?",
                    "label": 0
                },
                {
                    "sent": "So somehow the math is failing to capture.",
                    "label": 0
                },
                {
                    "sent": "What is actually doable?",
                    "label": 0
                },
                {
                    "sent": "So what that means mathematically is that we need to either make more or different assumptions and have been several attempts to make more assumptions stronger assumptions in order to capture what's going to happen in the real world.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of these variants is called factor D cubed.",
                    "label": 0
                },
                {
                    "sent": "The basic idea infected cubed is.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a lot of different people in this room and I think none of you are actually touching.",
                    "label": 0
                },
                {
                    "sent": "What that means is that.",
                    "label": 0
                },
                {
                    "sent": "Around you, each of you can move independently of the other person, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The things you can do individually.",
                    "label": 0
                },
                {
                    "sent": "Factor with respect to things that somebody else can do individually.",
                    "label": 0
                },
                {
                    "sent": "So we could imagine some sort of system where.",
                    "label": 0
                },
                {
                    "sent": "You decide to rotate your chair and maybe that increases your reward in some way and somebody else can decide to rotate their chair in some other way and they can increase the reward that way, right?",
                    "label": 0
                },
                {
                    "sent": "So in a factored MDP.",
                    "label": 0
                },
                {
                    "sent": "There's a set of bits which may be individual people, and the claim is that the transition probability.",
                    "label": 0
                },
                {
                    "sent": "Just depends upon some other small subset of bits.",
                    "label": 0
                },
                {
                    "sent": "In particular, might just depend upon a panyu, so it could be just B1, depends upon.",
                    "label": 0
                },
                {
                    "sent": "This capital B Subiya which is contains B1.",
                    "label": 0
                },
                {
                    "sent": "So, so this is the notion of factoring.",
                    "label": 0
                },
                {
                    "sent": "Factoring is extremely powerful.",
                    "label": 0
                },
                {
                    "sent": "I mean, factoring is used all the time in science, it's kind of.",
                    "label": 0
                },
                {
                    "sent": "Refactoring if you think about it, is sort of.",
                    "label": 0
                },
                {
                    "sent": "How do you think about?",
                    "label": 0
                },
                {
                    "sent": "The fall of an Apple being equivalent to.",
                    "label": 0
                },
                {
                    "sent": "Oh my mikes not working interesting.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Hello, excellent, so factoring is very strong.",
                    "label": 0
                },
                {
                    "sent": "It's how in science we can think about.",
                    "label": 0
                },
                {
                    "sent": "Discussion of gravity which affects the movement of planets in the fall of apples.",
                    "label": 0
                },
                {
                    "sent": "But OK, so we can make some sort of statement.",
                    "label": 0
                },
                {
                    "sent": "Yeah, if you know the factoring in advance.",
                    "label": 0
                },
                {
                    "sent": "So you know which subset of the bits is critical.",
                    "label": 0
                },
                {
                    "sent": "In specifying the probability of outcome of any individual bit.",
                    "label": 0
                },
                {
                    "sent": "Then we can make the factory Cube statement, which is that.",
                    "label": 0
                },
                {
                    "sent": "For all factored MDP's.",
                    "label": 0
                },
                {
                    "sent": "With high probability.",
                    "label": 0
                },
                {
                    "sent": "In polynomial this description link to the factory MVP so that factored in deciding to have a description link which is much smaller than the description length of the full MVP.",
                    "label": 0
                },
                {
                    "sent": "And for this reason, is falling to pieces is going to be.",
                    "label": 0
                },
                {
                    "sent": "Squared a.",
                    "label": 0
                },
                {
                    "sent": "When when is this strong factoring so the set Capital B survives is much smaller than the set of all bits.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For all except some number of actions, we can make any cube statement.",
                    "label": 0
                },
                {
                    "sent": "We can behave near optimally.",
                    "label": 0
                },
                {
                    "sent": "Everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So I guess the first claim here is this isn't quite enough.",
                    "label": 0
                },
                {
                    "sent": "If you just if you think about what happens if everybody tries to go to lunch at the same time, there's going to be some collisions.",
                    "label": 0
                },
                {
                    "sent": "That means that this is going to be not much as much factoring as you might hope.",
                    "label": 0
                },
                {
                    "sent": "There's another kind of severe difficulty in here, which is where we're something hidden in the factory cubed algorithm, which is the ability to do dynamic programming on a factored MDP.",
                    "label": 0
                },
                {
                    "sent": "And turned up.",
                    "label": 0
                },
                {
                    "sent": "This is a very difficult thing to do.",
                    "label": 0
                },
                {
                    "sent": "So, and it's sort of.",
                    "label": 0
                },
                {
                    "sent": "It's not even like NP hard.",
                    "label": 0
                },
                {
                    "sent": "It's kind of representation.",
                    "label": 0
                },
                {
                    "sent": "Lee hard to even specify how you would do it.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is another approach people use is called magic metric cubed.",
                    "label": 0
                },
                {
                    "sent": "So if you go back to the factoring.",
                    "label": 0
                },
                {
                    "sent": "If you just think about.",
                    "label": 0
                },
                {
                    "sent": "Even if we believe that the factoring is is simple an even if we manage to overcome this computational difficulty, which may be able to do in practice.",
                    "label": 0
                },
                {
                    "sent": "There's still an issue because even for you, individually in this room the number of states for you, let alone the cross product of all of you.",
                    "label": 0
                },
                {
                    "sent": "Is is just too large?",
                    "label": 0
                },
                {
                    "sent": "The number of possible positions poses you could have in this room is.",
                    "label": 0
                },
                {
                    "sent": "More than his representable on any computer.",
                    "label": 0
                },
                {
                    "sent": "So we want some approach which allows us to avoid this.",
                    "label": 0
                },
                {
                    "sent": "So we're going to assume.",
                    "label": 0
                },
                {
                    "sent": "That there exists some modeling function, right?",
                    "label": 0
                },
                {
                    "sent": "The modeling function is going to take a history of interactions with the world and a state.",
                    "label": 0
                },
                {
                    "sent": "And we're going to assume this modeling function.",
                    "label": 0
                },
                {
                    "sent": "What's going on with lots of succeed in the real world is we can somehow generalize past experiences to predict future experiences without actually having experienced precisely the same.",
                    "label": 0
                },
                {
                    "sent": "Experience.",
                    "label": 0
                },
                {
                    "sent": "Right, so we're not somehow in the real world.",
                    "label": 0
                },
                {
                    "sent": "We don't have to experience the state in action in order to figure out what's going to happen next.",
                    "label": 0
                },
                {
                    "sent": "Who want to quantify that somehow that's quantified according to this assumption, we're going to say, suppose they're just a modeling algorithm, something which let's generalize, such that if there are any experiences which are nearby according to some metric.",
                    "label": 0
                },
                {
                    "sent": "Then the model is going to output in next date in a reward from almost the same distribution as the real world.",
                    "label": 0
                },
                {
                    "sent": "Is one way to capture the notion of ability to generalize?",
                    "label": 0
                },
                {
                    "sent": "And the claim is there is an algorithm metric y ^3.",
                    "label": 0
                },
                {
                    "sent": "Which uses this modeling.",
                    "label": 0
                },
                {
                    "sent": "Operation.",
                    "label": 0
                },
                {
                    "sent": "Just that for all metric NDPS.",
                    "label": 0
                },
                {
                    "sent": "So Magic MDP is like an MDP, except.",
                    "label": 0
                },
                {
                    "sent": "This modeling assumption holds where using this metric.",
                    "label": 0
                },
                {
                    "sent": "Probably 1 minus Delta.",
                    "label": 0
                },
                {
                    "sent": "For all except.",
                    "label": 0
                },
                {
                    "sent": "Some number of samples, the EQ statements going to hold.",
                    "label": 0
                },
                {
                    "sent": "And the thing which is new here is we have a covering number on the MDP according to this metric.",
                    "label": 0
                },
                {
                    "sent": "The covering number is sort of how many States and actions do you have to experience in the real world in MDP such that every state in action is near to something that you've experienced.",
                    "label": 0
                },
                {
                    "sent": "The covering number for an MVP for metric MVP is the number of transitions that you have to experience.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                },
                {
                    "sent": "Every sidaction is near according to the metric to one of your experiences.",
                    "label": 0
                },
                {
                    "sent": "Covers is a very intuitive notion, but let me try to make it more intuitive.",
                    "label": 0
                },
                {
                    "sent": "So let's think about just covering this room right?",
                    "label": 0
                },
                {
                    "sent": "So let's say we wanted to cover this room to within 10 meters, right?",
                    "label": 0
                },
                {
                    "sent": "So then maybe we could think about going there and there and there and there and there and there and there.",
                    "label": 0
                },
                {
                    "sent": "And it and then so I pointed out a bunch of points in the room.",
                    "label": 0
                },
                {
                    "sent": "The payments, these points cover all locations in this room to within 10 meters.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's a set such that everything in a larger set is near to something in the smaller set.",
                    "label": 0
                },
                {
                    "sent": "Sorry cover.",
                    "label": 0
                },
                {
                    "sent": "How do you cover?",
                    "label": 0
                },
                {
                    "sent": "BP.",
                    "label": 0
                },
                {
                    "sent": "When you cover Andy, what do you mean right?",
                    "label": 0
                },
                {
                    "sent": "So what I mean is?",
                    "label": 0
                },
                {
                    "sent": "So I'm covering a metric MDP.",
                    "label": 0
                },
                {
                    "sent": "So there's an MDP, and there's a metric on this MDP, so we have this metric D, right?",
                    "label": 0
                },
                {
                    "sent": "The metric is just going to measure how far apart I stayed.",
                    "label": 0
                },
                {
                    "sent": "The action is from another state action.",
                    "label": 0
                },
                {
                    "sent": "And so the cover on the metric MDP is going to be.",
                    "label": 0
                },
                {
                    "sent": "A set of states.",
                    "label": 0
                },
                {
                    "sent": "An action such that every state in action is near something in that set.",
                    "label": 0
                },
                {
                    "sent": "So there's still some issues here.",
                    "label": 0
                },
                {
                    "sent": "Clear that we can reasonably expect model to be this accurate, is saying that.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it a little bit, it's actually pretty hard too.",
                    "label": 0
                },
                {
                    "sent": "Model The world so well that you know the distribution of our next states.",
                    "label": 0
                },
                {
                    "sent": "Precisely enough is required here, and I think it's the most significant drawback.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, and then there's a really fundamental problem.",
                    "label": 0
                },
                {
                    "sent": "Which this cartoon explains.",
                    "label": 0
                },
                {
                    "sent": "If you think about what these this EQ statement is saying.",
                    "label": 0
                },
                {
                    "sent": "It's saying that wherever you end up.",
                    "label": 0
                },
                {
                    "sent": "You're going to behave in your optimally, right?",
                    "label": 0
                },
                {
                    "sent": "But the E ^3.",
                    "label": 0
                },
                {
                    "sent": "It's not me.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The cubed algorithm is.",
                    "label": 0
                },
                {
                    "sent": "He's kind of a very eager exploration algorithm, so if it finds a Cliff, it'll very happily jump off a Cliff.",
                    "label": 0
                },
                {
                    "sent": "And then if it lands, the bottom is agent leaves, the bottom has a broken neck, so it can't move at all.",
                    "label": 1
                },
                {
                    "sent": "Then the cube statement holds.",
                    "label": 0
                },
                {
                    "sent": "It will behave near optimally given where it is right?",
                    "label": 0
                },
                {
                    "sent": "Just going to be like.",
                    "label": 1
                },
                {
                    "sent": "It's the best you can do when you have a broken neck.",
                    "label": 0
                },
                {
                    "sent": "So there's some failure.",
                    "label": 0
                },
                {
                    "sent": "To capture what we actually want in the cube statement.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of discussion, so I guess these guys have worried about trying to make a helicopter fly.",
                    "label": 0
                },
                {
                    "sent": "If you imagine trying to use the cube to fly helicopter, you're going to have a lot of wrecked helicopters, so some people have been doing some work trying to figure out how to avoid this, but it's not that easy.",
                    "label": 0
                },
                {
                    "sent": "To me, this is the natural statement and.",
                    "label": 0
                },
                {
                    "sent": "We just need a little more insight to figure out what the right statement is.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think we should take a break now.",
                    "label": 0
                }
            ]
        }
    }
}