{
    "id": "dbpwvnyysqwcuwb37nvibyoiez2n5o47",
    "title": "Learning Various Classes of Models of Lexicographic Orderings",
    "info": {
        "author": [
            "J\u00e9r\u00f4me Mengin, University of Toulouse III: Paul Sabatier"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_mengin_lvcmlo/",
    "segmentation": [
        [
            "So.",
            "Richard Bution shattuck.",
            "Wilson better.",
            "A work in Mastercam in Thailand, and Jean long on general area work in Paris.",
            "And I come from Toulouse, and this was made possible by a nice grant from the French ministries for Foreign affairs, education and time in history.",
            "For travels between France and Thailand, which was nice.",
            "So."
        ],
        [
            "I think just two.",
            "To situate with what we're doing in comparison with what we've seen today.",
            "So we try to learn to other objects.",
            "I have a computer and I tell your domain.",
            "So I will often refer in examples to computers which I describe with the weather here 3.",
            "Attributes so the type without it can be the desktop or laptop color or black.",
            "They can have a DVD reader or writer.",
            "By the way, I usually use the term variable instead of being of more logical background than machine learning, and we often use variable you can understand it as feature or attribute.",
            "OK, and so the idea is to, for example, in your recommendation system to learn how a user orders these objects in order to suggest.",
            "Better objects, the best of ones.",
            "To future for future interaction or group of users.",
            "So I think what what is different from a I have seen today is that we want to to learn an explicit representation of the ordering, so not a function which just ranks the objects for example, but we want to have something more explicit that can be used for example to explain why some comparisons are made this way by the by the system.",
            "And of course we need a compact representation of the orderings, even if we don't have just a function or a system that does that we want.",
            "And one way to do that is to.",
            "To have local preferences.",
            "So on each attribute or variable and then to have some extra structure which is going to say how we aggregate or this or these preferences on each attribute to get a global relation or global ordering.",
            "I don't know, maybe thanks.",
            "So."
        ],
        [
            "Basically we are.",
            "I'm going to talk about two types of orderings.",
            "Lexicographic orderings at length, and I will after that refer to another type of ordering briefly.",
            "So lexicographic ordering.",
            "I don't know if it's how much you know about this, so I start by explaining some with details.",
            "So what it is so as I said, we have a local preference relation over each attribute, so here.",
            "For the type of computer, for example, here we have a relation that says that laptop preferred to desktops.",
            "And for the color here we have a relation that says that yellow is preferred to black OK, and on top of that we have this this arrow here which represents an important soldering between the two attributes and basically says that here the type of computer is more important than its color.",
            "When we want to order the objects.",
            "OK, so how do we use that?"
        ],
        [
            "So here I have objects where I have only two attributes.",
            "UI to make things a bit simpler.",
            "So here that says that black laptop is preferred to a yellow desktop and so how do we know that it's because we go down the tree on?",
            "Look at the first well here sequence.",
            "Look at the first node where the two objects are different and here they are.",
            "They have different types, so here it is a relation laptop preferred to desktop which decides which of the two objects is the preferred one.",
            "For the second pair, here it's both laptops, so at the first node it's not decided, so we go down a level and here one is yellow, the other one is black, so it's the ordering here yellow greater than preferred to black, which decides."
        ],
        [
            "So what is good with this type of model is that comparison are very easy to make.",
            "Now we just go down the sequence.",
            "You can learn that in polynomial time, so there have been two recent papers that prove this.",
            "I'm very well in somewhere different settings.",
            "It'll be different.",
            "The problem with that is it has a very weak expressive power, so you cannot represent.",
            "Essentially.",
            "Essentially, you cannot represent conditional preferences.",
            "And this is an example of something that you cannot represent you.",
            "You may want to say that for.",
            "For laptops, you prefer yellow one.",
            "And for a desktop you prefer a black one, for example, and that you cannot represent with a sequence like this one, just a simple lexicographic ordering."
        ],
        [
            "So.",
            "Recently there have been there have been a number of works on another type of preference representation.",
            "So conditional preference networks or CP Nets?",
            "So which is made explicitly to be able to represent conditional preferences?",
            "And so conditional local preferences like the one which is here, which says if I if I consider laptops then I prefer yellow one to block one OK. And here I have another rule that would say that for desktops I prefer black to yellow.",
            "And here I can also have rules simple rules without any conditionality that say that in general pressure laptop to desktops and so the mechanism for exploiting these these rules is different.",
            "You don't have this notion of sequence.",
            "Basically, it's so comparisons or other things being equal.",
            "So here for example.",
            "We have these two objects which only differ in one attribute.",
            "OK, they're both laptops, and the yellow is preferred to black, so we can say that.",
            "LY is greater than LV, and so on.",
            "You can only compare when the objects differ by one variable by one attribute, and since you assume that the ordering is transitive and you can have.",
            "A chain of comparisons which give.",
            "More interesting ordering, which is not total.",
            "In general, so this is."
        ],
        [
            "Expressive, the big problem is that comparisons are very difficult.",
            "It's NP complete engine in the general case.",
            "And they are hard to learn them, so there has been a there's quite a lot of work going on at the moment on this.",
            "There has been a session on CP net landing at each eye and basically what people do."
        ],
        [
            "They try to learn easy classes of CP Nets usually wear.",
            "Where the comparisons are easy to make or easy classes of examples or even incomplete algorithms."
        ],
        [
            "So what we're trying to do is find something between the two models where we could have better expressivity than with the lexicographic orderings, with the usual ones, and something easier to learn than CP Nets."
        ],
        [
            "Yeah, so the contribution of this paper is that actually it's possible to add conditionality in lexicographic preference models without increasing the complexity of reasoning or learning these models.",
            "So I'll start with some results about the so the basic care lexicographic."
        ],
        [
            "Now that I've just introduced him.",
            "So the sample complexity.",
            "So the VC dimension is N. Which is not surprising since it is a very weak model.",
            "So when I give the results, it's always about models where we have only binary variables.",
            "So assume also the examples sometimes."
        ],
        [
            "All the examples also have binary variable.",
            "So in the context of active learning, so here we have a learner that can ask the user queries with what he prefers between pairs of objects.",
            "Here for example LYVD and so the goal of the learner is to ask the right questions to learn the model of the user as quickly as possible.",
            "And so in this paper by yeah, so should have gone beyond others.",
            "It was proved that.",
            "You need log the log of factorial N queries in the worst case.",
            "So the best strategy you have in the worst case log of factor in queries and it was in a setting where the local preferences, so the preferences on each attribute were fixed at the beginning.",
            "So for example, when you.",
            "So I assume that if you want to buy a computer, should have to choose between one with a DVD writer and a DVD.",
            "Unique can write and read another one that can only read you will always prefer the one which can write so often you have the setting where the local preferences are known, but it's not always the case and what we proved we describe it in the paper is that in fact this result is easy to generalize to the case where.",
            "The local preferences have to be learned as well.",
            "You don't.",
            "You don't only want to learn the sequence of variables well, so the local preferences so under all you have is this end.",
            "Here that comes from.",
            "You have to identify for each variable which of the two possible values is greater one.",
            "So if you have any variable so."
        ],
        [
            "Now for passive learning.",
            "So here we have as examples a set of comparisons there always comparison between objects, and we want to output a structure which which gives the same comparisons for the pairs that we had in the examples."
        ],
        [
            "And so instead in the same paper, they give a greedy algorithm.",
            "That that works well, that's returned failure when it's not possible to output a structure which has the desired property.",
            "And.",
            "So this should be proved with this algorithm that.",
            "The passive learning.",
            "Is empty.",
            "So they prove it with fixed local preferences, and while it's easy to extend the algorithm to prove to use the same kind of algorithm, and so it proves that if you don't know the local preferences in advance, the problem is still in P. Two to find, so I'm going to describe the algorithm soon."
        ],
        [
            "Say.",
            "I first want to give a last result about these simple models.",
            "It's model optimization here you want basically want to see if there exists a model.",
            "Given so still, given the set of examples here, can you find a model that has less than K errors for fixed K?",
            "And this problem is NP complete, so it was proved in the other paper on preference learning.",
            "And so still with fixed local preferences, and we prove that it's still the case, is unknown local preferences.",
            "So we still at the same level.",
            "So now I'll give some detail on the on the greedy."
        ],
        [
            "Some because.",
            "I think it will clarify why where it's the results can be generalized to other models.",
            "So the algorithm is simple.",
            "You start, you have your set of examples.",
            "You don't have at the beginning.",
            "You have an empty sequence."
        ],
        [
            "Available so you must.",
            "It's so the algorithm works like a decision tree learning algorithm, so the same principle you must choose the first variable.",
            "So what can you choose as a valuable?",
            "You must choose a variable that she's not going to give any wrong comparison, because otherwise.",
            "You cannot get back the right comparison afterwards, so here I think here you have read a greater than sorry read a written right here and the opposite here.",
            "So the type of DVD unit cannot be used.",
            "You have yellow written and black on black greater than yellow, so the color cannot be used as either.",
            "If you choose any these two.",
            "Violate what gives gives the wrong.",
            "Result for this one example so."
        ],
        [
            "There remains only the type.",
            "And of course, for these examples doesn't matter.",
            "But for this one you have to choose a greater than D, so you have the first variable.",
            "Then you you did."
        ],
        [
            "The first example and then you have to choose the second variable.",
            "And I think here."
        ],
        [
            "Only the color works.",
            "Why only the color?",
            "Because here you still have red sorry reader, greater than right here on the right are greater than reader, so it doesn't work.",
            "But you have in both examples you have yellow on the left and black on the right, so it works."
        ],
        [
            "So this is the greedy algorithm, so you never have never have to backtrack.",
            "And it gives us success.",
            "So now what did we do, we do."
        ],
        [
            "Could use some conditionality in these these examples in these models.",
            "So the first thing we did was to introduce conditional local preferences.",
            "So here I still have.",
            "That's a laptop.",
            "I always prefer to desktops, but for the color I had, I have these two rules now that say that in the case of laptops, I prefer yellow to black and in the case of desktops I prefer black to yellow.",
            "So this is a."
        ],
        [
            "The results that we found with this type of model just by adding conditional local preferences.",
            "So now we have a much better sample complexity.",
            "It's two to the N -- 1.",
            "Which of course the mother is much richer than before.",
            "For active learning, we also have know that the number of queries work because the.",
            "The the class of models is much bigger.",
            "For active learning you need more queries to identify the right one, right model.",
            "But the good thing is that the greedy algorithm still works, so the passive learning is still in P. Under model optimization remains NPR."
        ],
        [
            "Wait at least NPR.",
            "So the greedy algorithm.",
            "I don't know if I need to do this, so it's exactly the same algorithm."
        ],
        [
            "So here I start this."
        ],
        [
            "Anyway, I I have almost the same example, except I change here black rather than yellow whether it was the opposite.",
            "Just to illustrate way, now we need this model, so the beginning."
        ],
        [
            "The same and then here.",
            "Here we have yellow, green and black on black rather than yellow.",
            "So we need 2 rules.",
            "For the color and we have this."
        ],
        [
            "To these tools, actually, here we could have chosen the type of right as well.",
            "We could have chosen.",
            "Let us another.",
            "And it's."
        ],
        [
            "Success.",
            "So no third kind.",
            "So we know trying to make sure that the model even richer.",
            "So now what we do is we want to be able to say that the importance of the attributes those depend on some other attributes attributes.",
            "So now instead of having a sequence of attributes will have a tree of attributes.",
            "So for desktops here.",
            "We can represent the fact that.",
            "The type of DVD unit is more important than the color.",
            "And it's it's the opposite on the on the other side.",
            "So for laptops the color is more important than the type of DVD unit.",
            "And we combine that with the conditional rules as well.",
            "And so we here we have a conditional rule that says that for desktops we prefer writer to a reader.",
            "Actually we have the same.",
            "Comparisons here.",
            "And for laptop yellow is better than black anfora.",
            "Uh.",
            "Add a desktop with the reader, then black is written and why?",
            "What is important here is that we have to add one rule.",
            "Well, we have to have local rules at each node.",
            "OK, so we have to come back to that on the next slide we have to.",
            "To have local preferences on the variables as well.",
            "If you were here."
        ],
        [
            "So that really need to be complete to say, actually the same for the other models.",
            "But of course, if the tree is not complete.",
            "We then have a partial ordering.",
            "Which may be a problem otherwise with the normal lexicographic ordering, you always get a total ordering of everything.",
            "So that's different from.",
            "I think this is the main difference with decision trees.",
            "For example with decision trees.",
            "Whatever the size of the tree, you can always classify everything here.",
            "If the tree is too small, you're going to get to have problems because there are some comparisons that cannot be made.",
            "So the result?"
        ],
        [
            "So, so we do with this extra possibility.",
            "We didn't change the sample complexity.",
            "Active learning gets even bigger.",
            "Here we had N factorial.",
            "Now we have something a bit bigger here.",
            "But the good thing is that the greedy algorithm still works.",
            "So we are still in P as far as passive learning is common is concerned and the model optimization is still NP complete.",
            "So no."
        ],
        [
            "Another family on here will have something be surprising.",
            "So at first we had nothing conditional.",
            "Then I say we can have conditional preferences and still a sequence of viable.",
            "Then we I said we can have conditional important unconditional preferences.",
            "So of course the remaining type of structure that we could have is.",
            "We could have a tree for the important but still have only unconditional local preferences.",
            "And in this case."
        ],
        [
            "So for simplicity, we don't know what it is.",
            "Active learning, we still have the same type of.",
            "For a formula and the bad thing with this model is that now passive learning becomes NP complete.",
            "So what happens is that you cannot construct the tree in a recursive manner because at some point, for example, if I go down the first branch here and then I have to choose here for the examples which on this branch I have to choose some local preference relation for the IT may not be the same.",
            "The right one for the note that I will construct later here.",
            "So here we have to backtrack when you can't read the tree.",
            "So This is why it becomes NP complete, so this one is a.",
            "Is not a good model.",
            "Right?"
        ],
        [
            "So just to.",
            "Another view of the results that we have.",
            "So.",
            "Basically, what's interesting here?",
            "So as we concern, is that so most of them are for most of the of the of the models.",
            "We get the same algorithm and the same complexity of passive learning.",
            "For model approximation, it's also the same.",
            "For older models.",
            "Except so the only problem we have, as I said, just the model we've just seen here we've got something NP complete.",
            "And the sample complexity here.",
            "So it's not very difficult to see that the model where we have.",
            "Only a sequence of variables we have much.",
            "Less expressive model, and so the sample complexity is much, much lower.",
            "OK, well this is it."
        ],
        [
            "So somehow we need to make some some comments about these are so destructures this conditional.",
            "Lexicographic orderings, actually, they were introduced not with the same name.",
            "By Nick Wilson.",
            "We worked a lot on CP Nets and on algorithms on CP net.",
            "And introduced these distractions to get an approximation of CP Nets.",
            "Because the CP Nets.",
            "So inference in CP Nets is very hard and it proved that you can approximate the CP net with a family of conditional.",
            "Preference ordering.",
            "A lexicographic ordering.",
            "Which also is interesting because it means that we may in the future be able to learn the CP net by learning, or at least approximate.",
            "Approximately learning a CPA will be able to approximately learn the CP net by learning a family of lexicographic orderings.",
            "So what we need to do?",
            "We have some ideas.",
            "We didn't start to implement anything.",
            "So of course, at some point we've got to do that and to explore Sonya Risztics to choose the valuable like it's done in decision trees.",
            "As I said, we have a problem if we don't completely build the tree, then the the ordering is not is only partial, which is a problem.",
            "You cannot compare any any pair of objects.",
            "And the solution to that would be too, I think, because if you build a complete trade and the structure is huge, its size is exponential.",
            "So you don't want to build a complete tree.",
            "But a solution could be to start to build a tree and then to have unconditional structure below it below a certain level to get something which is a reasonable size but.",
            "Which which gives a total ordering.",
            "And the thing that we don't really know which she has to do with the testing, the algorithm is how it's going to deal with the noisy data at the moment.",
            "OK, well thank you for your attention."
        ],
        [
            "So.",
            "The last point here you may actually by yourself.",
            "This one would have been my main question because from a learning point of view it is clearly extremely important to be able to deal with noisy data and mistakes.",
            "But nevertheless, my question on that point is that I guess that all your complexity results will also become much worse, right?",
            "If you say, try to find the lexicographic ordering with that minimizes the number of mistakes.",
            "What about the complexity?",
            "What is the optimization problem that?",
            "We know it's NP complete.",
            "But you may not want if you've got noisy data you may not want to be able to say that the tree that you've learned is the one that has the best results, and then just a matter of finding at each level.",
            "A variable that gives somehow the best results, and then you hope that the resulting trail together is not too bad.",
            "But of course you will not be able to prove that it's the best one.",
            "Yeah, you Ristic, yeah yeah.",
            "Another question I would have is that in the scenario you consider in the noise free scenario, basically where you are looking for a model that exactly reproduces the data you have seen.",
            "Um, you may often have the problem that the the.",
            "The model will not be unique.",
            "So there might be a lot of different lexicographic ordering.",
            "In that case.",
            "You simply choose at random.",
            "What?",
            "Yes, the algorithm picks one just by yeah, yeah.",
            "And but again, this is a, you know.",
            "I think a lot depends on what we're going to use as a function that selects the variable on the ordering at each stage because.",
            "There may be some, but we have not explored that as well, and that's what I want to do is.",
            "Is there some some way of selecting the variable that is going to ensure at least some?",
            "I don't think it will be possible to guarantee that we get the best one, but maybe we could have some.",
            "Some guarantee that it's not too bad though, anyway.",
            "Interesting because it nicely complements the kind of work that has been done in preference learning so far, in particular because you come up with an explicit model is extremely interesting, so if you have a domain and you can learn such a model that you can easily interpret and see for example, OK, the user typically puts more put, more emphasis on this variable then on another one.",
            "This would be extremely valuable information.",
            "So yes, definitely one advantage of this kind.",
            "Yeah.",
            "I think it's also useful to be able to tell the user well, because the user may be maybe a bit disappointed.",
            "Disappointed at some point.",
            "For now, when you navigate, send some objects disappear on the my hand.",
            "You may be able to say well in the in the past you for this valuable you have preferred this over this, so This is why.",
            "We've got this mother now, somehow.",
            "I had a similar cycle, maybe I can rephrase it.",
            "What kind of real applications would you have in mind?",
            "Because if you look at consumer references or people comparing objects.",
            "The question is whether with the data, when they violate factor just selects a graphical, they are noisy or if the data are really like that.",
            "This is a different story.",
            "Psychological experiments have shown that people it is not due to noise, just to have cyclical preference.",
            "Yeah, yeah.",
            "Circumstances, do you expect there is no noise?",
            "Actually fit such.",
            "No, no I don't.",
            "I this is this is a.",
            "An intriguing question for me is, is it going to?",
            "Are we going to be able to match some user with with this type of model or is it going to be completely wrong because they are cyclic?",
            "References this is.",
            "Number.",
            "It's not just a problem of one point, it is a problem that is connected with any other form quite difficult for that noise.",
            "No case is already quite an important dampening of the model assumption.",
            "Maybe you could compromise indeed, between city Nets.",
            "On the one hand, which are extremely expressive.",
            "Graphic orders yeah, yeah, I think so.",
            "But they would be interested because the on on your talk with non transitive relations.",
            "You had this Caesar rock example but it's not really an example with preferences but I would like.",
            "I would be interested if you add examples where you really have a consumer preferences.",
            "For example on where they are.",
            "Cyclic, that's it.",
            "Yeah, because there there is some debate travel East or not, so I wanted to have some applications where you really know that it's really.",
            "There are some papers for them.",
            "Psychology did say that in the context of human preferences you also have these kind of interesting.",
            "Yeah, I'd be interesting is that.",
            "Well, yeah, but it's not really.",
            "I mean, it's not really recommended system.",
            "I mean, it would be interesting to see.",
            "I'd like to see examples where on the record.",
            "Yes, but yeah.",
            "Yeah, yeah.",
            "I like that.",
            "Thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Richard Bution shattuck.",
                    "label": 0
                },
                {
                    "sent": "Wilson better.",
                    "label": 0
                },
                {
                    "sent": "A work in Mastercam in Thailand, and Jean long on general area work in Paris.",
                    "label": 0
                },
                {
                    "sent": "And I come from Toulouse, and this was made possible by a nice grant from the French ministries for Foreign affairs, education and time in history.",
                    "label": 0
                },
                {
                    "sent": "For travels between France and Thailand, which was nice.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think just two.",
                    "label": 0
                },
                {
                    "sent": "To situate with what we're doing in comparison with what we've seen today.",
                    "label": 0
                },
                {
                    "sent": "So we try to learn to other objects.",
                    "label": 0
                },
                {
                    "sent": "I have a computer and I tell your domain.",
                    "label": 0
                },
                {
                    "sent": "So I will often refer in examples to computers which I describe with the weather here 3.",
                    "label": 0
                },
                {
                    "sent": "Attributes so the type without it can be the desktop or laptop color or black.",
                    "label": 1
                },
                {
                    "sent": "They can have a DVD reader or writer.",
                    "label": 1
                },
                {
                    "sent": "By the way, I usually use the term variable instead of being of more logical background than machine learning, and we often use variable you can understand it as feature or attribute.",
                    "label": 0
                },
                {
                    "sent": "OK, and so the idea is to, for example, in your recommendation system to learn how a user orders these objects in order to suggest.",
                    "label": 1
                },
                {
                    "sent": "Better objects, the best of ones.",
                    "label": 1
                },
                {
                    "sent": "To future for future interaction or group of users.",
                    "label": 0
                },
                {
                    "sent": "So I think what what is different from a I have seen today is that we want to to learn an explicit representation of the ordering, so not a function which just ranks the objects for example, but we want to have something more explicit that can be used for example to explain why some comparisons are made this way by the by the system.",
                    "label": 0
                },
                {
                    "sent": "And of course we need a compact representation of the orderings, even if we don't have just a function or a system that does that we want.",
                    "label": 0
                },
                {
                    "sent": "And one way to do that is to.",
                    "label": 0
                },
                {
                    "sent": "To have local preferences.",
                    "label": 0
                },
                {
                    "sent": "So on each attribute or variable and then to have some extra structure which is going to say how we aggregate or this or these preferences on each attribute to get a global relation or global ordering.",
                    "label": 0
                },
                {
                    "sent": "I don't know, maybe thanks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically we are.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about two types of orderings.",
                    "label": 0
                },
                {
                    "sent": "Lexicographic orderings at length, and I will after that refer to another type of ordering briefly.",
                    "label": 1
                },
                {
                    "sent": "So lexicographic ordering.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's how much you know about this, so I start by explaining some with details.",
                    "label": 0
                },
                {
                    "sent": "So what it is so as I said, we have a local preference relation over each attribute, so here.",
                    "label": 0
                },
                {
                    "sent": "For the type of computer, for example, here we have a relation that says that laptop preferred to desktops.",
                    "label": 0
                },
                {
                    "sent": "And for the color here we have a relation that says that yellow is preferred to black OK, and on top of that we have this this arrow here which represents an important soldering between the two attributes and basically says that here the type of computer is more important than its color.",
                    "label": 1
                },
                {
                    "sent": "When we want to order the objects.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we use that?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I have objects where I have only two attributes.",
                    "label": 0
                },
                {
                    "sent": "UI to make things a bit simpler.",
                    "label": 0
                },
                {
                    "sent": "So here that says that black laptop is preferred to a yellow desktop and so how do we know that it's because we go down the tree on?",
                    "label": 0
                },
                {
                    "sent": "Look at the first well here sequence.",
                    "label": 0
                },
                {
                    "sent": "Look at the first node where the two objects are different and here they are.",
                    "label": 0
                },
                {
                    "sent": "They have different types, so here it is a relation laptop preferred to desktop which decides which of the two objects is the preferred one.",
                    "label": 0
                },
                {
                    "sent": "For the second pair, here it's both laptops, so at the first node it's not decided, so we go down a level and here one is yellow, the other one is black, so it's the ordering here yellow greater than preferred to black, which decides.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is good with this type of model is that comparison are very easy to make.",
                    "label": 0
                },
                {
                    "sent": "Now we just go down the sequence.",
                    "label": 0
                },
                {
                    "sent": "You can learn that in polynomial time, so there have been two recent papers that prove this.",
                    "label": 0
                },
                {
                    "sent": "I'm very well in somewhere different settings.",
                    "label": 0
                },
                {
                    "sent": "It'll be different.",
                    "label": 0
                },
                {
                    "sent": "The problem with that is it has a very weak expressive power, so you cannot represent.",
                    "label": 1
                },
                {
                    "sent": "Essentially.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you cannot represent conditional preferences.",
                    "label": 0
                },
                {
                    "sent": "And this is an example of something that you cannot represent you.",
                    "label": 0
                },
                {
                    "sent": "You may want to say that for.",
                    "label": 1
                },
                {
                    "sent": "For laptops, you prefer yellow one.",
                    "label": 0
                },
                {
                    "sent": "And for a desktop you prefer a black one, for example, and that you cannot represent with a sequence like this one, just a simple lexicographic ordering.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Recently there have been there have been a number of works on another type of preference representation.",
                    "label": 0
                },
                {
                    "sent": "So conditional preference networks or CP Nets?",
                    "label": 1
                },
                {
                    "sent": "So which is made explicitly to be able to represent conditional preferences?",
                    "label": 0
                },
                {
                    "sent": "And so conditional local preferences like the one which is here, which says if I if I consider laptops then I prefer yellow one to block one OK. And here I have another rule that would say that for desktops I prefer black to yellow.",
                    "label": 1
                },
                {
                    "sent": "And here I can also have rules simple rules without any conditionality that say that in general pressure laptop to desktops and so the mechanism for exploiting these these rules is different.",
                    "label": 0
                },
                {
                    "sent": "You don't have this notion of sequence.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's so comparisons or other things being equal.",
                    "label": 0
                },
                {
                    "sent": "So here for example.",
                    "label": 1
                },
                {
                    "sent": "We have these two objects which only differ in one attribute.",
                    "label": 0
                },
                {
                    "sent": "OK, they're both laptops, and the yellow is preferred to black, so we can say that.",
                    "label": 0
                },
                {
                    "sent": "LY is greater than LV, and so on.",
                    "label": 0
                },
                {
                    "sent": "You can only compare when the objects differ by one variable by one attribute, and since you assume that the ordering is transitive and you can have.",
                    "label": 0
                },
                {
                    "sent": "A chain of comparisons which give.",
                    "label": 0
                },
                {
                    "sent": "More interesting ordering, which is not total.",
                    "label": 0
                },
                {
                    "sent": "In general, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expressive, the big problem is that comparisons are very difficult.",
                    "label": 0
                },
                {
                    "sent": "It's NP complete engine in the general case.",
                    "label": 0
                },
                {
                    "sent": "And they are hard to learn them, so there has been a there's quite a lot of work going on at the moment on this.",
                    "label": 0
                },
                {
                    "sent": "There has been a session on CP net landing at each eye and basically what people do.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They try to learn easy classes of CP Nets usually wear.",
                    "label": 0
                },
                {
                    "sent": "Where the comparisons are easy to make or easy classes of examples or even incomplete algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're trying to do is find something between the two models where we could have better expressivity than with the lexicographic orderings, with the usual ones, and something easier to learn than CP Nets.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so the contribution of this paper is that actually it's possible to add conditionality in lexicographic preference models without increasing the complexity of reasoning or learning these models.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with some results about the so the basic care lexicographic.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that I've just introduced him.",
                    "label": 0
                },
                {
                    "sent": "So the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "So the VC dimension is N. Which is not surprising since it is a very weak model.",
                    "label": 0
                },
                {
                    "sent": "So when I give the results, it's always about models where we have only binary variables.",
                    "label": 0
                },
                {
                    "sent": "So assume also the examples sometimes.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All the examples also have binary variable.",
                    "label": 0
                },
                {
                    "sent": "So in the context of active learning, so here we have a learner that can ask the user queries with what he prefers between pairs of objects.",
                    "label": 1
                },
                {
                    "sent": "Here for example LYVD and so the goal of the learner is to ask the right questions to learn the model of the user as quickly as possible.",
                    "label": 0
                },
                {
                    "sent": "And so in this paper by yeah, so should have gone beyond others.",
                    "label": 1
                },
                {
                    "sent": "It was proved that.",
                    "label": 0
                },
                {
                    "sent": "You need log the log of factorial N queries in the worst case.",
                    "label": 0
                },
                {
                    "sent": "So the best strategy you have in the worst case log of factor in queries and it was in a setting where the local preferences, so the preferences on each attribute were fixed at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So for example, when you.",
                    "label": 0
                },
                {
                    "sent": "So I assume that if you want to buy a computer, should have to choose between one with a DVD writer and a DVD.",
                    "label": 1
                },
                {
                    "sent": "Unique can write and read another one that can only read you will always prefer the one which can write so often you have the setting where the local preferences are known, but it's not always the case and what we proved we describe it in the paper is that in fact this result is easy to generalize to the case where.",
                    "label": 0
                },
                {
                    "sent": "The local preferences have to be learned as well.",
                    "label": 0
                },
                {
                    "sent": "You don't.",
                    "label": 0
                },
                {
                    "sent": "You don't only want to learn the sequence of variables well, so the local preferences so under all you have is this end.",
                    "label": 0
                },
                {
                    "sent": "Here that comes from.",
                    "label": 0
                },
                {
                    "sent": "You have to identify for each variable which of the two possible values is greater one.",
                    "label": 0
                },
                {
                    "sent": "So if you have any variable so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for passive learning.",
                    "label": 0
                },
                {
                    "sent": "So here we have as examples a set of comparisons there always comparison between objects, and we want to output a structure which which gives the same comparisons for the pairs that we had in the examples.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so instead in the same paper, they give a greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "That that works well, that's returned failure when it's not possible to output a structure which has the desired property.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this should be proved with this algorithm that.",
                    "label": 0
                },
                {
                    "sent": "The passive learning.",
                    "label": 0
                },
                {
                    "sent": "Is empty.",
                    "label": 0
                },
                {
                    "sent": "So they prove it with fixed local preferences, and while it's easy to extend the algorithm to prove to use the same kind of algorithm, and so it proves that if you don't know the local preferences in advance, the problem is still in P. Two to find, so I'm going to describe the algorithm soon.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say.",
                    "label": 0
                },
                {
                    "sent": "I first want to give a last result about these simple models.",
                    "label": 0
                },
                {
                    "sent": "It's model optimization here you want basically want to see if there exists a model.",
                    "label": 1
                },
                {
                    "sent": "Given so still, given the set of examples here, can you find a model that has less than K errors for fixed K?",
                    "label": 1
                },
                {
                    "sent": "And this problem is NP complete, so it was proved in the other paper on preference learning.",
                    "label": 1
                },
                {
                    "sent": "And so still with fixed local preferences, and we prove that it's still the case, is unknown local preferences.",
                    "label": 0
                },
                {
                    "sent": "So we still at the same level.",
                    "label": 0
                },
                {
                    "sent": "So now I'll give some detail on the on the greedy.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some because.",
                    "label": 0
                },
                {
                    "sent": "I think it will clarify why where it's the results can be generalized to other models.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm is simple.",
                    "label": 0
                },
                {
                    "sent": "You start, you have your set of examples.",
                    "label": 0
                },
                {
                    "sent": "You don't have at the beginning.",
                    "label": 0
                },
                {
                    "sent": "You have an empty sequence.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Available so you must.",
                    "label": 0
                },
                {
                    "sent": "It's so the algorithm works like a decision tree learning algorithm, so the same principle you must choose the first variable.",
                    "label": 0
                },
                {
                    "sent": "So what can you choose as a valuable?",
                    "label": 0
                },
                {
                    "sent": "You must choose a variable that she's not going to give any wrong comparison, because otherwise.",
                    "label": 0
                },
                {
                    "sent": "You cannot get back the right comparison afterwards, so here I think here you have read a greater than sorry read a written right here and the opposite here.",
                    "label": 0
                },
                {
                    "sent": "So the type of DVD unit cannot be used.",
                    "label": 0
                },
                {
                    "sent": "You have yellow written and black on black greater than yellow, so the color cannot be used as either.",
                    "label": 0
                },
                {
                    "sent": "If you choose any these two.",
                    "label": 0
                },
                {
                    "sent": "Violate what gives gives the wrong.",
                    "label": 0
                },
                {
                    "sent": "Result for this one example so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There remains only the type.",
                    "label": 0
                },
                {
                    "sent": "And of course, for these examples doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "But for this one you have to choose a greater than D, so you have the first variable.",
                    "label": 0
                },
                {
                    "sent": "Then you you did.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first example and then you have to choose the second variable.",
                    "label": 0
                },
                {
                    "sent": "And I think here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only the color works.",
                    "label": 0
                },
                {
                    "sent": "Why only the color?",
                    "label": 0
                },
                {
                    "sent": "Because here you still have red sorry reader, greater than right here on the right are greater than reader, so it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "But you have in both examples you have yellow on the left and black on the right, so it works.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the greedy algorithm, so you never have never have to backtrack.",
                    "label": 0
                },
                {
                    "sent": "And it gives us success.",
                    "label": 0
                },
                {
                    "sent": "So now what did we do, we do.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Could use some conditionality in these these examples in these models.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we did was to introduce conditional local preferences.",
                    "label": 0
                },
                {
                    "sent": "So here I still have.",
                    "label": 0
                },
                {
                    "sent": "That's a laptop.",
                    "label": 0
                },
                {
                    "sent": "I always prefer to desktops, but for the color I had, I have these two rules now that say that in the case of laptops, I prefer yellow to black and in the case of desktops I prefer black to yellow.",
                    "label": 1
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The results that we found with this type of model just by adding conditional local preferences.",
                    "label": 0
                },
                {
                    "sent": "So now we have a much better sample complexity.",
                    "label": 0
                },
                {
                    "sent": "It's two to the N -- 1.",
                    "label": 0
                },
                {
                    "sent": "Which of course the mother is much richer than before.",
                    "label": 0
                },
                {
                    "sent": "For active learning, we also have know that the number of queries work because the.",
                    "label": 0
                },
                {
                    "sent": "The the class of models is much bigger.",
                    "label": 0
                },
                {
                    "sent": "For active learning you need more queries to identify the right one, right model.",
                    "label": 0
                },
                {
                    "sent": "But the good thing is that the greedy algorithm still works, so the passive learning is still in P. Under model optimization remains NPR.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait at least NPR.",
                    "label": 0
                },
                {
                    "sent": "So the greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "I don't know if I need to do this, so it's exactly the same algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I start this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, I I have almost the same example, except I change here black rather than yellow whether it was the opposite.",
                    "label": 0
                },
                {
                    "sent": "Just to illustrate way, now we need this model, so the beginning.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same and then here.",
                    "label": 0
                },
                {
                    "sent": "Here we have yellow, green and black on black rather than yellow.",
                    "label": 0
                },
                {
                    "sent": "So we need 2 rules.",
                    "label": 0
                },
                {
                    "sent": "For the color and we have this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To these tools, actually, here we could have chosen the type of right as well.",
                    "label": 0
                },
                {
                    "sent": "We could have chosen.",
                    "label": 0
                },
                {
                    "sent": "Let us another.",
                    "label": 0
                },
                {
                    "sent": "And it's.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Success.",
                    "label": 0
                },
                {
                    "sent": "So no third kind.",
                    "label": 0
                },
                {
                    "sent": "So we know trying to make sure that the model even richer.",
                    "label": 0
                },
                {
                    "sent": "So now what we do is we want to be able to say that the importance of the attributes those depend on some other attributes attributes.",
                    "label": 0
                },
                {
                    "sent": "So now instead of having a sequence of attributes will have a tree of attributes.",
                    "label": 0
                },
                {
                    "sent": "So for desktops here.",
                    "label": 0
                },
                {
                    "sent": "We can represent the fact that.",
                    "label": 0
                },
                {
                    "sent": "The type of DVD unit is more important than the color.",
                    "label": 1
                },
                {
                    "sent": "And it's it's the opposite on the on the other side.",
                    "label": 0
                },
                {
                    "sent": "So for laptops the color is more important than the type of DVD unit.",
                    "label": 1
                },
                {
                    "sent": "And we combine that with the conditional rules as well.",
                    "label": 0
                },
                {
                    "sent": "And so we here we have a conditional rule that says that for desktops we prefer writer to a reader.",
                    "label": 0
                },
                {
                    "sent": "Actually we have the same.",
                    "label": 0
                },
                {
                    "sent": "Comparisons here.",
                    "label": 0
                },
                {
                    "sent": "And for laptop yellow is better than black anfora.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Add a desktop with the reader, then black is written and why?",
                    "label": 0
                },
                {
                    "sent": "What is important here is that we have to add one rule.",
                    "label": 0
                },
                {
                    "sent": "Well, we have to have local rules at each node.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have to come back to that on the next slide we have to.",
                    "label": 0
                },
                {
                    "sent": "To have local preferences on the variables as well.",
                    "label": 0
                },
                {
                    "sent": "If you were here.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that really need to be complete to say, actually the same for the other models.",
                    "label": 0
                },
                {
                    "sent": "But of course, if the tree is not complete.",
                    "label": 0
                },
                {
                    "sent": "We then have a partial ordering.",
                    "label": 0
                },
                {
                    "sent": "Which may be a problem otherwise with the normal lexicographic ordering, you always get a total ordering of everything.",
                    "label": 0
                },
                {
                    "sent": "So that's different from.",
                    "label": 0
                },
                {
                    "sent": "I think this is the main difference with decision trees.",
                    "label": 0
                },
                {
                    "sent": "For example with decision trees.",
                    "label": 0
                },
                {
                    "sent": "Whatever the size of the tree, you can always classify everything here.",
                    "label": 0
                },
                {
                    "sent": "If the tree is too small, you're going to get to have problems because there are some comparisons that cannot be made.",
                    "label": 0
                },
                {
                    "sent": "So the result?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so we do with this extra possibility.",
                    "label": 0
                },
                {
                    "sent": "We didn't change the sample complexity.",
                    "label": 0
                },
                {
                    "sent": "Active learning gets even bigger.",
                    "label": 0
                },
                {
                    "sent": "Here we had N factorial.",
                    "label": 0
                },
                {
                    "sent": "Now we have something a bit bigger here.",
                    "label": 0
                },
                {
                    "sent": "But the good thing is that the greedy algorithm still works.",
                    "label": 1
                },
                {
                    "sent": "So we are still in P as far as passive learning is common is concerned and the model optimization is still NP complete.",
                    "label": 0
                },
                {
                    "sent": "So no.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another family on here will have something be surprising.",
                    "label": 0
                },
                {
                    "sent": "So at first we had nothing conditional.",
                    "label": 0
                },
                {
                    "sent": "Then I say we can have conditional preferences and still a sequence of viable.",
                    "label": 0
                },
                {
                    "sent": "Then we I said we can have conditional important unconditional preferences.",
                    "label": 0
                },
                {
                    "sent": "So of course the remaining type of structure that we could have is.",
                    "label": 0
                },
                {
                    "sent": "We could have a tree for the important but still have only unconditional local preferences.",
                    "label": 1
                },
                {
                    "sent": "And in this case.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for simplicity, we don't know what it is.",
                    "label": 0
                },
                {
                    "sent": "Active learning, we still have the same type of.",
                    "label": 0
                },
                {
                    "sent": "For a formula and the bad thing with this model is that now passive learning becomes NP complete.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that you cannot construct the tree in a recursive manner because at some point, for example, if I go down the first branch here and then I have to choose here for the examples which on this branch I have to choose some local preference relation for the IT may not be the same.",
                    "label": 0
                },
                {
                    "sent": "The right one for the note that I will construct later here.",
                    "label": 0
                },
                {
                    "sent": "So here we have to backtrack when you can't read the tree.",
                    "label": 0
                },
                {
                    "sent": "So This is why it becomes NP complete, so this one is a.",
                    "label": 0
                },
                {
                    "sent": "Is not a good model.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to.",
                    "label": 0
                },
                {
                    "sent": "Another view of the results that we have.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically, what's interesting here?",
                    "label": 0
                },
                {
                    "sent": "So as we concern, is that so most of them are for most of the of the of the models.",
                    "label": 0
                },
                {
                    "sent": "We get the same algorithm and the same complexity of passive learning.",
                    "label": 0
                },
                {
                    "sent": "For model approximation, it's also the same.",
                    "label": 0
                },
                {
                    "sent": "For older models.",
                    "label": 0
                },
                {
                    "sent": "Except so the only problem we have, as I said, just the model we've just seen here we've got something NP complete.",
                    "label": 0
                },
                {
                    "sent": "And the sample complexity here.",
                    "label": 0
                },
                {
                    "sent": "So it's not very difficult to see that the model where we have.",
                    "label": 0
                },
                {
                    "sent": "Only a sequence of variables we have much.",
                    "label": 0
                },
                {
                    "sent": "Less expressive model, and so the sample complexity is much, much lower.",
                    "label": 0
                },
                {
                    "sent": "OK, well this is it.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So somehow we need to make some some comments about these are so destructures this conditional.",
                    "label": 0
                },
                {
                    "sent": "Lexicographic orderings, actually, they were introduced not with the same name.",
                    "label": 0
                },
                {
                    "sent": "By Nick Wilson.",
                    "label": 0
                },
                {
                    "sent": "We worked a lot on CP Nets and on algorithms on CP net.",
                    "label": 1
                },
                {
                    "sent": "And introduced these distractions to get an approximation of CP Nets.",
                    "label": 0
                },
                {
                    "sent": "Because the CP Nets.",
                    "label": 0
                },
                {
                    "sent": "So inference in CP Nets is very hard and it proved that you can approximate the CP net with a family of conditional.",
                    "label": 0
                },
                {
                    "sent": "Preference ordering.",
                    "label": 0
                },
                {
                    "sent": "A lexicographic ordering.",
                    "label": 0
                },
                {
                    "sent": "Which also is interesting because it means that we may in the future be able to learn the CP net by learning, or at least approximate.",
                    "label": 0
                },
                {
                    "sent": "Approximately learning a CPA will be able to approximately learn the CP net by learning a family of lexicographic orderings.",
                    "label": 0
                },
                {
                    "sent": "So what we need to do?",
                    "label": 1
                },
                {
                    "sent": "We have some ideas.",
                    "label": 0
                },
                {
                    "sent": "We didn't start to implement anything.",
                    "label": 1
                },
                {
                    "sent": "So of course, at some point we've got to do that and to explore Sonya Risztics to choose the valuable like it's done in decision trees.",
                    "label": 0
                },
                {
                    "sent": "As I said, we have a problem if we don't completely build the tree, then the the ordering is not is only partial, which is a problem.",
                    "label": 1
                },
                {
                    "sent": "You cannot compare any any pair of objects.",
                    "label": 0
                },
                {
                    "sent": "And the solution to that would be too, I think, because if you build a complete trade and the structure is huge, its size is exponential.",
                    "label": 0
                },
                {
                    "sent": "So you don't want to build a complete tree.",
                    "label": 0
                },
                {
                    "sent": "But a solution could be to start to build a tree and then to have unconditional structure below it below a certain level to get something which is a reasonable size but.",
                    "label": 0
                },
                {
                    "sent": "Which which gives a total ordering.",
                    "label": 0
                },
                {
                    "sent": "And the thing that we don't really know which she has to do with the testing, the algorithm is how it's going to deal with the noisy data at the moment.",
                    "label": 0
                },
                {
                    "sent": "OK, well thank you for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The last point here you may actually by yourself.",
                    "label": 0
                },
                {
                    "sent": "This one would have been my main question because from a learning point of view it is clearly extremely important to be able to deal with noisy data and mistakes.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless, my question on that point is that I guess that all your complexity results will also become much worse, right?",
                    "label": 0
                },
                {
                    "sent": "If you say, try to find the lexicographic ordering with that minimizes the number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "What about the complexity?",
                    "label": 0
                },
                {
                    "sent": "What is the optimization problem that?",
                    "label": 0
                },
                {
                    "sent": "We know it's NP complete.",
                    "label": 0
                },
                {
                    "sent": "But you may not want if you've got noisy data you may not want to be able to say that the tree that you've learned is the one that has the best results, and then just a matter of finding at each level.",
                    "label": 0
                },
                {
                    "sent": "A variable that gives somehow the best results, and then you hope that the resulting trail together is not too bad.",
                    "label": 0
                },
                {
                    "sent": "But of course you will not be able to prove that it's the best one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you Ristic, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Another question I would have is that in the scenario you consider in the noise free scenario, basically where you are looking for a model that exactly reproduces the data you have seen.",
                    "label": 0
                },
                {
                    "sent": "Um, you may often have the problem that the the.",
                    "label": 0
                },
                {
                    "sent": "The model will not be unique.",
                    "label": 0
                },
                {
                    "sent": "So there might be a lot of different lexicographic ordering.",
                    "label": 0
                },
                {
                    "sent": "In that case.",
                    "label": 0
                },
                {
                    "sent": "You simply choose at random.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Yes, the algorithm picks one just by yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "And but again, this is a, you know.",
                    "label": 0
                },
                {
                    "sent": "I think a lot depends on what we're going to use as a function that selects the variable on the ordering at each stage because.",
                    "label": 0
                },
                {
                    "sent": "There may be some, but we have not explored that as well, and that's what I want to do is.",
                    "label": 0
                },
                {
                    "sent": "Is there some some way of selecting the variable that is going to ensure at least some?",
                    "label": 0
                },
                {
                    "sent": "I don't think it will be possible to guarantee that we get the best one, but maybe we could have some.",
                    "label": 0
                },
                {
                    "sent": "Some guarantee that it's not too bad though, anyway.",
                    "label": 0
                },
                {
                    "sent": "Interesting because it nicely complements the kind of work that has been done in preference learning so far, in particular because you come up with an explicit model is extremely interesting, so if you have a domain and you can learn such a model that you can easily interpret and see for example, OK, the user typically puts more put, more emphasis on this variable then on another one.",
                    "label": 0
                },
                {
                    "sent": "This would be extremely valuable information.",
                    "label": 0
                },
                {
                    "sent": "So yes, definitely one advantage of this kind.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I think it's also useful to be able to tell the user well, because the user may be maybe a bit disappointed.",
                    "label": 0
                },
                {
                    "sent": "Disappointed at some point.",
                    "label": 0
                },
                {
                    "sent": "For now, when you navigate, send some objects disappear on the my hand.",
                    "label": 0
                },
                {
                    "sent": "You may be able to say well in the in the past you for this valuable you have preferred this over this, so This is why.",
                    "label": 0
                },
                {
                    "sent": "We've got this mother now, somehow.",
                    "label": 0
                },
                {
                    "sent": "I had a similar cycle, maybe I can rephrase it.",
                    "label": 0
                },
                {
                    "sent": "What kind of real applications would you have in mind?",
                    "label": 0
                },
                {
                    "sent": "Because if you look at consumer references or people comparing objects.",
                    "label": 0
                },
                {
                    "sent": "The question is whether with the data, when they violate factor just selects a graphical, they are noisy or if the data are really like that.",
                    "label": 0
                },
                {
                    "sent": "This is a different story.",
                    "label": 0
                },
                {
                    "sent": "Psychological experiments have shown that people it is not due to noise, just to have cyclical preference.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Circumstances, do you expect there is no noise?",
                    "label": 0
                },
                {
                    "sent": "Actually fit such.",
                    "label": 0
                },
                {
                    "sent": "No, no I don't.",
                    "label": 0
                },
                {
                    "sent": "I this is this is a.",
                    "label": 0
                },
                {
                    "sent": "An intriguing question for me is, is it going to?",
                    "label": 0
                },
                {
                    "sent": "Are we going to be able to match some user with with this type of model or is it going to be completely wrong because they are cyclic?",
                    "label": 0
                },
                {
                    "sent": "References this is.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "It's not just a problem of one point, it is a problem that is connected with any other form quite difficult for that noise.",
                    "label": 0
                },
                {
                    "sent": "No case is already quite an important dampening of the model assumption.",
                    "label": 0
                },
                {
                    "sent": "Maybe you could compromise indeed, between city Nets.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, which are extremely expressive.",
                    "label": 0
                },
                {
                    "sent": "Graphic orders yeah, yeah, I think so.",
                    "label": 0
                },
                {
                    "sent": "But they would be interested because the on on your talk with non transitive relations.",
                    "label": 0
                },
                {
                    "sent": "You had this Caesar rock example but it's not really an example with preferences but I would like.",
                    "label": 0
                },
                {
                    "sent": "I would be interested if you add examples where you really have a consumer preferences.",
                    "label": 0
                },
                {
                    "sent": "For example on where they are.",
                    "label": 0
                },
                {
                    "sent": "Cyclic, that's it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because there there is some debate travel East or not, so I wanted to have some applications where you really know that it's really.",
                    "label": 0
                },
                {
                    "sent": "There are some papers for them.",
                    "label": 0
                },
                {
                    "sent": "Psychology did say that in the context of human preferences you also have these kind of interesting.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'd be interesting is that.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, but it's not really.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not really recommended system.",
                    "label": 0
                },
                {
                    "sent": "I mean, it would be interesting to see.",
                    "label": 0
                },
                {
                    "sent": "I'd like to see examples where on the record.",
                    "label": 0
                },
                {
                    "sent": "Yes, but yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "I like that.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}