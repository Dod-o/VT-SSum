{
    "id": "guuusfrpr2bkg4uurvtzds6enrqkvrjy",
    "title": "Social recommender systems: a graphical model based approach",
    "info": {
        "author": [
            "Jurgen Van Gael, Computer Laboratory, University of Cambridge"
        ],
        "published": "Aug. 4, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Web Mining",
            "Top->Computer Science->Machine Learning->Recommender Systems"
        ]
    },
    "url": "http://videolectures.net/socialweb2011_gael_social/",
    "segmentation": [
        [
            "Also thanks, thanks a lot for the invitation to come and talk here and present a bit of our work.",
            "So I think what I'll try to do today is give a bit of a flavor of what the team that I'm part of, what we're sort of doing, the prototype or the research project that we've built and then sort of do a deep dive technical deep dive into some of the algorithms and you'll see the algorithms are fairly technical.",
            "I'll just give you sort of an overview and all of these things is sort of published so we can talk about the details, or I can point you to the papers.",
            "So it's all written up, so I'm working for Microsoft Research and Bing, so I'm part of prototyping team so we look at.",
            "You know what kind of things that we could be doing, sort of in the next next generation of Bing.",
            "And the thing that I'm going to be talking about today is sort of our large aspect of it is on news, news, recommendation, so sort of a new wave of news consumption."
        ],
        [
            "So the."
        ],
        [
            "This is you probably have heard this like a gazillion times before, but it keeps blowing me away so this is from our friends at Google.",
            "This is something Eric Schmidt has sort of thrown in a couple of years back, so so apparently from the dawn to civilization up until 2003.",
            "Sort of humanity has had created 5 exabytes of data.",
            "Now that same amount of data is now being generated every two days, and you know the reason that happens is because of some of these services that we sort of all really like to use.",
            "You know, services where you know people actually generate data on a daily basis, and what that sort of leads to is this this decision, quicksand, as some people like to call it?",
            "So there's just so much choice, so much stuff out there that you know it becomes not only hard for humans to navigate, that you know that space, but even for machines it's quite complicated to sort of make sense.",
            "And you know how people achieve what they want to achieve, So what?"
        ],
        [
            "Order to abstract this a little bit.",
            "This is sort of the picture that we take on all the projects that the team that I'm in our sort of involved in all our projects around.",
            "Sort of this abstractions.",
            "So I guess when the web got started, we sort of had all these all these web pages and then the friends Google sort of made this really interesting observation that there's this web of links and that you can use that set of links between pages to do certain middle ranking.",
            "And pay tracks.",
            "A great example of that and sort of spread a lot of research, but there's a whole bunch of other interesting entities where you also can sort of impose this graph structure on.",
            "You know places, physical places, and they have relationships.",
            "You know physical proximity news they have, again, a relationship.",
            "News is sort of, you know, they follow sort of histories and the branch into sort of sub news stories.",
            "Certainly with products that there's.",
            "Relationship with the product between facts.",
            "And then more recently, I guess very interesting Lee social relationships, which are now sort of.",
            "You know, we get to measure those we get to measure who is related to whom in what context, for what purpose.",
            "So there's going from this idea of the graph of web pages.",
            "There's now so many other graphs and the cool thing which at least we think is cool.",
            "And we'd like to use this abstraction to drive our research in our prototyping, is that, you know.",
            "People express well.",
            "First of all, there exists links between these physical things, like, say places and news to the web, right?",
            "So places my restaurant might have a home page of fact might have a Wikipedia page and people might have maybe a Facebook or Google Plus page.",
            "But now, Interestingly enough, is that people are starting to associate themselves with certain of these of these physical entities in the real world.",
            "So people might do checkins, people might do shares or likes of web pages.",
            "News articles they might buy stuff on E Commerce stores.",
            "And you know this is just to give a sense of scale.",
            "I mean, there's a huge number of these associations that people may.",
            "Can we call this sort of the interest graph for us is interesting, as an abstraction is they think about what kind of links do we observe in this interest graph from people to sort of physical objects or digital representations in the web?",
            "But then, from a scientific point of view, the question is, how can we now sort of augment this interest graph?",
            "How can we make sort of present users with new new?",
            "Links how can we recommend the links like for example for places to be, you know can we recommend to people that they should be visiting a restaurant?",
            "Maybe we want to recommend them anew.",
            "A news article and so by taking this abstraction, you know, sort of all these different recommendation tasks and places, restaurants, news apps you want.",
            "All these things sort of can be captured in sort of 1 and the same framework and hopefully we can sort of use information that we have from 1:00 and transfer that to the next.",
            "So that's sort of the grand scheme of things.",
            "That's sort of our grand goal.",
            "And what I'm going to talk to you about today is you know how we sort of, you know, a start for us, something a project that we've been working on the last nine months and this is particularly for news.",
            "And in recent months we've been doing places and deals, but I'm not going to talk about those today."
        ],
        [
            "So OK, so with this grand scheme of things in mind.",
            "We started thinking about what tools can we use to actually be be building these recommender systems and one of the things that certainly our lab in Cambridge is very keen on and there's quite a bit of expertise is to use graphical modeling graphical modeling.",
            "I mean we think is an exceptionally useful tool because it's sort of this all encompassing framework in which you can express things.",
            "You can derive algorithms from them and the key point here is that your graphical models allow us to do modeling.",
            "It allows us.",
            "To specify facts that we think are true in the world so we can focus on the modeling aspect.",
            "And you know the algorithms and everything just fall out.",
            "We don't.",
            "We don't have to sort of specify algorithms and we think that models are much easier to understand than than algorithms, certainly complicated algorithms."
        ],
        [
            "So just to give you sense of what we think we could do with graphical models, I'll give sort of a 5 minute intro and then we'll apply it to this news reading experience.",
            "So OK, so what's graphical models all about?",
            "Graphical models are all about, you know, taking the world around us.",
            "There's things we're not sure about.",
            "For example, do you want?",
            "Are you interested in this news article?",
            "Yes or no?",
            "OK, we're not sure whether you are, but we might have seen lots and lots of data about you.",
            "And so we want to express that you know that that decision as a random variable and we sort of use probabilities to capture the uncertainty of that variable.",
            "So the first key thing of graphical models is everything in the world.",
            "We sort of describe as a random variable is something that we're uncertain about, and we're going to try to model what a system computer system thinks it believes that you're interested in.",
            "Second thing second, big ingredient of graphical models is that, well, there graphical, so we could sort of take.",
            "You know?",
            "Particular domain and we could sort of encode particular prior knowledge about that domain in an.",
            "We sort of generally would represent that with a graph.",
            "So in this case this graph would represent that you know there's three things that you know.",
            "Maybe sometimes we observe.",
            "Sometimes we don't observe them.",
            "And we sort of say that this particular variable M influences G. But otherwise, and this variable M influences A.",
            "But for example G and A or or sort of, they don't influence each other.",
            "And all of that graphical structure can sort of be captured in equations in probability distributions, and there's sort of very specific rules about how sort of a distribution over all the things in the world could sort of be teased apart in little sort of local interactions.",
            "So in this case, this graph would actually represent that you know there's a distribution over this variable M. This P of em.",
            "There's a distribution over this valuable G which is only influenced by an, which is, you know, the probability of G given M. And similarly for a so you know, by using graphical models it's a very powerful tool to specify relationships that we believe exists in the world, and to encode sort of our domain or expert knowledge into into mathematical structure.",
            "And then the really useful thing, and I'll point that out in our prototype as well.",
            "Is that these graphical models allow us to do efficient inference, so it turns out that these graphical models code the mathematical structure of the models that were going to be describing, and Interestingly enough, we can use that to do when the algorithms come out of the models they turn out to be algorithms that are message passing algorithms, so these message passes.",
            "So whenever we create a model and will give you examples in a second, then whenever we create a model sort of, an algorithm falls out in the algorithm itself is just an algorithm, that is, you know that.",
            "Passing of messages that contain probabilities between nodes in a graph.",
            "So in this case you'd have sort of three nodes in some computational system and they would be generating messages and you know they do that for a couple of iterations.",
            "At some point they all say Yep, we're happy with the messages.",
            "We've got no more changes, and then we have a solution for the graphical model, and that's very very useful because by doing message passing we know this is very scalable, so we can take a massive graph with billions and billions of nodes.",
            "Spread them over a cluster and then have all these nodes communicate with each other on the cluster at some point of things going to converge and then we have our solution.",
            "So why do we think that you know if we have these three things, why do we think that's that's a useful thing?",
            "Oh, sorry.",
            "And so the question is, we like to answer about graphical models or things like, OK, we have specified all these local relationships that we believe are true in the world.",
            "And now we want to sort of ask questions about this distribution.",
            "For example, you know, given that we've made all these connections with.",
            "About you and the data we've observed in the data, we'd like to ask about yourself.",
            "We can ask.",
            "OK, now are you going to like this news article, or are you going to like to eat in this restaurant and you know, that would be asking about the marginal of this.",
            "Distribute marginals of these distributions.",
            "So what what?"
        ],
        [
            "We do.",
            "Why do we think that graphical models is so powerful?",
            "So I'm talking about modeling all the time.",
            "And let's say let's take a very, very, very simple problem, which is, how do we model sort of text?",
            "General texts?",
            "I'm not talking about personal personalization, I'm not talking about recommender systems just yet.",
            "If we're just going to model text, well, one of the simplest thing that we can do, and that's useful in certain situations is to model text, or probabilistically model text as a unigram.",
            "So does that mean we take a text corpus?",
            "We have a bunch of documents and we say that each word, and so we assume that there's a probability distribution over words.",
            "What's the probability that you know when we see a word for each word with that probability is we take our corpus?",
            "Every document we go through words and then for each word that we observe, we can say OK, what's the probability of this word and this word then this were then this word.",
            "So the only parameter in this model is this probability distribution over vocabulary.",
            "We can compute the probability distribution over a corpus, or the probability for a particular document.",
            "Now you might say OK, well, wait a second.",
            "That's probably a really lousy probabilistic model for a text, because not you know, not every document is generated by one in the same vocabulary.",
            "When I'm giving a talk here, then I use a very different vocabulary.",
            "Then next week when I'm giving a talk at a wedding, wedding speech is going to be a very different vocabulary.",
            "OK, so you could say, well, maybe from.",
            "Document the document or occasion to occasion.",
            "I have a different vocabulary, so we'll assume that there are, you know, hundred different possible vocabularies.",
            "That person might be using an for each document.",
            "We first pick which of the vocabularies we generate all the words from, and then we're going to generate the words in that document from that particular vocabulary.",
            "That particular model is known as a mixture of unigrams and it's sort of an extension of this unigram model.",
            "It sort of says that there's this thing which we haven't observed in each document, which is what was the vocabulary that that was used to generate that word.",
            "So talking in terms of graphical models, sort of, we have M documents, an end words in each document, and in this unigram there's just we just observe the words in that document when we talk about this mixture, we have M documents and words in each document and what we don't observe.",
            "So This is why it's not shaded.",
            "Is which vocabulary that this word come from?",
            "In that particular document, so it's an extension of that unigram model and it's sort of represents a sort of a more sophisticated view of the world, and both are very valid models, and they're useful in different ways.",
            "You pay computational price to do this mixture ING.",
            "Make sure model, but it's more accurate model of the world, and you know people weren't happy with that as a model for text.",
            "And then people came up with or David Black came up with this thing called latent, richly allocation, which is."
        ],
        [
            "Other particular model to model text which will give you flavor of it.",
            "So this model says OK. Well, actually when I talk about.",
            "So here in this in this, in this academic conference at this workshop, and maybe sort of the next workshop about a different piece of my work, I'm going to be using maybe different different words, because in this talk I'm going to be talking about lot about recommender systems and things like that in another talk might be talking about nonparametric Bayes, which is the topic of my thesis.",
            "So it's two different vocabularies.",
            "But in those both those talks, you know there's certainly a more overlap.",
            "There's a certain amount of vocabulary that I share between the two talks, mainly things like probability and statistics, and those kinds of things.",
            "So David Bly came up with a model to explicitly represent that, and so his model says OK.",
            "So now I have again M documents and end words, but instead of for each document, choosing your vocabulary to generate a word from what he says is OK. Every document can have, let's say, two or three vocabularies from which all the words are generated, and then for each word I'm going to select one of those two or three vocabulary.",
            "So in total there might be 100 vocabularies.",
            "For a particular document, you know I I choose a subset of, say, three or four or something like that, and then for every word in that vocabulary I'm going to choose.",
            "You know each vocabulary individually to generate a word from when you run that, that particular graphical model over text, you get some nice things.",
            "For example, if you run this over news text and you look at what are the vocabularies, then you know over.",
            "I think this was a news corpus and then you find that in news corpus one of the vocabularies is very distinctive about arts, another vocabulary is.",
            "These are the top words in each of the vocabularies.",
            "So in RTT new film show music in, Let's Say Education which is another vocabulary.",
            "The system is just automatically learned.",
            "You might see words like school students, schools, education teachers.",
            "So this type of what this model is been doing?",
            "Is it sort of extracted these topics or vocabularies from this corpus without any human intervention.",
            "The only thing that human had to do was say how it believed that you know the words in those corpora were generated.",
            "My point of these two last slides is that you know you can start with a very simple model of the world.",
            "That might be wrong and might not capture sort of the effects you're interested in, but by using the language of graphical models you can sort of always add bits and pieces to your probabilistic or statistical models, sort of grow them organically and always capture more and more effects of sort of the real world, and then in the end hopefully make really good predictions an this sort of way of thinking about the world way of thinking about learning.",
            "And data mining is sort of what our team is really into.",
            "We think this is a really good idea and we think this this is also very computationally very scalable, so OK, that was my."
        ],
        [
            "My my oh by the way, this particular line of thinking about modeling text for documents, you know I sort of showed you sort of four sort of steps of increasing complexity, but people have been, you know, if you look at this web page by David Mem, know if you look at this webpage, people have developed hundreds of extensions of this particular model.",
            "It's just modeling text based on graphical models.",
            "So perhaps adding the authors of the documents into the mix or adding grammar into the mix so.",
            "This way of thinking is, I mean it's not unique to our team.",
            "I mean, there's a lot of people in the world that are sort of into this, and I just wanted to show you that it's very flexible and it's a very nice way of communicating models between people.",
            "It's a very nice way of reusing statistical model speaking.",
            "You could just say, oh, here's the model we used and share it with people.",
            "So that's what."
        ],
        [
            "We've been doing so before I get into the details of the algorithms that we use for our news recognition system.",
            "I'll give you a demo and this is a little bit tricky.",
            "This is a this is an academic conference, quite a few people are using laptops."
        ],
        [
            "Wi-Fi, so it might be incredibly slow, depending on how useful you guys think this is, so this is the thing that we've built, so you could try it out for yourself, so it's a.",
            "It's an experimental research prototype.",
            "It's up at projectemporia.com.",
            "It's something that we launched in January an essentially what we do is we roughly do the following, so we Microsoft has a deal with Twitter where we get all the.",
            "Public tweets that get posted.",
            "So we get all these public tweets and we look at all the links that get shared and that is, you know.",
            "Depending on the time of the day and whether the US is awake are not, you know that might be between 20 or 50 links a second that we get from Twitter.",
            "Those are all the links that get shared and we sort of take all those links and we look at the web pages that they point to and we build up representation of the actual thing that is pointed at.",
            "So we would then sort of look for English news articles.",
            "In this case an.",
            "The first thing we do is we categorize it so we categorize all the news into sort of different different cat into these eight different categories and then we we string them to this web page.",
            "So the idea of the web pages that we want to make a personalized news reader.",
            "So in order for the system to know who you are, you actually have to make yourself known.",
            "So in this case.",
            "I've sort of linked up my Facebook and my Twitter account with Project Emporia so it now knows that it needs to make a recommendation for Mianite could look at sort of the previous data that is gathered about me and I'll show that to you in a second.",
            "So if I go into particular technology into particular categories, in this case technology what we show is sort of what project Emporia thinks is the top six news articles for you in the last 24 hours.",
            "So we show those six.",
            "Now the question is, how the hell does this thing, no?",
            "What you know, the top six articles for you should be.",
            "So the way we do that is that we've made a very explicit sort of design choice early on that we believe that it's very important that people know how and what they're influencing this recommender system.",
            "And we decided that we didn't want to sort of use sort of usage signals, but rather have an explicit feedback mechanism.",
            "So in this case I can sort of look at this this particular article, and I could say, Yep, this is sort of the type of news that I'm interested in, and now obviously you know this has been trained for six months, so it's doing a pretty good job.",
            "But I can say, Yep, this is an article that I want to see more up so I can give an explicit vote on this article.",
            "Or, you know, Windows Phone.",
            "Yes, that's something I'm interested in.",
            "Yeah, this is all stuff I'm interested in.",
            "I could also for example, you know, you know, say an article.",
            "I could say I don't.",
            "I want to see less of this, so I give I give.",
            "I'm not going to mess up my profile right now, but so I give explicit feedback to the system.",
            "Now what's interesting is that I will come back to that point in a second is that you know that when I give that feedback, that sort of goes instantly to our recommender system, which runs on the cloud on Windows Azure and instantly it will sort of update its recommendation and sort of when I hit refresh, it'll sort of.",
            "Take that that feedback that I just gave it will sort of have integrated that into the recommendation system, which I'll talk about in a second and come up with sort of a new six new new recommendations so.",
            "OK, so that's sort of the 1st 1st.",
            "Sort of thing that we launched back in January, but then we thought, look, we think there's quite a like in the previous talk suggested already.",
            "Is quite a bit of.",
            "We can leverage the social network and just like like the previous speaker mentioned, you know we can leverage the social network by leveraging the trust network that people have built.",
            "So the way that works is that right now this was sort of a technology category technology channel.",
            "You know I could go to politics channel or other type of channels.",
            "In this case.",
            "We've also created this thing called Social Channel and the social channel.",
            "Essentially what we do is we have this Twitter friends and Twitter friends of Friends channel.",
            "So essentially what this is doing?",
            "Is what we do in the Twitter friends of Friends Channel.",
            "I'm looking at all the links that my friends or my friends of friends have shared.",
            "In their public timelines, you know that might be anything from a few you know, few tends to a few hundred too few.",
            "1000 links.",
            "I take that set of links, and I sort of re rank that according to my personal taste.",
            "That's what I'm presenting here, sort of.",
            "This re ranked list from my Twitter friends of friends and then I can.",
            "Similarly I could.",
            "I could give feedback.",
            "On on on these articles and influence how the system is going to rank things in the future.",
            "So this is, this is something that we launched maybe three months ago as sort of as an experiment.",
            "And really we think that it's doing or what it really is doing is it's taking the filter that you've already built by following people on Twitter, so you've already done that work.",
            "It's sort of you've already expressed in your social network.",
            "Whom you?",
            "Trusting for forgetting particular news items or not, you've already expressed that, and without having to do any extra work just by linking your Twitter account, we can then give you that, that that personalized experience, and obviously, given that people follow different people on Twitter.",
            "This is a completely personalized personalized experience.",
            "So that's sort of the the demo of Project Emporia and what I think I'll do next is I'll talk you through some of the steps of this of the architecture, and then also give you a quite some details about the recommendation algorithms that we've built for this.",
            "And."
        ],
        [
            "Alright, so.",
            "From a high level point of view, how this this whole thing work?",
            "So people in Twitter write tweets they share links with each other and these links they come into.",
            "Well, they get shared to Twitter and Twitter sends them to us.",
            "Now the first thing we do as I suggest, as I said already, is we index all the links that get shared on Twitter and we categorize them enough for the categorization.",
            "We're doing something which I think is cool.",
            "It doesn't take us a lot of work.",
            "So essentially what we do is we subscribe to RSS feeds from the BBC, The Guardian CNN.",
            "You know, a lot of different news sources and we know we subscribe to the technology feeds from BBC.",
            "The technology feed from CNN and we get all the all the articles from the technology feeds.",
            "Then what we do is we train a classifier, a very simple classifier based on the labeled data that we get from CNN into Guardian etc.",
            "So we train a classifier based on the data that the editors of these new sources have been have been labeling.",
            "We train a classifier to say whether an article is from technology or not, and that happens every five minutes.",
            "So the RSS crawler just runs and every five minutes the classifier gets trained and gets updated and then gets put into into our pipeline an every link that comes in, we get the web page and we classified according to that classifier.",
            "So that's quite useful because you know, we get a very up-to-date classifier on technology and politics and all that kind of stuff, especially for free.",
            "So we sort of use, I guess, crowdsourcing if you want or expert sourcing to do that.",
            "This is really important, because when we launched there was, let's say, for world events there was, we launched in January and for world events there is very little news on earthquakes in those kinds of tragic events.",
            "In our pipeline, and so if you know the first news article that came in on the Japan earthquake almost surely got misclassified.",
            "But because these sort of new sources on the web, these editors are there on the ball on after 5 or 10 minutes, they'll have their first news articles about these events, and so the classifier knows how to pick that up in sort of no time, and so we get a, you know, almost real time classifier of these events that have never been seen before.",
            "OK, so these these links and these tweets come into our categorizer we index them, put them into a big store in the clouds and then we have our recommendation engine and our recommendation engine.",
            "I'll give you the details of how it works in a second, but it's a recognition engine which is called Matchbox and essentially what it does is it assigns to every news article of vector of low dimensional vector, Let's say 5 or 10 dimensional vector.",
            "Based on the content of that article, Anna model that's been trained, so that's the trade vector.",
            "So we store all those trade factors for the last 24 hours and now a user goes to the web page 2 important emporia.com and he says OK, give me the give me the top news dashboard and the system talks to the query engine, the query instances.",
            "OK, well it looks at the at those vectors and essentially what Emporia or what the recommendation engine does.",
            "Is it takes that sort of, let's say 5 dimensional vector for the item and every user also gets represented by a 5 dimensional vector and it's sort of computes all the inner product.",
            "So let's say there's a million articles every day in our news index it takes the users vector and it computes the inner product with all the million articles and then it uses.",
            "It looks at the top ten of those at the top of the top ten articles with the highest inner product and that product related to the probability of the user liking that article.",
            "And we've sent those articles back to the user and then, Interestingly enough, the user gives feed or he might not, but generally certainly in the beginning he will give feedback to the system and the system will update itself.",
            "So I think the interesting thing here that was an important driver for us is that I think a lot of, um, I think you know a lot of these.",
            "Search architectures that have been built in the between 2000 2010 are often very static, so they might have some kind of real time ingestion feed, like new articles coming into search index for example, but there's very little infrastructure for real time changes to an index, and we think similarly like I think.",
            "For example, I'm not Privy to the details of, say, the Amazon recommender system, but for example, our understanding is that the Amazon recommender system, for example, computes recommendations, or at least in the past that used to compute recommendations everyday.",
            "And so it won't.",
            "You know, if you buy a new article, it won't update its recommendations right away, and probably doesn't make sense for the system to do that because you know, you don't buy something in 10 minutes later by something else before news.",
            "This is absolutely critical if you're going to see a news article, and it's about something I don't know that happened in the world that you're absolutely not interested in.",
            "You say at this like this.",
            "I don't want to see any more news of this.",
            "You want instant personalization, you do not want to see that you know.",
            "10 minutes later, pop up again because that's a really poor user experience and so one of the core things that we spend a lot of time trying to do is how can we sort of, you know, Bing scale builds, a personalization engine where you might have thousands of 10,100 thousand millions of users and be able to get real time feedback and be able to train your recommender system, sort of in an online fashion.",
            "So that was the challenge.",
            "So that's sort of, you know what project Emporia from a 30,000 feet perspective looks like an I think next."
        ],
        [
            "I'll sort of do a bit of a deep dive into the into the recommender system.",
            "So first I'm going to talk to you about the recommender system that we actually use an, and then next I'll be talking about sort of some extensions of recommender systems based on sort of graphical models that were sort of playing around with and testing out.",
            "So this is work that's been done.",
            "This recommender systems work that's been done maybe two years ago three years ago, and was sort of initially initiated."
        ],
        [
            "By the Netflix prize.",
            "So what's what's the tricky thing about or what does recommender system?",
            "What's it all about?",
            "So we have a particular user and we have items and items.",
            "Could be anything from product Stew news, to anything that you really want.",
            "And then we have feedback and feedback.",
            "Could be anything again from explicit ratings like on Amazon.",
            "It could be a click, maybe on a search engine."
        ],
        [
            "Or it could actually be no money spent or something like that.",
            "Now again, abstracting away the problem and not thinking about the actual math and the algorithms that want to use what does recommender system do so recommender system we have a bunch of users.",
            "We have a bunch of movies.",
            "In this case we get to see feedback so particular user dislike that movie and we get a whole bunch of that.",
            "Now the important thing here is that it's sparse, so we have compared to the number of entries in this table.",
            "We only have very very few entries that we actually got explicit ratings about.",
            "And the question that we want to ask is given a particular user, we might want to know whether he or she liked that particular movie, and in this case you could say, well, it's kind of obvious because you know user DN.",
            "User B are actually quite similar.",
            "They both like movie one.",
            "They sort of this like movie for so it's probably very likely that you know user D likes movie 6 based on user base behavior.",
            "OK, so that's how a lot of these recommender systems generally work.",
            "But now an interesting question happens when what if no ones ever rated this movie before?",
            "And for movies, this is perhaps not as big of a deal because there's actually not that many movies out in the world.",
            "If you think about it, I think Netflix is about 200,000 movies.",
            "That's about as much news as we get in a day.",
            "So every so for news.",
            "This is absolutely critical.",
            "There's a lots of stuff that no one's ever rated before, so how do you deal with that?",
            "Well, the way Matchbox deals with that is by using features.",
            "So by using features we can actually see that this movie is actually similar to other movies.",
            "And, well, we already established that the user was similar to another user, but in this case those two movies are in some sense similar because they're both action movies.",
            "I don't know if there's any other any other not so you know this is just a toy example, but because they're both action movies, you know the system can learn that maybe user dies actually going to like movie 5, so features are absolutely critical here, so extracting metadata and this."
        ],
        [
            "What Matchbox is doing?",
            "So how does Matchbox roughly work?",
            "I'll paint sketch here 1st and then I'll show you show you what the model actually looks like.",
            "So the way the model works in practice is you know you have a bunch of movies and a bunch of users and a user comes into the blue users, any dislikes Rambo.",
            "So in in this sort of vector space.",
            "So every user as I said before and every movie gets sort of allocated represented by a vector in a particular space.",
            "In this case it's 2 dimensions.",
            "But it could be 1020 fifty 100 dimensional, so the blue user dislikes the Rambo movie, so they get sort of moved apart in this space.",
            "Maybe he likes Harry Potter so they move closer together.",
            "The Gray user likes Rambo, so he moves closer to that movie The.",
            "Also, there was yellow Degray user dislikes nothing Hills, they move further apart but he likes Rambo so you know all those guys come closer together and you could sort of imagine that if you have like a billion news articles or a billion movies ratings on movies that you know all these guys move in this space and what you can show is that at some point the system is going to find sort of the best possible layout where people who like particular movies are close together and and people who dislike movies there further apart.",
            "So that's essentially how Matchbox words works.",
            "It takes users, an movies, embeds them in sort of a low dimensional space.",
            "In this case A5 dimensional, 2 dimensional space, and then looks at who is closer to."
        ],
        [
            "So what does that look like from a graphical model point of view?",
            "So the graphical models?",
            "This might look a bit scary, but it's actually not not too complicated.",
            "So the thing that we observe are the metadata about the user, the ratings that user give to movies, and the metadata about the movies.",
            "So going from left to right when we get the metadata about the user, we take that metadata and we sort of take a matrix or some parameter and we map that metadata which could potentially be.",
            "You know a million dimensional we might have like a million pieces of metadata about a particular user.",
            "You know his age, his address an and let's say you know H could be.",
            "We could have sort of a feature for being one year old and two year old in three year old.",
            "So that's 100 or we might have the users you know, just good or something in the system, which might be, you know, a billion if we have a billion users.",
            "A billion different features.",
            "So this is very high dimensional.",
            "We take a parameter and this could be same matrix and we multiply those two together and map them.",
            "Onto this 5 dimensional or three dimensional space and this is like the key thing.",
            "This is what we need to learn.",
            "This is what we want to learn from data.",
            "So we take the users metadata.",
            "We take this parameter which we're going to be learning and map it onto sort of a low dimensional space.",
            "We similarly take the metadata about a movie or an item.",
            "We take the parameter parameter, which we're going to learn and we sort of.",
            "In this case multiply them, say and map that into sort of a low dimensional space.",
            "So now we have two vectors.",
            "Again, we don't know what the vectors are because we don't know what those parameters are going to learn all that.",
            "We take those two vectors.",
            "The user in the item vectors and then we compute the inner product.",
            "So this number represents the inner product between those two vectors and then we save that inner product is.",
            "Let's say you know very big and very positive.",
            "Then the the movie gets a high rating from that user.",
            "If it's very negative to movie gets a low low rating.",
            "So that's the graphical model for Matchbox and when we this is sort of what are.",
            "Our representation of how recommendations could be generated potentially and now the key thing is that at some point we sort of throw it all into an algorithm, which I'll say something about next, and then we take a user and a movie which haven't.",
            "You know, this user hasn't seen that movie and now we can compute what's the probability that that user is going to like that particular movie so.",
            "Sort of, in math.",
            "You could do sort of differently.",
            "You could say if this X is the representation of the user XI, the reputation of the movie, and you sort of matrix, multiply them into sort of smaller vectors S&T, you compute the inner product and then we say that the rating potential is normally distributed around around that inner product."
        ],
        [
            "So essentially what you get out of that is that, let's say for a particular user there's, let's say, a bunch of features that represent the user ID of a bunch of features that represent his or her gender.",
            "Maybe the country they live in, the height, etc.",
            "And so for each of these features, a user only has one of them on, so the user only has one user ID is only one gender, lives in a particular country, and each of these.",
            "Sort of, each of these features that are on or off.",
            "Contributes to the vector of that user in this low dimensional space, so this particular user represented with these features sort of sits here in this space, and each of these individual features sort of contributed a particular component to that space.",
            "And that's what we're going to have to learn is.",
            "How do we map?",
            "How do you know?",
            "For each of these, sort of there's a vector?",
            "And how do we learn these vectors?",
            "That's what we're going to have to learn from data.",
            "Similarly for the items.",
            "So on item might have an ID.",
            "Which sort of points in that direction.",
            "There might be genre points in a little bit in that direction, and the actual position of a movie sort of the sum of all these vectors.",
            "OK, so now you know the user in the movie, sit in this space here and their inner product is then going to represent you, know how much the user is going to like that movie."
        ],
        [
            "So the nice thing about saying OK, so about graphical models is we we said we state.",
            "We claim that that's how the world works.",
            "OK, we claim that these ratings have been generated according to these mechanisms and we can do that.",
            "Maybe it's wrong, but we can just say that this is how it was.",
            "And then we've sort of defined a probability distribution, the one using the graphical model showed before we've defined a probability distribution where some of the.",
            "You know random variables have been observed, some haven't been observed, and then it's just sort of math.",
            "You could sort of, you know, crank the handle and you know this whole books about it.",
            "Chris Bishop, who's our lab directors, written a really nice sort of intro to machine learning book where there's one particular chapter that just says if you define a graphical model then this is the algorithm that you need to run in order to to now make start making predictions and you know there's no no tuning, no questions about how to do it, it's just it just.",
            "Falls out and that algorithm turns out to be message passing, so this is sort of the message passing representation and essentially you know all the numbers and everything don't matter that much, but you know there's still the observed.",
            "Metadata, there's still the rating, which I guess in this case is not observed.",
            "And then there's all the links to all the variables that were on the previous slide and essentially the algorithm is literally just sending messages between all these nodes, so it scales very nicely, and so there's sort of, you know, we've."
        ],
        [
            "And this we've done this, and this was done on Movielens, which is a movie recommendation datasets.",
            "And when we do this, so generally we wouldn't do this.",
            "This mapping into 2 dimensions because that's a very it's not a very expressive space.",
            "We do this in 10 or 20 dimensions, but if we do this in two dimensions, we can sort of visualize it.",
            "So all these Gray dots are users in this movie lens database and all the red dots are movies and essentially what you could sort of see here is that for example, the 20 Four Seasons are sort of sort of together in that space, which makes sense because if a user sort of sits here.",
            "It means he's probably going to like 24 season three, which is probably quite suggestive of him liking Season 2, so you could sort of see that you'd get this sort of embedding of movies and users in the same space, and then we can start making recommendations.",
            "Now when we look at a particular user, let's say this Gray user here, then we could sort of draw cone of all the movies that have a particular inner product with this user.",
            "And so this is sort of the users preference.",
            "Cone is sort of the movies that would potentially have a high compatibility with him."
        ],
        [
            "So when we did this couple of years ago, we did this on Netflix with 100 million ratings an on one machine.",
            "We sort of train this thing took 2 hours and we can sort of well cinematch is the system that Netflix used themselves and they get a particular score.",
            "Root mean squared error score and then we sort of trained our system with very different degrees of dimensionality so different degrees of expressiveness and you know we sort of get we just in quite a bit.",
            "It's quite a.",
            "Accurate system, so this recommender system is quite accurate.",
            "But the cool thing about is for our point of view is although it might not be like the best system in the system that you know the one the Netflix prize you know we can actually train this in two hours on 100 million ratings and I guess at web scale that's important.",
            "So we can train this.",
            "This type of system we can train this in an online fashion.",
            "So that was first first very important and then we took this this.",
            "Matchbox System an we implement we made that sort of the core recommender system that's it's sort of at the heart of project Emporia, and that's the one that's making the recommendations.",
            "Now when we did, this project was really a research prototype and it was really meant for us to understand how we could make this scale.",
            "What are the important things when we build these recommender systems?",
            "How can we do this in real time?",
            "And there's sort of three things that we found that we we didn't sort of that weren't as big of a deal when we did Netflix and things like that.",
            "And I think I think if there's one thing that I can convey to sort of the academic community from, I guess from industry experience.",
            "Is that you know these three things I personally think are very interesting research topic."
        ],
        [
            "So the first problem is that Matchbox needs positive and negative training data, so you cannot train a system like Matchbox and actually not very many recommender systems using only positive data.",
            "There's a whole lot of philosophical questions about that, but say for example from an algorithmic POV Matchbox.",
            "If you just give it like likes or you know people liking certain things, then essentially what it will do is it will just say everybody sits in the middle of this sort of Euclidean space.",
            "So it essentially says everybody likes everything and it's perfectly happy because the only data it sees is everybody liking everything.",
            "Well, it doesn't see every everybody liking doesn't see an Association of everybody with everything but everything it sees is alike, so it's a perfect, perfectly well, perfectly good explanation of the data, which is obviously really, really bad if you, let's say, want to train a link recommender system from Facebook likes or Google Plus or something.",
            "Then obviously that's going to be a problem.",
            "You know you're only going to see positive feedback.",
            "And for Emporia it was sort of easy because we designed around that.",
            "But not every domain sort of allows for that, so a lot of probabilistic and non probabilistic recommender systems I think or we think have had this problem.",
            "And there's not very many good systems that know how to deal with this, so I think that's a very interesting you know open problem for the research community.",
            "Another one is the problem of fast indexing using sequential updates.",
            "So with Matchbox you have to imagine that there's about a million.",
            "New so we have a bunch of users, you know X number of users and we get about a million new news articles that we need to put in this space.",
            "Some of them expire, but we need to put them in at a quite a fast rate while at the same time you know doing learning, so changing.",
            "You know where all these guys live in in this low dimensional space while making recommendations.",
            "And it's not a very easy, very easy problem and I think again, it's sort of overlooked.",
            "You know, in the Netflix Prize, because I guess in Netflix you know everybody was really into beating their system by 10% and winning $1,000,000.",
            "But I think people sort of overlooked a little bit.",
            "You know how you could make these recommender systems work in real time, which I think again is an open and open space where lots of good things can be can be done, and then third, and this is absolutely critical, this has been our biggest thing is that all of these recommender systems and I think.",
            "Pretty much every you know, every recommender system in the world that's really good at doing.",
            "Recommendations is actually really bad at making at doing explanations and what people very often do, so we don't explain why a particular user Emporia got to see a particular news article.",
            "What people have said, you know, people have built like the.",
            "Some of the other recommender systems, what they often do is they just lie about it.",
            "They just light, so they use a very powerful, maybe a probabilistic model or some matrix factorization.",
            "Something fancy to do the recommendation.",
            "But then when they actually need to explain it, they'll just say you know, it's because you also liked this article or it's because you also like and it's sort of a drastic simplification of reality.",
            "Now that's absolutely fine if people don't mind that you know that's absolutely fine.",
            "There's no problem with that.",
            "But what I think is interesting from a research point of view.",
            "Is how can we design A recommender system where we can both get sort of good predictions and do accurate recommendations but still be able to explain to people you know why it happens and you know my grandmother is not interested in low dimensional embeddings?",
            "Certainly not if they're probabilistic, so you know those those are not explanations.",
            "So I think those are three challenges and definitely challenges for us and things that we're thinking about and love to share this with you guys and see if there's other ideas.",
            "And hopefully people can think with us."
        ],
        [
            "On these things.",
            "So let me tell you a little bit in the last.",
            "In the last couple of minutes I won't take much very much longer about.",
            "And attempt an attempt from Mars to address this problem, and again building on these graphical model based approach.",
            "You know how can we do things differently or extend Matchbox in order to perhaps solve one or two?"
        ],
        [
            "With these problems.",
            "So we developed this model and we found out that actually the core of this model had been already been developed by some people in Portis I think at UC Irvine.",
            "And his collaborators.",
            "So essentially, the idea is the following.",
            "So instead of taking a geometric approach in saying you know a movie gets gets represented as a vector and a user get represented as a vector, maybe we should take more like a discrete approach of clustering based approach.",
            "Ana closer in Bridge approach we could say, OK, we're going to cluster users and cluster movies, and then we're going to learn which user clusters like which movie clusters.",
            "Now that's a bit 1 dimensional.",
            "The world is not as simple.",
            "You know, I'm not just an academic, I'm also dad.",
            "I'm also.",
            "I don't know.",
            "I'm a UK citizen or resident, not a citizen, so the world is not as one dimensional, but Luckily this latent nursery allocation that I sort of mentioned for text suggests how one could get around that.",
            "So let's say that every user gets represented by.",
            "Let's say there's 100 user categories and 100 item categories, and now every user we could say gets represented by a subset of those hundred categories.",
            "Let's say four.",
            "OK everybody gets to choose four.",
            "Profiles that he's part of with different sort of probabilities, so might be like 80%.",
            "No, not not that much 40% and academic 30% of Dad, 10% living in the UK, etc.",
            "So I get to choose four categories and similarly a movie or a news article gets to choose sort of four or five or whatever category out of 100 categories that is representative of its content.",
            "And that's what this graphical model represents.",
            "So every user.",
            "Gets to choose sort of a small set of profiles that he's he's part of, and every movie.",
            "In this case, let's say, gets to choose, you know, a few 4 out of 100 profiles that he's part of an now.",
            "Every time we see a rating of a user for a particular movie, we assume that when the user made that rating, he was sort of in his being an academic mode or being a dad mode.",
            "So he sort of chooses one of these categories specifically.",
            "And you know, for let's say a movies or romantic comedy, and he sort of chose it because you know it was.",
            "It was he was interested in the romantic aspect of it.",
            "So the user and the movie sort of virtually decides which of the profiles that rating was generated for.",
            "And then we look OK.",
            "So let's say it's the academic who chose a comedy movie.",
            "Then we go look into a particular matrix and we say, how much do academics like comedy movies?",
            "Generate that rating.",
            "OK, so we generate that rating.",
            "So now what do we need to learn?",
            "Well, for each user we need to sort of learn by just looking at ratings.",
            "We need to learn how much you know what are the different profiles that people belong to.",
            "They can't specify.",
            "We learn it, or at least that's what this model does and we need to learn for each of the profile user item profile combinations.",
            "You know what's the compatibility and when the guys import this, did this a couple of years back.",
            "They sort of got it.",
            "Reasonably good Netflix.",
            "Accuracy and they did, that would say 50 users group.",
            "So this was for movies 50 user and 500 item groups.",
            "So by just using this data and this particular probabilistic model that's not geometry based but sort of clustering based but not clustering as in hard assignment but sort of soft assignments.",
            "They got a reasonable score so we started thinking OK how can we?",
            "How can we make this bet?"
        ],
        [
            "Well, the first extension that we thought of was OK. Well, we've we've been quite successful with this sort of idea of Matchbox.",
            "That says we're going to take the users metadata and map him onto this sort of $2 five dimensional vector.",
            "So why can we take the users metadata and map him onto a set of profiles?",
            "And essentially, if you sort of look at, you know if you forget about these sort of two things at the end here.",
            "I mean, it's exactly the same model as before, so it's exactly users or sort of part of.",
            "A small subset of profiles, but now what we do is we actually say we look at the users metadata and we sort of map that metadata onto this soft assignment of a clustering OK and I'll show this in friendlier version in a second."
        ],
        [
            "So going back to sort of this idea of, you know using features as to make recommendations."
        ],
        [
            "You know we started thinking, well, you know we've now taken this idea of Matchbox, so we use features to generalize across users.",
            "So if we know that a particular user lives in the UK, then he has the UK profile and a new user comes in.",
            "He's never given rating data and we know he lives in the UK that he's going to get that UK data profile, but that's sort of taking a discriminative approach to learning.",
            "But if you're sort of into machine learning, you might also know that you could sort of use generative approaches so we could.",
            "Actually, instead of using this metadata to generate how people map onto their profiles, we could sort of take the profiles and generate their metadata, and essentially what makes that sort of interesting is that you can do unsupervised learning, so you don't actually have to have ratings to learn this mapping, so this might be a bit technical, but."
        ],
        [
            "Explain, let me explain it in in sort of a caricature.",
            "So essentially what we could be doing is the following.",
            "So let's say we have users and we have movies and now we have metadata.",
            "Then it seems a bit silly that a system like Matchbox actually now needs to see ratings in order for it to know what users are similar and what movies are similar and how to embed them in a way.",
            "If you get the metadata it sounds very valuable that you should be able to design A probabilistic system.",
            "That sort of clusters them.",
            "Or soft clusters them, so that's what sort of this new graphical model that we're proposing is doing, so it takes the user metadata IT clusters users.",
            "So for each user it assigns.",
            "This is now showing a hard assignment, but imagine that it's a soft assignment, so it assigns the user to be maybe an intellectual, maybe a little bit adventures an emotional user, and similarly for movies.",
            "So we first use the metadata to sort of soft cluster users and movies.",
            "And then we take our rating data and now for each of the pairs.",
            "Now we get to see a particular user in a particular movie and for each of the pairs we now sort of learn whether users who were adventurous generally like comedy movies or not.",
            "So this is let's say the probability that the adventures user likes a comedy movie or not.",
            "So what we've done is we've sort of taken the metadata to sort of come up with this clustering.",
            "Kind of like this geometric embedding and then we've taken the rating data and use it, sort of.",
            "For a different purpose to to compute this sort of the matrix, we call it the matrix.",
            "OK, so why is this useful?",
            "Well, it addresses or partly addresses some of the problems that Matchbox had.",
            "So first of all."
        ],
        [
            "This positive training data only well one of the like I said Matchbox.",
            "Initially it assumes that everybody is sort of sitting in the middle and it needs to see what people dislike in order to move people further apart.",
            "But this system can actually very easily express that most people dislike most things because I can cluster users and then without seeing any of the rating data I can just say that the probability that you know each of these categories likes each of these categories is very low.",
            "So this particular probabilistic model.",
            "Initially expresses that most people dislike most things, which I think is a good assumption about the world.",
            "So we can sort of set that type of structure up.",
            "And now if we see a particular profile user with Abreu profile movie and we see that he likes it, we know that we could sort of update one particular entry in this matrix.",
            "OK, so we can learn from positive training data only because we can set up our belief that most people dislike Moe."
        ],
        [
            "Things.",
            "Again, fast indexing is also much easier because instead of having to compute the inner product between all millions of items and users, we can actually do something a lot more clever because it's an index.",
            "It's a discrete structure, we can index it.",
            "So the question we want to ask is, given the blue user, give me something that he likes.",
            "Well, if we know that we're just looking for the blue user, we could sort of look at what are the sort of the entries in this row of sort of category movie categories that he likes.",
            "And then we know well, this one is seems to be the most liked category.",
            "So now we can go into this sort of subset of all the comedy movies and make a recommendation.",
            "So it's by what we essentially done is.",
            "We've sort of, you know, if there's 100 different categories here, we might only have to look at two or three rather than all possible movies in our catalog.",
            "So again, we can do a lot faster recommendations using these discrete structures than these."
        ],
        [
            "Metric structures.",
            "Finally.",
            "Is certainly not the answer to make doing good explanations, but it's sort of our first attempt is that when we make a recommendation for a user for a particular movie, we could sort of, for example, an intellectual user and we made him a comedy recommendation.",
            "We could sort of look and say, OK, well, we made you that recommendation because people who have Masters degree generally like movies with Will Ferrell or something along those lines, and we can do that because each of these sort of sort of soft clusters is actually.",
            "We can sort of describe sort of the center of that cluster."
        ],
        [
            "So.",
            "So that's that's essentially what we've been doing.",
            "That's sort of our way of thinking.",
            "And well, one thing that we're very what we found is that we get similar sort of predictive performance as the one that we got for Matchbox, which has been the one that we really like because it works really well.",
            "So we get very similar predictive performance.",
            "It's not worse, let me put it that way, but we've also found is that it's definitely faster, so we could definitely write algorithms that make predictions a lot faster.",
            "So it definitely has that plus.",
            "This we're experimenting with these two things, and although it has, it has bits and pieces of this so we can learn from positive feedback only, but it's still not there yet.",
            "We we think the explanatory power is also not where we want it to be.",
            "I mean, it's not good enough to sort of surface in a user interface for Emporia, so these are certainly things that we need to sort of change our graphical models further and come up with better things to make recommendations."
        ],
        [
            "So this is sort of what our group's been working on, and so I talked about Matchbox, which is, you know, you know our initial work on geometric representations for recommender systems.",
            "I've talked about what we've been doing most recently, which is sort of mixed membership, so it is soft clustering based ideas of making recommendations, and then we have an intern which is trying yet other representations.",
            "The as I said right now in Emporia we use the social network as a filter.",
            "So we look at all the news articles that have been shared by your friends and then re rank them.",
            "But we could also imagine using the social network to say group people.",
            "So we also have an intern working on that.",
            "Another interesting thing is like I said in earlier when we make a recommendation, we look at the top 10 or the top hundreds news articles and then we have some sort of simple heuristic to choose the six best.",
            "According to some juristic out of that, top 100.",
            "But you know, that's hand to heuristic, and we think we can actually learn that from data as well.",
            "How to do that so you know more of a decision theoretic approach to recommendation is something else that we're that we're sort of working on.",
            "One thing that you'll find when you use project import.",
            "At least we find, is that it's sort of hyper personalized.",
            "A little bit.",
            "You'll see if you go there because the employee has been released on the Windows Phone app and Windows Phone was sort of initially tested and quite popular among Microsoft employees.",
            "You sort of got quite a bias towards Microsoft News, which is I guess, not surprising, but you know that's not good for somebody who comes in for the first time.",
            "So again, hyper personalization is something that we're again."
        ],
        [
            "Working on so with that, I'm going to sort of call it a day.",
            "So if you want to try it out, then definitely go there and send us feedback.",
            "I try to reply to every email that we get.",
            "If people like to see what people like or don't like about emporea.",
            "Again, keep in mind it's three people, three people's work.",
            "It's a research prototype.",
            "It's not sort of Bing News yet, but we're working on it, so I'll take any questions.",
            "So when you're talking about the new text Jess about the nearest neighbors, socialism more than that.",
            "So yeah, so one of the things that's a little bit subtle about, say, the Matchbox system, is that right now it's set up to when it makes a recommendation, it looks at the user vector and the item vector, and it computes the inner product.",
            "Now one thing that we haven't been able to solve yet is.",
            "If you nearest neighbors works.",
            "If it's position close in position, but we're talking about close as far as inner product ghosts and having a fast way of finding all the vectors that have a large inner product is not easy because it's it's not related to position sort of at all is a bit is a bit strong, but you can have a vector that's very far away because it's extremely long still has a large inner product, so it's hard to sort of index a space based on inner product, and we've been we've done some work with.",
            "Random random hashing and random projections.",
            "And they work, but they also have disadvantages when you sort of want to ingest new things into the index.",
            "So it's it's not all it's I think it's unsolved that I know we couldn't find anything in the literature and haven't been able to come up with a good answer ourselves either, so.",
            "So how about this more traditional computational geometry approaches?",
            "Like in each?",
            "Umm, yeah.",
            "So KD trees also one thing that we tried, but again a KD tree we found is quite works very well, but it doesn't work very well when you have like, you know, 10s or hundreds of new items coming in a second.",
            "Like you know the amount of time it takes to sort of build and extend your KD trees just just.",
            "Much for real time.",
            "Right, so that's what we've been talking about with the with the random hashing.",
            "Sorry, the random projections.",
            "But again, so I'm not an expert on locality sensitive hashing, so my colleagues been working on that mostly.",
            "My understanding is that it's a position based metric, right?",
            "Most of the LSH algorithms are based on position, whereas here we're working with inner product and so some of these techniques don't don't apply.",
            "When you're working with inner products just because of that problem that I was saying, you can have a user vector sort of sitting somewhere and then an item vector that's sort of at a really big angle.",
            "But because it's so large it still has a large inner product.",
            "So another another solution to that would be to create a different graphical model where the it's not the inner product that matters but the distance.",
            "But you know that has other problems that we've struggled with, but I think I think a discussion of those issues are in the.",
            "I think it was a KDD paper that we published on Matchbox 2 years ago.",
            "So there's a discussion about those things in the paper there.",
            "You mentioned that you can have negative feedback and then you deal with that and re recommend.",
            "How do we actually deal with that feedback in order to get to a point where you are not excluding something that you shouldn't have, or because if you don't give too much weight, then it's not useful and give too much weight to the features that you select from what had been excluded.",
            "Then you end up leaving more stuff.",
            "Yes, yes no, absolutely so that's that's all encoded in the probabilistic model, so we don't have very much control over that.",
            "Except for the fact that when a person gives negative feedback, it's not like well.",
            "He definitely excludes the thing that he's given feedback on.",
            "But then it's sort of like, again, we sort of make or the public model makes a soft assignment.",
            "He moves the user away from items that are sort of similar to the item he gave feedback on.",
            "So there's still some probability that he's going to see things in that neighborhood, but it's less and less likely an you know.",
            "Worst case scenario, a user might have to give negative feedback, sort of twice.",
            "But then you know the system is going to learn, adapt itself and again also the nice thing about these probabilistic approaches is that it's very adaptive.",
            "So when a user gives goes in for the first time, one vote is going to have quite a big effect when you know like me, for me, when it's been trained for 6 seven months, one specific votes not going to have a massive effect anymore.",
            "So that's that sort of balance of how strongly to weigh each of the votes, that's.",
            "Taking care of by the probabilistic, but a probabilistic model so.",
            "For him to be a, we have one more question.",
            "So yeah.",
            "So so where are you this is that you are emphasizing that you are really the positive and negative numbers, so why dispatcher number 102 things?",
            "There is Ono, so it doesn't have to be positive negative.",
            "It needs to be at least more than one.",
            "So positive, negative or zero to 10 or real number.",
            "All of that works, but just like or nothing that doesn't work for these sort of type of approaches.",
            "Cool."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also thanks, thanks a lot for the invitation to come and talk here and present a bit of our work.",
                    "label": 0
                },
                {
                    "sent": "So I think what I'll try to do today is give a bit of a flavor of what the team that I'm part of, what we're sort of doing, the prototype or the research project that we've built and then sort of do a deep dive technical deep dive into some of the algorithms and you'll see the algorithms are fairly technical.",
                    "label": 0
                },
                {
                    "sent": "I'll just give you sort of an overview and all of these things is sort of published so we can talk about the details, or I can point you to the papers.",
                    "label": 0
                },
                {
                    "sent": "So it's all written up, so I'm working for Microsoft Research and Bing, so I'm part of prototyping team so we look at.",
                    "label": 0
                },
                {
                    "sent": "You know what kind of things that we could be doing, sort of in the next next generation of Bing.",
                    "label": 0
                },
                {
                    "sent": "And the thing that I'm going to be talking about today is sort of our large aspect of it is on news, news, recommendation, so sort of a new wave of news consumption.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is you probably have heard this like a gazillion times before, but it keeps blowing me away so this is from our friends at Google.",
                    "label": 0
                },
                {
                    "sent": "This is something Eric Schmidt has sort of thrown in a couple of years back, so so apparently from the dawn to civilization up until 2003.",
                    "label": 0
                },
                {
                    "sent": "Sort of humanity has had created 5 exabytes of data.",
                    "label": 0
                },
                {
                    "sent": "Now that same amount of data is now being generated every two days, and you know the reason that happens is because of some of these services that we sort of all really like to use.",
                    "label": 0
                },
                {
                    "sent": "You know, services where you know people actually generate data on a daily basis, and what that sort of leads to is this this decision, quicksand, as some people like to call it?",
                    "label": 0
                },
                {
                    "sent": "So there's just so much choice, so much stuff out there that you know it becomes not only hard for humans to navigate, that you know that space, but even for machines it's quite complicated to sort of make sense.",
                    "label": 0
                },
                {
                    "sent": "And you know how people achieve what they want to achieve, So what?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Order to abstract this a little bit.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the picture that we take on all the projects that the team that I'm in our sort of involved in all our projects around.",
                    "label": 0
                },
                {
                    "sent": "Sort of this abstractions.",
                    "label": 0
                },
                {
                    "sent": "So I guess when the web got started, we sort of had all these all these web pages and then the friends Google sort of made this really interesting observation that there's this web of links and that you can use that set of links between pages to do certain middle ranking.",
                    "label": 0
                },
                {
                    "sent": "And pay tracks.",
                    "label": 0
                },
                {
                    "sent": "A great example of that and sort of spread a lot of research, but there's a whole bunch of other interesting entities where you also can sort of impose this graph structure on.",
                    "label": 0
                },
                {
                    "sent": "You know places, physical places, and they have relationships.",
                    "label": 0
                },
                {
                    "sent": "You know physical proximity news they have, again, a relationship.",
                    "label": 0
                },
                {
                    "sent": "News is sort of, you know, they follow sort of histories and the branch into sort of sub news stories.",
                    "label": 0
                },
                {
                    "sent": "Certainly with products that there's.",
                    "label": 0
                },
                {
                    "sent": "Relationship with the product between facts.",
                    "label": 0
                },
                {
                    "sent": "And then more recently, I guess very interesting Lee social relationships, which are now sort of.",
                    "label": 0
                },
                {
                    "sent": "You know, we get to measure those we get to measure who is related to whom in what context, for what purpose.",
                    "label": 0
                },
                {
                    "sent": "So there's going from this idea of the graph of web pages.",
                    "label": 0
                },
                {
                    "sent": "There's now so many other graphs and the cool thing which at least we think is cool.",
                    "label": 0
                },
                {
                    "sent": "And we'd like to use this abstraction to drive our research in our prototyping, is that, you know.",
                    "label": 0
                },
                {
                    "sent": "People express well.",
                    "label": 0
                },
                {
                    "sent": "First of all, there exists links between these physical things, like, say places and news to the web, right?",
                    "label": 0
                },
                {
                    "sent": "So places my restaurant might have a home page of fact might have a Wikipedia page and people might have maybe a Facebook or Google Plus page.",
                    "label": 0
                },
                {
                    "sent": "But now, Interestingly enough, is that people are starting to associate themselves with certain of these of these physical entities in the real world.",
                    "label": 0
                },
                {
                    "sent": "So people might do checkins, people might do shares or likes of web pages.",
                    "label": 0
                },
                {
                    "sent": "News articles they might buy stuff on E Commerce stores.",
                    "label": 0
                },
                {
                    "sent": "And you know this is just to give a sense of scale.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a huge number of these associations that people may.",
                    "label": 0
                },
                {
                    "sent": "Can we call this sort of the interest graph for us is interesting, as an abstraction is they think about what kind of links do we observe in this interest graph from people to sort of physical objects or digital representations in the web?",
                    "label": 0
                },
                {
                    "sent": "But then, from a scientific point of view, the question is, how can we now sort of augment this interest graph?",
                    "label": 0
                },
                {
                    "sent": "How can we make sort of present users with new new?",
                    "label": 0
                },
                {
                    "sent": "Links how can we recommend the links like for example for places to be, you know can we recommend to people that they should be visiting a restaurant?",
                    "label": 0
                },
                {
                    "sent": "Maybe we want to recommend them anew.",
                    "label": 0
                },
                {
                    "sent": "A news article and so by taking this abstraction, you know, sort of all these different recommendation tasks and places, restaurants, news apps you want.",
                    "label": 0
                },
                {
                    "sent": "All these things sort of can be captured in sort of 1 and the same framework and hopefully we can sort of use information that we have from 1:00 and transfer that to the next.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the grand scheme of things.",
                    "label": 0
                },
                {
                    "sent": "That's sort of our grand goal.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to talk to you about today is you know how we sort of, you know, a start for us, something a project that we've been working on the last nine months and this is particularly for news.",
                    "label": 0
                },
                {
                    "sent": "And in recent months we've been doing places and deals, but I'm not going to talk about those today.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, so with this grand scheme of things in mind.",
                    "label": 0
                },
                {
                    "sent": "We started thinking about what tools can we use to actually be be building these recommender systems and one of the things that certainly our lab in Cambridge is very keen on and there's quite a bit of expertise is to use graphical modeling graphical modeling.",
                    "label": 0
                },
                {
                    "sent": "I mean we think is an exceptionally useful tool because it's sort of this all encompassing framework in which you can express things.",
                    "label": 0
                },
                {
                    "sent": "You can derive algorithms from them and the key point here is that your graphical models allow us to do modeling.",
                    "label": 0
                },
                {
                    "sent": "It allows us.",
                    "label": 0
                },
                {
                    "sent": "To specify facts that we think are true in the world so we can focus on the modeling aspect.",
                    "label": 0
                },
                {
                    "sent": "And you know the algorithms and everything just fall out.",
                    "label": 0
                },
                {
                    "sent": "We don't.",
                    "label": 0
                },
                {
                    "sent": "We don't have to sort of specify algorithms and we think that models are much easier to understand than than algorithms, certainly complicated algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to give you sense of what we think we could do with graphical models, I'll give sort of a 5 minute intro and then we'll apply it to this news reading experience.",
                    "label": 0
                },
                {
                    "sent": "So OK, so what's graphical models all about?",
                    "label": 0
                },
                {
                    "sent": "Graphical models are all about, you know, taking the world around us.",
                    "label": 0
                },
                {
                    "sent": "There's things we're not sure about.",
                    "label": 0
                },
                {
                    "sent": "For example, do you want?",
                    "label": 0
                },
                {
                    "sent": "Are you interested in this news article?",
                    "label": 0
                },
                {
                    "sent": "Yes or no?",
                    "label": 0
                },
                {
                    "sent": "OK, we're not sure whether you are, but we might have seen lots and lots of data about you.",
                    "label": 0
                },
                {
                    "sent": "And so we want to express that you know that that decision as a random variable and we sort of use probabilities to capture the uncertainty of that variable.",
                    "label": 0
                },
                {
                    "sent": "So the first key thing of graphical models is everything in the world.",
                    "label": 0
                },
                {
                    "sent": "We sort of describe as a random variable is something that we're uncertain about, and we're going to try to model what a system computer system thinks it believes that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "Second thing second, big ingredient of graphical models is that, well, there graphical, so we could sort of take.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Particular domain and we could sort of encode particular prior knowledge about that domain in an.",
                    "label": 0
                },
                {
                    "sent": "We sort of generally would represent that with a graph.",
                    "label": 0
                },
                {
                    "sent": "So in this case this graph would represent that you know there's three things that you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe sometimes we observe.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we don't observe them.",
                    "label": 0
                },
                {
                    "sent": "And we sort of say that this particular variable M influences G. But otherwise, and this variable M influences A.",
                    "label": 0
                },
                {
                    "sent": "But for example G and A or or sort of, they don't influence each other.",
                    "label": 0
                },
                {
                    "sent": "And all of that graphical structure can sort of be captured in equations in probability distributions, and there's sort of very specific rules about how sort of a distribution over all the things in the world could sort of be teased apart in little sort of local interactions.",
                    "label": 0
                },
                {
                    "sent": "So in this case, this graph would actually represent that you know there's a distribution over this variable M. This P of em.",
                    "label": 0
                },
                {
                    "sent": "There's a distribution over this valuable G which is only influenced by an, which is, you know, the probability of G given M. And similarly for a so you know, by using graphical models it's a very powerful tool to specify relationships that we believe exists in the world, and to encode sort of our domain or expert knowledge into into mathematical structure.",
                    "label": 0
                },
                {
                    "sent": "And then the really useful thing, and I'll point that out in our prototype as well.",
                    "label": 0
                },
                {
                    "sent": "Is that these graphical models allow us to do efficient inference, so it turns out that these graphical models code the mathematical structure of the models that were going to be describing, and Interestingly enough, we can use that to do when the algorithms come out of the models they turn out to be algorithms that are message passing algorithms, so these message passes.",
                    "label": 0
                },
                {
                    "sent": "So whenever we create a model and will give you examples in a second, then whenever we create a model sort of, an algorithm falls out in the algorithm itself is just an algorithm, that is, you know that.",
                    "label": 0
                },
                {
                    "sent": "Passing of messages that contain probabilities between nodes in a graph.",
                    "label": 0
                },
                {
                    "sent": "So in this case you'd have sort of three nodes in some computational system and they would be generating messages and you know they do that for a couple of iterations.",
                    "label": 0
                },
                {
                    "sent": "At some point they all say Yep, we're happy with the messages.",
                    "label": 0
                },
                {
                    "sent": "We've got no more changes, and then we have a solution for the graphical model, and that's very very useful because by doing message passing we know this is very scalable, so we can take a massive graph with billions and billions of nodes.",
                    "label": 0
                },
                {
                    "sent": "Spread them over a cluster and then have all these nodes communicate with each other on the cluster at some point of things going to converge and then we have our solution.",
                    "label": 0
                },
                {
                    "sent": "So why do we think that you know if we have these three things, why do we think that's that's a useful thing?",
                    "label": 0
                },
                {
                    "sent": "Oh, sorry.",
                    "label": 0
                },
                {
                    "sent": "And so the question is, we like to answer about graphical models or things like, OK, we have specified all these local relationships that we believe are true in the world.",
                    "label": 0
                },
                {
                    "sent": "And now we want to sort of ask questions about this distribution.",
                    "label": 0
                },
                {
                    "sent": "For example, you know, given that we've made all these connections with.",
                    "label": 0
                },
                {
                    "sent": "About you and the data we've observed in the data, we'd like to ask about yourself.",
                    "label": 0
                },
                {
                    "sent": "We can ask.",
                    "label": 0
                },
                {
                    "sent": "OK, now are you going to like this news article, or are you going to like to eat in this restaurant and you know, that would be asking about the marginal of this.",
                    "label": 0
                },
                {
                    "sent": "Distribute marginals of these distributions.",
                    "label": 0
                },
                {
                    "sent": "So what what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We do.",
                    "label": 0
                },
                {
                    "sent": "Why do we think that graphical models is so powerful?",
                    "label": 0
                },
                {
                    "sent": "So I'm talking about modeling all the time.",
                    "label": 0
                },
                {
                    "sent": "And let's say let's take a very, very, very simple problem, which is, how do we model sort of text?",
                    "label": 0
                },
                {
                    "sent": "General texts?",
                    "label": 0
                },
                {
                    "sent": "I'm not talking about personal personalization, I'm not talking about recommender systems just yet.",
                    "label": 0
                },
                {
                    "sent": "If we're just going to model text, well, one of the simplest thing that we can do, and that's useful in certain situations is to model text, or probabilistically model text as a unigram.",
                    "label": 0
                },
                {
                    "sent": "So does that mean we take a text corpus?",
                    "label": 0
                },
                {
                    "sent": "We have a bunch of documents and we say that each word, and so we assume that there's a probability distribution over words.",
                    "label": 0
                },
                {
                    "sent": "What's the probability that you know when we see a word for each word with that probability is we take our corpus?",
                    "label": 0
                },
                {
                    "sent": "Every document we go through words and then for each word that we observe, we can say OK, what's the probability of this word and this word then this were then this word.",
                    "label": 0
                },
                {
                    "sent": "So the only parameter in this model is this probability distribution over vocabulary.",
                    "label": 0
                },
                {
                    "sent": "We can compute the probability distribution over a corpus, or the probability for a particular document.",
                    "label": 0
                },
                {
                    "sent": "Now you might say OK, well, wait a second.",
                    "label": 0
                },
                {
                    "sent": "That's probably a really lousy probabilistic model for a text, because not you know, not every document is generated by one in the same vocabulary.",
                    "label": 1
                },
                {
                    "sent": "When I'm giving a talk here, then I use a very different vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Then next week when I'm giving a talk at a wedding, wedding speech is going to be a very different vocabulary.",
                    "label": 0
                },
                {
                    "sent": "OK, so you could say, well, maybe from.",
                    "label": 0
                },
                {
                    "sent": "Document the document or occasion to occasion.",
                    "label": 0
                },
                {
                    "sent": "I have a different vocabulary, so we'll assume that there are, you know, hundred different possible vocabularies.",
                    "label": 0
                },
                {
                    "sent": "That person might be using an for each document.",
                    "label": 0
                },
                {
                    "sent": "We first pick which of the vocabularies we generate all the words from, and then we're going to generate the words in that document from that particular vocabulary.",
                    "label": 0
                },
                {
                    "sent": "That particular model is known as a mixture of unigrams and it's sort of an extension of this unigram model.",
                    "label": 1
                },
                {
                    "sent": "It sort of says that there's this thing which we haven't observed in each document, which is what was the vocabulary that that was used to generate that word.",
                    "label": 0
                },
                {
                    "sent": "So talking in terms of graphical models, sort of, we have M documents, an end words in each document, and in this unigram there's just we just observe the words in that document when we talk about this mixture, we have M documents and words in each document and what we don't observe.",
                    "label": 0
                },
                {
                    "sent": "So This is why it's not shaded.",
                    "label": 0
                },
                {
                    "sent": "Is which vocabulary that this word come from?",
                    "label": 0
                },
                {
                    "sent": "In that particular document, so it's an extension of that unigram model and it's sort of represents a sort of a more sophisticated view of the world, and both are very valid models, and they're useful in different ways.",
                    "label": 0
                },
                {
                    "sent": "You pay computational price to do this mixture ING.",
                    "label": 0
                },
                {
                    "sent": "Make sure model, but it's more accurate model of the world, and you know people weren't happy with that as a model for text.",
                    "label": 0
                },
                {
                    "sent": "And then people came up with or David Black came up with this thing called latent, richly allocation, which is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other particular model to model text which will give you flavor of it.",
                    "label": 0
                },
                {
                    "sent": "So this model says OK. Well, actually when I talk about.",
                    "label": 0
                },
                {
                    "sent": "So here in this in this, in this academic conference at this workshop, and maybe sort of the next workshop about a different piece of my work, I'm going to be using maybe different different words, because in this talk I'm going to be talking about lot about recommender systems and things like that in another talk might be talking about nonparametric Bayes, which is the topic of my thesis.",
                    "label": 0
                },
                {
                    "sent": "So it's two different vocabularies.",
                    "label": 0
                },
                {
                    "sent": "But in those both those talks, you know there's certainly a more overlap.",
                    "label": 0
                },
                {
                    "sent": "There's a certain amount of vocabulary that I share between the two talks, mainly things like probability and statistics, and those kinds of things.",
                    "label": 0
                },
                {
                    "sent": "So David Bly came up with a model to explicitly represent that, and so his model says OK.",
                    "label": 0
                },
                {
                    "sent": "So now I have again M documents and end words, but instead of for each document, choosing your vocabulary to generate a word from what he says is OK. Every document can have, let's say, two or three vocabularies from which all the words are generated, and then for each word I'm going to select one of those two or three vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So in total there might be 100 vocabularies.",
                    "label": 0
                },
                {
                    "sent": "For a particular document, you know I I choose a subset of, say, three or four or something like that, and then for every word in that vocabulary I'm going to choose.",
                    "label": 0
                },
                {
                    "sent": "You know each vocabulary individually to generate a word from when you run that, that particular graphical model over text, you get some nice things.",
                    "label": 0
                },
                {
                    "sent": "For example, if you run this over news text and you look at what are the vocabularies, then you know over.",
                    "label": 0
                },
                {
                    "sent": "I think this was a news corpus and then you find that in news corpus one of the vocabularies is very distinctive about arts, another vocabulary is.",
                    "label": 0
                },
                {
                    "sent": "These are the top words in each of the vocabularies.",
                    "label": 0
                },
                {
                    "sent": "So in RTT new film show music in, Let's Say Education which is another vocabulary.",
                    "label": 0
                },
                {
                    "sent": "The system is just automatically learned.",
                    "label": 0
                },
                {
                    "sent": "You might see words like school students, schools, education teachers.",
                    "label": 0
                },
                {
                    "sent": "So this type of what this model is been doing?",
                    "label": 0
                },
                {
                    "sent": "Is it sort of extracted these topics or vocabularies from this corpus without any human intervention.",
                    "label": 0
                },
                {
                    "sent": "The only thing that human had to do was say how it believed that you know the words in those corpora were generated.",
                    "label": 0
                },
                {
                    "sent": "My point of these two last slides is that you know you can start with a very simple model of the world.",
                    "label": 0
                },
                {
                    "sent": "That might be wrong and might not capture sort of the effects you're interested in, but by using the language of graphical models you can sort of always add bits and pieces to your probabilistic or statistical models, sort of grow them organically and always capture more and more effects of sort of the real world, and then in the end hopefully make really good predictions an this sort of way of thinking about the world way of thinking about learning.",
                    "label": 0
                },
                {
                    "sent": "And data mining is sort of what our team is really into.",
                    "label": 0
                },
                {
                    "sent": "We think this is a really good idea and we think this this is also very computationally very scalable, so OK, that was my.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My my oh by the way, this particular line of thinking about modeling text for documents, you know I sort of showed you sort of four sort of steps of increasing complexity, but people have been, you know, if you look at this web page by David Mem, know if you look at this webpage, people have developed hundreds of extensions of this particular model.",
                    "label": 0
                },
                {
                    "sent": "It's just modeling text based on graphical models.",
                    "label": 0
                },
                {
                    "sent": "So perhaps adding the authors of the documents into the mix or adding grammar into the mix so.",
                    "label": 0
                },
                {
                    "sent": "This way of thinking is, I mean it's not unique to our team.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a lot of people in the world that are sort of into this, and I just wanted to show you that it's very flexible and it's a very nice way of communicating models between people.",
                    "label": 0
                },
                {
                    "sent": "It's a very nice way of reusing statistical model speaking.",
                    "label": 0
                },
                {
                    "sent": "You could just say, oh, here's the model we used and share it with people.",
                    "label": 0
                },
                {
                    "sent": "So that's what.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've been doing so before I get into the details of the algorithms that we use for our news recognition system.",
                    "label": 0
                },
                {
                    "sent": "I'll give you a demo and this is a little bit tricky.",
                    "label": 0
                },
                {
                    "sent": "This is a this is an academic conference, quite a few people are using laptops.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wi-Fi, so it might be incredibly slow, depending on how useful you guys think this is, so this is the thing that we've built, so you could try it out for yourself, so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's an experimental research prototype.",
                    "label": 0
                },
                {
                    "sent": "It's up at projectemporia.com.",
                    "label": 0
                },
                {
                    "sent": "It's something that we launched in January an essentially what we do is we roughly do the following, so we Microsoft has a deal with Twitter where we get all the.",
                    "label": 0
                },
                {
                    "sent": "Public tweets that get posted.",
                    "label": 0
                },
                {
                    "sent": "So we get all these public tweets and we look at all the links that get shared and that is, you know.",
                    "label": 0
                },
                {
                    "sent": "Depending on the time of the day and whether the US is awake are not, you know that might be between 20 or 50 links a second that we get from Twitter.",
                    "label": 0
                },
                {
                    "sent": "Those are all the links that get shared and we sort of take all those links and we look at the web pages that they point to and we build up representation of the actual thing that is pointed at.",
                    "label": 0
                },
                {
                    "sent": "So we would then sort of look for English news articles.",
                    "label": 0
                },
                {
                    "sent": "In this case an.",
                    "label": 0
                },
                {
                    "sent": "The first thing we do is we categorize it so we categorize all the news into sort of different different cat into these eight different categories and then we we string them to this web page.",
                    "label": 0
                },
                {
                    "sent": "So the idea of the web pages that we want to make a personalized news reader.",
                    "label": 0
                },
                {
                    "sent": "So in order for the system to know who you are, you actually have to make yourself known.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "I've sort of linked up my Facebook and my Twitter account with Project Emporia so it now knows that it needs to make a recommendation for Mianite could look at sort of the previous data that is gathered about me and I'll show that to you in a second.",
                    "label": 0
                },
                {
                    "sent": "So if I go into particular technology into particular categories, in this case technology what we show is sort of what project Emporia thinks is the top six news articles for you in the last 24 hours.",
                    "label": 0
                },
                {
                    "sent": "So we show those six.",
                    "label": 0
                },
                {
                    "sent": "Now the question is, how the hell does this thing, no?",
                    "label": 0
                },
                {
                    "sent": "What you know, the top six articles for you should be.",
                    "label": 0
                },
                {
                    "sent": "So the way we do that is that we've made a very explicit sort of design choice early on that we believe that it's very important that people know how and what they're influencing this recommender system.",
                    "label": 0
                },
                {
                    "sent": "And we decided that we didn't want to sort of use sort of usage signals, but rather have an explicit feedback mechanism.",
                    "label": 0
                },
                {
                    "sent": "So in this case I can sort of look at this this particular article, and I could say, Yep, this is sort of the type of news that I'm interested in, and now obviously you know this has been trained for six months, so it's doing a pretty good job.",
                    "label": 0
                },
                {
                    "sent": "But I can say, Yep, this is an article that I want to see more up so I can give an explicit vote on this article.",
                    "label": 0
                },
                {
                    "sent": "Or, you know, Windows Phone.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's something I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is all stuff I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "I could also for example, you know, you know, say an article.",
                    "label": 0
                },
                {
                    "sent": "I could say I don't.",
                    "label": 0
                },
                {
                    "sent": "I want to see less of this, so I give I give.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to mess up my profile right now, but so I give explicit feedback to the system.",
                    "label": 0
                },
                {
                    "sent": "Now what's interesting is that I will come back to that point in a second is that you know that when I give that feedback, that sort of goes instantly to our recommender system, which runs on the cloud on Windows Azure and instantly it will sort of update its recommendation and sort of when I hit refresh, it'll sort of.",
                    "label": 0
                },
                {
                    "sent": "Take that that feedback that I just gave it will sort of have integrated that into the recommendation system, which I'll talk about in a second and come up with sort of a new six new new recommendations so.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's sort of the 1st 1st.",
                    "label": 0
                },
                {
                    "sent": "Sort of thing that we launched back in January, but then we thought, look, we think there's quite a like in the previous talk suggested already.",
                    "label": 0
                },
                {
                    "sent": "Is quite a bit of.",
                    "label": 0
                },
                {
                    "sent": "We can leverage the social network and just like like the previous speaker mentioned, you know we can leverage the social network by leveraging the trust network that people have built.",
                    "label": 0
                },
                {
                    "sent": "So the way that works is that right now this was sort of a technology category technology channel.",
                    "label": 0
                },
                {
                    "sent": "You know I could go to politics channel or other type of channels.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "We've also created this thing called Social Channel and the social channel.",
                    "label": 0
                },
                {
                    "sent": "Essentially what we do is we have this Twitter friends and Twitter friends of Friends channel.",
                    "label": 0
                },
                {
                    "sent": "So essentially what this is doing?",
                    "label": 0
                },
                {
                    "sent": "Is what we do in the Twitter friends of Friends Channel.",
                    "label": 0
                },
                {
                    "sent": "I'm looking at all the links that my friends or my friends of friends have shared.",
                    "label": 0
                },
                {
                    "sent": "In their public timelines, you know that might be anything from a few you know, few tends to a few hundred too few.",
                    "label": 0
                },
                {
                    "sent": "1000 links.",
                    "label": 0
                },
                {
                    "sent": "I take that set of links, and I sort of re rank that according to my personal taste.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm presenting here, sort of.",
                    "label": 0
                },
                {
                    "sent": "This re ranked list from my Twitter friends of friends and then I can.",
                    "label": 0
                },
                {
                    "sent": "Similarly I could.",
                    "label": 0
                },
                {
                    "sent": "I could give feedback.",
                    "label": 0
                },
                {
                    "sent": "On on on these articles and influence how the system is going to rank things in the future.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is something that we launched maybe three months ago as sort of as an experiment.",
                    "label": 0
                },
                {
                    "sent": "And really we think that it's doing or what it really is doing is it's taking the filter that you've already built by following people on Twitter, so you've already done that work.",
                    "label": 0
                },
                {
                    "sent": "It's sort of you've already expressed in your social network.",
                    "label": 0
                },
                {
                    "sent": "Whom you?",
                    "label": 0
                },
                {
                    "sent": "Trusting for forgetting particular news items or not, you've already expressed that, and without having to do any extra work just by linking your Twitter account, we can then give you that, that that personalized experience, and obviously, given that people follow different people on Twitter.",
                    "label": 0
                },
                {
                    "sent": "This is a completely personalized personalized experience.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the the demo of Project Emporia and what I think I'll do next is I'll talk you through some of the steps of this of the architecture, and then also give you a quite some details about the recommendation algorithms that we've built for this.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "From a high level point of view, how this this whole thing work?",
                    "label": 0
                },
                {
                    "sent": "So people in Twitter write tweets they share links with each other and these links they come into.",
                    "label": 0
                },
                {
                    "sent": "Well, they get shared to Twitter and Twitter sends them to us.",
                    "label": 0
                },
                {
                    "sent": "Now the first thing we do as I suggest, as I said already, is we index all the links that get shared on Twitter and we categorize them enough for the categorization.",
                    "label": 0
                },
                {
                    "sent": "We're doing something which I think is cool.",
                    "label": 0
                },
                {
                    "sent": "It doesn't take us a lot of work.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we do is we subscribe to RSS feeds from the BBC, The Guardian CNN.",
                    "label": 0
                },
                {
                    "sent": "You know, a lot of different news sources and we know we subscribe to the technology feeds from BBC.",
                    "label": 0
                },
                {
                    "sent": "The technology feed from CNN and we get all the all the articles from the technology feeds.",
                    "label": 0
                },
                {
                    "sent": "Then what we do is we train a classifier, a very simple classifier based on the labeled data that we get from CNN into Guardian etc.",
                    "label": 0
                },
                {
                    "sent": "So we train a classifier based on the data that the editors of these new sources have been have been labeling.",
                    "label": 0
                },
                {
                    "sent": "We train a classifier to say whether an article is from technology or not, and that happens every five minutes.",
                    "label": 0
                },
                {
                    "sent": "So the RSS crawler just runs and every five minutes the classifier gets trained and gets updated and then gets put into into our pipeline an every link that comes in, we get the web page and we classified according to that classifier.",
                    "label": 0
                },
                {
                    "sent": "So that's quite useful because you know, we get a very up-to-date classifier on technology and politics and all that kind of stuff, especially for free.",
                    "label": 0
                },
                {
                    "sent": "So we sort of use, I guess, crowdsourcing if you want or expert sourcing to do that.",
                    "label": 0
                },
                {
                    "sent": "This is really important, because when we launched there was, let's say, for world events there was, we launched in January and for world events there is very little news on earthquakes in those kinds of tragic events.",
                    "label": 0
                },
                {
                    "sent": "In our pipeline, and so if you know the first news article that came in on the Japan earthquake almost surely got misclassified.",
                    "label": 0
                },
                {
                    "sent": "But because these sort of new sources on the web, these editors are there on the ball on after 5 or 10 minutes, they'll have their first news articles about these events, and so the classifier knows how to pick that up in sort of no time, and so we get a, you know, almost real time classifier of these events that have never been seen before.",
                    "label": 0
                },
                {
                    "sent": "OK, so these these links and these tweets come into our categorizer we index them, put them into a big store in the clouds and then we have our recommendation engine and our recommendation engine.",
                    "label": 0
                },
                {
                    "sent": "I'll give you the details of how it works in a second, but it's a recognition engine which is called Matchbox and essentially what it does is it assigns to every news article of vector of low dimensional vector, Let's say 5 or 10 dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "Based on the content of that article, Anna model that's been trained, so that's the trade vector.",
                    "label": 0
                },
                {
                    "sent": "So we store all those trade factors for the last 24 hours and now a user goes to the web page 2 important emporia.com and he says OK, give me the give me the top news dashboard and the system talks to the query engine, the query instances.",
                    "label": 0
                },
                {
                    "sent": "OK, well it looks at the at those vectors and essentially what Emporia or what the recommendation engine does.",
                    "label": 0
                },
                {
                    "sent": "Is it takes that sort of, let's say 5 dimensional vector for the item and every user also gets represented by a 5 dimensional vector and it's sort of computes all the inner product.",
                    "label": 0
                },
                {
                    "sent": "So let's say there's a million articles every day in our news index it takes the users vector and it computes the inner product with all the million articles and then it uses.",
                    "label": 0
                },
                {
                    "sent": "It looks at the top ten of those at the top of the top ten articles with the highest inner product and that product related to the probability of the user liking that article.",
                    "label": 0
                },
                {
                    "sent": "And we've sent those articles back to the user and then, Interestingly enough, the user gives feed or he might not, but generally certainly in the beginning he will give feedback to the system and the system will update itself.",
                    "label": 0
                },
                {
                    "sent": "So I think the interesting thing here that was an important driver for us is that I think a lot of, um, I think you know a lot of these.",
                    "label": 0
                },
                {
                    "sent": "Search architectures that have been built in the between 2000 2010 are often very static, so they might have some kind of real time ingestion feed, like new articles coming into search index for example, but there's very little infrastructure for real time changes to an index, and we think similarly like I think.",
                    "label": 0
                },
                {
                    "sent": "For example, I'm not Privy to the details of, say, the Amazon recommender system, but for example, our understanding is that the Amazon recommender system, for example, computes recommendations, or at least in the past that used to compute recommendations everyday.",
                    "label": 0
                },
                {
                    "sent": "And so it won't.",
                    "label": 0
                },
                {
                    "sent": "You know, if you buy a new article, it won't update its recommendations right away, and probably doesn't make sense for the system to do that because you know, you don't buy something in 10 minutes later by something else before news.",
                    "label": 0
                },
                {
                    "sent": "This is absolutely critical if you're going to see a news article, and it's about something I don't know that happened in the world that you're absolutely not interested in.",
                    "label": 0
                },
                {
                    "sent": "You say at this like this.",
                    "label": 0
                },
                {
                    "sent": "I don't want to see any more news of this.",
                    "label": 0
                },
                {
                    "sent": "You want instant personalization, you do not want to see that you know.",
                    "label": 0
                },
                {
                    "sent": "10 minutes later, pop up again because that's a really poor user experience and so one of the core things that we spend a lot of time trying to do is how can we sort of, you know, Bing scale builds, a personalization engine where you might have thousands of 10,100 thousand millions of users and be able to get real time feedback and be able to train your recommender system, sort of in an online fashion.",
                    "label": 0
                },
                {
                    "sent": "So that was the challenge.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of, you know what project Emporia from a 30,000 feet perspective looks like an I think next.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll sort of do a bit of a deep dive into the into the recommender system.",
                    "label": 0
                },
                {
                    "sent": "So first I'm going to talk to you about the recommender system that we actually use an, and then next I'll be talking about sort of some extensions of recommender systems based on sort of graphical models that were sort of playing around with and testing out.",
                    "label": 0
                },
                {
                    "sent": "So this is work that's been done.",
                    "label": 0
                },
                {
                    "sent": "This recommender systems work that's been done maybe two years ago three years ago, and was sort of initially initiated.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the Netflix prize.",
                    "label": 0
                },
                {
                    "sent": "So what's what's the tricky thing about or what does recommender system?",
                    "label": 0
                },
                {
                    "sent": "What's it all about?",
                    "label": 0
                },
                {
                    "sent": "So we have a particular user and we have items and items.",
                    "label": 0
                },
                {
                    "sent": "Could be anything from product Stew news, to anything that you really want.",
                    "label": 0
                },
                {
                    "sent": "And then we have feedback and feedback.",
                    "label": 0
                },
                {
                    "sent": "Could be anything again from explicit ratings like on Amazon.",
                    "label": 0
                },
                {
                    "sent": "It could be a click, maybe on a search engine.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or it could actually be no money spent or something like that.",
                    "label": 0
                },
                {
                    "sent": "Now again, abstracting away the problem and not thinking about the actual math and the algorithms that want to use what does recommender system do so recommender system we have a bunch of users.",
                    "label": 0
                },
                {
                    "sent": "We have a bunch of movies.",
                    "label": 0
                },
                {
                    "sent": "In this case we get to see feedback so particular user dislike that movie and we get a whole bunch of that.",
                    "label": 0
                },
                {
                    "sent": "Now the important thing here is that it's sparse, so we have compared to the number of entries in this table.",
                    "label": 0
                },
                {
                    "sent": "We only have very very few entries that we actually got explicit ratings about.",
                    "label": 0
                },
                {
                    "sent": "And the question that we want to ask is given a particular user, we might want to know whether he or she liked that particular movie, and in this case you could say, well, it's kind of obvious because you know user DN.",
                    "label": 0
                },
                {
                    "sent": "User B are actually quite similar.",
                    "label": 0
                },
                {
                    "sent": "They both like movie one.",
                    "label": 0
                },
                {
                    "sent": "They sort of this like movie for so it's probably very likely that you know user D likes movie 6 based on user base behavior.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how a lot of these recommender systems generally work.",
                    "label": 0
                },
                {
                    "sent": "But now an interesting question happens when what if no ones ever rated this movie before?",
                    "label": 0
                },
                {
                    "sent": "And for movies, this is perhaps not as big of a deal because there's actually not that many movies out in the world.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, I think Netflix is about 200,000 movies.",
                    "label": 0
                },
                {
                    "sent": "That's about as much news as we get in a day.",
                    "label": 0
                },
                {
                    "sent": "So every so for news.",
                    "label": 0
                },
                {
                    "sent": "This is absolutely critical.",
                    "label": 0
                },
                {
                    "sent": "There's a lots of stuff that no one's ever rated before, so how do you deal with that?",
                    "label": 0
                },
                {
                    "sent": "Well, the way Matchbox deals with that is by using features.",
                    "label": 0
                },
                {
                    "sent": "So by using features we can actually see that this movie is actually similar to other movies.",
                    "label": 0
                },
                {
                    "sent": "And, well, we already established that the user was similar to another user, but in this case those two movies are in some sense similar because they're both action movies.",
                    "label": 0
                },
                {
                    "sent": "I don't know if there's any other any other not so you know this is just a toy example, but because they're both action movies, you know the system can learn that maybe user dies actually going to like movie 5, so features are absolutely critical here, so extracting metadata and this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What Matchbox is doing?",
                    "label": 0
                },
                {
                    "sent": "So how does Matchbox roughly work?",
                    "label": 0
                },
                {
                    "sent": "I'll paint sketch here 1st and then I'll show you show you what the model actually looks like.",
                    "label": 0
                },
                {
                    "sent": "So the way the model works in practice is you know you have a bunch of movies and a bunch of users and a user comes into the blue users, any dislikes Rambo.",
                    "label": 0
                },
                {
                    "sent": "So in in this sort of vector space.",
                    "label": 0
                },
                {
                    "sent": "So every user as I said before and every movie gets sort of allocated represented by a vector in a particular space.",
                    "label": 0
                },
                {
                    "sent": "In this case it's 2 dimensions.",
                    "label": 0
                },
                {
                    "sent": "But it could be 1020 fifty 100 dimensional, so the blue user dislikes the Rambo movie, so they get sort of moved apart in this space.",
                    "label": 0
                },
                {
                    "sent": "Maybe he likes Harry Potter so they move closer together.",
                    "label": 0
                },
                {
                    "sent": "The Gray user likes Rambo, so he moves closer to that movie The.",
                    "label": 0
                },
                {
                    "sent": "Also, there was yellow Degray user dislikes nothing Hills, they move further apart but he likes Rambo so you know all those guys come closer together and you could sort of imagine that if you have like a billion news articles or a billion movies ratings on movies that you know all these guys move in this space and what you can show is that at some point the system is going to find sort of the best possible layout where people who like particular movies are close together and and people who dislike movies there further apart.",
                    "label": 0
                },
                {
                    "sent": "So that's essentially how Matchbox words works.",
                    "label": 0
                },
                {
                    "sent": "It takes users, an movies, embeds them in sort of a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "In this case A5 dimensional, 2 dimensional space, and then looks at who is closer to.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does that look like from a graphical model point of view?",
                    "label": 1
                },
                {
                    "sent": "So the graphical models?",
                    "label": 0
                },
                {
                    "sent": "This might look a bit scary, but it's actually not not too complicated.",
                    "label": 0
                },
                {
                    "sent": "So the thing that we observe are the metadata about the user, the ratings that user give to movies, and the metadata about the movies.",
                    "label": 0
                },
                {
                    "sent": "So going from left to right when we get the metadata about the user, we take that metadata and we sort of take a matrix or some parameter and we map that metadata which could potentially be.",
                    "label": 0
                },
                {
                    "sent": "You know a million dimensional we might have like a million pieces of metadata about a particular user.",
                    "label": 0
                },
                {
                    "sent": "You know his age, his address an and let's say you know H could be.",
                    "label": 0
                },
                {
                    "sent": "We could have sort of a feature for being one year old and two year old in three year old.",
                    "label": 0
                },
                {
                    "sent": "So that's 100 or we might have the users you know, just good or something in the system, which might be, you know, a billion if we have a billion users.",
                    "label": 0
                },
                {
                    "sent": "A billion different features.",
                    "label": 0
                },
                {
                    "sent": "So this is very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "We take a parameter and this could be same matrix and we multiply those two together and map them.",
                    "label": 0
                },
                {
                    "sent": "Onto this 5 dimensional or three dimensional space and this is like the key thing.",
                    "label": 0
                },
                {
                    "sent": "This is what we need to learn.",
                    "label": 0
                },
                {
                    "sent": "This is what we want to learn from data.",
                    "label": 0
                },
                {
                    "sent": "So we take the users metadata.",
                    "label": 0
                },
                {
                    "sent": "We take this parameter which we're going to be learning and map it onto sort of a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "We similarly take the metadata about a movie or an item.",
                    "label": 0
                },
                {
                    "sent": "We take the parameter parameter, which we're going to learn and we sort of.",
                    "label": 0
                },
                {
                    "sent": "In this case multiply them, say and map that into sort of a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So now we have two vectors.",
                    "label": 0
                },
                {
                    "sent": "Again, we don't know what the vectors are because we don't know what those parameters are going to learn all that.",
                    "label": 0
                },
                {
                    "sent": "We take those two vectors.",
                    "label": 0
                },
                {
                    "sent": "The user in the item vectors and then we compute the inner product.",
                    "label": 0
                },
                {
                    "sent": "So this number represents the inner product between those two vectors and then we save that inner product is.",
                    "label": 0
                },
                {
                    "sent": "Let's say you know very big and very positive.",
                    "label": 0
                },
                {
                    "sent": "Then the the movie gets a high rating from that user.",
                    "label": 0
                },
                {
                    "sent": "If it's very negative to movie gets a low low rating.",
                    "label": 0
                },
                {
                    "sent": "So that's the graphical model for Matchbox and when we this is sort of what are.",
                    "label": 0
                },
                {
                    "sent": "Our representation of how recommendations could be generated potentially and now the key thing is that at some point we sort of throw it all into an algorithm, which I'll say something about next, and then we take a user and a movie which haven't.",
                    "label": 0
                },
                {
                    "sent": "You know, this user hasn't seen that movie and now we can compute what's the probability that that user is going to like that particular movie so.",
                    "label": 0
                },
                {
                    "sent": "Sort of, in math.",
                    "label": 0
                },
                {
                    "sent": "You could do sort of differently.",
                    "label": 0
                },
                {
                    "sent": "You could say if this X is the representation of the user XI, the reputation of the movie, and you sort of matrix, multiply them into sort of smaller vectors S&T, you compute the inner product and then we say that the rating potential is normally distributed around around that inner product.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially what you get out of that is that, let's say for a particular user there's, let's say, a bunch of features that represent the user ID of a bunch of features that represent his or her gender.",
                    "label": 0
                },
                {
                    "sent": "Maybe the country they live in, the height, etc.",
                    "label": 0
                },
                {
                    "sent": "And so for each of these features, a user only has one of them on, so the user only has one user ID is only one gender, lives in a particular country, and each of these.",
                    "label": 0
                },
                {
                    "sent": "Sort of, each of these features that are on or off.",
                    "label": 0
                },
                {
                    "sent": "Contributes to the vector of that user in this low dimensional space, so this particular user represented with these features sort of sits here in this space, and each of these individual features sort of contributed a particular component to that space.",
                    "label": 0
                },
                {
                    "sent": "And that's what we're going to have to learn is.",
                    "label": 0
                },
                {
                    "sent": "How do we map?",
                    "label": 0
                },
                {
                    "sent": "How do you know?",
                    "label": 0
                },
                {
                    "sent": "For each of these, sort of there's a vector?",
                    "label": 0
                },
                {
                    "sent": "And how do we learn these vectors?",
                    "label": 0
                },
                {
                    "sent": "That's what we're going to have to learn from data.",
                    "label": 0
                },
                {
                    "sent": "Similarly for the items.",
                    "label": 0
                },
                {
                    "sent": "So on item might have an ID.",
                    "label": 0
                },
                {
                    "sent": "Which sort of points in that direction.",
                    "label": 0
                },
                {
                    "sent": "There might be genre points in a little bit in that direction, and the actual position of a movie sort of the sum of all these vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you know the user in the movie, sit in this space here and their inner product is then going to represent you, know how much the user is going to like that movie.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the nice thing about saying OK, so about graphical models is we we said we state.",
                    "label": 0
                },
                {
                    "sent": "We claim that that's how the world works.",
                    "label": 0
                },
                {
                    "sent": "OK, we claim that these ratings have been generated according to these mechanisms and we can do that.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's wrong, but we can just say that this is how it was.",
                    "label": 0
                },
                {
                    "sent": "And then we've sort of defined a probability distribution, the one using the graphical model showed before we've defined a probability distribution where some of the.",
                    "label": 0
                },
                {
                    "sent": "You know random variables have been observed, some haven't been observed, and then it's just sort of math.",
                    "label": 0
                },
                {
                    "sent": "You could sort of, you know, crank the handle and you know this whole books about it.",
                    "label": 0
                },
                {
                    "sent": "Chris Bishop, who's our lab directors, written a really nice sort of intro to machine learning book where there's one particular chapter that just says if you define a graphical model then this is the algorithm that you need to run in order to to now make start making predictions and you know there's no no tuning, no questions about how to do it, it's just it just.",
                    "label": 0
                },
                {
                    "sent": "Falls out and that algorithm turns out to be message passing, so this is sort of the message passing representation and essentially you know all the numbers and everything don't matter that much, but you know there's still the observed.",
                    "label": 0
                },
                {
                    "sent": "Metadata, there's still the rating, which I guess in this case is not observed.",
                    "label": 0
                },
                {
                    "sent": "And then there's all the links to all the variables that were on the previous slide and essentially the algorithm is literally just sending messages between all these nodes, so it scales very nicely, and so there's sort of, you know, we've.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this we've done this, and this was done on Movielens, which is a movie recommendation datasets.",
                    "label": 0
                },
                {
                    "sent": "And when we do this, so generally we wouldn't do this.",
                    "label": 0
                },
                {
                    "sent": "This mapping into 2 dimensions because that's a very it's not a very expressive space.",
                    "label": 0
                },
                {
                    "sent": "We do this in 10 or 20 dimensions, but if we do this in two dimensions, we can sort of visualize it.",
                    "label": 0
                },
                {
                    "sent": "So all these Gray dots are users in this movie lens database and all the red dots are movies and essentially what you could sort of see here is that for example, the 20 Four Seasons are sort of sort of together in that space, which makes sense because if a user sort of sits here.",
                    "label": 0
                },
                {
                    "sent": "It means he's probably going to like 24 season three, which is probably quite suggestive of him liking Season 2, so you could sort of see that you'd get this sort of embedding of movies and users in the same space, and then we can start making recommendations.",
                    "label": 0
                },
                {
                    "sent": "Now when we look at a particular user, let's say this Gray user here, then we could sort of draw cone of all the movies that have a particular inner product with this user.",
                    "label": 0
                },
                {
                    "sent": "And so this is sort of the users preference.",
                    "label": 0
                },
                {
                    "sent": "Cone is sort of the movies that would potentially have a high compatibility with him.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we did this couple of years ago, we did this on Netflix with 100 million ratings an on one machine.",
                    "label": 0
                },
                {
                    "sent": "We sort of train this thing took 2 hours and we can sort of well cinematch is the system that Netflix used themselves and they get a particular score.",
                    "label": 0
                },
                {
                    "sent": "Root mean squared error score and then we sort of trained our system with very different degrees of dimensionality so different degrees of expressiveness and you know we sort of get we just in quite a bit.",
                    "label": 0
                },
                {
                    "sent": "It's quite a.",
                    "label": 0
                },
                {
                    "sent": "Accurate system, so this recommender system is quite accurate.",
                    "label": 0
                },
                {
                    "sent": "But the cool thing about is for our point of view is although it might not be like the best system in the system that you know the one the Netflix prize you know we can actually train this in two hours on 100 million ratings and I guess at web scale that's important.",
                    "label": 0
                },
                {
                    "sent": "So we can train this.",
                    "label": 0
                },
                {
                    "sent": "This type of system we can train this in an online fashion.",
                    "label": 0
                },
                {
                    "sent": "So that was first first very important and then we took this this.",
                    "label": 0
                },
                {
                    "sent": "Matchbox System an we implement we made that sort of the core recommender system that's it's sort of at the heart of project Emporia, and that's the one that's making the recommendations.",
                    "label": 0
                },
                {
                    "sent": "Now when we did, this project was really a research prototype and it was really meant for us to understand how we could make this scale.",
                    "label": 0
                },
                {
                    "sent": "What are the important things when we build these recommender systems?",
                    "label": 0
                },
                {
                    "sent": "How can we do this in real time?",
                    "label": 0
                },
                {
                    "sent": "And there's sort of three things that we found that we we didn't sort of that weren't as big of a deal when we did Netflix and things like that.",
                    "label": 0
                },
                {
                    "sent": "And I think I think if there's one thing that I can convey to sort of the academic community from, I guess from industry experience.",
                    "label": 0
                },
                {
                    "sent": "Is that you know these three things I personally think are very interesting research topic.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first problem is that Matchbox needs positive and negative training data, so you cannot train a system like Matchbox and actually not very many recommender systems using only positive data.",
                    "label": 0
                },
                {
                    "sent": "There's a whole lot of philosophical questions about that, but say for example from an algorithmic POV Matchbox.",
                    "label": 0
                },
                {
                    "sent": "If you just give it like likes or you know people liking certain things, then essentially what it will do is it will just say everybody sits in the middle of this sort of Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So it essentially says everybody likes everything and it's perfectly happy because the only data it sees is everybody liking everything.",
                    "label": 0
                },
                {
                    "sent": "Well, it doesn't see every everybody liking doesn't see an Association of everybody with everything but everything it sees is alike, so it's a perfect, perfectly well, perfectly good explanation of the data, which is obviously really, really bad if you, let's say, want to train a link recommender system from Facebook likes or Google Plus or something.",
                    "label": 0
                },
                {
                    "sent": "Then obviously that's going to be a problem.",
                    "label": 0
                },
                {
                    "sent": "You know you're only going to see positive feedback.",
                    "label": 0
                },
                {
                    "sent": "And for Emporia it was sort of easy because we designed around that.",
                    "label": 0
                },
                {
                    "sent": "But not every domain sort of allows for that, so a lot of probabilistic and non probabilistic recommender systems I think or we think have had this problem.",
                    "label": 0
                },
                {
                    "sent": "And there's not very many good systems that know how to deal with this, so I think that's a very interesting you know open problem for the research community.",
                    "label": 0
                },
                {
                    "sent": "Another one is the problem of fast indexing using sequential updates.",
                    "label": 1
                },
                {
                    "sent": "So with Matchbox you have to imagine that there's about a million.",
                    "label": 0
                },
                {
                    "sent": "New so we have a bunch of users, you know X number of users and we get about a million new news articles that we need to put in this space.",
                    "label": 0
                },
                {
                    "sent": "Some of them expire, but we need to put them in at a quite a fast rate while at the same time you know doing learning, so changing.",
                    "label": 0
                },
                {
                    "sent": "You know where all these guys live in in this low dimensional space while making recommendations.",
                    "label": 0
                },
                {
                    "sent": "And it's not a very easy, very easy problem and I think again, it's sort of overlooked.",
                    "label": 0
                },
                {
                    "sent": "You know, in the Netflix Prize, because I guess in Netflix you know everybody was really into beating their system by 10% and winning $1,000,000.",
                    "label": 0
                },
                {
                    "sent": "But I think people sort of overlooked a little bit.",
                    "label": 0
                },
                {
                    "sent": "You know how you could make these recommender systems work in real time, which I think again is an open and open space where lots of good things can be can be done, and then third, and this is absolutely critical, this has been our biggest thing is that all of these recommender systems and I think.",
                    "label": 0
                },
                {
                    "sent": "Pretty much every you know, every recommender system in the world that's really good at doing.",
                    "label": 0
                },
                {
                    "sent": "Recommendations is actually really bad at making at doing explanations and what people very often do, so we don't explain why a particular user Emporia got to see a particular news article.",
                    "label": 0
                },
                {
                    "sent": "What people have said, you know, people have built like the.",
                    "label": 0
                },
                {
                    "sent": "Some of the other recommender systems, what they often do is they just lie about it.",
                    "label": 0
                },
                {
                    "sent": "They just light, so they use a very powerful, maybe a probabilistic model or some matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "Something fancy to do the recommendation.",
                    "label": 0
                },
                {
                    "sent": "But then when they actually need to explain it, they'll just say you know, it's because you also liked this article or it's because you also like and it's sort of a drastic simplification of reality.",
                    "label": 0
                },
                {
                    "sent": "Now that's absolutely fine if people don't mind that you know that's absolutely fine.",
                    "label": 0
                },
                {
                    "sent": "There's no problem with that.",
                    "label": 0
                },
                {
                    "sent": "But what I think is interesting from a research point of view.",
                    "label": 0
                },
                {
                    "sent": "Is how can we design A recommender system where we can both get sort of good predictions and do accurate recommendations but still be able to explain to people you know why it happens and you know my grandmother is not interested in low dimensional embeddings?",
                    "label": 0
                },
                {
                    "sent": "Certainly not if they're probabilistic, so you know those those are not explanations.",
                    "label": 0
                },
                {
                    "sent": "So I think those are three challenges and definitely challenges for us and things that we're thinking about and love to share this with you guys and see if there's other ideas.",
                    "label": 0
                },
                {
                    "sent": "And hopefully people can think with us.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On these things.",
                    "label": 0
                },
                {
                    "sent": "So let me tell you a little bit in the last.",
                    "label": 0
                },
                {
                    "sent": "In the last couple of minutes I won't take much very much longer about.",
                    "label": 0
                },
                {
                    "sent": "And attempt an attempt from Mars to address this problem, and again building on these graphical model based approach.",
                    "label": 0
                },
                {
                    "sent": "You know how can we do things differently or extend Matchbox in order to perhaps solve one or two?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With these problems.",
                    "label": 0
                },
                {
                    "sent": "So we developed this model and we found out that actually the core of this model had been already been developed by some people in Portis I think at UC Irvine.",
                    "label": 0
                },
                {
                    "sent": "And his collaborators.",
                    "label": 0
                },
                {
                    "sent": "So essentially, the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "So instead of taking a geometric approach in saying you know a movie gets gets represented as a vector and a user get represented as a vector, maybe we should take more like a discrete approach of clustering based approach.",
                    "label": 0
                },
                {
                    "sent": "Ana closer in Bridge approach we could say, OK, we're going to cluster users and cluster movies, and then we're going to learn which user clusters like which movie clusters.",
                    "label": 0
                },
                {
                    "sent": "Now that's a bit 1 dimensional.",
                    "label": 0
                },
                {
                    "sent": "The world is not as simple.",
                    "label": 0
                },
                {
                    "sent": "You know, I'm not just an academic, I'm also dad.",
                    "label": 0
                },
                {
                    "sent": "I'm also.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I'm a UK citizen or resident, not a citizen, so the world is not as one dimensional, but Luckily this latent nursery allocation that I sort of mentioned for text suggests how one could get around that.",
                    "label": 0
                },
                {
                    "sent": "So let's say that every user gets represented by.",
                    "label": 0
                },
                {
                    "sent": "Let's say there's 100 user categories and 100 item categories, and now every user we could say gets represented by a subset of those hundred categories.",
                    "label": 0
                },
                {
                    "sent": "Let's say four.",
                    "label": 0
                },
                {
                    "sent": "OK everybody gets to choose four.",
                    "label": 0
                },
                {
                    "sent": "Profiles that he's part of with different sort of probabilities, so might be like 80%.",
                    "label": 0
                },
                {
                    "sent": "No, not not that much 40% and academic 30% of Dad, 10% living in the UK, etc.",
                    "label": 0
                },
                {
                    "sent": "So I get to choose four categories and similarly a movie or a news article gets to choose sort of four or five or whatever category out of 100 categories that is representative of its content.",
                    "label": 0
                },
                {
                    "sent": "And that's what this graphical model represents.",
                    "label": 0
                },
                {
                    "sent": "So every user.",
                    "label": 0
                },
                {
                    "sent": "Gets to choose sort of a small set of profiles that he's he's part of, and every movie.",
                    "label": 0
                },
                {
                    "sent": "In this case, let's say, gets to choose, you know, a few 4 out of 100 profiles that he's part of an now.",
                    "label": 0
                },
                {
                    "sent": "Every time we see a rating of a user for a particular movie, we assume that when the user made that rating, he was sort of in his being an academic mode or being a dad mode.",
                    "label": 0
                },
                {
                    "sent": "So he sort of chooses one of these categories specifically.",
                    "label": 0
                },
                {
                    "sent": "And you know, for let's say a movies or romantic comedy, and he sort of chose it because you know it was.",
                    "label": 0
                },
                {
                    "sent": "It was he was interested in the romantic aspect of it.",
                    "label": 0
                },
                {
                    "sent": "So the user and the movie sort of virtually decides which of the profiles that rating was generated for.",
                    "label": 0
                },
                {
                    "sent": "And then we look OK.",
                    "label": 0
                },
                {
                    "sent": "So let's say it's the academic who chose a comedy movie.",
                    "label": 0
                },
                {
                    "sent": "Then we go look into a particular matrix and we say, how much do academics like comedy movies?",
                    "label": 0
                },
                {
                    "sent": "Generate that rating.",
                    "label": 0
                },
                {
                    "sent": "OK, so we generate that rating.",
                    "label": 0
                },
                {
                    "sent": "So now what do we need to learn?",
                    "label": 0
                },
                {
                    "sent": "Well, for each user we need to sort of learn by just looking at ratings.",
                    "label": 0
                },
                {
                    "sent": "We need to learn how much you know what are the different profiles that people belong to.",
                    "label": 0
                },
                {
                    "sent": "They can't specify.",
                    "label": 0
                },
                {
                    "sent": "We learn it, or at least that's what this model does and we need to learn for each of the profile user item profile combinations.",
                    "label": 0
                },
                {
                    "sent": "You know what's the compatibility and when the guys import this, did this a couple of years back.",
                    "label": 0
                },
                {
                    "sent": "They sort of got it.",
                    "label": 0
                },
                {
                    "sent": "Reasonably good Netflix.",
                    "label": 0
                },
                {
                    "sent": "Accuracy and they did, that would say 50 users group.",
                    "label": 0
                },
                {
                    "sent": "So this was for movies 50 user and 500 item groups.",
                    "label": 0
                },
                {
                    "sent": "So by just using this data and this particular probabilistic model that's not geometry based but sort of clustering based but not clustering as in hard assignment but sort of soft assignments.",
                    "label": 0
                },
                {
                    "sent": "They got a reasonable score so we started thinking OK how can we?",
                    "label": 0
                },
                {
                    "sent": "How can we make this bet?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the first extension that we thought of was OK. Well, we've we've been quite successful with this sort of idea of Matchbox.",
                    "label": 0
                },
                {
                    "sent": "That says we're going to take the users metadata and map him onto this sort of $2 five dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "So why can we take the users metadata and map him onto a set of profiles?",
                    "label": 0
                },
                {
                    "sent": "And essentially, if you sort of look at, you know if you forget about these sort of two things at the end here.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's exactly the same model as before, so it's exactly users or sort of part of.",
                    "label": 0
                },
                {
                    "sent": "A small subset of profiles, but now what we do is we actually say we look at the users metadata and we sort of map that metadata onto this soft assignment of a clustering OK and I'll show this in friendlier version in a second.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So going back to sort of this idea of, you know using features as to make recommendations.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know we started thinking, well, you know we've now taken this idea of Matchbox, so we use features to generalize across users.",
                    "label": 0
                },
                {
                    "sent": "So if we know that a particular user lives in the UK, then he has the UK profile and a new user comes in.",
                    "label": 0
                },
                {
                    "sent": "He's never given rating data and we know he lives in the UK that he's going to get that UK data profile, but that's sort of taking a discriminative approach to learning.",
                    "label": 0
                },
                {
                    "sent": "But if you're sort of into machine learning, you might also know that you could sort of use generative approaches so we could.",
                    "label": 0
                },
                {
                    "sent": "Actually, instead of using this metadata to generate how people map onto their profiles, we could sort of take the profiles and generate their metadata, and essentially what makes that sort of interesting is that you can do unsupervised learning, so you don't actually have to have ratings to learn this mapping, so this might be a bit technical, but.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Explain, let me explain it in in sort of a caricature.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we could be doing is the following.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have users and we have movies and now we have metadata.",
                    "label": 0
                },
                {
                    "sent": "Then it seems a bit silly that a system like Matchbox actually now needs to see ratings in order for it to know what users are similar and what movies are similar and how to embed them in a way.",
                    "label": 0
                },
                {
                    "sent": "If you get the metadata it sounds very valuable that you should be able to design A probabilistic system.",
                    "label": 0
                },
                {
                    "sent": "That sort of clusters them.",
                    "label": 0
                },
                {
                    "sent": "Or soft clusters them, so that's what sort of this new graphical model that we're proposing is doing, so it takes the user metadata IT clusters users.",
                    "label": 1
                },
                {
                    "sent": "So for each user it assigns.",
                    "label": 0
                },
                {
                    "sent": "This is now showing a hard assignment, but imagine that it's a soft assignment, so it assigns the user to be maybe an intellectual, maybe a little bit adventures an emotional user, and similarly for movies.",
                    "label": 0
                },
                {
                    "sent": "So we first use the metadata to sort of soft cluster users and movies.",
                    "label": 0
                },
                {
                    "sent": "And then we take our rating data and now for each of the pairs.",
                    "label": 0
                },
                {
                    "sent": "Now we get to see a particular user in a particular movie and for each of the pairs we now sort of learn whether users who were adventurous generally like comedy movies or not.",
                    "label": 0
                },
                {
                    "sent": "So this is let's say the probability that the adventures user likes a comedy movie or not.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is we've sort of taken the metadata to sort of come up with this clustering.",
                    "label": 0
                },
                {
                    "sent": "Kind of like this geometric embedding and then we've taken the rating data and use it, sort of.",
                    "label": 0
                },
                {
                    "sent": "For a different purpose to to compute this sort of the matrix, we call it the matrix.",
                    "label": 1
                },
                {
                    "sent": "OK, so why is this useful?",
                    "label": 0
                },
                {
                    "sent": "Well, it addresses or partly addresses some of the problems that Matchbox had.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This positive training data only well one of the like I said Matchbox.",
                    "label": 1
                },
                {
                    "sent": "Initially it assumes that everybody is sort of sitting in the middle and it needs to see what people dislike in order to move people further apart.",
                    "label": 0
                },
                {
                    "sent": "But this system can actually very easily express that most people dislike most things because I can cluster users and then without seeing any of the rating data I can just say that the probability that you know each of these categories likes each of these categories is very low.",
                    "label": 0
                },
                {
                    "sent": "So this particular probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Initially expresses that most people dislike most things, which I think is a good assumption about the world.",
                    "label": 0
                },
                {
                    "sent": "So we can sort of set that type of structure up.",
                    "label": 0
                },
                {
                    "sent": "And now if we see a particular profile user with Abreu profile movie and we see that he likes it, we know that we could sort of update one particular entry in this matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can learn from positive training data only because we can set up our belief that most people dislike Moe.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "Again, fast indexing is also much easier because instead of having to compute the inner product between all millions of items and users, we can actually do something a lot more clever because it's an index.",
                    "label": 0
                },
                {
                    "sent": "It's a discrete structure, we can index it.",
                    "label": 0
                },
                {
                    "sent": "So the question we want to ask is, given the blue user, give me something that he likes.",
                    "label": 0
                },
                {
                    "sent": "Well, if we know that we're just looking for the blue user, we could sort of look at what are the sort of the entries in this row of sort of category movie categories that he likes.",
                    "label": 0
                },
                {
                    "sent": "And then we know well, this one is seems to be the most liked category.",
                    "label": 0
                },
                {
                    "sent": "So now we can go into this sort of subset of all the comedy movies and make a recommendation.",
                    "label": 0
                },
                {
                    "sent": "So it's by what we essentially done is.",
                    "label": 0
                },
                {
                    "sent": "We've sort of, you know, if there's 100 different categories here, we might only have to look at two or three rather than all possible movies in our catalog.",
                    "label": 0
                },
                {
                    "sent": "So again, we can do a lot faster recommendations using these discrete structures than these.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Metric structures.",
                    "label": 0
                },
                {
                    "sent": "Finally.",
                    "label": 0
                },
                {
                    "sent": "Is certainly not the answer to make doing good explanations, but it's sort of our first attempt is that when we make a recommendation for a user for a particular movie, we could sort of, for example, an intellectual user and we made him a comedy recommendation.",
                    "label": 0
                },
                {
                    "sent": "We could sort of look and say, OK, well, we made you that recommendation because people who have Masters degree generally like movies with Will Ferrell or something along those lines, and we can do that because each of these sort of sort of soft clusters is actually.",
                    "label": 0
                },
                {
                    "sent": "We can sort of describe sort of the center of that cluster.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So that's that's essentially what we've been doing.",
                    "label": 0
                },
                {
                    "sent": "That's sort of our way of thinking.",
                    "label": 0
                },
                {
                    "sent": "And well, one thing that we're very what we found is that we get similar sort of predictive performance as the one that we got for Matchbox, which has been the one that we really like because it works really well.",
                    "label": 0
                },
                {
                    "sent": "So we get very similar predictive performance.",
                    "label": 0
                },
                {
                    "sent": "It's not worse, let me put it that way, but we've also found is that it's definitely faster, so we could definitely write algorithms that make predictions a lot faster.",
                    "label": 0
                },
                {
                    "sent": "So it definitely has that plus.",
                    "label": 0
                },
                {
                    "sent": "This we're experimenting with these two things, and although it has, it has bits and pieces of this so we can learn from positive feedback only, but it's still not there yet.",
                    "label": 0
                },
                {
                    "sent": "We we think the explanatory power is also not where we want it to be.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not good enough to sort of surface in a user interface for Emporia, so these are certainly things that we need to sort of change our graphical models further and come up with better things to make recommendations.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is sort of what our group's been working on, and so I talked about Matchbox, which is, you know, you know our initial work on geometric representations for recommender systems.",
                    "label": 0
                },
                {
                    "sent": "I've talked about what we've been doing most recently, which is sort of mixed membership, so it is soft clustering based ideas of making recommendations, and then we have an intern which is trying yet other representations.",
                    "label": 0
                },
                {
                    "sent": "The as I said right now in Emporia we use the social network as a filter.",
                    "label": 0
                },
                {
                    "sent": "So we look at all the news articles that have been shared by your friends and then re rank them.",
                    "label": 0
                },
                {
                    "sent": "But we could also imagine using the social network to say group people.",
                    "label": 0
                },
                {
                    "sent": "So we also have an intern working on that.",
                    "label": 0
                },
                {
                    "sent": "Another interesting thing is like I said in earlier when we make a recommendation, we look at the top 10 or the top hundreds news articles and then we have some sort of simple heuristic to choose the six best.",
                    "label": 0
                },
                {
                    "sent": "According to some juristic out of that, top 100.",
                    "label": 0
                },
                {
                    "sent": "But you know, that's hand to heuristic, and we think we can actually learn that from data as well.",
                    "label": 0
                },
                {
                    "sent": "How to do that so you know more of a decision theoretic approach to recommendation is something else that we're that we're sort of working on.",
                    "label": 0
                },
                {
                    "sent": "One thing that you'll find when you use project import.",
                    "label": 0
                },
                {
                    "sent": "At least we find, is that it's sort of hyper personalized.",
                    "label": 0
                },
                {
                    "sent": "A little bit.",
                    "label": 0
                },
                {
                    "sent": "You'll see if you go there because the employee has been released on the Windows Phone app and Windows Phone was sort of initially tested and quite popular among Microsoft employees.",
                    "label": 0
                },
                {
                    "sent": "You sort of got quite a bias towards Microsoft News, which is I guess, not surprising, but you know that's not good for somebody who comes in for the first time.",
                    "label": 0
                },
                {
                    "sent": "So again, hyper personalization is something that we're again.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Working on so with that, I'm going to sort of call it a day.",
                    "label": 0
                },
                {
                    "sent": "So if you want to try it out, then definitely go there and send us feedback.",
                    "label": 0
                },
                {
                    "sent": "I try to reply to every email that we get.",
                    "label": 0
                },
                {
                    "sent": "If people like to see what people like or don't like about emporea.",
                    "label": 0
                },
                {
                    "sent": "Again, keep in mind it's three people, three people's work.",
                    "label": 0
                },
                {
                    "sent": "It's a research prototype.",
                    "label": 0
                },
                {
                    "sent": "It's not sort of Bing News yet, but we're working on it, so I'll take any questions.",
                    "label": 0
                },
                {
                    "sent": "So when you're talking about the new text Jess about the nearest neighbors, socialism more than that.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so one of the things that's a little bit subtle about, say, the Matchbox system, is that right now it's set up to when it makes a recommendation, it looks at the user vector and the item vector, and it computes the inner product.",
                    "label": 0
                },
                {
                    "sent": "Now one thing that we haven't been able to solve yet is.",
                    "label": 0
                },
                {
                    "sent": "If you nearest neighbors works.",
                    "label": 0
                },
                {
                    "sent": "If it's position close in position, but we're talking about close as far as inner product ghosts and having a fast way of finding all the vectors that have a large inner product is not easy because it's it's not related to position sort of at all is a bit is a bit strong, but you can have a vector that's very far away because it's extremely long still has a large inner product, so it's hard to sort of index a space based on inner product, and we've been we've done some work with.",
                    "label": 0
                },
                {
                    "sent": "Random random hashing and random projections.",
                    "label": 0
                },
                {
                    "sent": "And they work, but they also have disadvantages when you sort of want to ingest new things into the index.",
                    "label": 0
                },
                {
                    "sent": "So it's it's not all it's I think it's unsolved that I know we couldn't find anything in the literature and haven't been able to come up with a good answer ourselves either, so.",
                    "label": 0
                },
                {
                    "sent": "So how about this more traditional computational geometry approaches?",
                    "label": 0
                },
                {
                    "sent": "Like in each?",
                    "label": 0
                },
                {
                    "sent": "Umm, yeah.",
                    "label": 0
                },
                {
                    "sent": "So KD trees also one thing that we tried, but again a KD tree we found is quite works very well, but it doesn't work very well when you have like, you know, 10s or hundreds of new items coming in a second.",
                    "label": 0
                },
                {
                    "sent": "Like you know the amount of time it takes to sort of build and extend your KD trees just just.",
                    "label": 0
                },
                {
                    "sent": "Much for real time.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's what we've been talking about with the with the random hashing.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the random projections.",
                    "label": 0
                },
                {
                    "sent": "But again, so I'm not an expert on locality sensitive hashing, so my colleagues been working on that mostly.",
                    "label": 0
                },
                {
                    "sent": "My understanding is that it's a position based metric, right?",
                    "label": 0
                },
                {
                    "sent": "Most of the LSH algorithms are based on position, whereas here we're working with inner product and so some of these techniques don't don't apply.",
                    "label": 0
                },
                {
                    "sent": "When you're working with inner products just because of that problem that I was saying, you can have a user vector sort of sitting somewhere and then an item vector that's sort of at a really big angle.",
                    "label": 0
                },
                {
                    "sent": "But because it's so large it still has a large inner product.",
                    "label": 0
                },
                {
                    "sent": "So another another solution to that would be to create a different graphical model where the it's not the inner product that matters but the distance.",
                    "label": 0
                },
                {
                    "sent": "But you know that has other problems that we've struggled with, but I think I think a discussion of those issues are in the.",
                    "label": 0
                },
                {
                    "sent": "I think it was a KDD paper that we published on Matchbox 2 years ago.",
                    "label": 0
                },
                {
                    "sent": "So there's a discussion about those things in the paper there.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that you can have negative feedback and then you deal with that and re recommend.",
                    "label": 0
                },
                {
                    "sent": "How do we actually deal with that feedback in order to get to a point where you are not excluding something that you shouldn't have, or because if you don't give too much weight, then it's not useful and give too much weight to the features that you select from what had been excluded.",
                    "label": 0
                },
                {
                    "sent": "Then you end up leaving more stuff.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes no, absolutely so that's that's all encoded in the probabilistic model, so we don't have very much control over that.",
                    "label": 0
                },
                {
                    "sent": "Except for the fact that when a person gives negative feedback, it's not like well.",
                    "label": 0
                },
                {
                    "sent": "He definitely excludes the thing that he's given feedback on.",
                    "label": 0
                },
                {
                    "sent": "But then it's sort of like, again, we sort of make or the public model makes a soft assignment.",
                    "label": 0
                },
                {
                    "sent": "He moves the user away from items that are sort of similar to the item he gave feedback on.",
                    "label": 0
                },
                {
                    "sent": "So there's still some probability that he's going to see things in that neighborhood, but it's less and less likely an you know.",
                    "label": 0
                },
                {
                    "sent": "Worst case scenario, a user might have to give negative feedback, sort of twice.",
                    "label": 0
                },
                {
                    "sent": "But then you know the system is going to learn, adapt itself and again also the nice thing about these probabilistic approaches is that it's very adaptive.",
                    "label": 0
                },
                {
                    "sent": "So when a user gives goes in for the first time, one vote is going to have quite a big effect when you know like me, for me, when it's been trained for 6 seven months, one specific votes not going to have a massive effect anymore.",
                    "label": 0
                },
                {
                    "sent": "So that's that sort of balance of how strongly to weigh each of the votes, that's.",
                    "label": 0
                },
                {
                    "sent": "Taking care of by the probabilistic, but a probabilistic model so.",
                    "label": 0
                },
                {
                    "sent": "For him to be a, we have one more question.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "So so where are you this is that you are emphasizing that you are really the positive and negative numbers, so why dispatcher number 102 things?",
                    "label": 0
                },
                {
                    "sent": "There is Ono, so it doesn't have to be positive negative.",
                    "label": 0
                },
                {
                    "sent": "It needs to be at least more than one.",
                    "label": 0
                },
                {
                    "sent": "So positive, negative or zero to 10 or real number.",
                    "label": 0
                },
                {
                    "sent": "All of that works, but just like or nothing that doesn't work for these sort of type of approaches.",
                    "label": 0
                },
                {
                    "sent": "Cool.",
                    "label": 0
                }
            ]
        }
    }
}