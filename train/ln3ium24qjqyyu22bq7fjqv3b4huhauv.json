{
    "id": "ln3ium24qjqyyu22bq7fjqv3b4huhauv",
    "title": "Online Learning: Random Averages, Combinatorial Parameters, and Learnability",
    "info": {
        "author": [
            "Karthik Sridharan, Department of Computer Science, Cornell University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_sridharan_online/",
    "segmentation": [
        [
            "This joint work with the Alexander Rakhlin and ambushed."
        ],
        [
            "So there are two popular frameworks for learning.",
            "One is the statistical learning and the other is online learning."
        ],
        [
            "Framework so in the Statisical learning framework Nature pics a distribution D on the instant space.",
            "Cryptex and."
        ],
        [
            "Basically gives us a sample of size T run ID from this distribution."
        ],
        [
            "And the job of the learner is to pick a hypothesis from the function class script F."
        ],
        [
            "And what's the goal?",
            "The goal is to make sure that the expected loss for this for this function F hat is small.",
            "So specifically we want it to be small compared to the best function in the function class.",
            "And if we can drive this to 0, then we say that the problem is learned."
        ],
        [
            "On the other hand, the online learning problem is a multi round game where at."
        ],
        [
            "Strong, the learner picks a hypothesis FT and simultaneously the adversary picks a picture of instance XD from the script."
        ],
        [
            "Space and the goal goal of the learner here is to make sure that the average loss suffered for around 40 rounds is small compared to what we can do.",
            "To the best we can do in hindsight.",
            "So compared to the best we can do from the function class in hindsight, and if we can drive this to zero, we say that it's online learnable."
        ],
        [
            "So traditionally, in statistical learning theory we have complexity measures that we use for characterizing learnability, and for studying the rates that we can achieve."
        ],
        [
            "So examples of this are the classic VC dimension, the Rademacher complexity, the covering numbers and fat shattering dimension."
        ],
        [
            "And whenever we have a handle on these complexity measures, the empirical risk minimization algorithm works and we get we basically have an algorithm that gives us a learning algorithm that successful."
        ],
        [
            "However, in the online setting, things are slightly different, so up to now, most of the techniques are algorithmic, so we give an algorithm for the given problem we come up with an algorithm with regret bound, and that's the way we show that the problem is learnable, and this is very case by case.",
            "So the question that we are."
        ],
        [
            "Ask today in this talk is are there complexity measures for the function class script F that tells us that the problem is learnable in the online setting.",
            "So can we come up with a generic bag of tools for online?"
        ],
        [
            "Running so before we go to the question, I would like to 1st introduce the notion of value of the game.",
            "So before we do that, I'll change the game slightly.",
            "So instead of."
        ],
        [
            "So we now allow the learner to have randomized algorithms.",
            "So instead of outputing a particular hypothesis, he's going to output a probability distribution over."
        ],
        [
            "But this is."
        ],
        [
            "And when, when, at the end of the round, when we pay the loss, we Draw Something, we draw basically a hypothesis from this distribution, and we paid the loss for that hypothesis."
        ],
        [
            "So what's the value of the game?",
            "OK, actually I'm sorry.",
            "I also for notational convenience we're going to drop the loss there, and we're just going to say FT of XD it inculcates the loss."
        ],
        [
            "So the value of the game is the regret of the optimal learner against the optimal adversary.",
            "So in a similar scenario was studied by Abernathy Agarwal, Bartlett and Rocklin and."
        ],
        [
            "Similar lines for this randomized algorithm.",
            "We define the value of the game, so this is the regret.",
            "And when the learner goes first, he plays optimally, so he picks the best distribution that he can pick in for the distributions on F then."
        ],
        [
            "The player the adversary goes next and he does the best."
        ],
        [
            "That he can, and then we of course draw from the."
        ],
        [
            "The distribution and we can."
        ],
        [
            "And you so on.",
            "So this we define as the value of the game.",
            "So why is the value of the game in?"
        ],
        [
            "Written for us.",
            "Well, first by the definition of the value, we know that there exists a randomized online algorithm whose expected regret is bounded by the value of the game."
        ],
        [
            "And Moreover, we can say that no algorithm can guarantee regret better than the value of the game.",
            "And so basically, if you want to show that a problem is learnable, a problem is learnable.",
            "If and only if the value of the game is smaller of T."
        ],
        [
            "OK, so now what's the 1st?",
            "So the first barrier tool that we consider that from the classical statistical setting is the classical Rademacher complexity.",
            "So this is basically defined as you take supremum over tuples of size of lengthy from the input space and we see how well our function class aligns with noise.",
            "Here epsilons are random variables that take value plus or minus one equal probability.",
            "And so we make a slight adjustment to this so it works for the online setting so that."
        ],
        [
            "Adjustment as we go from classical Rademacher complexity to the sequential Rademacher complexity, and the"
        ],
        [
            "Thing that happens is instead of supremum or couples we gotta supremum over trees and the epsilons gives the give the path in the trees and OK.",
            "So what do we mean by it?"
        ],
        [
            "We hear a tree in the instance path, so that's the formal definition there, but pictorially, a tree is basically a binary tree, where each node in each node sets an instance from the instance based, and the epsilons gives.",
            "The gives the path either plus one or minus one."
        ],
        [
            "OK, so let's look at an example of how to calculate this Rademacher complexity.",
            "So here we take an example of epsilon as plus 1 -- 1 -- 1.",
            "So the path is right, left and left.",
            "So that's a path out there."
        ],
        [
            "And so for a given function F, how do we calculate this sum?",
            "Well, the first node in the path is X1 and the corresponding sign is epsilon.",
            "One is plus one, so it's plus F of X 1 -- F of X3 because the second node is X3 corresponding sinus minus and so on."
        ],
        [
            "So why is this quantity interesting?",
            "Why is the sequential order complexity interesting?",
            "It's because we basically show that the value of the game is bounded by twice the sequential Rademacher complexity, so this gives us a handle on the value of the game."
        ],
        [
            "A classical example that that grammar complexities are used for in statistical learning is to establish rates for linear classifiers.",
            "So for instance, when we have a classifier where the norm of the of the weight is bounded by one and the input space is the unit ball, and in this case in the classical setting we show order square root, T order, square root T rates and we can show a similar rate in the sequential radmacher complexity.",
            "And in fact."
        ],
        [
            "There are a set of properties like Lipschitz contraction, lemma and other properties that are used.",
            "There are tools used in the statistical case for analyzing difficult problems and for removing the loss and looking at the complexity of the function class and similar properties hold even for the sequential Rademacher complexity."
        ],
        [
            "So that's the first thing that we have in our bag of tools.",
            "The sequential marker complexity."
        ],
        [
            "So the next thing that we look at is covering numbers, so the next complexity measure that's that's popular in the statisical setting is the notion of covering numbers.",
            "And now we define covering numbers for online learning, and again here the tuples are replaced by binary trees by binary by xvalue trees.",
            "OK, so the definition is that for a given tree bold X set V of real value trees is a cover.",
            "If for any function and any path there exists an element in the cover such that on that path it's close to the function evaluation.",
            "So let me show this pictorially so it makes more."
        ],
        [
            "And so, here is the instance tree that we have, and these are the set of function evaluations.",
            "So just for simplicity I'm taking binary functions.",
            "So."
        ],
        [
            "So this is an example of a set of cover set V of cover.",
            "Why is this a cover so we can pick any function?",
            "So let's say we pick the."
        ],
        [
            "Function F3 here and any path.",
            "So let's say we pick that path out."
        ],
        [
            "Here and what we see is that in our in our set we the rigs."
        ],
        [
            "Is a tree, so for instance, that that one where on the same path it evaluates the same thing?",
            "Of course, for the cover we just need that it's close, it's Alpha close."
        ],
        [
            "So the forgiven given tree X with the covering number is defined as the size of the smallest cover, and we define the covering number for the problem 40 rounds as Supreme.",
            "Overall trees of discovering number for the for those trees.",
            "OK, so now I've defined a quantity.",
            "The covering number for online learning."
        ],
        [
            "House is useful.",
            "So do you."
        ],
        [
            "This we basically show a similar bound that holds in the statisical setting the Dudley Integral Bound.",
            "Only difference here is that the covering number used here is the tree based covering number and what we show is that the value of the game can be bounded by the Dudley integral complexity, so simple."
        ],
        [
            "Example of this is the function class turns out to be finite.",
            "Then we basically get that the Dudley Integral bound is looks like square root, T log size of the function class.",
            "So we get back the classic experts algorithm reserve."
        ],
        [
            "So that's one more complexity measure into a bag of tools."
        ],
        [
            "OK, next we proceed to combinatorial para meters, so little stone defined particular combinatorial para meters.",
            "We called the Little Stone Dimension and recently last year Ben David Pal and Shalev Schwartz showed that showed this nice result that the binary classification problem is learnable in the online setting.",
            "If and only if the little stone dimension is finite.",
            "So what is this little stone dimension?",
            "So the idea here is that if we have a binary function class for a given tree X, we say."
        ],
        [
            "The function class shatters the tree if and only if, for every path the function class can explain the path.",
            "So what we mean by that?",
            "So we have a tree here."
        ],
        [
            "So let's consider example of that path marked in red.",
            "So here we say that this function path is explained by the function class because there is a function, say F3, that explains the path.",
            "So F3 of X one is minus one F3 of X2 plus one and F3 of X5 is minus one.",
            "It follows the path.",
            "So extending this to."
        ],
        [
            "Real case OK, I before I move on to the real case, I would like to point out that this tree this object tree object is very different from the covering tree object."
        ],
        [
            "OK, so."
        ],
        [
            "So now we move to the real case.",
            "So similar to the Little Stone dimension, we define the fat shattering dimension, which is an unlocked the fat shattering dimension in the classical setting.",
            "So instead of going through this definition, let me just show it to you pictorially so we have."
        ],
        [
            "The tree here and we say that this."
        ],
        [
            "So tree bold X is mean.",
            "Is Alpha shattered by the function class?",
            "If there exists a tree of thresholds.",
            "So this is the tree of thresholds out there such that any path is explained by this by the function class so."
        ],
        [
            "What do we mean by that?",
            "So let's consider the example of this path.",
            "So here we say that the function F6 explains this path if it can be such that whenever we have epsilon one to be plus one, we make sure that the function evaluates to something bigger than the threshold by a margin Alpha."
        ],
        [
            "Two and whenever the so the for instance the next case epsilon two is minus one.",
            "So we make sure that the threshold is smaller than the function evaluates to something smaller than the threshold by a marginal."
        ],
        [
            "Power 2.",
            "And again, F6 is plus, so we make sure that the function evaluates to something bigger than the threshold by margin Alpha or two, and the threshold is given by the tree out there, the threshold tree.",
            "OK, so now we have."
        ],
        [
            "Another quantity again."
        ],
        [
            "The question is how do we use this quantity?",
            "Well."
        ],
        [
            "OK, sorry.",
            "So how do we use this parameter so to use this parimeter we first prove an analog of social, We show that covering numbers that we introduced before can be bounded using these quantities.",
            "So first we showed that whenever the function class takes on values in a finite set, then we can show that the covering number.",
            "I mean we can basically show an analog of for the classical social law, and we."
        ],
        [
            "Also show analogous result for for the real valued case, analogous to the statistical learning theory setting, we showed that the covering number at any scale can be bounded by using the fat shattering dimension of that scale using the equation."
        ],
        [
            "So there we have another complexity measure in our bag of tools.",
            "For the next thing that we look at."
        ],
        [
            "Is supervised learning, so of course the classical supervised learning problem."
        ],
        [
            "The binary classification problem so."
        ],
        [
            "Here in the statisical setting we have this classic result that of binary function class is learnable if and only if it has finite VC dimension.",
            "And."
        ],
        [
            "So recently, as I mentioned before, Ben David Pal and Shalev Schwartz showed that in the online learning setting, function class is learnable if and only if it has finite little stone dimension."
        ],
        [
            "So another result that's there in the OK.",
            "So the next problem that we look at is a more generalized version of this.",
            "The general supervised learning prob."
        ],
        [
            "Where basically the loss is the absolute loss.",
            "So I'd like to mention here that we can actually extend the result to more than the absolute last acquired loss and few more losses.",
            "But for now I'm going to stick to the absolute loss."
        ],
        [
            "So in the status tickle in the statistical learning framework, there is this loser that says that a function class is learnable in the supervised learning problem if and only for all scales.",
            "The fat shattering dimension at that scale is finite.",
            "And when I say fat shattering dimension here, I mean the classical fat shattering dimension that holds for the statistical learning setting so."
        ],
        [
            "Cause the that begs the question is that analogous result for the online supervised learning setting?",
            "And we show that there in fact is such a theorem, so."
        ],
        [
            "We show that for any function class, the function classes learn online learnable.",
            "For the supervised learning problem if and only if for all scales Alpha.",
            "The fat shattering dimension.",
            "And here it's the sequential world.",
            "The tree version of the fat shattering dimension is finite."
        ],
        [
            "And in fact, we show that using the value of the supervised learning game, we show that the value of the supervised learning game, the sequential Rademacher complexity, the Dudley Integral complexity, are all within log factors of each other."
        ],
        [
            "And in fact, for the specific case of Rademacher complexity, we can show something tighter.",
            "We can show that the Rademacher complexity upper bounds the value of the supervised learning game by two week, and upon the value of the supervised learning game by twice the Rademacher complexity, and we can lower bounded by the red marker complexity, so ramaker complexity in some sense is as tight as you can get in general."
        ],
        [
            "So we have and I'd like to mention here that the value of the Super is learning game in some sense.",
            "Acts by itself acts as a complexity for the function class F. Anne.",
            "OK, so."
        ],
        [
            "Also, following the work of Ben, David Pal and Shalev Schwartz, we show that we also provide a generic algorithm that works for supervised learning, setting the general supervised learning setting in using the fact that the tree version of the fat shattering dimension."
        ],
        [
            "OK, so based on the results that we showed so far, what we were able to do is we were able to provide nonconstructive bounds.",
            "First we were able to recover the bounds in the online convex optimization processing like Zinkevich bound and a few more pounds.",
            "And we were also able to analyze the complexity of a large class of linear function classes.",
            "Then we were able to show that multilayer neural networks are learnable and we were able to get bounds for them that mirror the known bounds in the status tickle setting.",
            "We were able to get some new bounds for decision trees online learning of decision trees and also generic margin bounds for online learning based on sequential marker complexity.",
            "We were able to show that the isotonic regression problem is learnable in the online setting, and we were able to show bounds for that.",
            "And we were actually able to show bounds for a larger class of problems of.",
            "Richard transformation we were also able to analyze online transitive learning easily and we were able to recover the.",
            "I'm sorry I'm missing a reference there.",
            "Recovered the result of.",
            "Just to be on Kia, tell of prediction of individual binary sequences and we were able to do much."
        ],
        [
            "More and for more details of this, please come by our poster.",
            "It's OT 17.",
            "OK."
        ],
        [
            "So in summary, we were able to come up with a bag of tools."
        ],
        [
            "And the first thing that we showed was that we can pull out."
        ],
        [
            "The bag is the sequential Rademacher complexity."
        ],
        [
            "Then we showed the Dudley Integral complexity based on the tree based covering numbers."
        ],
        [
            "And we were able to show, then we then we came up with this fat shattering version on the trees and we were able to show a generic social aluma for online learning."
        ],
        [
            "And we characterize the online learnability in the supervised setting."
        ],
        [
            "OK, so in OK so."
        ],
        [
            "Of the things that we would like to do further is one is right now.",
            "I gave results for complexity measures.",
            "We have the classical complexity measures for the statistical learning on one side and on the other side we just gave a bunch of complexity measures for online learning, which is completely adversarial.",
            "Of course there are problems in between where the adversary is not totally adversarial, and he's not.",
            "He's not as naive as being IID.",
            "So can we come up with complexity measures for these things?",
            "So this is something that we are currently working on."
        ],
        [
            "Another thing is we extended our results and techniques to problems beyond the simple notion of regret here to other performance measures.",
            "And we were able to get bounds for games like Blackwell, Approachability and Kaleb."
        ],
        [
            "Mission.",
            "The next question is, can we come up with?",
            "So we showed a generic algorithm for supervised learning.",
            "Can we come up with a generic algorithm whenever the these complexity measures are low?"
        ],
        [
            "And can we get fast rate results for?"
        ],
        [
            "Online learning based on these complexities and of course we are also looking for efficient algorithms algorithms for a few of these.",
            "Few of the applications."
        ],
        [
            "They mention.",
            "Thanks.",
            "Yeah, so we have time for a few questions.",
            "Please step up to one of the microphones.",
            "Very very interesting talk.",
            "I just have a short note the social aclima as you might be aware, was proved in 1968 already.",
            "Web uptick in German English, so I think it would be failed to mention their names as well, even though it's usually called the seller lemma in the community.",
            "You have sort of two different ways of generating examples, random and then worst case.",
            "And do you sometimes the worst case?",
            "Distribution OK, The the online algorithms.",
            "Are actually good for solving random problems.",
            "Have you have you ever?",
            "Seen a case where you are bag of tricks for online algorithms actually beat the best.",
            "Pounds you can prove for gets close to the best bounce you can prove when the examples are generated at random.",
            "Sure, so there are cases where you can show that there is that the online complexity is the same as the statistical complexity.",
            "For instance, the large class of the linear function classes that we're interested in.",
            "Most of the ones that we use in practice are actually the same Rademacher complexity for the online setting under status tickle setting are exactly the same.",
            "There are some really weird Banach spaces where they are not same, but in practice there same for linear classes.",
            "But once we go to like nonlinear classes with with.",
            "Discontinuity's then they start making a difference.",
            "Then online learning we need some notion of margin.",
            "So just like like a lot of the lower bounds in online in the binary.",
            "And sorry in the statisical cases are based on these threshold functions.",
            "The online case the building block is these Lipschitz functions of the margin functions that look like this.",
            "So once you have the margin you're good in the online setting.",
            "But there are differences.",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This joint work with the Alexander Rakhlin and ambushed.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are two popular frameworks for learning.",
                    "label": 0
                },
                {
                    "sent": "One is the statistical learning and the other is online learning.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Framework so in the Statisical learning framework Nature pics a distribution D on the instant space.",
                    "label": 0
                },
                {
                    "sent": "Cryptex and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically gives us a sample of size T run ID from this distribution.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the job of the learner is to pick a hypothesis from the function class script F.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what's the goal?",
                    "label": 0
                },
                {
                    "sent": "The goal is to make sure that the expected loss for this for this function F hat is small.",
                    "label": 0
                },
                {
                    "sent": "So specifically we want it to be small compared to the best function in the function class.",
                    "label": 0
                },
                {
                    "sent": "And if we can drive this to 0, then we say that the problem is learned.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, the online learning problem is a multi round game where at.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strong, the learner picks a hypothesis FT and simultaneously the adversary picks a picture of instance XD from the script.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space and the goal goal of the learner here is to make sure that the average loss suffered for around 40 rounds is small compared to what we can do.",
                    "label": 0
                },
                {
                    "sent": "To the best we can do in hindsight.",
                    "label": 0
                },
                {
                    "sent": "So compared to the best we can do from the function class in hindsight, and if we can drive this to zero, we say that it's online learnable.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So traditionally, in statistical learning theory we have complexity measures that we use for characterizing learnability, and for studying the rates that we can achieve.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So examples of this are the classic VC dimension, the Rademacher complexity, the covering numbers and fat shattering dimension.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And whenever we have a handle on these complexity measures, the empirical risk minimization algorithm works and we get we basically have an algorithm that gives us a learning algorithm that successful.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, in the online setting, things are slightly different, so up to now, most of the techniques are algorithmic, so we give an algorithm for the given problem we come up with an algorithm with regret bound, and that's the way we show that the problem is learnable, and this is very case by case.",
                    "label": 0
                },
                {
                    "sent": "So the question that we are.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ask today in this talk is are there complexity measures for the function class script F that tells us that the problem is learnable in the online setting.",
                    "label": 0
                },
                {
                    "sent": "So can we come up with a generic bag of tools for online?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Running so before we go to the question, I would like to 1st introduce the notion of value of the game.",
                    "label": 1
                },
                {
                    "sent": "So before we do that, I'll change the game slightly.",
                    "label": 0
                },
                {
                    "sent": "So instead of.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we now allow the learner to have randomized algorithms.",
                    "label": 0
                },
                {
                    "sent": "So instead of outputing a particular hypothesis, he's going to output a probability distribution over.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when, when, at the end of the round, when we pay the loss, we Draw Something, we draw basically a hypothesis from this distribution, and we paid the loss for that hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the value of the game?",
                    "label": 1
                },
                {
                    "sent": "OK, actually I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "I also for notational convenience we're going to drop the loss there, and we're just going to say FT of XD it inculcates the loss.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the value of the game is the regret of the optimal learner against the optimal adversary.",
                    "label": 0
                },
                {
                    "sent": "So in a similar scenario was studied by Abernathy Agarwal, Bartlett and Rocklin and.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similar lines for this randomized algorithm.",
                    "label": 0
                },
                {
                    "sent": "We define the value of the game, so this is the regret.",
                    "label": 1
                },
                {
                    "sent": "And when the learner goes first, he plays optimally, so he picks the best distribution that he can pick in for the distributions on F then.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The player the adversary goes next and he does the best.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That he can, and then we of course draw from the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The distribution and we can.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you so on.",
                    "label": 0
                },
                {
                    "sent": "So this we define as the value of the game.",
                    "label": 0
                },
                {
                    "sent": "So why is the value of the game in?",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Written for us.",
                    "label": 0
                },
                {
                    "sent": "Well, first by the definition of the value, we know that there exists a randomized online algorithm whose expected regret is bounded by the value of the game.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And Moreover, we can say that no algorithm can guarantee regret better than the value of the game.",
                    "label": 1
                },
                {
                    "sent": "And so basically, if you want to show that a problem is learnable, a problem is learnable.",
                    "label": 0
                },
                {
                    "sent": "If and only if the value of the game is smaller of T.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now what's the 1st?",
                    "label": 0
                },
                {
                    "sent": "So the first barrier tool that we consider that from the classical statistical setting is the classical Rademacher complexity.",
                    "label": 1
                },
                {
                    "sent": "So this is basically defined as you take supremum over tuples of size of lengthy from the input space and we see how well our function class aligns with noise.",
                    "label": 0
                },
                {
                    "sent": "Here epsilons are random variables that take value plus or minus one equal probability.",
                    "label": 0
                },
                {
                    "sent": "And so we make a slight adjustment to this so it works for the online setting so that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adjustment as we go from classical Rademacher complexity to the sequential Rademacher complexity, and the",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing that happens is instead of supremum or couples we gotta supremum over trees and the epsilons gives the give the path in the trees and OK.",
                    "label": 0
                },
                {
                    "sent": "So what do we mean by it?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We hear a tree in the instance path, so that's the formal definition there, but pictorially, a tree is basically a binary tree, where each node in each node sets an instance from the instance based, and the epsilons gives.",
                    "label": 0
                },
                {
                    "sent": "The gives the path either plus one or minus one.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at an example of how to calculate this Rademacher complexity.",
                    "label": 1
                },
                {
                    "sent": "So here we take an example of epsilon as plus 1 -- 1 -- 1.",
                    "label": 1
                },
                {
                    "sent": "So the path is right, left and left.",
                    "label": 0
                },
                {
                    "sent": "So that's a path out there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so for a given function F, how do we calculate this sum?",
                    "label": 0
                },
                {
                    "sent": "Well, the first node in the path is X1 and the corresponding sign is epsilon.",
                    "label": 0
                },
                {
                    "sent": "One is plus one, so it's plus F of X 1 -- F of X3 because the second node is X3 corresponding sinus minus and so on.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is this quantity interesting?",
                    "label": 0
                },
                {
                    "sent": "Why is the sequential order complexity interesting?",
                    "label": 0
                },
                {
                    "sent": "It's because we basically show that the value of the game is bounded by twice the sequential Rademacher complexity, so this gives us a handle on the value of the game.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A classical example that that grammar complexities are used for in statistical learning is to establish rates for linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "So for instance, when we have a classifier where the norm of the of the weight is bounded by one and the input space is the unit ball, and in this case in the classical setting we show order square root, T order, square root T rates and we can show a similar rate in the sequential radmacher complexity.",
                    "label": 0
                },
                {
                    "sent": "And in fact.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are a set of properties like Lipschitz contraction, lemma and other properties that are used.",
                    "label": 0
                },
                {
                    "sent": "There are tools used in the statistical case for analyzing difficult problems and for removing the loss and looking at the complexity of the function class and similar properties hold even for the sequential Rademacher complexity.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the first thing that we have in our bag of tools.",
                    "label": 0
                },
                {
                    "sent": "The sequential marker complexity.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next thing that we look at is covering numbers, so the next complexity measure that's that's popular in the statisical setting is the notion of covering numbers.",
                    "label": 0
                },
                {
                    "sent": "And now we define covering numbers for online learning, and again here the tuples are replaced by binary trees by binary by xvalue trees.",
                    "label": 0
                },
                {
                    "sent": "OK, so the definition is that for a given tree bold X set V of real value trees is a cover.",
                    "label": 1
                },
                {
                    "sent": "If for any function and any path there exists an element in the cover such that on that path it's close to the function evaluation.",
                    "label": 0
                },
                {
                    "sent": "So let me show this pictorially so it makes more.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, here is the instance tree that we have, and these are the set of function evaluations.",
                    "label": 0
                },
                {
                    "sent": "So just for simplicity I'm taking binary functions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is an example of a set of cover set V of cover.",
                    "label": 1
                },
                {
                    "sent": "Why is this a cover so we can pick any function?",
                    "label": 0
                },
                {
                    "sent": "So let's say we pick the.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function F3 here and any path.",
                    "label": 0
                },
                {
                    "sent": "So let's say we pick that path out.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here and what we see is that in our in our set we the rigs.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a tree, so for instance, that that one where on the same path it evaluates the same thing?",
                    "label": 0
                },
                {
                    "sent": "Of course, for the cover we just need that it's close, it's Alpha close.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the forgiven given tree X with the covering number is defined as the size of the smallest cover, and we define the covering number for the problem 40 rounds as Supreme.",
                    "label": 0
                },
                {
                    "sent": "Overall trees of discovering number for the for those trees.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I've defined a quantity.",
                    "label": 0
                },
                {
                    "sent": "The covering number for online learning.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "House is useful.",
                    "label": 0
                },
                {
                    "sent": "So do you.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This we basically show a similar bound that holds in the statisical setting the Dudley Integral Bound.",
                    "label": 0
                },
                {
                    "sent": "Only difference here is that the covering number used here is the tree based covering number and what we show is that the value of the game can be bounded by the Dudley integral complexity, so simple.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example of this is the function class turns out to be finite.",
                    "label": 0
                },
                {
                    "sent": "Then we basically get that the Dudley Integral bound is looks like square root, T log size of the function class.",
                    "label": 1
                },
                {
                    "sent": "So we get back the classic experts algorithm reserve.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's one more complexity measure into a bag of tools.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, next we proceed to combinatorial para meters, so little stone defined particular combinatorial para meters.",
                    "label": 0
                },
                {
                    "sent": "We called the Little Stone Dimension and recently last year Ben David Pal and Shalev Schwartz showed that showed this nice result that the binary classification problem is learnable in the online setting.",
                    "label": 0
                },
                {
                    "sent": "If and only if the little stone dimension is finite.",
                    "label": 0
                },
                {
                    "sent": "So what is this little stone dimension?",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that if we have a binary function class for a given tree X, we say.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The function class shatters the tree if and only if, for every path the function class can explain the path.",
                    "label": 0
                },
                {
                    "sent": "So what we mean by that?",
                    "label": 0
                },
                {
                    "sent": "So we have a tree here.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's consider example of that path marked in red.",
                    "label": 0
                },
                {
                    "sent": "So here we say that this function path is explained by the function class because there is a function, say F3, that explains the path.",
                    "label": 0
                },
                {
                    "sent": "So F3 of X one is minus one F3 of X2 plus one and F3 of X5 is minus one.",
                    "label": 0
                },
                {
                    "sent": "It follows the path.",
                    "label": 0
                },
                {
                    "sent": "So extending this to.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Real case OK, I before I move on to the real case, I would like to point out that this tree this object tree object is very different from the covering tree object.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we move to the real case.",
                    "label": 0
                },
                {
                    "sent": "So similar to the Little Stone dimension, we define the fat shattering dimension, which is an unlocked the fat shattering dimension in the classical setting.",
                    "label": 0
                },
                {
                    "sent": "So instead of going through this definition, let me just show it to you pictorially so we have.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The tree here and we say that this.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So tree bold X is mean.",
                    "label": 0
                },
                {
                    "sent": "Is Alpha shattered by the function class?",
                    "label": 0
                },
                {
                    "sent": "If there exists a tree of thresholds.",
                    "label": 1
                },
                {
                    "sent": "So this is the tree of thresholds out there such that any path is explained by this by the function class so.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do we mean by that?",
                    "label": 0
                },
                {
                    "sent": "So let's consider the example of this path.",
                    "label": 0
                },
                {
                    "sent": "So here we say that the function F6 explains this path if it can be such that whenever we have epsilon one to be plus one, we make sure that the function evaluates to something bigger than the threshold by a margin Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two and whenever the so the for instance the next case epsilon two is minus one.",
                    "label": 0
                },
                {
                    "sent": "So we make sure that the threshold is smaller than the function evaluates to something smaller than the threshold by a marginal.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Power 2.",
                    "label": 0
                },
                {
                    "sent": "And again, F6 is plus, so we make sure that the function evaluates to something bigger than the threshold by margin Alpha or two, and the threshold is given by the tree out there, the threshold tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another quantity again.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The question is how do we use this quantity?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, sorry.",
                    "label": 0
                },
                {
                    "sent": "So how do we use this parameter so to use this parimeter we first prove an analog of social, We show that covering numbers that we introduced before can be bounded using these quantities.",
                    "label": 0
                },
                {
                    "sent": "So first we showed that whenever the function class takes on values in a finite set, then we can show that the covering number.",
                    "label": 0
                },
                {
                    "sent": "I mean we can basically show an analog of for the classical social law, and we.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also show analogous result for for the real valued case, analogous to the statistical learning theory setting, we showed that the covering number at any scale can be bounded by using the fat shattering dimension of that scale using the equation.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there we have another complexity measure in our bag of tools.",
                    "label": 0
                },
                {
                    "sent": "For the next thing that we look at.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is supervised learning, so of course the classical supervised learning problem.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The binary classification problem so.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here in the statisical setting we have this classic result that of binary function class is learnable if and only if it has finite VC dimension.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So recently, as I mentioned before, Ben David Pal and Shalev Schwartz showed that in the online learning setting, function class is learnable if and only if it has finite little stone dimension.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another result that's there in the OK.",
                    "label": 0
                },
                {
                    "sent": "So the next problem that we look at is a more generalized version of this.",
                    "label": 0
                },
                {
                    "sent": "The general supervised learning prob.",
                    "label": 1
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where basically the loss is the absolute loss.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to mention here that we can actually extend the result to more than the absolute last acquired loss and few more losses.",
                    "label": 0
                },
                {
                    "sent": "But for now I'm going to stick to the absolute loss.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the status tickle in the statistical learning framework, there is this loser that says that a function class is learnable in the supervised learning problem if and only for all scales.",
                    "label": 1
                },
                {
                    "sent": "The fat shattering dimension at that scale is finite.",
                    "label": 1
                },
                {
                    "sent": "And when I say fat shattering dimension here, I mean the classical fat shattering dimension that holds for the statistical learning setting so.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cause the that begs the question is that analogous result for the online supervised learning setting?",
                    "label": 0
                },
                {
                    "sent": "And we show that there in fact is such a theorem, so.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We show that for any function class, the function classes learn online learnable.",
                    "label": 1
                },
                {
                    "sent": "For the supervised learning problem if and only if for all scales Alpha.",
                    "label": 1
                },
                {
                    "sent": "The fat shattering dimension.",
                    "label": 0
                },
                {
                    "sent": "And here it's the sequential world.",
                    "label": 0
                },
                {
                    "sent": "The tree version of the fat shattering dimension is finite.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, we show that using the value of the supervised learning game, we show that the value of the supervised learning game, the sequential Rademacher complexity, the Dudley Integral complexity, are all within log factors of each other.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, for the specific case of Rademacher complexity, we can show something tighter.",
                    "label": 0
                },
                {
                    "sent": "We can show that the Rademacher complexity upper bounds the value of the supervised learning game by two week, and upon the value of the supervised learning game by twice the Rademacher complexity, and we can lower bounded by the red marker complexity, so ramaker complexity in some sense is as tight as you can get in general.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have and I'd like to mention here that the value of the Super is learning game in some sense.",
                    "label": 0
                },
                {
                    "sent": "Acts by itself acts as a complexity for the function class F. Anne.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, following the work of Ben, David Pal and Shalev Schwartz, we show that we also provide a generic algorithm that works for supervised learning, setting the general supervised learning setting in using the fact that the tree version of the fat shattering dimension.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so based on the results that we showed so far, what we were able to do is we were able to provide nonconstructive bounds.",
                    "label": 0
                },
                {
                    "sent": "First we were able to recover the bounds in the online convex optimization processing like Zinkevich bound and a few more pounds.",
                    "label": 1
                },
                {
                    "sent": "And we were also able to analyze the complexity of a large class of linear function classes.",
                    "label": 1
                },
                {
                    "sent": "Then we were able to show that multilayer neural networks are learnable and we were able to get bounds for them that mirror the known bounds in the status tickle setting.",
                    "label": 0
                },
                {
                    "sent": "We were able to get some new bounds for decision trees online learning of decision trees and also generic margin bounds for online learning based on sequential marker complexity.",
                    "label": 1
                },
                {
                    "sent": "We were able to show that the isotonic regression problem is learnable in the online setting, and we were able to show bounds for that.",
                    "label": 0
                },
                {
                    "sent": "And we were actually able to show bounds for a larger class of problems of.",
                    "label": 0
                },
                {
                    "sent": "Richard transformation we were also able to analyze online transitive learning easily and we were able to recover the.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I'm missing a reference there.",
                    "label": 0
                },
                {
                    "sent": "Recovered the result of.",
                    "label": 1
                },
                {
                    "sent": "Just to be on Kia, tell of prediction of individual binary sequences and we were able to do much.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More and for more details of this, please come by our poster.",
                    "label": 0
                },
                {
                    "sent": "It's OT 17.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in summary, we were able to come up with a bag of tools.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the first thing that we showed was that we can pull out.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The bag is the sequential Rademacher complexity.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we showed the Dudley Integral complexity based on the tree based covering numbers.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we were able to show, then we then we came up with this fat shattering version on the trees and we were able to show a generic social aluma for online learning.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we characterize the online learnability in the supervised setting.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in OK so.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the things that we would like to do further is one is right now.",
                    "label": 0
                },
                {
                    "sent": "I gave results for complexity measures.",
                    "label": 0
                },
                {
                    "sent": "We have the classical complexity measures for the statistical learning on one side and on the other side we just gave a bunch of complexity measures for online learning, which is completely adversarial.",
                    "label": 0
                },
                {
                    "sent": "Of course there are problems in between where the adversary is not totally adversarial, and he's not.",
                    "label": 0
                },
                {
                    "sent": "He's not as naive as being IID.",
                    "label": 0
                },
                {
                    "sent": "So can we come up with complexity measures for these things?",
                    "label": 0
                },
                {
                    "sent": "So this is something that we are currently working on.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing is we extended our results and techniques to problems beyond the simple notion of regret here to other performance measures.",
                    "label": 0
                },
                {
                    "sent": "And we were able to get bounds for games like Blackwell, Approachability and Kaleb.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "The next question is, can we come up with?",
                    "label": 0
                },
                {
                    "sent": "So we showed a generic algorithm for supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Can we come up with a generic algorithm whenever the these complexity measures are low?",
                    "label": 1
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And can we get fast rate results for?",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Online learning based on these complexities and of course we are also looking for efficient algorithms algorithms for a few of these.",
                    "label": 0
                },
                {
                    "sent": "Few of the applications.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They mention.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we have time for a few questions.",
                    "label": 0
                },
                {
                    "sent": "Please step up to one of the microphones.",
                    "label": 0
                },
                {
                    "sent": "Very very interesting talk.",
                    "label": 0
                },
                {
                    "sent": "I just have a short note the social aclima as you might be aware, was proved in 1968 already.",
                    "label": 0
                },
                {
                    "sent": "Web uptick in German English, so I think it would be failed to mention their names as well, even though it's usually called the seller lemma in the community.",
                    "label": 0
                },
                {
                    "sent": "You have sort of two different ways of generating examples, random and then worst case.",
                    "label": 0
                },
                {
                    "sent": "And do you sometimes the worst case?",
                    "label": 0
                },
                {
                    "sent": "Distribution OK, The the online algorithms.",
                    "label": 0
                },
                {
                    "sent": "Are actually good for solving random problems.",
                    "label": 0
                },
                {
                    "sent": "Have you have you ever?",
                    "label": 0
                },
                {
                    "sent": "Seen a case where you are bag of tricks for online algorithms actually beat the best.",
                    "label": 0
                },
                {
                    "sent": "Pounds you can prove for gets close to the best bounce you can prove when the examples are generated at random.",
                    "label": 0
                },
                {
                    "sent": "Sure, so there are cases where you can show that there is that the online complexity is the same as the statistical complexity.",
                    "label": 0
                },
                {
                    "sent": "For instance, the large class of the linear function classes that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "Most of the ones that we use in practice are actually the same Rademacher complexity for the online setting under status tickle setting are exactly the same.",
                    "label": 0
                },
                {
                    "sent": "There are some really weird Banach spaces where they are not same, but in practice there same for linear classes.",
                    "label": 0
                },
                {
                    "sent": "But once we go to like nonlinear classes with with.",
                    "label": 0
                },
                {
                    "sent": "Discontinuity's then they start making a difference.",
                    "label": 0
                },
                {
                    "sent": "Then online learning we need some notion of margin.",
                    "label": 0
                },
                {
                    "sent": "So just like like a lot of the lower bounds in online in the binary.",
                    "label": 0
                },
                {
                    "sent": "And sorry in the statisical cases are based on these threshold functions.",
                    "label": 0
                },
                {
                    "sent": "The online case the building block is these Lipschitz functions of the margin functions that look like this.",
                    "label": 0
                },
                {
                    "sent": "So once you have the margin you're good in the online setting.",
                    "label": 0
                },
                {
                    "sent": "But there are differences.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}