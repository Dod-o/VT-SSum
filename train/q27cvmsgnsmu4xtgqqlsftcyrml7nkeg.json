{
    "id": "q27cvmsgnsmu4xtgqqlsftcyrml7nkeg",
    "title": "Piecewise-Stationary Bandit Problems with Side Information",
    "info": {
        "author": [
            "Jia Yuan Yu, Electrical and Computer Engineering Department, McGill University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_yu_psbpsi/",
    "segmentation": [
        [
            "So good morning.",
            "This is a joint work with Shimon or.",
            "And we're going to talk about multi armed bandits.",
            "A particular kind of multi arm bandit where OK first of all there are two very well studied versions of the multi arm bandit problem."
        ],
        [
            "The first one assumes that the rewards are stochastic, but follow a fixed distribution."
        ],
        [
            "The second version is 1, where it is assumed that the rewards are controlled by an opponent.",
            "So opponent writes down ahead of time what are, what's the sequence of rewards and you, will you discover them one by one overtime.",
            "And our model is basically trying to capture something between these two extremes so.",
            "In our model."
        ],
        [
            "It's a multi arm bandit where you have an arms and each arm will be stochastic with a fixed distribution over long times, but at unknown time instance the distribution is going to change and the new distribution is also a normal.",
            "No, we introduce also an additional concept of querying arms that you haven't actually pulled, so for this you can think of going to a casino and you have a lot of slot machines and there are people playing at each of the machines, so although you don't pull the arms on each of the machines, you can ask another gambler was pulling those arms to tell you what he got.",
            "Alright, but we also assume that you cannot ask every single gambler at every single slot machine what reward he will obtain.",
            "No no."
        ],
        [
            "Example of this kind of scenario is when you're trying to sell items.",
            "For example insurance policies.",
            "So what you could do is you should you can call people and ask them whether they want to buy a certain policy and at a certain price.",
            "And your reward will be the number of sales you make.",
            "And the queries in this situation would be calling people to make surveys without actually selling them the policies.",
            "Alright.",
            "So.",
            "The sequence of actions in this."
        ],
        [
            "Decision problem is as follows.",
            "1st at every time instant you choose an expert that you're going to follow.",
            "Second, you're going to query a subset of the remaining remaining arms or experts.",
            "Afterwards you receive the reward from the ARM or the expert and you observe.",
            "The reward from the.",
            "I'm glad you queried.",
            "And finally we introduce a cost for querying so that you don't get the benefit of querying every single expert.",
            "Now the goal is just like the stochastic multi Arm Bandit.",
            "You want to do as well as if you had known in advance.",
            "The distribution of the rewards.",
            "And basically in our model we also assume that.",
            "The baseline.",
            "You know what times?",
            "The changes occur and whether the new distributions.",
            "So we're."
        ],
        [
            "Going to denote the rewards by sequence BT each BT will be a vector composed of the rewards of the N experts.",
            "And.",
            "We'll denote the change points by news by news, and we denote by better the average reward at time T Betty.",
            "So.",
            "This is the average reward and for example here would be the change point."
        ],
        [
            "OK. Alright, so more notations will denote by the 80 the actions that you're taking.",
            "We'll denote by S the set of arms that you are querying.",
            "And we introduce a function C. To model the cost of querying.",
            "S. A subset S of arms."
        ],
        [
            "No.",
            "The basis of comparison that we use is.",
            "This term here, which is the sum of the best average reward at every time instant.",
            "And the actual performance of.",
            "The agent will be this expectation here.",
            "Of the rewards.",
            "On the actions.",
            "Now if you look at the combined cost of the queries and the regret.",
            "You can combine them by dividing over the whole time.",
            "Divide the whole regret overtime and you add the cost, but the cost of querying per time step.",
            "So obviously the objective is to minimize this this term."
        ],
        [
            "OK, so here are the known results for the two well known examples of multi arm bandits.",
            "For the stochastic case, using the same concept of regret.",
            "I has been shown that you can obtain regret that's logarithmic in time.",
            "An linear in the number of arm arms.",
            "Whereas for the stochastic bandit problem for the adversarial bandit problem.",
            "They adopt a different concept of regret where.",
            "The baseline will be.",
            "The best total reward you can obtain using a single.",
            "Action.",
            "So in this case you have the Max maximum taken outside of the sun.",
            "And for this case.",
            "Um, whoops.",
            "You can show that the regret will have an additional term that an additional factor that's.",
            "The square root of T. OK, so the model that we."
        ],
        [
            "Looking at the piecewise stationary band, it has been considered by.",
            "Two in two papers.",
            "The first one they provide a solution where they are only interested in detecting changes in the best current arm.",
            "So obviously they cannot detect when an additional arm becomes better than the one that you're currently convinced is the best.",
            "And in the other work by Gharibian Moline, they assume that you know the number of change points that are to be expected up to time T, and this will be denoted by K. And they show that there are various ways of obtaining.",
            "I regret that.",
            "Increases as the square root of T. And the square root of the number of changes and the square root of.",
            "The number of arms.",
            "Page not just get it slightly worse regret.",
            "OK.",
            "So by taking the K out of the square root, you can.",
            "OK. Um, this comes from.",
            "You can tune better if you don't.",
            "You can still get slightly better.",
            "Right so.",
            "I'm not sure about this fact, but probably you can do it OK.",
            "Even think that you can do it with Jane side without knowing it using this book.",
            "Play.",
            "Alright, and in their work they also showed that there is a lower bound.",
            "For.",
            "For the regret that increases as square root of T and I think I should be careful in mentioning that in these bounds there is a term in the constant term will depend on the difference between the best and the second best reward.",
            "OK. Actually I checked and he had.",
            "A constant term.",
            "So we can discuss offline."
        ],
        [
            "Alright, so in in our work we're going to show is that.",
            "It's there's actually a very simple solution if you're allowed to query a few arms at every time step, and in that solution you can obtain a regret that's log arhythmic in T and linear in the number of arms and the number of changes.",
            "And also it doesn't require any knowledge on.",
            "The number of changes.",
            "So."
        ],
        [
            "So the approach is very simple.",
            "We're going to."
        ],
        [
            "To detect the changes and once we detect the change will reset whatever algorithm that we are using to solve a stochastic multi armed bandit problem.",
            "Now, there are various ways for detecting changes.",
            "Some of them will use.",
            "Techniques norms are known as the change detection schemes, so some of them will compute likelihood ratios at every time step.",
            "And one, whenever this likelihood ratio goes above a threshold, you raise an alarm and detect the change.",
            "But what we?"
        ],
        [
            "Going to do is much simpler.",
            "We're going to compute empirical averages based on the queries that we obtain.",
            "And we're going to compare these empirical averages over different windows.",
            "Right?"
        ],
        [
            "So the details of the algorithm are as follows.",
            "So you break first.",
            "We break the time horizon into.",
            "Some length intervals of fixed length.",
            "And over these these intervals we use the queries to create a model of the average reward.",
            "And.",
            "The algorithm will work as follows, so at every time time instant.",
            "You will update for each arm a score.",
            "This score will be composed of the empirical average over all times of of the rewards.",
            "Plus a term that's inversely proportional to the number of times you have sampled the current arm.",
            "And you will choose.",
            "The arm that has the best.",
            "Search score and at the same time.",
            "You will compute.",
            "The empirical averages over the windows, and once you detect.",
            "A difference in the empirical averages that exceeds a certain threshold.",
            "You're going to reset the.",
            "This algorithm, the sub algorithm.",
            "No.",
            "So now."
        ],
        [
            "So for the guarantee of this algorithm.",
            "So suppose that we know that whenever the change occurs.",
            "The change will occur by an amount that's at least two epsilon, where the epsilon is a term that's given to you by an Oracle.",
            "Now with this epsilon you can.",
            "We did we determine an interval length of.",
            "The interval length tell which will be inversely proportional to the square of epsilon.",
            "And with this parameter in the previous algorithm, we can show that the regret will be.",
            "A sum of basically these three terms.",
            "When one of these terms, the second one.",
            "Will depend on the underlying algorithm that you're using to solve.",
            "The multi stochastic Multi armed bandit problem.",
            "In our case we use the UCB algorithm.",
            "And there is another term that that principally captures the delay in detecting a change in the mean.",
            "So.",
            "In comparison with the work by.",
            "Gary, Vee, and moulinas.",
            "They had regret bound that has a term that's polynomial in T. Was we only have a sqrt T?",
            "We only have a log arhythmic term.",
            "And in both bounds hours and the one by.",
            "By Gary V and bowling.",
            "We both have a term in the denominator, so the Delta here.",
            "That depends on the difference between the best and the second best arm.",
            "Delta squared in both cases.",
            "So in their paper they had a term.",
            "That's the divergences between two distributions.",
            "In ours, it's the square of the difference in.",
            "Number of.",
            "Alarms?",
            "Then the regret.",
            "There is one or the other.",
            "So it can be either one over Delta squared or it can be one over the divergance between the pre change and the post changed.",
            "The distribution of the best and second best arm.",
            "Multiply with the difference one of the square goes away.",
            "You don't account for the number of queries.",
            "So this leading term will actually depend on the number of queries you make.",
            "L is the number of queries you make at each time step.",
            "So you can see that.",
            "Account for the cost of the clear.",
            "It's just the regret right?",
            "We can combine them in a later optimization.",
            "Lucky bounce you need.",
            "So it's log T. For the regret only without considering the cost of the queries yet.",
            "Rate the rate of Korean.",
            "Sorry."
        ],
        [
            "So the ideas of the proof are quite simple.",
            "So first we have to define the expected number of times that.",
            "The algorithm is going to reset.",
            "So that number will be equal to the number of changes plus the number of.",
            "Um?",
            "Plus the number of false detections.",
            "So end of T is the number of false detection you added to the number of real changes, and this will compose one term.",
            "Um?",
            "This one.",
            "And in this term it's basically.",
            "It's basically the regret of the stochastic multi armed bandit problem.",
            "OK. And there's another term that depends on the the delay in detecting a changes in the mean, and this one will count for a term like this, and this term will also be logarithmic in T. And we will bound.",
            "We can bound both the N&L using tail probability bounds such as Hopkins inequality.",
            "OK, so.",
            "We can also add."
        ],
        [
            "What is a lower bound for such a?",
            "For an algorithm that's allowed to query in addition to.",
            "Trying to minimize.",
            "The regret.",
            "So we obtain a partial result for this question.",
            "Anne.",
            "But this this lower bound works for very reasonable algorithms, and these algorithms, where are those that detect changes and then react to them?",
            "So.",
            "Um?",
            "So this algorithm basically will try to detect changes without raising too many alarms.",
            "So therefore.",
            "The optimal way of doing this will be provided by the change detection algorithm that works on likelihood ratios.",
            "An example of such an algorithm is."
        ],
        [
            "The sure yeah yeah.",
            "If Robert Skin this one will raise an alarm.",
            "Suppose that you know the pre change and the post change distributions.",
            "This one will raise an alarm by computing a sum of likelihood ratios and compare it to a certain threshold and the optimality result is the following.",
            "It's actually a very strong result that says that if there are no changes.",
            "Your this rule will not give you.",
            "On average one.",
            "When alarm over the whole time horizon up to time T. Whereas if the change occurs at the first time instant, you will get.",
            "The alarm time will occur at the time that's logarithmic in the time horizon.",
            "And this is both an upper bound and the lower bound.",
            "On the alarm time.",
            "So basically if we look at all the algorithm."
        ],
        [
            "Where the number of switches between the different arms is bounded by.",
            "The true number of changes plus a constant term.",
            "Then we can say that.",
            "You have to incur this logarithmic delay in detection.",
            "And basically, if we also consider only Bernoulli distributed.",
            "Reward functions reward.",
            "Distributions.",
            "Then we have to detect the parameter of distributed distribution in order to.",
            "Detect a change in the mean because the mean of Bernoulli random variable is the parameter.",
            "OK. And we use the previous theorem and we can show that.",
            "The regret is lower bounded by a term.",
            "That's logarithmic in time.",
            "And linear in both the number of changes and the number of arms."
        ],
        [
            "So finally, if we look back, we had the cost of querying.",
            "We can include it.",
            "Add it up to the regret and you can optimize by doing a simple.",
            "Simple derivative.",
            "And you can.",
            "For example, if you assume a constant cost per query, you can show that you have the optimal number of queries will be.",
            "The square root of the number of arms, number of changes, and the logarithm of the term that decreases very fast."
        ],
        [
            "Alright, so there are a bunch of open questions.",
            "So for example, what is the optimal way of querying instead of querying?",
            "All the arms equally, should we query the arms that give you higher reward more often than the others?",
            "Another question is what happens when you include the concept of queries that may fail with a certain probability.",
            "And another question is.",
            "What is the minimum number of changes number of queries that you need?",
            "To have a reasonable confidence on detecting.",
            "Changing the mean and only the mean, not the changing distribution.",
            "And you can also extend this work.",
            "Two cases where the rewards are not piecewise stationary but actually evolve as a Markov process.",
            "Or a Markov chain?",
            "So that's all I have to say, thanks.",
            "So.",
            "So if you combine at the end so you ask from the taskbar is like an overskirt donkey and you combine that with the regret bomb, you would get the right, right?",
            "Depends on K. So dependence, so you mean at this term?",
            "Eventually the total cost is like square above, right?",
            "If you take into columns built in the way of doing the query.",
            "Right?",
            "Yes.",
            "After that is fine for the question is what is the dependence on K then bound?",
            "If you know K. Put an Apple.",
            "I don't remember, so it's either square root, K of or at most linear in K. OK.",
            "Question.",
            "Let's speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So good morning.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with Shimon or.",
                    "label": 0
                },
                {
                    "sent": "And we're going to talk about multi armed bandits.",
                    "label": 0
                },
                {
                    "sent": "A particular kind of multi arm bandit where OK first of all there are two very well studied versions of the multi arm bandit problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first one assumes that the rewards are stochastic, but follow a fixed distribution.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second version is 1, where it is assumed that the rewards are controlled by an opponent.",
                    "label": 0
                },
                {
                    "sent": "So opponent writes down ahead of time what are, what's the sequence of rewards and you, will you discover them one by one overtime.",
                    "label": 0
                },
                {
                    "sent": "And our model is basically trying to capture something between these two extremes so.",
                    "label": 1
                },
                {
                    "sent": "In our model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a multi arm bandit where you have an arms and each arm will be stochastic with a fixed distribution over long times, but at unknown time instance the distribution is going to change and the new distribution is also a normal.",
                    "label": 1
                },
                {
                    "sent": "No, we introduce also an additional concept of querying arms that you haven't actually pulled, so for this you can think of going to a casino and you have a lot of slot machines and there are people playing at each of the machines, so although you don't pull the arms on each of the machines, you can ask another gambler was pulling those arms to tell you what he got.",
                    "label": 1
                },
                {
                    "sent": "Alright, but we also assume that you cannot ask every single gambler at every single slot machine what reward he will obtain.",
                    "label": 0
                },
                {
                    "sent": "No no.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example of this kind of scenario is when you're trying to sell items.",
                    "label": 0
                },
                {
                    "sent": "For example insurance policies.",
                    "label": 0
                },
                {
                    "sent": "So what you could do is you should you can call people and ask them whether they want to buy a certain policy and at a certain price.",
                    "label": 0
                },
                {
                    "sent": "And your reward will be the number of sales you make.",
                    "label": 0
                },
                {
                    "sent": "And the queries in this situation would be calling people to make surveys without actually selling them the policies.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The sequence of actions in this.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Decision problem is as follows.",
                    "label": 0
                },
                {
                    "sent": "1st at every time instant you choose an expert that you're going to follow.",
                    "label": 0
                },
                {
                    "sent": "Second, you're going to query a subset of the remaining remaining arms or experts.",
                    "label": 0
                },
                {
                    "sent": "Afterwards you receive the reward from the ARM or the expert and you observe.",
                    "label": 0
                },
                {
                    "sent": "The reward from the.",
                    "label": 0
                },
                {
                    "sent": "I'm glad you queried.",
                    "label": 0
                },
                {
                    "sent": "And finally we introduce a cost for querying so that you don't get the benefit of querying every single expert.",
                    "label": 0
                },
                {
                    "sent": "Now the goal is just like the stochastic multi Arm Bandit.",
                    "label": 0
                },
                {
                    "sent": "You want to do as well as if you had known in advance.",
                    "label": 1
                },
                {
                    "sent": "The distribution of the rewards.",
                    "label": 0
                },
                {
                    "sent": "And basically in our model we also assume that.",
                    "label": 0
                },
                {
                    "sent": "The baseline.",
                    "label": 0
                },
                {
                    "sent": "You know what times?",
                    "label": 0
                },
                {
                    "sent": "The changes occur and whether the new distributions.",
                    "label": 0
                },
                {
                    "sent": "So we're.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to denote the rewards by sequence BT each BT will be a vector composed of the rewards of the N experts.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We'll denote the change points by news by news, and we denote by better the average reward at time T Betty.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the average reward and for example here would be the change point.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Alright, so more notations will denote by the 80 the actions that you're taking.",
                    "label": 0
                },
                {
                    "sent": "We'll denote by S the set of arms that you are querying.",
                    "label": 0
                },
                {
                    "sent": "And we introduce a function C. To model the cost of querying.",
                    "label": 0
                },
                {
                    "sent": "S. A subset S of arms.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "The basis of comparison that we use is.",
                    "label": 0
                },
                {
                    "sent": "This term here, which is the sum of the best average reward at every time instant.",
                    "label": 0
                },
                {
                    "sent": "And the actual performance of.",
                    "label": 0
                },
                {
                    "sent": "The agent will be this expectation here.",
                    "label": 0
                },
                {
                    "sent": "Of the rewards.",
                    "label": 0
                },
                {
                    "sent": "On the actions.",
                    "label": 0
                },
                {
                    "sent": "Now if you look at the combined cost of the queries and the regret.",
                    "label": 0
                },
                {
                    "sent": "You can combine them by dividing over the whole time.",
                    "label": 0
                },
                {
                    "sent": "Divide the whole regret overtime and you add the cost, but the cost of querying per time step.",
                    "label": 0
                },
                {
                    "sent": "So obviously the objective is to minimize this this term.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here are the known results for the two well known examples of multi arm bandits.",
                    "label": 0
                },
                {
                    "sent": "For the stochastic case, using the same concept of regret.",
                    "label": 0
                },
                {
                    "sent": "I has been shown that you can obtain regret that's logarithmic in time.",
                    "label": 0
                },
                {
                    "sent": "An linear in the number of arm arms.",
                    "label": 0
                },
                {
                    "sent": "Whereas for the stochastic bandit problem for the adversarial bandit problem.",
                    "label": 0
                },
                {
                    "sent": "They adopt a different concept of regret where.",
                    "label": 0
                },
                {
                    "sent": "The baseline will be.",
                    "label": 0
                },
                {
                    "sent": "The best total reward you can obtain using a single.",
                    "label": 0
                },
                {
                    "sent": "Action.",
                    "label": 0
                },
                {
                    "sent": "So in this case you have the Max maximum taken outside of the sun.",
                    "label": 0
                },
                {
                    "sent": "And for this case.",
                    "label": 0
                },
                {
                    "sent": "Um, whoops.",
                    "label": 0
                },
                {
                    "sent": "You can show that the regret will have an additional term that an additional factor that's.",
                    "label": 0
                },
                {
                    "sent": "The square root of T. OK, so the model that we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looking at the piecewise stationary band, it has been considered by.",
                    "label": 0
                },
                {
                    "sent": "Two in two papers.",
                    "label": 0
                },
                {
                    "sent": "The first one they provide a solution where they are only interested in detecting changes in the best current arm.",
                    "label": 1
                },
                {
                    "sent": "So obviously they cannot detect when an additional arm becomes better than the one that you're currently convinced is the best.",
                    "label": 0
                },
                {
                    "sent": "And in the other work by Gharibian Moline, they assume that you know the number of change points that are to be expected up to time T, and this will be denoted by K. And they show that there are various ways of obtaining.",
                    "label": 1
                },
                {
                    "sent": "I regret that.",
                    "label": 0
                },
                {
                    "sent": "Increases as the square root of T. And the square root of the number of changes and the square root of.",
                    "label": 0
                },
                {
                    "sent": "The number of arms.",
                    "label": 0
                },
                {
                    "sent": "Page not just get it slightly worse regret.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So by taking the K out of the square root, you can.",
                    "label": 0
                },
                {
                    "sent": "OK. Um, this comes from.",
                    "label": 0
                },
                {
                    "sent": "You can tune better if you don't.",
                    "label": 0
                },
                {
                    "sent": "You can still get slightly better.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure about this fact, but probably you can do it OK.",
                    "label": 0
                },
                {
                    "sent": "Even think that you can do it with Jane side without knowing it using this book.",
                    "label": 0
                },
                {
                    "sent": "Play.",
                    "label": 0
                },
                {
                    "sent": "Alright, and in their work they also showed that there is a lower bound.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For the regret that increases as square root of T and I think I should be careful in mentioning that in these bounds there is a term in the constant term will depend on the difference between the best and the second best reward.",
                    "label": 0
                },
                {
                    "sent": "OK. Actually I checked and he had.",
                    "label": 0
                },
                {
                    "sent": "A constant term.",
                    "label": 0
                },
                {
                    "sent": "So we can discuss offline.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so in in our work we're going to show is that.",
                    "label": 0
                },
                {
                    "sent": "It's there's actually a very simple solution if you're allowed to query a few arms at every time step, and in that solution you can obtain a regret that's log arhythmic in T and linear in the number of arms and the number of changes.",
                    "label": 0
                },
                {
                    "sent": "And also it doesn't require any knowledge on.",
                    "label": 0
                },
                {
                    "sent": "The number of changes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the approach is very simple.",
                    "label": 0
                },
                {
                    "sent": "We're going to.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To detect the changes and once we detect the change will reset whatever algorithm that we are using to solve a stochastic multi armed bandit problem.",
                    "label": 1
                },
                {
                    "sent": "Now, there are various ways for detecting changes.",
                    "label": 0
                },
                {
                    "sent": "Some of them will use.",
                    "label": 0
                },
                {
                    "sent": "Techniques norms are known as the change detection schemes, so some of them will compute likelihood ratios at every time step.",
                    "label": 0
                },
                {
                    "sent": "And one, whenever this likelihood ratio goes above a threshold, you raise an alarm and detect the change.",
                    "label": 0
                },
                {
                    "sent": "But what we?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to do is much simpler.",
                    "label": 0
                },
                {
                    "sent": "We're going to compute empirical averages based on the queries that we obtain.",
                    "label": 0
                },
                {
                    "sent": "And we're going to compare these empirical averages over different windows.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the details of the algorithm are as follows.",
                    "label": 0
                },
                {
                    "sent": "So you break first.",
                    "label": 0
                },
                {
                    "sent": "We break the time horizon into.",
                    "label": 1
                },
                {
                    "sent": "Some length intervals of fixed length.",
                    "label": 0
                },
                {
                    "sent": "And over these these intervals we use the queries to create a model of the average reward.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The algorithm will work as follows, so at every time time instant.",
                    "label": 0
                },
                {
                    "sent": "You will update for each arm a score.",
                    "label": 0
                },
                {
                    "sent": "This score will be composed of the empirical average over all times of of the rewards.",
                    "label": 0
                },
                {
                    "sent": "Plus a term that's inversely proportional to the number of times you have sampled the current arm.",
                    "label": 0
                },
                {
                    "sent": "And you will choose.",
                    "label": 0
                },
                {
                    "sent": "The arm that has the best.",
                    "label": 0
                },
                {
                    "sent": "Search score and at the same time.",
                    "label": 0
                },
                {
                    "sent": "You will compute.",
                    "label": 0
                },
                {
                    "sent": "The empirical averages over the windows, and once you detect.",
                    "label": 0
                },
                {
                    "sent": "A difference in the empirical averages that exceeds a certain threshold.",
                    "label": 0
                },
                {
                    "sent": "You're going to reset the.",
                    "label": 0
                },
                {
                    "sent": "This algorithm, the sub algorithm.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the guarantee of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we know that whenever the change occurs.",
                    "label": 0
                },
                {
                    "sent": "The change will occur by an amount that's at least two epsilon, where the epsilon is a term that's given to you by an Oracle.",
                    "label": 0
                },
                {
                    "sent": "Now with this epsilon you can.",
                    "label": 0
                },
                {
                    "sent": "We did we determine an interval length of.",
                    "label": 0
                },
                {
                    "sent": "The interval length tell which will be inversely proportional to the square of epsilon.",
                    "label": 0
                },
                {
                    "sent": "And with this parameter in the previous algorithm, we can show that the regret will be.",
                    "label": 0
                },
                {
                    "sent": "A sum of basically these three terms.",
                    "label": 0
                },
                {
                    "sent": "When one of these terms, the second one.",
                    "label": 0
                },
                {
                    "sent": "Will depend on the underlying algorithm that you're using to solve.",
                    "label": 0
                },
                {
                    "sent": "The multi stochastic Multi armed bandit problem.",
                    "label": 0
                },
                {
                    "sent": "In our case we use the UCB algorithm.",
                    "label": 0
                },
                {
                    "sent": "And there is another term that that principally captures the delay in detecting a change in the mean.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In comparison with the work by.",
                    "label": 0
                },
                {
                    "sent": "Gary, Vee, and moulinas.",
                    "label": 0
                },
                {
                    "sent": "They had regret bound that has a term that's polynomial in T. Was we only have a sqrt T?",
                    "label": 0
                },
                {
                    "sent": "We only have a log arhythmic term.",
                    "label": 0
                },
                {
                    "sent": "And in both bounds hours and the one by.",
                    "label": 0
                },
                {
                    "sent": "By Gary V and bowling.",
                    "label": 0
                },
                {
                    "sent": "We both have a term in the denominator, so the Delta here.",
                    "label": 0
                },
                {
                    "sent": "That depends on the difference between the best and the second best arm.",
                    "label": 0
                },
                {
                    "sent": "Delta squared in both cases.",
                    "label": 0
                },
                {
                    "sent": "So in their paper they had a term.",
                    "label": 0
                },
                {
                    "sent": "That's the divergences between two distributions.",
                    "label": 0
                },
                {
                    "sent": "In ours, it's the square of the difference in.",
                    "label": 0
                },
                {
                    "sent": "Number of.",
                    "label": 0
                },
                {
                    "sent": "Alarms?",
                    "label": 0
                },
                {
                    "sent": "Then the regret.",
                    "label": 0
                },
                {
                    "sent": "There is one or the other.",
                    "label": 0
                },
                {
                    "sent": "So it can be either one over Delta squared or it can be one over the divergance between the pre change and the post changed.",
                    "label": 0
                },
                {
                    "sent": "The distribution of the best and second best arm.",
                    "label": 0
                },
                {
                    "sent": "Multiply with the difference one of the square goes away.",
                    "label": 0
                },
                {
                    "sent": "You don't account for the number of queries.",
                    "label": 0
                },
                {
                    "sent": "So this leading term will actually depend on the number of queries you make.",
                    "label": 0
                },
                {
                    "sent": "L is the number of queries you make at each time step.",
                    "label": 0
                },
                {
                    "sent": "So you can see that.",
                    "label": 0
                },
                {
                    "sent": "Account for the cost of the clear.",
                    "label": 0
                },
                {
                    "sent": "It's just the regret right?",
                    "label": 0
                },
                {
                    "sent": "We can combine them in a later optimization.",
                    "label": 0
                },
                {
                    "sent": "Lucky bounce you need.",
                    "label": 0
                },
                {
                    "sent": "So it's log T. For the regret only without considering the cost of the queries yet.",
                    "label": 0
                },
                {
                    "sent": "Rate the rate of Korean.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the ideas of the proof are quite simple.",
                    "label": 0
                },
                {
                    "sent": "So first we have to define the expected number of times that.",
                    "label": 1
                },
                {
                    "sent": "The algorithm is going to reset.",
                    "label": 0
                },
                {
                    "sent": "So that number will be equal to the number of changes plus the number of.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Plus the number of false detections.",
                    "label": 1
                },
                {
                    "sent": "So end of T is the number of false detection you added to the number of real changes, and this will compose one term.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "And in this term it's basically.",
                    "label": 1
                },
                {
                    "sent": "It's basically the regret of the stochastic multi armed bandit problem.",
                    "label": 0
                },
                {
                    "sent": "OK. And there's another term that depends on the the delay in detecting a changes in the mean, and this one will count for a term like this, and this term will also be logarithmic in T. And we will bound.",
                    "label": 0
                },
                {
                    "sent": "We can bound both the N&L using tail probability bounds such as Hopkins inequality.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We can also add.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is a lower bound for such a?",
                    "label": 1
                },
                {
                    "sent": "For an algorithm that's allowed to query in addition to.",
                    "label": 0
                },
                {
                    "sent": "Trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "The regret.",
                    "label": 1
                },
                {
                    "sent": "So we obtain a partial result for this question.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "But this this lower bound works for very reasonable algorithms, and these algorithms, where are those that detect changes and then react to them?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this algorithm basically will try to detect changes without raising too many alarms.",
                    "label": 0
                },
                {
                    "sent": "So therefore.",
                    "label": 0
                },
                {
                    "sent": "The optimal way of doing this will be provided by the change detection algorithm that works on likelihood ratios.",
                    "label": 0
                },
                {
                    "sent": "An example of such an algorithm is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The sure yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "If Robert Skin this one will raise an alarm.",
                    "label": 0
                },
                {
                    "sent": "Suppose that you know the pre change and the post change distributions.",
                    "label": 0
                },
                {
                    "sent": "This one will raise an alarm by computing a sum of likelihood ratios and compare it to a certain threshold and the optimality result is the following.",
                    "label": 0
                },
                {
                    "sent": "It's actually a very strong result that says that if there are no changes.",
                    "label": 0
                },
                {
                    "sent": "Your this rule will not give you.",
                    "label": 0
                },
                {
                    "sent": "On average one.",
                    "label": 0
                },
                {
                    "sent": "When alarm over the whole time horizon up to time T. Whereas if the change occurs at the first time instant, you will get.",
                    "label": 1
                },
                {
                    "sent": "The alarm time will occur at the time that's logarithmic in the time horizon.",
                    "label": 0
                },
                {
                    "sent": "And this is both an upper bound and the lower bound.",
                    "label": 0
                },
                {
                    "sent": "On the alarm time.",
                    "label": 0
                },
                {
                    "sent": "So basically if we look at all the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the number of switches between the different arms is bounded by.",
                    "label": 0
                },
                {
                    "sent": "The true number of changes plus a constant term.",
                    "label": 0
                },
                {
                    "sent": "Then we can say that.",
                    "label": 0
                },
                {
                    "sent": "You have to incur this logarithmic delay in detection.",
                    "label": 0
                },
                {
                    "sent": "And basically, if we also consider only Bernoulli distributed.",
                    "label": 0
                },
                {
                    "sent": "Reward functions reward.",
                    "label": 0
                },
                {
                    "sent": "Distributions.",
                    "label": 0
                },
                {
                    "sent": "Then we have to detect the parameter of distributed distribution in order to.",
                    "label": 0
                },
                {
                    "sent": "Detect a change in the mean because the mean of Bernoulli random variable is the parameter.",
                    "label": 0
                },
                {
                    "sent": "OK. And we use the previous theorem and we can show that.",
                    "label": 0
                },
                {
                    "sent": "The regret is lower bounded by a term.",
                    "label": 0
                },
                {
                    "sent": "That's logarithmic in time.",
                    "label": 0
                },
                {
                    "sent": "And linear in both the number of changes and the number of arms.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally, if we look back, we had the cost of querying.",
                    "label": 0
                },
                {
                    "sent": "We can include it.",
                    "label": 0
                },
                {
                    "sent": "Add it up to the regret and you can optimize by doing a simple.",
                    "label": 0
                },
                {
                    "sent": "Simple derivative.",
                    "label": 0
                },
                {
                    "sent": "And you can.",
                    "label": 0
                },
                {
                    "sent": "For example, if you assume a constant cost per query, you can show that you have the optimal number of queries will be.",
                    "label": 0
                },
                {
                    "sent": "The square root of the number of arms, number of changes, and the logarithm of the term that decreases very fast.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so there are a bunch of open questions.",
                    "label": 1
                },
                {
                    "sent": "So for example, what is the optimal way of querying instead of querying?",
                    "label": 0
                },
                {
                    "sent": "All the arms equally, should we query the arms that give you higher reward more often than the others?",
                    "label": 1
                },
                {
                    "sent": "Another question is what happens when you include the concept of queries that may fail with a certain probability.",
                    "label": 0
                },
                {
                    "sent": "And another question is.",
                    "label": 0
                },
                {
                    "sent": "What is the minimum number of changes number of queries that you need?",
                    "label": 0
                },
                {
                    "sent": "To have a reasonable confidence on detecting.",
                    "label": 0
                },
                {
                    "sent": "Changing the mean and only the mean, not the changing distribution.",
                    "label": 0
                },
                {
                    "sent": "And you can also extend this work.",
                    "label": 0
                },
                {
                    "sent": "Two cases where the rewards are not piecewise stationary but actually evolve as a Markov process.",
                    "label": 0
                },
                {
                    "sent": "Or a Markov chain?",
                    "label": 0
                },
                {
                    "sent": "So that's all I have to say, thanks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So if you combine at the end so you ask from the taskbar is like an overskirt donkey and you combine that with the regret bomb, you would get the right, right?",
                    "label": 0
                },
                {
                    "sent": "Depends on K. So dependence, so you mean at this term?",
                    "label": 0
                },
                {
                    "sent": "Eventually the total cost is like square above, right?",
                    "label": 0
                },
                {
                    "sent": "If you take into columns built in the way of doing the query.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "After that is fine for the question is what is the dependence on K then bound?",
                    "label": 0
                },
                {
                    "sent": "If you know K. Put an Apple.",
                    "label": 0
                },
                {
                    "sent": "I don't remember, so it's either square root, K of or at most linear in K. OK.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Let's speak again.",
                    "label": 0
                }
            ]
        }
    }
}