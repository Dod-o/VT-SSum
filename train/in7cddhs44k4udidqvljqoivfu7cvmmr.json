{
    "id": "in7cddhs44k4udidqvljqoivfu7cvmmr",
    "title": "Competing With Strategies",
    "info": {
        "author": [
            "Karthik Sridharan, Department of Computer Science, Cornell University"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_sridharan_strategies/",
    "segmentation": [
        [
            "Does joint work with the Hanway?",
            "Who's there and Sasha?",
            "Alright, so I'll be talking about online learning while we're competing with strategies or competing with other online learning algorithms.",
            "So since is online learning set."
        ],
        [
            "And I shouldn't take too much time in explaining the framework, so the framework is.",
            "We basically play learning game where we play for T rounds and each round the learner basically picks hypothesis from a class of predictors.",
            "Script F. The adversary then simultaneously picks an instant ziti from the class of instances cripsy and we suffer a loss L of FT community.",
            "And what's the goal in the traditional online learning problem or goal is to minimize what's called the regret there is I look at what's the total loss that I suffered after T rounds.",
            "And compare it with the loss of the best single predictor in hindsight from the class of predictors.",
            "Script F. Now, when is this notion of regret useful, or when is this kind of intuitively useful notion to look at?",
            "Our basic goal is to make sure that our cumulative losses good and what we want to compare against here is the single best predictor in hindsight from this class of predictors scripts, and this is generally useful when we believe that this guy out here is small.",
            "That is what we are comparing against small.",
            "So when we believe that the class of predictors is good for this problem.",
            "For any sequence Z1 through ZT, now what we might want to consider is instead of competing with just a class of hypothesis script F, we might want to compete with something more we might want to compare ourselves with the class of other online learning algorithms, and Luckily for me a lot started the talk with arnp Arma model and there basically you're interested in competing with these models, which are themselves strategies or algorithms.",
            "Specifically what we're going to look at here is the notion of regret where we want to compete against.",
            "A set of strategies pie where a strategy pie where the set of strategy pie is can be looked at as a set of online learning algorithms.",
            "So each strategy Pi is indexed by \u03c0 one through \u03a0 capital T, where each party basically looks at the history or the data.",
            "That seems it seems so far, and it picks a hypothesis for the class script if and what we want to do is we want to get this regret to be small.",
            "OK."
        ],
        [
            "So the our goals today are going to be these two.",
            "So first we want to develop some tools for studying.",
            "What's the right rate for such problems for a general class of strategy spy and next is, can we get a recipe for building algorithms for these problems?",
            "Forgetting algorithms that will guarantee is that the regret is small.",
            "OK, so when I say that we are interested in building tools for getting the right rate, what we mean by right rate and this is base."
        ],
        [
            "What's the min Max rate so the min Max rate for an online learning problem is well, what's the best regret that I can guarantee?",
            "That is, take the best algorithm, hedge it against the worst adversary and what's the expected regret of this algorithm?",
            "So we are allowing randomized algorithms out here.",
            "So specifically, this can be written down in a more explicit form where basically we go first.",
            "We pick the best distribution over the set of predictors.",
            "The adversary picks the worst instance Z1, and then we draw our predictor from this distribution and we basically do this 40 rounds and we look at what our regret is.",
            "And so we're going to be interested in studying this quantity.",
            "And why is this quantity interesting?",
            "Because basically it's the best guarantee that we can give.",
            "So no algorithm can do better than this and every other existing algorithm, which irrespective of the sequence can achieve this rate and expectation.",
            "And the key idea here is going to be.",
            "We're going to try instead of dealing with this messy definition directly, we're going to try to instead move to a more stochastic or a status tickle quantity like done in previous works by Abernathy Italian in earlier paper with language and Sasha, and more specifically, what we're going to do is try to introduce sequential complexity tools that can give us that can tell us what the complexity of a class of strategies.",
            "Capital bias.",
            "So without further ado, let me get to the first complexity measure so the."
        ],
        [
            "Complexity measure is is what we call the sequential Rademacher complexity.",
            "But we are interested in getting this complexity for a set of strategies pipe, so let me describe what this term here is.",
            "So it's basically comes.",
            "It comes from the notion of classical Rademacher complexity, where we basically want to see how well our function class fits random noise.",
            "So here since we're interested in strategies and because we are interested in online learning where this dependence on the past.",
            "We need a little more complex notion, so specifically the term.",
            "Here we take soup over W&Z, which RZ value trees so pictorially it looks like this.",
            "So here is a tree of layer of deputy where each each nodes it's an instance from this set script Z&Z is a similar such tree and basically here W represents in some sense our history.",
            "So we're going to put what goes into into our strategy.",
            "So what we're interested in is.",
            "Is computing this quantity out here so we take the worst trees WNZ here.",
            "Epsilons are Rademacher random variables, which means that each epsilon T is plus or minus one with equal probability and what we're interested in is computing soup overall strategies.",
            "This quantity here, which is epsilon tee times the loss when we compute \u03c0 on this tree here.",
            "So specifically, for instance, if we take the example where epsilon if to look at the quantity here, if you take epsilon to be plus 1 -- 1 -- 1.",
            "This basically says left, right and right.",
            "So in this tree we go to W 1, W 3 and W 6, and so on.",
            "In this tree, Z1Z3 and Z6 and the way we compute this inner quantity here is basically first it's with a plus so plus and we take Z1 and initially we don't have any history, so there's nothing in here.",
            "The second guy is with Z3, because that's what basically occurs here.",
            "It goes with history W one, and when we go to the third guy at Z6 with these two S history, and so this is the way we compute this.",
            "Quantity out here and OK so the key thing to note in this quantity here is that our history is always consistent in the sense that our history is always drawn from this tree, and if W one and W three are basically what we see on round three, then around 4:00 we see WNW 3 W 6 and so on, so we always get a consistent history.",
            "OK, so why is this quantity interesting?",
            "This quantity is interest."
        ],
        [
            "Because we can basically show that the min Max rate while competing against a set of strategies pie is upper bounded by two times this quantity.",
            "This sequential order complexity and just to kind of.",
            "As examples, if we have history independent strategies, that is all Pi one through PT are equal then this recovers the sequential Rademacher complexity.",
            "In an earlier paper with Ambridge and Sasha, and if we look at the individual sequence prediction problem where we're looking at static experts, that is irrespective of what the history is, price of tea is just given by a single piece of tea.",
            "This recovers the classical Rademacher complexity which is an earlier result by Niccolo and Gay Bar.",
            "Alright so I."
        ],
        [
            "You also mentioned that we also in the paper.",
            "If you see we also introduce the notion of sequential covers for strategies.",
            "And basically we show how to bound these min Max rate in terms of these notions of covering number and much of the standard properties like Lipschitz, contraction lemma and other structural properties also hold for this notion of Rademacher complexity.",
            "OK, so this is good, so we have some hold on how to get bounds on what the right rate is when we're learning when we're competing against strategies.",
            "However, unfortunately all the bounds that I talked about right now are nonconstructive.",
            "All that I said was that the value is upper bounded by whatever complexity measured that we get.",
            "So what we would like is of course this opens up the question of how we, what do we do about algorithms?",
            "How do we develop algorithms for these problems?",
            "And that's what we're going to look at next and."
        ],
        [
            "So we're going to start with what's called the relaxation mechanism, which was introduced in an earlier paper with Sasha and Ohad an.",
            "Basically the idea is you solve this approximate dynamic programming.",
            "So if you want to minimize regret, you want to look for function relaxation, where the relaxation of a set of strategies Pi given all the history that is Z1 through Z capital T has to be an upper bound on minus the competitor that is minus take the best strategy pie and look at the cumulative loss of that strategy.",
            "And we also needed to satisfy admissibility condition, which is this recursive inequality.",
            "Here there is take the best distribution Q sub T take the worst instances of T look at the current expected loss plus the relaxation given all instances Z1 through Z sub T and we need this to be upper bounded by the relaxation given Z1 through Z T -- 1.",
            "So this happens if these two conditions hold then we say that this relaxation is admissible and an algorithm kind of obviously comes out of this, which is the one that.",
            "Solve this optimization problem here.",
            "So basically you want to find the distribution that minimizes super ZT of current expected loss plus relaxation, given Z1 through ZT.",
            "And this is interesting because almost immediately from this recursion what you can show is that the expected regret of the associated algorithm can be upper bounded as by relaxation when you're given nothing, so the relaxation, the very first step.",
            "OK, so this basically more or less is from here and it just goes through four strategies by by replacing in for a single best predictor by in for PIE.",
            "Now of course the key question is how do we actually develop such relaxations?",
            "And so this is where the nonconstructive part that I talked about in the beginning of the talk comes in."
        ],
        [
            "The picture so specifically the sequential order marker complexity that I described for strategies can actually be actually gives rise to an appropriate relaxation.",
            "So it's the relaxation out here, and if you notice, it basically resembles the sequential Rademacher complexity that offer strategy that I talked about earlier with this extra term out here, and so the relaxation is given out here, and this is interesting because, again, this is, and now we get.",
            "Since this is I've given you a relaxation.",
            "This relaxation automatically gives you an algorithm, and this algorithm enjoys the regret bound of.",
            "Actually there should be a 2 here, so of two times the sequential order complexity.",
            "Now, of course, this gives us the nonconstructive ants.",
            "I mean sorry, the constructive answer of how to get an algorithm, but this is close to useless for most problems, because really there's no way of actually getting ahold of this quantity here.",
            "So basically we need something more to talk about, more general problems about how to get.",
            "Efficient and tractable algorithms."
        ],
        [
            "So the general recipe for dealing with how to get these relaxation is that we start with what we know.",
            "So we start with the sequential order marker relaxation.",
            "We write it down, we formulate it, and then the next step is we try to get rid of the two trees which are kind of the main hindrance in terms of computation and we get rid of them by going to an appropriate upper bound.",
            "And so there are a few techniques generic tools to do that were listed in the paper and a lot of that can actually be done even for the set of strategies.",
            "Right and then we need to ensure that this relaxation that we get satisfies the admissibility condition and typically the one means min Max theorem is a useful tool for doing this step, and then we're all set, so the algorithm is going to be solving this optimization problem and the hope in all this is that we are able to relax too and good enough relaxation such that this optimization problem is easy to solve.",
            "OK, so let me give you some direct examples that probably will shed more light."
        ],
        [
            "So the first example that I'd like to talk about is a problem of binary sequence prediction, but where we are trying to compete with maximum posterior remodels.",
            "So specifically we are interested in the set of instances binary, so 0, one, and the loss that we are interested in is the absolute loss, and F is basically a number between zero and one, so this can be seen as this loss can equally be seen as the expected 01 loss when we use a prediction F. And the set of strategies that we are interested in competing with is this guy out here.",
            "So it's characterized by or it's parameterized by Alpha and beta, where Alpha some number larger than one, and beta is between one and see some beta.",
            "And so to give you a notion of what this equation here represents, basically what this is telling us is we're interested in competing with maximum posterior model, where our likelihood is given by the balloon redistribution, and our prior is the beta prior with Alpha and beta as parameters.",
            "OK, so for this we can basically write down the sequential Rademacher relaxation and with a little bit of work what we can arrive at is this relaxation out here and an associated algorithm automatically pops out, which is basically.",
            "Compute cutey by this equation out here.",
            "So notice that this is basically has two optimization problems, super Alpha and beta or this term and this term.",
            "So it looks hideous.",
            "It's nonconvex, but Luckily due to the form here, it turns out that this can actually be done in polynomial time, because this is basically like sum of ratios.",
            "And so specifically."
        ],
        [
            "We can get an algorithm whose time complexity is order T log T, so at time T the time complexity steel of T and the expected regret bound that we get for this problem is of order square root T. And when we compare for instance with the experts algorithm, so you can discretize Alpha and beta and try to run experts over this.",
            "So you'll get a slightly worse regret bound which has an extra lock T. And you also get a verse computational complexity over the algorithm.",
            "Alright, so the next."
        ],
        [
            "Example that we look at is autoregressive model.",
            "So in the auto regressive model, so we're going to consider F&Z to be numbers between minus one and one.",
            "So we're going to try and predict the number between minus one and one and then the true numbers really is revealed and the loss that we consider is the squared loss.",
            "But really it can also be any other convex loss.",
            "Provided us with convex and Lipschitz OK, so the set of strategies that we are considering is the set of autoregressive model where we allow dependence on all the full dependence on history.",
            "So it can depend all the way from time one to time T -- 1 at time T and basically the restriction that we put is in the weights that we take.",
            "So we for this example I'm going to assume that the L1 norm of the weights is bounded by 1.",
            "OK, so for this again we can write down the sequential market relaxation and which is just a little bit of massage ING of the relaxation.",
            "We can move to this upper bound which is given here which is so 80 here is is either two times the Rademacher random variable.",
            "Whenever S is larger than T, so for the future, and if it is, if it's smaller than T, it's basically minus the instance that we've seen and the relaxation is given out here and from this relaxation you can.",
            "Easily derive this randomized algorithm again.",
            "So basically what this randomized algorithm is doing is at each time T we basically draw epsilon future random variables epsilon T plus one through capital T. So these are America random variables and we need to compute this quantity out here.",
            "So basically is Max between these two quantities out here and so this is easy enough to compute and so specifically we can get a regret bound of order square root T for this relaxation, which is easy to check.",
            "And the time complexity that we incur per round, unfortunately, is of order P. But it turns out that by replacing these randomly, actually by replacing these random variables here by standard normal variables and doing a little bit more massage ING, you can actually get the same regret bound of order square root T. But you can get an improved time complexity of order, one per row.",
            "OK, so these are two specific exam."
        ],
        [
            "Apples in the paper.",
            "We also talk about other models, so competing with, for instance finite or Marco strategies and auto regressive models where we have a finite loop back, so we look at only the K previous observations or auto regressive model with full dependence on history.",
            "But other geometric restrictions on Theta.",
            "So here we looked at L1 norm.",
            "You can look at other other type of geometry, maybe other LP norms and other geometric restrictions on Theta.",
            "Another interesting thing that we also consider is.",
            "Strategies when we can where basically depends on the past, but in a compressed way.",
            "So it basically compresses history through a particular into a particular space, and then it depends only on that compressed format.",
            "So that's basically the sufficient statistic for the strategy.",
            "And we also look at competing with families of regularised least squares algorithms.",
            "So the usual notion of regret you are interested in looking at what's the regret with respect to, say, the best linear predictor?",
            "But maybe the best linear predictor is not good enough, but maybe it's known that if under the magical parameters the regularised least square algorithm actually does well for these for certain types of problems, and in that case we'd like to basically directly compete with the family of regularly square algorithm, and we also have an example where we try to compete with family of follow the perturb leader algorithm.",
            "In both of these, basically you have a starting point of where your vector is an, then you have a schedule for your step sizes, and we need to compete with all possible such step sizes.",
            "And starting point W not.",
            "And basically if you're looking at the at the usual notion of regret, it basically is just says that forget the step size and it's basically just says look at the starting point and you want to compare yourself with that."
        ],
        [
            "Alright, so in terms of further directions, so we'd like to also kind of consider more general classes of Bayesian algorithms indexed by some family of priors.",
            "And basically it takes this family of priors, takes a likelihood model and it updates and how well we can we do with that is there is more general result that we can get there and we'd like to today.",
            "In the first talk a lot talked about a model that wasn't totally the worst case, so there was a particular way in which data was generated.",
            "So there was some notion of.",
            "Predictability and what I talked about here was in some sense the worst case scenario, and what we'd like to do is develop adaptive algorithms that can adapt to both.",
            "So if the case if it is the case that the instances are a little more predictive under some right notion of predictability, then we get better, then we get a better regret bound.",
            "We'd like to also some of the examples that were given here, so we have.",
            "We do give algorithms, but they're not efficient enough, so we'd like to see if we can derive more efficient algorithms while competing with the family of Regularising.",
            "These squares and competing with the family of follow the regularised leader algorithms.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Does joint work with the Hanway?",
                    "label": 0
                },
                {
                    "sent": "Who's there and Sasha?",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'll be talking about online learning while we're competing with strategies or competing with other online learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "So since is online learning set.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I shouldn't take too much time in explaining the framework, so the framework is.",
                    "label": 0
                },
                {
                    "sent": "We basically play learning game where we play for T rounds and each round the learner basically picks hypothesis from a class of predictors.",
                    "label": 0
                },
                {
                    "sent": "Script F. The adversary then simultaneously picks an instant ziti from the class of instances cripsy and we suffer a loss L of FT community.",
                    "label": 0
                },
                {
                    "sent": "And what's the goal in the traditional online learning problem or goal is to minimize what's called the regret there is I look at what's the total loss that I suffered after T rounds.",
                    "label": 0
                },
                {
                    "sent": "And compare it with the loss of the best single predictor in hindsight from the class of predictors.",
                    "label": 0
                },
                {
                    "sent": "Script F. Now, when is this notion of regret useful, or when is this kind of intuitively useful notion to look at?",
                    "label": 0
                },
                {
                    "sent": "Our basic goal is to make sure that our cumulative losses good and what we want to compare against here is the single best predictor in hindsight from this class of predictors scripts, and this is generally useful when we believe that this guy out here is small.",
                    "label": 0
                },
                {
                    "sent": "That is what we are comparing against small.",
                    "label": 0
                },
                {
                    "sent": "So when we believe that the class of predictors is good for this problem.",
                    "label": 0
                },
                {
                    "sent": "For any sequence Z1 through ZT, now what we might want to consider is instead of competing with just a class of hypothesis script F, we might want to compete with something more we might want to compare ourselves with the class of other online learning algorithms, and Luckily for me a lot started the talk with arnp Arma model and there basically you're interested in competing with these models, which are themselves strategies or algorithms.",
                    "label": 0
                },
                {
                    "sent": "Specifically what we're going to look at here is the notion of regret where we want to compete against.",
                    "label": 1
                },
                {
                    "sent": "A set of strategies pie where a strategy pie where the set of strategy pie is can be looked at as a set of online learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "So each strategy Pi is indexed by \u03c0 one through \u03a0 capital T, where each party basically looks at the history or the data.",
                    "label": 0
                },
                {
                    "sent": "That seems it seems so far, and it picks a hypothesis for the class script if and what we want to do is we want to get this regret to be small.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the our goals today are going to be these two.",
                    "label": 0
                },
                {
                    "sent": "So first we want to develop some tools for studying.",
                    "label": 0
                },
                {
                    "sent": "What's the right rate for such problems for a general class of strategy spy and next is, can we get a recipe for building algorithms for these problems?",
                    "label": 1
                },
                {
                    "sent": "Forgetting algorithms that will guarantee is that the regret is small.",
                    "label": 0
                },
                {
                    "sent": "OK, so when I say that we are interested in building tools for getting the right rate, what we mean by right rate and this is base.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's the min Max rate so the min Max rate for an online learning problem is well, what's the best regret that I can guarantee?",
                    "label": 1
                },
                {
                    "sent": "That is, take the best algorithm, hedge it against the worst adversary and what's the expected regret of this algorithm?",
                    "label": 1
                },
                {
                    "sent": "So we are allowing randomized algorithms out here.",
                    "label": 0
                },
                {
                    "sent": "So specifically, this can be written down in a more explicit form where basically we go first.",
                    "label": 0
                },
                {
                    "sent": "We pick the best distribution over the set of predictors.",
                    "label": 0
                },
                {
                    "sent": "The adversary picks the worst instance Z1, and then we draw our predictor from this distribution and we basically do this 40 rounds and we look at what our regret is.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to be interested in studying this quantity.",
                    "label": 0
                },
                {
                    "sent": "And why is this quantity interesting?",
                    "label": 0
                },
                {
                    "sent": "Because basically it's the best guarantee that we can give.",
                    "label": 0
                },
                {
                    "sent": "So no algorithm can do better than this and every other existing algorithm, which irrespective of the sequence can achieve this rate and expectation.",
                    "label": 1
                },
                {
                    "sent": "And the key idea here is going to be.",
                    "label": 0
                },
                {
                    "sent": "We're going to try instead of dealing with this messy definition directly, we're going to try to instead move to a more stochastic or a status tickle quantity like done in previous works by Abernathy Italian in earlier paper with language and Sasha, and more specifically, what we're going to do is try to introduce sequential complexity tools that can give us that can tell us what the complexity of a class of strategies.",
                    "label": 1
                },
                {
                    "sent": "Capital bias.",
                    "label": 0
                },
                {
                    "sent": "So without further ado, let me get to the first complexity measure so the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complexity measure is is what we call the sequential Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "But we are interested in getting this complexity for a set of strategies pipe, so let me describe what this term here is.",
                    "label": 0
                },
                {
                    "sent": "So it's basically comes.",
                    "label": 0
                },
                {
                    "sent": "It comes from the notion of classical Rademacher complexity, where we basically want to see how well our function class fits random noise.",
                    "label": 0
                },
                {
                    "sent": "So here since we're interested in strategies and because we are interested in online learning where this dependence on the past.",
                    "label": 0
                },
                {
                    "sent": "We need a little more complex notion, so specifically the term.",
                    "label": 0
                },
                {
                    "sent": "Here we take soup over W&Z, which RZ value trees so pictorially it looks like this.",
                    "label": 0
                },
                {
                    "sent": "So here is a tree of layer of deputy where each each nodes it's an instance from this set script Z&Z is a similar such tree and basically here W represents in some sense our history.",
                    "label": 0
                },
                {
                    "sent": "So we're going to put what goes into into our strategy.",
                    "label": 0
                },
                {
                    "sent": "So what we're interested in is.",
                    "label": 0
                },
                {
                    "sent": "Is computing this quantity out here so we take the worst trees WNZ here.",
                    "label": 0
                },
                {
                    "sent": "Epsilons are Rademacher random variables, which means that each epsilon T is plus or minus one with equal probability and what we're interested in is computing soup overall strategies.",
                    "label": 0
                },
                {
                    "sent": "This quantity here, which is epsilon tee times the loss when we compute \u03c0 on this tree here.",
                    "label": 0
                },
                {
                    "sent": "So specifically, for instance, if we take the example where epsilon if to look at the quantity here, if you take epsilon to be plus 1 -- 1 -- 1.",
                    "label": 0
                },
                {
                    "sent": "This basically says left, right and right.",
                    "label": 0
                },
                {
                    "sent": "So in this tree we go to W 1, W 3 and W 6, and so on.",
                    "label": 0
                },
                {
                    "sent": "In this tree, Z1Z3 and Z6 and the way we compute this inner quantity here is basically first it's with a plus so plus and we take Z1 and initially we don't have any history, so there's nothing in here.",
                    "label": 0
                },
                {
                    "sent": "The second guy is with Z3, because that's what basically occurs here.",
                    "label": 0
                },
                {
                    "sent": "It goes with history W one, and when we go to the third guy at Z6 with these two S history, and so this is the way we compute this.",
                    "label": 0
                },
                {
                    "sent": "Quantity out here and OK so the key thing to note in this quantity here is that our history is always consistent in the sense that our history is always drawn from this tree, and if W one and W three are basically what we see on round three, then around 4:00 we see WNW 3 W 6 and so on, so we always get a consistent history.",
                    "label": 0
                },
                {
                    "sent": "OK, so why is this quantity interesting?",
                    "label": 0
                },
                {
                    "sent": "This quantity is interest.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because we can basically show that the min Max rate while competing against a set of strategies pie is upper bounded by two times this quantity.",
                    "label": 0
                },
                {
                    "sent": "This sequential order complexity and just to kind of.",
                    "label": 0
                },
                {
                    "sent": "As examples, if we have history independent strategies, that is all Pi one through PT are equal then this recovers the sequential Rademacher complexity.",
                    "label": 1
                },
                {
                    "sent": "In an earlier paper with Ambridge and Sasha, and if we look at the individual sequence prediction problem where we're looking at static experts, that is irrespective of what the history is, price of tea is just given by a single piece of tea.",
                    "label": 0
                },
                {
                    "sent": "This recovers the classical Rademacher complexity which is an earlier result by Niccolo and Gay Bar.",
                    "label": 0
                },
                {
                    "sent": "Alright so I.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You also mentioned that we also in the paper.",
                    "label": 0
                },
                {
                    "sent": "If you see we also introduce the notion of sequential covers for strategies.",
                    "label": 0
                },
                {
                    "sent": "And basically we show how to bound these min Max rate in terms of these notions of covering number and much of the standard properties like Lipschitz, contraction lemma and other structural properties also hold for this notion of Rademacher complexity.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is good, so we have some hold on how to get bounds on what the right rate is when we're learning when we're competing against strategies.",
                    "label": 0
                },
                {
                    "sent": "However, unfortunately all the bounds that I talked about right now are nonconstructive.",
                    "label": 0
                },
                {
                    "sent": "All that I said was that the value is upper bounded by whatever complexity measured that we get.",
                    "label": 0
                },
                {
                    "sent": "So what we would like is of course this opens up the question of how we, what do we do about algorithms?",
                    "label": 0
                },
                {
                    "sent": "How do we develop algorithms for these problems?",
                    "label": 0
                },
                {
                    "sent": "And that's what we're going to look at next and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to start with what's called the relaxation mechanism, which was introduced in an earlier paper with Sasha and Ohad an.",
                    "label": 0
                },
                {
                    "sent": "Basically the idea is you solve this approximate dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "So if you want to minimize regret, you want to look for function relaxation, where the relaxation of a set of strategies Pi given all the history that is Z1 through Z capital T has to be an upper bound on minus the competitor that is minus take the best strategy pie and look at the cumulative loss of that strategy.",
                    "label": 0
                },
                {
                    "sent": "And we also needed to satisfy admissibility condition, which is this recursive inequality.",
                    "label": 0
                },
                {
                    "sent": "Here there is take the best distribution Q sub T take the worst instances of T look at the current expected loss plus the relaxation given all instances Z1 through Z sub T and we need this to be upper bounded by the relaxation given Z1 through Z T -- 1.",
                    "label": 0
                },
                {
                    "sent": "So this happens if these two conditions hold then we say that this relaxation is admissible and an algorithm kind of obviously comes out of this, which is the one that.",
                    "label": 0
                },
                {
                    "sent": "Solve this optimization problem here.",
                    "label": 0
                },
                {
                    "sent": "So basically you want to find the distribution that minimizes super ZT of current expected loss plus relaxation, given Z1 through ZT.",
                    "label": 0
                },
                {
                    "sent": "And this is interesting because almost immediately from this recursion what you can show is that the expected regret of the associated algorithm can be upper bounded as by relaxation when you're given nothing, so the relaxation, the very first step.",
                    "label": 1
                },
                {
                    "sent": "OK, so this basically more or less is from here and it just goes through four strategies by by replacing in for a single best predictor by in for PIE.",
                    "label": 0
                },
                {
                    "sent": "Now of course the key question is how do we actually develop such relaxations?",
                    "label": 0
                },
                {
                    "sent": "And so this is where the nonconstructive part that I talked about in the beginning of the talk comes in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The picture so specifically the sequential order marker complexity that I described for strategies can actually be actually gives rise to an appropriate relaxation.",
                    "label": 1
                },
                {
                    "sent": "So it's the relaxation out here, and if you notice, it basically resembles the sequential Rademacher complexity that offer strategy that I talked about earlier with this extra term out here, and so the relaxation is given out here, and this is interesting because, again, this is, and now we get.",
                    "label": 0
                },
                {
                    "sent": "Since this is I've given you a relaxation.",
                    "label": 0
                },
                {
                    "sent": "This relaxation automatically gives you an algorithm, and this algorithm enjoys the regret bound of.",
                    "label": 0
                },
                {
                    "sent": "Actually there should be a 2 here, so of two times the sequential order complexity.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, this gives us the nonconstructive ants.",
                    "label": 0
                },
                {
                    "sent": "I mean sorry, the constructive answer of how to get an algorithm, but this is close to useless for most problems, because really there's no way of actually getting ahold of this quantity here.",
                    "label": 0
                },
                {
                    "sent": "So basically we need something more to talk about, more general problems about how to get.",
                    "label": 0
                },
                {
                    "sent": "Efficient and tractable algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the general recipe for dealing with how to get these relaxation is that we start with what we know.",
                    "label": 1
                },
                {
                    "sent": "So we start with the sequential order marker relaxation.",
                    "label": 0
                },
                {
                    "sent": "We write it down, we formulate it, and then the next step is we try to get rid of the two trees which are kind of the main hindrance in terms of computation and we get rid of them by going to an appropriate upper bound.",
                    "label": 1
                },
                {
                    "sent": "And so there are a few techniques generic tools to do that were listed in the paper and a lot of that can actually be done even for the set of strategies.",
                    "label": 0
                },
                {
                    "sent": "Right and then we need to ensure that this relaxation that we get satisfies the admissibility condition and typically the one means min Max theorem is a useful tool for doing this step, and then we're all set, so the algorithm is going to be solving this optimization problem and the hope in all this is that we are able to relax too and good enough relaxation such that this optimization problem is easy to solve.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me give you some direct examples that probably will shed more light.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first example that I'd like to talk about is a problem of binary sequence prediction, but where we are trying to compete with maximum posterior remodels.",
                    "label": 0
                },
                {
                    "sent": "So specifically we are interested in the set of instances binary, so 0, one, and the loss that we are interested in is the absolute loss, and F is basically a number between zero and one, so this can be seen as this loss can equally be seen as the expected 01 loss when we use a prediction F. And the set of strategies that we are interested in competing with is this guy out here.",
                    "label": 0
                },
                {
                    "sent": "So it's characterized by or it's parameterized by Alpha and beta, where Alpha some number larger than one, and beta is between one and see some beta.",
                    "label": 0
                },
                {
                    "sent": "And so to give you a notion of what this equation here represents, basically what this is telling us is we're interested in competing with maximum posterior model, where our likelihood is given by the balloon redistribution, and our prior is the beta prior with Alpha and beta as parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so for this we can basically write down the sequential Rademacher relaxation and with a little bit of work what we can arrive at is this relaxation out here and an associated algorithm automatically pops out, which is basically.",
                    "label": 0
                },
                {
                    "sent": "Compute cutey by this equation out here.",
                    "label": 0
                },
                {
                    "sent": "So notice that this is basically has two optimization problems, super Alpha and beta or this term and this term.",
                    "label": 0
                },
                {
                    "sent": "So it looks hideous.",
                    "label": 0
                },
                {
                    "sent": "It's nonconvex, but Luckily due to the form here, it turns out that this can actually be done in polynomial time, because this is basically like sum of ratios.",
                    "label": 0
                },
                {
                    "sent": "And so specifically.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can get an algorithm whose time complexity is order T log T, so at time T the time complexity steel of T and the expected regret bound that we get for this problem is of order square root T. And when we compare for instance with the experts algorithm, so you can discretize Alpha and beta and try to run experts over this.",
                    "label": 1
                },
                {
                    "sent": "So you'll get a slightly worse regret bound which has an extra lock T. And you also get a verse computational complexity over the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the next.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example that we look at is autoregressive model.",
                    "label": 1
                },
                {
                    "sent": "So in the auto regressive model, so we're going to consider F&Z to be numbers between minus one and one.",
                    "label": 0
                },
                {
                    "sent": "So we're going to try and predict the number between minus one and one and then the true numbers really is revealed and the loss that we consider is the squared loss.",
                    "label": 0
                },
                {
                    "sent": "But really it can also be any other convex loss.",
                    "label": 0
                },
                {
                    "sent": "Provided us with convex and Lipschitz OK, so the set of strategies that we are considering is the set of autoregressive model where we allow dependence on all the full dependence on history.",
                    "label": 1
                },
                {
                    "sent": "So it can depend all the way from time one to time T -- 1 at time T and basically the restriction that we put is in the weights that we take.",
                    "label": 0
                },
                {
                    "sent": "So we for this example I'm going to assume that the L1 norm of the weights is bounded by 1.",
                    "label": 0
                },
                {
                    "sent": "OK, so for this again we can write down the sequential market relaxation and which is just a little bit of massage ING of the relaxation.",
                    "label": 0
                },
                {
                    "sent": "We can move to this upper bound which is given here which is so 80 here is is either two times the Rademacher random variable.",
                    "label": 0
                },
                {
                    "sent": "Whenever S is larger than T, so for the future, and if it is, if it's smaller than T, it's basically minus the instance that we've seen and the relaxation is given out here and from this relaxation you can.",
                    "label": 0
                },
                {
                    "sent": "Easily derive this randomized algorithm again.",
                    "label": 0
                },
                {
                    "sent": "So basically what this randomized algorithm is doing is at each time T we basically draw epsilon future random variables epsilon T plus one through capital T. So these are America random variables and we need to compute this quantity out here.",
                    "label": 0
                },
                {
                    "sent": "So basically is Max between these two quantities out here and so this is easy enough to compute and so specifically we can get a regret bound of order square root T for this relaxation, which is easy to check.",
                    "label": 0
                },
                {
                    "sent": "And the time complexity that we incur per round, unfortunately, is of order P. But it turns out that by replacing these randomly, actually by replacing these random variables here by standard normal variables and doing a little bit more massage ING, you can actually get the same regret bound of order square root T. But you can get an improved time complexity of order, one per row.",
                    "label": 1
                },
                {
                    "sent": "OK, so these are two specific exam.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apples in the paper.",
                    "label": 0
                },
                {
                    "sent": "We also talk about other models, so competing with, for instance finite or Marco strategies and auto regressive models where we have a finite loop back, so we look at only the K previous observations or auto regressive model with full dependence on history.",
                    "label": 1
                },
                {
                    "sent": "But other geometric restrictions on Theta.",
                    "label": 1
                },
                {
                    "sent": "So here we looked at L1 norm.",
                    "label": 0
                },
                {
                    "sent": "You can look at other other type of geometry, maybe other LP norms and other geometric restrictions on Theta.",
                    "label": 0
                },
                {
                    "sent": "Another interesting thing that we also consider is.",
                    "label": 0
                },
                {
                    "sent": "Strategies when we can where basically depends on the past, but in a compressed way.",
                    "label": 0
                },
                {
                    "sent": "So it basically compresses history through a particular into a particular space, and then it depends only on that compressed format.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the sufficient statistic for the strategy.",
                    "label": 0
                },
                {
                    "sent": "And we also look at competing with families of regularised least squares algorithms.",
                    "label": 1
                },
                {
                    "sent": "So the usual notion of regret you are interested in looking at what's the regret with respect to, say, the best linear predictor?",
                    "label": 1
                },
                {
                    "sent": "But maybe the best linear predictor is not good enough, but maybe it's known that if under the magical parameters the regularised least square algorithm actually does well for these for certain types of problems, and in that case we'd like to basically directly compete with the family of regularly square algorithm, and we also have an example where we try to compete with family of follow the perturb leader algorithm.",
                    "label": 0
                },
                {
                    "sent": "In both of these, basically you have a starting point of where your vector is an, then you have a schedule for your step sizes, and we need to compete with all possible such step sizes.",
                    "label": 0
                },
                {
                    "sent": "And starting point W not.",
                    "label": 0
                },
                {
                    "sent": "And basically if you're looking at the at the usual notion of regret, it basically is just says that forget the step size and it's basically just says look at the starting point and you want to compare yourself with that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so in terms of further directions, so we'd like to also kind of consider more general classes of Bayesian algorithms indexed by some family of priors.",
                    "label": 1
                },
                {
                    "sent": "And basically it takes this family of priors, takes a likelihood model and it updates and how well we can we do with that is there is more general result that we can get there and we'd like to today.",
                    "label": 0
                },
                {
                    "sent": "In the first talk a lot talked about a model that wasn't totally the worst case, so there was a particular way in which data was generated.",
                    "label": 0
                },
                {
                    "sent": "So there was some notion of.",
                    "label": 0
                },
                {
                    "sent": "Predictability and what I talked about here was in some sense the worst case scenario, and what we'd like to do is develop adaptive algorithms that can adapt to both.",
                    "label": 0
                },
                {
                    "sent": "So if the case if it is the case that the instances are a little more predictive under some right notion of predictability, then we get better, then we get a better regret bound.",
                    "label": 0
                },
                {
                    "sent": "We'd like to also some of the examples that were given here, so we have.",
                    "label": 1
                },
                {
                    "sent": "We do give algorithms, but they're not efficient enough, so we'd like to see if we can derive more efficient algorithms while competing with the family of Regularising.",
                    "label": 1
                },
                {
                    "sent": "These squares and competing with the family of follow the regularised leader algorithms.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}