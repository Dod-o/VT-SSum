{
    "id": "55knmoggn4xyksg5ogr324w5iqizutob",
    "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations",
    "info": {
        "author": [
            "Felix Hill, Computer Laboratory, University of Cambridge"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_hill_goldilocks_principle/",
    "segmentation": [
        [
            "So it turns out that.",
            "Pretty difficult to understand language if we only consider sentence is in isolation.",
            "Can see."
        ],
        [
            "The following highly realistic example.",
            "So Goldilocks is asleep in her bed and she says what the heck should I do?",
            "Now, as a human, I think it's pretty difficult to based on the information you've got there, to think of any plausible advice for her.",
            "Anne and of course, for the personal assistant on her telephone, it's really difficult, right?",
            "The chances are is only going to come back with some sort of platitude."
        ],
        [
            "Now obviously a situation like that is not going to be very happy users, so we want to try a different approach.",
            "It may."
        ],
        [
            "In a situation instead in which Goldilocks is able to provide some something a little bit more context.",
            "So she says I'm hiding under the covers.",
            "There's someone at the door.",
            "It looks like there's what the heck should I do?",
            "Now in this situation, we can imagine potentially building models which are able to take into account that context.",
            "Anne Anne too.",
            "Based on the information in that context to come up with some sort of plausible suggestion for what she can do, which will hopefully lead to more."
        ],
        [
            "Satisfied customers or users.",
            "Say the takeaway from this is that it's really important to think about context when trying to build models that compute the meaning of sentences.",
            "And so in order to test the ability of models to do that."
        ],
        [
            "We developed a new test.",
            "It's called the Children's books test, so it consists of many questions and in an individual question."
        ],
        [
            "There are four parts, so the first part is what we call the context.",
            "All of this data comes from children's books, and the context is just twenty consecutive sentences.",
            "Then we have."
        ],
        [
            "Of what we call the query.",
            "So this is the 21st sentence in the book Anne from the query away."
        ],
        [
            "It is removed.",
            "OK, the next part of the question is that."
        ],
        [
            "Candidates, so it's a multiple choice test.",
            "And the important thing to note about the candidates is that they also appear in the context.",
            "So every one of the candidates is also in the context."
        ],
        [
            "There were ten candidates."
        ],
        [
            "And then we have the answer.",
            "The answer is also always in the."
        ],
        [
            "Context, right?",
            "So that."
        ],
        [
            "This book test is pretty extensive.",
            "We took data from 98 books, so that leaves 670,000 questions.",
            "We've also got a reasonable size validation set and then a test set and it covers such literary classics as Treasure Island, The Jungle Book, and Alice in Wonderland.",
            "Say."
        ],
        [
            "Another important detail about the children's books test.",
            "There are questions divided into four types, and this is one of the most important aspects of the data set.",
            "The first type is preposition, so in this case all that we've done is removed.",
            "A preposition from the query.",
            "We've also got questions that include missing verbs.",
            "Questions that include missing common nouns and then questions that include missing named entities.",
            "And it's important to note here that the candidates that usually of the same type is the missing word.",
            "So where possible, we selected candidates of the same type as the missing word.",
            "To be sure that they were as similar as the missing word.",
            "Of course, that wasn't always possible, especially in the case of named entities, but then there's likely not to be 10 named entities in the context, so in that situation we just backed off to common nouns.",
            "To collect a few more candidates."
        ],
        [
            "This aspect of the children's book tests illustrates quite an important general principle which might be related to things that Chris said earlier, and I think it's going to be important going forward in terms of language technology.",
            "So if we just sample randomly from naturally occurring language to create our evaluations, we're going to be sort of disproportionately focusing on very frequent language, but it's often the low frequency language which really has kind of the most important meaning, or maybe covers those really important cases for communication.",
            "The sorts of things I'm thinking over things like the the syntax is sort of intensive examples at Chris looked earlier and it may well be that if we can just wait our evaluations to focus slightly more on those sorts of cases as well, we can develop.",
            "We can encourage the development of models which have the flexibility to deal with both frequent language animefreak frequent language and that could lead to the right blend."
        ],
        [
            "So anyway, what does the CVT add in relation to what already existed?",
            "So there's a really good data set is a well known and quite important in the history of language modeling called the MSR sentence completion Challenge.",
            "And there's a lot of similarities.",
            "The differences, of course that we have a context here for the for the language, so I've sort of tried to argue in favor of modeling language in context, and we sort of extend the MSR sentence completion challenge to include the context well before the sentence in question.",
            "We also divide into the different word types and we include many more many more test questions.",
            "You can see similarities between other tests are machine comprehension like the MC test.",
            "Of course we've got many more, many more training examples and test examples then in that case, so this was a handwritten hand developed testing machine comprehension says it's not really possible to train statistical models on this data set.",
            "And it's also kind of quite obvious similarities with the CNN Daily Mail question and answer data set, so this wasn't available when we did the work.",
            "We're going to describe here, but there's obviously these two are very complementary and what they don't do in that data set is cover different prediction of different word types and all of their entities, so it just focuses on named entities and all of those are anonymized.",
            "So one of the nice things about the CBT is that in theory you can apply potentially some world knowledge games from other influence, maybe from images or from some dictionary or something, and apply that to the task.",
            "So anyway, we."
        ],
        [
            "And it's really quite important, and this is the main one of the main motivations for including the candidates in this data set was that we wanted to get a good handle on what humans can do, 'cause we think it's important to have an upper bound and also to understand the problem."
        ],
        [
            "So if you give humans just the query, so that's just the sentence with the missing word, then this is how they do, right?",
            "So they're not bad at predicting prepositions their best at predicting verbs are little bit worse at predicting named entities.",
            "But if you give them the."
        ],
        [
            "Context as well.",
            "It's a little bit laborious, but they go, they can go through the context, make a prediction and what we see is in fact that they're just as good at predicting named entities as common nouns or verbs.",
            "Not so good at predicting prepositions in that case, so that the prepositions thing is probably because in many cases a couple of prepositions are actually correct.",
            "So."
        ],
        [
            "How to machines doing comparison?"
        ],
        [
            "Six simple baselines.",
            "So if you just train and LSD on the training set, unstructured.",
            "And then at Test time you just feed it forward, but only feed it the query words and then compute the probability of the query with all the different candidates inserted.",
            "So that's that's the blue bar.",
            "So what we can see is that it actually beats humans on prepositions.",
            "But it's a lot worse than humans on named entities, and it's not so good, and common nouns as well.",
            "And then if the test time if we feed forward through the whole context and the query, we do slightly better, especially slightly better on the common nouns are named entities, but what we can see is that even though an LSM is designed to mitigate in some sense the problem of long term dependencies in the input, it's still not able to really extract all the useful information from the context in order to get up anywhere near human performance on the named entities.",
            "And finally, there's just an N gram language model.",
            "There you can see slightly lower performance all around.",
            "OK so also."
        ],
        [
            "Solution to try and improve the performance, especially on named entities was memory network which was presented at IKEA last year.",
            "So our application of a memory network here."
        ],
        [
            "Is pretty simple.",
            "We first embed in some way."
        ],
        [
            "The context into what we call the explicit memory.",
            "So this is a computation for embedding in a distributed representation.",
            "The content in the 20 preceding sentences we embed the."
        ],
        [
            "In a distributed representation.",
            "And then."
        ],
        [
            "An conditioned or combining the query and view of the explicit memory, we use that to compute a score for all the locations in the memory, so this could be considered as a type of attention mechanism over the locations in the memory."
        ],
        [
            "Based on that, we do a weighted sum to give one view one single output vector of the memory.",
            "And then here we."
        ],
        [
            "And actually iterate this process right?",
            "So we can take that.",
            "Combine it again with the query representation, make another computation of where to look in the memory and in theory get an updated view of the memory at that point."
        ],
        [
            "And then finally we can recombine the single view of the memory with the query representation to produce a prediction layer, and then using an output softmax which we heard a lot about in the previous talk.",
            "We can generate a problem with this probability distribution over possible answers and select the most probable candidate.",
            "But"
        ],
        [
            "A bit more detail, right?",
            "So how do we get these distributed representations of the content?",
            "And this is where we found some really interesting variation in the models.",
            "So we we in particular we didn't do anything really fancy, but we looked at three ways of representing the context in the memory.",
            "The 1st."
        ],
        [
            "One is what we call lexical memory, so this is kind of a naive approach.",
            "But essentially all we do is we take as the query the word immediately preceding the missing word, and then in the explicit memory we add in distributed represent."
        ],
        [
            "Nations, embeddings of all of the other words going back in the context.",
            "OK, in this case the memory haunts the updating.",
            "Iterating looks at look at the memory is particularly important."
        ],
        [
            "OK, in the window memory case, our memories correspond to five word chunks, so they just.",
            "Anne.",
            "Pass across a window A5 word window across the context, and those memories go into the explicit memory part of the model and then a five word window surrounding the missing word goes in as the query representation."
        ],
        [
            "And finally, in the in the sentence memory case.",
            "We simply embed the whole of each individual sentence in a distributed representation.",
            "So in this case we have 20 exactly 20 things in the explicit memory, and the query is a representation of the sentence with the missing word."
        ],
        [
            "And there's one final detail.",
            "Which turned out to make a really quite significant difference to our results.",
            "So this is, this is what we call self supervision for memory retrieval.",
            "So optimizing this whole network is challenging.",
            "In particular, the model needs to learn to interpret the information coming in and also needs to learn where to look in its memory in order to get the useful information out.",
            "And what we found is that if we developed a heuristic for giving the model a slightly stronger signal for where to look in its own memory.",
            "Based only on the information in the training data, then with that heuristic we could get better performance.",
            "Say it works by using a deterministic method.",
            "To determine what is the correct memory in each question?",
            "Just based on the information in the training data and the heuristic is simply that we take as the correct memory the memory that covers the correct answer.",
            "But of course in many questions there are multiple memories that contain the correct answer, so in those cases what we do is we take this part of the model and we use it to score produced the initial distribution of probabilities over the memories and amongst the ones which contain the correct answer, we take the highest scoring memory that then tells the model what the correct memory is, and then we come back propagating train this part of the network as a supervised memory just in terms of for weights which tell us where to access the memory, and then we can also then feed through the results of that to produce predictions."
        ],
        [
            "Default."
        ],
        [
            "So I'll just quickly go over the results so you can see the impact of all of these variations."
        ],
        [
            "OK, so these are the baselines that I talked about originally.",
            "These are classical language model STM models.",
            "In the sentence memory case, we found that performance is really low, right?",
            "So it's clearly not practical or carrying enough useful information to give these models view of a whole sentence in a distributed representation.",
            "OK, now in the word memory case performance starts to resemble actually quite similarly just a classical language model, right?",
            "So in this case performance is good, on prepositions, it's good on verbs, but it's it's not really done a lot to improve on the baselines.",
            "In common nouns and named entities.",
            "However, when we start to look at the window memories, we start to see a totally different distribution of performance, right?",
            "So these models are not so good at prepositions, but they're starting to get up well above the alternatives at predicting named entities.",
            "When we add in the sort of the heuristic trick for accessing giving a stronger signal to the model in terms of where to access the memory.",
            "We actually get a lot better performance now, so now we're approaching well, getting a lot close to human performance on predicting named entities.",
            "So we could say that if you consider that word memories are a little bit smaller, they don't contain the loan amount of information for the model and sensors memories are a little bit too big.",
            "Becausw.",
            "Perhaps we're trying to sort of represent too much information in one single distributed representation in the memory.",
            "Then if we take window memories, they may well be just right in terms of the amount of the information that they encouraged.",
            "And this ties in a little bit.",
            "Actually what Chris was saying as well about different levels of representation of a sentence in the fact that we might want to come up with models which can produce good representations of chunks of sentences and then find different ways of combining those chunks."
        ],
        [
            "So to corroborate our results, we thought it was important to evaluate on one additional data set, and so we decided to try this on the deep mind question.",
            "Answering data set which came available towards the end of doing this work.",
            "One of the nice things was that this this data set naturally focuses on named entities.",
            "So we thought it would be a good test of the same if we could recreate the same phenom."
        ],
        [
            "What we see is that if we just have a deep policy on this model, performance is round about 55 to 60%.",
            "With the contextual STM.",
            "So this is the model is implemented by the people who created the paper originally and this is a model which again computes attention looking at representations of the context.",
            "It works by computing bidirectional RNN views of the context.",
            "So this model performs so well and then the impatient reader, which is a variant of that model, performed slightly better.",
            "Now one model with Window Memories doesn't perform as well as those.",
            "But once we add in what we call the self supervision then it starts to extend the state of the art.",
            "And then we did a few other things to get the state of the art even better.",
            "So we did a bit of an songling.",
            "So in the in the DeepMind paper I think they had drop out in their model.",
            "We didn't implement drop out, but we just in stumbled 11 models and we got the state of the art a little bit higher.",
            "And then finally a trick, which is a sort of quirk of the particular data set, but for anyone trying this data set and wanting to beat the state of the art is something you should definitely do if you just.",
            "If you just exclude a candidate answer, if it already appears in the query, then you improve your likely to improve the overall score of your model right?",
            "So it works on the assumption that 2 words are unlikely to occur in the same sentence, but often they are a candidate.",
            "So just by discounting that, I imagine that they're actually they actually got to have a slightly higher than expected chance of being selected by the model because they already exist in the query.",
            "So if you just count those, you actually increase the performance of the model."
        ],
        [
            "So.",
            "To conclude.",
            "Hopefully I've made a good case with the fact that context is really important when modeling language, right?",
            "So at the moment huge amount of language models work on a sentence by sentence basis, but the future of language modeling is surely or even machine comprehension machine translation.",
            "They surely going to need for us to find better ways of interpreting the meaning of a sentence within the particular document or dialogue in which it occurs.",
            "It's also important to have evaluations which distinguish between frequent and the less frequent words.",
            "Often the less frequent ones are the ones that carry most of the semantics, and if we do things like perplexity where we average overall words, then the performance on those rare words is kind of getting lost, right?",
            "We're not putting a focus on that.",
            "And hopefully you you're convinced that the children's book Test is a useful resource for testing both of these things.",
            "And if you are, then you can download it from the Facebook web page.",
            "So.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it turns out that.",
                    "label": 0
                },
                {
                    "sent": "Pretty difficult to understand language if we only consider sentence is in isolation.",
                    "label": 0
                },
                {
                    "sent": "Can see.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The following highly realistic example.",
                    "label": 0
                },
                {
                    "sent": "So Goldilocks is asleep in her bed and she says what the heck should I do?",
                    "label": 1
                },
                {
                    "sent": "Now, as a human, I think it's pretty difficult to based on the information you've got there, to think of any plausible advice for her.",
                    "label": 0
                },
                {
                    "sent": "Anne and of course, for the personal assistant on her telephone, it's really difficult, right?",
                    "label": 0
                },
                {
                    "sent": "The chances are is only going to come back with some sort of platitude.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now obviously a situation like that is not going to be very happy users, so we want to try a different approach.",
                    "label": 0
                },
                {
                    "sent": "It may.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a situation instead in which Goldilocks is able to provide some something a little bit more context.",
                    "label": 0
                },
                {
                    "sent": "So she says I'm hiding under the covers.",
                    "label": 0
                },
                {
                    "sent": "There's someone at the door.",
                    "label": 1
                },
                {
                    "sent": "It looks like there's what the heck should I do?",
                    "label": 1
                },
                {
                    "sent": "Now in this situation, we can imagine potentially building models which are able to take into account that context.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne too.",
                    "label": 0
                },
                {
                    "sent": "Based on the information in that context to come up with some sort of plausible suggestion for what she can do, which will hopefully lead to more.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Satisfied customers or users.",
                    "label": 0
                },
                {
                    "sent": "Say the takeaway from this is that it's really important to think about context when trying to build models that compute the meaning of sentences.",
                    "label": 0
                },
                {
                    "sent": "And so in order to test the ability of models to do that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We developed a new test.",
                    "label": 0
                },
                {
                    "sent": "It's called the Children's books test, so it consists of many questions and in an individual question.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are four parts, so the first part is what we call the context.",
                    "label": 0
                },
                {
                    "sent": "All of this data comes from children's books, and the context is just twenty consecutive sentences.",
                    "label": 1
                },
                {
                    "sent": "Then we have.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of what we call the query.",
                    "label": 0
                },
                {
                    "sent": "So this is the 21st sentence in the book Anne from the query away.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is removed.",
                    "label": 0
                },
                {
                    "sent": "OK, the next part of the question is that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Candidates, so it's a multiple choice test.",
                    "label": 1
                },
                {
                    "sent": "And the important thing to note about the candidates is that they also appear in the context.",
                    "label": 0
                },
                {
                    "sent": "So every one of the candidates is also in the context.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There were ten candidates.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have the answer.",
                    "label": 0
                },
                {
                    "sent": "The answer is also always in the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Context, right?",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This book test is pretty extensive.",
                    "label": 1
                },
                {
                    "sent": "We took data from 98 books, so that leaves 670,000 questions.",
                    "label": 0
                },
                {
                    "sent": "We've also got a reasonable size validation set and then a test set and it covers such literary classics as Treasure Island, The Jungle Book, and Alice in Wonderland.",
                    "label": 0
                },
                {
                    "sent": "Say.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another important detail about the children's books test.",
                    "label": 0
                },
                {
                    "sent": "There are questions divided into four types, and this is one of the most important aspects of the data set.",
                    "label": 0
                },
                {
                    "sent": "The first type is preposition, so in this case all that we've done is removed.",
                    "label": 0
                },
                {
                    "sent": "A preposition from the query.",
                    "label": 0
                },
                {
                    "sent": "We've also got questions that include missing verbs.",
                    "label": 0
                },
                {
                    "sent": "Questions that include missing common nouns and then questions that include missing named entities.",
                    "label": 0
                },
                {
                    "sent": "And it's important to note here that the candidates that usually of the same type is the missing word.",
                    "label": 0
                },
                {
                    "sent": "So where possible, we selected candidates of the same type as the missing word.",
                    "label": 0
                },
                {
                    "sent": "To be sure that they were as similar as the missing word.",
                    "label": 0
                },
                {
                    "sent": "Of course, that wasn't always possible, especially in the case of named entities, but then there's likely not to be 10 named entities in the context, so in that situation we just backed off to common nouns.",
                    "label": 0
                },
                {
                    "sent": "To collect a few more candidates.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This aspect of the children's book tests illustrates quite an important general principle which might be related to things that Chris said earlier, and I think it's going to be important going forward in terms of language technology.",
                    "label": 0
                },
                {
                    "sent": "So if we just sample randomly from naturally occurring language to create our evaluations, we're going to be sort of disproportionately focusing on very frequent language, but it's often the low frequency language which really has kind of the most important meaning, or maybe covers those really important cases for communication.",
                    "label": 0
                },
                {
                    "sent": "The sorts of things I'm thinking over things like the the syntax is sort of intensive examples at Chris looked earlier and it may well be that if we can just wait our evaluations to focus slightly more on those sorts of cases as well, we can develop.",
                    "label": 0
                },
                {
                    "sent": "We can encourage the development of models which have the flexibility to deal with both frequent language animefreak frequent language and that could lead to the right blend.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So anyway, what does the CVT add in relation to what already existed?",
                    "label": 0
                },
                {
                    "sent": "So there's a really good data set is a well known and quite important in the history of language modeling called the MSR sentence completion Challenge.",
                    "label": 1
                },
                {
                    "sent": "And there's a lot of similarities.",
                    "label": 0
                },
                {
                    "sent": "The differences, of course that we have a context here for the for the language, so I've sort of tried to argue in favor of modeling language in context, and we sort of extend the MSR sentence completion challenge to include the context well before the sentence in question.",
                    "label": 0
                },
                {
                    "sent": "We also divide into the different word types and we include many more many more test questions.",
                    "label": 0
                },
                {
                    "sent": "You can see similarities between other tests are machine comprehension like the MC test.",
                    "label": 0
                },
                {
                    "sent": "Of course we've got many more, many more training examples and test examples then in that case, so this was a handwritten hand developed testing machine comprehension says it's not really possible to train statistical models on this data set.",
                    "label": 0
                },
                {
                    "sent": "And it's also kind of quite obvious similarities with the CNN Daily Mail question and answer data set, so this wasn't available when we did the work.",
                    "label": 0
                },
                {
                    "sent": "We're going to describe here, but there's obviously these two are very complementary and what they don't do in that data set is cover different prediction of different word types and all of their entities, so it just focuses on named entities and all of those are anonymized.",
                    "label": 0
                },
                {
                    "sent": "So one of the nice things about the CBT is that in theory you can apply potentially some world knowledge games from other influence, maybe from images or from some dictionary or something, and apply that to the task.",
                    "label": 0
                },
                {
                    "sent": "So anyway, we.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's really quite important, and this is the main one of the main motivations for including the candidates in this data set was that we wanted to get a good handle on what humans can do, 'cause we think it's important to have an upper bound and also to understand the problem.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you give humans just the query, so that's just the sentence with the missing word, then this is how they do, right?",
                    "label": 0
                },
                {
                    "sent": "So they're not bad at predicting prepositions their best at predicting verbs are little bit worse at predicting named entities.",
                    "label": 1
                },
                {
                    "sent": "But if you give them the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Context as well.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit laborious, but they go, they can go through the context, make a prediction and what we see is in fact that they're just as good at predicting named entities as common nouns or verbs.",
                    "label": 1
                },
                {
                    "sent": "Not so good at predicting prepositions in that case, so that the prepositions thing is probably because in many cases a couple of prepositions are actually correct.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to machines doing comparison?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Six simple baselines.",
                    "label": 0
                },
                {
                    "sent": "So if you just train and LSD on the training set, unstructured.",
                    "label": 0
                },
                {
                    "sent": "And then at Test time you just feed it forward, but only feed it the query words and then compute the probability of the query with all the different candidates inserted.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the blue bar.",
                    "label": 0
                },
                {
                    "sent": "So what we can see is that it actually beats humans on prepositions.",
                    "label": 0
                },
                {
                    "sent": "But it's a lot worse than humans on named entities, and it's not so good, and common nouns as well.",
                    "label": 1
                },
                {
                    "sent": "And then if the test time if we feed forward through the whole context and the query, we do slightly better, especially slightly better on the common nouns are named entities, but what we can see is that even though an LSM is designed to mitigate in some sense the problem of long term dependencies in the input, it's still not able to really extract all the useful information from the context in order to get up anywhere near human performance on the named entities.",
                    "label": 0
                },
                {
                    "sent": "And finally, there's just an N gram language model.",
                    "label": 0
                },
                {
                    "sent": "There you can see slightly lower performance all around.",
                    "label": 0
                },
                {
                    "sent": "OK so also.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution to try and improve the performance, especially on named entities was memory network which was presented at IKEA last year.",
                    "label": 0
                },
                {
                    "sent": "So our application of a memory network here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "We first embed in some way.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The context into what we call the explicit memory.",
                    "label": 1
                },
                {
                    "sent": "So this is a computation for embedding in a distributed representation.",
                    "label": 0
                },
                {
                    "sent": "The content in the 20 preceding sentences we embed the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a distributed representation.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An conditioned or combining the query and view of the explicit memory, we use that to compute a score for all the locations in the memory, so this could be considered as a type of attention mechanism over the locations in the memory.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on that, we do a weighted sum to give one view one single output vector of the memory.",
                    "label": 0
                },
                {
                    "sent": "And then here we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually iterate this process right?",
                    "label": 0
                },
                {
                    "sent": "So we can take that.",
                    "label": 0
                },
                {
                    "sent": "Combine it again with the query representation, make another computation of where to look in the memory and in theory get an updated view of the memory at that point.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then finally we can recombine the single view of the memory with the query representation to produce a prediction layer, and then using an output softmax which we heard a lot about in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "We can generate a problem with this probability distribution over possible answers and select the most probable candidate.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A bit more detail, right?",
                    "label": 0
                },
                {
                    "sent": "So how do we get these distributed representations of the content?",
                    "label": 0
                },
                {
                    "sent": "And this is where we found some really interesting variation in the models.",
                    "label": 0
                },
                {
                    "sent": "So we we in particular we didn't do anything really fancy, but we looked at three ways of representing the context in the memory.",
                    "label": 1
                },
                {
                    "sent": "The 1st.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is what we call lexical memory, so this is kind of a naive approach.",
                    "label": 0
                },
                {
                    "sent": "But essentially all we do is we take as the query the word immediately preceding the missing word, and then in the explicit memory we add in distributed represent.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nations, embeddings of all of the other words going back in the context.",
                    "label": 0
                },
                {
                    "sent": "OK, in this case the memory haunts the updating.",
                    "label": 0
                },
                {
                    "sent": "Iterating looks at look at the memory is particularly important.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, in the window memory case, our memories correspond to five word chunks, so they just.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Pass across a window A5 word window across the context, and those memories go into the explicit memory part of the model and then a five word window surrounding the missing word goes in as the query representation.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, in the in the sentence memory case.",
                    "label": 1
                },
                {
                    "sent": "We simply embed the whole of each individual sentence in a distributed representation.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have 20 exactly 20 things in the explicit memory, and the query is a representation of the sentence with the missing word.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's one final detail.",
                    "label": 0
                },
                {
                    "sent": "Which turned out to make a really quite significant difference to our results.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is what we call self supervision for memory retrieval.",
                    "label": 1
                },
                {
                    "sent": "So optimizing this whole network is challenging.",
                    "label": 0
                },
                {
                    "sent": "In particular, the model needs to learn to interpret the information coming in and also needs to learn where to look in its memory in order to get the useful information out.",
                    "label": 0
                },
                {
                    "sent": "And what we found is that if we developed a heuristic for giving the model a slightly stronger signal for where to look in its own memory.",
                    "label": 0
                },
                {
                    "sent": "Based only on the information in the training data, then with that heuristic we could get better performance.",
                    "label": 0
                },
                {
                    "sent": "Say it works by using a deterministic method.",
                    "label": 1
                },
                {
                    "sent": "To determine what is the correct memory in each question?",
                    "label": 0
                },
                {
                    "sent": "Just based on the information in the training data and the heuristic is simply that we take as the correct memory the memory that covers the correct answer.",
                    "label": 0
                },
                {
                    "sent": "But of course in many questions there are multiple memories that contain the correct answer, so in those cases what we do is we take this part of the model and we use it to score produced the initial distribution of probabilities over the memories and amongst the ones which contain the correct answer, we take the highest scoring memory that then tells the model what the correct memory is, and then we come back propagating train this part of the network as a supervised memory just in terms of for weights which tell us where to access the memory, and then we can also then feed through the results of that to produce predictions.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Default.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll just quickly go over the results so you can see the impact of all of these variations.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so these are the baselines that I talked about originally.",
                    "label": 0
                },
                {
                    "sent": "These are classical language model STM models.",
                    "label": 0
                },
                {
                    "sent": "In the sentence memory case, we found that performance is really low, right?",
                    "label": 1
                },
                {
                    "sent": "So it's clearly not practical or carrying enough useful information to give these models view of a whole sentence in a distributed representation.",
                    "label": 0
                },
                {
                    "sent": "OK, now in the word memory case performance starts to resemble actually quite similarly just a classical language model, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case performance is good, on prepositions, it's good on verbs, but it's it's not really done a lot to improve on the baselines.",
                    "label": 0
                },
                {
                    "sent": "In common nouns and named entities.",
                    "label": 1
                },
                {
                    "sent": "However, when we start to look at the window memories, we start to see a totally different distribution of performance, right?",
                    "label": 0
                },
                {
                    "sent": "So these models are not so good at prepositions, but they're starting to get up well above the alternatives at predicting named entities.",
                    "label": 0
                },
                {
                    "sent": "When we add in the sort of the heuristic trick for accessing giving a stronger signal to the model in terms of where to access the memory.",
                    "label": 0
                },
                {
                    "sent": "We actually get a lot better performance now, so now we're approaching well, getting a lot close to human performance on predicting named entities.",
                    "label": 0
                },
                {
                    "sent": "So we could say that if you consider that word memories are a little bit smaller, they don't contain the loan amount of information for the model and sensors memories are a little bit too big.",
                    "label": 0
                },
                {
                    "sent": "Becausw.",
                    "label": 0
                },
                {
                    "sent": "Perhaps we're trying to sort of represent too much information in one single distributed representation in the memory.",
                    "label": 0
                },
                {
                    "sent": "Then if we take window memories, they may well be just right in terms of the amount of the information that they encouraged.",
                    "label": 0
                },
                {
                    "sent": "And this ties in a little bit.",
                    "label": 0
                },
                {
                    "sent": "Actually what Chris was saying as well about different levels of representation of a sentence in the fact that we might want to come up with models which can produce good representations of chunks of sentences and then find different ways of combining those chunks.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to corroborate our results, we thought it was important to evaluate on one additional data set, and so we decided to try this on the deep mind question.",
                    "label": 0
                },
                {
                    "sent": "Answering data set which came available towards the end of doing this work.",
                    "label": 0
                },
                {
                    "sent": "One of the nice things was that this this data set naturally focuses on named entities.",
                    "label": 0
                },
                {
                    "sent": "So we thought it would be a good test of the same if we could recreate the same phenom.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we see is that if we just have a deep policy on this model, performance is round about 55 to 60%.",
                    "label": 0
                },
                {
                    "sent": "With the contextual STM.",
                    "label": 0
                },
                {
                    "sent": "So this is the model is implemented by the people who created the paper originally and this is a model which again computes attention looking at representations of the context.",
                    "label": 0
                },
                {
                    "sent": "It works by computing bidirectional RNN views of the context.",
                    "label": 0
                },
                {
                    "sent": "So this model performs so well and then the impatient reader, which is a variant of that model, performed slightly better.",
                    "label": 0
                },
                {
                    "sent": "Now one model with Window Memories doesn't perform as well as those.",
                    "label": 0
                },
                {
                    "sent": "But once we add in what we call the self supervision then it starts to extend the state of the art.",
                    "label": 0
                },
                {
                    "sent": "And then we did a few other things to get the state of the art even better.",
                    "label": 0
                },
                {
                    "sent": "So we did a bit of an songling.",
                    "label": 0
                },
                {
                    "sent": "So in the in the DeepMind paper I think they had drop out in their model.",
                    "label": 0
                },
                {
                    "sent": "We didn't implement drop out, but we just in stumbled 11 models and we got the state of the art a little bit higher.",
                    "label": 0
                },
                {
                    "sent": "And then finally a trick, which is a sort of quirk of the particular data set, but for anyone trying this data set and wanting to beat the state of the art is something you should definitely do if you just.",
                    "label": 0
                },
                {
                    "sent": "If you just exclude a candidate answer, if it already appears in the query, then you improve your likely to improve the overall score of your model right?",
                    "label": 0
                },
                {
                    "sent": "So it works on the assumption that 2 words are unlikely to occur in the same sentence, but often they are a candidate.",
                    "label": 0
                },
                {
                    "sent": "So just by discounting that, I imagine that they're actually they actually got to have a slightly higher than expected chance of being selected by the model because they already exist in the query.",
                    "label": 0
                },
                {
                    "sent": "So if you just count those, you actually increase the performance of the model.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "Hopefully I've made a good case with the fact that context is really important when modeling language, right?",
                    "label": 0
                },
                {
                    "sent": "So at the moment huge amount of language models work on a sentence by sentence basis, but the future of language modeling is surely or even machine comprehension machine translation.",
                    "label": 0
                },
                {
                    "sent": "They surely going to need for us to find better ways of interpreting the meaning of a sentence within the particular document or dialogue in which it occurs.",
                    "label": 0
                },
                {
                    "sent": "It's also important to have evaluations which distinguish between frequent and the less frequent words.",
                    "label": 1
                },
                {
                    "sent": "Often the less frequent ones are the ones that carry most of the semantics, and if we do things like perplexity where we average overall words, then the performance on those rare words is kind of getting lost, right?",
                    "label": 0
                },
                {
                    "sent": "We're not putting a focus on that.",
                    "label": 0
                },
                {
                    "sent": "And hopefully you you're convinced that the children's book Test is a useful resource for testing both of these things.",
                    "label": 1
                },
                {
                    "sent": "And if you are, then you can download it from the Facebook web page.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}