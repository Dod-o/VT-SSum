{
    "id": "us5eov42s6jkklepfuapzthovmqxymrq",
    "title": "Multimedia Signal Processing",
    "info": {
        "author": [
            "Noel E. O'Connor, Faculty Of Engineering And Computing, Dublin City University"
        ],
        "published": "March 11, 2009",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Information Retrieval->Multimedia Retrieval"
        ]
    },
    "url": "http://videolectures.net/ssms08_oconnor_msp/",
    "segmentation": [
        [
            "OK, good morning everybody.",
            "I hope you all had a nice evening last night.",
            "The city centre, yes.",
            "Everybody, bright eyed and bushy tailed for this morning.",
            "OK, well I'll try and I'll try and keep you awake.",
            "My name is Noel O'Connor.",
            "I'm from Dublin City University in Ireland an I'm in from the School of Electronic Engineering and I work very closely with Alan Smeaton who gave a lecture I think on Monday at this summer school.",
            "M. My background is from a signal processing background.",
            "Originally so I started my research in the area of image and video compression and the technologies underpinning popular standards like JPEG.",
            "Egg one MPEG, two and MPEG four, and with the advent of MPEG four, I moved into the area of content analysis, video analysis in particular, and image processing.",
            "An image image analysis would have used to supporting object based functionality in MPEG four which at the time was a very hot topic and with the advent of MPEG 7 then subsequently moved into the area of audiovisual analysis for feature extraction which is around the time I started working with information retrieval specialists like Alan.",
            "Review to supporting information and retrieval, and then most recently I've started working with the likes of Stephen, who gives a wonderful presentation yesterday to support semantic web type, an analysis and understanding of media.",
            "The focus of today's talk is going to be on multimedia signal processing, so I'm going to try to explain or motivate why we need to do feature extraction from audiovisual content and how in fact we'd go about doing that for specific applications.",
            "So I'll start with some motivation and some."
        ],
        [
            "Focus for the lecture as to why we need to do audiovisual processing.",
            "For knowledge extraction, an and.",
            "As a result of this, hopefully having convinced that it's an important thing to do, I'll then try and convince you that.",
            "Well, if we want to extract useful features from audio and video content, we need to understand how it's represented from the very basics, from capture time up to compression up delivery.",
            "I then talk about the individual features that we can extract from audio, visual and text with emphasizing what's important where.",
            "What's important typically corresponds to what's important to us as humans in terms of consuming content or what's important in terms of supporting functionality for certain applications, and I'll finish with some examples of bringing together different kinds of features for different types of analysis for different types of content."
        ],
        [
            "Am it's also always important these kinds of things to explain what's not covered in this lecture, so you understand where I'm coming from and what my main motivation is in my main focus, so I won't focus on text analysis, which is arguably a very important part of multimedia.",
            "I will, however, show how text can be used in combination with some of the techniques that I'm going to describe to extract some very useful and very interesting information from audiovisual content an I do cover audio to a certain extent in this presentation.",
            "I'm not an audio expert, I wouldn't claim to be.",
            "So I don't have a deep discussion of audio processing.",
            "I simply give you a high level overview to explain how there's useful information in an audio bitstream that can be extracted to assist in content based indexing and retrieval.",
            "So in a sense I only scratched the surface of what's possible with audio processing examples.",
            "I show a very, very rudimentary and of course there's much more sophisticated work out there by real experts in the field and the likes of Thomas Sykora's Group in Berlin.",
            "Mark Sanders Group in Queen Mary University Anne Gaelle Richards.",
            "Of course, in France.",
            "And as I go through the presentation, you'll see that most of my slides are a lot of my slides are peppered with these references, which are meant to be pointers to further reading for you.",
            "Anna's PhD students are beginning PhD students in this in this field, and the slides will be available at the end of the of the seminar in PDF format for download."
        ],
        [
            "OK, and thinking about how to present this to new PhD students.",
            "I guess most of your PhD students, right?",
            "Yeah, OK, that's good.",
            "'cause if you weren't, you'd really do derail my talk from this point then.",
            "So thinking about what to say to new PhD students are PhD students are in the depths of their of their thesis work.",
            "So as you know, and as you learn, going through your thesis, it's very important to have a hypothesis.",
            "That's what your thesis is based upon.",
            "OK, so you need to convince people about something that you believe in, and if you can convince external reviewers on your peers, that will result in a large number of publications in journals and conferences.",
            "And more importantly, if you can convince your supervisor on your external examiner, hopefully they'll award you with your PhD and I felt coming here to this kind of environment that it's important that we as lecturers lead by example.",
            "So I'm going to have hope hypothesis in this presentation and over the next few hours I'm going to try and convince you guys of something OK, and my hypothesis for this presentation is that PhD students never had it so good, OK?",
            "Or in other words, now is a great time in this field to be doing a PhD.",
            "I also refer to this as the.",
            "Grumpy supervisor terum.",
            "OK, which is a capsulated by the phrase that like it wasn't like that in our day.",
            "And by our day I mean my day your day, Steven and all the other lecturers days, OK?",
            "And that's not meant to sound critical.",
            "Any way, shape or form.",
            "It's simply trying to motivate you that there are very exciting problems to be addressed and to be considered in this emerging research field, which brings together audio processing, video processing and the semantic web."
        ],
        [
            "OK, so the motivation for the lecture under the focus so."
        ],
        [
            "Well, the motivation for why we're all doing research in this area.",
            "I guess to a greater or less extreme strength extent is the recent growth in multi medium which has really blossomed and bloomed in recent years due to the decreasing cost of digital capture and storage.",
            "So it's now easier than ever before to take photographs to capture audio to take video footage and to store this vast amount of content in your own personal repository.",
            "Now of course, broadcasters have been doing this for many many years.",
            "OK, but now we are all content producers.",
            "We all produce significant volumes of content.",
            "To try and give you a flavor for for how significant this growth is, I have some statistics which I've pulled from various sources in the web which tried to highlight the sheer enormity of content being produced.",
            "So there are studies which have shown that there were 490 digital photographs taken with each digital camera in the EU on average in 2006.",
            "Pam, the estimate is that half a trillion digital images will be captured in 2009.",
            "Camera production and this doesn't include camera phones.",
            "This is your common or garden traditional digital camera.",
            "Is reach will reach 89 million units in 2010 in Europe and worldwide camera phone sales was 370 million units in 2729 billion.",
            "Digital images were taken and 847 million units are projected for 2009.",
            "And of course, these are only some examples of the kinds of content production mechanisms that are in place.",
            "An constantly gathering data.",
            "There's another very good example which is CCTV.",
            "So due to recent world security concerns, we've witnessed a very strong growth in the deployment and capture of closed circuit television footage for security and for safety.",
            "So for example, in the UK alone there are 4.2 million cameras that corresponds to 1 camera for every 14 people in the UK.",
            "OK.",
            "So that's the first brick in my argument is that it's great to be a PhD student these days in this area because there's lots of data and lots of data.",
            "Means lots of interesting problems, lots of different kinds of data means lots of different kinds of problems, so there are many, many PhD theses topics to be researched.",
            "OK, back when I was doing my PhD back when dinosaurs ruled the earth many many, many years ago, we really had a problem with using or getting our hands on large quantities of test data.",
            "So for example, when I was working on video compression, we had a small handful of MPEG 4 standard standard test sequences that everybody used, and by small I mean about 8 to 10 sequences of 300 frames each.",
            "OK, a very small amount of data to work with.",
            "Of course you could gather your own data and work with that, but then you couldn't publish your results.",
            "OK, because it was your own data and there wasn't an agreed benchmark.",
            "These days we have very large, very large number of commonly agreed test corpuses such as the Trek fit.",
            "Collection OK, which is a large archive of test content for or search and retrieval for shot boundary detection for feature extraction.",
            "Most recently for event detection from CCTV type scenarios for copy detection and we have image clay collections.",
            "We have large collections of text repository and so on and this is a very exciting development 'cause it means it's a very stable framework for you as a PhD student.",
            "To get this data."
        ],
        [
            "Use this data and to prove convincingly and conclusively that the technology you develop extends the state of the art.",
            "OK."
        ],
        [
            "So back to the huge growth of multimedia content.",
            "Well, of course all of this content is useless unless we can actually access the relevant content that we need, and that means being able to index the audio, image, video data based on what it contains based on its contents or something other than time and date of creation and who the author was, what the actual content is depicting.",
            "And that's what underpins the whole area of content based information retrieval.",
            "Now, ideally we'd like to be able to perform this indexing in real time, or sorry, completely automatically, because studies have shown through the likes of our national broadcaster in Ireland, for example, in our TE, where they're charged with archiving and indexing everything that's produced in Ireland for reasons of national heritage that to properly index one hour of video content.",
            "Broadcast video content requires 8 hours of manual effort.",
            "OK, so any inroads you can make into reducing the burden on the annotator or the indexer is a huge saving in terms of human resources and in terms of financial resources.",
            "We'd like to be able to index the content based on what it represents in terms of who's present, what's depicted where it's occurring, why it's occurring when it's occurring, and how is occurring.",
            "Or in other words, the underlying semantics.",
            "The real world semantics of what the content represents.",
            "Of course, if we can do this, there are many advantages and just not just in the field of content based information retrieval, but it allows our paves the way for other advanced content functionality such as advanced interaction, improved compression, autonomous intelligent content that understands itself and can migrate from user to user in person to person, and so on.",
            "OK."
        ],
        [
            "So, given that we want to extract The Who, what, where, why, when and so on from this huge volume of content, there is very clearly a need for machine computable techniques for extracting the real world content from the video and audio data are responding to the semantics of what it represents.",
            "And just to get some terminology in the first instance at the applying place this discussion an it was a very interesting discussion in this paper or in his book by Young and his colleagues which tried to classify the various different stages of semantic knowledge extraction in the context of multimedia data.",
            "And they refer to image, Audio and video processing is where you take the image data in and you produce an image out.",
            "So that's it.",
            "For example, in sorry I'm using I'm using image in each of these examples as a single example of 1 modality.",
            "And so this is an example of taking an image in, performing some processing on the image that makes it easier to subsequently extract features or measurements.",
            "That's the next stage with the actual analysis taking the processed image and performing some measurements on it that are useful for inferring some kind of knowledge.",
            "And in the final stages the image and video and audio understanding where you take the measurements derived from the input image and produce a high level description as a result.",
            "And this is where we're where we're working towards OK.",
            "In this lecture I'll be focusing in the second level in this, which is taking an image or a video or an audio sequence in and producing some useful measurements where those measurements are termed features.",
            "Now, of course, in this aspiration of extracting semantic knowledge, OK, there is a a key problem that hopefully we're all aware of, and that's probably underpinning most of your PHD's that's commonly referred to as this."
        ],
        [
            "Month gap problem.",
            "So the problem is the difference between what we can measure from an audio or a video signal and what that signal actually means to a human in a given application scenario.",
            "So this was put very succinctly and very elegantly by Arnold Smolders and is seminal paper, which I presume everybody in this room has read it.",
            "If not, you should read this paper.",
            "It's a very important paper, and he states the semantic gap as the lack of coincidence between the information that one can extract from the visual data and the interpretation of the same data has for a user in a given situation."
        ],
        [
            "So to try to explain what exactly we mean by the semantic gap as a means of motivating summer technologies, I'll talk about let's consider the semantic gap in the context of image data just for the moment as opposed to audio or video.",
            "So this idea of the semantic gap isn't the new idea.",
            "OK, it's been around for a very, very long time, so it was put by Napoleon Bonaparte many, many years ago.",
            "Who said I won't even attempt to try and speak to French and embarrass myself with my dreadful French accent and pronunciation?",
            "But he said a good sketch is better than a long speech.",
            "Similarly, the Russian author I I've intergen I've said a picture shows me at a glance what it takes.",
            "It dozen pages of a book who expound?",
            "Or perhaps we're more familiar with the saying, which says that a picture is worth 1000 words.",
            "Which, incidentally, wasn't coined by Confucius as is sometimes thought to be the case, but was actually originally accredited to this guy, Fred Barnard, writing in a trade Journal in 1921, trying to encourage his industry to use images on the sides of St cards to promote advertising and products, and this was his rationale for doing it rather than putting text up there, you should put."
        ],
        [
            "An image.",
            "For me the best and most distinct, all embracing statement of the semantic gap is what Richard Hamming said, which is that the purpose of computing is insight, not numbers.",
            "In other words, we want to understand not just measure or not just compute.",
            "So here are three examples of images.",
            "OK, and these images it's quite straightforward, as we will see for the remaining of this lecture to extract measurements from these images.",
            "But the question is what do these measurements actually mean?",
            "And of course, for us, the problem is that even for humans OK with all our depth of experience on our evolution, hereditary and so on, it's not clear sometimes what a picture represents.",
            "So for example, these are optical illusions corresponding to.",
            "Is this a side profile of a of an old woman.",
            "Or at back profile of the other young lady.",
            "Now everyone see the difference there, yeah?",
            "Similar here, is this a single face bisected by a candle stick or is it 2 side profile faces looking at each other?",
            "And here is an example I created myself.",
            "OK, which is the result of an automatic segmentation process that we will talk about.",
            "This is where we try and extract the foreground from the background.",
            "Can anyone suggest what this might be?",
            "Well, I'm not surprised you can't guess it's actually the silhouette of somebody juggling.",
            "OK and entertainer juggling, so this is his arm raised.",
            "We've lost some of the hand here.",
            "This is actually the ball rolling down his his forehead and this is his other hand ready to catch the ball.",
            "OK, and during the course of this lecture will see how we can take an image of a juggler and we can produce something like this.",
            "OK, but the problem of course, is that how do we interpret this then as a juggler?"
        ],
        [
            "OK, this whole area is a hugely active research field.",
            "It's been the basis for many national and EU projects, so some of you be familiar with K space or Ace Media or bowl Mia Mesh which are hosting this summer school in this event.",
            "And there are many different applications and content domains targeted by this line of research.",
            "So in the area of news, sports content, movie content, personal collections of images and video, and there of course, as you would expect, many many, many different approaches attempting to solve the problem way too many to list or to go through in this particular presentation.",
            "The bottom line, I suppose, is that there is been a very very large amount of public investment and industrial investment into this research area in terms of research funding, and there have been hundreds of PHD's produced already in this area and there will be hundreds of PHD's produced in the future, and there are thousands of interesting papers coming out, so the bottom line is that this will be a very active, very fertile research area for many years to come because I would claim that the semantic gap isn't actually a gap but is a chasm.",
            "In other words, a huge rift that we have to try to breach, and that we're only now starting to build the foundations of bridges that will help us cross that gap.",
            "So that's my second, an proof that now is a good time doing PhD, so the semantic gap means that there are lots of interesting and exciting things for you."
        ],
        [
            "To do, and most importantly things for you to do, working with colleagues from traditionally complementary research areas.",
            "So in the past our research areas tend to be quite insular in that we focus on a particular area individually to the exclusion of all else in order to try to address the semantic gap we need to collaborate beyond our typical or traditional comfort zone.",
            "So if you're an image processor, you need to work with audio processing people.",
            "You need to work with semantic web experts like Steven and his group and his community.",
            "And we need to bring all of these different research areas and researchers together with A view to making real progress.",
            "And this means that that you have people working together who traditionally didn't work together in the past, producing very interesting."
        ],
        [
            "Else.",
            "OK, in terms of a taxonomy of approaches of how we might tackle this overarching problem well, I've tried to classify a number of different approaches in the literature from a very, very high level perspective, so we typically have the bottom up approach, which is usually coming from the computer vision community, image processing, community audio analysis, and audio processing community, and this is where we where the Community tends to focus on very specific application niches and challenges with view to extracting it very specific.",
            "Kind of information that perhaps is only relevant to that particular piece of content.",
            "Anne, however, having said that.",
            "The result of that huge investment in those areas is resulting in a variety of more mature and more magic generic solutions for very complex analysis operations, which can be leveraged in the context of content based information retrieval.",
            "Then we have the what I would term this top down approach whereby we start with tools for ontology creation and management and then we attempt to link low level features to these ontologies and that's a very exciting development.",
            "That means that as a result of the effort into this area we have a large number of custom built ontologies defining important concepts that can help guide our research on audiovisual analysis.",
            "So we have experts who are telling us here is the kinds of things we need you to extract from the audiovisual content.",
            "And then we have the more generic approach whereby we take robust machine learning approaches that can be broadly applied to detective variety of different concepts, for example.",
            "So this is where we take a large number of audiovisual features and use something like a support vector machine or a Bank of support vector machines to detect a wide range of concepts.",
            "So the result of this is that we have a set of classifier banks, a variety of early and late Fusion strategies which all produce very promising results in terms of now."
        ],
        [
            "Extraction.",
            "M Here are my favorite papers.",
            "More or less map to what I believe are some of the most important problems in the area.",
            "And so there's been some very interesting research recently from the computer vision community in terms of invariant image features.",
            "So what are stable, interesting low level features?",
            "Are measurements in movies or smoking in movies?",
            "Now he's about the theoretical framework which allows you to detect these kinds of generic periodically, reoccuring actions, and then with people who are working on combining audio and visual analysis.",
            "Sorry people working on the audio semantic so such as the groups I mentioned earlier, we're doing some very interesting works on technologies such as source separation, recognizing specific instruments in music, recognizing structure of songs and music videos, and so on, and with people who are taking the results from a variety of these different areas and combining them to detect audiovisual events.",
            "OK.",
            "So if I had to recommend as a result of this this lecture a reading list that you do well to go off and chase down, this is what this slide is meant to be."
        ],
        [
            "Continue on this on this team in the top down approach with some very interesting work coming out of it.",
            "I search here in Greece, Stephens work.",
            "Of course an on the far side of the Atlantic.",
            "The large scale monthly multimedia ontology that I guess Alexander Huffman talked about on Monday is a very interesting development.",
            "And then the work in media Webmail, Marcel Worring and his colleagues where they've trained a Bank of classifiers to detect over 100 concepts, sometimes with very high levels of precision.",
            "And recall again very interesting reading and I would guess that by the end of your of your PhD work you have come across.",
            "If not having had an intimate knowledge of more."
        ],
        [
            "Of these, alters under under work.",
            "OK, the point is, however, for the purposes of this lecture, is that for nearly all of those approaches that I've just mentioned in the previous two slides, the first step is calculating some measurements from the signal, extracting something upon which you can perform the higher level classification, or the higher level analysis.",
            "So these measurements are typically called features, and they can be used to train classifiers that can be mapped to higher level concepts via ontologies that can be used in a variety of inferencing processes.",
            "So this lecture focuses on this idea of low level.",
            "Feature extraction.",
            "M and specifically, what can we or what should we measure from a signal that makes sense?"
        ],
        [
            "So a focus on feature extraction for CBI are by necessity.",
            "We need to understand how images, video and audio are actually represented.",
            "In order for those features to make sense an given that we understand how we represent digital audiovisual data, I'll describe a selection of useful features.",
            "I'll show examples of the usefulness of individual features.",
            "I'll show examples, have combinations of features can be used an I'll cover a lot, but in each case I'll provide pointers to further reading where that you can follow up on."
        ],
        [
            "OK, so let's start with the with the basics, the basics."
        ],
        [
            "Of audio visual capture an this is the part where you might fall asleep 'cause many of you be very familiar with that, but for the purpose of some of you who aren't, let's start at the very beginning and work our way up.",
            "So we know that a digital image is captured when a camera scans.",
            "As seen, it typically scans tree arrays of samples corresponding to red, green and blue, and the density of samples, number of samples per per unit area gives us the resolution of the image.",
            "A video on the other hand, is captured when a camera scans as seen at multiple time instance.",
            "So it's like a single image being captured very, very quickly to 25 frames per second.",
            "For example, even higher resolution or temporal resolution, depending on the application.",
            "In this case, each sample each time instance is called a video frame, which gives rise to a frame rate.",
            "So for example, your TV at home your full motion video is 25 Hertz at 25 frames per second, whereas the video you might get on your mobile phone.",
            "It is typically only 8 to 15 frames per second.",
            "The difference you notice perceptually from that is that one will be more jerky as opposed to a more free flowing full motion video.",
            "Audio Dennis captured when a microphone temporary samples sound waves.",
            "So."
        ],
        [
            "To visualize that this is our are seen to be captured are still camera.",
            "Captures red, green and blue arrays of samples.",
            "Each sample is called a pixel, and Lee each pixel in each channel.",
            "Red, green, and blue is represented by 8 bits are responding to 256 different levels of green, blue and red, and the point being of course you can combine the red, green, and blue samples in additive manner to produce any color that would give us our our color picture.",
            "Video on the other hand.",
            "That is captured at most multiple time instance.",
            "So we now extend along the temporal dimension at each time instant, capturing tree arrays of samples now.",
            "Video typically is captured in a different color space than still image, so red, green and blue as we will see in the next slide is only one example of a color space.",
            "One way of representing digital color.",
            "There are other many other ways.",
            "A popular one in the context of video processing is something called yuv where we take the RGB single signal.",
            "We have a linear transformation which transforms red, green and blue into tree.",
            "New sets of coordinates called YU&V where.",
            "The Y coordinate or the white channel corresponds to the luminance or intensity or in brightness at each spatial location and the U&V collectively represented color data.",
            "OK, now this has.",
            "Two very important an.",
            "Benefits are motivations, so by transferring from red, green and blue to yuv, we've decoupled intensity or brightness from color OK and way back when that was very useful because it provided backwards compatibility with black and white systems, and many of you remember black and white systems.",
            "Black and white TV at home.",
            "OK, so I'm not the only old fogey here.",
            "OK so this is very this is very nice because it meant that you control way to U&V and just process the Y, the grayscale image and you had black and white OK. Also, it turns out the human visual system is much less sensitive to color information.",
            "That is to intensity or brightness information.",
            "OK, so in terms of compressing the data and having a compact amount of data to store, the first thing we can do after this transformation is throw away 3/4 of the U&V data.",
            "3/4 of the color data without the human visual system noticing a very perceptible loss that has significant implications in terms of storage.",
            "OK, as we will see in a moment, I put some figures on why we need to do.",
            "Image and video.",
            "Fresh and so in yuv space, each pixel of each sample in the luminance component again is typically represented using 8 bit.",
            "So we have a range of Gray level values that range from zero, which is black up to 255, which is white.",
            "In terms of audio, we take an audio signal captured via microphone and then we sample the signal in this way to produce a digit."
        ],
        [
            "Best audio signal.",
            "So this is what the image data actually looks like.",
            "This is an image which is 420 by 315 pixels with eight bits per pixels.",
            "So to store this image, this raw image captured by the camera on disk requires 387 kilobytes of storage and this is what the individual pixel values look like if we zoom in onto the image.",
            "So you can see each one of these samples corresponds to a red, green, blue triple value.",
            "So this guy here, for example, the value is actually seventeen 00.",
            "So it has a value of R for 17 and no contribution from green and blue, so it's almost black.",
            "It would be completely black if it was 000.",
            "This guy, on the other hand here is close to closer to white with high RGB values."
        ],
        [
            "For video we transform, let's say to the yuv color space.",
            "So we go from an image that looks like this for these three different color components.",
            "Luminance component under 2 color difference components.",
            "So just to reiterate or GBR.",
            "Known are known as color spaces.",
            "Many different color spaces exist, popular ones you'll see in the CBI.",
            "Our literature are things like HSV, which is the Hue saturation value color space, which is a different way of representing the same information where he represents the color types, which is red, green, blue or yellow.",
            "Saturation is the intensity of that color to paint, and the value is the actual brightness.",
            "So akj Anil K. Jain's book is a very good place to start for the fundamentals of digital image processing.",
            "So again, if we zoom up here, taking a corner of this guys hot, you'll see that this area here, the Y value is 230 very close to white, whereas here the Y value is 127, which is in the mid range corresponding to Gray."
        ],
        [
            "M audio data.",
            "This is what the audio data typically looks like for three different sounds, so I'll actually play the sounds.",
            "If I can.",
            "That's an audio sample that we captured in some work we were doing on measuring traffic flow based on low-cost microphones.",
            "That's the sound of a car moving past a microphone at the side of the road.",
            "So it's a very different kind of audio sample.",
            "OK, and we should be able to see by looking at the two waves on a significant difference between those.",
            "OK, so the second sound is much more periodic and you can recognize the frequencies within that.",
            "Within that sound we can take it to the next extreme.",
            "We can show it for human speech.",
            "Cat sat on the mat.",
            "OK, in the DCU student, the back might recognize that is no Murphy.",
            "Our current head of School of Engineering in DC you can.",
            "So you can see tree very different kinds of sounds very very different kinds of waveforms and even by looking at those waveforms.",
            "Hopefully it's intuitively clear to you that there are relatively simple straightforward measurements we could do on those waveforms to classify between those different kinds of sounds.",
            "So we have, for example, in the human speech we have a much larger number of zero crossing, so the signal goes up and comes down much more often than it does for the case of the.",
            "Of the car, or indeed for them."
        ],
        [
            "Sick.",
            "Video data this is what our video data looks like, so this is one of those standard test sequences.",
            "I was telling you about that I worked with and came to know and love or hate at times during my PhD and this is the kind of very boring data that we had to work with.",
            "This is a 352 by 288 video sequence which is a resolution referred to Asif Common interchange format in Yuv.",
            "It has 8 bits per pixel and this is 30 frames per second.",
            "So to store that video sequence.",
            "I'm on our 32nd clip of that video sequence requires 261 megabytes of storage on disk.",
            "OK, if you want to transmit it, then it's 8.7 megabits per second to transmit, so suddenly we're getting into the realms of significant volumes of data OK. And this is an example of the type of video you might get to a mobile device.",
            "This is the foreman sequence, again, another one of these standard test sequences that we use.",
            "This was developed by MPEG to represent the application scenario of mobile video telephony.",
            "So the scenario is that of someone talking into their mobile phone and the foreman is going his customer at the end of the video.",
            "Here's the wall I'm building for you.",
            "Everything an this particular video is cusip resolution, which is 1/4 of safe.",
            "It's 176 by 1448 bits per pixel.",
            "30 frames per second, so a 30 second clip store of this very low resolution image requires 65 megabits or megabytes of storage.",
            "If we scale it up to high definition TV, OK Like you get in your widescreen plasma screen.",
            "At home we typically have resolution of 128 by 720 pixels will typically have 24 bits per pixel.",
            "Now supposed to 8 bits per pixel, and we typically have 50 frames per second as opposed to 30 frames per second.",
            "So at 2 1/2 hour Hollywood movie in HDTV resolution will require 3.4 terabytes of storage.",
            "If it's taken directly from the camera, digitized and stored.",
            "On a hard drive.",
            "So typically, in the case of video, we don't actually process and store the raw data.",
            "Typically before we ever get to the video."
        ],
        [
            "No, it's undergone a compression process which takes the raw data, which is what it's referred to when it comes directly from the camera.",
            "They still camera video camera or microphone and it's compressed.",
            "It undergoes some software or hardware process to compact the data to make it as efficiently as representative make.",
            "It is easy to represent as possible in terms of efficiency.",
            "So this process is known as compression or encoding, and it typically results in the bitstream that can be stored or transmitted.",
            "For a variety of applications.",
            "The hard the encoding process is typically quite a computationally heavy process.",
            "There's quite some complex maths going on there and signal processing going on there, and typically we require a less complex process to uncompress or decode the content before it can be displayed or before it can be heard.",
            "OK, so what's the point of all of that discussion about how we capture video?",
            "Well, the point from a content based information retrieval perspective is that when we are trying to extract these features upon which we're going to base some inferencing post process, we have two options.",
            "Or we have two possibilities.",
            "In the first instance we can take the measurements from the raw data.",
            "If we have the raw data.",
            "Little, we can post the data in what's known as the uncompressed domain.",
            "This is where we end up processing raw audio samples or raw pixel values, or raw frames of video.",
            "Alternatively, we can take the compressed formats, which I'll mention in a moment, and we can work directly with those so we can process the actual structure stored in an MPEG one video bitstream.",
            "For example, an use that.",
            "As opposed to doing a complete full decode and then working with the pixel values so the benefits in the first instance is that it can be fast.",
            "It can be real time because we're not, we don't have to actually decode the data and store it at this, consequently process it.",
            "We can extract data directly from the bitstream.",
            "On the second hand, if we're processing the raw data, we end up with the possibility of extracting a greater range of more expressive features.",
            "OK, because we have the raw pixel values for example, as opposed to frequency domain coefficients, which is how we represent pixel values in a compressed bitstream."
        ],
        [
            "In compression there are two kinds of compression we should be aware of their verses called lossless compression, and this is where we don't change the data, but we simply reorganize the data in clever ways with A view to compacting it and inefficient bitstream format.",
            "So this is the kind of compression that's used in medical applications such as digitising X Rays for example and document scanning.",
            "So your facts in your in your office uses a form of lossless compression.",
            "It scans the image, it gets a raw set of RGB values in a, perform some processing on that.",
            "To represent them very efficiently so they can be squirted down at a telephone line.",
            "Lossy compression is the one that were perhaps more familiar with and is more ubiquitous and more widespread.",
            "And this is where by we make some compromises in order to reach the very strict requirements of our networks.",
            "Our applications in terms of bandwidth.",
            "In other words, we can't assume that we can keep all the image and video data or audio data we throw some of it away.",
            "And the trick isn't being clever about what we throw away.",
            "So we only throw away data that's of less important to the human visual system, or less important, to the human ear.",
            "So for example, in image is very often fine detail.",
            "Is less important depending on the image of course, but the point is the human visual system is less sensitive to fine detail.",
            "So if we can isolate fine detail in the image, we can remove it.",
            "Thereby having less information to represent and store achieving compression without the human eye noticing any difference.",
            "So popular image and video compression standards you're probably familiar with these.",
            "We have JPEG, which is ubiquitous under web for compressing still images.",
            "Most recently, we've an enhancement of JPEG that uses a different kind of compression technology and support enhance functionality called JPEG 2000 MPEG One.",
            "As an old standard at this stage and provides sub VHS video quality.",
            "The original target application was video from a CD ROM MPEG 2, which is a standard upon which digital TV is based, and.",
            "DVD technology is based most recently got MPEG 4, which is a very efficient.",
            "I'll be it quite computationally intense video compression standard targeting.",
            "Very low but very very low bitrate but very high quality video for applications and in the mobile domain and the most recent standards.",
            "H .264 which is advanced video coding above and beyond even MPEG 4 just a piece of trivia in case you're wondering, an people sometimes ask me MP tree or refer to MP3 as MPEG.",
            "Trey there is no mpeg tree.",
            "In actual fact, MP Tree actually means MPEG one layer tree audio encoding, so the MPEG series of standards goes 124 OK, which is a nice binary progression that maxel engineers computer scientists very excited and but there was no MPEG tree.",
            "There actually was an MPEG tree.",
            "It would start to be developed, but it turned out that the activity in MPEG Two overtook MPEG Tree, excuse me and so they stopped the work item in MPEG Tree and move directly to MPEG 4.",
            "OK."
        ],
        [
            "A little bit of trivia.",
            "You can amaze your friends with after your next dinner party.",
            "OK, if you don't like them, 'cause I probably won't come back.",
            "OK, so image compression.",
            "How does it work?",
            "And again the very high level overview just to try to explain to you what are the kind of features that we can extract.",
            "This is my one slide overview of how JPEG works.",
            "OK, so the bottom line is that we use frequency domain analysis.",
            "We take our spatial data, samples are responding to an array of pixels and we transform to a new representation in the frequency domain and that allows us to select the most important information.",
            "So if we take an image like this, which is a close up of a pizza, OK, it's in the spatial domain.",
            "It's highly textured and it has fine detail.",
            "If we perform a 2D discrete cosine transform, we can transform this spatial data into the frequency domain and this is what we end up here with here.",
            "So this is an array of frequency domain coefficients.",
            "Your upper left hand corner is what's known as a DC coefficient, which is the average color.",
            "If you like over the entire image, and then we have values here, frequency coefficients that represent.",
            "Increasing horizontal and vertical spatial frequency.",
            "So in other words, these correspond to the low frequency content of the image.",
            "And the upper right hand corner of this block corresponds to the high frequency content.",
            "Now high frequency content and image corresponds to fine detail.",
            "The point is that we can take this representation of frequency coefficients and we can filter it so we can remove in this case, for example, all the high frequency coefficients simply by zeroing them.",
            "So in this case we've gotten rid of 3/4 of the data that's used to represent the image.",
            "And then we can take this truncated data.",
            "We can perform the inverse transform back to the spatial domain and we end up with this kind of image here.",
            "And.",
            "OK, here is that there's a low pass filtering effect on the under the beamer, but in actual fact these two images are very very similar, as you'll see in the slides when you finally get the PDF version.",
            "So we removed a significant chunk of the day to 75% of the data OK with no perceptible difference between the input and the output image."
        ],
        [
            "So.",
            "Of course, we're throwing data away in this process, so we're taking some data were saying you're not important and we're throwing it away.",
            "Typically we don't do this across the entire image for reasons of computational complexity and efficient implementation.",
            "Rather, we typically take blocks of image data, so we take 8 by 8 blocks, or 16 by 16 blocks of image data and apply this frequency domain processing to each individual block that has benefits in terms of implementation architectures.",
            "It is benefits in terms of error limitation, error propagation, and so on.",
            "Of course, given that we're throwing away some data we don't, there's no such thing as a free lunch, so we are prone to throw away too much data.",
            "Sometimes, depending on our application requirements and end up with reduced quality in our in our images.",
            "So here's an example of successively throwing away data from a still image, so.",
            "See, this is the original image.",
            "OK, this is the output image after performing some of this, throwing away some of this high frequency data and this is the difference.",
            "Images is the difference between this image and this image.",
            "OK, and as I throw away successively more data.",
            "You'll see that the image becomes more and more pixelated, and what we're actually seeing here this kind of blocking effect corresponds to the edges of the individual blocks that were processing OK, which introduces a very perceptually noisy, very perceptually disturbing effect into the image.",
            "Representation, but perhaps for your application if it's a low bandwidth mobile application on a mobile phone, perhaps this quality is sufficient.",
            "OK for the image you want to convey.",
            "The bottom row is another example of the exact same thing.",
            "We applied to a very different kind of image data, so in this case we have a scanned piece of text or responding to a input on a next term and you'll see that as I successively throw away high frequency information.",
            "Our image very quickly gets very very noisy.",
            "So certainly we've gone from this image here, which is desirable or application something like this, which is potentially useless 'cause you can no longer see the text.",
            "OK.",
            "So the bottom line, I suppose, is that different kinds of data required different kinds of compression, so you wouldn't you wouldn't, or you shouldn't apply the same compression technique to this image as you would for this image, because in this image you can get away with throwing away to find detail the high frequency content, whereas in this image the high frequency content defined detailed corresponds to the actual text that's being represented."
        ],
        [
            "This is my one slide overview of how video compression works, so video compression.",
            "Is similar in a way to taking an individual image at 25 frames per second, 25 times a second and applying still image compression to every single frame.",
            "OK, so taking the individual image transform into the frequency domain, throwing away to high frequency content.",
            "Now it turns out that even doing that, given the sheer volume of video data that we're dealing with that still not sufficient in order to reach the very strict bandwidth requirements of some applications.",
            "So we need to do some more processing to the video.",
            "In order to get down those kinds of band bandwidths.",
            "Am so in addition to taking the video frame and performing discrete cosine transform and throwing away to high frequency data and producing a bitstream, we also have a process known as motion compensation and motion estimation, and this process takes successive images from one frame to the next and estimates the difference from one frame to the next.",
            "Point being that if you have video captured at 25 frames per second, well, even if it's a high activity, high octane car chase in a Hollywood blockbuster movie.",
            "Whilst there might be significant change in the video content from.",
            "0 seconds to 25 seconds from zero to 125th of a second.",
            "There's very little change between individual images.",
            "And we can potentially measure this change and quantify this change and only encode and transmit the change and therefore get very efficient encoding of video data.",
            "So we use the exact same processing as we do for still images, but we have this extra process called motion estimation.",
            "Whereby we take the previous frame in the video sequence.",
            "If this is the current block that we're trying to encode, so again for video for reasons of computational complexity and implementation, we don't process entire frames of video, but rather we break on more manageable chunks corresponding to 16 by 16 blocks.",
            "So we take the 16 by 16 block in the current frame and we try and find a good match for where that block came from in the previous frame.",
            "So where is a good predictor for that block?",
            "In the previous video frame?",
            "And then then we can get away with only sending the motion vectors.",
            "It's called the displacement or the offset from that frame to the previous frame.",
            "So what actually gets sent in a video bitstream is not pixel datas.",
            "Pixel data, but rather it's a combination of these frequency domain.",
            "Coefficients which represent the frequency content of each image on a series of motion vectors.",
            "2 numbers X&Y which give you displacement already offset for the best predicting block from the previous frame."
        ],
        [
            "This is a visualization of actually what actually gets sent.",
            "This is another one of those very, very boring test sequences we used to work with.",
            "So what you're seeing here?",
            "This is the current frame to be encoded.",
            "This is the previous frame in the video sequence and this is the motion that we're measuring from one frame to the next.",
            "So in this sequence the guy is bouncing a table tennis ball OK?",
            "I played it again and then the camera zooms out.",
            "So the green arrows here represent these motion vectors.",
            "These predictions that we've measured.",
            "And the interesting thing to note about this is that the motion vectors actually correspond to what's happening in the scene.",
            "OK, so the green arrows when the guy is bouncing to table tennis but are located around his arm.",
            "That hopefully implies to us that if we want to perform some processing on that video sequence to extract the guys arm from the background, we could for example use the motion vectors.",
            "'cause that they indicate parts of the image that are moving."
        ],
        [
            "Here's another example for a very different kind of scene, but again, one of these boring standard test sequences where we have a camera that spanning across the scene, tracking an object as it's moving, and you'll see that all of the motion vectors in the first part of the sequence point to the left, which indicates that the camera is moving to the left, and then they all move upwards when the camera tilts upwards.",
            "So of course these motion vectors were derived and were produced for the purposes of compression efficiency.",
            "In order to have a very compact representation of the bitstream.",
            "But the point is that every MPEG bitstream that's out there on the net or on your hard drive contains these motion vectors.",
            "OK, so will contain some potentially useful information in terms of extracting features.",
            "If you're trying to infer what the camera motion is, for example, or what the object motion is."
        ],
        [
            "OK, so let's do a brief at whirlwind tour of audio compression then, so audio compression.",
            "It turns out that the human ear is not is not linearly sensitive across all frequency ranges but is non linear across the audio.",
            "The audible range towards humans which ranges from about 200 Hertz at 20 kilohertz and this audible range is broken into regions where humans cannot perceive a difference was like a quantization of the audio spectrum.",
            "If you like, which are called critical bands, so audio compression starts by taking the audio signal, the raw audio signal and performing a multi resolution frequency decompoze.",
            "Addition of that data and then that multiresolution frequency representation, which corresponds to 32 subbands of the data.",
            "Is processed by a psycho acoustic model with models what the human ear hears and determines which parts of which subbands are most important for that particular sound that's being compressed and allocates more bits to the more important sounds."
        ],
        [
            "So.",
            "This is what an MPEG audio encoder looks like.",
            "We have the digital audio here.",
            "It undergoes the filtering process tinted.",
            "The construction into a variety of subbands cycle acoustic models calculates an adjust, noticeable noise level in each of these bands.",
            "In other words, what humans can just perceive on the basis of that allocates bits to the most important parts of the audio signal.",
            "One allocates more bits to the most important parts.",
            "All of that gets formatted and sent to an MPEG."
        ],
        [
            "Stream so.",
            "This is what's happening.",
            "The audio signal comes in, we take.",
            "We have this set of subband filters that typically 32.",
            "We group 12 samples from each son Bob.",
            "So Bannon code them in what are known as frames.",
            "So frame corresponds to 384 audio samples and each group is encoded with zero to 15 bits per sample typically.",
            "Now Interestingly, each group is also associated as associated with a 6 pack 6 bit scale factor and a scale factor.",
            "Audio compression terminology corresponds to the Max value of samples.",
            "Within a group of 12 samples.",
            "So it says, here's a group of 10 sample data points on.",
            "Here's the maximum value within that 12 samples.",
            "That's useful in terms of ensuring that the full dynamic range of the quantizer is actually used.",
            "But as we will see, it's also very interesting information that allows us to do some very interesting things with audio for content."
        ],
        [
            "Based information retrieval.",
            "So this is what the audio bitstream."
        ],
        [
            "Actually looks like for each frame where a frame is 1152 samples we have 32 subbands.",
            "Each subband has 12 samples, and there are three groups.",
            "So the frame is divided into this kind of data where we have some header data which allows re synchronization purposes.",
            "We have some error checking.",
            "We have some bit allocation which tells the decoder.",
            "Here is the number of bits allocated to this part of the bitstream and then we have this scale factor information which says the highest value in the following 12 samples.",
            "Is this value and then we have the actual encoded samples and I'll show you an example a bit later in the lecture where by simply parsing this single value of scale factor.",
            "Open Audio Bitstream allows us characterize or classify between the music sample you heard earlier and the speech sample you heard earlier.",
            "OK, so we can take an audio bitstream.",
            "We can perform faster than real time processing of that bitstream to extract a single number at each time interval and recognize the difference between speech and music."
        ],
        [
            "OK, so any questions on any of that.",
            "That's probably familiar to most of you, so I'll start moving a bit more quickly now.",
            "At this stage I guess.",
            "Any questions?",
            "OK, so let's move on to audio, image and video features and what we can or what we should."
        ],
        [
            "Measure from the from the data.",
            "So the first thing we should ask ourselves is what's important in the data.",
            "So let's start with still images.",
            "OK, so in still images.",
            "Presumably it should be clear to us that color is important in a still image.",
            "So for example in these nice images of sunsets, an color is quite important in terms of conveying the mood, the atmosphere and the ambience and so on.",
            "That picture is trying to impart.",
            "There's also something called texture which we might not be as familiar with.",
            "That's very important.",
            "I believe Alan Smeaton talk some something about texture in his presentation, so texture is kind of a hard thing to describe in the context of images.",
            "What corresponds to a texture and image is the best definition I can come up with.",
            "Is that it?",
            "Corresponds to the field.",
            "The appearance of the consistency of a surface.",
            "That's the textbook definition of what texture is an.",
            "So if we try and."
        ],
        [
            "Represent that in the context of images at.",
            "Here's an example of a an image I'll be at and only four by four pixel image an, which is all of the one color OK and on the right hand side.",
            "But I've shown here is I've plotted this image as a surface where the act says here are the X&Y dimensions of the image and the height zed component corresponds to the color value.",
            "OK, so a flat image corresponds to entry dimensional space, A flat surface, so a single color image corresponds to a flat surface, or in other words, the absence of texture.",
            "So there's zero texture in that image."
        ],
        [
            "This kind of image, on the other hand, where you have this combination of colors, and here I'm assuming that blue is plotted lower on the Z axis followed by red followed by yellow, followed by green followed by blue gives rise to this kind of stepped surface.",
            "So suddenly the surface is no longer flat.",
            "This is more than one color in the image."
        ],
        [
            "So when you have something like this in the image, you end up with this very complex 3D structure.",
            "And if you look at the surface of this treaty structure where these are points above the ground plane, we end up with this almost wireframe mesh like 3D surface correspond to a very textured surface.",
            "So that's how I visualized."
        ],
        [
            "Texture an in an image.",
            "So in the very simplest case, this is an image with no texture, which is a single color.",
            "This is a checkerboard pattern are responding to an image that's highly textured.",
            "Do you think we should consider is whether or not color and texture as important things to measure whether they're important over the image as a whole, or whether it is only particular parts of the image which are more or less important.",
            "So in the image of the Flowers, for example, are we concerned that the image is green and red or pink or we can send that it's really an image of a pink flower?",
            "So sometimes we want to concentrate our measurements on particular parts of the image.",
            "And if we want to do that well, then sometimes we can just overlay a grid on the image, and that's useful and convenient because our image compression functionality over laser grid on the image and processes block by block.",
            "So again we could take out the average color for each block by processing and parsing.",
            "The DCT coefficients are very efficient process.",
            "But sometimes a grid is not sufficient.",
            "Sometimes we're interested very in very accurate information about a specific object present in the image.",
            "So if we're interested in horses, for example, well, the side profile of a horse is a very definite shape, and if we want to find more side profiles of horses, we need to represent the color of the horse, but also the shape of the horse balance meat and show you some examples on Monday of object based based retrieval.",
            "Anne."
        ],
        [
            "OK, so it's still images, color, texture and the shape of potential objects present in the scene are important.",
            "Video if we consider what's important in video, well, it extends images along the temporal direction.",
            "So now we can measure the color and texture of each individual frame in the video sequence, so we can have 25 measurements per second for every block in the image, or every object in the image.",
            "But we can also now measure motion.",
            "So you saw how we compress MPEG BITSTREAM'S using these motion vectors, which is some measure of the motion that's present either to camera motion in the video sequence or the motion of objects such as the guys and playing ping pong.",
            "So we can define motion is where a pixel moves from one frame to the next or what's changed from one frame to the next.",
            "And of course, that information is already available to us from our compression process, so again, we can take the MPEG bitstream.",
            "We can simply parse the bitstream and we can extract the motion vectors and use those motion vectors to do interesting things.",
            "There are two kinds of motion we might want to measure.",
            "The first is what's known as global motion, and this is the kind of motion that we shot saw when the camera panned.",
            "In the example, with the speedboat, this is where the entire frame is moving because the camera is moving from one frame to the next and that corresponds to the motion of the camera.",
            "Man is carrying out and there are different kinds of camera motions such as pan, tilt and zoom, which is very specific.",
            "Definitions in the field of filmmaking.",
            "Order something called local motion.",
            "This is where the emotion is localized to specific parts of the image corresponding to, for example, in the previous example you guys hand OK where that's moving with a very coherent kind of motion relative to the rest of the scene."
        ],
        [
            "OK, so how do we represent color?",
            "Assuming that it's important color is very visually important to humans to humans and turns out that color features are very easy to compute and similarity metrics between color features are very easy to compute.",
            "So we use our well known well of global image histogram.",
            "OK, the benefits.",
            "So we can take an image like this and we can represent the color distribution in a vector histogram format like so.",
            "And then we can.",
            "We can compare histograms using a variety of histogram intersection methods.",
            "We can use that.",
            "Cosine measure we can use the Euclidean distance.",
            "We can use so on so forth.",
            "OK, so we can transform an entire image to a histogram and compare images very efficiently, and we'll get images which are similar in color.",
            "In other words, they have red and green and pink images of Flowers.",
            "An I'm not attractive because these histograms are invariant to translation and rotation, and they can be made invariant to the size of the image to a normalization process.",
            "An MPEG SEVEN has standardized a variety of histogram type color.",
            "Our features, so the scalable color descriptor is a histogram and HSV space, so it encodes the higher transform coefficients of the color distribution."
        ],
        [
            "Order MPEG 7 color descriptors an.",
            "Dominant color, for example, is target similarity, retrieval and image databases.",
            "So the algorithm involves a sequence of clustering steps where we try and group similar colors colored pixels in the image and then represent them via colored vector that includes the percentage pixel area covered by that color, the variance, the spatial coherency of the dominant colors and and there are other color descriptors that are specified by MPEG 7.",
            "Anne, I know Steven had a very interesting discussion about MPEG 7 in his lecture or after his lecture yesterday, and I think the comment that he made was that MPEG Seven was state of the art for its time.",
            "OK, but there's been significant work on feature description since down.",
            "That's very, very true.",
            "So people have been working on different ways of representing color, since MPEG Seven will standardize.",
            "However, if you're a new beginning PhD student working in this field and you're wondering, how do I represent color in my retrieval algorithm or in my?",
            "Relevance feedback algorithm.",
            "For example, the MPEG 7 features form a kind of lowest common denominator set that you can use.",
            "So there are good place to start effectively and that's why I over the course of this lecture repeatedly referenced the MPEG 7 features.",
            "They're well documented as loads of papers about them.",
            "Just source code out there for extracting them, so there are good good place to."
        ],
        [
            "OK.",
            "In terms of representing textured or variety of texture features available, it's been something that's been worked on.",
            "The image processing community for 20 odd years or so, if not longer.",
            "So in 1991, for example, there was suggested that very straightforward things like Co, occurrence matrices and autocorrelation function and edge frequency and primitive length could be used to represent.",
            "Texture features there are more sophisticated approaches which characterize the textures in the frequency domain.",
            "So in the wavelet domain and the higher domain or indicative or domain, for example, all of these are valid ways of representing texture.",
            "Anne and then there are other, perhaps more left of center ways of representing texture, so the mathematical morphological community spent a long time understanding how you could use the how you could use morphological primitives represent texture.",
            "An fractal coding was involved at one point as well as a very efficient way of using the fractal concept."
        ],
        [
            "Then texture here is one of the simplest ways of representing texture that's standardized in MPEG 7, so again, it's not particularly complex, and it's shown here as as a good grounding or a good starting point, so it's called the edge histogram.",
            "It represents the global texture distribution and image, and there's been some work in the literature which has used that represent local texture distribution as well.",
            "So the first thing is we do we take an image like our good old test image Lana here, and we perform edge based filtering on that.",
            "So we transform this image.",
            "Into an image that where the edges are highlighted and there are a variety of ways of doing that.",
            "So there are many edge detection algorithms.",
            "There are Robert, Sobel, canny and so on, so forth.",
            "And then once we have this image, we quantify the different kinds of images, different kinds of edges in the image.",
            "So we say that there are only five kinds of edges possible.",
            "We can have a vertical edge, we can have a horizontal edge.",
            "We can have a 45 degree edge, or we can have 100.",
            "35 degree edge and then we have a kind of a catchall of any edge which doesn't fit into these four categories.",
            "So we have 5 bin histogram which counts the occurrence of this.",
            "These kinds of edges in an image in the same way that we count the distribution of different kinds of colors in an image.",
            "And we can compare edge histograms in exactly the same way as we compare color histograms and we can find images that have similar edge distribution.",
            "So a majority of vertical lines for example, and that might be interesting if you're trying to find more images of buildings which tend to be characterized by vertical and horizontal lines.",
            "As opposed to images of natural scenes where you'll have non directional edges.",
            "More sophisticated edge.",
            "Or text."
        ],
        [
            "Sure.",
            "Classification can be done at a transform domain, so here's an example of taking it as an image from Irish news program.",
            "We're showing only the luminance component here, and we can decompose this image into a variety of frequency bands using this thing called a higher transform.",
            "So here's an example of the image, decomposed into a variety of different frequency frequency bands.",
            "To give you a concrete example, this frequency band here, H2 can be seen as a low pass horizontal filtering, which is followed by high pass vertical filtering, thereby enter emphasizing.",
            "Vertical frequencies and we have the converse in this image here, so we can transform this image into two frequency domain representations where we emphasize the horizontal or vertical edges and then we can extract some kind of measure such as a histogram or similar, and even the direct coefficients from the frequency domain from the filtering process from here to represent the edge distribution in this image."
        ],
        [
            "OK, so if we think back to our slide as to what's important in images, we said that color is important.",
            "We said that texture is important and I've shown you some slides which hopefully explains to you or convince you that it's actually quite straightforward to represent color and texture.",
            "OK, I'll be at a reasonably coarse granularity, but it'll work for image similarity.",
            "The third thing that we said was important was that sometimes it's not the entire image that's important, but specific parts of the image.",
            "So sometimes you want to break the image into smaller, more meaningful segments, where ideally those segments represent real world objects, such as the horse against the green background, for example.",
            "And this is an area of research that's known as image segmentation.",
            "Now it turns out that this is where the wheels come off the bus.",
            "OK, in terms of feature extraction because image segmentation is a very difficult thing to do accurately and robustly across many different kinds of images.",
            "So a histogram is a histogram and you can play histogram to any color image and you'll get a good representation of the color distribution of that image, irrespective of what that image represents.",
            "Segmentation, on the other hand, trying to detect the real world boundaries of objects and scenes in the image is a much more complex.",
            "Process and it's complex because it's difficult for us as humans to do that.",
            "So here are more of these optical illusions that park back to the semantic gap.",
            "So, for example, this image here is this face.",
            "Of a lady?",
            "Or is this a musician playing the saxophone?",
            "Similarly, is this a Native American, or is it an Eskimo but his back to us?",
            "Is this two faces?",
            "Facing each other?",
            "Or is it a non invasive table or something?",
            "OK. And depending on your interpretation, as humans, those images will mean different things to us.",
            "To try to develop an algorithm that extracts those the meaning in terms of the real world boundaries is very, very challenging, even when we have relatively simple images like this.",
            "OK, so this is an image from a standard video phone, video conferencing test sequence called Mother and Daughter, and you might think, well, it's very straightforward as a mother and child in front of the background.",
            "OK, but actually what constitutes the regions?",
            "The important region?",
            "This image is depends upon your application.",
            "So for example, is the motor important?",
            "Is the child important?",
            "Are both the mother and child important, or is the entire image in part?",
            "So segmentation is very much dependent upon what we want to extract from the image, which is very often not known in advance."
        ],
        [
            "So segmentation is had a very long history of research and it's a very popular topic.",
            "It's used in multimedia information retrieval, so this idea of extracting useful regions against which we can associate their color and texture descriptors for more fine grained retrieval.",
            "It's used in shape analysis for optical character recognition.",
            "I'll show you some examples of that.",
            "It's used in shape classification.",
            "Anne, it's used in medical image processing, so processing the artifacts in an X Ray or in CAT scans or an MRI, for example.",
            "It's obviously used in industrial vision systems such as automated packing and robotic assembly line applications, and it's also used in knowledge assisted analysis, and I think we've seen some examples of that in previous.",
            "Presentations I know.",
            "Stephen talked at length about that yesterday.",
            "We're trying to characterize what objects are in the image."
        ],
        [
            "So.",
            "The process of image segmentation is typically to try.",
            "The group pixels at into regions on the basis of some kind of homogeneity criterion.",
            "An so we try to look for clusters of pixels which are similar in terms of their color or in terms of their texture.",
            "Ann, and hopefully the output segments that we produce will reflect the real world structure of the of the image.",
            "Am and there are many, many different approaches to how you might perform this.",
            "What it's hard as a clustering process, so there are graph theoretic models, Markov random field approaches, probabilistic solutions, semi-automatic approaches.",
            "So here is an example of an image and the result of four different segmentation process is applied to it and you can see that the results are all similar, but they're all different.",
            "OK. And of course it's this poses a problem for image understanding and for content based retrieval.",
            "So if for example you're interested in the boat in this image.",
            "And you use this segmentation algorithm.",
            "Well, you suddenly lost your boat and you'll never be able to retrieve that image with that boat.",
            "And if that's your information need.",
            "OK."
        ],
        [
            "So the challenges of segmentation can be characterized in two ways.",
            "First is what's known as over segmentation, and this is the idea where the segmentation algorithm is all about merging or clustering groups of pixels.",
            "M is stopped too early, so typically you start with every pixel as an independent entity independent region and you start to merge them and at some point you have to stop and when you stop the resulting clusters should correspond to real world objects or components of real world objects.",
            "Here's an example for the form and test sequence where that's not the case.",
            "OK, so we've stopped the merging process much too early.",
            "We end up with a very large number of arbitrarily shaped regions in the image, which are arguably completely useless to us in a content based information retrieval framework.",
            "On the other hand, we have a process known as under segmentation, and this is where we stop the algorithm too late.",
            "So we continue merging until we actually lose what we're actually looking for.",
            "So here's an example of this young boy on the beach.",
            "We run the segmentation algorithm.",
            "We let it run, and we stop it too late, and hey presto, we've lost the guys face.",
            "OK, so we're left with a headless child.",
            "So a key aspect of research and segmentation is finding smarter merging processes and or clever stopping criteria trying to understand when to stop.",
            "There are other."
        ],
        [
            "Challenges associated with segmentation that make it a very problematic technology will be the very desirable technology for information retrieval, so it's an ill ill posed problem.",
            "We've seen that it depends on what the application is.",
            "That's what the segmentation should be.",
            "There are other problems, such as what criteria should be optimized when we're trying to develop a segmentation algorithm.",
            "In other words, what features are most important color texture, edges or some combination of these?",
            "Which grouping principles, which clustering principles are most important when we're trying to construct these segments?",
            "Is it their similarity in terms of their pixel values or texture values, or is it the fact they are close to one another?",
            "Or is it the fact that when we merge them seen exhibits, some kind of symmetry, or some kind of compactness?",
            "Also, we're going to combine some of these features in what proportion do they interact?",
            "So there's a significant research effort undergoing which is trying to understand how to efficiently optimize such criteria.",
            "So if we assume that there is no one Golden segmentation algorithm that's going to work for all images for all applications we need to take existing segmentation algorithms and optimize the more tailored and for specific types of content of specific applications.",
            "And the question is, how do we do that?"
        ],
        [
            "So.",
            "That leads to the next challenge.",
            "The next question, which is that in order to optimize something in terms of whatever criteria we need to understand what a good result is, not a bad result is beautiful construct some kind of energy function that we can we can minimize.",
            "So before we can develop that better segmentation algorithms, we need to answer what makes a particular segmentation good or what makes a particular segmentation.",
            "But what makes one segmentation better than another segmentation in a given application?",
            "And we need some way of evaluating segmentation.",
            "So this is another problem with segmentation in that.",
            "We as humans can look at the contours of the regions produced and say that's a good segmentation.",
            "The contours are bang on the object boundaries.",
            "But how do we measure that objectively?",
            "For image quality we have measures like SNR for example which allow us to gauge objectively the quality of an image that in some way reflects the human visual system.",
            "But for something as subjective as image segmentation, how do we come up with a single number that represents good or bad segmentation results?",
            "So a key research topic in this area that's ongoing and you're particularly active in this area is to develop a comprehensive framework for characterizing segmentation and evaluating their performance in the context of specific applications.",
            "This is a very nice research topic, which is that OK if you want to use segmentation, here's a way of taking the available existing state of the art, which is extensive characterizing it in different ways and settling upon the algorithm that's best suited to your application.",
            "So this speaks to Steven's point yesterday when he said that segmentation is great, but very often it's very hard to use the existing state of the art on it's normally less hassle to just go and develop your own segmentation algorithm.",
            "OK, which obviously is not ideal because bending the wheel time and time again."
        ],
        [
            "So here's the solution we developed the CU to segmentation problem that we're working with.",
            "Targeting the idea of what we call syntactic segmentation, so most segmentation techniques group pixels in the basis of color and texture and don't really take account of the actual real world content of the image.",
            "So what we're doing here is trying to have a pseudo generic approach that doesn't look for specific objects but looks for the characteristics of objects in images.",
            "So we start with an input image like to form an image.",
            "We perform a fine region based segmentation.",
            "So here is this 255 regions.",
            "And we end up with a grouping of pixels like this.",
            "OK, where again the regions don't really mean anything, except that this arbitrarily shaped region is all white.",
            "And then perform some further processing on that segmentation result.",
            "So for example, we start to merge regions on the basis of histogram matching.",
            "So we take our color histograms that we normally apply to full images.",
            "We apply them to individual regions and we merge similarly colored regions and we can go from a segmentation that looks like this.",
            "The one that looks like this.",
            "So here to regions OK, they still don't mean an awful lot.",
            "OK, but they better reflect the real world structure.",
            "The next step, then, is to say that, well, you know what?",
            "Typically, in a real world environment, we don't have weak borders in our segmentation, so we don't have these kind of arbitrary lines in the segmentation, which are the result of lighting effects.",
            "So we take each region.",
            "We characterize each region in terms of its boundary complexity, and we remove the very very complex boundaries that allows us to end up with a segmentation like this, which again is perhaps more useful in the previous one.",
            "We can then say, well, you know what in the context of of content based information retrieval we're interested in generating an image or a segmentation of images are relatively small.",
            "Number of the most important regions.",
            "We want large regions that cover a significant portion of the image, so let's get rid of small regions and regions which are included by significantly larger regions.",
            "So in this case we get rid of some of the eyes.",
            "We then say, well, you know what most regions in the real world tend to be reasonably compact and reasonably not particularly complex, so we're looking at the overall shape complexity of regions.",
            "We can further filter the.",
            "Image regions and end up with a segmentation result that looks like this.",
            "So we're not using any real knowledge of the content of the image.",
            "We don't know this is an image of a guy wearing a hat on a built background.",
            "OK, by trying to incorporate some pseudo semantic can use that.",
            "Features of real world objects into the merging process.",
            "We can end up with potentially more useful segmentation."
        ],
        [
            "Here's an example of the exact same process applied to different kinds of images, and as you would expect, we end up here.",
            "Going from this image of the eagle in the corral database set, I think to this set of colored regions on a blue background.",
            "Which is more useful and more tractable and more easy to process in an information retrieval perspective.",
            "Because now we can assign color or texture descriptors to a much smaller subset of regions where the regions in this case actually have some semantic meaning, and there are other examples here.",
            "This image has significant lighting variation across this scene, so if you were to throw a typical region clustering approach or image clustering approach to this, you end up much noisier segmentation, whereas we can end up with a smaller number of regions that are easier to.",
            "Process and so on and so forth."
        ],
        [
            "OK, so that leaves me nicely just before the break into my first demo.",
            "And.",
            "This whole area of an image segmentation region based segmentation to support content based information retrieval.",
            "Ann is a significant ongoing effort within an European project called Pay space for anyone who's actually working on K space or funded by K space.",
            "A couple of people, so one of the work packages in the research program is focused specifically at this technology and the point was that, well, there's many academic partners in this case based project.",
            "We all have our own pet approach to segmentation that we're very happy with and very proud of, and I've just shown you DC use approach, but your groups have equally good and equally interesting approaches, so the idea of this part of the project was to develop a web service for segmentation.",
            "OK for the broader community, so.",
            "If you are working on content based information retrieval and you don't care about segmentation, would you want to use segmentation in some of your experiments for your PhD?",
            "Well, the idea is that you can use this web service without the hassle of going and reading the literature and implementing your own version of these segmentation algorithms.",
            "So the idea is you go to this website.",
            "An you can browse images from your hard drive.",
            "You can upload the image.",
            "And if the network is working, this should offload.",
            "OK, so I've got a problem with my my network.",
            "Anne.",
            "So you get the idea anyway.",
            "You can upload the image, you can select a variety of different segmentation algorithms, so this is the one I just showed you.",
            "There are two orders currently integrated at the moment there's the image uploaded so you can specify the.",
            "The segmentation algorithm you want to run.",
            "Run the segmentation algorithm and again the network is slow here, but the result will be will be produced.",
            "That's that image uploaded and segmented in real time on the DCU server and a variety of different ways of viewing it.",
            "So that's the average color for each region.",
            "You can view the outline.",
            "You can view the segmentation mask that's produced.",
            "You can view the bounding boxes, so sometimes for information retrieval you don't actually need segmentation boundary.",
            "You just need the bounding box around that OK.",
            "So there's an example of the outline.",
            "Here is an example of the segmentation mask that produced so you can upload an image and you can download these results OK, you needn't worry about how the algorithm actually works.",
            "You can of course specified the parameters of the algorithm, so here are the parameters we've exposed in that particular segmentation algorithm can play around with those to get a better or worse segmentation.",
            "And we also support batch upload.",
            "So you say, well, that's great, but that's for a single image.",
            "But there is an interface whereby you can upload a sequence an entire directory of images, or indeed an MPEG file or an Avi file.",
            "The segmentation algorithm will chug along John Eudy output in real time, and then you can download the sequence of mass sort of sequence of conference.",
            "OK, now we're using this in K space as a stable.",
            "Level playing field to compare segmentation algorithms which helps us develop better segmentation algorithms.",
            "But we believe it's also potentially useful resource for anybody who needs access to segmentation.",
            "That the hassle of going in implementing it themselves.",
            "OK, so that's freely available on the net.",
            "Encourage anyone interested going going play with him going and going to use it.",
            "OK, that brings up the coffee break time, so it's 10:30.",
            "Now we reconvene at 11 for the second part, electric."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "I hope you all had a nice evening last night.",
                    "label": 0
                },
                {
                    "sent": "The city centre, yes.",
                    "label": 0
                },
                {
                    "sent": "Everybody, bright eyed and bushy tailed for this morning.",
                    "label": 0
                },
                {
                    "sent": "OK, well I'll try and I'll try and keep you awake.",
                    "label": 0
                },
                {
                    "sent": "My name is Noel O'Connor.",
                    "label": 0
                },
                {
                    "sent": "I'm from Dublin City University in Ireland an I'm in from the School of Electronic Engineering and I work very closely with Alan Smeaton who gave a lecture I think on Monday at this summer school.",
                    "label": 1
                },
                {
                    "sent": "M. My background is from a signal processing background.",
                    "label": 0
                },
                {
                    "sent": "Originally so I started my research in the area of image and video compression and the technologies underpinning popular standards like JPEG.",
                    "label": 0
                },
                {
                    "sent": "Egg one MPEG, two and MPEG four, and with the advent of MPEG four, I moved into the area of content analysis, video analysis in particular, and image processing.",
                    "label": 0
                },
                {
                    "sent": "An image image analysis would have used to supporting object based functionality in MPEG four which at the time was a very hot topic and with the advent of MPEG 7 then subsequently moved into the area of audiovisual analysis for feature extraction which is around the time I started working with information retrieval specialists like Alan.",
                    "label": 0
                },
                {
                    "sent": "Review to supporting information and retrieval, and then most recently I've started working with the likes of Stephen, who gives a wonderful presentation yesterday to support semantic web type, an analysis and understanding of media.",
                    "label": 0
                },
                {
                    "sent": "The focus of today's talk is going to be on multimedia signal processing, so I'm going to try to explain or motivate why we need to do feature extraction from audiovisual content and how in fact we'd go about doing that for specific applications.",
                    "label": 0
                },
                {
                    "sent": "So I'll start with some motivation and some.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Focus for the lecture as to why we need to do audiovisual processing.",
                    "label": 0
                },
                {
                    "sent": "For knowledge extraction, an and.",
                    "label": 0
                },
                {
                    "sent": "As a result of this, hopefully having convinced that it's an important thing to do, I'll then try and convince you that.",
                    "label": 0
                },
                {
                    "sent": "Well, if we want to extract useful features from audio and video content, we need to understand how it's represented from the very basics, from capture time up to compression up delivery.",
                    "label": 1
                },
                {
                    "sent": "I then talk about the individual features that we can extract from audio, visual and text with emphasizing what's important where.",
                    "label": 0
                },
                {
                    "sent": "What's important typically corresponds to what's important to us as humans in terms of consuming content or what's important in terms of supporting functionality for certain applications, and I'll finish with some examples of bringing together different kinds of features for different types of analysis for different types of content.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Am it's also always important these kinds of things to explain what's not covered in this lecture, so you understand where I'm coming from and what my main motivation is in my main focus, so I won't focus on text analysis, which is arguably a very important part of multimedia.",
                    "label": 0
                },
                {
                    "sent": "I will, however, show how text can be used in combination with some of the techniques that I'm going to describe to extract some very useful and very interesting information from audiovisual content an I do cover audio to a certain extent in this presentation.",
                    "label": 1
                },
                {
                    "sent": "I'm not an audio expert, I wouldn't claim to be.",
                    "label": 0
                },
                {
                    "sent": "So I don't have a deep discussion of audio processing.",
                    "label": 1
                },
                {
                    "sent": "I simply give you a high level overview to explain how there's useful information in an audio bitstream that can be extracted to assist in content based indexing and retrieval.",
                    "label": 0
                },
                {
                    "sent": "So in a sense I only scratched the surface of what's possible with audio processing examples.",
                    "label": 0
                },
                {
                    "sent": "I show a very, very rudimentary and of course there's much more sophisticated work out there by real experts in the field and the likes of Thomas Sykora's Group in Berlin.",
                    "label": 1
                },
                {
                    "sent": "Mark Sanders Group in Queen Mary University Anne Gaelle Richards.",
                    "label": 0
                },
                {
                    "sent": "Of course, in France.",
                    "label": 0
                },
                {
                    "sent": "And as I go through the presentation, you'll see that most of my slides are a lot of my slides are peppered with these references, which are meant to be pointers to further reading for you.",
                    "label": 0
                },
                {
                    "sent": "Anna's PhD students are beginning PhD students in this in this field, and the slides will be available at the end of the of the seminar in PDF format for download.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and thinking about how to present this to new PhD students.",
                    "label": 0
                },
                {
                    "sent": "I guess most of your PhD students, right?",
                    "label": 1
                },
                {
                    "sent": "Yeah, OK, that's good.",
                    "label": 0
                },
                {
                    "sent": "'cause if you weren't, you'd really do derail my talk from this point then.",
                    "label": 0
                },
                {
                    "sent": "So thinking about what to say to new PhD students are PhD students are in the depths of their of their thesis work.",
                    "label": 0
                },
                {
                    "sent": "So as you know, and as you learn, going through your thesis, it's very important to have a hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That's what your thesis is based upon.",
                    "label": 0
                },
                {
                    "sent": "OK, so you need to convince people about something that you believe in, and if you can convince external reviewers on your peers, that will result in a large number of publications in journals and conferences.",
                    "label": 0
                },
                {
                    "sent": "And more importantly, if you can convince your supervisor on your external examiner, hopefully they'll award you with your PhD and I felt coming here to this kind of environment that it's important that we as lecturers lead by example.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to have hope hypothesis in this presentation and over the next few hours I'm going to try and convince you guys of something OK, and my hypothesis for this presentation is that PhD students never had it so good, OK?",
                    "label": 1
                },
                {
                    "sent": "Or in other words, now is a great time in this field to be doing a PhD.",
                    "label": 0
                },
                {
                    "sent": "I also refer to this as the.",
                    "label": 0
                },
                {
                    "sent": "Grumpy supervisor terum.",
                    "label": 1
                },
                {
                    "sent": "OK, which is a capsulated by the phrase that like it wasn't like that in our day.",
                    "label": 0
                },
                {
                    "sent": "And by our day I mean my day your day, Steven and all the other lecturers days, OK?",
                    "label": 0
                },
                {
                    "sent": "And that's not meant to sound critical.",
                    "label": 0
                },
                {
                    "sent": "Any way, shape or form.",
                    "label": 0
                },
                {
                    "sent": "It's simply trying to motivate you that there are very exciting problems to be addressed and to be considered in this emerging research field, which brings together audio processing, video processing and the semantic web.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the motivation for the lecture under the focus so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, the motivation for why we're all doing research in this area.",
                    "label": 0
                },
                {
                    "sent": "I guess to a greater or less extreme strength extent is the recent growth in multi medium which has really blossomed and bloomed in recent years due to the decreasing cost of digital capture and storage.",
                    "label": 0
                },
                {
                    "sent": "So it's now easier than ever before to take photographs to capture audio to take video footage and to store this vast amount of content in your own personal repository.",
                    "label": 0
                },
                {
                    "sent": "Now of course, broadcasters have been doing this for many many years.",
                    "label": 0
                },
                {
                    "sent": "OK, but now we are all content producers.",
                    "label": 0
                },
                {
                    "sent": "We all produce significant volumes of content.",
                    "label": 0
                },
                {
                    "sent": "To try and give you a flavor for for how significant this growth is, I have some statistics which I've pulled from various sources in the web which tried to highlight the sheer enormity of content being produced.",
                    "label": 0
                },
                {
                    "sent": "So there are studies which have shown that there were 490 digital photographs taken with each digital camera in the EU on average in 2006.",
                    "label": 1
                },
                {
                    "sent": "Pam, the estimate is that half a trillion digital images will be captured in 2009.",
                    "label": 0
                },
                {
                    "sent": "Camera production and this doesn't include camera phones.",
                    "label": 1
                },
                {
                    "sent": "This is your common or garden traditional digital camera.",
                    "label": 0
                },
                {
                    "sent": "Is reach will reach 89 million units in 2010 in Europe and worldwide camera phone sales was 370 million units in 2729 billion.",
                    "label": 0
                },
                {
                    "sent": "Digital images were taken and 847 million units are projected for 2009.",
                    "label": 0
                },
                {
                    "sent": "And of course, these are only some examples of the kinds of content production mechanisms that are in place.",
                    "label": 0
                },
                {
                    "sent": "An constantly gathering data.",
                    "label": 0
                },
                {
                    "sent": "There's another very good example which is CCTV.",
                    "label": 0
                },
                {
                    "sent": "So due to recent world security concerns, we've witnessed a very strong growth in the deployment and capture of closed circuit television footage for security and for safety.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the UK alone there are 4.2 million cameras that corresponds to 1 camera for every 14 people in the UK.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's the first brick in my argument is that it's great to be a PhD student these days in this area because there's lots of data and lots of data.",
                    "label": 0
                },
                {
                    "sent": "Means lots of interesting problems, lots of different kinds of data means lots of different kinds of problems, so there are many, many PhD theses topics to be researched.",
                    "label": 0
                },
                {
                    "sent": "OK, back when I was doing my PhD back when dinosaurs ruled the earth many many, many years ago, we really had a problem with using or getting our hands on large quantities of test data.",
                    "label": 0
                },
                {
                    "sent": "So for example, when I was working on video compression, we had a small handful of MPEG 4 standard standard test sequences that everybody used, and by small I mean about 8 to 10 sequences of 300 frames each.",
                    "label": 0
                },
                {
                    "sent": "OK, a very small amount of data to work with.",
                    "label": 0
                },
                {
                    "sent": "Of course you could gather your own data and work with that, but then you couldn't publish your results.",
                    "label": 0
                },
                {
                    "sent": "OK, because it was your own data and there wasn't an agreed benchmark.",
                    "label": 0
                },
                {
                    "sent": "These days we have very large, very large number of commonly agreed test corpuses such as the Trek fit.",
                    "label": 0
                },
                {
                    "sent": "Collection OK, which is a large archive of test content for or search and retrieval for shot boundary detection for feature extraction.",
                    "label": 0
                },
                {
                    "sent": "Most recently for event detection from CCTV type scenarios for copy detection and we have image clay collections.",
                    "label": 0
                },
                {
                    "sent": "We have large collections of text repository and so on and this is a very exciting development 'cause it means it's a very stable framework for you as a PhD student.",
                    "label": 0
                },
                {
                    "sent": "To get this data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use this data and to prove convincingly and conclusively that the technology you develop extends the state of the art.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So back to the huge growth of multimedia content.",
                    "label": 1
                },
                {
                    "sent": "Well, of course all of this content is useless unless we can actually access the relevant content that we need, and that means being able to index the audio, image, video data based on what it contains based on its contents or something other than time and date of creation and who the author was, what the actual content is depicting.",
                    "label": 1
                },
                {
                    "sent": "And that's what underpins the whole area of content based information retrieval.",
                    "label": 1
                },
                {
                    "sent": "Now, ideally we'd like to be able to perform this indexing in real time, or sorry, completely automatically, because studies have shown through the likes of our national broadcaster in Ireland, for example, in our TE, where they're charged with archiving and indexing everything that's produced in Ireland for reasons of national heritage that to properly index one hour of video content.",
                    "label": 0
                },
                {
                    "sent": "Broadcast video content requires 8 hours of manual effort.",
                    "label": 0
                },
                {
                    "sent": "OK, so any inroads you can make into reducing the burden on the annotator or the indexer is a huge saving in terms of human resources and in terms of financial resources.",
                    "label": 1
                },
                {
                    "sent": "We'd like to be able to index the content based on what it represents in terms of who's present, what's depicted where it's occurring, why it's occurring when it's occurring, and how is occurring.",
                    "label": 0
                },
                {
                    "sent": "Or in other words, the underlying semantics.",
                    "label": 0
                },
                {
                    "sent": "The real world semantics of what the content represents.",
                    "label": 0
                },
                {
                    "sent": "Of course, if we can do this, there are many advantages and just not just in the field of content based information retrieval, but it allows our paves the way for other advanced content functionality such as advanced interaction, improved compression, autonomous intelligent content that understands itself and can migrate from user to user in person to person, and so on.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, given that we want to extract The Who, what, where, why, when and so on from this huge volume of content, there is very clearly a need for machine computable techniques for extracting the real world content from the video and audio data are responding to the semantics of what it represents.",
                    "label": 0
                },
                {
                    "sent": "And just to get some terminology in the first instance at the applying place this discussion an it was a very interesting discussion in this paper or in his book by Young and his colleagues which tried to classify the various different stages of semantic knowledge extraction in the context of multimedia data.",
                    "label": 0
                },
                {
                    "sent": "And they refer to image, Audio and video processing is where you take the image data in and you produce an image out.",
                    "label": 0
                },
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "For example, in sorry I'm using I'm using image in each of these examples as a single example of 1 modality.",
                    "label": 0
                },
                {
                    "sent": "And so this is an example of taking an image in, performing some processing on the image that makes it easier to subsequently extract features or measurements.",
                    "label": 0
                },
                {
                    "sent": "That's the next stage with the actual analysis taking the processed image and performing some measurements on it that are useful for inferring some kind of knowledge.",
                    "label": 0
                },
                {
                    "sent": "And in the final stages the image and video and audio understanding where you take the measurements derived from the input image and produce a high level description as a result.",
                    "label": 0
                },
                {
                    "sent": "And this is where we're where we're working towards OK.",
                    "label": 0
                },
                {
                    "sent": "In this lecture I'll be focusing in the second level in this, which is taking an image or a video or an audio sequence in and producing some useful measurements where those measurements are termed features.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, in this aspiration of extracting semantic knowledge, OK, there is a a key problem that hopefully we're all aware of, and that's probably underpinning most of your PHD's that's commonly referred to as this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Month gap problem.",
                    "label": 0
                },
                {
                    "sent": "So the problem is the difference between what we can measure from an audio or a video signal and what that signal actually means to a human in a given application scenario.",
                    "label": 1
                },
                {
                    "sent": "So this was put very succinctly and very elegantly by Arnold Smolders and is seminal paper, which I presume everybody in this room has read it.",
                    "label": 0
                },
                {
                    "sent": "If not, you should read this paper.",
                    "label": 0
                },
                {
                    "sent": "It's a very important paper, and he states the semantic gap as the lack of coincidence between the information that one can extract from the visual data and the interpretation of the same data has for a user in a given situation.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to try to explain what exactly we mean by the semantic gap as a means of motivating summer technologies, I'll talk about let's consider the semantic gap in the context of image data just for the moment as opposed to audio or video.",
                    "label": 0
                },
                {
                    "sent": "So this idea of the semantic gap isn't the new idea.",
                    "label": 1
                },
                {
                    "sent": "OK, it's been around for a very, very long time, so it was put by Napoleon Bonaparte many, many years ago.",
                    "label": 0
                },
                {
                    "sent": "Who said I won't even attempt to try and speak to French and embarrass myself with my dreadful French accent and pronunciation?",
                    "label": 0
                },
                {
                    "sent": "But he said a good sketch is better than a long speech.",
                    "label": 1
                },
                {
                    "sent": "Similarly, the Russian author I I've intergen I've said a picture shows me at a glance what it takes.",
                    "label": 1
                },
                {
                    "sent": "It dozen pages of a book who expound?",
                    "label": 0
                },
                {
                    "sent": "Or perhaps we're more familiar with the saying, which says that a picture is worth 1000 words.",
                    "label": 0
                },
                {
                    "sent": "Which, incidentally, wasn't coined by Confucius as is sometimes thought to be the case, but was actually originally accredited to this guy, Fred Barnard, writing in a trade Journal in 1921, trying to encourage his industry to use images on the sides of St cards to promote advertising and products, and this was his rationale for doing it rather than putting text up there, you should put.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An image.",
                    "label": 0
                },
                {
                    "sent": "For me the best and most distinct, all embracing statement of the semantic gap is what Richard Hamming said, which is that the purpose of computing is insight, not numbers.",
                    "label": 1
                },
                {
                    "sent": "In other words, we want to understand not just measure or not just compute.",
                    "label": 0
                },
                {
                    "sent": "So here are three examples of images.",
                    "label": 0
                },
                {
                    "sent": "OK, and these images it's quite straightforward, as we will see for the remaining of this lecture to extract measurements from these images.",
                    "label": 0
                },
                {
                    "sent": "But the question is what do these measurements actually mean?",
                    "label": 0
                },
                {
                    "sent": "And of course, for us, the problem is that even for humans OK with all our depth of experience on our evolution, hereditary and so on, it's not clear sometimes what a picture represents.",
                    "label": 0
                },
                {
                    "sent": "So for example, these are optical illusions corresponding to.",
                    "label": 0
                },
                {
                    "sent": "Is this a side profile of a of an old woman.",
                    "label": 0
                },
                {
                    "sent": "Or at back profile of the other young lady.",
                    "label": 0
                },
                {
                    "sent": "Now everyone see the difference there, yeah?",
                    "label": 0
                },
                {
                    "sent": "Similar here, is this a single face bisected by a candle stick or is it 2 side profile faces looking at each other?",
                    "label": 0
                },
                {
                    "sent": "And here is an example I created myself.",
                    "label": 0
                },
                {
                    "sent": "OK, which is the result of an automatic segmentation process that we will talk about.",
                    "label": 0
                },
                {
                    "sent": "This is where we try and extract the foreground from the background.",
                    "label": 0
                },
                {
                    "sent": "Can anyone suggest what this might be?",
                    "label": 0
                },
                {
                    "sent": "Well, I'm not surprised you can't guess it's actually the silhouette of somebody juggling.",
                    "label": 0
                },
                {
                    "sent": "OK and entertainer juggling, so this is his arm raised.",
                    "label": 0
                },
                {
                    "sent": "We've lost some of the hand here.",
                    "label": 0
                },
                {
                    "sent": "This is actually the ball rolling down his his forehead and this is his other hand ready to catch the ball.",
                    "label": 0
                },
                {
                    "sent": "OK, and during the course of this lecture will see how we can take an image of a juggler and we can produce something like this.",
                    "label": 0
                },
                {
                    "sent": "OK, but the problem of course, is that how do we interpret this then as a juggler?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this whole area is a hugely active research field.",
                    "label": 1
                },
                {
                    "sent": "It's been the basis for many national and EU projects, so some of you be familiar with K space or Ace Media or bowl Mia Mesh which are hosting this summer school in this event.",
                    "label": 1
                },
                {
                    "sent": "And there are many different applications and content domains targeted by this line of research.",
                    "label": 0
                },
                {
                    "sent": "So in the area of news, sports content, movie content, personal collections of images and video, and there of course, as you would expect, many many, many different approaches attempting to solve the problem way too many to list or to go through in this particular presentation.",
                    "label": 0
                },
                {
                    "sent": "The bottom line, I suppose, is that there is been a very very large amount of public investment and industrial investment into this research area in terms of research funding, and there have been hundreds of PHD's produced already in this area and there will be hundreds of PHD's produced in the future, and there are thousands of interesting papers coming out, so the bottom line is that this will be a very active, very fertile research area for many years to come because I would claim that the semantic gap isn't actually a gap but is a chasm.",
                    "label": 0
                },
                {
                    "sent": "In other words, a huge rift that we have to try to breach, and that we're only now starting to build the foundations of bridges that will help us cross that gap.",
                    "label": 0
                },
                {
                    "sent": "So that's my second, an proof that now is a good time doing PhD, so the semantic gap means that there are lots of interesting and exciting things for you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do, and most importantly things for you to do, working with colleagues from traditionally complementary research areas.",
                    "label": 0
                },
                {
                    "sent": "So in the past our research areas tend to be quite insular in that we focus on a particular area individually to the exclusion of all else in order to try to address the semantic gap we need to collaborate beyond our typical or traditional comfort zone.",
                    "label": 0
                },
                {
                    "sent": "So if you're an image processor, you need to work with audio processing people.",
                    "label": 0
                },
                {
                    "sent": "You need to work with semantic web experts like Steven and his group and his community.",
                    "label": 0
                },
                {
                    "sent": "And we need to bring all of these different research areas and researchers together with A view to making real progress.",
                    "label": 0
                },
                {
                    "sent": "And this means that that you have people working together who traditionally didn't work together in the past, producing very interesting.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Else.",
                    "label": 0
                },
                {
                    "sent": "OK, in terms of a taxonomy of approaches of how we might tackle this overarching problem well, I've tried to classify a number of different approaches in the literature from a very, very high level perspective, so we typically have the bottom up approach, which is usually coming from the computer vision community, image processing, community audio analysis, and audio processing community, and this is where we where the Community tends to focus on very specific application niches and challenges with view to extracting it very specific.",
                    "label": 0
                },
                {
                    "sent": "Kind of information that perhaps is only relevant to that particular piece of content.",
                    "label": 0
                },
                {
                    "sent": "Anne, however, having said that.",
                    "label": 0
                },
                {
                    "sent": "The result of that huge investment in those areas is resulting in a variety of more mature and more magic generic solutions for very complex analysis operations, which can be leveraged in the context of content based information retrieval.",
                    "label": 0
                },
                {
                    "sent": "Then we have the what I would term this top down approach whereby we start with tools for ontology creation and management and then we attempt to link low level features to these ontologies and that's a very exciting development.",
                    "label": 1
                },
                {
                    "sent": "That means that as a result of the effort into this area we have a large number of custom built ontologies defining important concepts that can help guide our research on audiovisual analysis.",
                    "label": 0
                },
                {
                    "sent": "So we have experts who are telling us here is the kinds of things we need you to extract from the audiovisual content.",
                    "label": 0
                },
                {
                    "sent": "And then we have the more generic approach whereby we take robust machine learning approaches that can be broadly applied to detective variety of different concepts, for example.",
                    "label": 1
                },
                {
                    "sent": "So this is where we take a large number of audiovisual features and use something like a support vector machine or a Bank of support vector machines to detect a wide range of concepts.",
                    "label": 0
                },
                {
                    "sent": "So the result of this is that we have a set of classifier banks, a variety of early and late Fusion strategies which all produce very promising results in terms of now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extraction.",
                    "label": 0
                },
                {
                    "sent": "M Here are my favorite papers.",
                    "label": 0
                },
                {
                    "sent": "More or less map to what I believe are some of the most important problems in the area.",
                    "label": 0
                },
                {
                    "sent": "And so there's been some very interesting research recently from the computer vision community in terms of invariant image features.",
                    "label": 0
                },
                {
                    "sent": "So what are stable, interesting low level features?",
                    "label": 0
                },
                {
                    "sent": "Are measurements in movies or smoking in movies?",
                    "label": 0
                },
                {
                    "sent": "Now he's about the theoretical framework which allows you to detect these kinds of generic periodically, reoccuring actions, and then with people who are working on combining audio and visual analysis.",
                    "label": 0
                },
                {
                    "sent": "Sorry people working on the audio semantic so such as the groups I mentioned earlier, we're doing some very interesting works on technologies such as source separation, recognizing specific instruments in music, recognizing structure of songs and music videos, and so on, and with people who are taking the results from a variety of these different areas and combining them to detect audiovisual events.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if I had to recommend as a result of this this lecture a reading list that you do well to go off and chase down, this is what this slide is meant to be.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Continue on this on this team in the top down approach with some very interesting work coming out of it.",
                    "label": 0
                },
                {
                    "sent": "I search here in Greece, Stephens work.",
                    "label": 0
                },
                {
                    "sent": "Of course an on the far side of the Atlantic.",
                    "label": 0
                },
                {
                    "sent": "The large scale monthly multimedia ontology that I guess Alexander Huffman talked about on Monday is a very interesting development.",
                    "label": 1
                },
                {
                    "sent": "And then the work in media Webmail, Marcel Worring and his colleagues where they've trained a Bank of classifiers to detect over 100 concepts, sometimes with very high levels of precision.",
                    "label": 0
                },
                {
                    "sent": "And recall again very interesting reading and I would guess that by the end of your of your PhD work you have come across.",
                    "label": 0
                },
                {
                    "sent": "If not having had an intimate knowledge of more.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of these, alters under under work.",
                    "label": 0
                },
                {
                    "sent": "OK, the point is, however, for the purposes of this lecture, is that for nearly all of those approaches that I've just mentioned in the previous two slides, the first step is calculating some measurements from the signal, extracting something upon which you can perform the higher level classification, or the higher level analysis.",
                    "label": 1
                },
                {
                    "sent": "So these measurements are typically called features, and they can be used to train classifiers that can be mapped to higher level concepts via ontologies that can be used in a variety of inferencing processes.",
                    "label": 1
                },
                {
                    "sent": "So this lecture focuses on this idea of low level.",
                    "label": 1
                },
                {
                    "sent": "Feature extraction.",
                    "label": 0
                },
                {
                    "sent": "M and specifically, what can we or what should we measure from a signal that makes sense?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a focus on feature extraction for CBI are by necessity.",
                    "label": 1
                },
                {
                    "sent": "We need to understand how images, video and audio are actually represented.",
                    "label": 1
                },
                {
                    "sent": "In order for those features to make sense an given that we understand how we represent digital audiovisual data, I'll describe a selection of useful features.",
                    "label": 1
                },
                {
                    "sent": "I'll show examples of the usefulness of individual features.",
                    "label": 0
                },
                {
                    "sent": "I'll show examples, have combinations of features can be used an I'll cover a lot, but in each case I'll provide pointers to further reading where that you can follow up on.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's start with the with the basics, the basics.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of audio visual capture an this is the part where you might fall asleep 'cause many of you be very familiar with that, but for the purpose of some of you who aren't, let's start at the very beginning and work our way up.",
                    "label": 0
                },
                {
                    "sent": "So we know that a digital image is captured when a camera scans.",
                    "label": 1
                },
                {
                    "sent": "As seen, it typically scans tree arrays of samples corresponding to red, green and blue, and the density of samples, number of samples per per unit area gives us the resolution of the image.",
                    "label": 0
                },
                {
                    "sent": "A video on the other hand, is captured when a camera scans as seen at multiple time instance.",
                    "label": 1
                },
                {
                    "sent": "So it's like a single image being captured very, very quickly to 25 frames per second.",
                    "label": 0
                },
                {
                    "sent": "For example, even higher resolution or temporal resolution, depending on the application.",
                    "label": 1
                },
                {
                    "sent": "In this case, each sample each time instance is called a video frame, which gives rise to a frame rate.",
                    "label": 0
                },
                {
                    "sent": "So for example, your TV at home your full motion video is 25 Hertz at 25 frames per second, whereas the video you might get on your mobile phone.",
                    "label": 1
                },
                {
                    "sent": "It is typically only 8 to 15 frames per second.",
                    "label": 0
                },
                {
                    "sent": "The difference you notice perceptually from that is that one will be more jerky as opposed to a more free flowing full motion video.",
                    "label": 0
                },
                {
                    "sent": "Audio Dennis captured when a microphone temporary samples sound waves.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To visualize that this is our are seen to be captured are still camera.",
                    "label": 0
                },
                {
                    "sent": "Captures red, green and blue arrays of samples.",
                    "label": 0
                },
                {
                    "sent": "Each sample is called a pixel, and Lee each pixel in each channel.",
                    "label": 0
                },
                {
                    "sent": "Red, green, and blue is represented by 8 bits are responding to 256 different levels of green, blue and red, and the point being of course you can combine the red, green, and blue samples in additive manner to produce any color that would give us our our color picture.",
                    "label": 1
                },
                {
                    "sent": "Video on the other hand.",
                    "label": 0
                },
                {
                    "sent": "That is captured at most multiple time instance.",
                    "label": 0
                },
                {
                    "sent": "So we now extend along the temporal dimension at each time instant, capturing tree arrays of samples now.",
                    "label": 0
                },
                {
                    "sent": "Video typically is captured in a different color space than still image, so red, green and blue as we will see in the next slide is only one example of a color space.",
                    "label": 0
                },
                {
                    "sent": "One way of representing digital color.",
                    "label": 0
                },
                {
                    "sent": "There are other many other ways.",
                    "label": 0
                },
                {
                    "sent": "A popular one in the context of video processing is something called yuv where we take the RGB single signal.",
                    "label": 0
                },
                {
                    "sent": "We have a linear transformation which transforms red, green and blue into tree.",
                    "label": 0
                },
                {
                    "sent": "New sets of coordinates called YU&V where.",
                    "label": 0
                },
                {
                    "sent": "The Y coordinate or the white channel corresponds to the luminance or intensity or in brightness at each spatial location and the U&V collectively represented color data.",
                    "label": 0
                },
                {
                    "sent": "OK, now this has.",
                    "label": 0
                },
                {
                    "sent": "Two very important an.",
                    "label": 0
                },
                {
                    "sent": "Benefits are motivations, so by transferring from red, green and blue to yuv, we've decoupled intensity or brightness from color OK and way back when that was very useful because it provided backwards compatibility with black and white systems, and many of you remember black and white systems.",
                    "label": 0
                },
                {
                    "sent": "Black and white TV at home.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm not the only old fogey here.",
                    "label": 0
                },
                {
                    "sent": "OK so this is very this is very nice because it meant that you control way to U&V and just process the Y, the grayscale image and you had black and white OK. Also, it turns out the human visual system is much less sensitive to color information.",
                    "label": 0
                },
                {
                    "sent": "That is to intensity or brightness information.",
                    "label": 0
                },
                {
                    "sent": "OK, so in terms of compressing the data and having a compact amount of data to store, the first thing we can do after this transformation is throw away 3/4 of the U&V data.",
                    "label": 0
                },
                {
                    "sent": "3/4 of the color data without the human visual system noticing a very perceptible loss that has significant implications in terms of storage.",
                    "label": 0
                },
                {
                    "sent": "OK, as we will see in a moment, I put some figures on why we need to do.",
                    "label": 0
                },
                {
                    "sent": "Image and video.",
                    "label": 0
                },
                {
                    "sent": "Fresh and so in yuv space, each pixel of each sample in the luminance component again is typically represented using 8 bit.",
                    "label": 0
                },
                {
                    "sent": "So we have a range of Gray level values that range from zero, which is black up to 255, which is white.",
                    "label": 0
                },
                {
                    "sent": "In terms of audio, we take an audio signal captured via microphone and then we sample the signal in this way to produce a digit.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Best audio signal.",
                    "label": 0
                },
                {
                    "sent": "So this is what the image data actually looks like.",
                    "label": 1
                },
                {
                    "sent": "This is an image which is 420 by 315 pixels with eight bits per pixels.",
                    "label": 0
                },
                {
                    "sent": "So to store this image, this raw image captured by the camera on disk requires 387 kilobytes of storage and this is what the individual pixel values look like if we zoom in onto the image.",
                    "label": 0
                },
                {
                    "sent": "So you can see each one of these samples corresponds to a red, green, blue triple value.",
                    "label": 0
                },
                {
                    "sent": "So this guy here, for example, the value is actually seventeen 00.",
                    "label": 0
                },
                {
                    "sent": "So it has a value of R for 17 and no contribution from green and blue, so it's almost black.",
                    "label": 0
                },
                {
                    "sent": "It would be completely black if it was 000.",
                    "label": 0
                },
                {
                    "sent": "This guy, on the other hand here is close to closer to white with high RGB values.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For video we transform, let's say to the yuv color space.",
                    "label": 0
                },
                {
                    "sent": "So we go from an image that looks like this for these three different color components.",
                    "label": 0
                },
                {
                    "sent": "Luminance component under 2 color difference components.",
                    "label": 0
                },
                {
                    "sent": "So just to reiterate or GBR.",
                    "label": 0
                },
                {
                    "sent": "Known are known as color spaces.",
                    "label": 1
                },
                {
                    "sent": "Many different color spaces exist, popular ones you'll see in the CBI.",
                    "label": 0
                },
                {
                    "sent": "Our literature are things like HSV, which is the Hue saturation value color space, which is a different way of representing the same information where he represents the color types, which is red, green, blue or yellow.",
                    "label": 0
                },
                {
                    "sent": "Saturation is the intensity of that color to paint, and the value is the actual brightness.",
                    "label": 0
                },
                {
                    "sent": "So akj Anil K. Jain's book is a very good place to start for the fundamentals of digital image processing.",
                    "label": 1
                },
                {
                    "sent": "So again, if we zoom up here, taking a corner of this guys hot, you'll see that this area here, the Y value is 230 very close to white, whereas here the Y value is 127, which is in the mid range corresponding to Gray.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "M audio data.",
                    "label": 0
                },
                {
                    "sent": "This is what the audio data typically looks like for three different sounds, so I'll actually play the sounds.",
                    "label": 1
                },
                {
                    "sent": "If I can.",
                    "label": 0
                },
                {
                    "sent": "That's an audio sample that we captured in some work we were doing on measuring traffic flow based on low-cost microphones.",
                    "label": 0
                },
                {
                    "sent": "That's the sound of a car moving past a microphone at the side of the road.",
                    "label": 0
                },
                {
                    "sent": "So it's a very different kind of audio sample.",
                    "label": 0
                },
                {
                    "sent": "OK, and we should be able to see by looking at the two waves on a significant difference between those.",
                    "label": 0
                },
                {
                    "sent": "OK, so the second sound is much more periodic and you can recognize the frequencies within that.",
                    "label": 0
                },
                {
                    "sent": "Within that sound we can take it to the next extreme.",
                    "label": 0
                },
                {
                    "sent": "We can show it for human speech.",
                    "label": 0
                },
                {
                    "sent": "Cat sat on the mat.",
                    "label": 0
                },
                {
                    "sent": "OK, in the DCU student, the back might recognize that is no Murphy.",
                    "label": 0
                },
                {
                    "sent": "Our current head of School of Engineering in DC you can.",
                    "label": 0
                },
                {
                    "sent": "So you can see tree very different kinds of sounds very very different kinds of waveforms and even by looking at those waveforms.",
                    "label": 0
                },
                {
                    "sent": "Hopefully it's intuitively clear to you that there are relatively simple straightforward measurements we could do on those waveforms to classify between those different kinds of sounds.",
                    "label": 0
                },
                {
                    "sent": "So we have, for example, in the human speech we have a much larger number of zero crossing, so the signal goes up and comes down much more often than it does for the case of the.",
                    "label": 0
                },
                {
                    "sent": "Of the car, or indeed for them.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sick.",
                    "label": 0
                },
                {
                    "sent": "Video data this is what our video data looks like, so this is one of those standard test sequences.",
                    "label": 0
                },
                {
                    "sent": "I was telling you about that I worked with and came to know and love or hate at times during my PhD and this is the kind of very boring data that we had to work with.",
                    "label": 0
                },
                {
                    "sent": "This is a 352 by 288 video sequence which is a resolution referred to Asif Common interchange format in Yuv.",
                    "label": 0
                },
                {
                    "sent": "It has 8 bits per pixel and this is 30 frames per second.",
                    "label": 0
                },
                {
                    "sent": "So to store that video sequence.",
                    "label": 0
                },
                {
                    "sent": "I'm on our 32nd clip of that video sequence requires 261 megabytes of storage on disk.",
                    "label": 0
                },
                {
                    "sent": "OK, if you want to transmit it, then it's 8.7 megabits per second to transmit, so suddenly we're getting into the realms of significant volumes of data OK. And this is an example of the type of video you might get to a mobile device.",
                    "label": 0
                },
                {
                    "sent": "This is the foreman sequence, again, another one of these standard test sequences that we use.",
                    "label": 0
                },
                {
                    "sent": "This was developed by MPEG to represent the application scenario of mobile video telephony.",
                    "label": 0
                },
                {
                    "sent": "So the scenario is that of someone talking into their mobile phone and the foreman is going his customer at the end of the video.",
                    "label": 0
                },
                {
                    "sent": "Here's the wall I'm building for you.",
                    "label": 0
                },
                {
                    "sent": "Everything an this particular video is cusip resolution, which is 1/4 of safe.",
                    "label": 0
                },
                {
                    "sent": "It's 176 by 1448 bits per pixel.",
                    "label": 0
                },
                {
                    "sent": "30 frames per second, so a 30 second clip store of this very low resolution image requires 65 megabits or megabytes of storage.",
                    "label": 0
                },
                {
                    "sent": "If we scale it up to high definition TV, OK Like you get in your widescreen plasma screen.",
                    "label": 0
                },
                {
                    "sent": "At home we typically have resolution of 128 by 720 pixels will typically have 24 bits per pixel.",
                    "label": 0
                },
                {
                    "sent": "Now supposed to 8 bits per pixel, and we typically have 50 frames per second as opposed to 30 frames per second.",
                    "label": 0
                },
                {
                    "sent": "So at 2 1/2 hour Hollywood movie in HDTV resolution will require 3.4 terabytes of storage.",
                    "label": 0
                },
                {
                    "sent": "If it's taken directly from the camera, digitized and stored.",
                    "label": 0
                },
                {
                    "sent": "On a hard drive.",
                    "label": 0
                },
                {
                    "sent": "So typically, in the case of video, we don't actually process and store the raw data.",
                    "label": 0
                },
                {
                    "sent": "Typically before we ever get to the video.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, it's undergone a compression process which takes the raw data, which is what it's referred to when it comes directly from the camera.",
                    "label": 0
                },
                {
                    "sent": "They still camera video camera or microphone and it's compressed.",
                    "label": 0
                },
                {
                    "sent": "It undergoes some software or hardware process to compact the data to make it as efficiently as representative make.",
                    "label": 0
                },
                {
                    "sent": "It is easy to represent as possible in terms of efficiency.",
                    "label": 0
                },
                {
                    "sent": "So this process is known as compression or encoding, and it typically results in the bitstream that can be stored or transmitted.",
                    "label": 1
                },
                {
                    "sent": "For a variety of applications.",
                    "label": 0
                },
                {
                    "sent": "The hard the encoding process is typically quite a computationally heavy process.",
                    "label": 1
                },
                {
                    "sent": "There's quite some complex maths going on there and signal processing going on there, and typically we require a less complex process to uncompress or decode the content before it can be displayed or before it can be heard.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the point of all of that discussion about how we capture video?",
                    "label": 0
                },
                {
                    "sent": "Well, the point from a content based information retrieval perspective is that when we are trying to extract these features upon which we're going to base some inferencing post process, we have two options.",
                    "label": 0
                },
                {
                    "sent": "Or we have two possibilities.",
                    "label": 0
                },
                {
                    "sent": "In the first instance we can take the measurements from the raw data.",
                    "label": 0
                },
                {
                    "sent": "If we have the raw data.",
                    "label": 0
                },
                {
                    "sent": "Little, we can post the data in what's known as the uncompressed domain.",
                    "label": 0
                },
                {
                    "sent": "This is where we end up processing raw audio samples or raw pixel values, or raw frames of video.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, we can take the compressed formats, which I'll mention in a moment, and we can work directly with those so we can process the actual structure stored in an MPEG one video bitstream.",
                    "label": 0
                },
                {
                    "sent": "For example, an use that.",
                    "label": 0
                },
                {
                    "sent": "As opposed to doing a complete full decode and then working with the pixel values so the benefits in the first instance is that it can be fast.",
                    "label": 0
                },
                {
                    "sent": "It can be real time because we're not, we don't have to actually decode the data and store it at this, consequently process it.",
                    "label": 1
                },
                {
                    "sent": "We can extract data directly from the bitstream.",
                    "label": 0
                },
                {
                    "sent": "On the second hand, if we're processing the raw data, we end up with the possibility of extracting a greater range of more expressive features.",
                    "label": 0
                },
                {
                    "sent": "OK, because we have the raw pixel values for example, as opposed to frequency domain coefficients, which is how we represent pixel values in a compressed bitstream.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In compression there are two kinds of compression we should be aware of their verses called lossless compression, and this is where we don't change the data, but we simply reorganize the data in clever ways with A view to compacting it and inefficient bitstream format.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of compression that's used in medical applications such as digitising X Rays for example and document scanning.",
                    "label": 1
                },
                {
                    "sent": "So your facts in your in your office uses a form of lossless compression.",
                    "label": 0
                },
                {
                    "sent": "It scans the image, it gets a raw set of RGB values in a, perform some processing on that.",
                    "label": 0
                },
                {
                    "sent": "To represent them very efficiently so they can be squirted down at a telephone line.",
                    "label": 0
                },
                {
                    "sent": "Lossy compression is the one that were perhaps more familiar with and is more ubiquitous and more widespread.",
                    "label": 0
                },
                {
                    "sent": "And this is where by we make some compromises in order to reach the very strict requirements of our networks.",
                    "label": 0
                },
                {
                    "sent": "Our applications in terms of bandwidth.",
                    "label": 0
                },
                {
                    "sent": "In other words, we can't assume that we can keep all the image and video data or audio data we throw some of it away.",
                    "label": 0
                },
                {
                    "sent": "And the trick isn't being clever about what we throw away.",
                    "label": 0
                },
                {
                    "sent": "So we only throw away data that's of less important to the human visual system, or less important, to the human ear.",
                    "label": 0
                },
                {
                    "sent": "So for example, in image is very often fine detail.",
                    "label": 0
                },
                {
                    "sent": "Is less important depending on the image of course, but the point is the human visual system is less sensitive to fine detail.",
                    "label": 0
                },
                {
                    "sent": "So if we can isolate fine detail in the image, we can remove it.",
                    "label": 0
                },
                {
                    "sent": "Thereby having less information to represent and store achieving compression without the human eye noticing any difference.",
                    "label": 0
                },
                {
                    "sent": "So popular image and video compression standards you're probably familiar with these.",
                    "label": 0
                },
                {
                    "sent": "We have JPEG, which is ubiquitous under web for compressing still images.",
                    "label": 0
                },
                {
                    "sent": "Most recently, we've an enhancement of JPEG that uses a different kind of compression technology and support enhance functionality called JPEG 2000 MPEG One.",
                    "label": 0
                },
                {
                    "sent": "As an old standard at this stage and provides sub VHS video quality.",
                    "label": 1
                },
                {
                    "sent": "The original target application was video from a CD ROM MPEG 2, which is a standard upon which digital TV is based, and.",
                    "label": 0
                },
                {
                    "sent": "DVD technology is based most recently got MPEG 4, which is a very efficient.",
                    "label": 0
                },
                {
                    "sent": "I'll be it quite computationally intense video compression standard targeting.",
                    "label": 0
                },
                {
                    "sent": "Very low but very very low bitrate but very high quality video for applications and in the mobile domain and the most recent standards.",
                    "label": 0
                },
                {
                    "sent": "H .264 which is advanced video coding above and beyond even MPEG 4 just a piece of trivia in case you're wondering, an people sometimes ask me MP tree or refer to MP3 as MPEG.",
                    "label": 0
                },
                {
                    "sent": "Trey there is no mpeg tree.",
                    "label": 0
                },
                {
                    "sent": "In actual fact, MP Tree actually means MPEG one layer tree audio encoding, so the MPEG series of standards goes 124 OK, which is a nice binary progression that maxel engineers computer scientists very excited and but there was no MPEG tree.",
                    "label": 0
                },
                {
                    "sent": "There actually was an MPEG tree.",
                    "label": 0
                },
                {
                    "sent": "It would start to be developed, but it turned out that the activity in MPEG Two overtook MPEG Tree, excuse me and so they stopped the work item in MPEG Tree and move directly to MPEG 4.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit of trivia.",
                    "label": 0
                },
                {
                    "sent": "You can amaze your friends with after your next dinner party.",
                    "label": 0
                },
                {
                    "sent": "OK, if you don't like them, 'cause I probably won't come back.",
                    "label": 0
                },
                {
                    "sent": "OK, so image compression.",
                    "label": 0
                },
                {
                    "sent": "How does it work?",
                    "label": 0
                },
                {
                    "sent": "And again the very high level overview just to try to explain to you what are the kind of features that we can extract.",
                    "label": 0
                },
                {
                    "sent": "This is my one slide overview of how JPEG works.",
                    "label": 0
                },
                {
                    "sent": "OK, so the bottom line is that we use frequency domain analysis.",
                    "label": 1
                },
                {
                    "sent": "We take our spatial data, samples are responding to an array of pixels and we transform to a new representation in the frequency domain and that allows us to select the most important information.",
                    "label": 0
                },
                {
                    "sent": "So if we take an image like this, which is a close up of a pizza, OK, it's in the spatial domain.",
                    "label": 0
                },
                {
                    "sent": "It's highly textured and it has fine detail.",
                    "label": 0
                },
                {
                    "sent": "If we perform a 2D discrete cosine transform, we can transform this spatial data into the frequency domain and this is what we end up here with here.",
                    "label": 0
                },
                {
                    "sent": "So this is an array of frequency domain coefficients.",
                    "label": 0
                },
                {
                    "sent": "Your upper left hand corner is what's known as a DC coefficient, which is the average color.",
                    "label": 0
                },
                {
                    "sent": "If you like over the entire image, and then we have values here, frequency coefficients that represent.",
                    "label": 0
                },
                {
                    "sent": "Increasing horizontal and vertical spatial frequency.",
                    "label": 0
                },
                {
                    "sent": "So in other words, these correspond to the low frequency content of the image.",
                    "label": 0
                },
                {
                    "sent": "And the upper right hand corner of this block corresponds to the high frequency content.",
                    "label": 0
                },
                {
                    "sent": "Now high frequency content and image corresponds to fine detail.",
                    "label": 0
                },
                {
                    "sent": "The point is that we can take this representation of frequency coefficients and we can filter it so we can remove in this case, for example, all the high frequency coefficients simply by zeroing them.",
                    "label": 0
                },
                {
                    "sent": "So in this case we've gotten rid of 3/4 of the data that's used to represent the image.",
                    "label": 0
                },
                {
                    "sent": "And then we can take this truncated data.",
                    "label": 0
                },
                {
                    "sent": "We can perform the inverse transform back to the spatial domain and we end up with this kind of image here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, here is that there's a low pass filtering effect on the under the beamer, but in actual fact these two images are very very similar, as you'll see in the slides when you finally get the PDF version.",
                    "label": 0
                },
                {
                    "sent": "So we removed a significant chunk of the day to 75% of the data OK with no perceptible difference between the input and the output image.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Of course, we're throwing data away in this process, so we're taking some data were saying you're not important and we're throwing it away.",
                    "label": 0
                },
                {
                    "sent": "Typically we don't do this across the entire image for reasons of computational complexity and efficient implementation.",
                    "label": 0
                },
                {
                    "sent": "Rather, we typically take blocks of image data, so we take 8 by 8 blocks, or 16 by 16 blocks of image data and apply this frequency domain processing to each individual block that has benefits in terms of implementation architectures.",
                    "label": 0
                },
                {
                    "sent": "It is benefits in terms of error limitation, error propagation, and so on.",
                    "label": 0
                },
                {
                    "sent": "Of course, given that we're throwing away some data we don't, there's no such thing as a free lunch, so we are prone to throw away too much data.",
                    "label": 0
                },
                {
                    "sent": "Sometimes, depending on our application requirements and end up with reduced quality in our in our images.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of successively throwing away data from a still image, so.",
                    "label": 0
                },
                {
                    "sent": "See, this is the original image.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the output image after performing some of this, throwing away some of this high frequency data and this is the difference.",
                    "label": 0
                },
                {
                    "sent": "Images is the difference between this image and this image.",
                    "label": 0
                },
                {
                    "sent": "OK, and as I throw away successively more data.",
                    "label": 0
                },
                {
                    "sent": "You'll see that the image becomes more and more pixelated, and what we're actually seeing here this kind of blocking effect corresponds to the edges of the individual blocks that were processing OK, which introduces a very perceptually noisy, very perceptually disturbing effect into the image.",
                    "label": 0
                },
                {
                    "sent": "Representation, but perhaps for your application if it's a low bandwidth mobile application on a mobile phone, perhaps this quality is sufficient.",
                    "label": 0
                },
                {
                    "sent": "OK for the image you want to convey.",
                    "label": 0
                },
                {
                    "sent": "The bottom row is another example of the exact same thing.",
                    "label": 0
                },
                {
                    "sent": "We applied to a very different kind of image data, so in this case we have a scanned piece of text or responding to a input on a next term and you'll see that as I successively throw away high frequency information.",
                    "label": 0
                },
                {
                    "sent": "Our image very quickly gets very very noisy.",
                    "label": 0
                },
                {
                    "sent": "So certainly we've gone from this image here, which is desirable or application something like this, which is potentially useless 'cause you can no longer see the text.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the bottom line, I suppose, is that different kinds of data required different kinds of compression, so you wouldn't you wouldn't, or you shouldn't apply the same compression technique to this image as you would for this image, because in this image you can get away with throwing away to find detail the high frequency content, whereas in this image the high frequency content defined detailed corresponds to the actual text that's being represented.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is my one slide overview of how video compression works, so video compression.",
                    "label": 0
                },
                {
                    "sent": "Is similar in a way to taking an individual image at 25 frames per second, 25 times a second and applying still image compression to every single frame.",
                    "label": 0
                },
                {
                    "sent": "OK, so taking the individual image transform into the frequency domain, throwing away to high frequency content.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that even doing that, given the sheer volume of video data that we're dealing with that still not sufficient in order to reach the very strict bandwidth requirements of some applications.",
                    "label": 0
                },
                {
                    "sent": "So we need to do some more processing to the video.",
                    "label": 0
                },
                {
                    "sent": "In order to get down those kinds of band bandwidths.",
                    "label": 0
                },
                {
                    "sent": "Am so in addition to taking the video frame and performing discrete cosine transform and throwing away to high frequency data and producing a bitstream, we also have a process known as motion compensation and motion estimation, and this process takes successive images from one frame to the next and estimates the difference from one frame to the next.",
                    "label": 0
                },
                {
                    "sent": "Point being that if you have video captured at 25 frames per second, well, even if it's a high activity, high octane car chase in a Hollywood blockbuster movie.",
                    "label": 0
                },
                {
                    "sent": "Whilst there might be significant change in the video content from.",
                    "label": 0
                },
                {
                    "sent": "0 seconds to 25 seconds from zero to 125th of a second.",
                    "label": 0
                },
                {
                    "sent": "There's very little change between individual images.",
                    "label": 0
                },
                {
                    "sent": "And we can potentially measure this change and quantify this change and only encode and transmit the change and therefore get very efficient encoding of video data.",
                    "label": 0
                },
                {
                    "sent": "So we use the exact same processing as we do for still images, but we have this extra process called motion estimation.",
                    "label": 0
                },
                {
                    "sent": "Whereby we take the previous frame in the video sequence.",
                    "label": 0
                },
                {
                    "sent": "If this is the current block that we're trying to encode, so again for video for reasons of computational complexity and implementation, we don't process entire frames of video, but rather we break on more manageable chunks corresponding to 16 by 16 blocks.",
                    "label": 0
                },
                {
                    "sent": "So we take the 16 by 16 block in the current frame and we try and find a good match for where that block came from in the previous frame.",
                    "label": 0
                },
                {
                    "sent": "So where is a good predictor for that block?",
                    "label": 0
                },
                {
                    "sent": "In the previous video frame?",
                    "label": 0
                },
                {
                    "sent": "And then then we can get away with only sending the motion vectors.",
                    "label": 0
                },
                {
                    "sent": "It's called the displacement or the offset from that frame to the previous frame.",
                    "label": 0
                },
                {
                    "sent": "So what actually gets sent in a video bitstream is not pixel datas.",
                    "label": 0
                },
                {
                    "sent": "Pixel data, but rather it's a combination of these frequency domain.",
                    "label": 0
                },
                {
                    "sent": "Coefficients which represent the frequency content of each image on a series of motion vectors.",
                    "label": 0
                },
                {
                    "sent": "2 numbers X&Y which give you displacement already offset for the best predicting block from the previous frame.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a visualization of actually what actually gets sent.",
                    "label": 0
                },
                {
                    "sent": "This is another one of those very, very boring test sequences we used to work with.",
                    "label": 0
                },
                {
                    "sent": "So what you're seeing here?",
                    "label": 0
                },
                {
                    "sent": "This is the current frame to be encoded.",
                    "label": 0
                },
                {
                    "sent": "This is the previous frame in the video sequence and this is the motion that we're measuring from one frame to the next.",
                    "label": 0
                },
                {
                    "sent": "So in this sequence the guy is bouncing a table tennis ball OK?",
                    "label": 0
                },
                {
                    "sent": "I played it again and then the camera zooms out.",
                    "label": 0
                },
                {
                    "sent": "So the green arrows here represent these motion vectors.",
                    "label": 0
                },
                {
                    "sent": "These predictions that we've measured.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing to note about this is that the motion vectors actually correspond to what's happening in the scene.",
                    "label": 0
                },
                {
                    "sent": "OK, so the green arrows when the guy is bouncing to table tennis but are located around his arm.",
                    "label": 0
                },
                {
                    "sent": "That hopefully implies to us that if we want to perform some processing on that video sequence to extract the guys arm from the background, we could for example use the motion vectors.",
                    "label": 0
                },
                {
                    "sent": "'cause that they indicate parts of the image that are moving.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example for a very different kind of scene, but again, one of these boring standard test sequences where we have a camera that spanning across the scene, tracking an object as it's moving, and you'll see that all of the motion vectors in the first part of the sequence point to the left, which indicates that the camera is moving to the left, and then they all move upwards when the camera tilts upwards.",
                    "label": 0
                },
                {
                    "sent": "So of course these motion vectors were derived and were produced for the purposes of compression efficiency.",
                    "label": 0
                },
                {
                    "sent": "In order to have a very compact representation of the bitstream.",
                    "label": 0
                },
                {
                    "sent": "But the point is that every MPEG bitstream that's out there on the net or on your hard drive contains these motion vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so will contain some potentially useful information in terms of extracting features.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to infer what the camera motion is, for example, or what the object motion is.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's do a brief at whirlwind tour of audio compression then, so audio compression.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the human ear is not is not linearly sensitive across all frequency ranges but is non linear across the audio.",
                    "label": 0
                },
                {
                    "sent": "The audible range towards humans which ranges from about 200 Hertz at 20 kilohertz and this audible range is broken into regions where humans cannot perceive a difference was like a quantization of the audio spectrum.",
                    "label": 0
                },
                {
                    "sent": "If you like, which are called critical bands, so audio compression starts by taking the audio signal, the raw audio signal and performing a multi resolution frequency decompoze.",
                    "label": 0
                },
                {
                    "sent": "Addition of that data and then that multiresolution frequency representation, which corresponds to 32 subbands of the data.",
                    "label": 0
                },
                {
                    "sent": "Is processed by a psycho acoustic model with models what the human ear hears and determines which parts of which subbands are most important for that particular sound that's being compressed and allocates more bits to the more important sounds.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is what an MPEG audio encoder looks like.",
                    "label": 0
                },
                {
                    "sent": "We have the digital audio here.",
                    "label": 0
                },
                {
                    "sent": "It undergoes the filtering process tinted.",
                    "label": 0
                },
                {
                    "sent": "The construction into a variety of subbands cycle acoustic models calculates an adjust, noticeable noise level in each of these bands.",
                    "label": 0
                },
                {
                    "sent": "In other words, what humans can just perceive on the basis of that allocates bits to the most important parts of the audio signal.",
                    "label": 0
                },
                {
                    "sent": "One allocates more bits to the most important parts.",
                    "label": 0
                },
                {
                    "sent": "All of that gets formatted and sent to an MPEG.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stream so.",
                    "label": 0
                },
                {
                    "sent": "This is what's happening.",
                    "label": 0
                },
                {
                    "sent": "The audio signal comes in, we take.",
                    "label": 0
                },
                {
                    "sent": "We have this set of subband filters that typically 32.",
                    "label": 0
                },
                {
                    "sent": "We group 12 samples from each son Bob.",
                    "label": 1
                },
                {
                    "sent": "So Bannon code them in what are known as frames.",
                    "label": 0
                },
                {
                    "sent": "So frame corresponds to 384 audio samples and each group is encoded with zero to 15 bits per sample typically.",
                    "label": 0
                },
                {
                    "sent": "Now Interestingly, each group is also associated as associated with a 6 pack 6 bit scale factor and a scale factor.",
                    "label": 1
                },
                {
                    "sent": "Audio compression terminology corresponds to the Max value of samples.",
                    "label": 0
                },
                {
                    "sent": "Within a group of 12 samples.",
                    "label": 0
                },
                {
                    "sent": "So it says, here's a group of 10 sample data points on.",
                    "label": 0
                },
                {
                    "sent": "Here's the maximum value within that 12 samples.",
                    "label": 0
                },
                {
                    "sent": "That's useful in terms of ensuring that the full dynamic range of the quantizer is actually used.",
                    "label": 0
                },
                {
                    "sent": "But as we will see, it's also very interesting information that allows us to do some very interesting things with audio for content.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based information retrieval.",
                    "label": 0
                },
                {
                    "sent": "So this is what the audio bitstream.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually looks like for each frame where a frame is 1152 samples we have 32 subbands.",
                    "label": 1
                },
                {
                    "sent": "Each subband has 12 samples, and there are three groups.",
                    "label": 0
                },
                {
                    "sent": "So the frame is divided into this kind of data where we have some header data which allows re synchronization purposes.",
                    "label": 0
                },
                {
                    "sent": "We have some error checking.",
                    "label": 1
                },
                {
                    "sent": "We have some bit allocation which tells the decoder.",
                    "label": 0
                },
                {
                    "sent": "Here is the number of bits allocated to this part of the bitstream and then we have this scale factor information which says the highest value in the following 12 samples.",
                    "label": 1
                },
                {
                    "sent": "Is this value and then we have the actual encoded samples and I'll show you an example a bit later in the lecture where by simply parsing this single value of scale factor.",
                    "label": 1
                },
                {
                    "sent": "Open Audio Bitstream allows us characterize or classify between the music sample you heard earlier and the speech sample you heard earlier.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can take an audio bitstream.",
                    "label": 0
                },
                {
                    "sent": "We can perform faster than real time processing of that bitstream to extract a single number at each time interval and recognize the difference between speech and music.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so any questions on any of that.",
                    "label": 0
                },
                {
                    "sent": "That's probably familiar to most of you, so I'll start moving a bit more quickly now.",
                    "label": 0
                },
                {
                    "sent": "At this stage I guess.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's move on to audio, image and video features and what we can or what we should.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Measure from the from the data.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we should ask ourselves is what's important in the data.",
                    "label": 1
                },
                {
                    "sent": "So let's start with still images.",
                    "label": 0
                },
                {
                    "sent": "OK, so in still images.",
                    "label": 1
                },
                {
                    "sent": "Presumably it should be clear to us that color is important in a still image.",
                    "label": 0
                },
                {
                    "sent": "So for example in these nice images of sunsets, an color is quite important in terms of conveying the mood, the atmosphere and the ambience and so on.",
                    "label": 0
                },
                {
                    "sent": "That picture is trying to impart.",
                    "label": 0
                },
                {
                    "sent": "There's also something called texture which we might not be as familiar with.",
                    "label": 0
                },
                {
                    "sent": "That's very important.",
                    "label": 0
                },
                {
                    "sent": "I believe Alan Smeaton talk some something about texture in his presentation, so texture is kind of a hard thing to describe in the context of images.",
                    "label": 0
                },
                {
                    "sent": "What corresponds to a texture and image is the best definition I can come up with.",
                    "label": 0
                },
                {
                    "sent": "Is that it?",
                    "label": 0
                },
                {
                    "sent": "Corresponds to the field.",
                    "label": 0
                },
                {
                    "sent": "The appearance of the consistency of a surface.",
                    "label": 1
                },
                {
                    "sent": "That's the textbook definition of what texture is an.",
                    "label": 0
                },
                {
                    "sent": "So if we try and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Represent that in the context of images at.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of a an image I'll be at and only four by four pixel image an, which is all of the one color OK and on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "But I've shown here is I've plotted this image as a surface where the act says here are the X&Y dimensions of the image and the height zed component corresponds to the color value.",
                    "label": 0
                },
                {
                    "sent": "OK, so a flat image corresponds to entry dimensional space, A flat surface, so a single color image corresponds to a flat surface, or in other words, the absence of texture.",
                    "label": 0
                },
                {
                    "sent": "So there's zero texture in that image.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of image, on the other hand, where you have this combination of colors, and here I'm assuming that blue is plotted lower on the Z axis followed by red followed by yellow, followed by green followed by blue gives rise to this kind of stepped surface.",
                    "label": 0
                },
                {
                    "sent": "So suddenly the surface is no longer flat.",
                    "label": 0
                },
                {
                    "sent": "This is more than one color in the image.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you have something like this in the image, you end up with this very complex 3D structure.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the surface of this treaty structure where these are points above the ground plane, we end up with this almost wireframe mesh like 3D surface correspond to a very textured surface.",
                    "label": 0
                },
                {
                    "sent": "So that's how I visualized.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Texture an in an image.",
                    "label": 0
                },
                {
                    "sent": "So in the very simplest case, this is an image with no texture, which is a single color.",
                    "label": 0
                },
                {
                    "sent": "This is a checkerboard pattern are responding to an image that's highly textured.",
                    "label": 1
                },
                {
                    "sent": "Do you think we should consider is whether or not color and texture as important things to measure whether they're important over the image as a whole, or whether it is only particular parts of the image which are more or less important.",
                    "label": 0
                },
                {
                    "sent": "So in the image of the Flowers, for example, are we concerned that the image is green and red or pink or we can send that it's really an image of a pink flower?",
                    "label": 1
                },
                {
                    "sent": "So sometimes we want to concentrate our measurements on particular parts of the image.",
                    "label": 0
                },
                {
                    "sent": "And if we want to do that well, then sometimes we can just overlay a grid on the image, and that's useful and convenient because our image compression functionality over laser grid on the image and processes block by block.",
                    "label": 0
                },
                {
                    "sent": "So again we could take out the average color for each block by processing and parsing.",
                    "label": 0
                },
                {
                    "sent": "The DCT coefficients are very efficient process.",
                    "label": 0
                },
                {
                    "sent": "But sometimes a grid is not sufficient.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we're interested very in very accurate information about a specific object present in the image.",
                    "label": 0
                },
                {
                    "sent": "So if we're interested in horses, for example, well, the side profile of a horse is a very definite shape, and if we want to find more side profiles of horses, we need to represent the color of the horse, but also the shape of the horse balance meat and show you some examples on Monday of object based based retrieval.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it's still images, color, texture and the shape of potential objects present in the scene are important.",
                    "label": 0
                },
                {
                    "sent": "Video if we consider what's important in video, well, it extends images along the temporal direction.",
                    "label": 0
                },
                {
                    "sent": "So now we can measure the color and texture of each individual frame in the video sequence, so we can have 25 measurements per second for every block in the image, or every object in the image.",
                    "label": 0
                },
                {
                    "sent": "But we can also now measure motion.",
                    "label": 0
                },
                {
                    "sent": "So you saw how we compress MPEG BITSTREAM'S using these motion vectors, which is some measure of the motion that's present either to camera motion in the video sequence or the motion of objects such as the guys and playing ping pong.",
                    "label": 0
                },
                {
                    "sent": "So we can define motion is where a pixel moves from one frame to the next or what's changed from one frame to the next.",
                    "label": 1
                },
                {
                    "sent": "And of course, that information is already available to us from our compression process, so again, we can take the MPEG bitstream.",
                    "label": 0
                },
                {
                    "sent": "We can simply parse the bitstream and we can extract the motion vectors and use those motion vectors to do interesting things.",
                    "label": 1
                },
                {
                    "sent": "There are two kinds of motion we might want to measure.",
                    "label": 0
                },
                {
                    "sent": "The first is what's known as global motion, and this is the kind of motion that we shot saw when the camera panned.",
                    "label": 0
                },
                {
                    "sent": "In the example, with the speedboat, this is where the entire frame is moving because the camera is moving from one frame to the next and that corresponds to the motion of the camera.",
                    "label": 0
                },
                {
                    "sent": "Man is carrying out and there are different kinds of camera motions such as pan, tilt and zoom, which is very specific.",
                    "label": 1
                },
                {
                    "sent": "Definitions in the field of filmmaking.",
                    "label": 0
                },
                {
                    "sent": "Order something called local motion.",
                    "label": 0
                },
                {
                    "sent": "This is where the emotion is localized to specific parts of the image corresponding to, for example, in the previous example you guys hand OK where that's moving with a very coherent kind of motion relative to the rest of the scene.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how do we represent color?",
                    "label": 0
                },
                {
                    "sent": "Assuming that it's important color is very visually important to humans to humans and turns out that color features are very easy to compute and similarity metrics between color features are very easy to compute.",
                    "label": 1
                },
                {
                    "sent": "So we use our well known well of global image histogram.",
                    "label": 0
                },
                {
                    "sent": "OK, the benefits.",
                    "label": 0
                },
                {
                    "sent": "So we can take an image like this and we can represent the color distribution in a vector histogram format like so.",
                    "label": 0
                },
                {
                    "sent": "And then we can.",
                    "label": 0
                },
                {
                    "sent": "We can compare histograms using a variety of histogram intersection methods.",
                    "label": 0
                },
                {
                    "sent": "We can use that.",
                    "label": 0
                },
                {
                    "sent": "Cosine measure we can use the Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "We can use so on so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can transform an entire image to a histogram and compare images very efficiently, and we'll get images which are similar in color.",
                    "label": 0
                },
                {
                    "sent": "In other words, they have red and green and pink images of Flowers.",
                    "label": 0
                },
                {
                    "sent": "An I'm not attractive because these histograms are invariant to translation and rotation, and they can be made invariant to the size of the image to a normalization process.",
                    "label": 1
                },
                {
                    "sent": "An MPEG SEVEN has standardized a variety of histogram type color.",
                    "label": 0
                },
                {
                    "sent": "Our features, so the scalable color descriptor is a histogram and HSV space, so it encodes the higher transform coefficients of the color distribution.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Order MPEG 7 color descriptors an.",
                    "label": 0
                },
                {
                    "sent": "Dominant color, for example, is target similarity, retrieval and image databases.",
                    "label": 1
                },
                {
                    "sent": "So the algorithm involves a sequence of clustering steps where we try and group similar colors colored pixels in the image and then represent them via colored vector that includes the percentage pixel area covered by that color, the variance, the spatial coherency of the dominant colors and and there are other color descriptors that are specified by MPEG 7.",
                    "label": 1
                },
                {
                    "sent": "Anne, I know Steven had a very interesting discussion about MPEG 7 in his lecture or after his lecture yesterday, and I think the comment that he made was that MPEG Seven was state of the art for its time.",
                    "label": 0
                },
                {
                    "sent": "OK, but there's been significant work on feature description since down.",
                    "label": 0
                },
                {
                    "sent": "That's very, very true.",
                    "label": 0
                },
                {
                    "sent": "So people have been working on different ways of representing color, since MPEG Seven will standardize.",
                    "label": 0
                },
                {
                    "sent": "However, if you're a new beginning PhD student working in this field and you're wondering, how do I represent color in my retrieval algorithm or in my?",
                    "label": 0
                },
                {
                    "sent": "Relevance feedback algorithm.",
                    "label": 0
                },
                {
                    "sent": "For example, the MPEG 7 features form a kind of lowest common denominator set that you can use.",
                    "label": 0
                },
                {
                    "sent": "So there are good place to start effectively and that's why I over the course of this lecture repeatedly referenced the MPEG 7 features.",
                    "label": 0
                },
                {
                    "sent": "They're well documented as loads of papers about them.",
                    "label": 0
                },
                {
                    "sent": "Just source code out there for extracting them, so there are good good place to.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In terms of representing textured or variety of texture features available, it's been something that's been worked on.",
                    "label": 0
                },
                {
                    "sent": "The image processing community for 20 odd years or so, if not longer.",
                    "label": 0
                },
                {
                    "sent": "So in 1991, for example, there was suggested that very straightforward things like Co, occurrence matrices and autocorrelation function and edge frequency and primitive length could be used to represent.",
                    "label": 1
                },
                {
                    "sent": "Texture features there are more sophisticated approaches which characterize the textures in the frequency domain.",
                    "label": 0
                },
                {
                    "sent": "So in the wavelet domain and the higher domain or indicative or domain, for example, all of these are valid ways of representing texture.",
                    "label": 0
                },
                {
                    "sent": "Anne and then there are other, perhaps more left of center ways of representing texture, so the mathematical morphological community spent a long time understanding how you could use the how you could use morphological primitives represent texture.",
                    "label": 0
                },
                {
                    "sent": "An fractal coding was involved at one point as well as a very efficient way of using the fractal concept.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then texture here is one of the simplest ways of representing texture that's standardized in MPEG 7, so again, it's not particularly complex, and it's shown here as as a good grounding or a good starting point, so it's called the edge histogram.",
                    "label": 0
                },
                {
                    "sent": "It represents the global texture distribution and image, and there's been some work in the literature which has used that represent local texture distribution as well.",
                    "label": 1
                },
                {
                    "sent": "So the first thing is we do we take an image like our good old test image Lana here, and we perform edge based filtering on that.",
                    "label": 0
                },
                {
                    "sent": "So we transform this image.",
                    "label": 0
                },
                {
                    "sent": "Into an image that where the edges are highlighted and there are a variety of ways of doing that.",
                    "label": 0
                },
                {
                    "sent": "So there are many edge detection algorithms.",
                    "label": 0
                },
                {
                    "sent": "There are Robert, Sobel, canny and so on, so forth.",
                    "label": 0
                },
                {
                    "sent": "And then once we have this image, we quantify the different kinds of images, different kinds of edges in the image.",
                    "label": 0
                },
                {
                    "sent": "So we say that there are only five kinds of edges possible.",
                    "label": 0
                },
                {
                    "sent": "We can have a vertical edge, we can have a horizontal edge.",
                    "label": 0
                },
                {
                    "sent": "We can have a 45 degree edge, or we can have 100.",
                    "label": 0
                },
                {
                    "sent": "35 degree edge and then we have a kind of a catchall of any edge which doesn't fit into these four categories.",
                    "label": 0
                },
                {
                    "sent": "So we have 5 bin histogram which counts the occurrence of this.",
                    "label": 0
                },
                {
                    "sent": "These kinds of edges in an image in the same way that we count the distribution of different kinds of colors in an image.",
                    "label": 1
                },
                {
                    "sent": "And we can compare edge histograms in exactly the same way as we compare color histograms and we can find images that have similar edge distribution.",
                    "label": 0
                },
                {
                    "sent": "So a majority of vertical lines for example, and that might be interesting if you're trying to find more images of buildings which tend to be characterized by vertical and horizontal lines.",
                    "label": 0
                },
                {
                    "sent": "As opposed to images of natural scenes where you'll have non directional edges.",
                    "label": 0
                },
                {
                    "sent": "More sophisticated edge.",
                    "label": 0
                },
                {
                    "sent": "Or text.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Classification can be done at a transform domain, so here's an example of taking it as an image from Irish news program.",
                    "label": 0
                },
                {
                    "sent": "We're showing only the luminance component here, and we can decompose this image into a variety of frequency bands using this thing called a higher transform.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of the image, decomposed into a variety of different frequency frequency bands.",
                    "label": 1
                },
                {
                    "sent": "To give you a concrete example, this frequency band here, H2 can be seen as a low pass horizontal filtering, which is followed by high pass vertical filtering, thereby enter emphasizing.",
                    "label": 1
                },
                {
                    "sent": "Vertical frequencies and we have the converse in this image here, so we can transform this image into two frequency domain representations where we emphasize the horizontal or vertical edges and then we can extract some kind of measure such as a histogram or similar, and even the direct coefficients from the frequency domain from the filtering process from here to represent the edge distribution in this image.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if we think back to our slide as to what's important in images, we said that color is important.",
                    "label": 0
                },
                {
                    "sent": "We said that texture is important and I've shown you some slides which hopefully explains to you or convince you that it's actually quite straightforward to represent color and texture.",
                    "label": 1
                },
                {
                    "sent": "OK, I'll be at a reasonably coarse granularity, but it'll work for image similarity.",
                    "label": 0
                },
                {
                    "sent": "The third thing that we said was important was that sometimes it's not the entire image that's important, but specific parts of the image.",
                    "label": 0
                },
                {
                    "sent": "So sometimes you want to break the image into smaller, more meaningful segments, where ideally those segments represent real world objects, such as the horse against the green background, for example.",
                    "label": 0
                },
                {
                    "sent": "And this is an area of research that's known as image segmentation.",
                    "label": 1
                },
                {
                    "sent": "Now it turns out that this is where the wheels come off the bus.",
                    "label": 0
                },
                {
                    "sent": "OK, in terms of feature extraction because image segmentation is a very difficult thing to do accurately and robustly across many different kinds of images.",
                    "label": 0
                },
                {
                    "sent": "So a histogram is a histogram and you can play histogram to any color image and you'll get a good representation of the color distribution of that image, irrespective of what that image represents.",
                    "label": 1
                },
                {
                    "sent": "Segmentation, on the other hand, trying to detect the real world boundaries of objects and scenes in the image is a much more complex.",
                    "label": 0
                },
                {
                    "sent": "Process and it's complex because it's difficult for us as humans to do that.",
                    "label": 0
                },
                {
                    "sent": "So here are more of these optical illusions that park back to the semantic gap.",
                    "label": 0
                },
                {
                    "sent": "So, for example, this image here is this face.",
                    "label": 0
                },
                {
                    "sent": "Of a lady?",
                    "label": 0
                },
                {
                    "sent": "Or is this a musician playing the saxophone?",
                    "label": 0
                },
                {
                    "sent": "Similarly, is this a Native American, or is it an Eskimo but his back to us?",
                    "label": 0
                },
                {
                    "sent": "Is this two faces?",
                    "label": 0
                },
                {
                    "sent": "Facing each other?",
                    "label": 0
                },
                {
                    "sent": "Or is it a non invasive table or something?",
                    "label": 0
                },
                {
                    "sent": "OK. And depending on your interpretation, as humans, those images will mean different things to us.",
                    "label": 0
                },
                {
                    "sent": "To try to develop an algorithm that extracts those the meaning in terms of the real world boundaries is very, very challenging, even when we have relatively simple images like this.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an image from a standard video phone, video conferencing test sequence called Mother and Daughter, and you might think, well, it's very straightforward as a mother and child in front of the background.",
                    "label": 0
                },
                {
                    "sent": "OK, but actually what constitutes the regions?",
                    "label": 0
                },
                {
                    "sent": "The important region?",
                    "label": 0
                },
                {
                    "sent": "This image is depends upon your application.",
                    "label": 1
                },
                {
                    "sent": "So for example, is the motor important?",
                    "label": 0
                },
                {
                    "sent": "Is the child important?",
                    "label": 0
                },
                {
                    "sent": "Are both the mother and child important, or is the entire image in part?",
                    "label": 0
                },
                {
                    "sent": "So segmentation is very much dependent upon what we want to extract from the image, which is very often not known in advance.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So segmentation is had a very long history of research and it's a very popular topic.",
                    "label": 0
                },
                {
                    "sent": "It's used in multimedia information retrieval, so this idea of extracting useful regions against which we can associate their color and texture descriptors for more fine grained retrieval.",
                    "label": 0
                },
                {
                    "sent": "It's used in shape analysis for optical character recognition.",
                    "label": 1
                },
                {
                    "sent": "I'll show you some examples of that.",
                    "label": 0
                },
                {
                    "sent": "It's used in shape classification.",
                    "label": 1
                },
                {
                    "sent": "Anne, it's used in medical image processing, so processing the artifacts in an X Ray or in CAT scans or an MRI, for example.",
                    "label": 0
                },
                {
                    "sent": "It's obviously used in industrial vision systems such as automated packing and robotic assembly line applications, and it's also used in knowledge assisted analysis, and I think we've seen some examples of that in previous.",
                    "label": 1
                },
                {
                    "sent": "Presentations I know.",
                    "label": 0
                },
                {
                    "sent": "Stephen talked at length about that yesterday.",
                    "label": 0
                },
                {
                    "sent": "We're trying to characterize what objects are in the image.",
                    "label": 1
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The process of image segmentation is typically to try.",
                    "label": 0
                },
                {
                    "sent": "The group pixels at into regions on the basis of some kind of homogeneity criterion.",
                    "label": 1
                },
                {
                    "sent": "An so we try to look for clusters of pixels which are similar in terms of their color or in terms of their texture.",
                    "label": 0
                },
                {
                    "sent": "Ann, and hopefully the output segments that we produce will reflect the real world structure of the of the image.",
                    "label": 0
                },
                {
                    "sent": "Am and there are many, many different approaches to how you might perform this.",
                    "label": 1
                },
                {
                    "sent": "What it's hard as a clustering process, so there are graph theoretic models, Markov random field approaches, probabilistic solutions, semi-automatic approaches.",
                    "label": 0
                },
                {
                    "sent": "So here is an example of an image and the result of four different segmentation process is applied to it and you can see that the results are all similar, but they're all different.",
                    "label": 0
                },
                {
                    "sent": "OK. And of course it's this poses a problem for image understanding and for content based retrieval.",
                    "label": 0
                },
                {
                    "sent": "So if for example you're interested in the boat in this image.",
                    "label": 0
                },
                {
                    "sent": "And you use this segmentation algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, you suddenly lost your boat and you'll never be able to retrieve that image with that boat.",
                    "label": 0
                },
                {
                    "sent": "And if that's your information need.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the challenges of segmentation can be characterized in two ways.",
                    "label": 0
                },
                {
                    "sent": "First is what's known as over segmentation, and this is the idea where the segmentation algorithm is all about merging or clustering groups of pixels.",
                    "label": 0
                },
                {
                    "sent": "M is stopped too early, so typically you start with every pixel as an independent entity independent region and you start to merge them and at some point you have to stop and when you stop the resulting clusters should correspond to real world objects or components of real world objects.",
                    "label": 0
                },
                {
                    "sent": "Here's an example for the form and test sequence where that's not the case.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've stopped the merging process much too early.",
                    "label": 1
                },
                {
                    "sent": "We end up with a very large number of arbitrarily shaped regions in the image, which are arguably completely useless to us in a content based information retrieval framework.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, we have a process known as under segmentation, and this is where we stop the algorithm too late.",
                    "label": 0
                },
                {
                    "sent": "So we continue merging until we actually lose what we're actually looking for.",
                    "label": 0
                },
                {
                    "sent": "So here's an example of this young boy on the beach.",
                    "label": 1
                },
                {
                    "sent": "We run the segmentation algorithm.",
                    "label": 0
                },
                {
                    "sent": "We let it run, and we stop it too late, and hey presto, we've lost the guys face.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're left with a headless child.",
                    "label": 0
                },
                {
                    "sent": "So a key aspect of research and segmentation is finding smarter merging processes and or clever stopping criteria trying to understand when to stop.",
                    "label": 1
                },
                {
                    "sent": "There are other.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Challenges associated with segmentation that make it a very problematic technology will be the very desirable technology for information retrieval, so it's an ill ill posed problem.",
                    "label": 0
                },
                {
                    "sent": "We've seen that it depends on what the application is.",
                    "label": 0
                },
                {
                    "sent": "That's what the segmentation should be.",
                    "label": 0
                },
                {
                    "sent": "There are other problems, such as what criteria should be optimized when we're trying to develop a segmentation algorithm.",
                    "label": 1
                },
                {
                    "sent": "In other words, what features are most important color texture, edges or some combination of these?",
                    "label": 1
                },
                {
                    "sent": "Which grouping principles, which clustering principles are most important when we're trying to construct these segments?",
                    "label": 0
                },
                {
                    "sent": "Is it their similarity in terms of their pixel values or texture values, or is it the fact they are close to one another?",
                    "label": 0
                },
                {
                    "sent": "Or is it the fact that when we merge them seen exhibits, some kind of symmetry, or some kind of compactness?",
                    "label": 1
                },
                {
                    "sent": "Also, we're going to combine some of these features in what proportion do they interact?",
                    "label": 1
                },
                {
                    "sent": "So there's a significant research effort undergoing which is trying to understand how to efficiently optimize such criteria.",
                    "label": 0
                },
                {
                    "sent": "So if we assume that there is no one Golden segmentation algorithm that's going to work for all images for all applications we need to take existing segmentation algorithms and optimize the more tailored and for specific types of content of specific applications.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how do we do that?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That leads to the next challenge.",
                    "label": 0
                },
                {
                    "sent": "The next question, which is that in order to optimize something in terms of whatever criteria we need to understand what a good result is, not a bad result is beautiful construct some kind of energy function that we can we can minimize.",
                    "label": 0
                },
                {
                    "sent": "So before we can develop that better segmentation algorithms, we need to answer what makes a particular segmentation good or what makes a particular segmentation.",
                    "label": 1
                },
                {
                    "sent": "But what makes one segmentation better than another segmentation in a given application?",
                    "label": 0
                },
                {
                    "sent": "And we need some way of evaluating segmentation.",
                    "label": 0
                },
                {
                    "sent": "So this is another problem with segmentation in that.",
                    "label": 0
                },
                {
                    "sent": "We as humans can look at the contours of the regions produced and say that's a good segmentation.",
                    "label": 0
                },
                {
                    "sent": "The contours are bang on the object boundaries.",
                    "label": 0
                },
                {
                    "sent": "But how do we measure that objectively?",
                    "label": 0
                },
                {
                    "sent": "For image quality we have measures like SNR for example which allow us to gauge objectively the quality of an image that in some way reflects the human visual system.",
                    "label": 0
                },
                {
                    "sent": "But for something as subjective as image segmentation, how do we come up with a single number that represents good or bad segmentation results?",
                    "label": 0
                },
                {
                    "sent": "So a key research topic in this area that's ongoing and you're particularly active in this area is to develop a comprehensive framework for characterizing segmentation and evaluating their performance in the context of specific applications.",
                    "label": 1
                },
                {
                    "sent": "This is a very nice research topic, which is that OK if you want to use segmentation, here's a way of taking the available existing state of the art, which is extensive characterizing it in different ways and settling upon the algorithm that's best suited to your application.",
                    "label": 0
                },
                {
                    "sent": "So this speaks to Steven's point yesterday when he said that segmentation is great, but very often it's very hard to use the existing state of the art on it's normally less hassle to just go and develop your own segmentation algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, which obviously is not ideal because bending the wheel time and time again.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the solution we developed the CU to segmentation problem that we're working with.",
                    "label": 0
                },
                {
                    "sent": "Targeting the idea of what we call syntactic segmentation, so most segmentation techniques group pixels in the basis of color and texture and don't really take account of the actual real world content of the image.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing here is trying to have a pseudo generic approach that doesn't look for specific objects but looks for the characteristics of objects in images.",
                    "label": 0
                },
                {
                    "sent": "So we start with an input image like to form an image.",
                    "label": 0
                },
                {
                    "sent": "We perform a fine region based segmentation.",
                    "label": 0
                },
                {
                    "sent": "So here is this 255 regions.",
                    "label": 0
                },
                {
                    "sent": "And we end up with a grouping of pixels like this.",
                    "label": 0
                },
                {
                    "sent": "OK, where again the regions don't really mean anything, except that this arbitrarily shaped region is all white.",
                    "label": 0
                },
                {
                    "sent": "And then perform some further processing on that segmentation result.",
                    "label": 0
                },
                {
                    "sent": "So for example, we start to merge regions on the basis of histogram matching.",
                    "label": 0
                },
                {
                    "sent": "So we take our color histograms that we normally apply to full images.",
                    "label": 0
                },
                {
                    "sent": "We apply them to individual regions and we merge similarly colored regions and we can go from a segmentation that looks like this.",
                    "label": 0
                },
                {
                    "sent": "The one that looks like this.",
                    "label": 0
                },
                {
                    "sent": "So here to regions OK, they still don't mean an awful lot.",
                    "label": 0
                },
                {
                    "sent": "OK, but they better reflect the real world structure.",
                    "label": 0
                },
                {
                    "sent": "The next step, then, is to say that, well, you know what?",
                    "label": 0
                },
                {
                    "sent": "Typically, in a real world environment, we don't have weak borders in our segmentation, so we don't have these kind of arbitrary lines in the segmentation, which are the result of lighting effects.",
                    "label": 0
                },
                {
                    "sent": "So we take each region.",
                    "label": 0
                },
                {
                    "sent": "We characterize each region in terms of its boundary complexity, and we remove the very very complex boundaries that allows us to end up with a segmentation like this, which again is perhaps more useful in the previous one.",
                    "label": 0
                },
                {
                    "sent": "We can then say, well, you know what in the context of of content based information retrieval we're interested in generating an image or a segmentation of images are relatively small.",
                    "label": 0
                },
                {
                    "sent": "Number of the most important regions.",
                    "label": 0
                },
                {
                    "sent": "We want large regions that cover a significant portion of the image, so let's get rid of small regions and regions which are included by significantly larger regions.",
                    "label": 0
                },
                {
                    "sent": "So in this case we get rid of some of the eyes.",
                    "label": 0
                },
                {
                    "sent": "We then say, well, you know what most regions in the real world tend to be reasonably compact and reasonably not particularly complex, so we're looking at the overall shape complexity of regions.",
                    "label": 0
                },
                {
                    "sent": "We can further filter the.",
                    "label": 0
                },
                {
                    "sent": "Image regions and end up with a segmentation result that looks like this.",
                    "label": 0
                },
                {
                    "sent": "So we're not using any real knowledge of the content of the image.",
                    "label": 0
                },
                {
                    "sent": "We don't know this is an image of a guy wearing a hat on a built background.",
                    "label": 0
                },
                {
                    "sent": "OK, by trying to incorporate some pseudo semantic can use that.",
                    "label": 0
                },
                {
                    "sent": "Features of real world objects into the merging process.",
                    "label": 0
                },
                {
                    "sent": "We can end up with potentially more useful segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example of the exact same process applied to different kinds of images, and as you would expect, we end up here.",
                    "label": 0
                },
                {
                    "sent": "Going from this image of the eagle in the corral database set, I think to this set of colored regions on a blue background.",
                    "label": 0
                },
                {
                    "sent": "Which is more useful and more tractable and more easy to process in an information retrieval perspective.",
                    "label": 0
                },
                {
                    "sent": "Because now we can assign color or texture descriptors to a much smaller subset of regions where the regions in this case actually have some semantic meaning, and there are other examples here.",
                    "label": 0
                },
                {
                    "sent": "This image has significant lighting variation across this scene, so if you were to throw a typical region clustering approach or image clustering approach to this, you end up much noisier segmentation, whereas we can end up with a smaller number of regions that are easier to.",
                    "label": 0
                },
                {
                    "sent": "Process and so on and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that leaves me nicely just before the break into my first demo.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This whole area of an image segmentation region based segmentation to support content based information retrieval.",
                    "label": 0
                },
                {
                    "sent": "Ann is a significant ongoing effort within an European project called Pay space for anyone who's actually working on K space or funded by K space.",
                    "label": 0
                },
                {
                    "sent": "A couple of people, so one of the work packages in the research program is focused specifically at this technology and the point was that, well, there's many academic partners in this case based project.",
                    "label": 0
                },
                {
                    "sent": "We all have our own pet approach to segmentation that we're very happy with and very proud of, and I've just shown you DC use approach, but your groups have equally good and equally interesting approaches, so the idea of this part of the project was to develop a web service for segmentation.",
                    "label": 0
                },
                {
                    "sent": "OK for the broader community, so.",
                    "label": 0
                },
                {
                    "sent": "If you are working on content based information retrieval and you don't care about segmentation, would you want to use segmentation in some of your experiments for your PhD?",
                    "label": 0
                },
                {
                    "sent": "Well, the idea is that you can use this web service without the hassle of going and reading the literature and implementing your own version of these segmentation algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the idea is you go to this website.",
                    "label": 0
                },
                {
                    "sent": "An you can browse images from your hard drive.",
                    "label": 0
                },
                {
                    "sent": "You can upload the image.",
                    "label": 0
                },
                {
                    "sent": "And if the network is working, this should offload.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've got a problem with my my network.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So you get the idea anyway.",
                    "label": 0
                },
                {
                    "sent": "You can upload the image, you can select a variety of different segmentation algorithms, so this is the one I just showed you.",
                    "label": 0
                },
                {
                    "sent": "There are two orders currently integrated at the moment there's the image uploaded so you can specify the.",
                    "label": 0
                },
                {
                    "sent": "The segmentation algorithm you want to run.",
                    "label": 0
                },
                {
                    "sent": "Run the segmentation algorithm and again the network is slow here, but the result will be will be produced.",
                    "label": 0
                },
                {
                    "sent": "That's that image uploaded and segmented in real time on the DCU server and a variety of different ways of viewing it.",
                    "label": 0
                },
                {
                    "sent": "So that's the average color for each region.",
                    "label": 0
                },
                {
                    "sent": "You can view the outline.",
                    "label": 0
                },
                {
                    "sent": "You can view the segmentation mask that's produced.",
                    "label": 0
                },
                {
                    "sent": "You can view the bounding boxes, so sometimes for information retrieval you don't actually need segmentation boundary.",
                    "label": 0
                },
                {
                    "sent": "You just need the bounding box around that OK.",
                    "label": 0
                },
                {
                    "sent": "So there's an example of the outline.",
                    "label": 0
                },
                {
                    "sent": "Here is an example of the segmentation mask that produced so you can upload an image and you can download these results OK, you needn't worry about how the algorithm actually works.",
                    "label": 0
                },
                {
                    "sent": "You can of course specified the parameters of the algorithm, so here are the parameters we've exposed in that particular segmentation algorithm can play around with those to get a better or worse segmentation.",
                    "label": 0
                },
                {
                    "sent": "And we also support batch upload.",
                    "label": 0
                },
                {
                    "sent": "So you say, well, that's great, but that's for a single image.",
                    "label": 0
                },
                {
                    "sent": "But there is an interface whereby you can upload a sequence an entire directory of images, or indeed an MPEG file or an Avi file.",
                    "label": 0
                },
                {
                    "sent": "The segmentation algorithm will chug along John Eudy output in real time, and then you can download the sequence of mass sort of sequence of conference.",
                    "label": 0
                },
                {
                    "sent": "OK, now we're using this in K space as a stable.",
                    "label": 0
                },
                {
                    "sent": "Level playing field to compare segmentation algorithms which helps us develop better segmentation algorithms.",
                    "label": 0
                },
                {
                    "sent": "But we believe it's also potentially useful resource for anybody who needs access to segmentation.",
                    "label": 0
                },
                {
                    "sent": "That the hassle of going in implementing it themselves.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's freely available on the net.",
                    "label": 0
                },
                {
                    "sent": "Encourage anyone interested going going play with him going and going to use it.",
                    "label": 0
                },
                {
                    "sent": "OK, that brings up the coffee break time, so it's 10:30.",
                    "label": 0
                },
                {
                    "sent": "Now we reconvene at 11 for the second part, electric.",
                    "label": 0
                }
            ]
        }
    }
}