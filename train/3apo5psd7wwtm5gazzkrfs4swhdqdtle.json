{
    "id": "3apo5psd7wwtm5gazzkrfs4swhdqdtle",
    "title": "Seeing People with Deep Learning",
    "info": {
        "author": [
            "Graham Taylor, School of Engineering, University of Guelph"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_taylor_deep_learning/",
    "segmentation": [
        [
            "Thanks a lot, yeah sure it's great to be back for another day so this talk is going to be focused more on the application end.",
            "So yesterday I was talking more at the architecture level and different objective functions on a particular topic.",
            "Today we're going to talk about a subject that has been explored in the deep learning community but is not as mature as detection and recognition.",
            "So we're all familiar that deep learning approaches have really exceeded in areas like object recognition.",
            "But in terms of understanding people and their behavior and their roles in a social context, I would say that deep learning is still at a pretty early stage, so that's what we're going to be focusing on today."
        ],
        [
            "Video now is ubic wittus and if I say this, the first thing that comes to mind is probably a picture like this.",
            "There are many of these things out there and our behavior is being recorded all the time, so there's many different purposes.",
            "You know if we can advance understanding of people's behavior and their interactions in a social context.",
            "Advances in learning techniques are obviously very fundamental to rendering this kind of data useful.",
            "That's not the only type of data that people are exploiting."
        ],
        [
            "It contains humans, so many of you are aware of sports teams that are hiring their own sort of data science groups.",
            "Now most of the professional NBA teams have such divisions where they're taking the hours and hours of video data from the recorded games and then doing analysis to try to improve outcomes of their players.",
            "There's other examples, of course.",
            "Egocentric video is an area of interest in the vision community.",
            "Devices like Google glasses.",
            "People are recording their lives now.",
            "And then one makes sense of that after."
        ],
        [
            "So really, humans are the dominant subject in nearly all video, so if you go to YouTube and you look at collection of video, alot of it contains humans and advancing the way that we understand humans and the interactions they have in video is of fundamental importance to a number of different domains.",
            "So I have a few of them listed up here.",
            "Some of them might not have thought of our areas about say design of public spaces, so we can understand how flows of people are moving through.",
            "Spaces we can design them better.",
            "Health care is an obvious one.",
            "People, a lot of people are working in improving outcomes for the aged individuals through the use of understanding people through video.",
            "And there's ways that we can now augment peoples interaction with the world.",
            "So who's heard of the Oculus Rift?",
            "Not as many people as I thought.",
            "OK, people are shaping up their hands.",
            "You know, there's a device that you strap on your face and you see the the world through a computer vision system so we can augment peoples reality.",
            "And there are numerous applications of that.",
            "Finally, in terms of improving the interactions that people have with computers and machines, particularly in robotics, understanding people in video is of fundamental importance."
        ],
        [
            "OK, so what's cool about studying humans?",
            "Well, understanding humans and their behavior is fundamentally different than studying objects.",
            "Humans are very complex entities.",
            "They are articulated.",
            "So they're not treated as rigid.",
            "Their visual appearance appearance is much more complicated than rigid objects, and they also have properties of cognition and self reflection and social awareness that makes their behavior different than the behavior of objects.",
            "So human behavior can be governed by very complex interactions, and we're really just getting at sort of the basics of starting to understand that.",
            "So basically, our long-term goals are too.",
            "Sort of begin to move towards the complexity of understanding such dependencies and scenes like the one I have up here."
        ],
        [
            "OK, so in vision has the field of vision has approached understanding human activity and behavior for a number of years and the applications so far have been enabled really amazing.",
            "Amazing changes in our lives so we see gesture recognition, face detection rolled out to consumer devices like cell phones and in the last few years machine learning has really been at the forefront of these developments.",
            "They have machine learning, has capitalized on vast amounts of data, and the inherent variability of human appearance and behavior in these data sources have been very important.",
            "The emergence of new computational paradigms like GPU's have also been important for this very high dimensional data.",
            "And deep learning has now emerged as a major force in vision."
        ],
        [
            "Of course this is not to say that vision has machine learning or deep learning has solved vision.",
            "There are many challenges out there, so understanding for example person to person or person to object interactions, understanding long running dynamical behavior and video and very large scale variation of parents are still out of reach right now.",
            "But these are problems that are being tackled by the learning community and they should open up new applications.",
            "So this is really just to give a motivation of what's out there.",
            "I'm going to focus today on because this is a summer school I'm focusing on.",
            "Achievements in the field, but this is really to motivate you and say that vision has not been solved yet, even though we're seeing these amazing applications, there's still a lot of room for improvement and new applications to tackle."
        ],
        [
            "So that's the intro I'd like to now just sort of divide this this lecture up like yesterday into three sections.",
            "I'm not going to be covering all of human activity and behavior understanding.",
            "I'm going to be of course missing important areas like emotion, recognition, facial identity recognition, and so forth.",
            "I'm going to be focusing mainly on the full body as opposed to just faces, but I will tackle three areas where it in which deep learning has played an important role.",
            "Pose, estimation, tracking and.",
            "Activity and gesture understanding.",
            "So in all of these, the focus is on seeing people and understanding them."
        ],
        [
            "So the first area I'll talk I'll tackle is pose."
        ],
        [
            "Estimation, So what makes pose estimation challenging is that you're trying to essentially estimate the locations of all the joints or a subset of joints of an individual from video or still images.",
            "And there is extreme variability in a person's articulation.",
            "I mentioned that before the difference between humans and rigid objects is that humans can configure their bodies and all kinds of weird and wonderful ways.",
            "Probably those that practice yoga or better at doing that.",
            "But I guess in many of these applications and the.",
            "Data that we're considering a lot of the joints might be barely visible, so they might be truncated because they're close to image borders.",
            "They might be occluded by other objects or other people.",
            "Self occlusions is also a major problem, so you know if I'm standing like this and you're facing me and my arms behind my back.",
            "I'm blocking my own body so we can.",
            "You can get around that sometimes with multiple cameras, but when we're dealing with a single camera, as as many applications, we need to use the models that can reason about things that we can't necessarily perceive through data.",
            "Now, contributing to the fact that many joints are barely visible also is our issues like resolution.",
            "So if we're dealing with say, images collected collected from YouTube frames, we may have the hands in some of these videos only occupying a very small number of pixels in each of these frames.",
            "There's a series of models from the vision literature called parts are parts models, one particular deformable parts model has been very popular, particularly in addressing the variability in articulations.",
            "These are very strong structured models that know a lot about the relationships between the different parts of the human body.",
            "These these systems are very effective in addressing structural structural relationships among the parts, but they're less effective in feature learning.",
            "They typically use very local sort of low level features, and people are starting to combine these sorts of models with deep learning techniques, and this is another interesting area of future research."
        ],
        [
            "So if we're going to use deep neural networks for precise localization of body parts, the first thing comes to mind is OK.",
            "Deep learning in particular comments which have been very popular in the last few years.",
            "I've been very effective at recognition test.",
            "Why don't just roll them out and apply them to direct regression of body posts?",
            "So put an image in regress to a dementia relatively low dimensional vector representing the two year 3.",
            "The positions of body parts.",
            "Now there's a future."
        ],
        [
            "Change is doing this, so if you take the standard com.",
            "Net Alex.",
            "Net or inception or whatever off this off the shelf, these typically use a lot of pooling modules in them and pooling of course destroys precise spatial information and precise spatial information of this is of fundamental importance for localising body parts, so this makes this isn't this is an issue with just applying these Nets off the shelf.",
            "One of the other more subtle issues is that the mapping from input space two poses is highly nonlinear.",
            "And it's multimodal, so these these sort of off the shelf comments that are good for recognition, aren't good at making multimodal predictions.",
            "So another way of looking at this is my hands behind my back.",
            "And you are reasoning about where you know the the location of the my wrist joint is.",
            "And maybe my fingers as well.",
            "There are many possibilities of where those could be alright, so we'd like to use systems that can allow for uncertainty in post.",
            "And finally the valid poses or the space of all, opposes.",
            "Presents a much lower dimensional manifold in this high dimensional space of potential configurations so you know we might be regressing to a vector of same dimension 20 which represents 10 body parts and their 2D locations.",
            "So if you look at the actual valid configurations, you can't put your arms through your your body, or at least I can't.",
            "You know you can't put your your one leg through the other leg and so forth.",
            "The space of potential configurations is actually very small, so in some sense it seems a little bit.",
            "Troublesome to make a discriminative network map to a space in which most of the configurations are really unallowable once.",
            "So what we might want to do is restrict the neural Nets output to a much smaller class of potential outputs, where were the majority of them, are actually valid instead of invalid.",
            "OK, so that's the approach that we take in some work we did last year."
        ],
        [
            "So this is the first work in pose estimation using deep learning.",
            "I'd like to highlight this was done.",
            "The person doing most of the heavy lifting on this was a postdoc at NYU Arjun Jain and what we were doing here is that instead of taking one big continent, taking an image in and the training it to map to pose, we actually used a series of part based combats.",
            "So we had one commnet per body part and they would look at as much smaller region of the image.",
            "So this allows us to use obviously much smaller comments that were applied in a sliding window fashion, so the comment would look at a Patch of an image and it would use a standard architecture using Reluz and Max pooling and predict a single binary output which was the presence or absence of that joint.",
            "So we would have our say, risk detector comment.",
            "We would have an elbow detector, a shoulder detector commnet and this allowed us to basically apply calmness to a problem that they were good at.",
            "Those detection or recognition, but that use them on a problem.",
            "A different kind of problem.",
            "Now the output of this part detector.",
            "These part detectors comments was quite noisy so ANAN.",
            "The other thing is that having independent part based Comnets wasn't able to allow us to enforce any consistency among the poses.",
            "So the the wrist and the shoulder obviously have a relationship and so did the rest of the elbow and so forth.",
            "So what we did is we had a second phase pose consistency model."
        ],
        [
            "That we use so you can see coming out, you know, say we're trying to detect risks.",
            "For this image we might have some hits on the output of the comment.",
            "This is a comment that again trained to produce to predict the risk location in 2D.",
            "It's applied again using a sliding window, so we get a risk prediction at every pixel.",
            "That's the probability of the risk being in that location, and you see it firing.",
            "See, it's trying to find the left wrist it's firing on both the right wrist and the left left risk, and it's firing in other places as well, so it's quite noisy if we apply this spatial model that uses.",
            "Relationships between different body parts.",
            "Then we get a much cleaner, much more cleaned up map and I'll tell you a little bit more detail about how that works."
        ],
        [
            "So we use essentially a graphical model in which we capitalize on probabilities, conditional probabilities that can be estimated from the training data.",
            "So based on all the labeled training data of where the how the body is configured, we can estimate distribution, say where the shoulder is relative to the elbow, where the elbow is relative the wrist, and so forth, and so forth, and so once we've made a bottom up prediction with this commnet, we can do some cleaning up using this spatial model.",
            "And these these priors are easy to fit.",
            "There's just essentially counting and."
        ],
        [
            "They look essentially something like this so.",
            "For a given the full set of distributions we can go and construct a distribution save for a particular body part using the movies.",
            "Here the first term.",
            "This is essentially the bottom up information coming from the confidence and then this second term is essentially a product over all of its neighbors in this graph.",
            "So for the for the elbow, the neighbors would be the wrist in the shoulder.",
            "For the shoulder, the neighbor would be the.",
            "The face and the and the and the elbow.",
            "So these are the distribute the distributions look like.",
            "So assuming that the face is at the position 0, which is the center of this image.",
            "The conditional distribution for the shoulders located here.",
            "And again this is just for one of the shoulders.",
            "So here is the left shoulder we use basically all the data.",
            "We just estimated a single shoulder conditional distribution.",
            "We didn't do one for the right shoulder and one for the left.",
            "We just flip them.",
            "And then we had a conditional distribution for the elbow given the shoulder being at zero and one for the wrist as well.",
            "Hey, so there's more variability in terms of the wrist location as you would expect, and so if we look at this in so then in the log probability domain we end up having safer the shoulder joint, a term that's coming from the convent.",
            "So this is the part of the shoulder part detector.",
            "It puts some probability of where it thinks the shoulder is and then we have terms coming from the face and then terms coming from the shoulder.",
            "And this takes into the account where we've estimated the the face location in this and the elbow location to be.",
            "We also introduced a sensually awaiting term which will allow us to sort of up, wait the bottom up prediction come from the from the commnet, but essentially we set that to 1:00 so we had a balance between the prior terms and the bottom up information.",
            "The visual information.",
            "Is there a question at the back?",
            "In the training data, exactly so these were just estimated essentially by counting the what we saw in the training data.",
            "Yep, Yep.",
            "Changing yeah, so I guess you could say that some this.",
            "This bears some similarity to those these models in terms of the the spatial prior.",
            "The part based prior.",
            "And really what we're changing is of course is this first term which is the bottom up information.",
            "Instead of using some.",
            "Some features were using essentially the convolutional neural net.",
            "So you're right, and it's certainly very similarities.",
            "Those approaches and later on I'll talk about a variant of this model which doesn't just fit these.",
            "It doesn't assume that you know the relationships among the body parts, but you actually learn this.",
            "This graph over the body in terms of what joints are neighbors of other joints."
        ],
        [
            "OK, so for the face we found that having a basically adding information from the shoulder in terms of the face location actually made the model worse.",
            "We think that's just basically because the network was already pretty good at estimating faces.",
            "Faces up typically occupy much larger number of pixels in the image, and people typically don't wear clothing over their face and include it, so we have we're able to estimate the face pretty well and incorporating information noisy information about the shoulder just makes the prediction worse.",
            "So what we did for the face is we used essentially a global position prior, so we just looked at the location of the face and all of the training data and use that instead of incorporating shoulder information for the face."
        ],
        [
            "Now there is sort of at the time we were developing this, there are researchers at Google who are developing a competing model and they called the depots and this came out slightly after so our work was that I clear 2014 this work came out, CVPR 2014, but they're essentially developed at the same time and they took the approach of doing exactly what I said you shouldn't do, which is taking an image and then just regressing it to oppose vector.",
            "Now there's some some intelligent tricks that they used to make this thing actually work.",
            "And that's why they were able to get away with this so that the two main things that they looked at doing is that they normalized both the outputs and the inputs with respect to the body.",
            "So the joints that it was actually predicting our joints in the normalized body coordinates and also the images that are being shown to the network or essentially cropped around the person, so they're using essentially an off the shelf person detector to 1st get a rough idea of where the person is.",
            "And then feed that as input to the confident.",
            "So that's sort of the step one to get such a system working.",
            "The the architecture that they used was pretty standard.",
            "This is the Alex Net network from Image Net 2012 that most people are now familiar with.",
            "So the the second trick that they used."
        ],
        [
            "Was to roll out a cascade of posed Regressors.",
            "So this is actually kind of a recursive mechanism similar to what's used in language as well, so they take an image right and do exactly what we just talked about.",
            "Say we're doing a risk detector.",
            "We apply this image of the body to the combat outcomes of prediction of all the locations and say we have the location of the risks and we want to refine that.",
            "Then we go and we take the current prediction we crop.",
            "A region around that, and then we feed it into the next stage of the continent to produce a more precise localization.",
            "And what you're doing when you do.",
            "When you apply this to subsequent stages is you don't predict the absolute location, but you produce the change or the offset and location to your original prediction.",
            "So you're just learning to refine your prediction prediction at every successive stage and each successive stages essentially seeing a higher resolution version of that image at that particular joint you're interested in.",
            "Now all of the joint regression networks are essentially sharing this.",
            "The parameters in the lower stages, so you get basically implicit relationships among the body part, but they have no explicit model that's coding in any priors about how joints are related to one another, so they don't have this sort of structural term."
        ],
        [
            "OK, so that's how their method works, so these are well look about will look a little bit later at how these these methods stack up against one another.",
            "I wanted to 1st talk a little bit about, you know if you're getting started in this area, pose estimation, what kind of datasets should you turn to?",
            "And I mentioned that yesterday when I was talking about similarity learning, embeddings of pose when I started doing this back in 2010 that there weren't a lot of really great data sets out there.",
            "Since that time there's been a tremendous number of datasets become available for 2D.",
            "Pose estimation, so the ones that were used in the genital work as well as Szegedy and Anto chef, they evaluate on two modern datasets.",
            "One is flick for frames label in cinema.",
            "In the order of thousands of labeled frames, antennen upper body joints and they also evaluate on lead sports.",
            "Lead Sports is a larger data set, but it's restricted to the sports domain and it has 14 full body joints labeled now there's."
        ],
        [
            "I mentioned this yesterday the the MPI human pose datasets.",
            "It's actually much larger than both of these datasets, so this is another great place to start.",
            "But when this wasn't available, when those when these these techniques such as depots and Arjun James NRS work came out."
        ],
        [
            "So the the metrics for pose estimation are typically focused on on two approaches.",
            "So either we look at something called the percentage of correct parts which is just measuring the detection rate of limbs and what we mean by detection rate is.",
            "A limb is considered detected if the distance between the two, the true.",
            "The two predicted joint locations and the true joint locations is smaller than half of the limb length.",
            "OK, so you look at the error.",
            "And then you look at that sort of relative to the full limb.",
            "Now the problem is that this tends to penalize shorter limbs, So what people have been doing more recently is using something called protect percentage detected joints, which is similar.",
            "It's just normalized against the torso or the the person's body, so this is again just the distance between the Detroit detected in true joints.",
            "Typically what people do is they look at certain fractions of the torso width.",
            "So let's say how well are we doing when we consider a detection to be.",
            "10% of that or so, and then 20% or 30% or so to be accurate detection.",
            "So you generally generate these curves to show how your how robust your technique is.",
            "As you become more and more sort of particular about your evaluation metric."
        ],
        [
            "OK, so in terms of the state of the art we've talked about these different techniques, so there's the technique using part detectors.",
            "There is a technique depots that just uses a big convolutional net and regresses to pose, and then there's also this enhanced version which is being shown at ours and these results come from that paper which is enhanced version of the part detector I described before.",
            "So what it's using is essentially, you know it does 2 enhancements.",
            "One is really an implementation detail.",
            "It efficiently implements the sliding.",
            "Window part detection through using convolution.",
            "So it's actually it's it's borrowing something from this work from the NYU lab called over feet and it says you know if you're going to apply a sliding window detector.",
            "There's a lot of redundancy in applying certain spatial redundancy and applying these convolutions to every single location, because these sliding windows are obviously overlapping, so it made the method is much quicker, but they also make some modifications to the spatial prior component.",
            "Like I mentioned before, when we were working on this.",
            "Last year we had coded alot of in prior information about how the body was wired up essentially right.",
            "We knew that the risk and the elbow were adjacent and that was adjacent to the shoulder and so forth.",
            "They define a more general structured model of Markov random field and they learn the relationships among the body parts.",
            "So that tends to work much better than the existing techniques.",
            "So in terms of you know what's the best out there?",
            "This work they published at NIPS 2014.",
            "Arjun Jain again is the lead author that seems to be outperforming.",
            "The depots work an the earlier work on both the flick datasets and Leeds and Leeds Sports."
        ],
        [
            "OK, so that is that sort of state of the art right now in pose estimation.",
            "Next I'm going to be talking about tracking.",
            "So."
        ],
        [
            "One reason why I think tracking is particularly interesting is that it seems to be unexplored to date in the deep learning literature.",
            "So we had some work at a number of years ago, which we'll talk about shortly, and this work really combines two different things that have seen a lot of developments in the last five years in deep learning.",
            "So the two different things.",
            "One is dynamical models, so this is a model that understands the way the body moves and the other thing.",
            "Is visual recognition model or visual model so?"
        ],
        [
            "Actually, the problem here is like pose estimation, but there's a time element, so at every you now have a video instead of still images, and every time you're trying to essentially in the 3D set up, put a 3D model of the body on that frame, fit it to that frame so the time element comes in.",
            "Now that these these these friend there's obviously relationships between them.",
            "So if you can understand something about the way that the body moves and you can learn this from data, then you should have a better job predicting.",
            "The change of the body from one frame to the next.",
            "So I'd like to say to you that you know motion capture data, the type that's used to fit these prior models of human motion have often been used to assess the performance of different dynamical model, so people you know recurrent neural networks right now are very hot.",
            "Often people will use motion capture data as sort of a benchmark in a predictive setup and people will say no.",
            "Our dynamical systems dynamical model is better than the previous work because it's able to predict one frame in the future much better.",
            "But in terms of sort of the the applicability of a generative model of motion that it doesn't see a whole lot of use.",
            "But this is actually one area where a generative model of motion is very useful as a prior in this tracking setup."
        ],
        [
            "So essentially, this is what I'm talking about.",
            "You have a model that governs the motion of the body, so you have some latent variables and those represent the pose and the dynamics.",
            "And then at each time frame, this generative model can actually generate some human motion as conditioned on what happened in the past, and the ANAN information flows among the different hidden units.",
            "So we use a particular type of model derived from restricted Boltzmann machines.",
            "I'll talk about the details of that shortly, but this could be something like a recurrent neural network.",
            "This structures is quite similar.",
            "As well.",
            "OK, so basically we have a visual model, Anna dynamical model combined and this makes tracking a very interesting problem and actually not a lot of I haven't seen really any modern works in this in this area."
        ],
        [
            "So up into the point sort of 2005 2006 when deep learning really the name, got coin and people became very excited about it.",
            "People were using dynamical models as priors for tracking, but the previous models were all activity specific.",
            "And ideally we would like to have are these dynamical models that can cope with different types of activities.",
            "When I say activity, I mean things like walking and running and jumping and boxing and so forth.",
            "The types of dynamical models like RN ends that we can build from deep learning within the deep learning community are actually quite capable of capturing many different activities, and some of the work I did in the past, we were exploring these sort of and generative setting.",
            "Can you generate all kinds of different types of motions?",
            "But here it can be used for.",
            "Recognition of."
        ],
        [
            "These activities as well.",
            "So we introduced in this paper something called the implicit mixture of of conditional restricted Boltzmann machines.",
            "It's kind of a long name and I'll go through each of the pieces of this shortly, but its main advantages is that these sorts of you can call them connectionist or deep deep deep learning style models are very easy or capable to train using quite larger datasets in the work that were considered in the past.",
            "Now the interesting thing about this particular model.",
            "This implicit mixture model is that it can be trained, supervised.",
            "If you know activities, so the model has some some special hidden units that can assign to activities.",
            "So if you're know you're only dealing with walking and running and jumping and blocks and label those activities, or if you know you're just going to have a whole lot of different activities and you don't have labels for them, a priore, you can allow it to assign these discrete activity units on its own.",
            "However, it sees sees fit.",
            "So you actually get two things from this model.",
            "You get a posterior where we're showing the the the maximum over the posterior of the pose.",
            "So this is what you're interested in for tracking.",
            "But the other thing you get for this model, which is a useful byproduct, are the states of these different activities.",
            "So this is showing one that you haven't trained at supervised.",
            "You train it unsupervised, and it's it's assigned what we've been calling these movings or discrete units of activity, and these can be interpreted.",
            "I I have a.",
            "There's a postdoc that I'm working with who's trying to understand the social dynamics of fruit flies, so this is sort of the MNIST of the biology world.",
            "And.",
            "So he's actually capturing interactions between different families of fruit flies in Petri dishes with the vision system overhead and trying to assign sort of different moving so that the types of behaviors that there are undergoing.",
            "So there's there's cool things you can do from fruit flies you can sort of knockout their their olfactory system and see if that has an effect of their social interactions.",
            "You can introduce no different variants, different families, genetic variants of these flies, and see how that affects their social interactions and so.",
            "Yeah, this is so.",
            "It's it's interesting.",
            "A very interesting area of research in my opinion.",
            "OK, so back to human motion, not fruit fly motion.",
            "We want to track."
        ],
        [
            "So sort of a high level view of how tracking works in this setup is the input is obviously the frames of video, so we're going to call these why these are the image features and what we are ultimately interested in getting to.",
            "Is this 3D body pose, so we're going to call that X. OK, so during training we have some information.",
            "We have mapped video in motion capture, so this is observed during learning, but at Test time or tracking time, we're trying to recover this and then we have some latent variables.",
            "So I mentioned this is based on an RBM.",
            "So as you're familiar now, given the previous lectures, the PBM has a bunch of hidden units, binary hidden units, but we have augmented this model with some additional hidden units which represent activities, and I said we either know about these activities, or we allow the model to assign them as it sees fit, will call these discrete ones Q and the standard DBM hidden.",
            "Zed you've seen them as H before, but we're going to call them zed.",
            "OK, so that's the."
        ],
        [
            "Doubleview I know.",
            "I think it was Aaron, I think discussed our BMS already.",
            "So I don't need to go through these.",
            "Probably you know you have basically every visible is connected to all of the hidden's.",
            "All of the hidden's or connect or each hidden is connected to all of the visibles there, efficient to do inference in, and can be trained with well known approximate learning."
        ],
        [
            "Times Now.",
            "You've seen this picture before.",
            "OK, So what I'm going to do is I'm going to represent the model by this simplified picture over here, so I'm going to collapse all of the observations into a single random variable, which I'm calling X and then collapse all the hidden's into this hidden variable zed.",
            "And that's over a vector of binary observations, and this is the picture I'm going to use for the pose of the person, and this is the picture that I'm going to use for the binary latent variables.",
            "OK, so that's just the PBM you guys have seen this."
        ],
        [
            "Already in the summer school.",
            "What I'm now going to do is I'm going to introduce these additional action units to allow the model to kind of switch between different activities, So what?"
        ],
        [
            "I'm going to do is I'm going to 1st modify the model so it can capture temporal effects, so I've introduced a connection from the past.",
            "OK, so this is what makes it a conditional RBM.",
            "We've added the history.",
            "I'm showing HT.",
            "This picture is showing one frame of the past, but HT means it could actually represent a number of frames of the past, and in practice usually a few frames are used.",
            "So here's the history of the motion.",
            "Here's the current observation, and we're going to condition inference and reconstruction.",
            "On the history, that's what makes it a conditional RBM.",
            "That sort of the point I want to make here is, it really doesn't change inference, and learning when you do this, you can still use contrastive divergent or whatever your favorite learning approximate learning technique is question.",
            "OK, So what we're doing is we're so it think of you.",
            "Think of just a standard DBM.",
            "OK, so going back."
        ],
        [
            "Here before we've introduced these different connections we have, we want to model pose, so the the features here are binary latent variables and if we were to just train this model on body pose, it would capture something about the relationships among the joints.",
            "So their features about body pose about how the body is configured.",
            "When we add in connections from sorry went the wrong way."
        ],
        [
            "Add in connections from the past, then those features.",
            "Arts also start capturing things about the way that the pose changes through time, so it starts to connect.",
            "Learn something about dynamics.",
            "OK, so yes.",
            "Yes.",
            "Yes, so there's a couple of ways that this differs from the HMM models, so one is that you're not restricting to a single hidden state, right?",
            "So we have multiple binary latent variables, which makes it more powerful representation than HMM.",
            "The the other thing is that the thing that shares with Hmm's is that you can do exact inference easily.",
            "Now.",
            "The thing is, we've introduced these additional connections which the HMM doesn't have in it.",
            "So in terms of the way that we train this, it's kind of.",
            "It's a bit sneaky in terms of adding these additional connections because you can.",
            "Essentially, when you train the model is getting a bit detailed into the practical implementation, but you can take Windows.",
            "So you can take a frame and its previous say 5 frames of history.",
            "Anne Anne Anne Anne.",
            "Shuffle those up into mini batch and just do training on those you don't have these long term dependencies that you have to deal with it training time.",
            "However if you connected these hidden units through time like the HMM is doing, that makes inference more challenging.",
            "So the HMM is able to cope with that because it has a very simple hidden state.",
            "But as soon as you take something with complicated hidden state and you connect it through time and you're trying to influence over it.",
            "That becomes a little bit tricky, trickier.",
            "There are variants of this model, so ileus discovered it a bunch of work on temporal variance of restricted Boltzmann machines, where he actually did wire up the hidden units through time, like in is in a recurrent neural net, but he used approximate inference schemes.",
            "He's kind of ignored, sort of.",
            "The affect of the future sort on on what happened in the past.",
            "And there's another paper shortly after that that we collaborated on that essentially does sort of a hybrid of this model with a recurrent neural net and leverages backdrop through time.",
            "For learning these these these connections at the weights that connect hidden variables through time.",
            "So this is, this is a very simple variant of this model.",
            "It's a good one to introduce the summer school, but there's actually quite a rich literature on these sorts of models with varying degrees of complexity in their connections.",
            "OK?",
            "Gray"
        ],
        [
            "So we've gone from GBM to conditional RBM to model time.",
            "Now we're going to introduce."
        ],
        [
            "These sorry, these additional state variables which are used for activities.",
            "OK, so the way to think of this is this additional Q variable is able to set an effective CRB Mo for every this.",
            "The way that this guy is modulating or changing these weights for every setting of this discrete variable, you effectively get a new CRBMO depending on the activity or if you're unsure about these activities with these were taken in real values.",
            "It kind of blends in.",
            "Different dynamical models, and that's and those are inferred over overtime."
        ],
        [
            "OK, so in terms of the math not going into too many details about the math, but what we're interested in is we're interested in a predictive model of the pose at time T. Given the history of poses, that's what we need for tracking.",
            "Remember, we said if we have an idea of the way the body moves through time?",
            "There goes the phone again.",
            "This I can't believe this happened to me in both talks.",
            "Yeah, I'll hang it up.",
            "Hopefully nobody calls again.",
            "So if you have an idea of how the body moves through time, you can do a better job of tracking.",
            "So this is what this is.",
            "The distribution we want to estimate.",
            "But now we have these latent variables, so we're going to marginalized over the PBM style variables.",
            "An these activity variables.",
            "To get this distribution were interested in, and actually, when we do this, we get a form or expression that looks very familiar to people that know about mixture models.",
            "So we actually have a mixture of C GBM models and This is why we call it the implicit mixture of CR, bcause the mixture form kind of falls out from the way that we set up the energy function.",
            "So essentially, if you have a term that's being controlled by these activity variables and those activity variables like a setter, sort of switching in and switching out these dynamical models and by doing inference over these guys gives you that little black and white picture I showed you before of the movie games.",
            "Right, and this is the thing that my post Doc is using to sort of infer the sort of groups of activities that the fruit flies are doing.",
            "OK, so."
        ],
        [
            "The advantages so compared to the previous dynamical models, people looked at.",
            "This can leverage large datasets like I said and be trained pretty quickly, sort of in on the order of minutes on GPS.",
            "You can sample front it from it.",
            "An I said you can train it with or without active."
        ],
        [
            "The labels.",
            "OK, so that's really a lot on the dynamical model aspect.",
            "How does this fit into the whole tracking framework 'cause we need to connect this to the image observations?",
            "So what we're ultimately interested in for tracking?",
            "Remember, we're given a series of image frames were calling these Y and we want to predict.",
            "So this is the history of image frames and we want to predict the current pose.",
            "So that's the filtering distribution.",
            "What we are ultimately interested in, and that can be decomposed.",
            "So assuming the independence.",
            "Which we do in this model of the the observations.",
            "Given the pose, we can break this up into two terms, and so these two terms one is the likelihood.",
            "OK, so say given the body pose, what's the likelihood of observing a particular frame and the other one is a prediction.",
            "OK, so I'm going to talk about the likelihood a little bit later that we just define what that looks like the prediction."
        ],
        [
            "Is actually written in a recursive form.",
            "So assuming now a first order Markov assumption, which can be actually relaxed to an NTH order Markov assumption, we can break up this predictive distribution into an integration over a series of terms that involve a product of two things.",
            "So one is the dynamical model, so this is what we just talked about.",
            "This whole IMC PBM, and then the most recent posterior.",
            "So this guy look, we're predicting this.",
            "So this is the previous.",
            "Action that we got, so it's really a recursive definition.",
            "This all this all is nice.",
            "We we can define our likelihood.",
            "We can define our dynamical model and then we can update this recursively.",
            "The only problem is that this integral right here is not analytically tractable.",
            "So what we do is we use something that is well known and quite popular in in tracking as we use up what's called a particle filter.",
            "So we have basically a discrete number of observations.",
            "We represent this integral by just a series of points and they waited.",
            "OK, so that's how we."
        ],
        [
            "Use there to get around that intractable integral."
        ],
        [
            "OK, so.",
            "There's two things that we defined the dynamical model which we just talked about.",
            "Anne."
        ],
        [
            "And the likelihood.",
            "So for the likelihood, I'm not going to go into too many details.",
            "We essentially took what other people had used in the past.",
            "There pretty simple likelihoods.",
            "Given a body model, we can basically figure out the edges if we project that, sort that body into the current frame and match those edges along the edges of these.",
            "The cylindrical body model with an edge map coming from the observation observed frame.",
            "It's a very simple model.",
            "You can also do this, something similar, which is take the cylindrical body model.",
            "Which you know, if you have the pose you have this configuration, the body model, you can project it into the frame and essentially fill the interior with white and match that up with the silhouette from the image and this allows you to match the image to your current hypothesis about the body model.",
            "These are pretty these are handcrafted, they're pretty simple, they work OK in our setup, but I think there's work here to be done in terms of you could learn likelihoods, of course using deep learning techniques, and you can also learn the dynamics using deep learning.",
            "Or you could essentially learn the whole tracking setup using deep learning, and I haven't seen anybody do this yet.",
            "OK."
        ],
        [
            "So in terms of question at the back."
        ],
        [
            "Yes.",
            "Right, OK, so the questions about Q does it?",
            "Does it have to correspond to the number of known activities in your data set?",
            "If you train it in a labeled way, then it does, because those the there's information attached to each other or an identity attached to each of those variables.",
            "If you're training in an unsupervised way, then you just have to choose an arbitrary number of variables to give it, just like the arbitrary number of zeds you give it.",
            "We could also think of another situation where you split these queues.",
            "Into the unsupervised movings and then the activity specific variables.",
            "We didn't try that, but you could do a mixture of both.",
            "Yeah.",
            "So I missed the first part of the question, I didn't.",
            "Yep.",
            "Oh, I haven't seen that often.",
            "You know in the on.",
            "So when you deploy this model right and you're doing tracking at Test time, you don't know the future necessarily, right?",
            "So you, unless there's some sort of hypothesis about the future using some external sort of non post specific information.",
            "That might work, but I I haven't seen anything like that.",
            "OK, so."
        ],
        [
            "So in terms of the way that we experimented, we use something called the human Eva data set.",
            "And as I mentioned before, that has simultaneous video from a number of cameras and motion capture an they're synced up.",
            "So this is what allows you to train the model.",
            "The limitation of human Eva is that it's an indoor controlled setting, so to get the mocap they actually had to put markers on the individual, suit them up.",
            "As you can see an they do all these different activities.",
            "You have multiple subjects, so it's a nice fairly big data set.",
            "But you can't take the model and deploy it outdoors, so again this is an area of future research.",
            "It's kind of unexplored in my my opinion, doing sort of less controlled tracking using these sorts of dynamical models and possibly adding in visual models as well.",
            "So the other thing that we really we we measure at Test time is we.",
            "We attempt to do the track and then we measure in millimeters the average error between the true three depots coming from the motion capture and the pose that's estimated from our tracking system."
        ],
        [
            "So there's there's lots of results that are in the paper.",
            "There's a couple that I'll highlight.",
            "1st, If we look at doing these sort of different variants of the model, we do this the multiview setup, where we're actually using multiple cameras.",
            "This makes it easier because we can cope with occlusions and when we look at walking and jogging with transitions are modeled as much better than the baseline, which is a very simple dynamical model is just using sort of a constant velocity prediction.",
            "He adding in these discrete variables and using their labels.",
            "So that's what the two L. So we have a walking and jogging label that does extremely well when we know the activities.",
            "If you just give it 10 latent variables and you allow it to assign these sort of moving."
        ],
        [
            "This is the way that it does it.",
            "It doesn't do quite as well in terms of prediction, but for a totally unsupervised model it does a reasonable job.",
            "And the cool thing is that the way that it uses its hidden units.",
            "So it seems that some of them are actually like this.",
            "This guy and this guy along here.",
            "So the third feature is used for both of the different activities.",
            "So it's something about the motion itself.",
            "But some variables are really activity specific.",
            "So for example in this the 9th variable isn't.",
            "Use very much in the jogging component and same with the first variable, so some other sort of activity.",
            "Specific and summer activity.",
            "Generic variables, yes.",
            "OK, so typically what we do in this setup is we look at two things, so I actually.",
            "This is important 'cause I have some stars here, so the stars mean when we we train the prior model on the same subject.",
            "So we have a subject specific motion model.",
            "So we kind of understand the way that that person moves and that tends to work well.",
            "You when you are using a simpler model, but as soon as you add more parameters to your model that sorts starts to overfit.",
            "So in that case we we train a generic model so it's multiple subjects, learn how they move and deploy it at Test time.",
            "Now at Test time we always see motion of the same subject, so we never take your motion and then train a model and then try to predict my motion.",
            "There's always either that individual in a larger group, or a person specific prior.",
            "Not great, but we test on completely different frames so there's different sequences of that person moving around.",
            "OK, so the."
        ],
        [
            "Last thing I want to show in this work is the more complicated setup where we do not multi view, but manach monocular tracking.",
            "So this is where you have a single camera and this makes it harder to track an individual because there can be occlusions.",
            "So you'll see our method doing a reasonable job.",
            "This is essentially how well it works.",
            "It makes errors sometimes, particularly on the arms when there's a transition, but it does a pretty good job tracking a person from a single camera when there's transitions.",
            "And before this approach, other people weren't able to handle this situation very well at all.",
            "When there were transitions in the data."
        ],
        [
            "OK, so that's tracking so remainder of the talk I'm going to focus on activity and gesture.",
            "Oh yes right there.",
            "Yes.",
            "Sure, so I should.",
            "Also that brings up a good point in that in this work we always initialized with the correct initial pose.",
            "So we started our model off knowing where the person was in the frame, and that's pretty crucial to making it work.",
            "Well now you're saying what happens if you don't have that initialization?",
            "Then you're essentially falling back to pose estimation, which is a problem I talked about originally.",
            "Given an image that is the 1st frame of the video.",
            "How is that person configured?",
            "So if you have a very good pose estimation method, you say one of these ones like depots.",
            "Yes.",
            "Oh well, I guess I mean, say it's it's making a probabilistic prediction.",
            "You would kind of know about the uncertainty outputs, but that's really all you have.",
            "I mean, you don't know if it's a perfect model of predicting the pose, so you have to use that model and rely on that as your initial prediction.",
            "Unless you had multiple models right, she has sort of an ensemble and you could use that to give you some idea of uncertainty.",
            "So if all the models didn't agree on a particular pose, then you may have to fall back to, say a human initialization or something like that.",
            "That's the other way or rely on different types of models.",
            "OK, so the 3rd and final section is activity in gesture.",
            "So while pose estimation and tracking, we're focused on where is the person in the scene and how is their body configured.",
            "Activity and gesture understanding really looks at what is that person doing, and so this is received a little bit more attention from the deep learning community, so I'm going to divide this up into 2."
        ],
        [
            "Actions the first is going to be models that I would call are unsupervised and supervised hybrid models.",
            "So what these techniques do is they learn using unsupervised learning, a way to extract features and then they build a recognition pipeline on top of that.",
            "The second approach is just an end to end supervised learning approaches, say applying a 3D convolutional Nets to activity recognition.",
            "So let's talk about the hybrid approach."
        ],
        [
            "1st, So going back to some work that we did a number of years ago, we used a basic building block called a gated PBM.",
            "So I roll and unfortunately hasn't had a chance to do his next lecture yet.",
            "I was maybe hoping he might introduce some of this stuff, so I'm gonna give you 1 slide overview of this model and then roll is going to tell you all the great advances in multiview feature learning because he's really expert in this area and he invented a lot of this.",
            "He invented this.",
            "Some of this stuff, and he also sort of looked back in the past and understands a lot of the work that's been done in the past as well.",
            "So the gated GBM.",
            "So the way to think about this again, you'll get more information a little bit later, but think about this in one of two ways.",
            "If you understand linear autoregressive models.",
            "You can see this as sorry I want.",
            "I want to take a step back and say something else that's more basic.",
            "Think of this as a type of GBM that no, that doesn't take a single observation, but it takes pairs of observations.",
            "OK, so we had GBM has visible units and hidden units.",
            "Now we're going to have input units and output units and hidden units.",
            "So both the inputs and outputs are going to be observed and these guys are going to be latent.",
            "OK, so that's the first first thing you need to think about.",
            "Takes pairs so that and the pairs that we're going to use here are adjacent frames.",
            "Videos were used to extract motion features.",
            "So given the input and output, if you're familiar with auto regressive models, this is like a linear autoregressive model that say predicts output from the input.",
            "But you make it non linear by introducing a series of latent variables that can modulate the connections in that model.",
            "So given a particular setting of the hiddens, this kind of defines this.",
            "This model or relationship between input and output.",
            "So that's one view.",
            "Another equivalent review is view is if you understand our BMS and kind of like that picture, it's just like having a standard GBM defined over the output.",
            "But now you have the input modulating or changing the weights in that DBM.",
            "OK, so for every setting of the input that defines basically a different DBM on the output.",
            "OK, so that was the basic building block."
        ],
        [
            "Of this model.",
            "We scaled it up to convolutional type of gated GBM which would allow it to do pretty efficient at the time.",
            "Feature extraction from video.",
            "So we look at pairs of frames from video.",
            "The input and the adjacent frame.",
            "We would extract features from this and inspired front by the convolutional RBM which hung Black talked about yesterday.",
            "We also had a Max pooling layer built into that, and So what we did you know instead of connecting up all of the inputs and all the outputs in a frame.",
            "These these guys were connected locally and we had feature sharing like in standard convolutional Nets, so the types of features that you get out when you essentially do feature extraction across multiple frames are features that tend to highlight."
        ],
        [
            "Motion, so I'm now showing some images of feature Maps, so each row here is a different feature map that's been learned and produced by this model on a particular data set.",
            "This gets me every time OK. And you can see that some of these features are very motion sensitive, so there's there's two types of activity here.",
            "One is hand clapping, one is walking, so in the hand clapping setup you'll see that features one and three are very motion sense of their targeted on on the hands.",
            "The part of the video that's moving, you get some features that actually capture static information too.",
            "So one feature actually learns to just.",
            "Pick out edges, in particular around the individual, and then the 6th feature tends to do essentially a segmentation operation.",
            "So combined these give you a nice idea of what's going on in the image, both statically and dynamically."
        ],
        [
            "So once you've extracted these motion features, I said this is essentially a hybrid approach where this first block here is doing this unsupervised feature extraction with that then use that as an input, which is consumed by a 3D convolutional net.",
            "But it's a very shallow 3D convolutional net.",
            "We then want to produce a single prediction for a whole video or snippet of video, so we have to do some pooling across the video to get a single output.",
            "So we have a temporal pooling setup, and then we have a number of fully connected layers.",
            "I'm going to look at the results a little bit later on.",
            "When?"
        ],
        [
            "Look at some different approaches compared to one another at the same time that we were doing this, there were some researchers at Stanford Kwok Lai and his colleagues who were using a different unsupervised module called Independent Subspace Analysis to also do feature extraction.",
            "Now ISA produces filters which are tuned to velocity, frequency and rotation.",
            "So their key idea was basically scale up ISA through.",
            "Convolution in stacking and we were scaling up these.",
            "These are BMS through convolution and stacking them as well, so it's just a different unsupervised learning building block, but a similar approach."
        ],
        [
            "What they would do basically is take a quite a high dimensional video, break it up into blocks.",
            "You can think of sort spatiotemporal patches.",
            "Train these ISA model on these patches and then and concatenate them.",
            "Perform a PCA whitening and dimensionality reduction and then do another layer of ISA on top of this, we just repeat the same module multiple times, but you have some dimensionality reduction happening in the middle to help you deal with the sheer mass of data that you have."
        ],
        [
            "This is just another picture of it, where you're taking a video.",
            "You're cutting it up into little blocks.",
            "You're applying isa to each of those blocks, and then you're combining the outputs, reducing them dementia, reducing the dimensionality, and then training another ISA block on top of them.",
            "Now the interesting part is, once you do the highlight when you're looking at high level features, you're not only taking the output of the higher level ISA module, but you're also taking sort of.",
            "You're doing what people call Skip connections where you take the PC of previous.",
            "Features that have been extracted and that basically gives a descriptor just like we had those descriptors I showed you of the motion sensitive features and then those are applied to an activity recognition pipeline.",
            "The last approach, that sort of falls in this."
        ],
        [
            "Unsupervised supervised learning hybrid."
        ],
        [
            "Is some work that Roland Ankush or Conda did, and both of them are here today.",
            "I hope I don't do a terrible job explaining this in front of the authors, but they looked back at some of the previous work that had happened and both basically energy motion models that were used through the 90s and also these cross correlational models and they found that the way that these models were shaped essentially they were confounding two things, so the first thing that was the extraction of the motion.",
            "For the detection of the type of motion, but in these models also had sort of a invariants component to it, so they were trying to learn invariant representations, and these were confounded.",
            "And So what?",
            "Rolling in cash, or were looking at were essentially a particular type of architecture that could be couple these and essentially do motion detection separately from learning invariants.",
            "And so the way that they."
        ],
        [
            "Detected the end up detection mode.",
            "Detecting motion is through a concept they call motion synchrony, and it's quite a clever idea.",
            "So first of all, they say, let's assume that two images are related through an orthogonal warp, so there's some matrix P, and this is orthogonal matrix as Roland mentioned yesterday.",
            "You can think of this as sort of a permutation matrix or matrix that shuffles around ink from image X to image X1 to X2.",
            "But it doesn't destroy any anchor.",
            "Introduce any any new ink in the process, and so if they are related via this transformation, one way that you can detect this transformation and they prove this in their paper is all you need to do is take two filters that are related through the same transformation and then check through.",
            "Check for a condition that they call motion synchrony.",
            "So they're really determining whether these these two filters yield equal responses when they are applied in sequence 22 frames.",
            "You have two frames.",
            "Adjacent to one another, you're going to apply these related filters, and then if these guys are equivalent, you've successfully detected this particular type of motion.",
            "OK, so the proof of that is in the paper.",
            "Let's go on to, oh, sorry.",
            "OK."
        ],
        [
            "How do you actually detect synchrony in in practice so?",
            "You know, if you fall back to standard neural networks, which essentially use sort of a standard, some of the filter responses, and then do a threshold.",
            "It's it's impossible for you to tell whether the reasonable active activation beyond some threshold is a result of essentially one really good match.",
            "So say W1X1 is a great match and W2X2 is a crappy match versus both of these guys matching pretty well, so they both match pretty well.",
            "That's an evidence of the synchrony happening if one guys matching really well and the other guys matching not so well, then synchrony isn't present, but you really can't tell that if you just sum their responses and you threshold it.",
            "But what you can do is you can use these multiplicative style interactions like I talked about before as a way to check them.",
            "See you apply the filtering operation.",
            "These are just two pixel images.",
            "These are the filters you apply them and then you check their product.",
            "OK, that becomes your basic detection module for this motion synchrony and so why is This is why this is useful is that you can actually learn a system of the types of motion you want to be able to detect, and you can do the detection.",
            "Using a reasonably simple neural net."
        ],
        [
            "OK, So what Roland Ankush or proposed were two different models for actually being able to detect this synchrony.",
            "One was based on a model similar to the gated GBM that I just showed you, but it gated autoencoder variant and I'm sure rolling will talk about that tomorrow.",
            "And they also proposed AK means variant, which is sort of a temporal online variant.",
            "Both of these methods could be trained by gradient based optimization, and neither of them had a pooling operation built into them.",
            "Like our method, the convolutional get DBM had.",
            "So where this model really shines is fixed."
        ],
        [
            "Namely efficient, so I told you I would look at the results of all these different sort of unsupervised supervised hybrid approaches.",
            "They all do pretty well actually.",
            "You know, in terms of absolute performance qua clays, convolutional ISA tends to outperform all of them on these.",
            "These are popular datasets.",
            "KTH is this kind of toy is like the M nest of activity recognition.",
            "It's pretty small, people get can get like 100% on it now, but it's a good way of sanity checking your work.",
            "You see esports, an Hollywood, two or more reasonably sized activity recognition datasets?",
            "So even though you know qua clays work, ANAN, rolling and shores, recent work are similar in terms of performance.",
            "Their work is extremely efficient in the way that uses primarily local operations, so you can train it in two minutes on a GPU or three minutes and the other previous work was one to two hours and our work.",
            "Actually I showed you before as a couple of days to train, even with the GPU.",
            "OK, so."
        ],
        [
            "The last thing I want to talk about in terms of activity recognition before I get a little bit to gesture or fully end to end supervised methods.",
            "So just like I said for pose, the obvious thing that comes to mind is just apply a combinant to images of people and try to extract their pose for activity recognition.",
            "The obvious thing to do is take a commnet which is really good detection, make its filters 3D and apply that to activity."
        ],
        [
            "Ignition.",
            "So going back to 2010, there's a guy name she wanted G who actually looked at.",
            "Looked at doing this, and so he said, you know, the obvious thing you could train to.",
            "The comments are very efficient, they work well, you can train it frame to frame by frame and apply that states do some temporal pooling, but he said I'm going to actually create a commnet where the filters are three dimensional an so he trained."
        ],
        [
            "Oh my goodness.",
            "If it's if it's not the phone, it's the screen.",
            "OK. OK OK, so this convolutional neural net looks a lot like the standard 2D convolutional neural Nets.",
            "He actually hardwired his input so we had sort of blocks of seven temporal frames at a resolution of 60 by 40, and each of these guys were pre process to have a grayscale channel X&Y gradient channel and optical flow.",
            "And actually if you look at the modern day sort of five years later, the types of 3D comments that are being used, some of them notably coming from.",
            "Andrews sermons lab.",
            "There are also using optical flow as an input to that net, so just extracting that I apriori even though.",
            "Interestingly enough, these types of gated models can learn things like optical flow.",
            "The standard convolutional Nets, 3D convolutional Nets actually advantageous to provide them with a flow based representation, so this is sort of a standard convolutional net pipeline.",
            "3D convolutions fully connected, fully connected layers, and then some action units."
        ],
        [
            "If you compare this to five years later, as sort of a state of the art architecture that's trained on a much larger database, some of the architectural improvements that you're seeing are multiscale pathways, which have also been very important for advances in static vision tests.",
            "So they're using something called a foveated representation, where you take a very.",
            "A sort of a low dimensional representation of the entire low resolution entire scene representation, but you also have this foveated component which is just right sort of at the middle of the image you have a high resolution input, and you feed each of these through identical architectures, and then you do a sort of a late Fusion.",
            "You get pretty significant performance compared to Standard Vision feature based pipelines.",
            "And then the interesting thing here is that you only get a modest improvement compared to modern single frame combats.",
            "So if you have all this multiscale stuff going on and you kind of forget about the temporal dimension and just do two decomp Nets, they actually work fairly well in practice and there are lot quicker to train.",
            "So they also introduced this Google Sports 1 million data set with 47 classes.",
            "The problem with this data set that we've learned recently and there's a few that have done this as they release the data set as metadata.",
            "So they basically have You Tube URLs where you can grab these videos and then they have labels for them.",
            "Is YouTube videos tend to disappear overtime and we recently were talking with the authors on this and they said yeah, so the data is what's going to happen.",
            "Is the datasets it's going to kind of change overtime?",
            "That's interesting, but I guess you know one thing you could do.",
            "Is it just take a snapshot at the beginning, right and and sort of pass around a hard drive or something like that, but you know we're at the point of these datasets you it's difficult to ship them around.",
            "So 1,000,000 YouTube videos.",
            "This is particularly important for video, right?",
            "So what do you do in this situation?",
            "So anyways, that's that's kind of an aside, there's a.",
            "There's another recent data set, sort of on the same scale, not just looking at sports.",
            "Same sort of issue, a lot of the videos are starting to disappear because it's it's YouTube URLs OK."
        ],
        [
            "The last thing, yes."
        ],
        [
            "This one is not using optical flow, but there's a reference down here out of The Andrews Sisters lab.",
            "Karen simonyan.",
            "Ann Andrews Isermann they use an optical flow input, and so these these architectures are kind of neck and neck right now for this data set.",
            "This is on raw inputs color considering color input, but on all inputs.",
            "OK, so the last thing I want to talk about, I think I I'm about 510 minutes before 20 total.",
            "OK, so about 10 minutes left before questions maybe.",
            "I'm going to talk about gesture, so there's some work we just had accepted and Pammy."
        ],
        [
            "With Natalia, who's here again, she's done.",
            "The heavy lifting is lifting on this project as part of her PhD, and so this project problem is related to activity recognition, but it's in a specialized sort of limited type of activity domain, so it's limited in terms of the number of gestures, and we're looking at something called communicated gestures, so this is where someone is trying to actually communicate in.",
            "Really, it's really language actually, so so different languages use this more than others.",
            "So Italian is a perfect example.",
            "When people say words, they they basically make a gesture with it, right?",
            "So we call this communicated gestures or intentional gestures, and this the data set that we actually used was based on Italian speakers.",
            "So they'll say a word, they'll make a gesture, right.",
            "Quite dramatically as Italians do, and I'm married to an Italian, so I guess I can.",
            "I can say this and the cool thing with this data set, so it's limited to a certain type of gesture.",
            "But it's actually a multimodal data set, so it makes the modeling problem interesting and challenging.",
            "So we get with the data set, pose information.",
            "So the person who's been in front of a connect system so we can articulated skeleton, we get.",
            "We have video depth video and RGB video, and so the connect.",
            "I think I mentioned this yesterday will give you a skeleton, but it won't give you any information about the hands.",
            "The configuration of the hands, and that's because the hands typically occupied just a few.",
            "It's very difficult to track them.",
            "There's also some other work Natalia's done on detecting hands, which is at ACV last year.",
            "I'm not going to talk about it, but there's people that study hand specifically, but what we're going to do is we're going to use the skeleton to tell us where the hand is, not the layout of the fingers, but just sort of the center of the hand.",
            "And then we'll we'll take a crop of the depth video in the in the grayscale video around that location.",
            "The reason we do that is that.",
            "Some gestures can be fully fully characterized by upper body motion, like this one, so you just need to see the body to understand the gesture.",
            "Other ones you really need to look at subtle finger movements in this in this particular gestures question or just a stretch you look like you're stretching, OK, great.",
            "OK OK great.",
            "So we also get an audio channel so we have visual information.",
            "We also have auditory information."
        ],
        [
            "OK.",
            "The type of architecture that we use is sort of based on the challenges that we're facing, so the challenges that we're facing is we need to learn representations at multiple spatial and temporal scales, so the spatial comes from you that find hand versus full body information.",
            "The temporal scales are something that we build into this architecture, so we actually have pipelines that are there.",
            "Generally these are sort of temporal, scale specific identical pipelines.",
            "That operate In Sync.",
            "Alright, So what we do is we essentially sample.",
            "We call dynamic, which are essentially a collection of frames that are concatenated into a little spatial temporal volume.",
            "But the way that we sample these frames is different for these different steps, so this step the S Some people call this stride in the in the commnet literature.",
            "It's just how frequently we do temporal sampling, so we this changes the inputs to each of these streams.",
            "But these networks are essentially parameterized the same, but the values of these parameters are different, so each of these these.",
            "Scales learns to detect the gesture on its own, and then they refused.",
            "Later on in the pipeline.",
            "So let's now you know we have these multiple scale temporal scale detectors.",
            "They get fuse.",
            "Let's look at what a single scale looks, or a single stage looks like."
        ],
        [
            "OK, so it's fairly complicated because we're dealing with quite a lot of data types, so remember we're looking at depth video intensity video of both hands, so there's hand one hands too.",
            "We have full body mocap and then we have audio, so both the visual information.",
            "So the depth and the intensity video use convolutional Nets and for the mocap we just use fully connected Nets and then for the.",
            "Audio we use some standard pre processing to create.",
            "A spectrogram which role in talks about yesterday, and then we convert that to Mel frequency scale and then we have a common data 1D comment that operates or to the comment that.",
            "Is that one day we just looking at across spectrum or across time?",
            "Judy, OK, two DCOM.",
            "Net that operates on the audio pipeline."
        ],
        [
            "OK. Just some details on articulated post, so when you're dealing with motion capture human motion, it's really important to get the representation right.",
            "So in my own PhD thesis I spent a lot of time doing mocap and it took several months to really find the right representation.",
            "I remember and I wrote up his junk in my thesis about how to do this, 'cause I wanted people not to sort of feel the same pain of discovering the representation that I did.",
            "Once you get the right representation it, everything works, works nicely.",
            "Now since that time there have been a number of papers published in the.",
            "Vision literature about good representations of human motion for efficient activity recognition, and one of them came out in 2013 and we've borrowed from this one, so Christian's mission SQ and and colleagues Ascentia Lee proposed quite a high dimensional descriptor of 11 joints from the bodies.",
            "The Connect system gives us these 11 joints.",
            "We build this descriptor.",
            "You can read the details in the paper, but essentially it's containing information about the locations.",
            "The velocity is the accelerations of these joints as well as some angles relative to the internal body coordinate system and then pairwise distance information.",
            "You have to do some normalization to make everything work and the details are in."
        ],
        [
            "Paper the depth video is pretty standard.",
            "We apply local contrast normalization.",
            "We threshold on a particular depth setting, so we get essentially the the hand information threshold from background and we share parameters between the left and the right hand path."
        ],
        [
            "Yes.",
            "Now.",
            "If you just throw the data into all these paths, wired it up, you do Fusion here and you kind of hope for the best training with gradient descent it doesn't work well so.",
            "We're dealing with on the order of 12 million parameters per scale, 37 million parameters total, and the number of training gestures we have is only around 10,000.",
            "So compared to things like image net, we have fewer observations.",
            "Of course we have high dimensional observations and we have temporal information.",
            "But still it's an issue to fit these Nets.",
            "So what we do is we need to fall back on a lot of sort of architectural structuring to make everything work well.",
            "And So what we're using is very structured weight matrices that we generally relax.",
            "We pre train the individual channels.",
            "OK, so for each particular modality and of course, each of the temporal scales an when we do this careful initialization of the shared layers, we basically have.",
            "The motivation is that we have essentially a network that does independent predictions using the modalities.",
            "And it combines them in and something like an arithmetic average that we know works reasonably well, and so it doesn't learn a lot of cross modality structure right at the beginning of learning.",
            "So it learns to do a reasonable job, and we gently relax.",
            "That and the cross modality structure starts to be incorporated, and that tends to."
        ],
        [
            "To allow us to optimize the network by restricting its effective parameters early on in general and relaxing that.",
            "So the way that we actually implement this is essentially, you know, if we consider these as sort of the top or penultimate layer hidden's coming from each of the modalities.",
            "We define our shared hidden layer as sort of mood and modality specific chunks.",
            "OK, so we actually attach meaning to what the hidden, how the hidden unit should be organized in the layer where we start to merge the modalities, and so we have these blocks of modality specific chunks and we initialize their weights such that there's no.",
            "It's sort of as a as blockwise, so there's no cross modality information or transfer at the beginning of learning, and then we way we go from this.",
            "Shared hidden layer to the output is that we wire up these weights such that they compute approximately an arithmetic average of the predictions coming from the individual modalities.",
            "OK, so at the beginning of training you just have these sort of modality specific networks that predict the gesture and their combined in a reasonable reasonable but non complicated way.",
            "We then."
        ],
        [
            "Um during learning, start to relax them, but the insight here is that it actually makes sense to combine modalities that are nearby in nature first.",
            "So we start by combining the depth in the video so they're both hands, hands and visual information.",
            "Then we take the mocap and we combine that with the video and the depth, and so we call that sort of visual merger.",
            "And then we bring in the audio after awhile so we gently merge this, and this tends to work.",
            "How much better?",
            "This was all done in the sorry.",
            "I wanted to say this is what the weights look like after training.",
            "So after doing the relaxation there's really very strong Inter modality structure.",
            "But you see, here's the different hands.",
            "So after we relax it, there's a lot of cross modality structure between the hands, but there's also some cross modality structure between the hands and the motion capture, but less so with the audio.",
            "So after we relax, this is what the network actually learns in terms of this structured matrix.",
            "Yeah.",
            "Correct, yeah?",
            "And I'll show you a little picture of how that the stages work very shortly."
        ],
        [
            "This was done in the context of a competition called the Ecv Chandler and looking at People challenge and this system that Natalya built actually one out of 17 different competitors and I Natalia this running again this year child learn.",
            "OK, not for gesture.",
            "So challenge typically runs, but I guess this ran for a couple of years.",
            "Twenty 2013 and 2014."
        ],
        [
            "OK, so this is what I mentioned before and this is relevant to the last question.",
            "Is what happens when you start merging these modalities together so the different colors here are the different temporal strides memories that we have.",
            "We have networks at different temporal scales, and they're trained the same way.",
            "They just get merged very late.",
            "Now the numbers here are related to different modalities when they start training and when we merge them.",
            "OK, so these are these are in steps, it's not.",
            "Necessarily the time scale in terms of iterations or epoch's of training.",
            "So we start by training intensity and depth on its own.",
            "We then merge these guys into a video.",
            "Channel then we're also training the mocap on its own, and then the Mocap gets merged with the video stuff at at Step 6, so you see that each this is the error on the validation set, and we're showing that you know every time you do a merger of these modalities or groups of modalities, you're getting a drop in terms of the error.",
            "We have the audio being trained out here, but it doesn't actually get merged into the video stuff until quite late in the game.",
            "And then we also do, you know, these separate scales which get merged at the very end, and that seems to be the most reasonable way and effective way to."
        ],
        [
            "To merge these things.",
            "I don't have a lot of time left there is one other contribution of this paper which is looking at a way to make the networks robust to the disappearance or noise of a certain modalities.",
            "So it's a type of dropout that's which we call MoD drop.",
            "I'll let you read about that if you're interested in in the paper, but it was a.",
            "It was an interesting additional contribution of the paper, so I'm going to do.",
            "I can already see there's a question out here out there, so I think what I'm going to do."
        ],
        [
            "Summary and then I'll.",
            "I'll take questions, so at least we have 5 minutes for questions.",
            "OK, so we talked about pose estimation.",
            "We talked about why it was hard.",
            "All these different reasons.",
            "Right now it's dominated by convolutional neural Nets, but I think going into the future there's probably gonna be a merger of structured output techniques because the body is very structured as well as sort of very strong visual models.",
            "People already starting to do this in the scene parsing and seem seem labeling areas.",
            "They're combining things like MRF's and comments and they're proving very effective in those areas.",
            "Tracking I said this is really pose estimation plus a dynamical model.",
            "I talked about some work we did a number of years ago, but I really think this area is kind of underexplored and there's a lot of promise here for deep learning techniques that are very good at predicting the future and also very good at dealing with visual uncertainty.",
            "Finally, I talked about activity and gesture, which is seen more activity as of late from the deep learning community, but still offers a lot of challenges, particularly in things like multimodal data.",
            "We showed that you know, you just can throw all these modalities in there and hope for the best.",
            "Russell.",
            "Russ salakhutdinov OK. Russ Salakhutdinov is going to do a whole lecture in this summer school about multimodal learning, so you'll be able to hear his perspective on that as well so."
        ],
        [
            "Looking forward, where do we go from here?",
            "These types of datasets have much more limited amount of data available to them, so certainly I said this in the last lecture to unsupervised and weakly supervised learning is going to play a part in these domains.",
            "We want to certainly move, but beyond the classification of very short simple activities and look at structural relationships, understanding social Contacts, right?",
            "So the combination as I said before of deep learning and structural.",
            "Structured models will probably prove quite fruitful for these sorts of applications, so again, it's been awesome."
        ],
        [
            "Someone asked me before acknowledging these guys have worked with a lot within the field.",
            "Greg Maurissa researcher at Simon Fraser.",
            "These are collaborators.",
            "Natalia's Co advisor an insulin Christian Wolff and is called Julian Mill and also some researchers Matthew Core and Nicholas Nicola Tom who are at in Paris.",
            "I've been doing a lot of work with these guys recently in this area and they are responsible for a lot of the background here."
        ],
        [
            "Someone asked where is Guelph OK so Guelph is near Toronto?",
            "OK, so people don't even know how to say Guelph, so that's how you say it.",
            "It's a great.",
            "It's a beautiful city.",
            "We have a research lab.",
            "There were very close to the hot spots so we're always looking for great students and postdocs, so keep that in mind when you're when you're looking for a place to go after you're done.",
            "Whatever you're doing, OK, I'll take questions, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks a lot, yeah sure it's great to be back for another day so this talk is going to be focused more on the application end.",
                    "label": 0
                },
                {
                    "sent": "So yesterday I was talking more at the architecture level and different objective functions on a particular topic.",
                    "label": 0
                },
                {
                    "sent": "Today we're going to talk about a subject that has been explored in the deep learning community but is not as mature as detection and recognition.",
                    "label": 1
                },
                {
                    "sent": "So we're all familiar that deep learning approaches have really exceeded in areas like object recognition.",
                    "label": 1
                },
                {
                    "sent": "But in terms of understanding people and their behavior and their roles in a social context, I would say that deep learning is still at a pretty early stage, so that's what we're going to be focusing on today.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Video now is ubic wittus and if I say this, the first thing that comes to mind is probably a picture like this.",
                    "label": 0
                },
                {
                    "sent": "There are many of these things out there and our behavior is being recorded all the time, so there's many different purposes.",
                    "label": 0
                },
                {
                    "sent": "You know if we can advance understanding of people's behavior and their interactions in a social context.",
                    "label": 0
                },
                {
                    "sent": "Advances in learning techniques are obviously very fundamental to rendering this kind of data useful.",
                    "label": 0
                },
                {
                    "sent": "That's not the only type of data that people are exploiting.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It contains humans, so many of you are aware of sports teams that are hiring their own sort of data science groups.",
                    "label": 0
                },
                {
                    "sent": "Now most of the professional NBA teams have such divisions where they're taking the hours and hours of video data from the recorded games and then doing analysis to try to improve outcomes of their players.",
                    "label": 0
                },
                {
                    "sent": "There's other examples, of course.",
                    "label": 0
                },
                {
                    "sent": "Egocentric video is an area of interest in the vision community.",
                    "label": 0
                },
                {
                    "sent": "Devices like Google glasses.",
                    "label": 0
                },
                {
                    "sent": "People are recording their lives now.",
                    "label": 0
                },
                {
                    "sent": "And then one makes sense of that after.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So really, humans are the dominant subject in nearly all video, so if you go to YouTube and you look at collection of video, alot of it contains humans and advancing the way that we understand humans and the interactions they have in video is of fundamental importance to a number of different domains.",
                    "label": 1
                },
                {
                    "sent": "So I have a few of them listed up here.",
                    "label": 0
                },
                {
                    "sent": "Some of them might not have thought of our areas about say design of public spaces, so we can understand how flows of people are moving through.",
                    "label": 0
                },
                {
                    "sent": "Spaces we can design them better.",
                    "label": 0
                },
                {
                    "sent": "Health care is an obvious one.",
                    "label": 0
                },
                {
                    "sent": "People, a lot of people are working in improving outcomes for the aged individuals through the use of understanding people through video.",
                    "label": 0
                },
                {
                    "sent": "And there's ways that we can now augment peoples interaction with the world.",
                    "label": 1
                },
                {
                    "sent": "So who's heard of the Oculus Rift?",
                    "label": 0
                },
                {
                    "sent": "Not as many people as I thought.",
                    "label": 0
                },
                {
                    "sent": "OK, people are shaping up their hands.",
                    "label": 0
                },
                {
                    "sent": "You know, there's a device that you strap on your face and you see the the world through a computer vision system so we can augment peoples reality.",
                    "label": 0
                },
                {
                    "sent": "And there are numerous applications of that.",
                    "label": 0
                },
                {
                    "sent": "Finally, in terms of improving the interactions that people have with computers and machines, particularly in robotics, understanding people in video is of fundamental importance.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so what's cool about studying humans?",
                    "label": 0
                },
                {
                    "sent": "Well, understanding humans and their behavior is fundamentally different than studying objects.",
                    "label": 0
                },
                {
                    "sent": "Humans are very complex entities.",
                    "label": 0
                },
                {
                    "sent": "They are articulated.",
                    "label": 0
                },
                {
                    "sent": "So they're not treated as rigid.",
                    "label": 0
                },
                {
                    "sent": "Their visual appearance appearance is much more complicated than rigid objects, and they also have properties of cognition and self reflection and social awareness that makes their behavior different than the behavior of objects.",
                    "label": 0
                },
                {
                    "sent": "So human behavior can be governed by very complex interactions, and we're really just getting at sort of the basics of starting to understand that.",
                    "label": 0
                },
                {
                    "sent": "So basically, our long-term goals are too.",
                    "label": 0
                },
                {
                    "sent": "Sort of begin to move towards the complexity of understanding such dependencies and scenes like the one I have up here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in vision has the field of vision has approached understanding human activity and behavior for a number of years and the applications so far have been enabled really amazing.",
                    "label": 1
                },
                {
                    "sent": "Amazing changes in our lives so we see gesture recognition, face detection rolled out to consumer devices like cell phones and in the last few years machine learning has really been at the forefront of these developments.",
                    "label": 1
                },
                {
                    "sent": "They have machine learning, has capitalized on vast amounts of data, and the inherent variability of human appearance and behavior in these data sources have been very important.",
                    "label": 1
                },
                {
                    "sent": "The emergence of new computational paradigms like GPU's have also been important for this very high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "And deep learning has now emerged as a major force in vision.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course this is not to say that vision has machine learning or deep learning has solved vision.",
                    "label": 0
                },
                {
                    "sent": "There are many challenges out there, so understanding for example person to person or person to object interactions, understanding long running dynamical behavior and video and very large scale variation of parents are still out of reach right now.",
                    "label": 1
                },
                {
                    "sent": "But these are problems that are being tackled by the learning community and they should open up new applications.",
                    "label": 0
                },
                {
                    "sent": "So this is really just to give a motivation of what's out there.",
                    "label": 0
                },
                {
                    "sent": "I'm going to focus today on because this is a summer school I'm focusing on.",
                    "label": 0
                },
                {
                    "sent": "Achievements in the field, but this is really to motivate you and say that vision has not been solved yet, even though we're seeing these amazing applications, there's still a lot of room for improvement and new applications to tackle.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the intro I'd like to now just sort of divide this this lecture up like yesterday into three sections.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to be covering all of human activity and behavior understanding.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be of course missing important areas like emotion, recognition, facial identity recognition, and so forth.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be focusing mainly on the full body as opposed to just faces, but I will tackle three areas where it in which deep learning has played an important role.",
                    "label": 0
                },
                {
                    "sent": "Pose, estimation, tracking and.",
                    "label": 0
                },
                {
                    "sent": "Activity and gesture understanding.",
                    "label": 0
                },
                {
                    "sent": "So in all of these, the focus is on seeing people and understanding them.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first area I'll talk I'll tackle is pose.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Estimation, So what makes pose estimation challenging is that you're trying to essentially estimate the locations of all the joints or a subset of joints of an individual from video or still images.",
                    "label": 0
                },
                {
                    "sent": "And there is extreme variability in a person's articulation.",
                    "label": 1
                },
                {
                    "sent": "I mentioned that before the difference between humans and rigid objects is that humans can configure their bodies and all kinds of weird and wonderful ways.",
                    "label": 0
                },
                {
                    "sent": "Probably those that practice yoga or better at doing that.",
                    "label": 0
                },
                {
                    "sent": "But I guess in many of these applications and the.",
                    "label": 0
                },
                {
                    "sent": "Data that we're considering a lot of the joints might be barely visible, so they might be truncated because they're close to image borders.",
                    "label": 1
                },
                {
                    "sent": "They might be occluded by other objects or other people.",
                    "label": 0
                },
                {
                    "sent": "Self occlusions is also a major problem, so you know if I'm standing like this and you're facing me and my arms behind my back.",
                    "label": 0
                },
                {
                    "sent": "I'm blocking my own body so we can.",
                    "label": 0
                },
                {
                    "sent": "You can get around that sometimes with multiple cameras, but when we're dealing with a single camera, as as many applications, we need to use the models that can reason about things that we can't necessarily perceive through data.",
                    "label": 0
                },
                {
                    "sent": "Now, contributing to the fact that many joints are barely visible also is our issues like resolution.",
                    "label": 1
                },
                {
                    "sent": "So if we're dealing with say, images collected collected from YouTube frames, we may have the hands in some of these videos only occupying a very small number of pixels in each of these frames.",
                    "label": 0
                },
                {
                    "sent": "There's a series of models from the vision literature called parts are parts models, one particular deformable parts model has been very popular, particularly in addressing the variability in articulations.",
                    "label": 0
                },
                {
                    "sent": "These are very strong structured models that know a lot about the relationships between the different parts of the human body.",
                    "label": 0
                },
                {
                    "sent": "These these systems are very effective in addressing structural structural relationships among the parts, but they're less effective in feature learning.",
                    "label": 0
                },
                {
                    "sent": "They typically use very local sort of low level features, and people are starting to combine these sorts of models with deep learning techniques, and this is another interesting area of future research.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we're going to use deep neural networks for precise localization of body parts, the first thing comes to mind is OK.",
                    "label": 1
                },
                {
                    "sent": "Deep learning in particular comments which have been very popular in the last few years.",
                    "label": 0
                },
                {
                    "sent": "I've been very effective at recognition test.",
                    "label": 0
                },
                {
                    "sent": "Why don't just roll them out and apply them to direct regression of body posts?",
                    "label": 1
                },
                {
                    "sent": "So put an image in regress to a dementia relatively low dimensional vector representing the two year 3.",
                    "label": 1
                },
                {
                    "sent": "The positions of body parts.",
                    "label": 0
                },
                {
                    "sent": "Now there's a future.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Change is doing this, so if you take the standard com.",
                    "label": 0
                },
                {
                    "sent": "Net Alex.",
                    "label": 0
                },
                {
                    "sent": "Net or inception or whatever off this off the shelf, these typically use a lot of pooling modules in them and pooling of course destroys precise spatial information and precise spatial information of this is of fundamental importance for localising body parts, so this makes this isn't this is an issue with just applying these Nets off the shelf.",
                    "label": 0
                },
                {
                    "sent": "One of the other more subtle issues is that the mapping from input space two poses is highly nonlinear.",
                    "label": 1
                },
                {
                    "sent": "And it's multimodal, so these these sort of off the shelf comments that are good for recognition, aren't good at making multimodal predictions.",
                    "label": 0
                },
                {
                    "sent": "So another way of looking at this is my hands behind my back.",
                    "label": 0
                },
                {
                    "sent": "And you are reasoning about where you know the the location of the my wrist joint is.",
                    "label": 0
                },
                {
                    "sent": "And maybe my fingers as well.",
                    "label": 0
                },
                {
                    "sent": "There are many possibilities of where those could be alright, so we'd like to use systems that can allow for uncertainty in post.",
                    "label": 1
                },
                {
                    "sent": "And finally the valid poses or the space of all, opposes.",
                    "label": 0
                },
                {
                    "sent": "Presents a much lower dimensional manifold in this high dimensional space of potential configurations so you know we might be regressing to a vector of same dimension 20 which represents 10 body parts and their 2D locations.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the actual valid configurations, you can't put your arms through your your body, or at least I can't.",
                    "label": 0
                },
                {
                    "sent": "You know you can't put your your one leg through the other leg and so forth.",
                    "label": 0
                },
                {
                    "sent": "The space of potential configurations is actually very small, so in some sense it seems a little bit.",
                    "label": 0
                },
                {
                    "sent": "Troublesome to make a discriminative network map to a space in which most of the configurations are really unallowable once.",
                    "label": 0
                },
                {
                    "sent": "So what we might want to do is restrict the neural Nets output to a much smaller class of potential outputs, where were the majority of them, are actually valid instead of invalid.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the approach that we take in some work we did last year.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the first work in pose estimation using deep learning.",
                    "label": 1
                },
                {
                    "sent": "I'd like to highlight this was done.",
                    "label": 0
                },
                {
                    "sent": "The person doing most of the heavy lifting on this was a postdoc at NYU Arjun Jain and what we were doing here is that instead of taking one big continent, taking an image in and the training it to map to pose, we actually used a series of part based combats.",
                    "label": 0
                },
                {
                    "sent": "So we had one commnet per body part and they would look at as much smaller region of the image.",
                    "label": 0
                },
                {
                    "sent": "So this allows us to use obviously much smaller comments that were applied in a sliding window fashion, so the comment would look at a Patch of an image and it would use a standard architecture using Reluz and Max pooling and predict a single binary output which was the presence or absence of that joint.",
                    "label": 1
                },
                {
                    "sent": "So we would have our say, risk detector comment.",
                    "label": 0
                },
                {
                    "sent": "We would have an elbow detector, a shoulder detector commnet and this allowed us to basically apply calmness to a problem that they were good at.",
                    "label": 0
                },
                {
                    "sent": "Those detection or recognition, but that use them on a problem.",
                    "label": 0
                },
                {
                    "sent": "A different kind of problem.",
                    "label": 0
                },
                {
                    "sent": "Now the output of this part detector.",
                    "label": 0
                },
                {
                    "sent": "These part detectors comments was quite noisy so ANAN.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that having independent part based Comnets wasn't able to allow us to enforce any consistency among the poses.",
                    "label": 0
                },
                {
                    "sent": "So the the wrist and the shoulder obviously have a relationship and so did the rest of the elbow and so forth.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we had a second phase pose consistency model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we use so you can see coming out, you know, say we're trying to detect risks.",
                    "label": 0
                },
                {
                    "sent": "For this image we might have some hits on the output of the comment.",
                    "label": 0
                },
                {
                    "sent": "This is a comment that again trained to produce to predict the risk location in 2D.",
                    "label": 0
                },
                {
                    "sent": "It's applied again using a sliding window, so we get a risk prediction at every pixel.",
                    "label": 0
                },
                {
                    "sent": "That's the probability of the risk being in that location, and you see it firing.",
                    "label": 0
                },
                {
                    "sent": "See, it's trying to find the left wrist it's firing on both the right wrist and the left left risk, and it's firing in other places as well, so it's quite noisy if we apply this spatial model that uses.",
                    "label": 0
                },
                {
                    "sent": "Relationships between different body parts.",
                    "label": 0
                },
                {
                    "sent": "Then we get a much cleaner, much more cleaned up map and I'll tell you a little bit more detail about how that works.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we use essentially a graphical model in which we capitalize on probabilities, conditional probabilities that can be estimated from the training data.",
                    "label": 0
                },
                {
                    "sent": "So based on all the labeled training data of where the how the body is configured, we can estimate distribution, say where the shoulder is relative to the elbow, where the elbow is relative the wrist, and so forth, and so forth, and so once we've made a bottom up prediction with this commnet, we can do some cleaning up using this spatial model.",
                    "label": 0
                },
                {
                    "sent": "And these these priors are easy to fit.",
                    "label": 0
                },
                {
                    "sent": "There's just essentially counting and.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They look essentially something like this so.",
                    "label": 0
                },
                {
                    "sent": "For a given the full set of distributions we can go and construct a distribution save for a particular body part using the movies.",
                    "label": 1
                },
                {
                    "sent": "Here the first term.",
                    "label": 0
                },
                {
                    "sent": "This is essentially the bottom up information coming from the confidence and then this second term is essentially a product over all of its neighbors in this graph.",
                    "label": 0
                },
                {
                    "sent": "So for the for the elbow, the neighbors would be the wrist in the shoulder.",
                    "label": 1
                },
                {
                    "sent": "For the shoulder, the neighbor would be the.",
                    "label": 0
                },
                {
                    "sent": "The face and the and the and the elbow.",
                    "label": 0
                },
                {
                    "sent": "So these are the distribute the distributions look like.",
                    "label": 0
                },
                {
                    "sent": "So assuming that the face is at the position 0, which is the center of this image.",
                    "label": 0
                },
                {
                    "sent": "The conditional distribution for the shoulders located here.",
                    "label": 0
                },
                {
                    "sent": "And again this is just for one of the shoulders.",
                    "label": 0
                },
                {
                    "sent": "So here is the left shoulder we use basically all the data.",
                    "label": 0
                },
                {
                    "sent": "We just estimated a single shoulder conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "We didn't do one for the right shoulder and one for the left.",
                    "label": 0
                },
                {
                    "sent": "We just flip them.",
                    "label": 0
                },
                {
                    "sent": "And then we had a conditional distribution for the elbow given the shoulder being at zero and one for the wrist as well.",
                    "label": 0
                },
                {
                    "sent": "Hey, so there's more variability in terms of the wrist location as you would expect, and so if we look at this in so then in the log probability domain we end up having safer the shoulder joint, a term that's coming from the convent.",
                    "label": 0
                },
                {
                    "sent": "So this is the part of the shoulder part detector.",
                    "label": 0
                },
                {
                    "sent": "It puts some probability of where it thinks the shoulder is and then we have terms coming from the face and then terms coming from the shoulder.",
                    "label": 0
                },
                {
                    "sent": "And this takes into the account where we've estimated the the face location in this and the elbow location to be.",
                    "label": 0
                },
                {
                    "sent": "We also introduced a sensually awaiting term which will allow us to sort of up, wait the bottom up prediction come from the from the commnet, but essentially we set that to 1:00 so we had a balance between the prior terms and the bottom up information.",
                    "label": 0
                },
                {
                    "sent": "The visual information.",
                    "label": 0
                },
                {
                    "sent": "Is there a question at the back?",
                    "label": 0
                },
                {
                    "sent": "In the training data, exactly so these were just estimated essentially by counting the what we saw in the training data.",
                    "label": 0
                },
                {
                    "sent": "Yep, Yep.",
                    "label": 0
                },
                {
                    "sent": "Changing yeah, so I guess you could say that some this.",
                    "label": 0
                },
                {
                    "sent": "This bears some similarity to those these models in terms of the the spatial prior.",
                    "label": 0
                },
                {
                    "sent": "The part based prior.",
                    "label": 0
                },
                {
                    "sent": "And really what we're changing is of course is this first term which is the bottom up information.",
                    "label": 0
                },
                {
                    "sent": "Instead of using some.",
                    "label": 0
                },
                {
                    "sent": "Some features were using essentially the convolutional neural net.",
                    "label": 0
                },
                {
                    "sent": "So you're right, and it's certainly very similarities.",
                    "label": 0
                },
                {
                    "sent": "Those approaches and later on I'll talk about a variant of this model which doesn't just fit these.",
                    "label": 0
                },
                {
                    "sent": "It doesn't assume that you know the relationships among the body parts, but you actually learn this.",
                    "label": 0
                },
                {
                    "sent": "This graph over the body in terms of what joints are neighbors of other joints.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so for the face we found that having a basically adding information from the shoulder in terms of the face location actually made the model worse.",
                    "label": 1
                },
                {
                    "sent": "We think that's just basically because the network was already pretty good at estimating faces.",
                    "label": 0
                },
                {
                    "sent": "Faces up typically occupy much larger number of pixels in the image, and people typically don't wear clothing over their face and include it, so we have we're able to estimate the face pretty well and incorporating information noisy information about the shoulder just makes the prediction worse.",
                    "label": 0
                },
                {
                    "sent": "So what we did for the face is we used essentially a global position prior, so we just looked at the location of the face and all of the training data and use that instead of incorporating shoulder information for the face.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there is sort of at the time we were developing this, there are researchers at Google who are developing a competing model and they called the depots and this came out slightly after so our work was that I clear 2014 this work came out, CVPR 2014, but they're essentially developed at the same time and they took the approach of doing exactly what I said you shouldn't do, which is taking an image and then just regressing it to oppose vector.",
                    "label": 0
                },
                {
                    "sent": "Now there's some some intelligent tricks that they used to make this thing actually work.",
                    "label": 0
                },
                {
                    "sent": "And that's why they were able to get away with this so that the two main things that they looked at doing is that they normalized both the outputs and the inputs with respect to the body.",
                    "label": 0
                },
                {
                    "sent": "So the joints that it was actually predicting our joints in the normalized body coordinates and also the images that are being shown to the network or essentially cropped around the person, so they're using essentially an off the shelf person detector to 1st get a rough idea of where the person is.",
                    "label": 0
                },
                {
                    "sent": "And then feed that as input to the confident.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the step one to get such a system working.",
                    "label": 0
                },
                {
                    "sent": "The the architecture that they used was pretty standard.",
                    "label": 0
                },
                {
                    "sent": "This is the Alex Net network from Image Net 2012 that most people are now familiar with.",
                    "label": 0
                },
                {
                    "sent": "So the the second trick that they used.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was to roll out a cascade of posed Regressors.",
                    "label": 0
                },
                {
                    "sent": "So this is actually kind of a recursive mechanism similar to what's used in language as well, so they take an image right and do exactly what we just talked about.",
                    "label": 0
                },
                {
                    "sent": "Say we're doing a risk detector.",
                    "label": 0
                },
                {
                    "sent": "We apply this image of the body to the combat outcomes of prediction of all the locations and say we have the location of the risks and we want to refine that.",
                    "label": 0
                },
                {
                    "sent": "Then we go and we take the current prediction we crop.",
                    "label": 0
                },
                {
                    "sent": "A region around that, and then we feed it into the next stage of the continent to produce a more precise localization.",
                    "label": 0
                },
                {
                    "sent": "And what you're doing when you do.",
                    "label": 0
                },
                {
                    "sent": "When you apply this to subsequent stages is you don't predict the absolute location, but you produce the change or the offset and location to your original prediction.",
                    "label": 0
                },
                {
                    "sent": "So you're just learning to refine your prediction prediction at every successive stage and each successive stages essentially seeing a higher resolution version of that image at that particular joint you're interested in.",
                    "label": 0
                },
                {
                    "sent": "Now all of the joint regression networks are essentially sharing this.",
                    "label": 0
                },
                {
                    "sent": "The parameters in the lower stages, so you get basically implicit relationships among the body part, but they have no explicit model that's coding in any priors about how joints are related to one another, so they don't have this sort of structural term.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's how their method works, so these are well look about will look a little bit later at how these these methods stack up against one another.",
                    "label": 0
                },
                {
                    "sent": "I wanted to 1st talk a little bit about, you know if you're getting started in this area, pose estimation, what kind of datasets should you turn to?",
                    "label": 0
                },
                {
                    "sent": "And I mentioned that yesterday when I was talking about similarity learning, embeddings of pose when I started doing this back in 2010 that there weren't a lot of really great data sets out there.",
                    "label": 0
                },
                {
                    "sent": "Since that time there's been a tremendous number of datasets become available for 2D.",
                    "label": 0
                },
                {
                    "sent": "Pose estimation, so the ones that were used in the genital work as well as Szegedy and Anto chef, they evaluate on two modern datasets.",
                    "label": 1
                },
                {
                    "sent": "One is flick for frames label in cinema.",
                    "label": 1
                },
                {
                    "sent": "In the order of thousands of labeled frames, antennen upper body joints and they also evaluate on lead sports.",
                    "label": 0
                },
                {
                    "sent": "Lead Sports is a larger data set, but it's restricted to the sports domain and it has 14 full body joints labeled now there's.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mentioned this yesterday the the MPI human pose datasets.",
                    "label": 0
                },
                {
                    "sent": "It's actually much larger than both of these datasets, so this is another great place to start.",
                    "label": 0
                },
                {
                    "sent": "But when this wasn't available, when those when these these techniques such as depots and Arjun James NRS work came out.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the the metrics for pose estimation are typically focused on on two approaches.",
                    "label": 0
                },
                {
                    "sent": "So either we look at something called the percentage of correct parts which is just measuring the detection rate of limbs and what we mean by detection rate is.",
                    "label": 1
                },
                {
                    "sent": "A limb is considered detected if the distance between the two, the true.",
                    "label": 0
                },
                {
                    "sent": "The two predicted joint locations and the true joint locations is smaller than half of the limb length.",
                    "label": 0
                },
                {
                    "sent": "OK, so you look at the error.",
                    "label": 0
                },
                {
                    "sent": "And then you look at that sort of relative to the full limb.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that this tends to penalize shorter limbs, So what people have been doing more recently is using something called protect percentage detected joints, which is similar.",
                    "label": 0
                },
                {
                    "sent": "It's just normalized against the torso or the the person's body, so this is again just the distance between the Detroit detected in true joints.",
                    "label": 1
                },
                {
                    "sent": "Typically what people do is they look at certain fractions of the torso width.",
                    "label": 0
                },
                {
                    "sent": "So let's say how well are we doing when we consider a detection to be.",
                    "label": 0
                },
                {
                    "sent": "10% of that or so, and then 20% or 30% or so to be accurate detection.",
                    "label": 0
                },
                {
                    "sent": "So you generally generate these curves to show how your how robust your technique is.",
                    "label": 0
                },
                {
                    "sent": "As you become more and more sort of particular about your evaluation metric.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in terms of the state of the art we've talked about these different techniques, so there's the technique using part detectors.",
                    "label": 0
                },
                {
                    "sent": "There is a technique depots that just uses a big convolutional net and regresses to pose, and then there's also this enhanced version which is being shown at ours and these results come from that paper which is enhanced version of the part detector I described before.",
                    "label": 0
                },
                {
                    "sent": "So what it's using is essentially, you know it does 2 enhancements.",
                    "label": 0
                },
                {
                    "sent": "One is really an implementation detail.",
                    "label": 0
                },
                {
                    "sent": "It efficiently implements the sliding.",
                    "label": 0
                },
                {
                    "sent": "Window part detection through using convolution.",
                    "label": 0
                },
                {
                    "sent": "So it's actually it's it's borrowing something from this work from the NYU lab called over feet and it says you know if you're going to apply a sliding window detector.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of redundancy in applying certain spatial redundancy and applying these convolutions to every single location, because these sliding windows are obviously overlapping, so it made the method is much quicker, but they also make some modifications to the spatial prior component.",
                    "label": 0
                },
                {
                    "sent": "Like I mentioned before, when we were working on this.",
                    "label": 0
                },
                {
                    "sent": "Last year we had coded alot of in prior information about how the body was wired up essentially right.",
                    "label": 0
                },
                {
                    "sent": "We knew that the risk and the elbow were adjacent and that was adjacent to the shoulder and so forth.",
                    "label": 0
                },
                {
                    "sent": "They define a more general structured model of Markov random field and they learn the relationships among the body parts.",
                    "label": 0
                },
                {
                    "sent": "So that tends to work much better than the existing techniques.",
                    "label": 0
                },
                {
                    "sent": "So in terms of you know what's the best out there?",
                    "label": 0
                },
                {
                    "sent": "This work they published at NIPS 2014.",
                    "label": 0
                },
                {
                    "sent": "Arjun Jain again is the lead author that seems to be outperforming.",
                    "label": 0
                },
                {
                    "sent": "The depots work an the earlier work on both the flick datasets and Leeds and Leeds Sports.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that is that sort of state of the art right now in pose estimation.",
                    "label": 0
                },
                {
                    "sent": "Next I'm going to be talking about tracking.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One reason why I think tracking is particularly interesting is that it seems to be unexplored to date in the deep learning literature.",
                    "label": 0
                },
                {
                    "sent": "So we had some work at a number of years ago, which we'll talk about shortly, and this work really combines two different things that have seen a lot of developments in the last five years in deep learning.",
                    "label": 0
                },
                {
                    "sent": "So the two different things.",
                    "label": 0
                },
                {
                    "sent": "One is dynamical models, so this is a model that understands the way the body moves and the other thing.",
                    "label": 0
                },
                {
                    "sent": "Is visual recognition model or visual model so?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, the problem here is like pose estimation, but there's a time element, so at every you now have a video instead of still images, and every time you're trying to essentially in the 3D set up, put a 3D model of the body on that frame, fit it to that frame so the time element comes in.",
                    "label": 1
                },
                {
                    "sent": "Now that these these these friend there's obviously relationships between them.",
                    "label": 0
                },
                {
                    "sent": "So if you can understand something about the way that the body moves and you can learn this from data, then you should have a better job predicting.",
                    "label": 0
                },
                {
                    "sent": "The change of the body from one frame to the next.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to say to you that you know motion capture data, the type that's used to fit these prior models of human motion have often been used to assess the performance of different dynamical model, so people you know recurrent neural networks right now are very hot.",
                    "label": 0
                },
                {
                    "sent": "Often people will use motion capture data as sort of a benchmark in a predictive setup and people will say no.",
                    "label": 0
                },
                {
                    "sent": "Our dynamical systems dynamical model is better than the previous work because it's able to predict one frame in the future much better.",
                    "label": 0
                },
                {
                    "sent": "But in terms of sort of the the applicability of a generative model of motion that it doesn't see a whole lot of use.",
                    "label": 0
                },
                {
                    "sent": "But this is actually one area where a generative model of motion is very useful as a prior in this tracking setup.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially, this is what I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "You have a model that governs the motion of the body, so you have some latent variables and those represent the pose and the dynamics.",
                    "label": 0
                },
                {
                    "sent": "And then at each time frame, this generative model can actually generate some human motion as conditioned on what happened in the past, and the ANAN information flows among the different hidden units.",
                    "label": 0
                },
                {
                    "sent": "So we use a particular type of model derived from restricted Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about the details of that shortly, but this could be something like a recurrent neural network.",
                    "label": 0
                },
                {
                    "sent": "This structures is quite similar.",
                    "label": 0
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically we have a visual model, Anna dynamical model combined and this makes tracking a very interesting problem and actually not a lot of I haven't seen really any modern works in this in this area.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So up into the point sort of 2005 2006 when deep learning really the name, got coin and people became very excited about it.",
                    "label": 0
                },
                {
                    "sent": "People were using dynamical models as priors for tracking, but the previous models were all activity specific.",
                    "label": 0
                },
                {
                    "sent": "And ideally we would like to have are these dynamical models that can cope with different types of activities.",
                    "label": 0
                },
                {
                    "sent": "When I say activity, I mean things like walking and running and jumping and boxing and so forth.",
                    "label": 0
                },
                {
                    "sent": "The types of dynamical models like RN ends that we can build from deep learning within the deep learning community are actually quite capable of capturing many different activities, and some of the work I did in the past, we were exploring these sort of and generative setting.",
                    "label": 0
                },
                {
                    "sent": "Can you generate all kinds of different types of motions?",
                    "label": 0
                },
                {
                    "sent": "But here it can be used for.",
                    "label": 0
                },
                {
                    "sent": "Recognition of.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These activities as well.",
                    "label": 0
                },
                {
                    "sent": "So we introduced in this paper something called the implicit mixture of of conditional restricted Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a long name and I'll go through each of the pieces of this shortly, but its main advantages is that these sorts of you can call them connectionist or deep deep deep learning style models are very easy or capable to train using quite larger datasets in the work that were considered in the past.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting thing about this particular model.",
                    "label": 0
                },
                {
                    "sent": "This implicit mixture model is that it can be trained, supervised.",
                    "label": 0
                },
                {
                    "sent": "If you know activities, so the model has some some special hidden units that can assign to activities.",
                    "label": 0
                },
                {
                    "sent": "So if you're know you're only dealing with walking and running and jumping and blocks and label those activities, or if you know you're just going to have a whole lot of different activities and you don't have labels for them, a priore, you can allow it to assign these discrete activity units on its own.",
                    "label": 0
                },
                {
                    "sent": "However, it sees sees fit.",
                    "label": 0
                },
                {
                    "sent": "So you actually get two things from this model.",
                    "label": 0
                },
                {
                    "sent": "You get a posterior where we're showing the the the maximum over the posterior of the pose.",
                    "label": 0
                },
                {
                    "sent": "So this is what you're interested in for tracking.",
                    "label": 0
                },
                {
                    "sent": "But the other thing you get for this model, which is a useful byproduct, are the states of these different activities.",
                    "label": 0
                },
                {
                    "sent": "So this is showing one that you haven't trained at supervised.",
                    "label": 0
                },
                {
                    "sent": "You train it unsupervised, and it's it's assigned what we've been calling these movings or discrete units of activity, and these can be interpreted.",
                    "label": 0
                },
                {
                    "sent": "I I have a.",
                    "label": 0
                },
                {
                    "sent": "There's a postdoc that I'm working with who's trying to understand the social dynamics of fruit flies, so this is sort of the MNIST of the biology world.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So he's actually capturing interactions between different families of fruit flies in Petri dishes with the vision system overhead and trying to assign sort of different moving so that the types of behaviors that there are undergoing.",
                    "label": 0
                },
                {
                    "sent": "So there's there's cool things you can do from fruit flies you can sort of knockout their their olfactory system and see if that has an effect of their social interactions.",
                    "label": 0
                },
                {
                    "sent": "You can introduce no different variants, different families, genetic variants of these flies, and see how that affects their social interactions and so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is so.",
                    "label": 0
                },
                {
                    "sent": "It's it's interesting.",
                    "label": 0
                },
                {
                    "sent": "A very interesting area of research in my opinion.",
                    "label": 0
                },
                {
                    "sent": "OK, so back to human motion, not fruit fly motion.",
                    "label": 0
                },
                {
                    "sent": "We want to track.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So sort of a high level view of how tracking works in this setup is the input is obviously the frames of video, so we're going to call these why these are the image features and what we are ultimately interested in getting to.",
                    "label": 0
                },
                {
                    "sent": "Is this 3D body pose, so we're going to call that X. OK, so during training we have some information.",
                    "label": 0
                },
                {
                    "sent": "We have mapped video in motion capture, so this is observed during learning, but at Test time or tracking time, we're trying to recover this and then we have some latent variables.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned this is based on an RBM.",
                    "label": 0
                },
                {
                    "sent": "So as you're familiar now, given the previous lectures, the PBM has a bunch of hidden units, binary hidden units, but we have augmented this model with some additional hidden units which represent activities, and I said we either know about these activities, or we allow the model to assign them as it sees fit, will call these discrete ones Q and the standard DBM hidden.",
                    "label": 0
                },
                {
                    "sent": "Zed you've seen them as H before, but we're going to call them zed.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doubleview I know.",
                    "label": 0
                },
                {
                    "sent": "I think it was Aaron, I think discussed our BMS already.",
                    "label": 0
                },
                {
                    "sent": "So I don't need to go through these.",
                    "label": 0
                },
                {
                    "sent": "Probably you know you have basically every visible is connected to all of the hidden's.",
                    "label": 0
                },
                {
                    "sent": "All of the hidden's or connect or each hidden is connected to all of the visibles there, efficient to do inference in, and can be trained with well known approximate learning.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Times Now.",
                    "label": 0
                },
                {
                    "sent": "You've seen this picture before.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to do is I'm going to represent the model by this simplified picture over here, so I'm going to collapse all of the observations into a single random variable, which I'm calling X and then collapse all the hidden's into this hidden variable zed.",
                    "label": 0
                },
                {
                    "sent": "And that's over a vector of binary observations, and this is the picture I'm going to use for the pose of the person, and this is the picture that I'm going to use for the binary latent variables.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's just the PBM you guys have seen this.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Already in the summer school.",
                    "label": 0
                },
                {
                    "sent": "What I'm now going to do is I'm going to introduce these additional action units to allow the model to kind of switch between different activities, So what?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to do is I'm going to 1st modify the model so it can capture temporal effects, so I've introduced a connection from the past.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what makes it a conditional RBM.",
                    "label": 0
                },
                {
                    "sent": "We've added the history.",
                    "label": 0
                },
                {
                    "sent": "I'm showing HT.",
                    "label": 0
                },
                {
                    "sent": "This picture is showing one frame of the past, but HT means it could actually represent a number of frames of the past, and in practice usually a few frames are used.",
                    "label": 0
                },
                {
                    "sent": "So here's the history of the motion.",
                    "label": 0
                },
                {
                    "sent": "Here's the current observation, and we're going to condition inference and reconstruction.",
                    "label": 0
                },
                {
                    "sent": "On the history, that's what makes it a conditional RBM.",
                    "label": 0
                },
                {
                    "sent": "That sort of the point I want to make here is, it really doesn't change inference, and learning when you do this, you can still use contrastive divergent or whatever your favorite learning approximate learning technique is question.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we're doing is we're so it think of you.",
                    "label": 0
                },
                {
                    "sent": "Think of just a standard DBM.",
                    "label": 0
                },
                {
                    "sent": "OK, so going back.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here before we've introduced these different connections we have, we want to model pose, so the the features here are binary latent variables and if we were to just train this model on body pose, it would capture something about the relationships among the joints.",
                    "label": 0
                },
                {
                    "sent": "So their features about body pose about how the body is configured.",
                    "label": 0
                },
                {
                    "sent": "When we add in connections from sorry went the wrong way.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add in connections from the past, then those features.",
                    "label": 0
                },
                {
                    "sent": "Arts also start capturing things about the way that the pose changes through time, so it starts to connect.",
                    "label": 0
                },
                {
                    "sent": "Learn something about dynamics.",
                    "label": 0
                },
                {
                    "sent": "OK, so yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so there's a couple of ways that this differs from the HMM models, so one is that you're not restricting to a single hidden state, right?",
                    "label": 0
                },
                {
                    "sent": "So we have multiple binary latent variables, which makes it more powerful representation than HMM.",
                    "label": 0
                },
                {
                    "sent": "The the other thing is that the thing that shares with Hmm's is that you can do exact inference easily.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The thing is, we've introduced these additional connections which the HMM doesn't have in it.",
                    "label": 0
                },
                {
                    "sent": "So in terms of the way that we train this, it's kind of.",
                    "label": 0
                },
                {
                    "sent": "It's a bit sneaky in terms of adding these additional connections because you can.",
                    "label": 0
                },
                {
                    "sent": "Essentially, when you train the model is getting a bit detailed into the practical implementation, but you can take Windows.",
                    "label": 0
                },
                {
                    "sent": "So you can take a frame and its previous say 5 frames of history.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne Anne Anne.",
                    "label": 0
                },
                {
                    "sent": "Shuffle those up into mini batch and just do training on those you don't have these long term dependencies that you have to deal with it training time.",
                    "label": 0
                },
                {
                    "sent": "However if you connected these hidden units through time like the HMM is doing, that makes inference more challenging.",
                    "label": 0
                },
                {
                    "sent": "So the HMM is able to cope with that because it has a very simple hidden state.",
                    "label": 0
                },
                {
                    "sent": "But as soon as you take something with complicated hidden state and you connect it through time and you're trying to influence over it.",
                    "label": 0
                },
                {
                    "sent": "That becomes a little bit tricky, trickier.",
                    "label": 0
                },
                {
                    "sent": "There are variants of this model, so ileus discovered it a bunch of work on temporal variance of restricted Boltzmann machines, where he actually did wire up the hidden units through time, like in is in a recurrent neural net, but he used approximate inference schemes.",
                    "label": 0
                },
                {
                    "sent": "He's kind of ignored, sort of.",
                    "label": 0
                },
                {
                    "sent": "The affect of the future sort on on what happened in the past.",
                    "label": 0
                },
                {
                    "sent": "And there's another paper shortly after that that we collaborated on that essentially does sort of a hybrid of this model with a recurrent neural net and leverages backdrop through time.",
                    "label": 0
                },
                {
                    "sent": "For learning these these these connections at the weights that connect hidden variables through time.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is a very simple variant of this model.",
                    "label": 0
                },
                {
                    "sent": "It's a good one to introduce the summer school, but there's actually quite a rich literature on these sorts of models with varying degrees of complexity in their connections.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Gray",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've gone from GBM to conditional RBM to model time.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to introduce.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These sorry, these additional state variables which are used for activities.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way to think of this is this additional Q variable is able to set an effective CRB Mo for every this.",
                    "label": 0
                },
                {
                    "sent": "The way that this guy is modulating or changing these weights for every setting of this discrete variable, you effectively get a new CRBMO depending on the activity or if you're unsure about these activities with these were taken in real values.",
                    "label": 0
                },
                {
                    "sent": "It kind of blends in.",
                    "label": 0
                },
                {
                    "sent": "Different dynamical models, and that's and those are inferred over overtime.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in terms of the math not going into too many details about the math, but what we're interested in is we're interested in a predictive model of the pose at time T. Given the history of poses, that's what we need for tracking.",
                    "label": 0
                },
                {
                    "sent": "Remember, we said if we have an idea of the way the body moves through time?",
                    "label": 0
                },
                {
                    "sent": "There goes the phone again.",
                    "label": 0
                },
                {
                    "sent": "This I can't believe this happened to me in both talks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll hang it up.",
                    "label": 0
                },
                {
                    "sent": "Hopefully nobody calls again.",
                    "label": 0
                },
                {
                    "sent": "So if you have an idea of how the body moves through time, you can do a better job of tracking.",
                    "label": 0
                },
                {
                    "sent": "So this is what this is.",
                    "label": 0
                },
                {
                    "sent": "The distribution we want to estimate.",
                    "label": 0
                },
                {
                    "sent": "But now we have these latent variables, so we're going to marginalized over the PBM style variables.",
                    "label": 0
                },
                {
                    "sent": "An these activity variables.",
                    "label": 0
                },
                {
                    "sent": "To get this distribution were interested in, and actually, when we do this, we get a form or expression that looks very familiar to people that know about mixture models.",
                    "label": 0
                },
                {
                    "sent": "So we actually have a mixture of C GBM models and This is why we call it the implicit mixture of CR, bcause the mixture form kind of falls out from the way that we set up the energy function.",
                    "label": 0
                },
                {
                    "sent": "So essentially, if you have a term that's being controlled by these activity variables and those activity variables like a setter, sort of switching in and switching out these dynamical models and by doing inference over these guys gives you that little black and white picture I showed you before of the movie games.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is the thing that my post Doc is using to sort of infer the sort of groups of activities that the fruit flies are doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The advantages so compared to the previous dynamical models, people looked at.",
                    "label": 0
                },
                {
                    "sent": "This can leverage large datasets like I said and be trained pretty quickly, sort of in on the order of minutes on GPS.",
                    "label": 0
                },
                {
                    "sent": "You can sample front it from it.",
                    "label": 0
                },
                {
                    "sent": "An I said you can train it with or without active.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's really a lot on the dynamical model aspect.",
                    "label": 0
                },
                {
                    "sent": "How does this fit into the whole tracking framework 'cause we need to connect this to the image observations?",
                    "label": 0
                },
                {
                    "sent": "So what we're ultimately interested in for tracking?",
                    "label": 0
                },
                {
                    "sent": "Remember, we're given a series of image frames were calling these Y and we want to predict.",
                    "label": 0
                },
                {
                    "sent": "So this is the history of image frames and we want to predict the current pose.",
                    "label": 0
                },
                {
                    "sent": "So that's the filtering distribution.",
                    "label": 0
                },
                {
                    "sent": "What we are ultimately interested in, and that can be decomposed.",
                    "label": 0
                },
                {
                    "sent": "So assuming the independence.",
                    "label": 0
                },
                {
                    "sent": "Which we do in this model of the the observations.",
                    "label": 0
                },
                {
                    "sent": "Given the pose, we can break this up into two terms, and so these two terms one is the likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, so say given the body pose, what's the likelihood of observing a particular frame and the other one is a prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to talk about the likelihood a little bit later that we just define what that looks like the prediction.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is actually written in a recursive form.",
                    "label": 0
                },
                {
                    "sent": "So assuming now a first order Markov assumption, which can be actually relaxed to an NTH order Markov assumption, we can break up this predictive distribution into an integration over a series of terms that involve a product of two things.",
                    "label": 0
                },
                {
                    "sent": "So one is the dynamical model, so this is what we just talked about.",
                    "label": 0
                },
                {
                    "sent": "This whole IMC PBM, and then the most recent posterior.",
                    "label": 0
                },
                {
                    "sent": "So this guy look, we're predicting this.",
                    "label": 0
                },
                {
                    "sent": "So this is the previous.",
                    "label": 0
                },
                {
                    "sent": "Action that we got, so it's really a recursive definition.",
                    "label": 0
                },
                {
                    "sent": "This all this all is nice.",
                    "label": 0
                },
                {
                    "sent": "We we can define our likelihood.",
                    "label": 0
                },
                {
                    "sent": "We can define our dynamical model and then we can update this recursively.",
                    "label": 0
                },
                {
                    "sent": "The only problem is that this integral right here is not analytically tractable.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we use something that is well known and quite popular in in tracking as we use up what's called a particle filter.",
                    "label": 0
                },
                {
                    "sent": "So we have basically a discrete number of observations.",
                    "label": 0
                },
                {
                    "sent": "We represent this integral by just a series of points and they waited.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how we.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use there to get around that intractable integral.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "There's two things that we defined the dynamical model which we just talked about.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So for the likelihood, I'm not going to go into too many details.",
                    "label": 0
                },
                {
                    "sent": "We essentially took what other people had used in the past.",
                    "label": 0
                },
                {
                    "sent": "There pretty simple likelihoods.",
                    "label": 0
                },
                {
                    "sent": "Given a body model, we can basically figure out the edges if we project that, sort that body into the current frame and match those edges along the edges of these.",
                    "label": 0
                },
                {
                    "sent": "The cylindrical body model with an edge map coming from the observation observed frame.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple model.",
                    "label": 0
                },
                {
                    "sent": "You can also do this, something similar, which is take the cylindrical body model.",
                    "label": 0
                },
                {
                    "sent": "Which you know, if you have the pose you have this configuration, the body model, you can project it into the frame and essentially fill the interior with white and match that up with the silhouette from the image and this allows you to match the image to your current hypothesis about the body model.",
                    "label": 0
                },
                {
                    "sent": "These are pretty these are handcrafted, they're pretty simple, they work OK in our setup, but I think there's work here to be done in terms of you could learn likelihoods, of course using deep learning techniques, and you can also learn the dynamics using deep learning.",
                    "label": 0
                },
                {
                    "sent": "Or you could essentially learn the whole tracking setup using deep learning, and I haven't seen anybody do this yet.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in terms of question at the back.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so the questions about Q does it?",
                    "label": 0
                },
                {
                    "sent": "Does it have to correspond to the number of known activities in your data set?",
                    "label": 0
                },
                {
                    "sent": "If you train it in a labeled way, then it does, because those the there's information attached to each other or an identity attached to each of those variables.",
                    "label": 0
                },
                {
                    "sent": "If you're training in an unsupervised way, then you just have to choose an arbitrary number of variables to give it, just like the arbitrary number of zeds you give it.",
                    "label": 0
                },
                {
                    "sent": "We could also think of another situation where you split these queues.",
                    "label": 0
                },
                {
                    "sent": "Into the unsupervised movings and then the activity specific variables.",
                    "label": 0
                },
                {
                    "sent": "We didn't try that, but you could do a mixture of both.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I missed the first part of the question, I didn't.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Oh, I haven't seen that often.",
                    "label": 0
                },
                {
                    "sent": "You know in the on.",
                    "label": 0
                },
                {
                    "sent": "So when you deploy this model right and you're doing tracking at Test time, you don't know the future necessarily, right?",
                    "label": 0
                },
                {
                    "sent": "So you, unless there's some sort of hypothesis about the future using some external sort of non post specific information.",
                    "label": 0
                },
                {
                    "sent": "That might work, but I I haven't seen anything like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in terms of the way that we experimented, we use something called the human Eva data set.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned before, that has simultaneous video from a number of cameras and motion capture an they're synced up.",
                    "label": 0
                },
                {
                    "sent": "So this is what allows you to train the model.",
                    "label": 0
                },
                {
                    "sent": "The limitation of human Eva is that it's an indoor controlled setting, so to get the mocap they actually had to put markers on the individual, suit them up.",
                    "label": 0
                },
                {
                    "sent": "As you can see an they do all these different activities.",
                    "label": 0
                },
                {
                    "sent": "You have multiple subjects, so it's a nice fairly big data set.",
                    "label": 0
                },
                {
                    "sent": "But you can't take the model and deploy it outdoors, so again this is an area of future research.",
                    "label": 0
                },
                {
                    "sent": "It's kind of unexplored in my my opinion, doing sort of less controlled tracking using these sorts of dynamical models and possibly adding in visual models as well.",
                    "label": 0
                },
                {
                    "sent": "So the other thing that we really we we measure at Test time is we.",
                    "label": 0
                },
                {
                    "sent": "We attempt to do the track and then we measure in millimeters the average error between the true three depots coming from the motion capture and the pose that's estimated from our tracking system.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's there's lots of results that are in the paper.",
                    "label": 0
                },
                {
                    "sent": "There's a couple that I'll highlight.",
                    "label": 0
                },
                {
                    "sent": "1st, If we look at doing these sort of different variants of the model, we do this the multiview setup, where we're actually using multiple cameras.",
                    "label": 0
                },
                {
                    "sent": "This makes it easier because we can cope with occlusions and when we look at walking and jogging with transitions are modeled as much better than the baseline, which is a very simple dynamical model is just using sort of a constant velocity prediction.",
                    "label": 0
                },
                {
                    "sent": "He adding in these discrete variables and using their labels.",
                    "label": 0
                },
                {
                    "sent": "So that's what the two L. So we have a walking and jogging label that does extremely well when we know the activities.",
                    "label": 0
                },
                {
                    "sent": "If you just give it 10 latent variables and you allow it to assign these sort of moving.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the way that it does it.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do quite as well in terms of prediction, but for a totally unsupervised model it does a reasonable job.",
                    "label": 0
                },
                {
                    "sent": "And the cool thing is that the way that it uses its hidden units.",
                    "label": 0
                },
                {
                    "sent": "So it seems that some of them are actually like this.",
                    "label": 0
                },
                {
                    "sent": "This guy and this guy along here.",
                    "label": 0
                },
                {
                    "sent": "So the third feature is used for both of the different activities.",
                    "label": 0
                },
                {
                    "sent": "So it's something about the motion itself.",
                    "label": 0
                },
                {
                    "sent": "But some variables are really activity specific.",
                    "label": 0
                },
                {
                    "sent": "So for example in this the 9th variable isn't.",
                    "label": 0
                },
                {
                    "sent": "Use very much in the jogging component and same with the first variable, so some other sort of activity.",
                    "label": 0
                },
                {
                    "sent": "Specific and summer activity.",
                    "label": 0
                },
                {
                    "sent": "Generic variables, yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so typically what we do in this setup is we look at two things, so I actually.",
                    "label": 0
                },
                {
                    "sent": "This is important 'cause I have some stars here, so the stars mean when we we train the prior model on the same subject.",
                    "label": 0
                },
                {
                    "sent": "So we have a subject specific motion model.",
                    "label": 0
                },
                {
                    "sent": "So we kind of understand the way that that person moves and that tends to work well.",
                    "label": 0
                },
                {
                    "sent": "You when you are using a simpler model, but as soon as you add more parameters to your model that sorts starts to overfit.",
                    "label": 0
                },
                {
                    "sent": "So in that case we we train a generic model so it's multiple subjects, learn how they move and deploy it at Test time.",
                    "label": 0
                },
                {
                    "sent": "Now at Test time we always see motion of the same subject, so we never take your motion and then train a model and then try to predict my motion.",
                    "label": 0
                },
                {
                    "sent": "There's always either that individual in a larger group, or a person specific prior.",
                    "label": 0
                },
                {
                    "sent": "Not great, but we test on completely different frames so there's different sequences of that person moving around.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last thing I want to show in this work is the more complicated setup where we do not multi view, but manach monocular tracking.",
                    "label": 0
                },
                {
                    "sent": "So this is where you have a single camera and this makes it harder to track an individual because there can be occlusions.",
                    "label": 0
                },
                {
                    "sent": "So you'll see our method doing a reasonable job.",
                    "label": 0
                },
                {
                    "sent": "This is essentially how well it works.",
                    "label": 0
                },
                {
                    "sent": "It makes errors sometimes, particularly on the arms when there's a transition, but it does a pretty good job tracking a person from a single camera when there's transitions.",
                    "label": 0
                },
                {
                    "sent": "And before this approach, other people weren't able to handle this situation very well at all.",
                    "label": 0
                },
                {
                    "sent": "When there were transitions in the data.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's tracking so remainder of the talk I'm going to focus on activity and gesture.",
                    "label": 0
                },
                {
                    "sent": "Oh yes right there.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Sure, so I should.",
                    "label": 0
                },
                {
                    "sent": "Also that brings up a good point in that in this work we always initialized with the correct initial pose.",
                    "label": 0
                },
                {
                    "sent": "So we started our model off knowing where the person was in the frame, and that's pretty crucial to making it work.",
                    "label": 0
                },
                {
                    "sent": "Well now you're saying what happens if you don't have that initialization?",
                    "label": 0
                },
                {
                    "sent": "Then you're essentially falling back to pose estimation, which is a problem I talked about originally.",
                    "label": 1
                },
                {
                    "sent": "Given an image that is the 1st frame of the video.",
                    "label": 1
                },
                {
                    "sent": "How is that person configured?",
                    "label": 0
                },
                {
                    "sent": "So if you have a very good pose estimation method, you say one of these ones like depots.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh well, I guess I mean, say it's it's making a probabilistic prediction.",
                    "label": 0
                },
                {
                    "sent": "You would kind of know about the uncertainty outputs, but that's really all you have.",
                    "label": 0
                },
                {
                    "sent": "I mean, you don't know if it's a perfect model of predicting the pose, so you have to use that model and rely on that as your initial prediction.",
                    "label": 0
                },
                {
                    "sent": "Unless you had multiple models right, she has sort of an ensemble and you could use that to give you some idea of uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So if all the models didn't agree on a particular pose, then you may have to fall back to, say a human initialization or something like that.",
                    "label": 0
                },
                {
                    "sent": "That's the other way or rely on different types of models.",
                    "label": 0
                },
                {
                    "sent": "OK, so the 3rd and final section is activity in gesture.",
                    "label": 0
                },
                {
                    "sent": "So while pose estimation and tracking, we're focused on where is the person in the scene and how is their body configured.",
                    "label": 0
                },
                {
                    "sent": "Activity and gesture understanding really looks at what is that person doing, and so this is received a little bit more attention from the deep learning community, so I'm going to divide this up into 2.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions the first is going to be models that I would call are unsupervised and supervised hybrid models.",
                    "label": 0
                },
                {
                    "sent": "So what these techniques do is they learn using unsupervised learning, a way to extract features and then they build a recognition pipeline on top of that.",
                    "label": 0
                },
                {
                    "sent": "The second approach is just an end to end supervised learning approaches, say applying a 3D convolutional Nets to activity recognition.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about the hybrid approach.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st, So going back to some work that we did a number of years ago, we used a basic building block called a gated PBM.",
                    "label": 0
                },
                {
                    "sent": "So I roll and unfortunately hasn't had a chance to do his next lecture yet.",
                    "label": 0
                },
                {
                    "sent": "I was maybe hoping he might introduce some of this stuff, so I'm gonna give you 1 slide overview of this model and then roll is going to tell you all the great advances in multiview feature learning because he's really expert in this area and he invented a lot of this.",
                    "label": 0
                },
                {
                    "sent": "He invented this.",
                    "label": 0
                },
                {
                    "sent": "Some of this stuff, and he also sort of looked back in the past and understands a lot of the work that's been done in the past as well.",
                    "label": 0
                },
                {
                    "sent": "So the gated GBM.",
                    "label": 0
                },
                {
                    "sent": "So the way to think about this again, you'll get more information a little bit later, but think about this in one of two ways.",
                    "label": 0
                },
                {
                    "sent": "If you understand linear autoregressive models.",
                    "label": 0
                },
                {
                    "sent": "You can see this as sorry I want.",
                    "label": 0
                },
                {
                    "sent": "I want to take a step back and say something else that's more basic.",
                    "label": 0
                },
                {
                    "sent": "Think of this as a type of GBM that no, that doesn't take a single observation, but it takes pairs of observations.",
                    "label": 0
                },
                {
                    "sent": "OK, so we had GBM has visible units and hidden units.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to have input units and output units and hidden units.",
                    "label": 0
                },
                {
                    "sent": "So both the inputs and outputs are going to be observed and these guys are going to be latent.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the first first thing you need to think about.",
                    "label": 0
                },
                {
                    "sent": "Takes pairs so that and the pairs that we're going to use here are adjacent frames.",
                    "label": 0
                },
                {
                    "sent": "Videos were used to extract motion features.",
                    "label": 0
                },
                {
                    "sent": "So given the input and output, if you're familiar with auto regressive models, this is like a linear autoregressive model that say predicts output from the input.",
                    "label": 0
                },
                {
                    "sent": "But you make it non linear by introducing a series of latent variables that can modulate the connections in that model.",
                    "label": 0
                },
                {
                    "sent": "So given a particular setting of the hiddens, this kind of defines this.",
                    "label": 0
                },
                {
                    "sent": "This model or relationship between input and output.",
                    "label": 0
                },
                {
                    "sent": "So that's one view.",
                    "label": 0
                },
                {
                    "sent": "Another equivalent review is view is if you understand our BMS and kind of like that picture, it's just like having a standard GBM defined over the output.",
                    "label": 0
                },
                {
                    "sent": "But now you have the input modulating or changing the weights in that DBM.",
                    "label": 0
                },
                {
                    "sent": "OK, so for every setting of the input that defines basically a different DBM on the output.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was the basic building block.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this model.",
                    "label": 0
                },
                {
                    "sent": "We scaled it up to convolutional type of gated GBM which would allow it to do pretty efficient at the time.",
                    "label": 0
                },
                {
                    "sent": "Feature extraction from video.",
                    "label": 0
                },
                {
                    "sent": "So we look at pairs of frames from video.",
                    "label": 0
                },
                {
                    "sent": "The input and the adjacent frame.",
                    "label": 0
                },
                {
                    "sent": "We would extract features from this and inspired front by the convolutional RBM which hung Black talked about yesterday.",
                    "label": 0
                },
                {
                    "sent": "We also had a Max pooling layer built into that, and So what we did you know instead of connecting up all of the inputs and all the outputs in a frame.",
                    "label": 0
                },
                {
                    "sent": "These these guys were connected locally and we had feature sharing like in standard convolutional Nets, so the types of features that you get out when you essentially do feature extraction across multiple frames are features that tend to highlight.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Motion, so I'm now showing some images of feature Maps, so each row here is a different feature map that's been learned and produced by this model on a particular data set.",
                    "label": 0
                },
                {
                    "sent": "This gets me every time OK. And you can see that some of these features are very motion sensitive, so there's there's two types of activity here.",
                    "label": 0
                },
                {
                    "sent": "One is hand clapping, one is walking, so in the hand clapping setup you'll see that features one and three are very motion sense of their targeted on on the hands.",
                    "label": 0
                },
                {
                    "sent": "The part of the video that's moving, you get some features that actually capture static information too.",
                    "label": 0
                },
                {
                    "sent": "So one feature actually learns to just.",
                    "label": 0
                },
                {
                    "sent": "Pick out edges, in particular around the individual, and then the 6th feature tends to do essentially a segmentation operation.",
                    "label": 0
                },
                {
                    "sent": "So combined these give you a nice idea of what's going on in the image, both statically and dynamically.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once you've extracted these motion features, I said this is essentially a hybrid approach where this first block here is doing this unsupervised feature extraction with that then use that as an input, which is consumed by a 3D convolutional net.",
                    "label": 0
                },
                {
                    "sent": "But it's a very shallow 3D convolutional net.",
                    "label": 0
                },
                {
                    "sent": "We then want to produce a single prediction for a whole video or snippet of video, so we have to do some pooling across the video to get a single output.",
                    "label": 0
                },
                {
                    "sent": "So we have a temporal pooling setup, and then we have a number of fully connected layers.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at the results a little bit later on.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at some different approaches compared to one another at the same time that we were doing this, there were some researchers at Stanford Kwok Lai and his colleagues who were using a different unsupervised module called Independent Subspace Analysis to also do feature extraction.",
                    "label": 0
                },
                {
                    "sent": "Now ISA produces filters which are tuned to velocity, frequency and rotation.",
                    "label": 0
                },
                {
                    "sent": "So their key idea was basically scale up ISA through.",
                    "label": 0
                },
                {
                    "sent": "Convolution in stacking and we were scaling up these.",
                    "label": 0
                },
                {
                    "sent": "These are BMS through convolution and stacking them as well, so it's just a different unsupervised learning building block, but a similar approach.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What they would do basically is take a quite a high dimensional video, break it up into blocks.",
                    "label": 0
                },
                {
                    "sent": "You can think of sort spatiotemporal patches.",
                    "label": 0
                },
                {
                    "sent": "Train these ISA model on these patches and then and concatenate them.",
                    "label": 0
                },
                {
                    "sent": "Perform a PCA whitening and dimensionality reduction and then do another layer of ISA on top of this, we just repeat the same module multiple times, but you have some dimensionality reduction happening in the middle to help you deal with the sheer mass of data that you have.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just another picture of it, where you're taking a video.",
                    "label": 0
                },
                {
                    "sent": "You're cutting it up into little blocks.",
                    "label": 0
                },
                {
                    "sent": "You're applying isa to each of those blocks, and then you're combining the outputs, reducing them dementia, reducing the dimensionality, and then training another ISA block on top of them.",
                    "label": 0
                },
                {
                    "sent": "Now the interesting part is, once you do the highlight when you're looking at high level features, you're not only taking the output of the higher level ISA module, but you're also taking sort of.",
                    "label": 0
                },
                {
                    "sent": "You're doing what people call Skip connections where you take the PC of previous.",
                    "label": 0
                },
                {
                    "sent": "Features that have been extracted and that basically gives a descriptor just like we had those descriptors I showed you of the motion sensitive features and then those are applied to an activity recognition pipeline.",
                    "label": 0
                },
                {
                    "sent": "The last approach, that sort of falls in this.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unsupervised supervised learning hybrid.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is some work that Roland Ankush or Conda did, and both of them are here today.",
                    "label": 0
                },
                {
                    "sent": "I hope I don't do a terrible job explaining this in front of the authors, but they looked back at some of the previous work that had happened and both basically energy motion models that were used through the 90s and also these cross correlational models and they found that the way that these models were shaped essentially they were confounding two things, so the first thing that was the extraction of the motion.",
                    "label": 0
                },
                {
                    "sent": "For the detection of the type of motion, but in these models also had sort of a invariants component to it, so they were trying to learn invariant representations, and these were confounded.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                },
                {
                    "sent": "Rolling in cash, or were looking at were essentially a particular type of architecture that could be couple these and essentially do motion detection separately from learning invariants.",
                    "label": 0
                },
                {
                    "sent": "And so the way that they.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detected the end up detection mode.",
                    "label": 0
                },
                {
                    "sent": "Detecting motion is through a concept they call motion synchrony, and it's quite a clever idea.",
                    "label": 0
                },
                {
                    "sent": "So first of all, they say, let's assume that two images are related through an orthogonal warp, so there's some matrix P, and this is orthogonal matrix as Roland mentioned yesterday.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as sort of a permutation matrix or matrix that shuffles around ink from image X to image X1 to X2.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't destroy any anchor.",
                    "label": 0
                },
                {
                    "sent": "Introduce any any new ink in the process, and so if they are related via this transformation, one way that you can detect this transformation and they prove this in their paper is all you need to do is take two filters that are related through the same transformation and then check through.",
                    "label": 0
                },
                {
                    "sent": "Check for a condition that they call motion synchrony.",
                    "label": 0
                },
                {
                    "sent": "So they're really determining whether these these two filters yield equal responses when they are applied in sequence 22 frames.",
                    "label": 0
                },
                {
                    "sent": "You have two frames.",
                    "label": 0
                },
                {
                    "sent": "Adjacent to one another, you're going to apply these related filters, and then if these guys are equivalent, you've successfully detected this particular type of motion.",
                    "label": 0
                },
                {
                    "sent": "OK, so the proof of that is in the paper.",
                    "label": 0
                },
                {
                    "sent": "Let's go on to, oh, sorry.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do you actually detect synchrony in in practice so?",
                    "label": 0
                },
                {
                    "sent": "You know, if you fall back to standard neural networks, which essentially use sort of a standard, some of the filter responses, and then do a threshold.",
                    "label": 0
                },
                {
                    "sent": "It's it's impossible for you to tell whether the reasonable active activation beyond some threshold is a result of essentially one really good match.",
                    "label": 0
                },
                {
                    "sent": "So say W1X1 is a great match and W2X2 is a crappy match versus both of these guys matching pretty well, so they both match pretty well.",
                    "label": 0
                },
                {
                    "sent": "That's an evidence of the synchrony happening if one guys matching really well and the other guys matching not so well, then synchrony isn't present, but you really can't tell that if you just sum their responses and you threshold it.",
                    "label": 0
                },
                {
                    "sent": "But what you can do is you can use these multiplicative style interactions like I talked about before as a way to check them.",
                    "label": 0
                },
                {
                    "sent": "See you apply the filtering operation.",
                    "label": 0
                },
                {
                    "sent": "These are just two pixel images.",
                    "label": 0
                },
                {
                    "sent": "These are the filters you apply them and then you check their product.",
                    "label": 0
                },
                {
                    "sent": "OK, that becomes your basic detection module for this motion synchrony and so why is This is why this is useful is that you can actually learn a system of the types of motion you want to be able to detect, and you can do the detection.",
                    "label": 0
                },
                {
                    "sent": "Using a reasonably simple neural net.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what Roland Ankush or proposed were two different models for actually being able to detect this synchrony.",
                    "label": 0
                },
                {
                    "sent": "One was based on a model similar to the gated GBM that I just showed you, but it gated autoencoder variant and I'm sure rolling will talk about that tomorrow.",
                    "label": 0
                },
                {
                    "sent": "And they also proposed AK means variant, which is sort of a temporal online variant.",
                    "label": 0
                },
                {
                    "sent": "Both of these methods could be trained by gradient based optimization, and neither of them had a pooling operation built into them.",
                    "label": 0
                },
                {
                    "sent": "Like our method, the convolutional get DBM had.",
                    "label": 0
                },
                {
                    "sent": "So where this model really shines is fixed.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Namely efficient, so I told you I would look at the results of all these different sort of unsupervised supervised hybrid approaches.",
                    "label": 0
                },
                {
                    "sent": "They all do pretty well actually.",
                    "label": 0
                },
                {
                    "sent": "You know, in terms of absolute performance qua clays, convolutional ISA tends to outperform all of them on these.",
                    "label": 0
                },
                {
                    "sent": "These are popular datasets.",
                    "label": 0
                },
                {
                    "sent": "KTH is this kind of toy is like the M nest of activity recognition.",
                    "label": 0
                },
                {
                    "sent": "It's pretty small, people get can get like 100% on it now, but it's a good way of sanity checking your work.",
                    "label": 0
                },
                {
                    "sent": "You see esports, an Hollywood, two or more reasonably sized activity recognition datasets?",
                    "label": 0
                },
                {
                    "sent": "So even though you know qua clays work, ANAN, rolling and shores, recent work are similar in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "Their work is extremely efficient in the way that uses primarily local operations, so you can train it in two minutes on a GPU or three minutes and the other previous work was one to two hours and our work.",
                    "label": 0
                },
                {
                    "sent": "Actually I showed you before as a couple of days to train, even with the GPU.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last thing I want to talk about in terms of activity recognition before I get a little bit to gesture or fully end to end supervised methods.",
                    "label": 0
                },
                {
                    "sent": "So just like I said for pose, the obvious thing that comes to mind is just apply a combinant to images of people and try to extract their pose for activity recognition.",
                    "label": 0
                },
                {
                    "sent": "The obvious thing to do is take a commnet which is really good detection, make its filters 3D and apply that to activity.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ignition.",
                    "label": 0
                },
                {
                    "sent": "So going back to 2010, there's a guy name she wanted G who actually looked at.",
                    "label": 0
                },
                {
                    "sent": "Looked at doing this, and so he said, you know, the obvious thing you could train to.",
                    "label": 0
                },
                {
                    "sent": "The comments are very efficient, they work well, you can train it frame to frame by frame and apply that states do some temporal pooling, but he said I'm going to actually create a commnet where the filters are three dimensional an so he trained.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh my goodness.",
                    "label": 0
                },
                {
                    "sent": "If it's if it's not the phone, it's the screen.",
                    "label": 0
                },
                {
                    "sent": "OK. OK OK, so this convolutional neural net looks a lot like the standard 2D convolutional neural Nets.",
                    "label": 0
                },
                {
                    "sent": "He actually hardwired his input so we had sort of blocks of seven temporal frames at a resolution of 60 by 40, and each of these guys were pre process to have a grayscale channel X&Y gradient channel and optical flow.",
                    "label": 0
                },
                {
                    "sent": "And actually if you look at the modern day sort of five years later, the types of 3D comments that are being used, some of them notably coming from.",
                    "label": 0
                },
                {
                    "sent": "Andrews sermons lab.",
                    "label": 0
                },
                {
                    "sent": "There are also using optical flow as an input to that net, so just extracting that I apriori even though.",
                    "label": 0
                },
                {
                    "sent": "Interestingly enough, these types of gated models can learn things like optical flow.",
                    "label": 0
                },
                {
                    "sent": "The standard convolutional Nets, 3D convolutional Nets actually advantageous to provide them with a flow based representation, so this is sort of a standard convolutional net pipeline.",
                    "label": 0
                },
                {
                    "sent": "3D convolutions fully connected, fully connected layers, and then some action units.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you compare this to five years later, as sort of a state of the art architecture that's trained on a much larger database, some of the architectural improvements that you're seeing are multiscale pathways, which have also been very important for advances in static vision tests.",
                    "label": 0
                },
                {
                    "sent": "So they're using something called a foveated representation, where you take a very.",
                    "label": 0
                },
                {
                    "sent": "A sort of a low dimensional representation of the entire low resolution entire scene representation, but you also have this foveated component which is just right sort of at the middle of the image you have a high resolution input, and you feed each of these through identical architectures, and then you do a sort of a late Fusion.",
                    "label": 0
                },
                {
                    "sent": "You get pretty significant performance compared to Standard Vision feature based pipelines.",
                    "label": 0
                },
                {
                    "sent": "And then the interesting thing here is that you only get a modest improvement compared to modern single frame combats.",
                    "label": 0
                },
                {
                    "sent": "So if you have all this multiscale stuff going on and you kind of forget about the temporal dimension and just do two decomp Nets, they actually work fairly well in practice and there are lot quicker to train.",
                    "label": 0
                },
                {
                    "sent": "So they also introduced this Google Sports 1 million data set with 47 classes.",
                    "label": 0
                },
                {
                    "sent": "The problem with this data set that we've learned recently and there's a few that have done this as they release the data set as metadata.",
                    "label": 0
                },
                {
                    "sent": "So they basically have You Tube URLs where you can grab these videos and then they have labels for them.",
                    "label": 0
                },
                {
                    "sent": "Is YouTube videos tend to disappear overtime and we recently were talking with the authors on this and they said yeah, so the data is what's going to happen.",
                    "label": 0
                },
                {
                    "sent": "Is the datasets it's going to kind of change overtime?",
                    "label": 0
                },
                {
                    "sent": "That's interesting, but I guess you know one thing you could do.",
                    "label": 0
                },
                {
                    "sent": "Is it just take a snapshot at the beginning, right and and sort of pass around a hard drive or something like that, but you know we're at the point of these datasets you it's difficult to ship them around.",
                    "label": 0
                },
                {
                    "sent": "So 1,000,000 YouTube videos.",
                    "label": 0
                },
                {
                    "sent": "This is particularly important for video, right?",
                    "label": 0
                },
                {
                    "sent": "So what do you do in this situation?",
                    "label": 0
                },
                {
                    "sent": "So anyways, that's that's kind of an aside, there's a.",
                    "label": 0
                },
                {
                    "sent": "There's another recent data set, sort of on the same scale, not just looking at sports.",
                    "label": 0
                },
                {
                    "sent": "Same sort of issue, a lot of the videos are starting to disappear because it's it's YouTube URLs OK.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last thing, yes.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one is not using optical flow, but there's a reference down here out of The Andrews Sisters lab.",
                    "label": 0
                },
                {
                    "sent": "Karen simonyan.",
                    "label": 0
                },
                {
                    "sent": "Ann Andrews Isermann they use an optical flow input, and so these these architectures are kind of neck and neck right now for this data set.",
                    "label": 0
                },
                {
                    "sent": "This is on raw inputs color considering color input, but on all inputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so the last thing I want to talk about, I think I I'm about 510 minutes before 20 total.",
                    "label": 0
                },
                {
                    "sent": "OK, so about 10 minutes left before questions maybe.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about gesture, so there's some work we just had accepted and Pammy.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With Natalia, who's here again, she's done.",
                    "label": 0
                },
                {
                    "sent": "The heavy lifting is lifting on this project as part of her PhD, and so this project problem is related to activity recognition, but it's in a specialized sort of limited type of activity domain, so it's limited in terms of the number of gestures, and we're looking at something called communicated gestures, so this is where someone is trying to actually communicate in.",
                    "label": 0
                },
                {
                    "sent": "Really, it's really language actually, so so different languages use this more than others.",
                    "label": 0
                },
                {
                    "sent": "So Italian is a perfect example.",
                    "label": 0
                },
                {
                    "sent": "When people say words, they they basically make a gesture with it, right?",
                    "label": 0
                },
                {
                    "sent": "So we call this communicated gestures or intentional gestures, and this the data set that we actually used was based on Italian speakers.",
                    "label": 0
                },
                {
                    "sent": "So they'll say a word, they'll make a gesture, right.",
                    "label": 0
                },
                {
                    "sent": "Quite dramatically as Italians do, and I'm married to an Italian, so I guess I can.",
                    "label": 0
                },
                {
                    "sent": "I can say this and the cool thing with this data set, so it's limited to a certain type of gesture.",
                    "label": 0
                },
                {
                    "sent": "But it's actually a multimodal data set, so it makes the modeling problem interesting and challenging.",
                    "label": 0
                },
                {
                    "sent": "So we get with the data set, pose information.",
                    "label": 0
                },
                {
                    "sent": "So the person who's been in front of a connect system so we can articulated skeleton, we get.",
                    "label": 0
                },
                {
                    "sent": "We have video depth video and RGB video, and so the connect.",
                    "label": 0
                },
                {
                    "sent": "I think I mentioned this yesterday will give you a skeleton, but it won't give you any information about the hands.",
                    "label": 0
                },
                {
                    "sent": "The configuration of the hands, and that's because the hands typically occupied just a few.",
                    "label": 0
                },
                {
                    "sent": "It's very difficult to track them.",
                    "label": 0
                },
                {
                    "sent": "There's also some other work Natalia's done on detecting hands, which is at ACV last year.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about it, but there's people that study hand specifically, but what we're going to do is we're going to use the skeleton to tell us where the hand is, not the layout of the fingers, but just sort of the center of the hand.",
                    "label": 0
                },
                {
                    "sent": "And then we'll we'll take a crop of the depth video in the in the grayscale video around that location.",
                    "label": 0
                },
                {
                    "sent": "The reason we do that is that.",
                    "label": 0
                },
                {
                    "sent": "Some gestures can be fully fully characterized by upper body motion, like this one, so you just need to see the body to understand the gesture.",
                    "label": 0
                },
                {
                    "sent": "Other ones you really need to look at subtle finger movements in this in this particular gestures question or just a stretch you look like you're stretching, OK, great.",
                    "label": 0
                },
                {
                    "sent": "OK OK great.",
                    "label": 0
                },
                {
                    "sent": "So we also get an audio channel so we have visual information.",
                    "label": 0
                },
                {
                    "sent": "We also have auditory information.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The type of architecture that we use is sort of based on the challenges that we're facing, so the challenges that we're facing is we need to learn representations at multiple spatial and temporal scales, so the spatial comes from you that find hand versus full body information.",
                    "label": 0
                },
                {
                    "sent": "The temporal scales are something that we build into this architecture, so we actually have pipelines that are there.",
                    "label": 0
                },
                {
                    "sent": "Generally these are sort of temporal, scale specific identical pipelines.",
                    "label": 0
                },
                {
                    "sent": "That operate In Sync.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what we do is we essentially sample.",
                    "label": 0
                },
                {
                    "sent": "We call dynamic, which are essentially a collection of frames that are concatenated into a little spatial temporal volume.",
                    "label": 0
                },
                {
                    "sent": "But the way that we sample these frames is different for these different steps, so this step the S Some people call this stride in the in the commnet literature.",
                    "label": 0
                },
                {
                    "sent": "It's just how frequently we do temporal sampling, so we this changes the inputs to each of these streams.",
                    "label": 0
                },
                {
                    "sent": "But these networks are essentially parameterized the same, but the values of these parameters are different, so each of these these.",
                    "label": 0
                },
                {
                    "sent": "Scales learns to detect the gesture on its own, and then they refused.",
                    "label": 0
                },
                {
                    "sent": "Later on in the pipeline.",
                    "label": 0
                },
                {
                    "sent": "So let's now you know we have these multiple scale temporal scale detectors.",
                    "label": 0
                },
                {
                    "sent": "They get fuse.",
                    "label": 0
                },
                {
                    "sent": "Let's look at what a single scale looks, or a single stage looks like.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it's fairly complicated because we're dealing with quite a lot of data types, so remember we're looking at depth video intensity video of both hands, so there's hand one hands too.",
                    "label": 0
                },
                {
                    "sent": "We have full body mocap and then we have audio, so both the visual information.",
                    "label": 0
                },
                {
                    "sent": "So the depth and the intensity video use convolutional Nets and for the mocap we just use fully connected Nets and then for the.",
                    "label": 0
                },
                {
                    "sent": "Audio we use some standard pre processing to create.",
                    "label": 0
                },
                {
                    "sent": "A spectrogram which role in talks about yesterday, and then we convert that to Mel frequency scale and then we have a common data 1D comment that operates or to the comment that.",
                    "label": 0
                },
                {
                    "sent": "Is that one day we just looking at across spectrum or across time?",
                    "label": 0
                },
                {
                    "sent": "Judy, OK, two DCOM.",
                    "label": 0
                },
                {
                    "sent": "Net that operates on the audio pipeline.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Just some details on articulated post, so when you're dealing with motion capture human motion, it's really important to get the representation right.",
                    "label": 0
                },
                {
                    "sent": "So in my own PhD thesis I spent a lot of time doing mocap and it took several months to really find the right representation.",
                    "label": 0
                },
                {
                    "sent": "I remember and I wrote up his junk in my thesis about how to do this, 'cause I wanted people not to sort of feel the same pain of discovering the representation that I did.",
                    "label": 0
                },
                {
                    "sent": "Once you get the right representation it, everything works, works nicely.",
                    "label": 0
                },
                {
                    "sent": "Now since that time there have been a number of papers published in the.",
                    "label": 0
                },
                {
                    "sent": "Vision literature about good representations of human motion for efficient activity recognition, and one of them came out in 2013 and we've borrowed from this one, so Christian's mission SQ and and colleagues Ascentia Lee proposed quite a high dimensional descriptor of 11 joints from the bodies.",
                    "label": 0
                },
                {
                    "sent": "The Connect system gives us these 11 joints.",
                    "label": 0
                },
                {
                    "sent": "We build this descriptor.",
                    "label": 0
                },
                {
                    "sent": "You can read the details in the paper, but essentially it's containing information about the locations.",
                    "label": 0
                },
                {
                    "sent": "The velocity is the accelerations of these joints as well as some angles relative to the internal body coordinate system and then pairwise distance information.",
                    "label": 0
                },
                {
                    "sent": "You have to do some normalization to make everything work and the details are in.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper the depth video is pretty standard.",
                    "label": 0
                },
                {
                    "sent": "We apply local contrast normalization.",
                    "label": 0
                },
                {
                    "sent": "We threshold on a particular depth setting, so we get essentially the the hand information threshold from background and we share parameters between the left and the right hand path.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "If you just throw the data into all these paths, wired it up, you do Fusion here and you kind of hope for the best training with gradient descent it doesn't work well so.",
                    "label": 0
                },
                {
                    "sent": "We're dealing with on the order of 12 million parameters per scale, 37 million parameters total, and the number of training gestures we have is only around 10,000.",
                    "label": 0
                },
                {
                    "sent": "So compared to things like image net, we have fewer observations.",
                    "label": 0
                },
                {
                    "sent": "Of course we have high dimensional observations and we have temporal information.",
                    "label": 0
                },
                {
                    "sent": "But still it's an issue to fit these Nets.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we need to fall back on a lot of sort of architectural structuring to make everything work well.",
                    "label": 0
                },
                {
                    "sent": "And So what we're using is very structured weight matrices that we generally relax.",
                    "label": 0
                },
                {
                    "sent": "We pre train the individual channels.",
                    "label": 0
                },
                {
                    "sent": "OK, so for each particular modality and of course, each of the temporal scales an when we do this careful initialization of the shared layers, we basically have.",
                    "label": 0
                },
                {
                    "sent": "The motivation is that we have essentially a network that does independent predictions using the modalities.",
                    "label": 0
                },
                {
                    "sent": "And it combines them in and something like an arithmetic average that we know works reasonably well, and so it doesn't learn a lot of cross modality structure right at the beginning of learning.",
                    "label": 0
                },
                {
                    "sent": "So it learns to do a reasonable job, and we gently relax.",
                    "label": 0
                },
                {
                    "sent": "That and the cross modality structure starts to be incorporated, and that tends to.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To allow us to optimize the network by restricting its effective parameters early on in general and relaxing that.",
                    "label": 0
                },
                {
                    "sent": "So the way that we actually implement this is essentially, you know, if we consider these as sort of the top or penultimate layer hidden's coming from each of the modalities.",
                    "label": 0
                },
                {
                    "sent": "We define our shared hidden layer as sort of mood and modality specific chunks.",
                    "label": 0
                },
                {
                    "sent": "OK, so we actually attach meaning to what the hidden, how the hidden unit should be organized in the layer where we start to merge the modalities, and so we have these blocks of modality specific chunks and we initialize their weights such that there's no.",
                    "label": 0
                },
                {
                    "sent": "It's sort of as a as blockwise, so there's no cross modality information or transfer at the beginning of learning, and then we way we go from this.",
                    "label": 0
                },
                {
                    "sent": "Shared hidden layer to the output is that we wire up these weights such that they compute approximately an arithmetic average of the predictions coming from the individual modalities.",
                    "label": 0
                },
                {
                    "sent": "OK, so at the beginning of training you just have these sort of modality specific networks that predict the gesture and their combined in a reasonable reasonable but non complicated way.",
                    "label": 0
                },
                {
                    "sent": "We then.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um during learning, start to relax them, but the insight here is that it actually makes sense to combine modalities that are nearby in nature first.",
                    "label": 0
                },
                {
                    "sent": "So we start by combining the depth in the video so they're both hands, hands and visual information.",
                    "label": 0
                },
                {
                    "sent": "Then we take the mocap and we combine that with the video and the depth, and so we call that sort of visual merger.",
                    "label": 0
                },
                {
                    "sent": "And then we bring in the audio after awhile so we gently merge this, and this tends to work.",
                    "label": 0
                },
                {
                    "sent": "How much better?",
                    "label": 0
                },
                {
                    "sent": "This was all done in the sorry.",
                    "label": 0
                },
                {
                    "sent": "I wanted to say this is what the weights look like after training.",
                    "label": 0
                },
                {
                    "sent": "So after doing the relaxation there's really very strong Inter modality structure.",
                    "label": 0
                },
                {
                    "sent": "But you see, here's the different hands.",
                    "label": 0
                },
                {
                    "sent": "So after we relax it, there's a lot of cross modality structure between the hands, but there's also some cross modality structure between the hands and the motion capture, but less so with the audio.",
                    "label": 0
                },
                {
                    "sent": "So after we relax, this is what the network actually learns in terms of this structured matrix.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Correct, yeah?",
                    "label": 0
                },
                {
                    "sent": "And I'll show you a little picture of how that the stages work very shortly.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was done in the context of a competition called the Ecv Chandler and looking at People challenge and this system that Natalya built actually one out of 17 different competitors and I Natalia this running again this year child learn.",
                    "label": 0
                },
                {
                    "sent": "OK, not for gesture.",
                    "label": 0
                },
                {
                    "sent": "So challenge typically runs, but I guess this ran for a couple of years.",
                    "label": 0
                },
                {
                    "sent": "Twenty 2013 and 2014.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is what I mentioned before and this is relevant to the last question.",
                    "label": 0
                },
                {
                    "sent": "Is what happens when you start merging these modalities together so the different colors here are the different temporal strides memories that we have.",
                    "label": 0
                },
                {
                    "sent": "We have networks at different temporal scales, and they're trained the same way.",
                    "label": 0
                },
                {
                    "sent": "They just get merged very late.",
                    "label": 0
                },
                {
                    "sent": "Now the numbers here are related to different modalities when they start training and when we merge them.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are these are in steps, it's not.",
                    "label": 0
                },
                {
                    "sent": "Necessarily the time scale in terms of iterations or epoch's of training.",
                    "label": 0
                },
                {
                    "sent": "So we start by training intensity and depth on its own.",
                    "label": 0
                },
                {
                    "sent": "We then merge these guys into a video.",
                    "label": 0
                },
                {
                    "sent": "Channel then we're also training the mocap on its own, and then the Mocap gets merged with the video stuff at at Step 6, so you see that each this is the error on the validation set, and we're showing that you know every time you do a merger of these modalities or groups of modalities, you're getting a drop in terms of the error.",
                    "label": 0
                },
                {
                    "sent": "We have the audio being trained out here, but it doesn't actually get merged into the video stuff until quite late in the game.",
                    "label": 0
                },
                {
                    "sent": "And then we also do, you know, these separate scales which get merged at the very end, and that seems to be the most reasonable way and effective way to.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To merge these things.",
                    "label": 0
                },
                {
                    "sent": "I don't have a lot of time left there is one other contribution of this paper which is looking at a way to make the networks robust to the disappearance or noise of a certain modalities.",
                    "label": 0
                },
                {
                    "sent": "So it's a type of dropout that's which we call MoD drop.",
                    "label": 0
                },
                {
                    "sent": "I'll let you read about that if you're interested in in the paper, but it was a.",
                    "label": 0
                },
                {
                    "sent": "It was an interesting additional contribution of the paper, so I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "I can already see there's a question out here out there, so I think what I'm going to do.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Summary and then I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll take questions, so at least we have 5 minutes for questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so we talked about pose estimation.",
                    "label": 0
                },
                {
                    "sent": "We talked about why it was hard.",
                    "label": 0
                },
                {
                    "sent": "All these different reasons.",
                    "label": 0
                },
                {
                    "sent": "Right now it's dominated by convolutional neural Nets, but I think going into the future there's probably gonna be a merger of structured output techniques because the body is very structured as well as sort of very strong visual models.",
                    "label": 0
                },
                {
                    "sent": "People already starting to do this in the scene parsing and seem seem labeling areas.",
                    "label": 0
                },
                {
                    "sent": "They're combining things like MRF's and comments and they're proving very effective in those areas.",
                    "label": 0
                },
                {
                    "sent": "Tracking I said this is really pose estimation plus a dynamical model.",
                    "label": 0
                },
                {
                    "sent": "I talked about some work we did a number of years ago, but I really think this area is kind of underexplored and there's a lot of promise here for deep learning techniques that are very good at predicting the future and also very good at dealing with visual uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Finally, I talked about activity and gesture, which is seen more activity as of late from the deep learning community, but still offers a lot of challenges, particularly in things like multimodal data.",
                    "label": 0
                },
                {
                    "sent": "We showed that you know, you just can throw all these modalities in there and hope for the best.",
                    "label": 0
                },
                {
                    "sent": "Russell.",
                    "label": 0
                },
                {
                    "sent": "Russ salakhutdinov OK. Russ Salakhutdinov is going to do a whole lecture in this summer school about multimodal learning, so you'll be able to hear his perspective on that as well so.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking forward, where do we go from here?",
                    "label": 0
                },
                {
                    "sent": "These types of datasets have much more limited amount of data available to them, so certainly I said this in the last lecture to unsupervised and weakly supervised learning is going to play a part in these domains.",
                    "label": 0
                },
                {
                    "sent": "We want to certainly move, but beyond the classification of very short simple activities and look at structural relationships, understanding social Contacts, right?",
                    "label": 0
                },
                {
                    "sent": "So the combination as I said before of deep learning and structural.",
                    "label": 0
                },
                {
                    "sent": "Structured models will probably prove quite fruitful for these sorts of applications, so again, it's been awesome.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Someone asked me before acknowledging these guys have worked with a lot within the field.",
                    "label": 0
                },
                {
                    "sent": "Greg Maurissa researcher at Simon Fraser.",
                    "label": 0
                },
                {
                    "sent": "These are collaborators.",
                    "label": 0
                },
                {
                    "sent": "Natalia's Co advisor an insulin Christian Wolff and is called Julian Mill and also some researchers Matthew Core and Nicholas Nicola Tom who are at in Paris.",
                    "label": 0
                },
                {
                    "sent": "I've been doing a lot of work with these guys recently in this area and they are responsible for a lot of the background here.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Someone asked where is Guelph OK so Guelph is near Toronto?",
                    "label": 0
                },
                {
                    "sent": "OK, so people don't even know how to say Guelph, so that's how you say it.",
                    "label": 0
                },
                {
                    "sent": "It's a great.",
                    "label": 0
                },
                {
                    "sent": "It's a beautiful city.",
                    "label": 0
                },
                {
                    "sent": "We have a research lab.",
                    "label": 0
                },
                {
                    "sent": "There were very close to the hot spots so we're always looking for great students and postdocs, so keep that in mind when you're when you're looking for a place to go after you're done.",
                    "label": 0
                },
                {
                    "sent": "Whatever you're doing, OK, I'll take questions, thanks.",
                    "label": 0
                }
            ]
        }
    }
}